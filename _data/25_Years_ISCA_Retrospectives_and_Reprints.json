{"25_Years_ISCA_Retrospectives_an":[{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"d41a43c80b20dc28e2659258b10a212aea479df4","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1992","title":"The DASH Prototype: Implementation and Performance","authors":"Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, John L. Hennessy","author_ids":"2350818, 2926266, 2437570, 1791850, 2390831, 1725517, 1772133","abstract":"The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design trade-offs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 16-processor prototype of the DASH multiprocessor has been operational for the last six months. In this paper, the hardware overhead of directory-based cache coherence in the prototype is examined. We also discuss the performance   of the system, and the speedups obtained by parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we characterize the effectiveness of coherent caches and the relationship between an application's reference behavior and its speedup.","cites":"151","conferencePercentile":"50"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"412a59dcc8fbfd797f83fabc437aec3db7bd5f61","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: The Turn Model for Adaptive Routing","authors":"Lionel M. Ni","author_ids":"1726587","abstract":"when the Caltech Cosmic Cube [l] was implemented in 1981, it triggered a new wave on parallel processing and refreshed interest in hyper-cube topology. The research community has studied various direct network architectures, especially the k-ary n-cube, and their topological properties. Based on the underlying graph-theoretical model, many new theories and properties, such as routing paths and graph embedding, were discovered for various network topologies. From the practical aspect, the demand of low communication latency had inspired the design of new switching mechanisms. In 1985, the wormhole routing (now also called wormhole switching or cut-through switching) was implemented in the torus routing chip [2]. While the wormhole routing can significantly reduce the communication latency, it can also introduce a unique deadlock situation , which is quite different from those in other traditional switching mechanisms, such as the store-and-forward switching. Although the concept of virtual channels was proposed in [3] as a possible approach to avoid deadlock, early multi-computers used fixed routing paths, mainly based on dimension order routing, to avoid deadlock due to cost and performance reasons from virtual channels or multiple physical channels. The channel dependence graph model [3] was considered as the theoretical foundation to develop deadlock-free routing algorithms. In January 1991, I offered a graduate-level course on Advanced Computer Systems at Michi-gan State University Due to our past research interest in multicast communication on multicom-puters, message routing was a focus in the course. When studying the 2D mesh network, I mentioned that if we could double the number of channels in both X and Y dimensions, it would support fully adaptive routing. As a homework problem, students were asked to prove that the adaptive routing in 2D mesh networks could be supported by doubling only the channels in either X or Y dimensions. Students had to find out a total ordering of those channels to prove the deadlock-free property. As an open question, students were asked to think about what is the minimum number of channels required to support adaptive routing. Chris, now Dr. Glass, was a Ph.D. student looking for his dissertation research topic. Due to the similarity between the routing algorithm and his interest in the street-walking algorithm (how to walk most quickly from one location to another along the rectilinear streets of a city), Chris chose this topic as his term project. In March 1991, he completed the technical report entitled \" Adaptive, Deadlock-Free, …","cites":"0","conferencePercentile":"27.27272727"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"1e89ddb6dad3fe80ac4c9b3a9e7518de545960df","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1994","title":"The Stanford FLASH Multiprocessor","authors":"Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, John L. Hennessy","author_ids":"1788833, 1770952, 1771536, 1807055, 1779444, 1695693, 4021386, 1791850, 1767544, 1764167, 1725517, 1769458, 1772133","abstract":"The FLASH multiprocessor efficiently integrates support for cache-coherent shared memory and high-performance message passing, while minimizing both hardware and software overhead. Each node in FLASH contains a microprocessor, a portion of the machine's global memory, a port to the interconnection network, an I/O interface, and a custom node controller called MAGIC. The MAGIC chip handles all communication both within the node and among nodes, using hardwired data paths for efficient data movement and a programmable processor optimized for executing protocol operations. The use of the protocol processor makes FLASH very flexible --- it can support a variety of different communication mechanisms --- and simplifies the design and implementation.This paper presents the architecture of FLASH and MAGIC, and discusses the base cache-coherence and message-passing protocols. Latency and occupancy numbers, which are derived from our system-level simulator and our Verilog code, are given for several common protocol operations. The paper also describes our software strategy and FLASH's current status.","cites":"539","conferencePercentile":"100"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"b07454a0a453be6a6cdadaf763746a118dde391d","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: A Processor for a High-Performance Personal Computer","authors":"Kenneth A. Pier","author_ids":"3157529","abstract":"Forward I would like to thank the International Symposium on Computer Architecture for selecting \" A Processor for a High-Performance Personal Computer \" for their 25th anniversary anthology. The editors have asked for a retrospective on the history and genesis of the ideas-what influenced the thinking, why were certain decisions made, and the like, reported in this paper. By happy coincidence , I (Ken Pier) already wrote and published, in ISCA 10, \" A Retrospective on the Dorado, A High-Performance Personal Computer \" [l] with detailed answers to those questions. So I'm going to take the opportunity here to write a more historical , personal, anecdotal, and entertaining (I hope) addition to the description in [l], which may be found at Prologue In 1974, the year of ISCA 1, the revolutionary concept of personal distributed computing was well underway within the walls of the Xerox Corporation at its Palo Alto Research Center. What we now consider commonplace computing: personal hardware dedicated to the individual, with a high-resolution display, mouse input device, network connection to document viewing, filing and printing services, graphical user interface, and a wide variety of applications had all been created there in the space of a very few years. Outside of PARC, the focus was still on mainframes, time-sharing, mini-computers, and special-purpose computers connected together with special-purpose networks. IBM was King, the Seven Dwarfs [2] scrambled at its feet, and that seemed unshakable. Today, hundreds of millions of people make daily use of personal distributed computing, having never even heard that phrase. However, as Alan Perlis and John R. White noted in their 1988 foreword to A History of Personal Workstations [3]: \" The transformation was not trivial, nor even inevitable. If was accomplished by people with energy, ambition, and purpose whose visions were supported by great technical expertise and insight. Their dijiculfies and triumphs are the stuff of which history is made. \" Genesis The Dorado was a proverbial \" second system, \" successor to the Alto personal workstation. Three excellent papers [4,5,6] author of this anthology paper) describe the ideas and implementations that created personal distributed computing, the Alto, and its environment. In brief, Alto was inspired by several visions. One vision came from Alan Kay in the early 1970s of personal dynamic media, to be embodied in a small, lightweight, interactive, personal device christened Dynabook. Dynabook would carry all the personal information, such as …","cites":"0","conferencePercentile":"27.27272727"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"7188757efce68586d2d04e8c4eb58e627ea1a15c","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: The MIT Alewife Machine: Architecture and Performance","authors":"Anant Agarwal","author_ids":"1751259","abstract":"lr 1 he MIT Alewife project evolved out of exploratory work at Stanford on directory schemes for cache coherence [l] (also included in this collection). Using data from small bus-based multipro-cessors, this early work demonstrated that directory schemes were as efficient as bus-based snooping protocols, and that by distributing directories along with main memory, they could provide the foundations for a cache-coherent shared-memory multiprocessor based on an interconnec-tion network. This paper further recognized the scaling limits of bit-vector directories-they consumed memory proportional to the square of the number of processors-and speculated that variants such as limited ointer directories or limited broadcast directories P might be attractive scalable alternatives. The paper, however, stopped short of demonstrating the feasibility of limited directories, largely because of the lack of either address traces or parallel programs written for a scalable coherent shared-memory system. This lack of data was not surprising given that such a machine had not been invented yet! Exploration The Alewife project was born out of a desire to build a shred-memory multiprocessor that was truly scalable (see the section \" Perspectives and Summary \" in the Alewife paper in the Proceedings of the Workshop on Scalable Shared Memory Mull. A limited pointer directory maintains pointers to a fixed number of cached copies of data. A limited broadcast directory divides the processors into sets, and maintains a pointer to each set of processors, sending broadcast invalidations to the entire set when needed. tiprocessors, Kluwer Academic Publishers, 1991, to get a sense of our early thinking). Although scal-able message-passing multicomputers had been around for years, they were known to be notoriously hard to program. We believed that shared memory was easier to program, and accordingly, we chose early on to offer no compromise on the shared memory programming model.' Notice that our early Alewife thinking offered no plans' to expose message passing to the software system. For scalability, we chose to borrow heavily from the message passing machines conceived by researchers such as Seitz and Dally. Message passing machines achieved their scalability by distributing constant per-processor resources over a point-to-point interconnect and exposing this distribution to the programmer. Accordingly, we decided early on to distribute memory and processors over a point-to-point mesh network (as opposed to a uniform-access multistage network) and strove to keep per-node costs more or less constant. We believed that scaling to even tens of processors required support for locality …","cites":"0","conferencePercentile":"27.27272727"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"ebbbb9ad916ad3371ed14a6c129730cf1059c408","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: A Retrospective on High-Level Language Computer Architecture","authors":"David R. Ditzel, David A. Patterson","author_ids":"2512201, 1701130","abstract":"vv ritten at the end of 1979, this paper challenged the 1970's trend in computer architecture toward ever increasing complexity. The decade of the 1970's was dominated by the growing influence of micro-coded minicomputers such as the Digital Equipment Corporation VAX computers. The VAX had over 200 instructions with complex addressing modes. Micro-coded implementations of computer instruction sets meant that adding new instructions was relatively easy. Research in computer architecture during the 1970's often proposed far more complicated computer hardware that would move towards implementing high level languages directly in hardware in order to facilitate programming in high level languages. Recall that during the 1970's there still existed considerable debate as to whether programming in high level languages would be too inefficient in both performance and code space. This paper reflected a mood that something was wrong with the direction of computer architecture research. The paper started by repeating six commonly held beliefs (\" Axioms \") of the day that we felt were not well justified, and ended with an appeal that what mattered was the effect of the combined hardware/software system, and not which individual component was implemented in hardware. In many ways, this retrospective set our direction toward the RISC movement of the 1980's, and heavily influenced us in our research and further publication on RISC processors. Our first paper on RISC was published only five months after this first retrospective paper appeared. Re-reading this paper 18 years later, we are surprised by how well it holds up. Most surprisingly , many of the same issues about High-level Language Computers are again resurfacing with proposals to implement JAVA byte-codes directly in hardware. Stack machines, byte-coded instruc-3 tion sets and small code size were the hallmarks of many of the papers calling for High-level Language Computers. We hope that the next generation of computer designers can learn from the lessons of the past, and not repeat the same mistakes over again. The paper starts with a criticism of the High-Level Language Computer work, including a summary of the High-Level Language Computer motivation as six High-Level Language Computer axioms, followed by our responses to each axiom. From today's perspective, the major observation about making sure a High-Level Language Computer can execute multiple programming languages is still a good one. It is not clear whether the standard JAVA byte-code instructions are also appropriate for implementing languages like C or C++. This …","cites":"1","conferencePercentile":"63.63636364"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"2f293f146fe48ec80a9de3336f727c9267e2cb0c","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: A Retrospective on the Warp Machines","authors":"Thomas R. Gross, Monica S. Lam","author_ids":"1738551, 1711151","abstract":"Context and background This paper presents the hardware and software architecture of the Warp machine, a parallel system of LIW processors. When the paper was written, CMU had just installed in a laboratory the first operational lo-cell system, which had been constructed by GE, one of the industrial partners. A second system was still under construction by Honeywell and was completed a few months later. The Warp system was installed and almost immediately used for application development. Additional software demands were the price of user acceptance-our collaborators wanted to use the system over the network [3], they demanded optimized code [7] and a debugger. When the project started, the plan was to hand microcode a set of core vision library routines. When a compiler effort proved feasible, the application developers stated that programs would be \" simple functions, about l/2 a page of code \". When the compiler was done, programs with a length of 10s of pages were written; the right hand side of one assignment alone contained 11,000 characters. (This statement had been generated by another tool. The compiler translated the statement correctly, but it took 30 minutes.) The Warp array was connected to a UNIX workstation host-this organization contributed significantly to user acceptance of the Warp system. Now (in 1998) UNIX (or a variant) is the dominant operating system for supercomputers, but in 1985, connecting a compute engine (the Warp array) to a workstation was a novel implementation. Overall this decision was a solid one and set the tone for the next 10 years of system development. 1. Evolution As we gained experience with the system, we noticed a number of features that limited the usefulness of the system. The biggest problem (already mentioned in the paper) is the tight coupling of the cells: if a cell sends data into a full input queue, then data are lost. This tight coupling made it impossible for the compiler to support the pipeline mode in practice. Since the Warp machine at CMU performed really well for vision applications, several ARPA projects decided to use the Warp machine. The wire-wrap systems however were hard to repli-cate, and since at least one system was targeted for a moving platform, the users demanded an implementation on PC boards. CMU was in no position to produce systems, but GE or Honeywell (the partners of the wire-wrap phase) lacked the design expertise. Therefore the …","cites":"0","conferencePercentile":"27.27272727"},{"venue":"25 Years ISCA: Retrospectives and Reprints","id":"62a1881eb6da66f0cfbb19d22e8197ed9066b748","venue_1":"25 Years ISCA: Retrospectives and Reprints","year":"1998","title":"Retrospective: RISC I: A Reduced Instruction Set Computer","authors":"David A. Patterson, Carlo H. Séquin","author_ids":"1701130, 1743300","abstract":"Jl his 1981 paper was written as part of the RISC movement that began to flourish in the early 1980s. The three groups leading the charge were at IBM, Berkeley, and Stanford. IBM was the earliest, focusing on advances in compiler technology and instruction sets that compilers could use to get good performance without the need for a microcode interpreter. Their targets were a 24-bit ECL minicomputer for hardware, called the 801, and a programming language they invented called PLB, and their competition was the IBM 370 family of computers. As the introduction to this paper suggests, the Berkeley effort was in trying to design an instruction set that made sense for a single VLSI chip. Our group did not include compiler experts, so that was not something that we were pushing. Our targets were a 40,000 transistors, 32-bit NMOS microprocessor and the programming language C and UNIX operating system, and the competition was the VAX-11 /780, a relatively new machine that was making big waves in the marketplace. The Stanford effort was also interested in a 32-bit single chip microprocessor, called MIPS for Microprocessor without Interlocked Pipeline Stages, and since Hennessy knew compilers they pushed it as well. They concentrated on the Pascal language, and while they didn't typically compare to other machines, occasionally they compared to the PDP-10. The Berkeley RISC effort was inspired in large part by Patterson's reaction to a sabbatical he took at DEC in Fall 1979, and by our goal to make our architecture courses \" hands-on \" and as relevant as possible. This was the first time a university planned to actually build a complete microprocessor on a chip, and many people let us know that we had almost zero chance of success. So we were well aware that we had to keep the structure and the logic of this chip as simple as we could get away with. Sequin, at that time, was involved as a consultant in the Mead-Conway revolution of getting universities involved in chip design. Having previously built several chips at Bell Labs, he was more aware of what it would take to make a working chip, but tried to hide his anxieties in order not to dampen the enthusiasm for the project. Patterson had worked on microprogramming tools for his Ph.D., and that was what he had been helping with at DEC. He wondered about building a VAX …","cites":"3","conferencePercentile":"83.33333333"}]}