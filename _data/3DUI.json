{"3DUI.csv":[{"venue":"3DUI","id":"18d9e9dd7a9c505c1ca57f485f7e0c88faa1a8c2","venue_1":"3DUI","year":"2010","title":"Towards a handheld stereo projector system for viewing and interacting in virtual worlds","authors":"Andrew Miller, Joseph J. LaViola","author_ids":"7691539, 1726039","abstract":"We present a proof-of-concept implementation of a handheld stereo projection display system for virtual worlds. We utilize a single pico projector coupled with a six DOF tracker to generate real-time stereo imagery that can be projected on walls or a projection screen.We discuss the iterative design of our display system, including three attempts at modifying our portable projector to produce stereo imagery, and the hardware and software tradeoff decisions made in our prototype.","cites":"0","conferencePercentile":"10.52631579"},{"venue":"3DUI","id":"9ac04423c5955871823ce1076b97d687d8e11f6b","venue_1":"3DUI","year":"2010","title":"RealNav: Exploring natural user interfaces for locomotion in video games","authors":"Brian Williamson, Chadwick A. Wingrave, Joseph J. LaViola","author_ids":"7320293, 3293975, 1726039","abstract":"We present a reality-based locomotion study directly applicable to video game interfaces; specifically, locomotion control of the quarterback in American football. Focusing on American football drives requirements and ecologically grounds the interface tasks of: running down the field, maneuvering in a small area, and evasive gestures such as spinning, jumping, and the \" juke. \" The locomotion interface is constructed by exploring data interpretation methods on two commodity hardware configurations. The choices represent a comparison between hardware available to video game designers, trading off traditional 3D interface data for greater hardware availability. Configuration one matches traditional 3D interface data, with a commodity head tracker and leg accelerometers for running in place. Configuration two uses a spatially convenient device with a single accelerometer and infrared camera. Data interpretation methods on configuration two use two elementary approaches and a third hybrid approach, making use of the disparate and intermittent input data combined with a Kalman filter. Methods incorporating gyroscopic data are used to further improve the interpretation. Our results show spatially convenient hardware, currently in many gamers' homes, when properly interpreted can lead to more robust interfaces. We support this by a user evaluation on the metrics of position and orientation accuracy, range and gesture recognition. 1 INTRODUCTION Physical locomotion in video games cannot rely on robust traditional 3D commercial trackers due to costs, maintenance and installation issues [21]. As such, tracking the user's natural and realistic motions must be performed with commodity consumer-oriented hardware. Additionally, video game control requires more than just position data that commercial trackers provide, so even these systems present recognition problems for game-specific actions such as jumping or walking in place. Figure 1 – Reality-based interaction in an American football video game In this paper, we explore physical locomotion video game interfaces using commodity hardware. Specifically, American football was the video game genre used due to its physical requirements of motion and interactive movement (see Figure 1). To address the needs of football video games with commodity hardware, we used the Natural Point TrackIR device, which provides typical positional and orientation tracking, but is difficult to utilize for all of the requirements in American Football locomotion. As such, this was contrasted with the data of a spatially convenient device [21], the Nintendo Wii Remote (Wiimote), which provides multiple spatial data channels but are partial and intermittent (see Figure 2). Both are commodity hardware designed for use by …","cites":"5","conferencePercentile":"52.63157895"},{"venue":"3DUI","id":"1e7991f7b297089eca79db1329cfc5c594167a22","venue_1":"3DUI","year":"2007","title":"The Visual Appearance of User's Avatar Can Influence the Manipulation of Both Real Devices and Virtual Objects","authors":"Abdelmajid Kadri, Anatole Lécuyer, Jean-Marie Burkhardt","author_ids":"2578253, 1693899, 2899899","abstract":"This paper describes two experiments conducted to study the influence of visual appearance of user's avatar (or 3D cursor) on the manipulation of both interaction devices and virtual objects in 3D Virtual Environments (VE). In both experiments, participants were asked to pick up a virtual cube and place it at a random location in a VE. The first experiment showed that the visual appearance of a 3D cursor could influence the participants in the way they manipulated the real interaction device. The participants changed the orientation of their hand as function of the orientation suggested visually by the shape of the 3D cursor. The second experiment showed that one visual property of the avatar (i.e., the presence or absence of a directional cue) could influence the way participants picked up the cube in the VE. When using avatars or 3D cursors with a strong directional cue (e.g., arrows pointing to the left or right), participants generally picked up the cube by a specific side (e.g., right or left side). When using 3D cursors with no main directional cue, participants picked up the virtual cube more frequently by its front or top side. Taken together our results suggest that some visual aspects (such as directional cues) of avatars or 3D cursors chosen to display the user in the VE could partially determine his/her behaviour during manipulation tasks. Such an influence could be used to prevent wrong uses or to favour optimal uses of manipulation interfaces such as haptic devices in virtual environments. 1 INTRODUCTION Numerous devices and interaction techniques have been designed during the past decade to define efficient ways of selecting and manipulating objects in Virtual Environments (VE) [1]. One of the most famous techniques is the \" virtual hand \" developed by Robinett and Holloway [2] that makes an analogy with our natural way of picking up objects in reality. A 3D cursor which looks like a hand is used to match the motions of the user's real hand and to","cites":"3","conferencePercentile":"46.66666667"},{"venue":"3DUI","id":"442bd37c76fd8cd6de174bd081fefd03a7fa4fd3","venue_1":"3DUI","year":"2015","title":"LazyNav: 3D ground navigation with non-critical body parts","authors":"Emilie Guy, Parinya Punpongsanon, Daisuke Iwai, Kosuke Sato, Tamy Boubekeur","author_ids":"2094959, 1905352, 1752105, 1808503, 1747280","abstract":"Figure 1: We performed a user study to evaluate several ground navigation metaphors. Top-left: Top view of the virtual city, users have to follow the green path as closely as possible. Top-right: At the beginning of the session, the available motions are displayed to the user, while her rest pose is captured. Bottom: a user is travelling in the scene while holding a cup in his hand. ABSTRACT With the growing interest in natural input devices and virtual reality, mid-air ground navigation is becoming a fundamental interaction for a large collection of application scenarios. While classical input devices (e.g., mouse/keyboard, gamepad, touchscreen) have their own ground navigation standards, natural input techniques still lack acknowledged mechanisms for travelling in a 3D scene. In particular , for most applications, navigation is not the primary interaction. Thus, the user should navigate in the scene while still being able to perform other interactions with her hands, and observe the displayed content by moving her eyes and locally rotating her head. Since most ground navigation scenarios require only two degrees of freedom to move forward or backward and rotate the view to the left or to the right, we propose LazyNav a mid-air ground navigation control model which lets the users hands, eyes or local head orientation completely free, making use of a single pair of the remaining tracked body elements to tailor the navigation. To this end, we design several navigation body motions and study their desired properties, such as being easy to discover, easy to control, socially acceptable, accurate and not tiring. We also develop several assumptions about motions design for ground navigation and evaluate them. Finally, we highlight general advices on mid-air ground navigation techniques.","cites":"2","conferencePercentile":"77.77777778"},{"venue":"3DUI","id":"93b04dc479860a620c873f05a86e4bc20af79aad","venue_1":"3DUI","year":"2010","title":"Immersive virtual studio for architectural exploration","authors":"Gerd Bruder, Frank Steinicke, Dimitar Valkov, Klaus H. Hinrichs","author_ids":"1867700, 1740244, 1962138, 1685162","abstract":"(a) (b) (c) (d) Figure 1: Virtual studio: (a) modified video-see-through eMagin Z800 head-mounted display, and (b) augmented virtual self-representation (inset showing augmented virtual view), (c) augmented virtual desktop workspace (left inset showing applied green and blue color key segmentation, right inset showing applied green color segmentation only), and (d) markers used to place and combine designs. ABSTRACT Architects use a variety of analog and digital tools and media to plan and design constructions. Immersive virtual reality (VR) technologies have shown great potential for architectural design, especially for exploration and review of design proposals. In this work we propose a virtual studio system, which allows architects and clients to use arbitrary real-world tools such as maps or rulers during immer-sive exploration of virtual 3D models. The user interface allows architects and clients to review designs and compose 3D architectural scenes, combining benefits of mixed-reality environments with im-mersive head-mounted display setups. 1 BACKGROUND Modern computer-based media have shown to be useful throughout architectural design processes and the life cycle of architectural constructions. In the last stages of the architectural design process, analog as well as digital models give architects and clients a good impression of the final result. In this context, immersive virtual environments (IVEs) have great potential to enhance architectural design. In IVEs users get a natural impression of virtual 3D designs from a realistic point of view via head-tracking, which allows them to turn and move their heads to explore the 3D models. In head-mounted display (HMD) environments users can explore 3D models at real scale by walking [10] or flying [1] through the virtual space. Recently developed user interfaces based on redirected walking [6] allow users to explore large virtual scenes on foot while walking in the restricted area of a VR laboratory. With information about the structure of virtual scenes, specifically in the context of architecture, large models can be subdivided in cells of approximately the size of the laboratory room, and explored sequentially using virtual doorways and portals as means for redirection, which ensure that the user does not collide with obstacles in the real world (cf. the Arch-Explore user interface [2]). In the context of mixed reality environments, video-see-through HMDs provide application developers with the ability to combine views from the real world with rendered views to a virtual environment (VE). Using simple color keying approaches [5, 9] (cf. chroma keying), all areas of the …","cites":"0","conferencePercentile":"10.52631579"},{"venue":"3DUI","id":"0e26597fb2cf2b7291da7a827f0cc4382c1ed8a2","venue_1":"3DUI","year":"2009","title":"Arch-Explore: A natural user interface for immersive architectural walkthroughs","authors":"Gerd Bruder, Frank Steinicke, Klaus H. Hinrichs","author_ids":"1867700, 1740244, 1685162","abstract":"In this paper we propose the Arch-Explore user interface, which supports natural exploration of architectural 3D models at different scales in a real walking virtual reality (VR) environment such as head-mounted display (HMD) or CAVE setups. We discuss in detail how user movements can be transferred to the virtual world to enable walking through virtual indoor environments. To overcome the limited interaction space in small VR laboratory setups, we have implemented redirected walking techniques to support natural exploration of comparably large-scale virtual models. Furthermore , the concept of virtual portals provides a means to cover long distances intuitively within architectural models. We describe the software and hardware setup and discuss benefits of Arch-Explore.","cites":"25","conferencePercentile":"95.83333333"},{"venue":"3DUI","id":"6685ed22161423d88bafa65fd1979c1c4749c7eb","venue_1":"3DUI","year":"2010","title":"Walking up and down in immersive virtual worlds: Novel interactive techniques based on visual feedback","authors":"Maud Marchal, Anatole Lécuyer, Gabriel Cirio, Laurent Bonnet, Mathieu Emily","author_ids":"1722424, 1693899, 3282744, 3304645, 2926394","abstract":"We introduce novel interactive techniques to simulate the sensation of walking up and down in immersive virtual worlds based on visual feedback. Our method consists in modifying the motion of the virtual subjective camera while the user is really walking in an immersive virtual environment. The modification of the virtual viewpoint is a function of the variations in the height of the virtual ground. Three effects are proposed: (1) a straightforward modification of the camera's height, (2) a modification of the camera's navigation velocity, (3) a modification of the camera's orientation. They were tested in an immersive virtual reality setup in which the user is really walking. A Desktop configuration where the user is seated and controls input devices was also tested and compared to the real walking configuration. Experimental results show that our visual techniques are very efficient for the simulation of two canon-ical shapes: bumps and holes located on the ground. Interestingly, a strong \" orientation-height illusion \" is found, as changes in pitch viewing orientation produce perception of height changes (although camera's height remains strictly the same in this case). Our visual effects could be applied in various virtual reality applications such as urban or architectural project reviews or training, as well as in videogames, in order to provide the sensation of walking on uneven grounds.","cites":"7","conferencePercentile":"73.68421053"},{"venue":"3DUI","id":"e5e87746ca1d6baa60b7ff84b612070a6de5c1be","venue_1":"3DUI","year":"2009","title":"Poster: A virtual body for augmented virtuality by chroma-keying of egocentric videos","authors":"Frank Steinicke, Gerd Bruder, Kai Rothaus, Klaus H. Hinrichs","author_ids":"1740244, 1867700, 2125398, 1685162","abstract":"(a) (b) (c) Figure 1: Virtual body in an augmented virtuality scenario: (a) A user in an immersive virtual environment with a video-see-through HMD mockup. (b) The user's view in the virtual world with visualization of his virtual hands in an indoor museum environment and (c) with virtual lower part of the body on the glass bridge of a virtual model of the Grand Canyon Skywalk. ABSTRACT A fully-articulated visual representation of oneself in an immersive virtual environment has considerable impact on the subjective sense of presence in the virtual world. Therefore, many approaches address this challenge and incorporate a virtual model of the user's body in the VE. Such a \" virtual body \" (VB) is manipulated according to user motions which are defined by feature points detected by a tracking system. The required tracking devices are unsuitable in scenarios which involve multiple persons simultaneously or in which participants frequently change. Furthermore, individual characteristics such as skin pigmentation, hairiness or clothes are not considered by this procedure. In this paper we present a software-based approach that allows to incorporate a realistic visual representation of oneself in the VE. The idea is to make use of images captured by cameras that are attached to video-see-through head-mounted displays. These ego-centric frames can be segmented into foreground showing parts of the human body and background. Then the extremities can be over-layed with the user's current view of the virtual world, and thus a high-fidelity virtual body can be visualized.","cites":"2","conferencePercentile":"37.5"},{"venue":"3DUI","id":"ba3233981f1d8a25957a01383b293bbb2c17c5db","venue_1":"3DUI","year":"2008","title":"Poster: Generic Redirected Walking & Dynamic Passive Haptics: Evaluation and Implications for Virtual Locomotion Interfaces","authors":"Frank Steinicke, Gerd Bruder, Timo Ropinski, Klaus H. Hinrichs","author_ids":"1740244, 1867700, 1703058, 1685162","abstract":"unster virtual direction real curve virtual turn real distance virtual distance real turn real environment HMD backpack proxy object Figure 1: Virtual Locomotion Scenario: a user walks through the real environment on a different path with a different length in comparison to the perceived path in the virtual world (left). The user touches a real proxy object (middle), which is considerably smaller than the virtual object seen from the user's perspective (right) (alternative representations provide different levels of optical flow (insets) (right)). ABSTRACT In this paper we introduce concepts for virtual locomotion interfaces that support exploration of large-scale virtual environments (VEs) by real walking. Based on the results of a pilot study we have quantified to which degree users can unknowingly be redirected in order to guide them through an arbitrarily sized VE in which virtual paths differ from the paths tracked in the real working space. We further introduce the concept of dynamic passive haptics which enables a user to interact with a VE. By means of this concept any number of virtual objects can be mapped to real proxy objects having similar haptic properties, i. e., size, shape and surface structure, such that the user can sense these virtual objects by touching their real world counterparts. Thus dynamic passive haptics provides the user with the illusion of interacting with a desired virtual object by redirecting her/him to the corresponding proxy object.","cites":"1","conferencePercentile":"30.55555556"},{"venue":"3DUI","id":"f3175ca934e88ff189d376e0fc0a803f751ba365","venue_1":"3DUI","year":"2013","title":"Effects of visual conflicts on 3D selection task performance in stereoscopic display environments","authors":"Gerd Bruder, Frank Steinicke, Wolfgang Stuerzlinger","author_ids":"1867700, 1740244, 3342964","abstract":"Mid-air direct-touch interaction in stereoscopic display environments poses challenges to the design of 3D user interfaces. Not only is passive haptic feedback usually absent when selecting a virtual object displayed with positive or negative parallax relative to a display surface, but such setups also suffer from inherent visual conflicts, such as vergence/accommodation mismatches and double vision. In particular, if the user tries to select a virtual object with a finger or input device, either the virtual object or the user's finger will appear blurred, resulting in an ambiguity for selections that may significantly impact the user's performance. In this paper we evaluate the effect of visual conflicts for mid-air 3D selection performance within arm's reach on a stereoscopic table with a Fitts' Law experiment. We compare three different techniques with different levels of visual conflicts for selecting a virtual object: real hand, virtual offset cursor, and virtual offset hand. Our results show that the error rate is highest for the real hand condition and less for the virtual offset-based techniques. However, our results indicate that selections with the real hand resulted in the highest effective throughput of all conditions. This suggests that virtual offset-based techniques do not improve overall performance.","cites":"13","conferencePercentile":"100"},{"venue":"3DUI","id":"37c4865b51c3ed1873210fdb266a4707a164da23","venue_1":"3DUI","year":"2010","title":"A multi-touch enabled human-transporter metaphor for virtual 3D traveling","authors":"Dimitar Valkov, Frank Steinicke, Gerd Bruder, Klaus H. Hinrichs","author_ids":"1962138, 1740244, 1867700, 1685162","abstract":"In this tech-note we demonstrate how multi-touch hand gestures in combination with foot gestures can be used to perform navigation tasks in interactive 3D environments. Geographic Information Systems (GIS) are well suited as a complex testbed for evaluation of user interfaces based on multi-modal input. Recent developments in the area of interactive surfaces enable the construction of low-cost multi-touch displays and relatively inexpensive sensor technology to detect foot gestures, which allows to explore these input modalities for virtual reality environments. In this tech-note, we describe an intuitive 3D user interface metaphor and corresponding hardware, which combine multi-touch hand and foot gestures for interaction with spatial data.","cites":"3","conferencePercentile":"31.57894737"},{"venue":"3DUI","id":"d47b356557edfa9c997d5bd714102e6d02d70f2e","venue_1":"3DUI","year":"2012","title":"Collaborative exploration in a multi-scale shared virtual environment","authors":"Thi Thuong Huyen Nguyen, Cédric Fleury, Thierry Duval","author_ids":"1759573, 3327755, 2790132","abstract":"Figure 1: The main user in immersion and the helping user in front of the desktop views ABSTRACT For the 3DUI 2012 contest, we propose a set of metaphors that enable two users to collaborate to find hidden targets in a 3D virtual environment. They are not allowed to communicate by talking or chatting, but only by interacting with shared virtual objects. We propose three solutions to enable one of the users to help the other one to navigate toward targets: displaying direction arrows, lighting up the path to one target, and remotely moving the second user.","cites":"6","conferencePercentile":"79.03225806"},{"venue":"3DUI","id":"acaf07cd4c51c96af8aa4402941cd6fd46a59d00","venue_1":"3DUI","year":"2006","title":"SkeweR: a 3D Interaction Technique for 2-User Collaborative Manipulation of Objects in Virtual Environments","authors":"Thierry Duval, Anatole Lécuyer, Sébastien Thomas","author_ids":"2790132, 1693899, 2323569","abstract":"This paper describes a novel 3D interaction technique called the \" SkeweR \" , dedicated to the 2-user collaborative manipulation of objects in virtual environments. This technique enables two users to move simultaneously the same virtual object in 3D. For this aim, each user manipulates the object by one crushing point, like handling the extremity of a skewer. The SkeweR uses only translation information from the users' motions to change both the position and orientation of the manipulated object. By using more crushing points, this technique could easily be extended to 3 or more users. Thus, the SkeweR technique could be used to improve the collaborative manipulation of objects in numerous applications of Virtual Reality, such as: virtual prototyping, maintenance and training simulations, architectural mock-up reviews, etc.","cites":"18","conferencePercentile":"79.41176471"},{"venue":"3DUI","id":"30fd43b7d5a229d6d2c84850c1ecc79de8bf3020","venue_1":"3DUI","year":"2014","title":"Poster: Superhumans: A 3DUI design metaphor","authors":"Ahmed E. Mostafa, Ehud Sharlin, Mario Costa Sousa","author_ids":"8216429, 1800680, 1777294","abstract":"We propose a design metaphor we call superhumans in order to empower 3D Interaction Designers to create and implement interactive mixed and virtual reality environments, and to help users familiarize themselves with interactive system's capabilities, behaviour and limitations. We describe how our superhumans metaphor can help 3DUI designers explore various sources of inspirations such as narration (storytelling) and transitional environments, when designing their systems.","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"e8d7617441239883c93c884dad4da9c28b854c85","venue_1":"3DUI","year":"2013","title":"Redirected Touching: Training and adaptation in warped virtual spaces","authors":"Luv Kohli, Mary C. Whitton, Frederick P. Brooks","author_ids":"2645070, 1761150, 1795780","abstract":"Redirected Touching is a technique in which virtual space is warped to map many virtual objects onto one real object that serves as a passive haptic prop. Recent work suggests that this mapping can often be predictably unnoticeable and have little effect on task performance. We investigated training and adaptation on a rapid aiming task in a real environment, an unwarped virtual environment, and a warped virtual environment. Participants who experienced a warped virtual space reported an initial strange sensation, but adapted to the warped space after short repeated exposure. Our data indicate that all the virtual training was less effective than real-world training, but after adaptation, participants trained as well in a warped virtual space as in an unwarped one.","cites":"4","conferencePercentile":"80"},{"venue":"3DUI","id":"2948c5f6ea1926cdd1f41c8395339f6f8f241366","venue_1":"3DUI","year":"2006","title":"A Hybrid User Interface for Manipulation of Volumetric Medical Data","authors":"Alexander Bornik, Reinhard Beichel, Ernst Kruijff, Bernhard Reitinger, Dieter Schmalstieg","author_ids":"1742270, 1749679, 2339768, 1696492, 1742819","abstract":"This paper presents a novel system for interactive visualization and manipulation of medical datasets for surgery planning based on a hybrid VR / Tablet PC user interface. The goal of the system is to facilitate efficient visual inspection and correction of surface models generated by automated segmentation algorithms based on x-ray computed tomography scans, needed for planning surgical resec-tions of liver tumors. Factors like the quality of the visualization, nature of the dataset and interaction efficiency strongly influence system design decisions, in particular the design of the user interface , input devices and interaction techniques, leading to a hybrid setup. Finally, a user study is presented, which characterizes the system in terms of method efficiency and usability.","cites":"18","conferencePercentile":"79.41176471"},{"venue":"3DUI","id":"2e645cb93c8b94cf4f3394c04b4b322746e4a2fd","venue_1":"3DUI","year":"2012","title":"Redirected touching: The effect of warping space on task performance","authors":"Luv Kohli, Mary C. Whitton, Frederick P. Brooks","author_ids":"2645070, 1761150, 1795780","abstract":"Figure 1: A user touches a virtual board that is oriented differently than the real board providing passive haptic feedback ABSTRACT Passive haptic feedback in virtual environments is compelling, but changes to virtual objects require changes to associated real objects. Recent work suggests that by leveraging visual dominance, virtual space can be warped to map a variety of virtual objects onto a single real object. However, it is unknown whether users can interact with a warped virtual space as effectively as with an unwarped one. We present a study in which we measured task performance using the Fitts'-law-based ISO 9241-9 multi-directional tapping task. With a few caveats, results suggest that for certain tasks, warped virtual objects are no worse than unwarped virtual objects. We also present preliminary exploratory data on how well people can detect discrepancies due to space-warping. 1 INTRODUCTION In virtual environments (VEs), a common way to provide users with touch feedback is to use passive haptics, or physical props to which virtual objects are mapped. This mapping is traditionally one-to-one. The result is compelling, because users touch a real object. However, passive haptic displays are inflexible; changing a virtual object requires changes to its associated real object. Researchers have addressed this inflexibility mechanically, via Robotic Shape Displays [16]: when a user reaches for a virtual object, a robotic arm places a real object correctly in front of the user's hand. One such robot has a Shape Approximation Device as its end-effector, which has several corners and curved and flat edges to approximate different shapes [22]. While impressive, these haptic displays are expensive and require sophisticated control mechanisms, and miscalculations and latency could be dangerous to users. This paper instead further explores Redirected Touching, a perception-based technique for addressing the inflexibility of passive haptic displays [11]. The technique generates different mappings between real and virtual space such that a single real object can provide haptic feedback for many differently shaped virtual objects. These different mappings are created by warping virtual space. This warping introduces a discrepancy between a user's real and virtual hand motion: the virtual hand moves in virtual directions different from its real-world motion, such that the real and virtual hands reach the real and virtual objects simultaneously. We call the case where real and virtual objects are the same one-to-one; otherwise, discrepant. Discrepant stimuli are sometimes introduced in VEs to useful effect. For example, in …","cites":"7","conferencePercentile":"85.48387097"},{"venue":"3DUI","id":"4e5815c0379a59a13bc6dd2d163c6cffc905e1db","venue_1":"3DUI","year":"2008","title":"LLCM-WIP: Low-Latency, Continuous-Motion Walking-in-Place","authors":"Jeff Feasel, Mary C. Whitton, Jeremy D. Wendt","author_ids":"2425712, 1761150, 2265589","abstract":"Walking-in-place techniques for locomotion in virtual environments typically have two problems that impact their usability: system latency (particularly troublesome when starting and stopping locomotion), and the fact that the change in the user's viewpoint may not be smooth and continuous. This paper describes a new WIP interface that improves both latency and the continuity of synthesized locomotion in the virtual environment. By basing the virtual avatar motion on the speed of the user's heel motion while walking in place, we create a direct mapping from foot-motion to locomotion that is responsive, intuitive, and easy to implement. In this paper, we describe the technique, analyze its starting and stopping latency, and provide experimental results on the suppression of false steps and general usability of the system. 1 INTRODUCTION The quality of a virtual-environment (VE) locomotion interface has a significant impact on the level of presence a user feels in a virtual environment [1, 2] and the interface affects the way a user moves [3]. Although head-tracked real walking in VEs consistently evokes user behavior most like walking in the real world, locomotion by really walking is impractical in large-scale VEs, because the tracked space must be as large as the virtual space. Scaling high-precision tracking systems to arbitrarily-large sizes is expensive; wide-area tracking systems do not provide sufficient precision for a first-person display [4]. Because of these problems, the many VEs that require locomotion in large virtual scenes employ interfaces through which users can move their avatar (and viewpoint) to anywhere in the scene while remaining essentially stationary in the real world. Although various stationary-user locomotion interfaces have been proposed, previous research demonstrated that walking-in-place (WIP) is more presence-inducing than pointing interfaces [1, 2]. One of WIP's greatest strengths is its similarity to real walking: The user controls their motion by moving their legs. From experience with our own WIP systems and others described in the in the literature [4, 5, 6, 7], we have identified two problems that impact WIP usability: system latency (particularly troublesome when starting and stopping movement), and the fact that the change in the user's viewpoint may not be smooth and continuous. Latency in visual feedback decreases the user's ability to precisely control their speed and stopping-position. Figure 1: The LLCM-WIP hardware. (a) Chest-orientation tracker (b) Foot trackers (rigid offset from mount-point to heel shown), In some systems viewpoint-movement is implemented as one or a series …","cites":"35","conferencePercentile":"100"},{"venue":"3DUI","id":"1ddc727e338b4718f3d5e58ace6f203b3c6c1fd7","venue_1":"3DUI","year":"2006","title":"The VR Scooter: Wind and Tactile Feedback Improve User Performance","authors":"Leonidas Deligiannidis, Robert J. K. Jacob","author_ids":"1745834, 1723792","abstract":"We present an experiment using the VR scooter in an Immersive Virtual Environment (IVE). The scooter is used for traveling naturally and easily in large scale Virtual Environments. Vibrotactile tactors are mounted on the VR scooter to simulate motion speed, and a fan is used to simulate wind blowing when the VR scooter is in motion. We compared user performance (time to complete a task) with and without the tactors and fan, and we observed reduced time with these additions. While they were originally designed to make the experience more realistic and convincing, we found that they also improved user performance. 1 INTRODUCTION We present a new input device, the VR scooter, for easily and naturally traveling in large Immersive Virtual Environments (IVEs). We analyze the human factors issues in designing a navigation interface incorporated with the VR scooter, which provides multiple degrees of freedom. The VR scooter is based on a real world metaphor, the scooter. We believe it is \" natural \" to use because most people can utilize skills they gained in the real world and apply them in IVEs naturally, and that includes riding a scooter, a bicycle, a motorcycle, a car, etc. Limitations and future improvements are also presented. The VR scooter was first introduced in [13]; here we present our experimental results along with further development of the scooter. Designing a 3D traveling technique is difficult in general. The traveling technique should be effective, easy to learn, and user friendly. Usually, the implementation of a traveling technique requires at least an input device. The input device should be natural to the user to use and also easy, so that the user does not have to remember to perform a specific coded gesture to change the speed of flight or to switch modes between drive and fly, for example. The interface becomes more complex when the traveling technique provides multiple degrees of freedom. To implement an effective and user friendly traveling technique, depends primarily on the input device; the device that instructs the computer of the user's intent. Large scale Immersive Virtual Environments (IVEs) are common in current research. One of the major problems in large scale IVEs, however, is traveling. Because most people know how to ride a bicycle, riding the VR scooter does not require much additional training, which makes the VR scooter a user-friendly device. Novice users may need some training since …","cites":"13","conferencePercentile":"64.70588235"},{"venue":"3DUI","id":"3c5df9b5ac4e4f05acd8dc0dd9fd3b2662bfb096","venue_1":"3DUI","year":"2008","title":"Navidget for Easy 3D Camera Positioning from 2D Inputs","authors":"Martin Hachet, Fabrice Decle, Sebastian Knödel, Pascal Guitton","author_ids":"2281511, 2483137, 2717762, 2010548","abstract":"Navidget is a new interaction technique for camera positioning in 3D environments. This technique derives from the Point-of-Interest (POI) approaches where the endpoint of a trajectory is selected for smooth camera motions. Unlike the existing POI techniques, Navidget does not attempt to automatically estimate where and how the user wants to move. Instead, it provides good feedback and control for fast and easy interactive camera positioning. Navidget can also be useful for distant inspection when used with a preview window. This new 3D User interface is totally based on 2D inputs. As a result, it is appropriate for a wide variety of visualization systems , from small handheld devices to large interactive displays. A user study on TabletPC shows that the usability of Navidget is very good for both expert and novice users. This new technique is more appropriate than the conventional 3D viewer interfaces for some camera positioning tasks in 3D environments.","cites":"33","conferencePercentile":"94.44444444"},{"venue":"3DUI","id":"9af3cae422f46aa6e92bc387496290c673855e97","venue_1":"3DUI","year":"2014","title":"Poster: Dynamic adaptation of 3D selection techniques for suitability across diverse scenarios","authors":"Jeffrey Cashion, Joseph J. LaViola","author_ids":"2926812, 1726039","abstract":"We performed a user study that measured the effectiveness of our new 3D selection technique, Scope, which dynamically adapts to the environment by altering its activation area and visual appearance with relation to cursor velocity. Users tested our new technique against existing techniques Raycast, Bendcast, and Hook across a variety of different 3D scenarios which featured three different levels of object density and three different levels of object velocity. Our two dependent variables were completion time and total attempts per scenario. Users also completed a post-questionnaire which yielded qualitative insights on their experience. Our study shows that Bendcast, Scope, and Hook all performed similarly across all scenarios, yet were all significantly faster and less error-prone than Raycast. Despite this similar performance, users strongly favored Scope over the other three techniques, and over the second most preferred technique nearly two to one. 1 INTRODUCTION The rapid increase in graphical processing power has made 3D graphics much more accessible to common devices. Scenes can be much richer and more detailed, with a higher number of objects and richer detail. With 3D visuals being the primary output of such software, one of the primary inputs is user selection. In video games, a user may select something as simple as a menu option, or something as complex as a small moving object hidden in foliage. Scientific applications such as medical simulators or bio-molecular visualization also rely heavily on precise and accurate selection. In any case, it is important that the selection techniques utilized best match the expected environment and its conditions. With such importance placed on selection, it is no surprise that much work has been done to reduce selection time, reduce errors, and increase overall usability [6] [3]. Comprehensive research has been done on categorizing and grouping techniques by common features and characteristics [1]. A primary characteristic of a technique is whether it operates statically or dynamically. Our work focuses on further exploring the area of dynamic selection techniques. Our goal was to develop a technique that would adapt to user input and dynamically change how it operated. We explored the existing field of techniques and identified a few that had features which could be worked with.","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"69ed63fb5b8cee97ae0eef175693bf6d50e4d972","venue_1":"3DUI","year":"2013","title":"Optimal 3D selection technique assignment using real-time contextual analysis","authors":"Jeffrey Cashion, Chadwick A. Wingrave, Joseph J. LaViola","author_ids":"2926812, 3293975, 1726039","abstract":"Selection in 3D virtual environments can vary wildly depending on the context of the selection. Various scene attributes such as object velocity and scene density will likely impact the user's ability to accurately select an object. While there are many existing 3D selection techniques that have been well studied, they all tend to be tailored to work best in a particular set of conditions, and may not perform well when these conditions are not met. As a result, designers must compromise by taking a holistic approach to choosing a primary technique; one which works well overall, but is possibly lacking in at least one scenario. We present a software framework that allows a flexible method of leveraging several selection techniques, each performing well under certain conditions. From these, the best one is utilized at any given moment to provide the user with an optimal selection experience across more scenarios and conditions. We performed a user study comparing our framework to two common 3D selection techniques, Bendcast and Expand. We evaluated the techniques across three levels of scene density and three levels of object velocity, collecting accuracy and timing data across a large sample of participants. From our results, we were able to conclude that our auto-selection technique approach is promising but there are several characteristics of the auto-selection process that can introduce drawbacks which need to be addressed and minimized.","cites":"2","conferencePercentile":"56"},{"venue":"3DUI","id":"cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0","venue_1":"3DUI","year":"2014","title":"HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications","authors":"Natalia Bogdan, Tovi Grossman, George W. Fitzmaurice","author_ids":"2098085, 3313809, 1703735","abstract":"Technologies for 3D input and output are rapidly advancing, and are becoming more common in home and workplace environments. However, viewing a stereoscopic display can cause eye strain and fully relying on 3D spatial interactions can be fatiguing and may be less efficient for common 2D tasks. In this paper we explore the design possibilities of transitioning between 2D and 3D modalities to best support the user's current task. In a formal study, we demonstrate that a Hybrid interaction technique, that transitions between 2D display and input, to 3D, mid-task, outper-forms 2D only and 3D only techniques. Guided by this result, we present HybridSpace, a proof-of-concept modeling environment that combines the benefits of 2D and 3D input and output modalities within a single coherent interface.","cites":"6","conferencePercentile":"84.7826087"},{"venue":"3DUI","id":"45d65ce4bb9b705af284f1a3a6f4450c1d23d3c7","venue_1":"3DUI","year":"2011","title":"An interactive augmented reality coloring book","authors":"Adrian J. Clark, Andreas Dünser","author_ids":"1749024, 1773698","abstract":"Creating entertaining and educational books not only requires providing visually stimulating content but also means for students to interact, create, and express themselves.","cites":"11","conferencePercentile":"86.95652174"},{"venue":"3DUI","id":"db56f6d60ccf031109365eebf2af47375aea47ea","venue_1":"3DUI","year":"2012","title":"Integrating spatial sensing to an interactive mobile 3D map","authors":"Ville Lehtinen, Antti Nurminen, Antti Oulasvirta","author_ids":"2124672, 1914398, 2663734","abstract":"ABSTR AC T We present an interaction technique that integrates spatial sensing to an interactive 3D city model in order to support efficient local-ization of objects (esp. buildings) known or remembered in the real world. The technique offers a unified 3D interaction scheme for both visible and remote objects. In the egocentric view, sensor data from the mobile device (accelometer, gyroscope, GPS) is utilized to couple the viewport to user's movement similarly as in mobile AR. The technique offers easy shifting from the viewport-coupled mode to a top-down view where movement is POI-based. A field experiment compared it to an exocentric technique resembling the traditional pan-and-zoom in 2D mobile maps. The two techniques showed differential benefits for target acquisition performance.","cites":"5","conferencePercentile":"74.19354839"},{"venue":"3DUI","id":"4e5e141d3f7cfc510717111b8f10850ff54dc4d2","venue_1":"3DUI","year":"2006","title":"Spatial Analysis Tools for Virtual Reality-based Surgical Planning","authors":"Bernhard Reitinger, Dieter Schmalstieg, Alexander Bornik, Reinhard Beichel","author_ids":"1696492, 1742819, 1742270, 1749679","abstract":"This paper presents a set of Virtual Reality-based interaction techniques for spatial analysis of medical datasets. Computer-aided medical planning tools often require precise and intuitive interaction for the quantitative inspection and analysis of anatomical and pathological structures. We claim that measurement tasks can be carried out more efficiently using Virtual Reality-based interaction tools rather than using common 2D input devices used for medical workstation. Due to the true direct manipulation of three-dimensional virtual objects, measurement tools can be used easily in 3D. An evaluation performed with a group of 20 subjects provides evidence to back up our claims.","cites":"8","conferencePercentile":"41.17647059"},{"venue":"3DUI","id":"033d037f71c5c186345399051f5b7212eafac49a","venue_1":"3DUI","year":"2007","title":"Cascading Hand and Eye Movement for Augmented Reality Videoconferencing","authors":"István Barakonyi, Helmut Prendinger, Dieter Schmalstieg, Mitsuru Ishizuka","author_ids":"2133082, 2356111, 1742819, 1687719","abstract":"We have implemented an augmented reality videoconferencing system that inserts virtual graphics overlays into the live video stream of remote conference participants. The virtual objects are manipulated using a novel interaction technique cascading bimanual tangible interaction and eye tracking. User studies prove that our user interface enriches remote collaboration by offering hitherto unexplored ways for collaborative object manipulation such as gaze controlled raypicking of remote physical and virtual objects. 1 INTRODUCTION While computer-supported collaborative work (CSCW) is one of the evident application domains that Milgram et al.'s definition of Augmented Reality (AR) [23] suggests, \" classic \" AR applications require that users are co-located, sharing the same physical space. We have created an augmented reality-based videoconferencing tool that allows users to discuss and manipulate real and virtual objects over great distances while preserving non-verbal communication and part of the conference parties' physical environment. We have experimented with user interface techniques that make communication and interaction smoother while discussing real and virtual objects with a remote videoconference party. Physical objects are pose-tracked with handheld fiducial markers and virtual objects are assigned to tracked physical placeholders. In our application scenario videoconference parties use both hands to carry out object manipulation and interaction tasks such as translation, rotation or selection with fiducial markers. While two-handed tangible interaction may rely solely on complex 2D and 3D gestures aided by traditional input devices such as mouse and keyboard, we have found that exploiting the human eye as a natural input device during bimanual object manipulation yields faster, richer and more intuitive communication between partners in remote collaboration tasks. To support this statement, we have implemented and evaluated an interaction technique cascading bimanual tangible interaction and eye tracking. Figure 1 shows a schema of our AR videoconferencing system enhanced by eye tracking. This paper first discusses how tangible augmented reality and non-intrusive eye tracking enhance remote collaboration while comparing our system with related work, then presents our application scenario and interaction technique with implementation details. We conclude our paper with the results of our user study. 2 REMOTE COLLABORATION IN AR Users of AR applications can see the real world, which provides a reference frame for their actions. They can see themselves and their collaborators, enabling smooth communication with non-verbal cues during collaborative work. Moreover, a virtual space with synthetic objects is aligned with and superimposed onto the real world and shared among the users. Thus …","cites":"2","conferencePercentile":"30"},{"venue":"3DUI","id":"14d86e7c9911dae9221d5282587a3c25d9cda211","venue_1":"3DUI","year":"2006","title":"Using the Non-Dominant Hand for Selection in 3D","authors":"Joan De Boeck, Tom De Weyer, Chris Raymaekers, Karin Coninx","author_ids":"1771327, 8476137, 1736922, 1695519","abstract":"Although 3D virtual environments are designed to provide the user with an intuitive interface to view or manipulate highly complex data, current solutions are still not ideal. In order to make the interaction as natural as possible, metaphors are used to allow the users to apply their everyday knowledge in the generated environment. In literature, a lot of experiments can be found, describing new or improved metaphors. In our former work, we presented the 'Object In Hand' metaphor [4], which addresses some problems regarding the access of objects and menus in a 3D world. Although the metaphor turned out to be very promising, the solution shifted the problem towards a selection problem. From the insights of our previous work, we believe the non-dominant hand can play a role in solving this problem. In this paper we formally compare three well-known selection metaphors and we will check their suitability to be carried out with the non-dominant hand in order to seamlessly integrate the most suitable selection metaphor within the 'Object In Hand' metaphor.","cites":"5","conferencePercentile":"26.47058824"},{"venue":"3DUI","id":"bd08d2f1e702a0ee17ae5bc2697f74a6f32c33d3","venue_1":"3DUI","year":"2013","title":"Poster: Creating a user-specific perspective view for mobile mixed reality systems on smartphones","authors":"Yuki Matsuda, Fumihisa Shibata, Asako Kimura, Hideyuki Tamura","author_ids":"2417493, 1723268, 1726979, 1730627","abstract":"We propose a method for creating a user-specific perspective view for mobile mixed reality (MR) systems that provides three-dimensional perception based on pseudo motion parallax. In general, most common mainstream interface for a hand-held AR/MR system is video-see-through style with a device-perspective view. On the other hand, our method provides a user-perspective view, which displays MR images according to the user's viewpoint and the position of the display. Namely, our method trims a scene behind the display and superimposes CG images in consideration of the user's point of view. This enables the user to perceive a stereoscopic effect with a non-3D display based on pseudo motion parallax. We implemented a prototype and tested our proposed method on smartphones.","cites":"1","conferencePercentile":"36"},{"venue":"3DUI","id":"cf860d162b64207ed416916a854c5b22b15011f5","venue_1":"3DUI","year":"2012","title":"HeatMeUp: A 3DUI serious game to explore collaborative wayfinding","authors":"Sofie Notelaers, Tom De Weyer, Patrik Goorts, Steven Maesen, Lode Vanacken, Karin Coninx, Philippe Bekaert","author_ids":"3122744, 8476137, 2604477, 2586390, 2823309, 1695519, 7583073","abstract":"Wayfinding inside a virtual environment is a cognitive process during navigation. Normally the user inside the virtual environment has to rely on himself and different cues such as waypoints to improve his knowledge with regard to his surroundings. In this paper we will present our solution for the 3DUI contest: a 3DUI serious game to explore collaborative alternatives, in which a partner is responsible for providing wayfinding cues. The game is set in a multi-storey building where several fires and gas leaks occur, a fire-fighter has to overcome several challenges guided by a fire chief.","cites":"4","conferencePercentile":"67.74193548"},{"venue":"3DUI","id":"ceb4d516c73ce77fc7af6b8959be5afa02c4a1b5","venue_1":"3DUI","year":"2013","title":"Poster: Real-time markerless kinect based finger tracking and hand gesture recognition for HCI","authors":"Arun Kulshreshth, Christopher Zorn, Joseph J. LaViola","author_ids":"2796615, 3222807, 1726039","abstract":"Hand gestures are intuitive ways to interact with a variety of user interfaces. We developed a real-time finger tracking technique using the Microsoft Kinect as an input device and compared its results with an existing technique that uses the K-curvature algorithm. Our technique calculates feature vectors based on Fourier descriptors of equidistant points chosen on the silhouette of the detected hand and uses template matching to find the best match. Our preliminary results show that our technique performed as well as an existing k-curvature algorithm based finger detection technique.","cites":"4","conferencePercentile":"80"},{"venue":"3DUI","id":"6e3508eaf5d8a82c8a0509a0314f829a085f6c6c","venue_1":"3DUI","year":"2011","title":"Joyman: A human-scale joystick for navigating in virtual worlds","authors":"Maud Marchal, Julien Pettré, Anatole Lécuyer","author_ids":"1722424, 2235773, 1693899","abstract":"In this paper, we propose a novel interface called Joyman, designed for immersive locomotion in virtual environments. Whereas many previous interfaces preserve or stimulate the users proprioception, the Joyman aims at preserving equilibrioception in order to improve the feeling of immersion during virtual locomotion tasks. The proposed interface is based on the metaphor of a human-scale joystick. The device has a simple mechanical design that allows a user to indicate his virtual navigation intentions by leaning accordingly. We also propose a control law inspired by the biomechanics of the human locomotion to transform the measured leaning angle into a walking direction and speed-i.e., a virtual velocity vector. A preliminary evaluation was conducted in order to evaluate the advantages and drawbacks of the proposed interface and to better draw the future expectations of such a device.","cites":"15","conferencePercentile":"91.30434783"},{"venue":"3DUI","id":"14dd4eb6dcdff582258bdecaff5c03817cbeefa3","venue_1":"3DUI","year":"2012","title":"Comparing isometric and elastic surfboard interfaces for leaning-based travel in 3D virtual environments","authors":"Jia Wang, Robert W. Lindeman","author_ids":"4263537, 1719686","abstract":"Inspired by the Silver Surfer comics we developed a leaning-based surfboard interface which allows the user to fly in 3D virtual environments by shifting his/her center of mass on the board. The interface works in either an elastic tilt mode or an isometric balance mode offering different equilibrioceptive feedback to the user. Interested in how different levels of equilibrioceptive feedback influence the usability of this interface, a formal user study was conducted comparing the two modes in both separated and combined pitch and yaw travel tasks. Six of 30 subjects dropped out of the experiment because of cyber-sickness and were interviewed by the experimenter. Statistical analysis of data from the remaining subjects showed that although objectively there was no significant difference between the two modes regarding training effects and task performance, subjectively most subjects preferred the elastic tilt mode because of its intuitiveness, realism, fun, and sense of presence despite the greater fatigue and after effects (e.g., loss of balance) of using it. Based on the results we suggest a general preference of elastic devices and several design guidelines to future 3D VE travel interface designers. 1 INTRODUCTION In virtual reality (VR), travel refers to the motor process of changing the position and orientation of one's virtual viewpoint to navigate from point A to B in an immersive virtual environment (VE). Designing travel interfaces has been a challenging problem because of the requirement of realistically, efficiently, and precisely mapping the user's limited locomotion in a finite real-world space to that of a potentially infinite virtual space. In addition, the usability of a travel interface impacts not only the efficiency of performing travel tasks, but also physiological and psychological human factors such as cyber-sickness [16], spatial orientation [17], and virtual-world cognition [24]. Recent locomotion research has focused on realizing endless real walking in a limited lab space either by building sophisticated mechanical systems that repeatedly place floor tiles on predicted positions and heights of the user's steps [6] [7] [8] or by gradually rotating the virtual world to redirect the user to walk in circles in the real world. Because of its success in preserving proprioception [11], comparative experiments have shown that when used as a travel interface, real walking has significant advantages in travel-task performance, VE immersion and cognition compared to joysticks [24]. Yet, this is not a consensus and other researchers believe that a well-designed joystick travel interface can …","cites":"3","conferencePercentile":"48.38709677"},{"venue":"3DUI","id":"2e198fde03e62efada27b07258e1e3ff999a9dce","venue_1":"3DUI","year":"2013","title":"ForceExtension: Extending isotonic position-controlled multi-touch gestures with rate-controlled force sensing for 3D manipulation","authors":"Jia Wang, Robert W. Lindeman","author_ids":"4263537, 1719686","abstract":"Recent advances in multi-touch technology have enabled pressure sensing of each touch point on a multi-touch touchpad in addition to position tracking. In this paper we propose two novel approaches for utilizing this extra dimension of input to extend the effect range of position controlled multi-touch gestures. Both ForceExtension approaches are only activated when the averaged force of all active fingers reaches a threshold. The first approach, context-force extension, tracks the most recent position-control movement as the context and combines it with the force input as an isometric rate-controlled extension. The second approach, shear-force extension, scales the micro displacement of the active fingers with the force input to simulate shear-force sensing as a viscoelastic rate-controlled extension. We collected feedback from several users who were asked to perform a 3D search task using variations of these interfaces. A single force sensing multi-touch touchpad was used to control the first-person camera during the search, and the multi-touch gestures to pan, rotate, and zoom the 3D camera were augmented through ForceExtension. Users preferred a medium gain position control combined with the context-force extension.","cites":"0","conferencePercentile":"14"},{"venue":"3DUI","id":"31a741a5764d59cb450cc3233f7185c0ea174416","venue_1":"3DUI","year":"2009","title":"Selection performance based on classes of bimanual actions","authors":"Amy Banic, Zachary Wartell, Paula Goolkasian, Evan A. Suma, Larry F. Hodges","author_ids":"2969935, 2656827, 2194713, 2817315, 1710833","abstract":"We evaluated four selection techniques for volumetric data based on the four classes of bimanual action: symmetric-synchronous, asymmetric-synchronous, symmetric-asynchronous, and asymmetric-asynchronous. The purpose of this study was to determine the relative performance characteristics of each of these classes. In addition, we compared two types of data representations to determine whether these selection techniques were suitable for interaction in different environments. The techniques were evaluated in terms of accuracy, completion times, TLX overall workload, TLX physical demand, and TLX cognitive demand. Our results suggest that symmetric and synchronous selection strategies both contribute to faster task completion. Our results also indicate that no class of bimanual selection was a significant contributor to reducing or increasing physical demand, while asynchronous action significantly increased cognitive demand in asymmetric techniques and decreased ease of use in symmetric techniques. However, for users with greater computer usage experience, accuracy performance differences diminished between the classes of bimanual action. No significant differences were found between the two types of data representations. 1 INTRODUCTION Interacting in three dimensions (3D) can be difficult due to the added third degree of freedom. It has been shown that bimanual interaction techniques can improve interaction in 3D over one-handed interaction techniques [4][8][17][18][20][22]. According to Guiard's framework of Bimanual manipulation, there exist different classes of bimanual actions [10]. The Bimanual symmetric classification involves each hand performing identical actions either synchronously (at the same time) or asynchronously (at different times). The Bimanual asymmetric classification consists of both hands performing different, but coordinated, actions to accomplish the same task [13]. Asymmetric actions can be performed synchronously or asynchronously as well. Therefore, four distinct classes of bimanual actions exist: • Symmetric-Synchronous • Symmetric-Asynchronous • Asymmetric-Synchronous • Asymmetric-Asynchronous It is important to understand the advantages, disadvantages and relative performance characteristics of each of these classes in order to provide specific guidelines to designers and developers as to which class of bimanual interaction is appropriate to their design goals. Although there has been previous work on bimanual interaction techniques [2], there is still a need to determine relative performance characteristics of these four classes. Research so far has also been limited to interaction with polygonal objects. Due to the differences in the properties of different data representations, these results may not generalize to other types of data, such as volumetric data used in many visualization applications. This study focuses on evaluating four selection techniques for volumetric data based on …","cites":"6","conferencePercentile":"81.25"},{"venue":"3DUI","id":"33e1e6f5250d79da021a8b2ab59d0c4f098d46e2","venue_1":"3DUI","year":"2007","title":"Comparison of Travel Techniques in a Complex, Multi-Level 3D Environment","authors":"Evan A. Suma, Sabarish V. Babu, Larry F. Hodges","author_ids":"2817315, 1778813, 1710833","abstract":"This paper reports on a study that compares three different methods of travel in a complex, multi-level virtual environment using a between-subjects design. A real walking travel technique was compared to two common virtual travel techniques. Participants explored a two-story 3D maze at their own pace and completed four post-tests requiring them to remember different aspects of the environment. Testing tasks included recall of objects from the environment, recognition of objects present and not present, sketching of maps, and placing objects on a map. We also analyzed task completion time and collision data captured during the experiment session. Participants that utilized the real walking technique were able to place more objects correctly on a map, completed the maze faster, and experienced fewer collisions with the environment. While none of the conditions outperformed each other on any other tests, our results indicate that for tasks involving the naive exploration of a complex, multi-level 3D environment, the real walking technique supports a more efficient exploration than common virtual travel techniques. While there was a consistent trend of better performance on our measures for the real walking technique, it is not clear from our data that the benefits of real walking in these types of environments always justify the cost and space trade-offs of maintaining a wide-area tracking system. 1 INTRODUCTION 1.1 Motivation Navigation in a virtual environment is commonly divided into two components: motor and cognitive [1]. The motor component, known as travel, refers to the movement of the viewpoint from one location to another. The cognitive component, known as wayfinding, is the process of defining a path through an environment. In this study, we investigate the travel component of navigation, and we explore the effects of travel technique on users' ability to learn about a complex, multi-level virtual environment. Immersive virtual environments (IVEs) attempt to give the user a sense of being present within a virtual space. Several IVEs, such as architecture walkthroughs and games, use a first person perspective [2]. In these systems, control of viewpoint is typically accomplished by either head motion and/or using a virtual travel technique (such as using a joystick) in order to simulate walking through the IVE. Some virtual environment systems use tracking equipment, typically attached to a user's head, to allow the user to control the viewpoint and improve level of immersion [3]. Welch and Foxlin provide a comprehensive overview of current tracking systems …","cites":"14","conferencePercentile":"70"},{"venue":"3DUI","id":"3eea169f62d4caab6488d1a7f5f093f171381d27","venue_1":"3DUI","year":"2007","title":"Two Handed Selection Techniques for Volumetric Data","authors":"Amy Banic, Catherine A. Zanbaka, Zachary Wartell, Paula Goolkasian, Larry F. Hodges","author_ids":"2969935, 1826425, 2656827, 2194713, 1710833","abstract":"We developed three distinct two-handed selection techniques for volumetric data visualizations that use splat-based rendering. Two techniques are bimanual asymmetric, where each hand has a different task. One technique is bimanual symmetric, where each hand has the same task. These techniques were then evaluated based on accuracy, completion times, TLX workload assessment, overall comfort and fatigue, ease of use, and ease of learning. Our results suggest that the bimanual asymmetric selection techniques are best used when performing gross selection for potentially long periods of time and for cognitively demanding tasks. However when optimum accuracy is needed, the bimanual symmetric technique was best for selection. 1 INTRODUCTION The complexity of visualizing 3D volumetric data can cause difficulty in interaction due to the added third degree of freedom, the type of rendering, or the overall functionality. Good 3D UI design is therefore critical for the success of 3D visualization applications. Visualization is a rapidly growing field that uses graphics to represent data in a more understandable way than in its raw form. Most of the applications in this field are domain dependent, thereby making it very difficult to develop standard visualization and interaction techniques. Not only does the developer need to create the interaction techniques specific to the domain, but the user must learn how to use the interaction techniques in addition to other aspects of the application. Most 3D visualizations have some fundamental interactions, such as selection or manipulation, which can be dependent on the type of rendering. The primary interactions can be developed exclusive of the domain-dependent interactions. This can reduce the cognitive load on the domain experts by facilitating users to go from one 3D visualization application to another without needing additional training on basic interaction tasks. 3D visualization developers can then focus more effort on developing the visualization rather than the interaction. In this paper, we focused on developing and quantifying selection techniques specifically for visualizations that use splat-based rendering [18][19][23]. Selecting a specific area from a splat-based volumetric rendering of data in this type of 3D visualization is difficult because the rendered objects are not precisely defined as polygonal objects. 3D visualizations using splat-based rendering represent data as clouds of various colors, sizes, shapes, opacities, levels of occlusion, and sparseness. These characteristics make it difficult to select areas for analysis using traditional selection techniques such as those incorporating point-based [24], ray-based [27][30], virtual hand [2][26][27], or aperture-based …","cites":"18","conferencePercentile":"80"},{"venue":"3DUI","id":"d6d74d54a105856064bd734f3990c82393c0fa2c","venue_1":"3DUI","year":"2014","title":"A study of street-level navigation techniques in 3D digital cities on mobile touch devices","authors":"Jacek Jankowski, Thomas Hulin, Martin Hachet","author_ids":"3312524, 1748507, 2281511","abstract":"To characterize currently most common interaction techniques for street-level navigation in 3D digital cities for mobile touch devices in terms of their efficiency and usability, we conducted a user study, where we compared target selection (Go-To), rate control (Joystick), position control, and stroke-based control navigation metaphors. The results suggest users performed best with the Go-To interaction technique. The subjective comments showed a preference of novices towards Go-To and expert users towards Joy-stick technique.","cites":"2","conferencePercentile":"52.17391304"},{"venue":"3DUI","id":"790c6594c347768dde72b5e71e3bae1286c9de2f","venue_1":"3DUI","year":"2011","title":"A multimode immersive conceptual design system for architectural modeling and lighting","authors":"Marcio Cabral, Peter Vangorp, Gaurav Chaurasia, Emmanuelle Chapoulie, Martin Hachet, George Drettakis","author_ids":"2582417, 2533751, 2585067, 2650457, 2281511, 1721779","abstract":"We present a new immersive system which allows initial conceptual design of simple architectural models, including lighting. Our system allows the manipulation of simple elements such as windows, doors and rooms while the overall model is automatically adjusted to the manipulation. The system runs on a four-sided stereoscopic, head-tracked immersive display. We also provide simple lighting design capabilities, with an abstract representation of sunlight and its effects when shining through a window. Our system provides three different modes of interaction, a miniature-model table mode, a fullscale immersive mode and a combination of table and immer-sive which we call mixed mode. We performed an initial pilot user test to evaluate the relative merits of each mode for a set of basic tasks such as resizing and moving windows or walls, and a basic light-matching task. The study indicates that users appreciated the immersive nature of the system, and found interaction to be natural and pleasant. In addition, the results indicate that the mean performance times seem quite similar in the different modes, opening up the possibility for their combined usage for effective immersive modeling systems for novice users.","cites":"1","conferencePercentile":"39.13043478"},{"venue":"3DUI","id":"879901f0e951f8d35f50dbab52856548f3eab5d8","venue_1":"3DUI","year":"2010","title":"Piivert: Percussion-based interaction for immersive virtual environments","authors":"Florent Berthaut, Martin Hachet, Myriam Desainte-Catherine","author_ids":"2174703, 2281511, 2635723","abstract":"3D graphical interaction offers a large amount of possibilities for musical applications. However it also carries several limitations that prevent it from being used as an efficient musical instrument. For example, input devices for 3D interaction or new gaming devices are usually based on 3 or 6 degrees-of-freedom tracking combined with pushbuttons or joysticks. While buttons and joysticks do not provide good resolution for musical gestures, graphical interaction using tracking may bring enough expressivity but is weakened by accuracy and haptic feedback problems. Moreover, interaction based solely on tracking limit the possibilities brought by graphical interfaces in terms of musical gestures. We propose a new approach that separates the input modalities according to traditional musical gestures. This allows to combine the possibilities of graphical interaction as selection and modulation gestures with the accuracy and the expressivity of musical interaction as excitation gestures. We implement this approach with a new input device, Piivert, which combines 6DOF tracking and pressure detection. We describe associated interaction techniques and show how this new device can be valuable for immersive musical applications .","cites":"4","conferencePercentile":"42.10526316"},{"venue":"3DUI","id":"89a6db6a267696adcf36667719bb09a2ebc1d709","venue_1":"3DUI","year":"2006","title":"Overcoming World in Miniature Limitations by a Scaled and Scrolling WIM","authors":"Chadwick A. Wingrave, Yonca Haciahmetoglu, Doug A. Bowman","author_ids":"3293975, 1782991, 1729869","abstract":"The World In Miniature (WIM) technique has effectively allowed users to interact and travel efficiently in Virtual Environments. However, WIM fails to work in worlds with tasks at various levels of scale. Such an example is using the WIM to arrange furniture and then leaving the room to travel the city using the WIM for navigation and wayfinding. To address this problem, scaling and scrolling were added to the WIM to create the Scaled Scrolling World In Miniature (SSWIM). The interface and testbed were iteratively created under expert evaluation and multiple formative user evaluations led to the final design. The WIM and SSWIM were then compared inside three differently sized cities by users who located a sphere and traveled into it to read the label at the sphere's center. Users were administered two standard psychology tests to account for spatial orientation (Cube Comparison Test) and spatial scanning (Maze Tracing Test) factors. The results show that the SSWIM's added functionality, and hence complexity, caused no significant hit in user performance and additionally that users were able to use SSWIM effectively after a short instructional period. To better understand the effect of experience, a follow-up experiment was performed showing performance plateaued after ten to fifteen minutes of use. 1. INTRODUCTION The standard World in Miniature (WIM) technique [1] has been successfully used to manipulate objects and travel in Virtual and Augmented [2] Environments. This benefit is accorded by a separate exocentric view of the world placed in the palm of their hand. The exocentric view allows user to understand the arrangements of objects and themselves by rotating and moving the WIM to view from different angles and at different distances. When the world becomes very large or very small, the fixed scale of the WIM makes it hard to interact with objects and to travel. The current work has been the addition of scaling and scrolling to the WIM to overcome these limitations with a new technique called the Scaled Scrolling WIM (SSWIM) (Figure 1). This technique was created over a series of formative expert and user evaluations to reach the final design. Since the WIM can only function in single level of detail worlds, a summative comparison evaluation of the SSWIM against the WIM was conducted in a world where the WIM could function to determine the amount of overhead the additional SSWIM functionality added. A follow-up study evaluated the effect …","cites":"22","conferencePercentile":"88.23529412"},{"venue":"3DUI","id":"8f7962f987c899f025aeb8f12444005484b9659c","venue_1":"3DUI","year":"2008","title":"Tech-note: rapMenu: Remote Menu Selection Using Freehand Gestural Input","authors":"Tao Ni, Ryan P. McMahan, Doug A. Bowman","author_ids":"2975479, 1728765, 1729869","abstract":"Traditional menu designs are not well-suited for command selection on distant displays. We describe a new interaction technique called the \" roll-and-pinch menu \" (rapMenu), and investigate the design space of remote menu selection using freehand gestural input. The rapMenu maps intuitive hand postures and gestures to a radial menu layout at two levels of granularity: the user rolls her hand to alter the wrist orientation, which makes a group of menu items selectable; then she touches her thumb to a particular finger (forming a pinch gesture) to select a specific item in that group. We present the visual design and behavior of the rapMenu, consider related design issues, examine design variations, and discuss the strengths and limitations of our approach. 1 INTRODUCTION Freehand gestural input is promising for interaction with remote display surfaces, since it eliminates the need for acquiring physical devices and affords fluid transitions between distant and up-close touch-screen interaction [2][14][15]. Previous research offers distant freehand techniques for pointing and clicking [15] and object control in specific applications (e.g., [2]), but few researchers have investigated remote freehand menu selection techniques. One approach is to adapt existing menu techniques (e.g., marking menus [9] or the FlowMenu [7]), but their applicability to remote freehand menu selection is questionable. For example, selection with a marking menu by drawing a mark in mid-air may be less accurate, more ambiguous, more error-prone, and may cause more fatigue than drawing a mark on a stable display surface with a pen-like device. This is largely due to the lack of haptic constraints [16]. In this paper, we contribute a new technique called the \" roll-and-pinch menu \" , or rapMenu, which is designed specifically for freehand menu selection on distant displays (Figure 1). The rapMenu allows users to perform menu selection with easy-to-understand postures and gestures: the user alters the wrist orientation (e.g., palm up, down, right or left) to specify a group of menu items, and then pinches the thumb to one of the other fingers to select an item in that group. In the following sections, we review previous work and provide a detailed description of the rapMenu technique. We discuss the strengths and limitations of our approach, and consider issues such as scalability, fatigue and user comfort. This work provides several important contributions. We describe a novel way to use freehand gestural input for remote menu selection. We identify related design …","cites":"15","conferencePercentile":"83.33333333"},{"venue":"3DUI","id":"b7c6a359dadd35c4a219e236cfe99e5a862b0ef7","venue_1":"3DUI","year":"2015","title":"Towards interactive authoring tools for composing spatialization","authors":"Jérémie Garcia, Jean Bresson, Thibaut Carpentier","author_ids":"1906365, 2608740, 2037755","abstract":"We present interactive tools designed to help music composers controlling sound spatialization processes in a computer-aided composition environment. We conducted interviews with composers to understand their needs and inform the design of new compositional interfaces. These interfaces support quick input, visualization and edition of three-dimensional trajectories, as well as the control of the temporal dimension in spatial scene descriptions.","cites":"2","conferencePercentile":"77.77777778"},{"venue":"3DUI","id":"cd527ca03b747815f89fd3d7a910bb91185fceb8","venue_1":"3DUI","year":"2013","title":"The god-finger method for improving 3D interaction with virtual objects through simulation of contact area","authors":"Anthony Talvas, Maud Marchal, Anatole Lécuyer","author_ids":"3329485, 1722424, 1693899","abstract":"In physically-based virtual environments, interaction with objects generally happens through contact points that barely represent the area of contact between the user's hand and the virtual object. This representation of contacts contrasts with real life situations where our finger pads have the ability to deform slightly to match the shape of a touched object. In this paper, we propose a method called god-finger to simulate a contact area from a single contact point determined by collision detection, and usable in a rigid body physics engine. The method uses the geometry of the object and the force applied to it to determine additional contact points that will emulate the presence of a contact area between the user's proxy and the virtual object. It could improve the manipulation of objects by constraining the rotation of touched objects in a similar manner to actual finger pads. An implementation in a physics engine shows that the method could make for more realistic behaviour when manipulating objects while keeping high simulation rates.","cites":"4","conferencePercentile":"80"},{"venue":"3DUI","id":"6d46620c6f740f9fc6e3a95508edf6a06c0031f5","venue_1":"3DUI","year":"2008","title":"Poster: Image-Based 3D Display with Motion Parallax using Face Tracking","authors":"Tsuyoshi Suenaga, Yasuyuki Tamai, Yuichi Kurita, Yoshio Matsumoto, Tsukasa Ogasawara","author_ids":"2231548, 8404639, 2971858, 1778544, 1785194","abstract":"We propose an image-based 3D display with motion parallax using face tracking. Multi-view images of target objects are recorded in advance by utilizing a camera mounted on a 6 DOF manipula-tor. The 3D viewpoint of the user is measured by using a real-time non-contact face measurement system. One of the multi-view images is projected according as the camera position corresponding to the viewpoint of the user. The user can obtain 3D information of the object through a standard monitor by moving his/her head. A prototype system is developed and the consistency of the proposed method is confirmed by comparing generated images with captured images at the same viewpoints. 1 IMAGE-BASED 3D DISPLAY WITH MOTION PARALLAX In tele-communication and tele-operation researches, it is expected that 3D displays induce higher realistic sensation. Some of glass-less stereoscopic displays based on lenticular lens or parallax barriers are commercially available, however they are still expensive to be a popular device. Such displays take advantage of binocular parallax, which is one of the depth cues. The depth cues are composed of various elements such as \" binocular parallax, \" \" motion parallax, \" \" convergence, \" and \" accommodation. \" Motion parallax is the difference of the images due to head movements while binocular parallax is the difference of the images due to the perspective difference of two eyes. As Cutting et al. reported on perceiving the layout of depth cues[1], motion parallax has an influence almost equal to the binocular parallax. Therefore, 3D displays based on motion parallax have been investigated as simple 3D display systems (e.g. [5]). We have developed a non-contact 3D display based on motion parallax[6]. This system does not require any special display devices expect a standard monitor. However, the system was able to present only CG objects. In this paper, we improve the current CG based system into real image based system. As methods of 3D presentation using real images, a 3D model-based approach[2] and an image-based approach without 3D models[3] have been proposed. The model-based approach generates images from reconstructed 3D models of target objects. However , the model-based approach needs high computational cost and","cites":"1","conferencePercentile":"30.55555556"},{"venue":"3DUI","id":"65a794992924e5d6fd80b554352a6d9fa92eb651","venue_1":"3DUI","year":"2014","title":"Interaction-free calibration for optical see-through head-mounted displays based on 3D Eye localization","authors":"Yuta Itoh, Gudrun Klinker","author_ids":"2792579, 1715693","abstract":"It is a common problem of AR applications that optical see-through head-mounted displays (OST-HMD) move on users' heads or are even temporarily taken off, thus requiring frequent (re)calibrations. If such calibrations involve user interactions, they are time consuming and distract users from their applications. Furthermore, they inject user-dependent errors into the system setup and reduce users' acceptance of OST-HMDs. To overcome these problems, we present a method that utilizes dynamic 3D eye position measurements from an eye tracker in combination with pre-computed, static display calibration parameters. Our experiments provide a comparison of our calibration with SPAAM (Single Point Active Alignment Method) for several head-display conditions: in the first condition, repeated calibrations are conducted while keeping the display position on the user's head fixed. In the second condition, users take the HMD off and put it back on in between calibrations. The result shows that our new calibration with eye tracking is more stable than repeated SPAAM calibrations. We close with a discussion on potential error sources which should be removed to achieve higher calibration quality.","cites":"14","conferencePercentile":"100"},{"venue":"3DUI","id":"39a1469f62b38ee46b75606cf321351a249cd42c","venue_1":"3DUI","year":"2014","title":"The Virtual Mitten: A novel interaction paradigm for visuo-haptic manipulation of objects using grip force","authors":"Merwan Achibet, Maud Marchal, Ferran Argelaguet, Anatole Lécuyer","author_ids":"2107280, 1722424, 1854224, 1693899","abstract":"Figure 1: Visuo-haptic manipulation as enabled by our novel approach called the \" Virtual Mitten \". Each hand holds an elastic device to control a corresponding virtual mitten (in gray) and to grasp virtual objects in a bimanual scenario. The grip force applied by the user is measured to generate pseudo-haptic feedback. ABSTRACT In this paper, we propose a novel visuo-haptic interaction paradigm called the \" Virtual Mitten \" for simulating the 3D manipulation of objects. Our approach introduces an elastic handheld device that provides a passive haptic feedback through the fingers and a mitten interaction metaphor that enables to grasp and manipulate objects. The grasping performed by the mitten is directly correlated with the grip force applied on the elastic device and a supplementary pseudo-haptic feedback modulates the visual feedback of the interaction in order to simulate different haptic perceptions. The Virtual Mitten allows natural interaction and grants users with an extended freedom of movement compared with rigid devices with limited workspaces. Our approach has been evaluated within two experiments focusing both on subjective appreciation and perception. Our results show that participants were able to well perceive different levels of effort during basic manipulation tasks thanks to our pseudo-haptic approach. They could also rapidly appreciate how to achieve different actions with the Virtual Mitten such as opening a drawer or pulling a lever. Taken together, our results suggest that our novel interaction paradigm could be used in a wide range of applications involving one or two-hand haptic manipulation such as virtual prototyping, virtual training or video games. 1 INTRODUCTION Object manipulation is a fundamental task in virtual reality applications [7]. Several methods have been proposed to grab and manipulate virtual objects by moving hands in 3D space. These techniques may rely on optical tracking [26] but the anatomical complexity of the human hand makes the accurate tracking of manual gestures a difficult task and haptic feedback, which is an important cue in object manipulation, is missing. The simulation of manipulation tasks can then be enhanced with haptic interfaces, which aim at generating sensations related to the sense of touch and effort. Force-feedback arms enable to manipulate virtual objects and generate interaction forces towards the user but generally require specific interaction metaphors that do not always reproduce the natural dynamics of grasping. Active hand-mounted devices enable to accurately track the hand and to feel virtual objects with the fingers [6] …","cites":"8","conferencePercentile":"91.30434783"},{"venue":"3DUI","id":"b60357f1feb88cb6bd769ff199cd26e87ffa8072","venue_1":"3DUI","year":"2014","title":"Feet movement in desktop 3D interaction","authors":"Adalberto Lafcadio Simeone, Eduardo Velloso, Jason Alexander, Hans-Werner Gellersen","author_ids":"2785514, 2520424, 2928764, 4919595","abstract":"In this paper we present an exploratory work on the use of foot movements to support fundamental 3D interaction tasks. Depth cameras such as the Microsoft Kinect are now able to track users' motion unobtrusively, making it possible to draw on the spatial context of gestures and movements to control 3D UIs. Whereas mul-titouch and mid-air hand gestures have been explored extensively for this purpose, little work has looked at how the same can be accomplished with the feet. We describe the interaction space of foot movements in a seated position and propose applications for such techniques in three-dimensional navigation, selection, manipulation and system control tasks in a 3D modelling context. We explore these applications in a user study and discuss the advantages and disadvantages of this modality for 3D UIs. 1 INTRODUCTION 3D interaction tasks are inherently multi-dimensional, requiring highly expressive input devices capable of providing at least three degrees of freedom [8]. Conventional input devices such as keyboards and mice are not designed for this purpose, so previous research has explored new interfaces that take into account the user's 3D spatial context to facilitate interaction [3]. In this context, our feet can provide an additional interaction space. When their movements are suitably tracked, the input can be combined with the input expressed by our hands to form a more expressive interaction intent. Technological advancements such as depth cameras and high precision touch-sensitive displays made mid-air and multitouch gestures a reality for a wide audience outside research labs. Whereas much work has explored the advantages of hand gestures for 3D interaction, feet gestures have been underexplored, especially in desktop settings. We aim to contribute in this domain through an exploratory investigation focused on the use of feet movements in the desktop setting. As a testbed environment, we built interaction techniques supporting 3D modelling tasks. Our work is motivated by the fact that even though gestural input can provide the necessary degrees of freedom for 3D interaction, they present challenges for stereoscopic displays, such as breaking the illusion of 3D [4]. This illusion is not hindered by foot interaction, as the feet are out of the user's field of view. In traditional interaction users often have to switch the function of the mouse between several modes of operation, to cope with the high number of degrees of freedom required. For example, in a 3D modelling application, the user may move …","cites":"9","conferencePercentile":"95.65217391"},{"venue":"3DUI","id":"c710881910cd118575f8a5f303417a5a46e40628","venue_1":"3DUI","year":"2013","title":"Poster: 3D referencing for remote task assistance in augmented reality","authors":"Ohan Oda, Mengu Sukan, Steven K. Feiner, Barbara Tversky","author_ids":"3147807, 2977638, 1809403, 1743610","abstract":"We present a 3D referencing technique tailored for remote maintenance tasks in augmented reality. The goal is to improve the accuracy and efficiency with which a remote expert can point out a real physical object at a local site to a technician at that site. In a typical referencing task, the remote expert instructs the local technician to navigate to a location from which a target object can be viewed, and then to attend to that object. The expert and technician both wear head-tracked, stereo, see-through, head-worn displays, and the expert's hands are tracked by a set of depth cameras. The remote expert first selects one of a set of prerecorded viewpoints of the local site, and a representation of that viewpoint is presented to the technician to help them navigate to the correct position and orientation. The expert then uses hand gestures to indicate the target.","cites":"4","conferencePercentile":"80"},{"venue":"3DUI","id":"72799d1ab1de84d5c1bcc4d1046d3ae0820b01bb","venue_1":"3DUI","year":"2015","title":"3D visualization to mitigate weather hazards in the flight deck: Findings from a user study","authors":"Catherine Letondal, Cédric Zimmerman, Jean-Luc Vinot, Stéphane Conversy","author_ids":"2346968, 2701278, 1807032, 1783132","abstract":"In this paper, we report on our exploration of 3D representations to support temporal navigation. We ground our discussion in a user study on the design of a tool that helps airliner pilots to manage dangerous and tall thunderstorm cells. The tool enables pilots to visualize thunderstorm cells, navigate into meteorological predictions in the near future and evaluate safe paths. The visualization is provided on the dual 2D horizontal/vertical view that is already used on the flight deck. In lieu of the standard 2D vertical view, the tool uses a 3D view that facilitates tracking of cells sliding along time slots and altitudes. The user navigates along two dimensions, heading and time slots, thanks to a rotary knob-button. The design is grounded in field studies with pilots. Prototypes have been iteratively evaluated during design walkthrough sessions with pilots. From the preliminary results of this study, we draw research questions related to the need of 3D in the cockpit navigation displays.","cites":"2","conferencePercentile":"77.77777778"},{"venue":"3DUI","id":"df12e5805579d01088a0580bbb96c044269c5707","venue_1":"3DUI","year":"2011","title":"DRIVE: Directly Reaching Into Virtual Environment with bare hand manipulation behind mobile display","authors":"Seung Wook Kim, Anton Treskunov, Stefan Marti","author_ids":"6557465, 2874689, 2807271","abstract":"We present DRIVE, an interaction method that allows a user to manipulate virtual content by reaching behind a mobile display device such as a cellphone, tablet PC, etc. Unlike prior work that uses front volume as well as front, side, and back surfaces, DRIVE utilizes the back volume of the device. Together with face tracking, out system creates the illusion that the user's hand is co-located with virtual volumetric content. 1 INTRODUCTION Touch-based input has become a de facto standard for many mobile applications, but it occludes the screen, introducing the \" fat finger \" problem [18]. Furthermore, surface based touch input is most suitable for two-dimensional content. Meanwhile, following rapid hardware advances in 3D rendering and an emergence of 3D content in games, virtual worlds, and other applications, there is a strong need for spatial interaction methods beyond traditional surface UI metaphors. In this poster, we propose a novel interaction method called DRIVE (a.k.a., Directly Reaching Into Virtual Environment), whose design addresses the occlusion and fat finger problems of touch input, as well as the conflicting focus problem of traditional gesture interaction, providing an improved user experience for 3D content on mobile devices. Figure 1 illustrates the concept of the DRIVE interaction method. 2 RELATED WORK To avoid content occlusion by finger touch interaction, researchers have proposed to use the side surfaces of a device [3], the back surface [1,20], and front volume [10]. Surface based methods are two-dimensional in interaction space as well as in the intended content. Front volume based in-air typing work [10] solves the occlusion problem by putting the interacting hands above the display. However, the user's hand in that case is not collocated with manipulated content. Visual hand tracking research generally uses flat markers [2,12] or multi-colored bands [9] or gloves [19], sometimes using maker-less tracking with explicit initialization [6], requiring outstretched hands [8], or measuring skin tone and stereo disparity [7]. Most of the mentioned works rely on markers [2,9,12,19], or on assumptions of finger position [8] or an initial configuration [6]. The work [7] reports that \" most of the users commented on the unstable fingertip tracking which sometimes affected their interaction. \" There is also fast growing field of time-of-flight cameras (TOF) [5] and its applications. As mentioned by [4], while \" range images of TOF cameras are independent of the texture and lighting conditions, they are somehow affected by the …","cites":"2","conferencePercentile":"52.17391304"},{"venue":"3DUI","id":"4fe4c48f42e5de0afa167f56acef25aba90b821f","venue_1":"3DUI","year":"2009","title":"Poster: Evaluation of menu techniques using a 3D game input device","authors":"Dustin B. Chertoff, Ross W. Byers, Joseph J. LaViola","author_ids":"2106784, 1953805, 1726039","abstract":"With the rise in popularity of 3D spatial interaction in console gaming, such as the Nintendo Wii, it is important to determine whether existing menuing technique findings still hold true when using a 3D pointing device such as the Wii Controller. Linear menus were compared with two other menu techniques: radial menus and rotary menus. Effectiveness was measured through task completion time and the number of task errors. A subjective measure was also taken to determine participant preferences. Participants performed faster and made fewer errors when using the radial menu technique. Radial menus were also preferred by participants. These results indicate that radial menus are an effective menu technique when used with a 3D pointing device. This is consistent with previous work regarding radial menus and indicates that the usage of radial menus in gaming applications should be investigated further. 1 INTRODUCTION Traditionally, in-game tasks are performed with a linear menu. The large number of items that can be placed into a linear menu has lead to the use of icons and keyboard shortcuts to facilitate faster item selection. However, these approaches still have limitations. According to [8], while icons are easier to recognize than text, a cognitive load is still imposed when the number of icons displayed at once becomes too high. The desire is for a menu technique that has limited impact on player task performance and allows for shortcuts to evolve based on muscle memory. Gesture based menus, such as radial menus (also known as pie [1] or marking [4] menus), may meet these requirements. The Nintendo Wii Controller contains motion-sensing elements, which we believe can be utilized to more quickly and accurately navigate a menu based on muscle memory. In order to determine if existing gesture menu findings can be applied to the Wii Controller, we conducted a comparison study between linear and gesture based menu techniques using the Wii Controller. 2 MENU IMPLEMENTATION Radial menus and a modified version of the rapMenu [6], called a rotary menu, were selected as the gesture menu techniques. A traditional linear menu was also included. In order to evaluate possible effects of sub-menus on performance, each technique had a layout with and a layout without submenus, for a total of 6 techniques. Under the linear menu technique, the menu was continuously visible. Menu items were laid out in a horizontal row at the bottom of the display, consistent with …","cites":"1","conferencePercentile":"22.91666667"},{"venue":"3DUI","id":"112a828918a89101a1bc9e1594791074af915474","venue_1":"3DUI","year":"2011","title":"Effects of redirection on spatial orientation in real and virtual environments","authors":"Evan A. Suma, David M. Krum, Samantha L. Finkelstein, Mark T. Bolas","author_ids":"2817315, 2962868, 1703122, 1767669","abstract":"We report a user study that investigated the effect of redirection in an immersive virtual environment on spatial orientation relative to both real world and virtual stimuli. Participants performed a series of spatial pointing tasks with real and virtual targets, during which they experienced three within-subjects conditions: rotation-based redirection, change blindness redirection, and no redirection. Our results indicate that when using the rotation technique, participants spatially updated both their virtual and real world orientations during redirection, resulting in pointing accuracy to the targets' recom-puted positions that was strikingly similar to the control condition. While our data also suggest that a similar spatial updating may have occurred when using a change blindness technique, the realignment of targets appeared to be more complicated than a simple rotation, and was thus difficult to measure quantitatively.","cites":"6","conferencePercentile":"76.08695652"},{"venue":"3DUI","id":"a31b61f423e8bde741b4064590fc5e69e91ce1fb","venue_1":"3DUI","year":"2010","title":"Effects of travel technique and gender on a divided attention task in a virtual environment","authors":"Evan A. Suma, Samantha L. Finkelstein, Seth Clark, Paula Goolkasian, Larry F. Hodges","author_ids":"2817315, 1703122, 2041857, 2194713, 1710833","abstract":"We report a user study which compared four virtual environment travel techniques using a divided attention task. Participants used either real walking, gaze-directed, pointing-directed, or torso-directed travel to follow a target through an environment while simultaneously responding to auditory stimuli. In addition to travel technique, we investigated gender as a between-subjects variable and task difficulty (simple or complex) and task type (single or divided) as within-subjects variables. Real walking allowed superior performance over the pointing-directed technique on measures of navigation task performance and recognition of stimuli presented during navigation. This indicates that participants using real walking may have had more spare cognitive capacity to process and encode stimuli than those using pointing-directed travel. We also found a gender-difficulty interaction where males performed worse and responded slower to the attention task when the spatial task was more difficult, but no differences were observed for females between difficulty levels. While these results may be pertinent for the design of virtual environments, the nature and goal of the virtual environment tasks must be carefully considered to determine whether similar effects on performance can be expected under different conditions.","cites":"10","conferencePercentile":"89.47368421"},{"venue":"3DUI","id":"b10a415f3d17f099a65a7d4561dee24968a82464","venue_1":"3DUI","year":"2013","title":"DIY World Builder: An immersive level-editing system","authors":"Jia Wang, Owen Leach, Robert W. Lindeman","author_ids":"4263537, 3170309, 1719686","abstract":"(a) (b) (c) (d) (e) Figure 1. (a) The physical interaction space, with motion-capture cameras, fans, and the passive floor surface, (b) terrain editing with the wand, (c) object creation and manipulation using the wand+tablet, (d) text input on the tablet, (e) lighting control on the tablet. ABSTRACT This paper presents an immersive system for building virtual worlds from within the worlds themselves. The system incorporates solutions for editing and texturing terrain, adding foliage, water, structures, and text, adding and controlling lights, including the time of day, and texturing objects in the scene. In a tracked head-mounted display environment, the user controls movement using a hybrid solution combining real walking and pointing-directed flying, and selects and manipulates objects using a hybrid wand and forearm-mounted interaction tablet interface. The user receives immersive cues such as wind from fans mounted around the physical space and audio from headphones. Feedback through a vibrotactile belt and a novel passive floor technique are used to indicate to the user when she is approaching the edge of the tracked space. Preliminary user testing showed positive user experience and suggestions for future improvements.","cites":"2","conferencePercentile":"56"},{"venue":"3DUI","id":"9834ace12c6998e04e8145881d1f9cfc52a78042","venue_1":"3DUI","year":"2007","title":"Exploring the Effects of Environment Density and Target Visibility on Object Selection in 3D Virtual Environments","authors":"Lode Vanacken, Tovi Grossman, Karin Coninx","author_ids":"2823309, 3313809, 1695519","abstract":"Object selection is a primary interaction technique which must be supported by any interactive three-dimensional virtual reality application. Although numerous techniques exist, few have been designed to support the selection of objects in dense target environments, or the selection of objects which are occluded from the user's viewpoint. There is, thus, a limited understanding on how these important factors will affect selection performance. In this paper, we present a set of design guidelines and strategies to aid the development of selection techniques which can compensate for environment density and target visibility. Based on these guidelines, we present two techniques, the depth ray and the 3D bubble cursor, both augmented to allow for the selection of fully occluded targets. In a formal experiment, we evaluate the relative performance of these techniques, varying both the environment density and target visibility. The results found that both of these techniques outperformed a baseline point cursor technique, with the depth ray performing best overall. 1 INTRODUCTION In recent years, three-dimensional (3D) display technologies, such as immersive Virtual Reality (VR) systems [5], or non-immersive fish-tank VR systems using LCD shutter stereo-glasses [27], have significantly improved in display quality. Experimental evaluations have also shown that these displays can improve the user's ability to perceive virtual 3D scenes [28], making them a potentially beneficial alternative for 3D applications. One of the primary techniques in any interactive application which must be supported is object selection. Within the realm of 3D virtual environments, selection has been repeatedly identified as one of the fundamental tasks [4, 18]. However, when selecting objects in a 3D environment, the standard 2D mouse metaphor breaks down, as the targets will have 3D coordinates, which the user must somehow specify. As such, it is important for VR researchers to consider new selection techniques, specifically designed for 3D environments. Indeed, research in VR environments has introduced numerous techniques for object selection. Most commonly seen are hand extension techniques [18], for which the 3D coordinates of the hand or handheld input device are mapped to the 3D coordinates of a virtual cursor, and ray casting techniques [15], for which a virtual ray is cast into the scene, and made to intersect targets of interest. Despite the numerous designs and evaluations of such techniques, there are a number of important factors which remain to be fully understood, two of which we focus on in this paper. The first is …","cites":"20","conferencePercentile":"86.66666667"},{"venue":"3DUI","id":"a895cd2ffecf8be3c4386130945b556f02b53df4","venue_1":"3DUI","year":"2012","title":"Poster: Comparing vibro-tactile feedback modes for collision proximity feedback in USAR virtual robot teleoperation","authors":"Paulo Gonçalves de Barros, Robert W. Lindeman","author_ids":"2123660, 1719686","abstract":"Although multi-sensorial interfaces have been shown to improve user experience in different settings in Virtual Reality, these interfaces are not yet fully explored in urban search-and-rescue robot teleoperation. This paper presents a study on the performance effects of adding different types of vibro-tactile collision proximity feedback to a virtual robot's interface during a search task in a virtual environment. Results indicate that the addition of vibro-tactile feedback causes positive performance effects, especially for the intensity variation mode. Nevertheless, it also has a comfort impact for prolonged use. 1. INTRODUCTION The area of teleoperated urban search-and-rescue (USAR) robotics has significantly grown in popularity in recent years. In order to help robot interface research advance, it is critical to understand the usefulness of mappings between types of sensed data and the types of notifications available in different display devices, so that each piece of information is correctly conveyed to the operator without hindering the detection of other notifications and the performance of the main search task itself. Understanding such mappings is the main focus of this research. The current work builds on previous result [1], and aims to evaluate the impact on performance when the robot output is enhanced with vibro-tactile feedback displays for robot collision avoidance in a search task. Two vibro-tactile interfaces, a vibration intensity variation mode and a vibratory pulse frequency mode, were compared to a no-vibration control case. 2. RELATED WORK Specific interface design and implementation guidelines have yet to be standardized. Nevertheless, some progress has been made [1]. Input devices generally consist of keyboard, mouse, and joystick. Data representation plays an important role in determining how difficult a USAR interface is to use. If the interface is too cognitively demanding, an operator may not succeed in both controlling the robot and completing his mission. Cognitive load variations have been previously claimed to affect the human-robot interaction (HRI) system performance due in part to variations in operator's situation awareness (SA) [3]. Although current USAR interfaces aim at improving SA and efficiency [5], little effort has been put on reducing cognitive load. The absence of operator-centered USAR-specific multi-sensorial interaction and interruption management evaluation studies indicates that such interfaces are still not fully explored. The work presented here attempts to attack the operator cognitive load problem through the use of feedback with multi-sensorial devices in addition to graphical ones, specifically vibro-tactile feedback. 3. ROBOT INTERFACE The robot interface design 1 …","cites":"1","conferencePercentile":"20.96774194"},{"venue":"3DUI","id":"6643e07d422bc3441e434a25de899c7ee0340303","venue_1":"3DUI","year":"2011","title":"Enhancing robot teleoperator situation awareness and performance using vibro-tactile and graphical feedback","authors":"Paulo Gonçalves de Barros, Robert W. Lindeman, Matthew O. Ward","author_ids":"2123660, 1719686, 1685227","abstract":"Most of the feedback received by operators of current robot-teleoperation systems is graphical. When a large variety of robot data needs to be displayed however, this may lead to operator overload. The research presented in this paper focuses on off-loading part of the feedback to other human senses, specifically to the sense of touch, to reduce the load due to the interface, and as a consequence, to increase the level of operator situation awareness. Graphical and vibro-tactile versions of feedback delivery for collision interfaces were evaluated in a search task using a virtual teleoperated robot. Parameters measured included task time, number of collisions between the robot and the environment, number of objects found and the quality of post-experiment reports through the use of sketch maps. Our results indicate that the combined use of both graphical and vibro-tactile feedback interfaces led to an increase in the quality of sketch maps, a possible indication of increased levels of operator situation awareness, but also a slight decrease in the number of robot collisions. 1 INTRODUCTION The process of robot teleoperation may be divided into four primary activities: sensing the state of the robot and the remote environment, making sense of such state, deciding on the next action to be taken, and carrying out that action. Any of these steps may make use of automation. The human-robot interaction (HRI) cycle in Figure 1 happens indefinitely as the task is carried out. In the case of urban search-and-rescue (USAR), the main focus area of this paper, little automation is generally present, though the use of point navigation has become a common approach in robot teleoperation [22][17][29][30]. USAR teleoperation is generally done through the use of ordinary input devices such as keyboard, mouse, and joystick. Most if not all of the information sensed from the robot is presented in a graphic display. During a mission, the operator uses this interface not only as a means to understand the state of the robot and its surrounding environment, but also as a tool to complete mission goals. Figure 1. Simple representation of interaction cycle between robot and operator in an HRI system. Depending on how data is represented on screen, succeeding in both of these tasks may turn out to be very cognitively demanding. This increase in cognitive load may cause a decrease in operator situation awareness (SA) [12], and hence hinder the performance of the entire HRI …","cites":"5","conferencePercentile":"65.2173913"},{"venue":"3DUI","id":"1a42ff15b9e0837b65189ead4d020a7e140ca603","venue_1":"3DUI","year":"2006","title":"Group Selection Techniques for Efficient 3D Modeling","authors":"Ji-Young Oh, Wolfgang Stuerzlinger, Darius Dadgari","author_ids":"1737278, 3342964, 1893481","abstract":"Object selection and manipulation (e.g. moving, rotating) are the most basic tasks in 3D scene construction. While most research on selection and manipulation techniques targets single objects, we examine the concept of group selection in this paper. Group selection is often given lesser importance than single object selection, yet is vital in providing users with a way to modify larger scenes with objects which are repetitive, sequential, or otherwise inherently understood as 'belonging together' by a user. We observed users manipulating objects in 3D scenes, and while doing so, they clearly expected that objects would be grouped based on their gravitational relationship. That is, all objects that are supported by some selected object will follow the motion of the selected object when manipulated. In this paper, we present a system that efficiently supports the manipulation of groups of objects via a gravitational hierarchy. As this hierarchy is derived with a collision detector, the new grouping techniques do not require semantic or user specified information to work. The results of the evaluation show that using the gravitational hierarchy improves scene rearrangement significantly compared to conventional non-hierarchical methods. Finally, we discuss lessons learned from this study and make some suggestions on how the results can be incorporated into other systems. 1 INTRODUCTION Selection and manipulation of objects in 3D modeling systems are typically used to modify a virtual representation of real world objects such as furniture, buildings, or architectural elements – walls or pillars, etc. These types of scenes generally include some sort of ground surface to provide users with a frame of reference and a depth cue [1, 21]. This ground surface is generally used as a base for all construction. In real world scenery, we can frequently observe scenes composed of repetitive or similar objects – e.g. rows of desks and chairs in an office, windows on a building, or houses in a suburban town. Even if some components in the scene do not look exactly the same, people tend to ignore those differences and group these objects into conceptual units to understand the overall structure, e.g. the way we can conceptually group a row of assorted books on a shelf. 3D scene modeling systems often replicate such real world situations where an element or a number of neighbouring elements are repeatedly used in different arrangements. Hence, selection and rearrangement of groups are important tasks in 3D scene modeling. In …","cites":"12","conferencePercentile":"55.88235294"},{"venue":"3DUI","id":"c766822d3f639f2917ccd44551e7384e433cbe91","venue_1":"3DUI","year":"2011","title":"Design and evaluation of methods to prevent frame cancellation in real-time stereoscopic rendering","authors":"Jérôme Ardouin, Anatole Lécuyer, Maud Marchal, Éric Marchand","author_ids":"2599954, 1693899, 1722424, 1695599","abstract":"Frame cancellation comes from the conflict between two depth cues: stereo disparity and occlusion with the screen border. When this conflict occurs, the user suffers from poor depth perception of the scene. It also leads to uncomfortable viewing and eyestrain due to problems in fusing left and right images. In this paper we propose a novel method to avoid frame cancellation in real-time stereoscopic rendering. To solve the disparity/frame occlusion conflict, we propose rendering only the part of the viewing volume that is free of conflict by using clipping methods available in standard real-time 3D APIs. This volume is called the \" Stereo Compatible Volume \" (SCV) and the method is named \" Stereo Compatible Volume Clipping \" (SCVC). Black Bands, a proven method initially designed for stereoscopic movies is also implemented to conduct an evaluation. Twenty two people were asked to answer open questions and to score criteria for SCVC, Black Bands and a Control method with no specific treatment. Results show that subjective preference and user's depth perception near screen edge seem improved by SCVC, and that Black Bands did not achieve the performance we expected. At a time when stereoscopic capable hardware is available from the mass consumer market, the disparity/frame occlusion conflict in stereoscopic rendering will become more noticeable. SCVC could be a solution to recommend. SCVC's simplicity of implementation makes the method able to target a wide range of rendering software from VR application to game engine. 1 INTRODUCTION In stereoscopic real-time rendering, depth perception problems occur when objects are displayed off screen (negative parallaxes) close to the screen border. This effect has been observed since the early works on stereoscopic movies. Valyus has named the phenomenon frame cancellation [1]. The problem comes from the conflict between two depth cues: stereo disparity tells the user that the perceived object is in front of the screen while the occlusion with the screen border indicates that the screen border is in front of the object. When this conflict occurs, the user suffers poor depth perception of the scene [2]. It also leads to uncomfortable viewing and eyestrain due to problems in fusing left and right images [3]. Figure 1 illustrates the typical configuration of left and right eye viewing volume for a determined display surface. Intersection of the left and right viewing volume is not subject of frame cancellation while the remaining volume represents location of geometry …","cites":"1","conferencePercentile":"39.13043478"},{"venue":"3DUI","id":"813e08a66d3422864ade65224b36d6d0e8850575","venue_1":"3DUI","year":"2011","title":"Silver Surfer: A system to compare isometric and elastic board interfaces for locomotion in VR","authors":"Jia Wang, Robert W. Lindeman","author_ids":"4263537, 1719686","abstract":"In this poster, we present the design and the implementation of a surfboard travel interface inspired by the \" Silver Surfer \" cartoons and movies. The board interface works in either tilt mode or balance mode, creating an interesting comparison of isometric and elastic devices for rate control and position control travel in virtual environments. We also demonstrate the setup of a complete virtual reality system aimed to evaluate the usability of this travel interface in a future study. 1 INTRODUCTION Travel in virtual environments (VE) has been a difficult problem since the beginning of virtual reality (VR), basically due to the difficulty of designing an intuitive, effective, and precise interface which can map the user's finite movements in the real world to a potentially infinite virtual world while maintaining as much presence as possible. Based on real world skateboarding, snowboard, and surfing, VR researchers and arcade game platform designers have implemented various board interfaces which enable the users to surf a VE intuitively and effectively, such as the PEMRAM motion base and the Hawaii Surf Simulator. Because it is hard for people to yaw a board when standing on it, most board interfaces only support two degrees of freedom (DOF), namely pitch and roll. And they limit the virtual movement to be on a surface (e.g., the ground) due to the 3-DOF requirement of complete 3D travel. This is sometimes not a sufficient solution because for many VR applications, such as virtual 3D modeling, virtual data visualization and virtual tourism, being able to travel along the Z axis is indispensable. Not willing to occupy the user's hands as they were designed to fulfill wayfinding tasks, Valkov [3] programmed a special foot gesture tracked by the Wii Fit balance board to extend his board-surfing metaphor to 3-DOF. It allows the user to travel in 3D completely by using her lower body alone, but is not very intuitive or effective, and is prone to undesired inputs. In the system we propose, we choose to add the third DOF to the user's arm, which is independent with the lower body movements, and is more intuitive based on the \" Silver Surfer \" cartoons and movies. In [5], Zhai reported a user study designed to compare a hand-held elastic device and a hand-held isometric device for 6-DOF manipulation by rate control. Results showed that the former has some superiority over the latter, but …","cites":"1","conferencePercentile":"39.13043478"},{"venue":"3DUI","id":"8bb3fb20f3f6d93c45f31caef29e5c79bf3445fc","venue_1":"3DUI","year":"2010","title":"Revisiting path steering for 3D manipulation tasks","authors":"Lei Liu, Jean-Bernard Martens, Robert van Liere","author_ids":"1703617, 1763266, 8361246","abstract":"The law of path steering, as proposed by Accot and Zhai, describes a quantitative relationship between the human temporal performance and the path spatial characteristics. The steering law is formulated as a continuous goal-crossing task, in which a large number of goals are crossed along the path. The steering law has been verified empirically for locomotion, in which a virtual driving task through straight and circular paths was performed. We revisit the path steering law for manipulation tasks in desktop virtual environments. We have conducted controlled experiments in which users operated a pen input device to steer a virtual ball through paths of varying length, width, curvature and orientation. Our results indicate that, although the steering law provides a good description of the overall task time as a function of index of difficulty ID = L/W, where L and W are the path length and width, it does not account for other relevant factors. We specifically show that the influence of curvature can be modeled by a percentage increase in steering time, independent of index of difficulty. The path orientation relative to the viewing direction has a periodic effect on the steering time, which can be optimally described by a function of Fourier series expansions. In addition, there is also an effect of the handedness of the subjects on the steering between the left and right districts in 3D manipulation tasks.","cites":"8","conferencePercentile":"78.94736842"},{"venue":"3DUI","id":"da2d35d0673ccfb73c42d0dee8586a89c954704a","venue_1":"3DUI","year":"2009","title":"Poster: RealDance: An exploration of 3D spatial interfaces for dancing games","authors":"Emiko Charbonneau, Andrew Miller, Chadwick A. Wingrave, Joseph J. LaViola","author_ids":"1989115, 7691539, 3293975, 1726039","abstract":"We present RealDance, a prototype video game for exploring spatial 3D interaction for dance-based gaming and instruction. Our interface uses four Nintendo Wii remotes and is independent of buttons, floor position, cameras, or sensor bars so the user is un-tethered, allowing for natural, full-body motion. Our range of detectable movements includes stationary poses, punches, kicks, claps and stomps, which are scored in the context of the dance routine. We describe our initial experiments in interface design, gesture evaluation and scoring, and user experience, which reveals interesting new areas for 3D spatial interaction research related to creating an 'ideal' dance game.","cites":"3","conferencePercentile":"54.16666667"},{"venue":"3DUI","id":"d23e17c57d07edcb0cf9636d3fd083eeeb0f256d","venue_1":"3DUI","year":"2014","title":"Poster: Exploring 3D volumetric medical data using mobile devices","authors":"Teddy Seyed, Francisco Marinho Rodrigues, Frank Maurer, Anthony Tang","author_ids":"3078189, 2068017, 1736996, 3151023","abstract":"Medical imaging specialists have traditionally used keyboard and mouse based techniques and interfaces for examining both 2D and 3D medical images, but with newer imaging technologies resulting in significantly larger volumes of 3D medical images, these techniques that have become increasingly cumbersome for imaging specialists. To replace traditional techniques, using mobile devices present an effective means for navigating and exploring complex 3D medical data sets, as they provide increased fluidity and flexibility, leveraging people's existing skills with tangible objects. 3D interactions using mobile devices may provide benefit for imaging specialists, but little is known about using these interactions in the medical imaging domain. In this paper, we explore the design of 3D interaction techniques using mobile devices and preliminary feedback from imaging specialists suggests that these interactions may be a viable solution for the medical imaging domain. 1 INTRODUCTION AND MOTIVATION Imaging techniques such as magnetic resonance imaging (MRI), computed tomography (CT) and X-Rays are now commonplace tools for medical investigations by imaging specialists (such as radiologists) [1]. These medical investigations ultimately provide evidence for important medical decisions, such as choice of treatment or avoiding risky surgeries for patients. Many of the imaging techniques (such as MRI and CT) produce multiple 2D cross-sections (or volumetric slices) of scanned tissue, and consequently, imaging specialists examine these images in \" abstract 2D \" [2], where a human body is examined by considering these 2D cross-sections and mentally reconstructed to fit anatomical structures [3]. Traditionally, imaging specialists have preferred this 2D visualization approach with keyboard and mouse-based interfaces over 3D visualizations because 3D interaction techniques with keyboard and mouse based interfaces were less practical than 2D ones, as the 3D images were hard to interpret on flat, 2D screens [3]. 2D navigation tools are still currently preferred in medical imaging [4], despite the increasing volume of images and need for 3D visualizations and interactions. Because a proper evaluation of 3D volumetric data in medical imaging requires 3D visualization and interactions, there is significant prior work into 3D interaction techniques to draw upon when designing interactions for navigation and manipulation of 3D","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"1e379f5888b98c16cab128a8527459e83ff3e629","venue_1":"3DUI","year":"2012","title":"The King-Kong Effects: Improving sensation of walking in VR with visual and tactile vibrations at each step","authors":"Léo Terziman, Maud Marchal, Franck Multon, Bruno Arnaldi, Anatole Lécuyer","author_ids":"2415447, 1722424, 1731192, 1809163, 1693899","abstract":"In this paper we present novel sensory feedbacks named \" King-Kong Effects \" to enhance the sensation of walking in virtual environments. King Kong Effects are inspired by special effects in movies in which the incoming of a gigantic creature is suggested by adding visual vibrations/pulses to the camera at each of its steps. In this paper, we propose to add artificial visual or tactile vibrations (King-Kong Effects or KKE) at each footstep detected (or simulated) during the virtual walk of the user. The user can be seated, and our system proposes to use vibrotactile tiles located under his/her feet for tactile rendering, in addition to the visual display. We have designed different kinds of KKE based on vertical or lateral oscillations, physical or metaphorical patterns, and one or two peaks for heal-toe contacts simulation. We have conducted different experiments to evaluate the preferences of users navigating with or without the various KKE. Taken together, our results identify the best choices for future uses of visual and tactile KKE, and they suggest a preference for multisensory combinations. Our King-Kong effects could be used in a variety of VR applications targeting the immersion of a user walking in a 3D virtual scene.","cites":"4","conferencePercentile":"67.74193548"},{"venue":"3DUI","id":"8ae12ba2afbb973104bc64c91d9c5f2fd6acd6e8","venue_1":"3DUI","year":"2007","title":"An Exploration of Non-Isomorphic 3D Rotation in Surround Screen Virtual Environments","authors":"Joseph J. LaViola, Michael Katzourin","author_ids":"1726039, 3043406","abstract":"Non-isomorphic rotational mappings have been shown to be an effective technique for rotation of virtual objects in 3D desktop environments. In this paper, we present an experimental study that explores the performance characteristics of isomorphic and non-isomorphic rotation techniques in a surround screen virtual environment. Our experiment compares isomorphic rotation with non-isomorphic rotation techniques utilizing three separate amplification factors, two different thresholds for task completion, and two different angular ranges for virtual object rotation. Our results show that a non-isomorphic mapping with an amplification factor of three is both optimal in terms of completion time and accuracy and is most preferred by our test subjects. In addition, our results suggest that, in a surround screen virtual environment, rotation tasks using both isomorphic and non-isomorphic rotational mappings can be completed faster and more accurately compared to previous studies exploring rotation in 3D user interfaces.","cites":"14","conferencePercentile":"70"},{"venue":"3DUI","id":"a8d42355d9fe62bae4eded3af7bd37bc583cf3bb","venue_1":"3DUI","year":"2016","title":"Floating charts: Data plotting using free-floating acoustically levitated representations","authors":"Themis Omirou, Asier Marzo Pérez, Sriram Subramanian, Anne Roudaut","author_ids":"3186291, 1820610, 1702794, 3219180","abstract":"Charts are graphical representations of numbers that help us to extract trends, relations and in general to have a better understanding of data. For this reason, multiple systems have been developed to display charts in a digital or physical manner. Here, we introduce Floating Charts, a modular display that utilizes acoustic levitation for positioning free-floating objects. Multiple objects are individually levitated to compose a dynamic floating chart with the ability to move in real time to reflect changes in data. Floating objects can have different sizes and colours to represent extra information. Additionally, they can be levitated across other physical structures to improve depth perception. We present the system design, a technical evaluation and a catalogue of chart variations.","cites":"1","conferencePercentile":"75"},{"venue":"3DUI","id":"f95772870c80cfd3b116a9e6a7498f08074ee10c","venue_1":"3DUI","year":"2012","title":"Design and evaluation of 3D cursors and motion parallax for the exploration of desktop virtual environments","authors":"David Antonio Gómez Jáuregui, Ferran Argelaguet, Anatole Lécuyer","author_ids":"1695798, 1854224, 1693899","abstract":"The selection and manipulation of 3D content in desktop virtual environments is commonly achieved with 2D mouse cursor-based interaction. However, by interacting with image-based techniques we introduce a conflict between the 2D space in which the 2D cursor lays and the 3D content. For example, the 2D mouse cursor does not provide any information about the depth of the selected objects. In this situation, the user has to rely on the depth cues provided by the virtual environment, such as perspective deformation, shading and shadows. In this paper, we explore new metaphors to improve the depth perception when interacting with 3D content. Our approach focus on the usage of 3D cursors controlled with 2D input devices (the Hand Avatar and the Torch) and a pseudo-motion parallax effect. The additional depth cues provided by the visual feedback of the 3D cursors and the motion parallax are expected to increase the users' depth perception of the environment. The evaluation of proposed techniques showed that users' depth perception was significantly increased. Users were able to better judge the depth ordering of virtual environment. Although 3D cursors showed a decrease of selection performance, it is compensated by the increased depth perception.","cites":"3","conferencePercentile":"48.38709677"},{"venue":"3DUI","id":"9e9315fc797b3e2e0a26bcf59e69a39f2a113c89","venue_1":"3DUI","year":"2012","title":"PapARt: Interactive 3D graphics and multi-touch augmented paper for artistic creation","authors":"Jérémy Laviole, Martin Hachet","author_ids":"3421303, 2281511","abstract":"(a) Scene manipulation (b) Simplified rendering (c) Drawing (d) Resulting drawing (e) projection + drawing Figure 1: A videoprojector projects a 3D scene on a paper sheet. A user can directly interact with the drawing using touch and tangible interfaces (1a). The application is made to create drawings such as the rabbit (1d) and the lily flower (1e). A toon shading is used to simplify, ease and speed up the drawing, an example result is the rabbit (1b). The lily flower (1e) is half drawing and half drawing plus projection. ABSTRACT Standard physical pen-and-paper creation and computer graphics tools tend to evolve in separate tracks. In this paper, we propose a new interface, PapARt, that bridges the gap between these two worlds. We developed a system that allow users to visualize, manipulate and edit a 3D scene projected onto a paper sheet. Using multitouch and tangible interfaces, users can directly interact with the 3D scene to prepare their drawings. Then, thanks to the projection of the 3D scene directly on the final surface medium, they can draw using standard tools while relying on the underlying 3D scene. Hence, users benefit from both the power of interactive 3D graphics and fast and easy interaction metaphors, while keeping a direct link with the physical material. PapARt has been tested during a large scale exhibition for general public. Such an interface, which combines computer-assisted drawing and free form user ex-pressiveness on a standard sheet of paper, opens new perspectives for enhancing user creation.","cites":"10","conferencePercentile":"95.16129032"},{"venue":"3DUI","id":"c7de442afaae8dc1defda7d35aa413d3c687ea15","venue_1":"3DUI","year":"2013","title":"A novel 3D carousel based on pseudo-haptic feedback and gestural interaction for virtual showcasing","authors":"Pierre Gaucher, Ferran Argelaguet, Jérôme Royan, Anatole Lécuyer","author_ids":"3310080, 1854224, 1789247, 1693899","abstract":"Figure 1: (Left) manipulation of our carousel based on gestural interaction, (Right) screenshot of our 3D carousel application ABSTRACT This paper describes a novel 3D carousel intended for virtual show-casing. The carousel is based on a 3D ring menu which is rendered on a 3D display. The user interaction with the carousel is achieved by tracking the user's gestures while the behavior of the carousel is modified through physically-based pseudo-haptic effects. The pseudo-haptic effect is based on modifying the friction coefficient taking into account the relevance of the objects displayed in the carousel. In addition a magnetic effect is introduced in order to attract to the active item of the carousel. The results from a first user study are globally encouraging and provide insights about the potential of our 3D pseudo-haptic carousel for virtual showcasing.","cites":"0","conferencePercentile":"14"},{"venue":"3DUI","id":"29e6e86b9baf882e88abc2cd3ef0e84288d75e57","venue_1":"3DUI","year":"2007","title":"An Exploration of Interaction-Display Offset in Surround Screen Virtual Environments","authors":"Dmitri K. Lemmerman, Joseph J. LaViola","author_ids":"3073334, 1726039","abstract":"We present a study exploring the effect of positional offset between the user's interaction frame-of-reference (the physical location of input) and the display frame-of-reference (where graphical feedback appears) in a surround-screen virtual environment (SSVE). Our research hypothesis states that, in such an environment, task performance improves given an offset between the two frames-of-reference. In our experiment, users were asked to match a target color using a 3D color widget under three different display-interaction offset conditions: no offset (i.e., collocation), a three inch offset, and a two foot offset. Our results suggest that collo-cation of the display and interaction frames-of-reference may degrade accuracy in widget-based tasks and that collocation does not necessarily lead the user to spend more time on the task. In addition , these results contrast with previous studies performed with head-mounted display (HMD) platforms, which have demonstrated significant performance advantages for collocation and the \" direct manipulation \" of virtual objects. Moreover, a previous study with a different task performed in a projector-based VE has also demonstrated that collocation is not detrimental to user performance. Our conclusion is that the most effective positional offset is dependent upon the specific display hardware and VE task.","cites":"1","conferencePercentile":"13.33333333"},{"venue":"3DUI","id":"5ac6701bec43a4e02d13efd52c14215a7a187d78","venue_1":"3DUI","year":"2015","title":"Creating an impression of virtual liquid by modeling Japanese sake bottle vibrations","authors":"Sakiko Ikeno, Ryuta Okazaki, Taku Hachisu, Hiroyuki Kajimoto","author_ids":"2469950, 2812591, 3242743, 1776927","abstract":"It is known that visual, auditory, and tactile modalities affect the experiences of eating and drinking. One such example is the \" glug \" sound and vibration from a Japanese sake bottle when pouring liquid. Previous studies have modeled the wave of the vibration by summation of two decaying sinusoidal waves with different frequencies; we examined the validity of this model by subjective evaluation. Furthermore, to enrich expression of various types of liquid, we included two new properties of liquid: the viscosity and the residual amount of liquid, both based on recorded data.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"0446bf18077c1b2896849c3d66d91f6ff8a34f4b","venue_1":"3DUI","year":"2011","title":"A 3D desktop puzzle assembly system","authors":"Dmitri Shuralyov, Wolfgang Stuerzlinger","author_ids":"2829617, 3342964","abstract":"We describe a desktop virtual reality system targeted at 3D puzzle assembly. The results of the evaluation show that all novices could successfully complete the puzzle within an average of about six minutes, while experts took about two minutes. 1 SYSTEM DESCRIPTION Our main design choice was to use a desktop system, with a mouse as the input device. This was motivated by the fact that desktop systems and mice have (typically) lower latencies than other alternatives. Also, today there are good methods to map 2D input to 3D manipulation. In particular, we use a contract assumption, snapping, sliding, collision detection, interaction only with visible objects, and viewer constraints, as previous work [4] has shown all these to be beneficial. We display the 3D scene in perspective, but do not use stereo display, as it is to us (at best) unclear, if stereo has strong benefits for 3D interaction. We used a PC with a 3.2 GHz QuadCore CPU, GeForce GTX470 graphics card, and 4 GB. For input we used a Logitech G9 Laser Precision Gaming Mouse and a 17 \" 1440x900 LCD for output. We created an OpenGL C++ application, which re-implements the sliding algorithm of the SESAME system [3], but uses a new rotation and two new navigation methods. The user can toggle navigation methods via a key on the keyboard. In the experiment we used only the object-centric navigation. 1.1.1 Translations The main idea of the SESAME sliding algorithm [3] is that a manipulated object is always in contact with other (static) surfaces in the scene (unless over the background) and slides on these surfaces stably under the mouse cursor. Continuous collision detection with Opcode 1.3 prevents object interpenetration. With this, only 2D input is necessary to specify a 3D position, as the object can only move on the 2D manifold of visible surfaces. This algorithm uses the frame buffer to detect all surface(s) behind the currently selected object. Then, the object is snapped in the view direction to the nearest surface behind it and slides on the corresponding background feature (face, edge, or vertex). Using the first surface behind the object ensures that this algorithm does not always \" pop to front \". This enables the user to slide an object under another. However, if the object disappears completely behind another object, i.e. no part of moving object is visible, the system \" pops \" the …","cites":"6","conferencePercentile":"76.08695652"},{"venue":"3DUI","id":"130ba881a03e2172dfc1dfa3a54a7131737f757e","venue_1":"3DUI","year":"2007","title":"Exploring 3D Interaction in Alternate Control-Display Space Mappings","authors":"Jeroen Keijser, M. Sheelagh T. Carpendale, Mark S. Hancock, Tobias Isenberg","author_ids":"2318842, 8238284, 1771907, 3008483","abstract":"The desire to have intuitive, seamless 3D interaction fuels research exploration into new approaches to 3D interaction. However, within these explorations we continue to rely on Brunelleschi's perspective for display and map the interactive control space directly into it without much thought on the effect that this default mapping has. In contrast, there are many possibilities for creating 3D interaction spaces, thus making it important to run user studies to examine these possibilities. Options in mapping the control space to the display space for 3D interaction have previously focused on the manipulation of control-display ratio or gain. In this paper, we present a conceptual framework that provides a more general control-display description that includes mappings for flip, rotation, skew, as well as scale(gain). We conduct a user study to explore 3D selection and manipulation tasks in three of these different mappings in comparison to the commonly used mapping (perspective mapping of control space to a perspective display). Our results show interesting differences between interactions and user preferences in these mappings and indicate that all may be considered viable alternatives. Together this framework and study open the door to further exploration of 3D interaction variations.","cites":"2","conferencePercentile":"30"},{"venue":"3DUI","id":"88b9439457ee3d02f0ba99f1059a3bdd6c11c46d","venue_1":"3DUI","year":"2014","title":"Evaluating dynamic-adjustment of stereo view parameters in a multi-scale virtual environment","authors":"Isaac Cho, Jialei Li, Zachary Wartell","author_ids":"2374697, 2769850, 2656827","abstract":"Dynamic view parameter adjustment can reduce visual fatigue issues in stereo displays. In a multi-scale virtual environment, which has geometric details ranging over several orders of magnitude, these adjustments are particularly important. We evaluate how two adjustment techniques interact with 7 degree-of-freedom navigation in desktop VR and a CAVE. The travel task has two stages, an initial targeted zoom and detailed geometric inspection. The results show benefits of the adjustments both for reducing fusion problems and for task completion time, but only in certain condition combinations. Peculiar view configuration examples show the difficulty of creating robust adjustment rules.","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"3a1ce2eaae218259fc513b949945041aa99244f4","venue_1":"3DUI","year":"2012","title":"Augmented textual data viewing in 3D visualizations using tablets","authors":"Charles Roberts, Basak Alper, JoAnn Kuchera-Morin, Tobias Höllerer","author_ids":"6114171, 2710038, 1727605, 1743721","abstract":"Many data sets contain structural 3D components along with associated textual data. While structural data is often best visualized on large stereoscopic displays, such displays can pose problems presenting accompanying textual information. Chief among these problems is a tradeoff between size dependent legibility of text and the occlusion of structural data if text is presented at larger sizes. Our solution to this problem integrates structural data shown on a large display while users select features of the structure and view the associated textual data on personal tablets. Our solution also lends itself to collaborative browsing tasks. In our initial implementation each user can individually select structural components and view the associated text on their own tablet; thus, everyone becomes an active participant in data mining. When individual users find textual data they deem of interest to the group they can share it with collaborators by temporarily pushing the text box to the large display. The current status and each individual's browsing history are shown on the large display in order to provide awareness of other team members' actions.","cites":"3","conferencePercentile":"48.38709677"},{"venue":"3DUI","id":"93134086fabd8508b0199c4806ab227e6cc87e6d","venue_1":"3DUI","year":"2015","title":"3D virtual hand pointing with EMS and vibration feedback","authors":"Max Pfeiffer, Wolfgang Stuerzlinger","author_ids":"2933297, 3342964","abstract":"Pointing is one of the most basic interaction methods for 3D user interfaces. Previous work has shown that visual feedback improves such actions. Here we investigate if electrical muscle stimulation (EMS) and vibration is beneficial for 3D virtual hand pointing. In our experiment we used a 3D version of a Fitts' task to compare visual feedback, EMS, vibration, with no feedback. The results demonstrate that both EMS and vibration provide reasonable addition to visual feedback. We also found good user acceptance for both technologies.","cites":"4","conferencePercentile":"94.44444444"},{"venue":"3DUI","id":"8ed18bfe16a7edd13f844dd87ed079a9dfb41b06","venue_1":"3DUI","year":"2013","title":"Expressing animated performances through puppeteering","authors":"Takaaki Shiratori, Moshe Mahler, Warren Trezevant, Jessica K. Hodgins","author_ids":"2463857, 3156160, 3093495, 1788773","abstract":"Figure 1: Animator puppeteering a character for a dialog, \" Minimum Wage, \" and frames from the resulting animation. ABSTRACT An essential form of communication between the director and the animators early in the animation pipeline is rough cut at the motion (a blocked-in animation). This version of the character's performance allows the director and animators to discuss how the character will play his/her role in each scene. However, blocked-in animation is also quite time consuming to construct, with short scenes requiring many hours of preparation between presentations. In this paper, we present a puppeteering interface for creating blocked-in motion for characters and various simulation effects more quickly than is possible in a keyframing interface. The animator manipulates one of a set of tracked objects in a motion capture system to control a few degrees of freedom of the character on each take. We explore the design space for the 3D puppeteering interface with a set of seven professional animators using a \" think-aloud \" protocol. We present a number of animations that they created and compare the time required to create similar animations in our 3D user interface and a commercial keyframing interface.","cites":"5","conferencePercentile":"92"},{"venue":"3DUI","id":"96a84ed4a839d29d7a45a218f6c6032a9229848b","venue_1":"3DUI","year":"2010","title":"The design and evaluation of 3D positioning techniques for multi-touch displays","authors":"Anthony Martinet, Géry Casiez, Laurent Grisoni","author_ids":"7193783, 3051289, 7908168","abstract":"Figure 1: (i) Illustration of the Z-technique. The first finger (right hand in the example) is used for direct positioning in the camera plane while the second finger (left hand) is used for depth positioning in an indirect way. Backward-forward movements move the object farther or closer to the user. (ii and iii) Illustration of the multi-touch viewport technique. The first finger (left hand) is used for 2D positioning in the camera plane corresponding to the viewport (ii) while the second finger (right hand) is used to move the object along the third coordinate (iii). The gray line is used as visual feedback to represent the displacement allowed for the second finger. ABSTRACT Multi-touch displays represent a promising technology for the display and manipulation of 3D data. To fully exploit their capabilities , appropriate interaction techniques must be designed. In this paper, we explore the design of free 3D positioning techniques for multi-touch displays to exploit the additional degrees of freedom provided by this technology. Our contribution is twofold: first we present an interaction technique to extend the standard four view-ports technique found in commercial CAD applications, and second we introduce a technique designed to allow free 3D positioning with a single view of the scene. The two techniques were evaluated in a preliminary experiment. The first results incline us to conclude that the two techniques are equivalent in term of performance showing that the Z-technique provides a real alternative to the statu quo viewport technique.","cites":"32","conferencePercentile":"100"},{"venue":"3DUI","id":"c63e474bcaa75f2b689847622884afe0979df6e0","venue_1":"3DUI","year":"2015","title":"Evaluation of a bimanual simultaneous 7DOF interaction technique in virtual environments","authors":"Isaac Cho, Zachary Wartell","author_ids":"2374697, 2656827","abstract":"c 2015 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. ABSTRACT This paper introduces our novel bimanual interaction technique, Spindle+Wheel that provides simultaneous 7DOF. Spindle+Wheel takes advantage of greater finger dexterity, the \" bandwidth-of-the-fingers \" and passive haptics, by using a pair of precision-grasp 6DOF isotonic input devices rather than using either a tracked pair of pinch gloves or a pair of power-grasped 6DOF isotonic input devices. Two user studies were conducted to show that our simultaneous 7DOF interaction technique outperforms a previous two-handed technique as well as a one-handed scene-in-hand technique for a 7DOF travel task.","cites":"1","conferencePercentile":"61.11111111"},{"venue":"3DUI","id":"68628285acb9225c049b0b252886862720fcbee5","venue_1":"3DUI","year":"2011","title":"Squeeze me and i'll change: An exploration of frustration-triggered adaptation for multimodal interaction","authors":"Johanna Renny Octavia, Karin Coninx, Kris Luyten","author_ids":"1714019, 1695519, 1681624","abstract":"Complex 3D interaction in virtual environments may inhibit user interaction and cause frustration. Supporting adaptivity based on the detected user frustration can be considered as one promising solution to enhance user interaction. Our work proposes to provide adaptive assistance to users who are frustrated during their interaction with 3D user interfaces in virtual environments. The obtrusive-ness of physiological measurements to detect frustration inspired us to investigate the pressure patterns exerted on a 3D input device for this purpose. The experiment presented in this paper has shown a great potential on utilizing the finger pressure measures as an alternative to physiological measures to indicate user frustration during interaction. Furthermore, the findings in this particular context showed that adaptation of haptic interaction was effective in increasing the user's performance and making users feel less frustrated in performing their tasks in the 3D environment.","cites":"1","conferencePercentile":"39.13043478"},{"venue":"3DUI","id":"0d03e44dc650a7dcb83b272fada50dea9bc310ed","venue_1":"3DUI","year":"2013","title":"Hook: Heuristics for selecting 3D moving objects in dense target environments","authors":"Michael Ortega-Binderberger","author_ids":"2163619","abstract":"Figure 1: Hook in action in dense environments and fast-moving targets. The visual feedback shows the moving target that, according to cursor behavior, the system detects as being the target-of-interest. ABSTRACT This paper presents Hook, a new interaction technique for selecting moving targets. As opposed to existing techniques, Hook uses heuristic methods. It allows pointing in dense 3D environments, and on targets moving with high velocity. Moreover, Hook minimizes the impact of its visual feedback for maintaining user's interaction comfort. Two adaptations of Hook for 2 dof (degrees-of-freedom) and 3 dof interaction for 3D environments have been evaluated. Results clearly show that Hook outperforms the existing methods in pointing time and error rates, for fast and slow targets, in the two configurations. All the participants confirmed the expected feeling with regards to ease of use.","cites":"4","conferencePercentile":"80"},{"venue":"3DUI","id":"0a485eee88bf219c6f76f361891ed644b47139c7","venue_1":"3DUI","year":"2006","title":"Coordination Strategies for Assisted Viewpoint Interaction","authors":"Stephen B. Hughes","author_ids":"3029149","abstract":"Viewpoint manipulation in 3D interfaces is a complex, yet critical task. Viewers must balance the cognitive demands of understanding the layout of the environment while mastering the physical interaction required to position the viewpoint in meaningful locations. Various methods have been designed to automatically lead the viewer through a scene; however, these techniques often prove intrusive, leading viewers to struggle for control of the viewpoint. This paper formalizes four coordination strategies for assisting the user with viewpoint selection: Manual Override Contextual Override, Fixed Time and Parallel Viewing. The results of a user evaluation suggest that Manual Override and Parallel Viewing offer the greatest benefits to the user in selecting meaningful viewing parameters. 1 INTRODUCTION Many three-dimensional visualizations are characterized by the use of a mobile viewpoint that offers multiple perspectives on a set of visual information. While this is a routine interaction with the real world, abstracting this behavior to virtual or remote environments adds significant complexity. This is not a trivial assignment and the effort applied to manipulating the viewpoint may compete with the task of extracting relevant information from the environment. Arthur points out that \" When interaction becomes highly attention demanding, memory for the present location frequently decays, with the result that the individual becomes lost in space. \" [1]. One way to facilitate viewpoint control is to develop and refine conceptual models that map controls to operational tasks. Several dominant metaphors for viewpoint interaction have emerged, including \" world-in-hand \" \" eyeball-in-hand \" \" walking \" \" flying vehicle \" , and \" gaze-directed steering \" [4, 25]. Despite the popularity of these techniques, successful navigation is also often dependent on adopting sophisticated strategies such as acquiring survey views, or moving is structured patterns [5]. Even with these strategies, there is still plenty of room for error; Goerger et al find that viewers in information-rich Virtual Environments are particularly susceptible to superfluous data and are easily distracted [14]. In fact, facilitating viewers' ability to extract relevant information from the surrounding context represents a challenge that is common to the design of all visual displays. This is commonly known among visualization researchers as the \" focus+context \" problem [8], and has been well researched in the domain of 2D displays. Many approaches to this problem allow the viewer to distort or magnify a portion of the display to allow for closer inspection [22]. Other methods attempt to …","cites":"2","conferencePercentile":"11.76470588"},{"venue":"3DUI","id":"6b8844d78fb32c7a9244469f6ab9b8e530eb50e5","venue_1":"3DUI","year":"2013","title":"Touch & Detach: Ungrouping and observation methods for complex virtual objects using an elastic metaphor","authors":"Mai Otsuki, Tsutomu Oshita, Asako Kimura, Fumihisa Shibata, Hideyuki Tamura","author_ids":"1683502, 2603583, 1726979, 1723268, 1730627","abstract":"In this study, we implemented a stable and intuitive detach method named \" Touch & Detach \" for 3D complex virtual objects. In typical modeling software, parts of a complex 3D object are grouped for efficient operation, and ungrouped for observing or manipulating a part in detail. Our method uses elastic metaphors to prevent incorrect operations and to improve the operational feel and responsiveness. In addition, our method can represent the connection and its strength between the parts by simulating a virtual elastic band connecting the parts. It helps users to understand the relationship between the parts of a complex virtual object. This paper presents the details of our proposed method and user study.","cites":"0","conferencePercentile":"14"},{"venue":"3DUI","id":"0eaf8498562c5ec2b8067a00520eeb308a1df428","venue_1":"3DUI","year":"2012","title":"Immersive 3DUI on one dollar a day","authors":"Aryabrata Basu, Christian Saupe, Eric Refour, Andrew Raij, Kyle Johnsen","author_ids":"3227042, 1853139, 2342434, 3056335, 2664323","abstract":"A convergence between consumer electronics and virtual reality is occurring. We present an immersive head-mounted-display-based, wearable 3D user interface that is inexpensive (less than $900 USD), robust (sourceless tracking), and portable (lightweight and untethered). While the current display has known deficiencies, the user tracking quality is within the constraints of many existing applications, while the portability and cost offers opportunities for innovative applications that are not currently feasible. 1 INTRODUCTION In 1991, Randolph Pausch published a paper entitled \" Virtual Reality on Five Dollars a Day \" describing the design of a $5000 (USD) virtual reality (VR) interface; which, amortized over three years, was about five dollars a day [1]. To build such a low-cost system, off-the-shelf hardware was combined with creative engineering to form a makeshift, but complete, immersive 3D user interface (3DUI). The cost was a stark contrast to expensive commercial systems available during that time. More important than the price, however, was the possibility of immersive technology being available to vast numbers of designers and end-users, enabling immense creative efforts and sparking a renaissance of VR. Two decades later, this possibility is now rapidly becoming a reality. In this paper, we present an evolution of this idea, a complete, immersive 3DUI for one dollar a day. Enabling this evolution are low-cost consumer electronics devices that are mass-produced for entertainment purposes, yet are essentially the same technologies once reserved for VR applications, and often have as-good or better performance than existing \" professional \" devices [2-4]. These consumer devices are thus viable alternatives for 3DUI designers, and have indeed been particularly popular for prototyping new systems [5, 6, 7 ]. Building upon this idea, we have previously reported on the design of a mobile-VR system for immersive collaborative virtual environments [8]. The goal was to design a low cost system that would allow a user to enter a shared virtual space from anywhere, with the immediacy of a phone-call. Our approach combined a networked smart phone device with its embedded motion sensors and a connected head-mounted-display (HMD). The effect was to produce a minimal virtual reality system that could be used within seconds of the user's desire to enter a shared virtual space. The primary limitation of the previous design was that only immersive viewing (orientation only) was well supported. For hand position tracking, a mechanical, head-mounted, 3-degrees-of-freedom (DOF) tracker (constructed from the MadCatz Gametrak device) was provided, …","cites":"10","conferencePercentile":"95.16129032"},{"venue":"3DUI","id":"bd2d357314fb5a9436e7bf8cb97aaa4587798248","venue_1":"3DUI","year":"2014","title":"Poster: Design and development of a virtual reality system for vocational rehabilitation of individuals with disabilities","authors":"Evren Bozgeyikli, Lal Bozgeyikli, Matthew Clevenger, Andrew Raij, Redwan Alqasemi, Rajiv V. Dubey","author_ids":"2995388, 1978668, 3279681, 3056335, 2845943, 1725815","abstract":"This paper considers the design and development of a virtual reality system that aids in vocational rehabilitation of individuals with disabilities. The system focuses on three types of disabilities: autism spectrum disorder, traumatic brain injury, and severe mobility impairment. The system allows job trainers to rapidly assess the capabilities of individuals with disabilities, detect the most suitable job for them, and train them in a safe and motivating environment where there are no significant consequences of making errors. Design considerations and research questions that arise throughout development are shared to create awareness of the special considerations involved in developing virtual reality systems for individuals with disabilities.","cites":"2","conferencePercentile":"52.17391304"},{"venue":"3DUI","id":"374c9361e1564ba2ff947c382ee8b491d96bc92e","venue_1":"3DUI","year":"2014","title":"Stretch 'n' cut: Method for observing and ungrouping complex virtual objects in 3D space using elastic band metaphor","authors":"Mai Otsuki, Asako Kimura, Fumihisa Shibata, Hideyuki Tamura","author_ids":"1683502, 1726979, 1723268, 1730627","abstract":"We propose a system which realizes gesture-based separation, and the observation of a group of parts from complex virtual objects in 3D space. One practical application of our system is for training purposes such as learning the structures of complex 3D object such as the human body or industrial products. By using an \" elastic band \" metaphor, our method enables users to (1) observe the relationship (connection and its strength) between the parts and (2) separate a part of the object efficiently.","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"ae81bdd0883d4bd3de3aafe53580465dd37e3361","venue_1":"3DUI","year":"2015","title":"Spatially-multiplexed MIMO markers","authors":"Hideaki Uchiyama, Shinichiro Haruyama, Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi","author_ids":"2025248, 1805951, 2438798, 1820301, 2175502","abstract":"We present spatially-multiplexed fiducial markers with the framework of code division multiple access (CDMA), which is a technique in the field of communications. Since CDMA based multi-plexing is robust to signal noise and interference, multiplexed markers can be demultiplexed under several image noises and transformation. With this framework, we explore the paradigm of multiple-input and multiple-output (MIMO) for fiducial markers so that the data capacity of markers can be improved and different users can receive different data from a multiplexed marker.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"000834db2001871d62c666993eef939b5ecdb77a","venue_1":"3DUI","year":"2015","title":"Freehand vs. micro gestures in the car: Driving performance and user experience","authors":"Renate Häuslschmid, Benjamin Menrad, Andreas Butz","author_ids":"1922701, 2521524, 8083190","abstract":"Until now, freehand and micro gestures have only been investigated separately. We conducted a driving simulator study to investigate on the effects on the driving performance when controlling a music player and the user experience provided. Subjects felt that stimulation, control, popularity, and physical-form were addressed by both gesture types, but slightly better by freehand gestures. But micro gestures were rated notably higher regarding their perceived degree of autonomy. Regarding driving performance, deteriorations were found for both gesture types. Results indicate, freehand gestures impair lateral control while micro gestures delay steering.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"4354bc3fd9ff29e9f694357847016dede57e7b78","venue_1":"3DUI","year":"2014","title":"Planning redirection techniques for optimal free walking experience using model predictive control","authors":"Thomas Nescher, Ying-Yin Huang, Andreas M. Kunz","author_ids":"2962928, 2011123, 2093125","abstract":"Redirected Walking (RDW) is a technique that allows exploring im-mersive virtual environments by real walking in a small physical room. RDW employs so-called redirection techniques (RETs) to manipulate the user's real world trajectory in such a way that he remains within the boundaries of the physical room. Different RETs were suggested and evaluated in the past. In addition, steering algorithms were proposed that apply a limited set of RETs to redirect a user away from the physical room's boundaries. Within this paper, a generalized approach to planning and applying RETs is presented. It is capable of dynamically selecting suitable RETs and also controlling parameters like their strengths. The problem of steering a user in a small physical room using RETs is formulated as an optimal control problem. This allows applying an efficient probabilistic planning algorithm to maximize the free walking experience. The proposed algorithm uses a map of the virtual environment to continuously determine the optimal RET that has to be applied next. The suggested algorithm is evaluated within a user study and compared to a state-of-the-art steering algorithm. Results show that for the given virtual environment, it is able to reduce the number of collisions with the room boundaries by 41% and furthermore reduces the amount of applied redirections significantly.","cites":"4","conferencePercentile":"67.39130435"},{"venue":"3DUI","id":"3f6ee9ae12f8c8c19d9572d5dc6fcdb3090df064","venue_1":"3DUI","year":"2016","title":"Interpreting 2D gesture annotations in 3D augmented reality","authors":"Benjamin Nuernberger, Kuo-Chin Lien, Tobias Höllerer, Matthew Turk","author_ids":"2089559, 2656227, 1743721, 1752714","abstract":"(a) Original drawing (b) Spray-paint (c) Median depth plane (d) Dominant plane (e) Gesture enhanced Figure 1: Alternative 3D interpretations (be) of the original 2D drawing (a) from different viewpoints. Previous methods (b-d) may not adequately convey the user's intention of referring to the printer compared to our gesture enhanced method (e). ABSTRACT A 2D gesture annotation provides a simple way to annotate the physical world in augmented reality for a range of applications such as remote collaboration. When rendered from novel viewpoints, these annotations have previously only worked with statically positioned cameras or planar scenes. However, if the camera moves and is observing an arbitrary environment, 2D gesture annotations can easily lose their meaning when shown from novel viewpoints due to perspective effects. In this paper, we present a new approach towards solving this problem by using a gesture enhanced annotation interpretation. By first classifying which type of gesture the user drew, we show that it is possible to render the 2D annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for an augmented reality enhanced remote collaboration scenario by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel real-time method to automatically handle the two most common 2D gesture annotations— arrows and circles—and give a detailed analysis of the ambiguities that must be handled in each case. Arrow gestures are interpreted by identifying their anchor points and using scene surface normals for better perspective rendering. For circle gestures, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our method outperforms previous approaches by better conveying the meaning of the original drawing from different viewpoints.","cites":"2","conferencePercentile":"100"},{"venue":"3DUI","id":"55136b5e014bef889c7edf14de77b312f691d55a","venue_1":"3DUI","year":"2014","title":"Poster: Investigating viewpoint visualizations for click & go navigation","authors":"Benjamin Nuernberger, Steffen Gauglitz, Tobias Höllerer, Matthew Turk","author_ids":"2089559, 1940938, 1743721, 1752714","abstract":"We present an investigation of viewpoint visualizations for \" Click & Go \" 3D navigation interfaces based on a pre-populated set of viewpoints. These scenarios often occur in 3D navigation systems that are based on sets of photos and possibly an underlying 3D reconstruction. Given these photos (and the 3D reconstruction), how does one most effectively navigate through this environment? Existing systems often employ Click & Go interfaces which allow users to navigate with one click of the mouse or tap of the finger. In this work, we investigate viewpoint visualizations for such Click & Go interfaces, describing a preliminary user study and providing valuable insights into Click & Go and its viewpoint visualizations.","cites":"2","conferencePercentile":"52.17391304"},{"venue":"3DUI","id":"55a2bc2e58776c4cec21eba2d507c8fad75983b7","venue_1":"3DUI","year":"2006","title":"Interactive Perspective Cut-away Views for General 3D Scenes","authors":"Christopher Coffin, Tobias Höllerer","author_ids":"3285106, 1743721","abstract":"We present a technique that allows a user to look beyond occluding objects in arbitrary 3D graphics scenes. In order to control this form of virtual x-ray vision, the user interactively cuts holes into the occluding geometry. The user can rapidly define a cutout shape or choose a standard shape and sweep it over the occluding wall segments to reveal what lies behind them. Holes are rendered in the correct 3D perspective as if they were actually cut into the obstructing geometry, including border regions that give the cutout shape physical depth, simulating penetration of a physical wall that possesses some generic thickness. 1 INTRODUCTION We present a new method for providing viewers of a 3D graphics scene with virtual x-ray vision — the ability to see through walls or other solid objects. The motivation for this work is derived from the need for effective methods for allowing participants in a virtual or augmented environment to gain accurate spatial knowledge about objects behind one or more layers of occlusion. Our goal is to provide a simple interaction technique which makes use of only the geometry information in any 3D scene. Our method should work in absence of any semantic information about the objects in the scene (but may take advantage of such information if it is available). Virtual X-ray vision easily suffers from confusing geometric relationships between occluders and revealed occluded infrastructure [7]. Our primary goal is to give the user a better understanding of the 3D scene by allowing the cuts themselves to impart information about the geometry through which the user is looking. To this end we define our points of interest and our cutout regions in 3D perspective on the actual surfaces of occluding objects. Through the apparent deformation of the cutout objects based on correct alignment, distance, and border geometry, we aim to give the user more information about the 3D scene. The ultimate goal of this technique is to make it appear to the user as if he was simply sweeping some cartoon-style hole of constant shape and area over the occluding geometry, which would deform to align with the objects being cut, and free up the view behind those objects. A cutout region should cling to the occluding geometry like a piece of cloth flattening to the respective surface geometry. 1.1 Related Work The idea of revealing layers of information behind or about certain …","cites":"26","conferencePercentile":"97.05882353"},{"venue":"3DUI","id":"1830aa95256163eae2b653c56829deae3409f853","venue_1":"3DUI","year":"2015","title":"Learning from rehabilitation: A bi-manual interface approach","authors":"Simon Hoermann, Jonny M. Collins, Holger Regenbrecht","author_ids":"3270680, 2708410, 1745309","abstract":"Figure 1: The user plays a virtual memory game where he tries to find matching pairs of 3D models on the left and the right side of the virtual board; he interacts by pointing with his hands at the tiles on each side; his hands are hidden from his direct view but video-captured by a camera and integrated into the virtual environment; an eye tracker is mounted on the lower part of the monitor's frame ABSTRACT Providing portable and affordable virtual reality systems for upper limb stroke rehabilitation is still a challenge. Here we present a simple user interface that allows the integration of various upper limb stroke exercises that can be autonomously performed by patients without the presence of a therapist, yet is portable and assembled using affordable off-the-shelf hardware components. In particular, the system integrates a bi-manual memory game where the user has to engage in meaningful therapeutic reaching exercises. We evaluated the user interface with a wide range of normal subjects and found that participants perceived the system as easy to use, they had no problems with the interaction and had an overall enjoyable experience with the system. This opens up possibilities of combining therapeutic reaching movements with goal-directed tasks to improve motivation and to enhance and increase rehabilitation outcomes for post-stroke patients. Beyond therapeutic use, our approach can also be applied to other 3D user interfaces for bi-manual interaction.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"15b8a88a98b80f9d4299e628aa96eadb620ff8b0","venue_1":"3DUI","year":"2009","title":"Tech-note: ScrutiCam: Camera manipulation technique for 3D objects inspection","authors":"Fabrice Decle, Martin Hachet, Pascal Guitton","author_ids":"2483137, 2281511, 2010548","abstract":"Figure 1: Camera movements provided by ScrutiCam. ABSTRACT Inspecting a 3D object is a common task in 3D applications. However , such a camera movement is not trivial and standard tools do not provide an efficient and unique tool for such a move. Scruti-Cam is a new 3D camera manipulation technique. It is based on the \" click-and-drag \" mouse move, where the user \" drags \" the point of interest on the screen to perform different camera movements such as zooming, panning and rotating around a model. ScrutiCam can stay aligned with the surface of the model in order to keep the area of interest visible. ScrutiCam is also based on the Point-Of-Interest (POI) approach, where the final camera position is specified by clicking on the screen. Contrary to other POI techniques, ScrutiCam allows the user to control the animation of the camera along the trajectory. It is also inspired by the \" Trackball \" technique, where the virtual camera moves along the bounding sphere of the model. However, ScrutiCam's camera stays close to the surface of the model, whatever its shape. It can be used with mice as well as with touch screens as it only needs a 2D input and a single button.","cites":"3","conferencePercentile":"54.16666667"},{"venue":"3DUI","id":"26f1e312cc6370ab2178b8ff90920505f1f04ed0","venue_1":"3DUI","year":"2008","title":"Poster: Effects of Head Tracking and Stereo on Non-Isomorphic 3D Rotation","authors":"Joseph J. LaViola, Andrew S. Forsberg, John Huffman, Andrew Bragdon","author_ids":"1726039, 1792958, 6107389, 1729297","abstract":"We present an experimental study that explores how head tracking and stereo affect user performance when rotating 3D virtual objects using isomorphic and non-isomorphic rotation techniques. Our experiment compares isomorphic with non-isomorphic rotation utilizing four different display modes (no head tracking/no stereo, head tracking/no stereo, no head tracking/stereo, and head track-ing/stereo) and two different angular error thresholds for task completion. Our results indicate that rotation error is significantly reduced when subjects perform the task using non-isomorphic 3D rotation with head tracking/stereo than with no head tracking/no stereo. In addition, subjects performed the rotation task with significantly less error with head tracking/stereo and no head track-ing/stereo than with no head tracking/no stereo, regardless of rotation technique. Subjects also highly rated the importance of stereo and non-isomorphic amplification in the 3D rotation task.","cites":"3","conferencePercentile":"41.66666667"},{"venue":"3DUI","id":"6caec67a2c4320d89aabda06be93617c4bfa7d41","venue_1":"3DUI","year":"2007","title":"Balloon Selection: A Multi-Finger Technique for Accurate Low-Fatigue 3D Selection","authors":"Hrvoje Benko, Steven K. Feiner","author_ids":"2704133, 1809403","abstract":"Balloon Selection is a 3D interaction technique that is modeled after the real world metaphor of manipulating a helium balloon attached to a string. Balloon Selection allows for precise 3D selection in the volume above a tabletop surface by using multiple fingers on a multi-touch–sensitive surface. The 3DOF selection tasks is decomposed in part into a 2DOF positioning task performed by one finger on the tabletop in an absolute 2D Cartesian coordinate system and a 1DOF positioning task performed by another finger on the tabletop in a relative 2D polar coordinate system. We have evaluated Balloon Selection in a formal user study that compared it to two well-known interaction techniques for selecting a static 3D target: a 3DOF tracked wand and keyboard cursor keys. We found that Balloon Selection was significantly faster than using cursor keys and had a significantly lower error rate than the wand. The lower error rate appeared to result from the user's hands being supported by the tabletop surface, resulting in significantly reduced hand tremor and arm fatigue. 1 INTRODUCTION In their survey paper, Hinckley et al. [23] identify many of the challenges associated with designing 3D interactions, dividing the design space into two broad categories: those that deal with human perception and those that deal with ergonomics. The authors suggest that spatial references (props or the user's own body), two-handed interaction, multi-sensory feedback, and physical constraints are invaluable for helping users to perceive and interact with a 3D object and environment. They also strongly recommend the reduction of extraneous degrees-of-freedom (DOF) to simplify the 3D task when possible. For example, in tasks requiring translation, but not rotation, rotation should be disabled. Furthermore , they suggest that providing a clear control metaphor (e.g., eyeball-in-hand [2, 9] camera or ray-casting) significantly improves the effectiveness of the interface and enhances the ability of the user to perceive the task at hand. In this paper, we present a novel interaction technique, called Balloon Selection (Figure 1), which follows these guidelines and allows for precise and accurate 3D selections in a constrained within-reach 3D environment. We are particularly interested in scenarios in which the user wishes to interact with a very small scale 3D environment, such as a model of the city, university campus, or an archaeological dig site [4, 25], seen from an outside in (exocentric) view. Such environments, also known as 3D maps or world-in-miniature (WIM) models [37], …","cites":"27","conferencePercentile":"93.33333333"},{"venue":"3DUI","id":"5ce18bcb93be61b600faa79cacb533b0c33c1ae5","venue_1":"3DUI","year":"2006","title":"Interaction Techniques for Exploring Historic Sites through Situated Media","authors":"Sinem Güven, Steven K. Feiner","author_ids":"3007804, 1809403","abstract":"We present a set of augmented reality and virtual reality interaction techniques that enable mobile users to visualize and interact virtually with representations of past events. These approaches use historic photographic imagery registered with real and virtual 3D objects to depict events in situ, and to provide interactive timelines. We demonstrate our techniques through examples developed for an important landmark, the Cathedral of St. John the Divine. 1 INTRODUCTION Augmented Reality (AR) [2, 7] enhances a user's perception of their surroundings by combining the physical world with a complementary virtual world. AR thus provides an especially effective way to present augmented real and virtual objects. Virtual Reality (VR) immerses the user in a 3D computer-generated world and allows them to navigate and interact with it. VR therefore offers the possibility of exploring objects and locations that may no longer exist, or may be too distant to reach conveniently. By overlaying a user interface on the world through AR or creating a self-contained world in VR, we can offer context-sensitive information to users. Being able to visualize information in its actual location and context is a powerful notion—the user does not need to perform extensive search, and their mere presence at a given location can provide the relevant information. To explore this concept further, we have developed interaction techniques that enable users to interact with and visualize representations of past events at historic sites. Our techniques rely on carefully registering and superimposing a set of images on the user's view of real or virtual objects, such as a 3D model of a historic site, as shown in Figure 1. These augmentations can then act as background references, provide additional detail not present in the original model, or depict scenes from important events that took place at the site. In this paper, we demonstrate these techniques using a database developed for an important landmark, the Cathedral of St. John the Divine. The Aspen Movie Map Project [13] provides a video-disk– based travel simulation through the streets of Aspen. To create its database, four cameras were mounted on a car at 90° angles to film a complete traversal of the city's streets. The scenes recorded by these cameras were later indexed and used to simulate vicarious travel down any street. The application also enabled users to visualize the buildings as they were years ago (using texture-mapped historic photographs), receive guided tours, see maps …","cites":"3","conferencePercentile":"17.64705882"},{"venue":"3DUI","id":"0b8ca9f57682c9e1747e9786a4097d55eb294acc","venue_1":"3DUI","year":"2014","title":"Interactive breadboard activity simulation (IBAS) for psychomotor skills education in electrical circuitry","authors":"Dhaval Parmar, Jeffrey W. Bertrand, Blair Shannon, Sabarish V. Babu, Kapil Chalil Madathil, Melissa Zelaya, Tianwei Wang, John Wagner, Kristin Frady, Anand K. Gramopadhye","author_ids":"3283595, 1770098, 2700996, 1778813, 3022772, 2857384, 3008041, 8054907, 3023674, 2652126","abstract":"We present an interactive breadboard activity simulation (IBAS) to educate users in acquiring psychomotor skills in electrical circuitry pertaining to the ammeter, voltmeter and multimeter instruments. Psychomotor skills learning involves the association of cognitive functions with motor functions to make the task autonomous with repeated practice, and promoting better recall. In electrical circuitry, this is demonstrated by the fine movements involved in handling and manipulating components on the electrical circuit, particularly while measuring electrical parameters that facilitates learning. Two display metaphors are currently implemented in IBAS, the FishtankVR (FVR) and head-mounted display (HMD) viewing conditions. In an initial user study, we found that users effectively learned the psychomotor skills pertaining to electrical circuitry using IBAS.","cites":"2","conferencePercentile":"52.17391304"},{"venue":"3DUI","id":"70eba6caa800eb5ba5eec87eaceae8c228e4f1b4","venue_1":"3DUI","year":"2013","title":"ARWand for an augmuented world builder","authors":"Taejin Ha, Woontack Woo","author_ids":"2373227, 1694303","abstract":"ARWand is a smartphone-based 3D user interface for manipulating 3D virtual objects in a wearable augmented reality (AR) environment without the use of a cumbersome external tracking system. In this paper, we apply ARWand to an augmented world builder that enables users to model the real world and add virtual content to it. First, the user, wearing a video, see-through, head-mounted display (HMO), selects an object of interest (001) in a physical space using AR Wand and assigns a local reference coordinate to the OoL This enables the program to construct a corresponding mirror space on the fly. Then, through control menus, ARWand lets the user load, select, and manipulate (3-00F translate, 3-00F rotate, scale, and texture) 3D virtual objects at various distances while the user moves within a wearable AR environment. We believe our proposed augmented world builder system using ARWand can help users easily construct augmented worlds anywhere and thus experience a more interesting and better informed reality. The proposed augmented world builder can be applied to various spaces such as galleries, museums, tour sites, or industrial locations and can create richly augmented reality ecosystems. 1 INTRODUCTION The augmented world combines real space and its mirror space in real time based on augmented reality (AR) technology and visualizes information on an object of interest (001) [1]. Augmented information helps users to better understand objects and situations and to have interesting experiences beyond space and time while preserving the original functions of real space. To do this, the augmented world pursues the following properties: real and mirror spaces connected to each other via an effective 3D link that includes context information, immersive augmented content that stimulates the users' five senses, and bi-directional interactions between real space and its mirror space in real time. Various AR authoring methods have been developed, but it is still very difficult for users to build an augmented world in a wearable AR environment. First, it is not easy to model a geometrical structure and generate/refine a local reference coordinate for an unknown real world (not previously modeled) on the fly in a situated space for normal users. It is also hard for users in motion to spatially annotate and manipulate virtual content at various distances in a wearable AR environment using conventional desktop user interfaces such as a conventional mouse or keyboard. In this paper, we propose an augmented world-building method using ARWand [2] …","cites":"0","conferencePercentile":"14"},{"venue":"3DUI","id":"b0ec6e3bcad148c4f606bf2e054e38ce70a50c21","venue_1":"3DUI","year":"2008","title":"Tangible User Interfaces Compensate for Low Spatial Cognition","authors":"John Quarles, Samsun Lampotang, Ira Fischler, Paul A. Fishwick, Benjamin Lok","author_ids":"1698407, 1705856, 1695136, 1734138, 7688188","abstract":"This research investigates how interacting with Tangible User Interfaces (TUIs) affects spatial cognition. To study the impact of TUIs, a between subjects study was conducted (n=60) in which students learned about the operation of an anesthesia machine. A TUI was compared to two other interfaces commonly used in anesthesia education: (1) a Graphical User Interface (a 2D abstract simulation model of an anesthesia machine) and (2) a Physical User Interface (a real world anesthesia machine). Overall, the TUI was found to significantly compensate for low user spatial cognition in the domain of anesthesia machine training. 1 INTRODUCTION Spatial cognition deals with how humans encode spatial information (i.e. about the position, orientation and movement of objects in the environment), and how this information is represented in memory and manipulated internally [5]. This research investigates how spatial cognition may be affected by Tangible User Interfaces (TUIs). In previous work, we showed that a TUI enabled students to more effectively learn how their physical interactions with an anesthesia machine controlled internal machine function [16]. These previous findings identified the anesthesia training application as a useful vehicle for exploring how spatial cognition may be facilitated by a TUI. To investigate the impact of TUI on spatial cognition, a between-subjects study was conducted with sixty participants. This study was part of a larger investigation into how mixed reality can enhance learning of complex concepts (i.e. anesthesia machine concepts). In the study presented in this paper, participants were trained in the operation of an anesthesia machine by training with one of three interfaces (figure 1) (20 participants in each condition): (1) Physical User Interface (PUI)-The Anesthesia Machine (AM) itself. (2) Graphical User Interface (GUI) The Virtual Anesthesia Machine (VAM) – a desktop-based 2D abstract simulation. (3) Tangible User Interface (TUI)-The Augmented Anesthesia Machine (AAM) – a 3D abstract simulation of an AM that employs: the physical AM as a TUI, a geometric model of the AM, and a 6DOF magic lens. Figure 1. A study was conducted to investigate how three interface types (GUI, PUI, TUI) affect spatial cognition differently. Note that the GUI and the PUI used in this study are widely used interfaces in anesthesia education. In comparing the GUI, the PUI, and the TUI, we explored how these three interfaces might facilitate spatial cognition differently. Through this comparison, we attempt to identify the spatial cognitive benefits unique to TUIs. Specifically, through this anesthesia …","cites":"9","conferencePercentile":"77.77777778"},{"venue":"3DUI","id":"ae526658907c91ffa81ce3e0e1634624183b78b8","venue_1":"3DUI","year":"2006","title":"Virtual Vouchers: Prototyping a Mobile Augmented Reality User Interface for Botanical Species Identification","authors":"Sean White, Steven K. Feiner, Jason K. Kopylec","author_ids":"5047258, 1809403, 2378815","abstract":"Figure 1: (a) Botanists gathering samples in the field. (b) View through a video see-though display of first prototype of the tangible augmented reality user interface. ABSTRACT As biodiversity research increases in importance and complexity, the tools that botanists require for fieldwork must evolve and take on new forms. Of particular importance is the ability to identify existing and new species in the field. Mobile augmented reality systems can make it possible to access, view, and inspect a large database of virtual species examples side-by-side with physical specimens. In this paper, we present prototypes of a mobile augmented reality electronic field guide and techniques for displaying and inspecting computer vision-based visual search results in the form of virtual vouchers. Our work addresses head-movement controlled augmented reality for hands-free interaction and tangible augmented reality. We describe results from our design and investigation process and discuss observations and feedback from lab trials by botanists. 1 INTRODUCTION As the understanding of bioinformatics and biodiversity grows, so must the tools that botanists require for fieldwork and research. Of particular importance is the ability to identify existing and new species in the field [27]. When botanists in the field need to identify a collected specimen or verify the existence of a new species, they initially consult their own personal knowledge and a paper field guide (Figure 1a). However, paper field guides are difficult to use, do not represent the full corpus of the specimen collection, and do not provide access to species samples (called vouchers). Thus, botanists must eventually borrow physical vouchers, with which to compare the specimens they collect, from museums and herbaria, such as the Smithsonian type specimen collection. Because of this, and the fact that vouchers are unique and fragile, the process of obtaining them is time-consuming. In short, botanical research is constrained by availability and access to necessary data. The opportunity exists to develop a new type of field guide that supports the work flow of a field botanist, combining immediate and holistic access to specimens in the form of virtual vouchers and providing even greater information than is found in the current physical voucher. We have been working with colleagues in computer science and botany to develop an electronic field guide (EFG) that addresses this opportunity and acts as a test-bed for exploring new user interface techniques for mobile augmented reality systems (MARS) in an iterative design process (Figure 1b). …","cites":"26","conferencePercentile":"97.05882353"},{"venue":"3DUI","id":"b62bd656824b434b28bf53493b6dc6fecf4d680a","venue_1":"3DUI","year":"2009","title":"Tech-note: Spatial interaction using depth camera for miniature AR","authors":"Kyungdahm Yun, Woontack Woo","author_ids":"2223409, 1694303","abstract":"Spatial Interaction (SPINT) is a non-contact passive interaction method that exploits a depth-sensing camera for monitoring the spaces around an augmented virtual object and interpreting their occupancy states as user input. The proposed method provides 3D hand interaction requiring no wearable device. The interaction schemes can be extended by combining virtual space sensors with different types of interpretation units. The depth perception anomaly caused by an incorrect occlusion between real and virtual objects is also alleviated for more precise interaction. The fluid interface will be used for a new exhibit platform, such as Miniature AR System (MINARS), to support a dynamic content manipulation by multiple users without severe tracking constraints.","cites":"1","conferencePercentile":"22.91666667"},{"venue":"3DUI","id":"590b2e9965084f65caf142c9714bfe4e4e310682","venue_1":"3DUI","year":"2007","title":"Pointman - A Device-Based Control for Realistic Tactical Movement","authors":"James N. Templeman, Linda E. Sibert, Robert C. Page, Patricia S. Denbrook","author_ids":"1745690, 2369335, 2628920, 1913409","abstract":"Pointman™ is a new virtual locomotion control that uses a conventional dual joystick gamepad in combination with a tracked head-mounted display and sliding foot pedals. Unlike the control mappings of a conventional gamepad, Pointman allows users to specify their direction of movement independently from the heading of the upper body. The motivation for this work is to develop a virtual infantry training simulator that is inexpensive, portable, and allows the user to execute realistic tactical infantry movements. Tactical movements rely heavily on the ability to scan while moving along a path, which requires the ability to independently coordinate course and heading. Conventional gamepad control mappings confound course and heading, and facilitate moving sideways and spiraling toward or away from targets. Pointman was derived from an analysis of how people move and coordinate actions in the real and virtual worlds. 1 INTRODUCTION Pointman™ is a new virtual locomotion control for moving the user's avatar through realistic 3D virtual simulations. (An avatar is the articulated representation of the user's body in the virtual world.) Pointman uses a dual joystick gamepad, common to console-based first-person shooter games, in a radically new way to allow users to turn their avatar's heading independently from the direction of motion. In addition, Pointman provides enhanced control over translation and viewing by using sliding foot pedals to mimic stepping and a 3 degree-of-freedom (DoF) tracked head-mounted display (HMD) to control the avatar's head pose and thus direct the user's view. Our work was motivated by the military's interest in using virtual simulators for training infantry teams. Our goal is to develop a compact, low cost infantry training simulator that gives users close to the same ability to move and coordinate actions as they have in the real world. We wanted Pointman to allow users to employ realistic military tactics, techniques, and procedures (TTPs) when training using virtual simulators. Few controls provide the ability to scan the user's view and rifle to cover danger areas without disrupting the user's motion along a path, yet the majority of tactical infantry movements depend on it. To develop Pointman, we examined the small unit infantry tactics employed in urban combat in general and building clearing operations in particular. Tactical infantry movement involves keeping the rifle directed where one is looking while scanning for threats. Scanning is used to cover and clear danger areas, especially areas exposed as one moves around corners. Figure …","cites":"2","conferencePercentile":"30"},{"venue":"3DUI","id":"07a9a234ac9c3cb7815a33ed9df2a04f2c9e0b81","venue_1":"3DUI","year":"2006","title":"Grab-and-Throw Metaphor: Adapting Desktop-based Interaction Paradigms to Virtual Reality","authors":"Frank Steinicke, Klaus H. Hinrichs","author_ids":"1740244, 1685162","abstract":"Figure 1: A user applies the grab-and-throw metaphor in a medical visualization application in a responsive workbench (RWB) environment. Abstract The drag-and-drop metaphor is one of the most common direct interaction metaphors used in desktop-based environments. This direct interaction paradigm enables an intuitive method to apply actions by associating iconic representation of objects to each other. Since dragging of these iconic representations is the most time-consuming subtask of the drag-and-drop metaphor, many extensions of this approach have been proposed to enhance this process. However, a transfer of these concepts to virtual reality (VR) systems has not been realized. In this paper we propose the grab-and-throw metaphor which is a VR-based analogon to the drag-and-drop metaphor. The proposed concepts enable users to select a virtual object by grabbing it and to throw the object within a virtual environment (VE) in the direction of another object. As soon as the object hits another object, an associated action is performed. The trajectory of the thrown object is based on physical motions adapted from the real-world. In order to ease hitting a desired target object, snapping strategies are used such that the aimed object attracts the thrown object. We give a technical description of the grab-and-throw metaphor and discuss example application scenarios which benefit from the usage of the described metaphor.","cites":"1","conferencePercentile":"5.882352941"},{"venue":"3DUI","id":"c4365388fab9df0607589b2544200677980dcad4","venue_1":"3DUI","year":"2009","title":"Poster: Interscopic multi-touch surfaces: Using bimanual interaction for intuitive manipulation of spatial data","authors":"Johannes Schöning, Frank Steinicke, Antonio Krüger, Klaus H. Hinrichs","author_ids":"2070910, 1740244, 1790548, 1685162","abstract":"In recent years visualization of and interaction with 3D data have become more and more popular and widespread due to the requirements of numerous application areas. Two-dimensional desktop systems are often limited in cases where natural and intuitive interfaces are desired. Sophisticated 3D user interfaces, as they are provided by virtual reality (VR) systems consisting of stereoscopic projection and tracked input devices, are rarely adopted by ordinary users or even by experts. Since most applications dealing with 3D data still use traditional 2D GUIs, current user interface designs lack adequate efficiency. Multi-touch interaction has received considerable attention in the last few years, in particular for non-immersive, natural 2D interaction. Interactive multi-touch surfaces even support three degrees of freedom in terms of 2D position on the surface and varying levels of pressure. Since multi-touch interfaces represent a good trade-off between intuitive, constrained interaction on a touch surface providing tangible feedback, and unrestricted natural interaction without any instrumentation, they have the potential to form the fundaments of the next generation 2D and 3D user interfaces. Indeed, stereoscopic display of 3D data provides an additional depth cue, but until now challenges and limitations for multi-touch interaction in this context have not been considered. In this paper we present new multi-touch paradigms that combine traditional 2D interaction performed in monoscopic mode with 3D interaction and stereoscopic projection, which we refer to as interscopic multi-touch surfaces (iMUTS).","cites":"2","conferencePercentile":"37.5"},{"venue":"3DUI","id":"cb11a0b9756e2d90bd7c410d8def779cfd82a7e5","venue_1":"3DUI","year":"2014","title":"Poster: Bimanual design of deformable objects thanks to the multi-tool visual metaphor","authors":"Morgan Le Chénéchal, Maud Marchal, Bruno Arnaldi","author_ids":"2500611, 1722424, 1809163","abstract":"Figure 1: Illustration with a mock-up edited figure of the bimanual design of deformable objects as proposed by our novel Multi-tool approach. The right hand manipulates the Multi-tool through a generic handle. The left hand allows to change the tip of the Multi-tool thanks to a panel selection on the left, as well as the selection of the virtual deformable objects. ABSTRACT In this paper, we introduce a novel visual interaction paradigm called the \" Multi-tool \" dedicated to the design of virtual deformable objects. Our approach proposes an interaction multi-tool metaphor that enables various types of interaction with deformable objects using a tool with a generic real handle and interchangeable virtual tips. Our bimanual control scheme allows a large amount of interaction possibilities as for real object design applications. Preliminary subjective evaluation by users shows that our approach can reproduce natural real hand gestures and the manipulation of many different tools in an intuitive way. The multi-tool interaction paradigm could thus be used in a wide range of applications involving the design or the shape modification of deformable objects such as for virtual prototyping, medical simulators or artistic projects.","cites":"0","conferencePercentile":"19.56521739"},{"venue":"3DUI","id":"05ba9f85d467725b8c9af4eba4a76f89a76fab1e","venue_1":"3DUI","year":"2009","title":"A multiscale interaction technique for large, high-resolution displays","authors":"Sarah Peck, Chris North, Doug A. Bowman","author_ids":"2256683, 1796013, 1729869","abstract":"This paper explores the link between users' physical navigation, specifically their distance from their current object(s) of focus, and their interaction scale. We define a new 3D interaction technique, called multiscale interaction, which links users' scale of perception and their scale of interaction. The technique exploits users' physical navigation in the 3D space in front of a large high-resolution display, using it to explicitly control scale of interaction, in addition to scale of perception. Other interaction techniques for large displays have not previously considered physical navigation to this degree. We identify the design space of the technique, which other researchers can continue to explore and build on, and evaluate one implementation of multiscale interaction to begin to quantify the benefits of the technique. We show evidence of a natural psychological link between scale of perception and scale of interaction and that exploiting it as an explicit control in the user interface can be beneficial to users in problem solving tasks. In addition, we show that designing against this philosophy can be detrimental. 1 INTRODUCTION The decreasing price of displays has enabled the exploration of ever-larger and higher-resolution displays. Previous research has quantified benefits from both the increased size and the increased resolution. Several studies have shown that with large datasets, such as those found in geospatial analysis, the larger viewport size improves users' performance time and decreases frustration [1,14]. A key benefit of large high-resolution displays is that they afford greater opportunity for physical navigation (moving one's body to navigate the displayed information). These studies found a correlation between faster user performance time, a decrease in virtual navigation and an increase in physical navigation. Evidence indicated that physical navigation was more efficient, effective, and preferred than virtual navigation. While increasing display size has significant benefits for user performance, it also creates a new difficulty – how do users interact with it? In particular, given that users physically navigate when using large display information spaces, how should that affect the design of interaction techniques? Stationary interaction devices, such as the traditional keyboard and mouse, can tether users and discourage physical navigation. Considering 3D input (such as tracking head, hand or body movement) for interaction with large 2D displays offers new possibilities for interface design that supports physical navigation [12]. However, when physically navigating, users move in and out from the display to zoom into details or out for an overview, essentially …","cites":"17","conferencePercentile":"91.66666667"},{"venue":"3DUI","id":"f696c87e4c1f2d87eb020a82c62bf3343cdc54f4","venue_1":"3DUI","year":"2013","title":"Poster: Real time hand pose recognition with depth sensors for mixed reality interfaces","authors":"Byungkyu Kang, Mathieu Rodrigue, Tobias Höllerer, Hwasup Lim","author_ids":"2428654, 2942059, 1743721, 1716138","abstract":"We present a method for predicting articulated hand poses in real-time with a single depth camera, such as the Kinect or Xtion Pro, for the purpose of interaction in a Mixed Reality environment and for studying the effects of realistic and non-realistic articulated hand models in a Mixed Reality simulator. We demonstrate that employing a randomized decision forest for hand recognition benefits real-time applications without the typical tracking pitfalls such as reini-tialization. This object recognition approach to predict hand poses results in relatively low computation, high prediction accuracy and sets the groundwork needed to utilize articulated hand movements for 3D tasks in Mixed Reality workspaces.","cites":"0","conferencePercentile":"14"},{"venue":"3DUI","id":"13a38245d3c8538a2b0e864588760baae7ea7aac","venue_1":"3DUI","year":"2011","title":"Neural network based motion segmentation for accelerometer applications","authors":"Jong Gwan Lim, Sang-Youn Kim, Dong-Soo Kwon","author_ids":"1842407, 1819790, 1750864","abstract":"Of several research issues related to motion interaction using inertia measurement units, faster motion segmentation without accuracy loss has recently been raised. Instead of using excessive filtering that produces time delay or tricky use of multiple thresholds that cause difficulty in parameter optimization, this poster demonstrates that time series prediction using neural networks significantly decreases time delay and guarantees rigid motion segmentation by detecting end points in accelerometer signals. According to a general pattern recognition procedure, feature selection is made by a filtering method and the optimal structure is determined by cross validation. Radial basis function networks and Multi-Layer Perceptrons (MLPs) are tested and the results are compared with the conventional methods to evaluate accuracy and time delay in a handwriting case in 3D space. This study confirms that MLP shows the best accuracy and shortens the time delay by 1/4~1/3 compared to the conventional methods. 1 INTRODUCTION Recently the Inertia Measurement Unit (IMU) used in the Inertia Navigation System (INS) in manned/unmanned aerial vehicles has been accepted widely in mobile HCI applications. These applications try to understand the context around the devices by detecting the natural user motions [1-5]. While conventional IMU applications based on trajectory estimation in 3D space by error prediction and compensation in double integration are at a standstill at the moment, approaches based on pattern recognition with respect to the raw acceleration signal itself attracting interests for motion detection due to their tolerance to noise [1]. Motion segmentation means discriminating significant motions from non-significant ones and it is notable that motion segmentation commonly lies at the heart of processing in both approaches. Since motion segmentation is undertaken by accelerometers in IMU, it is equivalent to End Point Detection (EPD) regarding accelerometer signals in terms of signal processing. In the research trends focusing on pattern recognition-based approaches, as a recognizer, Dynamic Time Warping (DTW) is mainly recommended and reportedly DTW records better performance in acceleration applications than the Hidden Markov Model (HMM) [2]. Compared to the HMM, the performance of DTW is so sensitive to EPD performance that the significance of acceleration EPD is again being emphasized. With the advent of real-time motion interaction games like Nintendo WII, a new point about faster processing has been raised, whereas most researches just aim at motion recognition accuracy. The problem is well described in [3] by Kim et al. who worked on decreasing the HMM computation time by …","cites":"0","conferencePercentile":"15.2173913"},{"venue":"3DUI","id":"1bfe66ffc5dfd5800ab9235971bea2e4e0bc5225","venue_1":"3DUI","year":"2009","title":"Virtual multi-tools for hand and tool-based interaction with life-size virtual human agents","authors":"Aaron Kotranza, Kyle Johnsen, Juan Cendan, Bayard Miller, D. Scott Lind, Benjamin Lok","author_ids":"2007211, 2664323, 1900861, 2648497, 2759190, 7688188","abstract":"A common approach when simulating face-to-face interpersonal scenarios with virtual humans is to afford users only verbal interaction while providing rich verbal and non-verbal interaction from the virtual human. This is due to the difficulty in providing robust recognition of user non-verbal behavior and interpretation of these behaviors within the context of the verbal interaction between user and virtual human. To afford robust hand and tool-based non-verbal interaction with life-sized virtual humans, we propose virtual multi-tools. A single hand-held, tracked interaction device acts as a surrogate for the virtual multi-tools: the user's hand, multiple tools, and other objects. By combining six degree-of-freedom, high update rate tracking with extra degrees of freedom provided by buttons and triggers, a commodity device, the Nintendo Wii Remote, provides the kinesthetic and haptic feedback necessary to provide a high-fidelity estimation of the natural, unencumbered interaction provided by one's hands and physical hand-held tools. These qualities allow virtual multi-tools to be a less error-prone interface to social and task-oriented non-verbal interaction with a life-sized virtual human. This paper discusses the implementation of virtual multi-tools for hand and tool-based interaction with life-sized virtual humans, and provides an initial evaluation of the usability of virtual multi-tools in the medical education scenario of conducting a neurological exam of a virtual human. 1 INTRODUCTION Virtual environments inhabited by virtual human (VH) agents have gained traction as an interpersonal skills training and therapy tool (e.g. doctor-patient interaction training, post-traumatic stress disorder and fear of public speaking therapy) [11][21][22]. However, simulating social scenarios requiring extensive verbal and nonverbal bi-directional communication with a VH agent is still a major challenge. While the expressive power of VHs is fast approaching that of real humans, natural and robust recognition and interpretation of complex user actions has been an elusive goal. This work presents a potential solution to this problem, the virtual multi-tool (VMT) technique. The VMT technique leverages a single, robustly-tracked hand-held interaction device to manipulate multiple virtual tools. This paper describes the application of the VMT technique to provide complex hand and hand-held tool based communication with VHs (Figure 1). In addition to enabling complex social and task-oriented interaction with VHs, the VMT technique may mitigate the derogatory effects of noisy-interfaces (e.g. error-prone speech recognition and understanding, which contributes to fractured conversations) typically employed in natural interaction with VHs. User satisfaction with these noisy, error-prone interfaces may be enhanced by providing a less error-prone …","cites":"5","conferencePercentile":"75"},{"venue":"3DUI","id":"4fe9542d395ff8f54b8cdc0a25b65bca915e8d6e","venue_1":"3DUI","year":"2015","title":"Comparing indirect and direct touch in a stereoscopic interaction task","authors":"Adalberto Lafcadio Simeone, Hans-Werner Gellersen","author_ids":"2785514, 4919595","abstract":"In this paper we studied the impact that the directedness of touch interaction has on a path following task performed on a stereoscopic display. The richness of direct touch interaction comes with the potential risk of occluding parts of the display area, in order to express one's interaction intent. In scenarios where attention to detail is of critical importance, such as browsing a 3D dataset or navigating a 3D environment, important details might be missed. We designed a user study in which participants were asked to move an object within a 3D environment while avoiding a set of static distractor objects. Participants used an indirect touch interaction technique on a tablet and a direct touch technique on the screen. Results of the study show that in the indirect touch condition, participants made 30% less collisions with the distractor objects.","cites":"4","conferencePercentile":"94.44444444"},{"venue":"3DUI","id":"ed5d37aa518fe99dcb55f70cdc57b4e926967339","venue_1":"3DUI","year":"2012","title":"3D Marking menu selection with freehand gestures","authors":"Gang Ren, Eamonn O'Neill","author_ids":"3728363, 1696153","abstract":"A controlled experiment was performed to evaluate the usability of freehand gestural target selection with different 3D marking menu layouts and target directions. We found that a rectangular layout was faster than an octagonal layout, with no significant increase in errors, and that our right-handed participants preferred to select to the right and forwards in a 3D marking menu. We propose an improved design for 3D marking menus based on our findings. The experimental results also suggest that designers should consider carefully whether or not findings from similar interaction techniques using hands-on devices can be carried over to the design of freehand gestural interaction. 1 INTRODUCTION Increasing use of 3D displays, e.g. in cinemas, TV and computer games, has come at the same time as increasing use of gestural input, the latter currently seeing its most widespread use in games. The combination of these two rapidly developing technologies offers the potential for much richer and more natural user interaction and immersive experiences. We are therefore motivated to investigate potential user interaction techniques to work with the combination of 3D display and gestural input. The use of 3D displays raises questions on the design of 3D interface elements. Menu item selection is a common task in almost all applications so we focus on it in this paper as an example of 3D user interaction techniques. 3D menus can potentially map well to their associated 3D environments, giving a more immersive 3D user experience. Although considerable research has been conducted on menu selection techniques in 3D environments, most have used 2D menus and there are no well established 3D menu selection techniques. Common approaches to tracking gestures include holding a motion sensing device (e.g. Wii remote), using fiducial markers that are tracked by a visual tracking system, or recognizing freehand movements using only a remote sensor (e.g. Microsoft Kinect) without on-body attachments or devices. As gestural interaction increases beyond the home gaming setting, freehand gestural interaction with displays that have no hands-on input device is likely to become even more important. But although the freehand gesture approach is often the most convenient because it does not require the user to hold or wear a device or to reach a touchscreen, it loses some benefits of such devices, such as the ability to take actions by clicking buttons or tapping surfaces. The increasingly common and relatively inexpensive freehand tracking devices such as Microsoft …","cites":"9","conferencePercentile":"90.32258065"},{"venue":"3DUI","id":"a1639395bbc7d9d7923741cf7fab9858b00bc270","venue_1":"3DUI","year":"2012","title":"Collaborative navigation in virtual search and rescue","authors":"Felipe Bacim, Eric D. Ragan, Cheryl Stinson, Siroberto Scerbo, Doug A. Bowman","author_ids":"1720157, 1777991, 1780989, 1795279, 1729869","abstract":"Disaster relief responders aim to quickly locate and extract survivors from dangerous environments. This research explores the use of a collaborative guidance system for search-and-rescue in a complex building. We implemented and evaluated a proof-of-concept system that allowed a scene commander and responder to efficiently search a building using visual, nonverbal communication. Participants found the interface to be both intuitive and fun, and results suggest that the collaborative navigation system was effective. 1 INTRODUCTION In designing a collaborative navigation task for the 2012 3DUI contest, we chose to focus on a real-world scenario of disaster relief. In building disasters (such as with fires and earthquakes), hazards and structural damages require constant changes to path planning and cause difficult navigation in 3D spaces. Our task specifically looks at communication between a scene commander and a disaster relief responder during a search and rescue operation. The responders inside the environment have great difficulty navigating because of hazards, reduced visibility, disorientation, and lack of survey knowledge of the environment. Observing the operation from outside of the disaster area, scene commanders work to help coordinate the response effort [1, 2]. With the responder's notifications about the environment, scene commanders can provide new instructions, alert the responders to risks, and issue evacuation orders. Since neither the commander nor the responder has complete information about the environment, effective communication is essential. As technology advances, the incorporation of new tools into search and rescue protocols shows promise for improving operation efficiency and safety. In this research, we explore the use of 3D user interfaces to assist collaborative search-and-rescue. Ideally, users should be able to focus on their primary tasks in the VE, rather than struggle with travel and wayfinding. Using virtual reality (VR) as a prototyping testbed, we implemented a proof-of-concept collaborative guidance system. Preliminary evaluation has demonstrated promising results for efficient rescue operations. 2 INTERFACE DESIGN In our system, a scene commander and disaster responder communicate to each other by simple marker placement and graphical notifications. The goal for the search-and-rescue exercise is safe and efficient search of a building during a disaster. The commander's task is to guide the responder through the building, while the responder finds survivors and points out environmental hazards and blockages to the commander. Thus, in this design, the guidance system has two main components: the commander view and the responder view. The commander view corresponds to a virtual 3D blueprint …","cites":"4","conferencePercentile":"67.74193548"},{"venue":"3DUI","id":"08a50160c2f9e8aa398dbaf12c5149f0a4f66f7e","venue_1":"3DUI","year":"2012","title":"Poster: Investigating one-eyed and stereo cursors for 3D pointing tasks","authors":"Robert J. Teather, Wolfgang Stuerzlinger","author_ids":"2343898, 3342964","abstract":"We compared two remote pointing techniques to two mouse pointing techniques using both with a stereo-and mono-rendered cursor. These were compared using a Fitts' law pointing experiment with varying target depths in a 3D scene. Results indicate that mouse-based techniques performed best and that the one-eyed cursor is beneficial only for some pointing techniques. 1 INTRODUCTION Stereo 3D cursors introduce issues of diplopia and cue conflicts, for example, if the cursor occludes geometry extending in front of its movement plane. Ware and Lowther's \" one-eyed cursor \" [9] is a mono-rendered cursor which eliminates these problems altogether. They report that the one-eyed cursor outperforms a stereo 3D cursor in 3D pointing tasks. Since graphics drivers now support stereo 3D in software that was originally non-stereo, issues of cursor rendering arise. These usually display a stereo-rendered non-perspective cursor using the disparity of the closest occluded surface. A similar idea is to use a sliding 3D cursor that always maintains contact with the background via mouse ray casting. It handles both diplopia and stereo conflicts, as the stereo cursor simply slides across it at the same depth. We investigate if the one-eyed cursor is beneficial for such a technique, and for remote pointing techniques. 2 RELATED WORK Ray-based pointing techniques work with both 2DOF devices, and 3/6DOF devices. There is still interest in these techniques in 3D user interface research [3, 6, 7]. A drawback of ray-based techniques is the relative difficulty in selecting remote objects [6]. Far objects take up proportionally less screen space due to perspective, but are also proportionally closer together. According to Fitts' law [2], pointing at screen-plane projections (object images) of same-depth targets should thus be unaffected by object depth. On the other hand, 6DOF ray control has higher angular precision up close, and closer objects can be treated as effectively larger than far objects [6]. Recent work [1, 5] investigated eye-and device-centric rays. Results of these studies are somewhat contradictory. One reports that device-centric rays perform better for 2D pointing tasks [5]. The other reports a new eye-centric ray technique outperforms traditional (device-centric) ray-casting [1]. We evaluate the difference between eye-centric and device-centric rays using both one-eyed and stereo cursors using Fitts' law [2], a model of the speed/accuracy tradeoff in pointing tasks. The model is MT = a+b×log 2 (D/W+1). MT is movement time, D is target distance, and W is target size, while a …","cites":"0","conferencePercentile":"8.064516129"},{"venue":"3DUI","id":"fdbae0d9f80432252e3ed0e86632bd3b8dde5c4e","venue_1":"3DUI","year":"2011","title":"Adaptive color marker for SAR environments","authors":"Ross T. Smith, Michael R. Marner, Bruce H. Thomas","author_ids":"1696911, 1742505, 1715550","abstract":"This paper explores how passive colored markers used for blob-tracking can be advanced to improve their operation in spatial augmented reality environments. We have created an adaptive marker that captures the color of environmental light, and then changes its own appearance in order to optimize tracking performance. We have provided implementation details and proof of concept for an active marker that supports interactive operations and visual feedback. We compared the performance of both the passive and adap-tive color marker in varying lighting conditions. Our results show a significant improvement in four scenarios where a passive marker is known to have poor tracking quality.","cites":"0","conferencePercentile":"15.2173913"},{"venue":"3DUI","id":"4d8e561b4853479e6e4bfb0775494ee909512cab","venue_1":"3DUI","year":"2012","title":"Poster: Evaluation of a 3D UI with different input technologies","authors":"Loutfouz Zaman, Dmitri Shuralyov, Robert J. Teather, Wolfgang Stuerzlinger","author_ids":"2065709, 2829617, 2343898, 3342964","abstract":"We present two studies of navigation and object manipulation in a virtual supermarket. The first study compared a mouse and keyboard setup to a game hardware setup using a Wii Remote, Wii Balance Board and a dancemat. The second study used more game-like software interfaces for both conditions and used only the Wii Remote and Nunchuk in the game-hardware setup. The mouse setup was around 36% faster in both studies. In the first study the mouse setup was 98% more accurate; no difference in accuracy was found in the second study. 1 INTRODUCTION A major obstacle preventing widespread use of 3DUIs remains hardware cost [2][10]. Recently, game consoles have included multiple degree-of-freedom input devices that can be used in 3D UIs. These are attractive alternatives to traditional tracking systems for are use in 3D UI research. An example is the award winning system by Bacim et al. [2] which employed six such devices: a Wii Remote (Wiimote), a Wii Sensor Bar, a Wii MotionPlus, a Wii Nunchuk, a Wii Balance Board, and a dancemat. This was used for navigation and item manipulation in a supermarket scene. LaViola [3] argues that such devices will remain popular for some time. Consequently, research on the usability of these devices within 3D UIs is fruitful [4][9][10]. On the other hand, these devices do not match the capabilities of specialized tracking hardware [2][10]. McArthur et al. [5] evaluated pointing with the Wii Remote in a Fitts' law pointing experiment, and found that it afforded pointing throughput of around 3 bits per second, with error rates of 5%. While this is significantly lower than mouse pointing throughput reported in the literature (typically 4–4.5 bits per second), it is still reasonable. We present an evaluation of a system similar to Bacim's [2] compared to a mouse-based 3D UI. Our second study evaluated an improved version of the system which used only the Wii Remote and Nunchuk. Our first user study compared a mouse-based 3D UI to the interface proposed by Bacim et al. [2], which uses the Wiimote, a Nunchuk, a Balance Board and a dancemat. Twelve paid participants (mean age 24.7, six females) were recruited. Eight were non-gamers and two were occasional gamers. The Wiimote was used for object selection/translation, and the MotionPlus gyro add-on rotated objects. The Nunchuk joystick rotated the view. The Balance Board and dancemat were used for navigation. Participants held the Nunchuk …","cites":"0","conferencePercentile":"8.064516129"},{"venue":"3DUI","id":"5d97142eb7cde678c01514c7f91816e5cad38235","venue_1":"3DUI","year":"2011","title":"Pointing at 3D targets in a stereo head-tracked virtual environment","authors":"Robert J. Teather, Wolfgang Stuerzlinger","author_ids":"2343898, 3342964","abstract":"We present three experiments that systematically examine pointing tasks in fish tank VR using the ISO 9241-9 standard. All experiments used a tracked stylus for a both direct touch and ray-based technique. Mouse-based techniques were also studied. Our goal was to investigate means of comparing 2D and 3D pointing techniques. The first experiment used a 2D task constrained to the display surface, allowing direct validation against other 2D studies. The second experiment used targets stereoscopically presented above and parallel to the display, i.e., the same task, but without tactile feedback afforded by the screen. The third experiment used targets varying in all three dimensions. Results of these studies suggest that the conventional 2D formulation of Fitts' law works well for planar pointing tasks even without tactile feedback, and with stereo display. Fully 3D motions using the ray and mouse based techniques are less well modeled. 1 INTRODUCTION Rapid aimed target pointing is a fundamental task in user interfaces. It is the basis of direct manipulation, and required for selecting objects and activating subsequent operations. The WIMP interface paradigm (Windows, Icons, Menus and Pointing device) popular in desktop computing exemplifies this, as many operations are accessible by pointing at interface widgets. Pointing tasks in 2D interfaces have received a great deal of attention from human-computer interaction researchers, and are well understood and modeled by Fitts' law [6, 14]. Three-dimensional pointing, or target selection, is less well understood. The pointing task itself is more complex, as the cursor is usually controlled by at least 3 degrees of freedom (DOF), i.e., the position in the x, y, and z directions. Some techniques (e.g., ray-casting) also require the control of the pointing device orientation, for up to 6 DOF's. In contrast, 2D pointing requires only control of 2 DOF's – position in the x and y directions. Several researchers have developed 3D extensions to Fitts' law by adding extra terms to the model, which (potentially artificially) improve its predictive capabilities. These attempts also use subtly different performance measures so it is hard to directly compare results across studies. This raises another question: Is 3D pointing kinematically different from 2D tasks? Or can technical issues, such as input device technology, explain the difference? We present three studies on 3D pointing in a fish tank virtual environment using a 3D version of the ISO 9241-9 task [9]. ISO 9241-9 describes a tapping task, based on Fitts' …","cites":"35","conferencePercentile":"100"},{"venue":"3DUI","id":"6f786d478b364692a5918ef69d462438901ec502","venue_1":"3DUI","year":"2011","title":"Quimo: A deformable material to support freeform modeling in spatial augmented reality environments","authors":"Ewald T. A. Maas, Michael R. Marner, Ross T. Smith, Bruce H. Thomas","author_ids":"3074062, 1742505, 1696911, 1715550","abstract":"This poster discusses a new free-form modeling material called Quimo (Quick Mock-up), designed for use in spatial augmented reality environments. Quimo is a white malleable material that can be sculpted and deformed with bare hands into an approximate model. The material is white in color, retains its shape once sculpted, and allows for later modification. Projecting imagery onto the surface of the low-fidelity mock-up allows for detailed prototype visualiza-tions to be presented.","cites":"5","conferencePercentile":"65.2173913"},{"venue":"3DUI","id":"aa9bc301440565caa2c7fdce207012a218f9bc6c","venue_1":"3DUI","year":"2010","title":"Augmented foam sculpting for capturing 3D models","authors":"Michael R. Marner, Bruce H. Thomas","author_ids":"1742505, 1715550","abstract":"This paper presents a new technique to simultaneously model in both the physical and virtual worlds. The intended application domain for this technique is industrial design. A designer physically sculpts a 3D model from foam using a hand-held hot wire foam cut-ter. Both the foam and cutting tool are tracked, allowing the system to digitally replicate the sculpting process to produce a matching 3D virtual model. Spatial Augmented Reality is used to project visualizations onto the foam. Inspired by the needs of industrial designers , we have developed two visualizations for sculpting specific models: Target, which shows where foam needs to be removed to produce a model, and Cut Animation, which projects the paths for cuts to be made to reproduce a previous artifact. A third visualiza-tion of the wireframe of the generated model is projected onto the foam and used for verification. The final visualization employs 3D procedural textures such as a wood grain texture, providing a simulation of volumetric rendering. Volumetric rendering techniques such as this provide a more natural look that is projected onto the foam. Once the object has been modeled physically and virtually, the designer is able to annotate and paint the finished model. The system has been evaluated through a user study conducted with students from the School of Industrial Design at the University of South Australia.","cites":"14","conferencePercentile":"94.73684211"},{"venue":"3DUI","id":"08a4c3be8b7b5361e285e85df80f6557a231fd86","venue_1":"3DUI","year":"2008","title":"Tech Note: Digital Foam","authors":"Ross T. Smith, Bruce H. Thomas, Wayne Piekarski","author_ids":"1696911, 1715550, 1825921","abstract":"This paper presents a new input device called Digital Foam designed to support natural sculpting operations similar to those used when sculpting clay. We have constructed two prototypes to test the concept of using a conductive foam input device to create 3D ge-ometries and perform sculpting operations. The novel contributions of this paper include the realization that conductive foam sensors are accurate enough to allow fine grained control of position sensing and can be used to build foam based input devices. We have designed a novel foam sensor array by combining both conductive and non-conductive foam to allow interference free sensor readings to be recorded. We also constructed two novel input devices, one flat input device with one hundred sensors, and a second spherical design with twenty one sensors, both allowing user interactions by touching or squeezing the foam surface. We present the design idea, foam sensor theory, two prototype designs, and the initial application ideas used to explore the possible uses of Digital Foam.","cites":"8","conferencePercentile":"72.22222222"},{"venue":"3DUI","id":"878232ec26b384e2e09dd3957789db91ef430323","venue_1":"3DUI","year":"2009","title":"Effects of tracking technology, latency, and spatial jitter on object movement","authors":"Robert J. Teather, Andriy Pavlovych, Wolfgang Stuerzlinger, I. Scott MacKenzie","author_ids":"2343898, 3163658, 3342964, 1692873","abstract":"We investigate the effects of input device latency and spatial jitter on 2D pointing tasks and 3D object movement tasks. First, we characterize jitter and latency in a 3D tracking device and an optical mouse used as a baseline comparison. We then present an experiment based on ISO 9241-9, which measures performance characteristics of pointing devices. We artificially introduce latency and jitter to the mouse and compared the results to the 3D tracker. Results indicate that latency has a much stronger effect on human performance than low amounts of spatial jitter. In a second study, we use a subset of conditions from the first to test latency and jitter on 3D object movement. The results indicate that large, uncharacterized jitter \" spikes \" significantly impact 3D performance. 1 INTRODUCTION Many virtual environments allow users to manipulate three-dimensional objects. These systems usually use a 3D input device supporting simultaneous manipulation of all 6 degrees-of-freedom (6DOF). These devices enable interaction schemes similar to real-world object manipulation, and may allow users to transfer real-world experience to VR. However, these devices have shortcomings. Compared to a mouse, 3D input devices have higher tracking noise and latency, and are subject to hand tremor if held in space. These factors degrade performance. Consequently, seemingly natural interaction schemes may not work as initially expected. Latency, or lag, is the delay in device position updates [10]. It has been previously demonstrated to significantly impact human performance in both 2D and 3D tasks, [13, 18, 21]. Spatial jitter, potentially due to both noise in the device signal and hand tremor, may also affect performance. These two factors often guide the choice of input device for a virtual environment. For high-precision tasks, designers may choose a device with low jitter, or smooth noisy input at the cost of introducing extra lag. However, since it is unclear which has a greater impact on performance, this trade-off should not be made lightly. We present two studies investigating the effects of latency and jitter on human performance with 3D input devices. The first employed Fitts' law, a well-established model of pointing device performance. Fitts' law is inherently 1-dimensional with strong 2D extensions, but it does not extend well to 3D movements. Consequently, we limited the first study to 2D pointing tasks using both a 3D tracker and mouse under a variety of lag/jitter conditions. We used the mouse as an exemplary low-latency, low-jitter …","cites":"38","conferencePercentile":"100"},{"venue":"3DUI","id":"3cdc6fabd014f4f1706920a737cfe29ed33997f0","venue_1":"3DUI","year":"2012","title":"Poster: AR-based social presence enhancement in video-chat communication","authors":"Igor de Souza Almeida, Marina Atsumi Oikawa, Jordi Polo Carres, Jun Miyazaki, Mark Billinghurst, Hirokazu Kato","author_ids":"2127741, 3311074, 2115743, 8147332, 1684805, 1758486","abstract":"Video-mediated communication systems attempt to provide users with a channel that could bring out the \" feeling \" of face-to-face communication. Among the many qualities these systems aim for, a high level of Social Presence is unquestionably a desirable one; however, little effort has been made to improve upon the user's perception of \" presence \". We propose an AR approach to enhance social presence for video-mediated systems by allowing one user to be present in the other user's video image. By using one extra camera on both sides, our system captures and merges the hand image from/to both sides resulting in an augmented experience where each user can \" reach \" to the other user's world. We conducted a preliminary pilot study with 10 participants coupled in 5 pairs in order to evaluate our system compared to the traditional video-chat setup. Tasks with varying equality of roles for the participants were performed. Results indicated that our system has higher degree of social presence compared to traditional video-chat systems. This conclusion was supported by the positive feedback from the subjects. 1 INTRODUCTION Video mediated communication systems are widely spread and easily accessible in their \" video-chat \" form, reason why it is one of the most popular communication channels being used between remotely located people. Traditional video-chat systems rely primarily on the two-way video/audio feedback in order to cover the cues for an efficient human-human communication. However, taking into account that face-to-face communication is the optimal communication case, there are a number of natural non-verbal communication cues which help one to convey the message, including gaze direction, proximity behavior and pointing in space [1][2]. Researchers have since explored new concepts to offer a more complete experience. The idea of Shared Space is one of them. Shared space draws similarities with Collaborative Virtual Environments (CVE) in which participants and information share a common display space [3]. Even though it has been proven that using a dedicated shared space environment improves the results of certain tasks [4][5], it does not address the effect of having an additional separated environment aside from the users' real world. In HyperMirror [6], a shared scene is created by capturing the front view of the users on one side and merging the images of all users in one video image. Although the resulting image displays all users as if they were side-by-side in front of a mirror, …","cites":"1","conferencePercentile":"20.96774194"},{"venue":"3DUI","id":"f3bf0672c5acf1e42d1f2377493900cca1a3a0f4","venue_1":"3DUI","year":"2008","title":"Assessing the Effects of Orientation and Device on (Constrained) 3D Movement Techniques","authors":"Robert J. Teather, Wolfgang Stuerzlinger","author_ids":"2343898, 3342964","abstract":"We present two studies to assess which physical factors influence 3D object movement tasks with various input devices. Since past research has shown that a mouse with suitable mapping techniques can serve as a good input device for some 3D object movement tasks, we also evaluate which characteristics of the mouse sustain its success. Our first study evaluates the effect of a supporting surface across orientation of input device movement and display orientation. A 3D tracking device was used in all conditions for consistency. The results of this study are inconclusive; no significant differences were found between the factors examined. The results of a second study show that the mouse outperforms the tracker for speed in all instances. The presence of support also improved accuracy when tracker movement is limited to 2D operation. A 3DOF movement mode performed worst overall. 1 INTRODUCTION Many research studies have targeted the development of intuitive 3D manipulation techniques for virtual environments. However, to this day, it is still far more difficult to perform simple tasks in a virtual reality (VR) setup compared to conceptually similar tasks in a desktop environment. Consider, for example, the relative ease of moving a desktop icon, and then compare this to the problem of moving an object in a 3D virtual environment. Most previous research focuses on creating better 3D manipulation techniques for use with 3D input devices such as trackers and wands, which allow the user to control up to 6 degrees of freedom (DOFs) simultaneously. However, the mouse often outperforms these devices for common tasks in many systems, although 3D devices seem better suited to the task. User familiarity may play a big factor here; most people use a mouse extensively in day-today computing and have very limited experience with 3D devices. Another factor is the dimensionality of the task. It is more difficult to accurately position an object in 3D space than in 2D space, mainly due to the additional degree(s) of freedom in which the object can move. Another factor is that the mouse requires a supporting surface on which to operate. This supporting surface reduces fatigue and hand jitter of the user, providing an advantage over the \" free-floating \" movement associated with most 6DOF devices. On the other hand, this is also a disadvantage for the mouse, as it is then unsuitable for virtual environments that require full 6DOF movement or for VR …","cites":"22","conferencePercentile":"88.88888889"},{"venue":"3DUI","id":"82272d63e66574977eb88e56c08413cd469f1710","venue_1":"3DUI","year":"2008","title":"Poster: Evaluation of an Approach for Remote Object Manipulation Utilizing Dynamic Magnifying Lenses","authors":"Anuraag Agrawal, Kiyoshi Kiyokawa, Haruo Takemura","author_ids":"2575277, 1680862, 1748743","abstract":"In this poster, we present a novel approach for manipulating remote objects in an immersive environment using dynamic magnifying lenses. These lenses are created seamlessly as the user focuses his gaze on an object, expanding the target object. The expanded object can then be manipulated as if the user was standing next to it. To determine its effectiveness, this interface was evaluated against three other well known interaction techniques-direct manipulation, Go-Go hand, and HOMER. Testing showed that the lens interface was superior in accuracy to the others, but it took more time to complete a given task.","cites":"0","conferencePercentile":"13.88888889"},{"venue":"3DUI","id":"1c22d5f8a0f4982c535e0faf20f3ae37573a2f6c","venue_1":"3DUI","year":"2012","title":"Poster: Manipulation techniques of 3D objects represented as multi-viewpoint images in a 3D scene","authors":"Juan Carlos Yu, Goshiro Yamamoto, Jun Miyazaki, Mark Billinghurst, Hirokazu Kato","author_ids":"3282323, 2215616, 8147332, 1684805, 1758486","abstract":"In this poster, we explore manipulation of an object represented by an image-based rendering approach in a 3D scene. We focus on two manipulation techniques that address the problems with using an image-based rendering approach and the constraints imposed by implementing such a system on a mobile device. We present results from our preliminary experiments.","cites":"1","conferencePercentile":"20.96774194"},{"venue":"3DUI","id":"f56b456cce64627c4dcce910885dcbec4a16ecd9","venue_1":"3DUI","year":"2015","title":"Finger-based manipulation in immersive spaces and the real world","authors":"Emmanuelle Chapoulie, Theophanis Tsandilas, Lora Oehlberg, Wendy E. Mackay, George Drettakis","author_ids":"2650457, 2110778, 1760608, 1732917, 1721779","abstract":"Inria Figure 1: A user in our immersive environment (left). Completing a 6 DoF manipulation task in real (center) and virtual (right) settings. ABSTRACT Immersive environments that approximate natural interaction with physical 3D objects are designed to increase the user's sense of presence and improve performance by allowing users to transfer existing skills and expertise from real to virtual environments. However , limitations of current Virtual Reality technologies, e.g., low-fidelity real-time physics simulations and tracking problems, make it difficult to ascertain the full potential of finger-based 3D manipulation techniques. This paper decomposes 3D object manipulation into the component movements, taking into account both physical constraints and mechanics. We fabricate five physical devices that simulate these movements in a measurable way under experimental conditions. We then implement the devices in an immersive environment and conduct an experiment to evaluate direct finger-based against ray-based object manipulation. The key contribution of this work is the careful design and creation of physical and virtual devices to study physics-based 3D object manipulation in a rigorous manner in both real and virtual setups.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"2a110f84f80b78802bed828793d1ee971ebdda17","venue_1":"3DUI","year":"2012","title":"Poster: Physically-based natural hand and tangible AR interaction for face-to-face collaboration on a tabletop","authors":"Thammathip Piumsomboon, Adrian J. Clark, Atsushi Umakatsu, Mark Billinghurst","author_ids":"2297177, 1749024, 2353749, 1684805","abstract":"In this paper, we present an AR framework that allows natural hand and tangible AR interaction for physically-based interaction and environment awareness to support face-to-face collaboration using Microsoft Kinect. Our framework comprises of six major components: (1) marker tracking (2) depth acquisition (3) image processing (4) physics simulation (5) communication and (6) rendering. The resulting augmented environment supports occlusion, shadows, and physically-based interaction of real and virtual objects. We propose three methods of natural hand representations including mesh-based, direct sphere substitution and variational optical flow. A tabletop racing game and AR Sandbox applications are created based on this framework, showing the application possibilities. 1 INTRODUCTION By overlaying virtual information into the real world, Augmented Reality (AR) [1] allows multiple users to share and collaborate on virtual tasks while carrying out natural face-to-face communication [2]. This overlap of task and communication spaces makes AR ideally suited for collocated collaboration. However there is a need to explore new three-dimensional (3D) interaction methods suitable for this environment. In the past, researchers have explored a variety of techniques for AR interaction and some of the most popular methods are based on the Tangible AR interface metaphor [3] where real objects are used to manipulate virtual content. This can be intuitive because it utilizes the users' natural ability of interacting with the physical world. However there are some limitations with the Tangible AR approach, such as the need for physical input devices. In this paper, we present an AR framework that supports face-to-face collaboration allowing people to use their natural hands to interact with virtual content such as a user can grasp a virtual object and pass it on to another user as they normally do with real objects. The system also aware of its environment so as to provide correct visual cues for each user and the virtual content behave realistically in the physical environment that it is placed in. Our framework uses a Microsoft Kinect, a single image-based marker, and AR viewing cameras. The Kinect provides both RGB and depth-sensing cameras, is used to create a 3D task space above a tabletop. When the transformation between the Kinect and the AR viewing cameras is known, virtual content can be realistically composited in the environment. We cover our framework in section 2. The performance is discussed in Section 3 and the conclusion and future works are described in section 4. 2 PROPOSED AR FRAMEWORK …","cites":"3","conferencePercentile":"48.38709677"},{"venue":"3DUI","id":"3d584e05add932ff559589da186798010302ec93","venue_1":"3DUI","year":"2009","title":"Poster: A virtual walkthrough system with a wide field-of-view stereo head mounted projective display","authors":"Natsuki Takeda, Kiyoshi Kiyokawa, Haruo Takemura","author_ids":"1817602, 1680862, 1748743","abstract":"A virtual walkthrough system is built to demonstrate the unique capability of our wide field-of-view (FOV) head mounted projective display, which can theoretically achieve more than 180 degrees of horizontal FOV by utilizing a half-silvered hyperbolic curved mirror. A fast rendering algorithm developed for image distortion correction and performance analysis on the rendering speed and precision of distortion correction are described. The developed virtual walkthrough system has successfully provided the observer the sense of presence at an interactive frame rate.","cites":"0","conferencePercentile":"10.41666667"},{"venue":"3DUI","id":"949d779fbcafe1e0456cbbdd6c847153962108c9","venue_1":"3DUI","year":"2006","title":"Turning Pages of 3D Electronic Books","authors":"Lichan Hong, Stuart K. Card, Jindong Chen","author_ids":"2217278, 1720492, 7557787","abstract":"Taking the form of physical books, virtual 3D books can be used as basic components of e-book systems, information workspaces, and digital libraries. This paper describes the page turning design of 3Book, a 3D book system that we recently developed. Our design aims to find a sensible balance among important factors such as visual realism, readability, interactivity, and scalability. To convey the impression of reading or viewing an actual physical book, we model all the faces of the book and synchronize the movements of various portions of the book during page turning. Our design delivers a seamless transition between two states of the book (i.e., when it is lying still and when it is turning pages). In addition, we deform the turning pages around an imaginary cone of changing sizes to produce realistically-looking curved pages. 1 INTRODUCTION In the past few years, several projects were initiated to digitize large amounts of books and documents. For example, the Million Book Project of Carnegie Mellon University aims to scan one million seminal books and make them freely available to the public [18]. Amazon.com has a web-based feature called \" Search Inside the Book \" , which allows a customer to search millions of pages to find the book that she wants to purchase [1]. Recently, Google, Yahoo!, and Microsoft announced separate plans to digitize millions of printed books and make them searchable over the Web. In order to capture the look and feel of a physical book in its electronic correspondent, the British Library has developed an interactive system called \" Turning the Pages \" [3]. Using animations, visitors can virtually turn pages of precious books or manuscripts, including the Lindisfarne Gospels and the Sherborne Missal. Additionally, Zinio Reader [23] and FlipViewer [14] are two commercial products displaying electronic pages in a 3D form simulating the appearance of a real book. Although the value of 3D vs. 2D is still under considerable debates [8,9], all these efforts suggest the interest in adopting a 3D book metaphor. As a visually enhanced object [5,p463], a book metaphor taps into the user's familiarity with physical books. From the user's perspective, whether a book is a physical book or a virtual book being shown on a computer screen, \" turn to the picture in the middle of page 124 \" has the same meaning. Another usage of virtual 3D books is as design elements in 3D …","cites":"5","conferencePercentile":"26.47058824"},{"venue":"3DUI","id":"a9cc990718b769ce003a788b5eb542bee4cf44fa","venue_1":"3DUI","year":"2009","title":"Wayfinding techniques for multiScale virtual environments","authors":"Felipe Bacim, Doug A. Bowman, Márcio Serolli Pinho","author_ids":"1720157, 1729869, 2371715","abstract":"Wayfinding in multiscale virtual environments can be rather complex , as users can and sometimes have to change their scale to access the entire environment. Hence, this work focuses on the understanding and classification of information needed for travel, as well as on the design of navigation techniques that provide this information. To this end, we first identified two kinds of information necessary for traveling effectively in this kind of environment: hierarchical information, based on the hierarchical structure formed by the levels of scale; and spatial information, related to orientation, distance between objects in different levels of scale and spatial lo-calization. Based on this, we designed and implemented one technique for each kind of information. The developed techniques were evaluated and compared to a baseline set of travel and wayfinding aid techniques for traveling through multiple scales. Results show that the developed techniques perform better and provide a better solution for both travel and wayfinding aid.","cites":"4","conferencePercentile":"66.66666667"},{"venue":"3DUI","id":"bd6433cecf8c2876922feaf4e4e6b320d98bf764","venue_1":"3DUI","year":"2008","title":"Tech-note: Dynamic Dragging for Input of 3D Trajectories","authors":"Daniel F. Keefe, Robert C. Zeleznik, David H. Laidlaw","author_ids":"1776441, 1713625, 2687218","abstract":"We present Dynamic Dragging, a virtual reality (VR) technique for input of smooth 3D trajectories with varying curvature. Users \" drag \" a virtual pen behind a hand-held tracked stylus to sweep out curving 3D paths in the air. Previous explorations of dragging-style input have established its utility for producing controlled, smooth inputs relative to freehand alternatives. However, a limitation of previous techniques is the reliance on a fixed-length drag line, biasing input toward trajectories of a particular curvature range. Dynamic Dragging explores the design space of techniques utilizing an adaptive drag line that adjusts length dynamically based on the local properties of the input, such as curvature and drawing speed. Finding the right mapping from these local properties to drag line length proves to be critical and challenging. Three potential map-pings have been explored, and results of informal evaluations are reported. Initial findings indicate that Dynamic Dragging makes input of many styles of 3D curves easier than traditional drag-style input, allowing drag techniques to approach the flexibility for varied input of more sophisticated and much harder to learn techniques, such as two-handed tape drawing.","cites":"7","conferencePercentile":"66.66666667"},{"venue":"3DUI","id":"f9443a6ec4811c2920442e9d427df67680ae7242","venue_1":"3DUI","year":"2011","title":"Rapid and accurate 3D selection by progressive refinement","authors":"Regis Kopper, Felipe Bacim, Doug A. Bowman","author_ids":"1778772, 1720157, 1729869","abstract":"Issues such as hand and tracker jitter negatively affect user performance with the ray-casting selection technique in 3D environments. This makes it difficult for users to perform tasks that require them to select objects that have a small visible area, since small targets require high levels of precision. We introduce an approach to address this issue that uses progressive refinement of the set of se-lectable objects to reduce the required precision of the task. We present a design space of progressive refinement techniques and an exemplar technique called Sphere-casting refined by QUAD-menu (SQUAD). We explore the tradeoffs between progressive refinement and immediate selection techniques in an evaluation comparing SQUAD to ray-casting. Both an analytical evaluation based on a distal pointing model and an empirical evaluation demonstrate that progressive refinement selection can be better than immediate selection. SQUAD was much more accurate than ray-casting, and SQUAD was faster than ray-casting with small targets and less cluttered environments.","cites":"20","conferencePercentile":"95.65217391"},{"venue":"3DUI","id":"3061f930236eb64a3894c923e93b3a7bf09f062d","venue_1":"3DUI","year":"2014","title":"Poster: Designing effective travel techniques with bare-hand interaction","authors":"Mahdi Nabiyouni, Bireswar Laha, Doug A. Bowman","author_ids":"1804979, 1702809, 1729869","abstract":"Emerging novel 3D interaction technologies allow precise tracking of bare hands and fingers, but due to the differences between these devices and traditional trackers, it is not clear how to design effective interaction techniques using these technologies. Using the Leap Motion Controller, we designed travel techniques with bare-hand interaction. We prototyped both unimanual and bimanual techniques using various metaphors (e.g., airplane and camera-in-hand), control mappings (position-vs. rate-control), camera movements (scene vs. camera dependent) and methods for speed control. Based on our experiences with these prototypes, we discuss the challenges and design issues for bare-hand interaction techniques. We present the results of a user study comparing the usability of five representative techniques for three travel tasks: absolute travel, naïve search and path following. We found that the limited workspace of the Leap caused movements with the camera-in-hand metaphor to be faster and less accurate, making it more effective for search but less effective for path following tasks. In addition, the Leap's ability to precisely track small finger movements benefited the usability of continuous speed control techniques.","cites":"6","conferencePercentile":"84.7826087"},{"venue":"3DUI","id":"52e15d3525e9366c799e322ed3486317d2c31169","venue_1":"3DUI","year":"2013","title":"Poster: Spatial Augmented Reality user interface techniques for room size modeling tasks","authors":"Michael R. Marner, Bruce H. Thomas","author_ids":"1742505, 1715550","abstract":"This poster presents results of our investigations into using Spatial Augmented Reality (SAR) to improve kitchen design and other interior architecture tasks. We present new user interface techniques for room sized modeling tasks, including cabinet layout, viewing and modifying preset designs, and modifying finishes. These tools and techniques address key user interface issues for spatial augmented reality systems, and we discuss how they can be generalized for other applications. The techniques have been developed in the context of a demonstration application, PimpMyKitchen, which allows architects to design kitchens, working with clients in an interactive SAR environment.","cites":"2","conferencePercentile":"56"},{"venue":"3DUI","id":"ed1d208c300a4a5018e65d81012d217f8087a4ce","venue_1":"3DUI","year":"2015","title":"Mapping 2D input to 3D immersive spatial augmented reality","authors":"Michael R. Marner, Ross T. Smith, Bruce H. Thomas","author_ids":"1742505, 1696911, 1715550","abstract":"This poster presents Viewpoint Cursor, a technique for mapping 2D user input from devices such as mobile phones, trackballs, or computer mice, to 3D multi-projector spatial augmented reality systems. While the ubiquity of input devices such as these make them obvious choices for spatial augmented reality, their 2D nature makes them difficult to use. Existing VR techniques rely on a display in front of the user's eyes on which to place virtual information. Immersive spatial augmented reality systems allow users to experience and interact with projected virtual information from any angle, using arbitrary placement of projectors. Viewpoint Cursor addresses these issues by mapping 2D input to a plane in front of the user's view. Ray casting is then used to find the 3D location for the cursor in the scene, which is then projected using the projection system. The user's position is tracked, with the input remapped accordingly , resulting in 2D input that matches what the user expects, regardless of their location.","cites":"0","conferencePercentile":"27.77777778"},{"venue":"3DUI","id":"c20190399855d8208eaf2a968fcb22f4bcf67f95","venue_1":"3DUI","year":"2010","title":"Contact sensing and interaction techniques for a distributed, multimodal floor display","authors":"Yon Visell, Severin Smith, Alvin Law, Rishi Rajalingham, Jeremy R. Cooperstock","author_ids":"3322706, 2233139, 7701722, 2533298, 2242019","abstract":"This paper presents a novel interface and set of techniques enabling users to interact via the feet with augmented floor surfaces. The interface consists of an array of instrumented floor tiles distributed over an area of several square meters. Intrinsic force sensing is used to capture foot-floor contact at resolutions as fine as 1 cm, for use with floor-based multimodal touch surface interfaces. We present the results of a preliminary evaluation of the usability of such a display.","cites":"6","conferencePercentile":"65.78947368"},{"venue":"3DUI","id":"234918b8055bfa759d6885eeb0df802f35cf6836","venue_1":"3DUI","year":"2013","title":"CaveCAD: Architectural design in the CAVE","authors":"Cathleen E. Hughes, Lelin Zhang, Jürgen P. Schulze, Eve Edelstein, Eduardo Macagno","author_ids":"2879084, 3059485, 1700671, 6463880, 2570913","abstract":"CaveCAD is our in-house developed 3D modeling tool, which runs in immersive virtual reality environments, such as CAVEs. We built it from the ground up, in collaboration with architects, to explore how immersive 3D interaction systems can support 3D modeling tasks. CaveCAD offers typical 3D modeling functions, such as geometry creation, modification of existing geometry, assignment of surface materials and textures, the use of libraries of 3D components , geographical placement functions, and shadows. CaveCAD goes beyond traditional 3D modeling tools by utilizing direct 3D interaction methods. We evaluated our modeling system by running a small pilot study with four participants: two novice users and two expert users were tasked to build Disney World's magic castle.","cites":"3","conferencePercentile":"68"},{"venue":"3DUI","id":"84c8f5996cf5adb70803e59617fb6d842493a31f","venue_1":"3DUI","year":"2008","title":"Elastic Control for Navigation Tasks on Pen-based Handheld Computers","authors":"Martin Hachet, Alexander Kulik","author_ids":"2281511, 1712907","abstract":"Isotonic pen and finger interfaces for handheld devices are very suitable for many interaction tasks (eg. pointing, drawing). However , they are not appropriate for rate controlled techniques, as required for other tasks such as navigation in 3D environments. In this paper, we investigate the influence of elastic feedback to enhance user performance in rate controlled interaction tasks. We conducted an experiment, which proves evidence that elastic feedback, given to input movements with the pen, provides better control for 3D travel tasks. Based on these findings, we designed several prototypes that illustrate the ease of applying various elastic control conditions to contemporary handheld computers with finger-or pen-based input capabilities.","cites":"5","conferencePercentile":"55.55555556"},{"venue":"3DUI","id":"9330c1a2a804c1df34e0b7ae0544a6450d34af5b","venue_1":"3DUI","year":"2008","title":"Poster: The NetEyes Collaborative, Augmented Reality, Digital Paper System","authors":"David McGee, Xiao Huang, Paulo Barthelmess, Philip R. Cohen","author_ids":"2565317, 5540503, 1961755, 2305444","abstract":"NetEyes is a system that allows remote and co-located partners to collaborate by annotating maps printed on digital paper..It combines natural language capabilities, in particular the interpretation of sketched symbols, with the display of three-dimensional representations of recognized objects, allowing users to jointly visualize a planned or evolving situation in detail. Visualization can take place either on a conventional monitor or through optical see-though, head-mounted displays. Seamless collaboration is promoted via the use of tangible paper maps, which makes it possible for multiple parties sitting around a table to place annotations as they would using regular paper and pen. To account for movements and rotation of the maps that are common during collaborative annotation sessions, a vision-based tracking component is used. This component recovers the location of the paper map in the real-world, and scales and rotates the digitally displayed objects so that they keep aligned with the map. 1 INTRODUCTION In a wide variety of domains, sketching over tangible paper artifacts, such as maps, floor plans and other schematics remains the most convenient collaboration practice. Paper is lightweight , robust (works even if punctured or thorn), does not require a SRZHU VRXUFH LV DOZD\\V ³RQ´LV KLJK-definition and come in a variety of sizes. That makes it ideal both in situations in which a group of co-located people wants to collaborate, as well as in situation in which mobility is important. In this work we explore a combination of techniques that aims at supporting this type of practice in a seamless way, matching the convenience of paper with the advantages derived from the use of computers, such as for instance rich 3D visualizations and support for collaboration across remote sites. A 3D scene is constructed iteratively as user sketched symbols are interpreted via natural language processing techniques. Both the digital ink and the 3D representations are propagated and displayed across remote sites to promote collaboration. A variety of different techniques have been explored in the past to bridge paper and digital technologies, starting with :HOOQHU ¶V 'LJLWDO'HVN >6]. Augmentation of planning exercises based on digital paper were explored by our Rasa and NISMap systems [4]. Similarly to Rasa, NetEyes interprets sketched symbols written on paper. Paper-augmentation techniques are usually based on the projection of images of digital objects over some surface that may contain paper documents. The technique we explore is similarly based on the composition of digital elements that …","cites":"0","conferencePercentile":"13.88888889"},{"venue":"3DUI","id":"d8c35157a55793c8ddd60c84b815aa59551de9be","venue_1":"3DUI","year":"2012","title":"A generalized God-object method for plausible finger-based interactions in virtual environments","authors":"Jan Jacobs, Michael Stengel, Bernd Fröhlich","author_ids":"2914219, 2267017, 5399927","abstract":"Figure 1: The God hand: The fingers of the real hand may penetrate an object (left) while the representation of the virtual hand remains in a plausible position outside of the object. ABSTRACT We generalize the six degree-of-freedom God-object approach to enable its use for multi-finger interactions in virtual environments. The connected finger phalanxes are modeled as multiple constrained God objects. The mutual interdependencies between multiple God objects are resolved using Gauss' principle of least constraint. This generalization of the God-object method allows us to avoid the penetration of multiple fingers and their phalanxes with objects within a physically simulated virtual world. Our observations indicate that the generalized God-object approach leads to plausible collision-free positions and motions of the phalanxes of the user's fingers during complex six degree-of-freedom manipulations, while artifacts such as artificial friction or a stuck hand are avoided.","cites":"14","conferencePercentile":"100"},{"venue":"3DUI","id":"b36324e6478586892d34b4585566c0d756de7e65","venue_1":"3DUI","year":"2010","title":"Improving co-located collaboration with show-through techniques","authors":"Ferran Argelaguet, André Kunert, Alexander Kulik, Bernd Fröhlich","author_ids":"1854224, 2843613, 1712907, 5399927","abstract":"Figure 1: The first two images (a, b) illustrate the issue of interpersonal occlusion between two tracked users in a multiuser VR system: an object that is fully visible to one user (a) can not or only partially be seen from other viewpoints (b). Show-through techniques can improve target discovery in such situations by showing the indicated object through the occluding environment (c). ABSTRACT Multiuser virtual reality systems enable natural interaction with shared virtual worlds. Users can talk to each other, gesture and point into the virtual scenery as if it were real. As in reality, referring to objects by pointing, results often in a situation whereon objects are occluded from the other users' viewpoints. While in reality this problem can only be solved by adapting the viewing position, specialized individual views of the shared virtual scene enable various other solutions. As one such solution we propose show-through techniques to make sure that the objects one is pointing to can be seen by others. We analyzed the influence of such augmented viewing techniques on the spatial understanding of the scene, the rapidity of mutual information exchange as well as the social behavior of users. The results of our user study revealed that show-through techniques support spatial understanding on a similar level as walking around to achieve a non-occluded view of specified objects. However, advantages in terms of comfort, user acceptance and compliance to social protocols could be shown, which suggest that virtual reality techniques can in fact be better than 3D reality.","cites":"2","conferencePercentile":"21.05263158"},{"venue":"3DUI","id":"c8750a96ee37eee3b30464a1c98b99b456c5a828","venue_1":"3DUI","year":"2009","title":"Poster: Collaborative data exploration using two navigation strategies","authors":"Omar Gomez, Helmuth Trefftz, Pierre Boulanger, Walter F. Bischof","author_ids":"3068411, 1690094, 1798278, 1766762","abstract":"Virtual collaborative systems are vital tools for accessing and sharing scientific data visualizations. This paper shows how two different modes of collaboration can affect user performance in a specific exploration task. Experiments with groups of users that are working in pairs showed that the lack of mobility can affect the ability to achieve specific exploration goals in a virtual environment. Our analysis reveals that the task was completed more efficiently when users were allowed to move freely and independently instead of working with limited mobility. In these systems, users adapted their own abilities and minimized the effect of mobility restrictions. 1 INTRODUCTION For years scientists have used scientific visualization techniques to gain insight into complex datasets and models. Scientists can infer and find relationships among phenomena relying on the human visual system rather than computerized analytical techniques. These systems have helped to build better drugs, to find global economic trends, or to predict the weather. But modern discoveries are no more made by single individuals because complex scientific projects often demand the collaboration of several specialists. This paper explores scientific visualization and its relation to virtual collaborative systems. Virtual collaboration is necessary for visualization analysis for many reasons: • Complex systems require the skills of several experts in different knowledge areas. • It is not uncommon to use supercomputers or other specialized hardware to run simulations that produce large amounts of data for visualization. It is thus better to make the visualization close to these resources or at the very least closer to the data from which it comes from. • Reliance on experts in the same geographical area is often difficult, and it is thus more practical to provide mechanisms for remote data exploration. For these reasons, the discipline of Computer-Supported Cooperative Work (CSCW) has emerged [1]. Visualization systems of the type this paper is dealing with have adopted different ways of enabling collaborative work. These ways lead to different architectures that allow users to collaborate in different modes. It is our belief that a good understanding of these modes of collaboration are of great importance when choosing which architecture to deploy in a distributed environment. For this study, a minimal visualization system was built that allowed to recreate an environment used by scientists working in fluid mechanics. A series of tests were performed on two collaborative models, and we investigated which mode of collaboration was more efficient for …","cites":"0","conferencePercentile":"10.41666667"},{"venue":"3DUI","id":"f8a8582a9d0ef141c9747dd0383a2e6bac3ace70","venue_1":"3DUI","year":"2012","title":"Democratizing rendering for multiple viewers in surround VR systems","authors":"Jürgen P. Schulze, Daniel Acevedo, John Mangan, Andrew Prudhomme, Phi Nguyen, Philip P. Weber","author_ids":"1700671, 1929580, 2645905, 2915360, 2844117, 3230090","abstract":"We present a new approach for how multiple users' views can be rendered in a surround virtual environment without using special multi-view hardware. It is based on the idea that different parts of the screen are often viewed by different users, so that they can be rendered from their own view point, or at least from a point closer to their view point than traditionally expected. The vast majority of 3D virtual reality systems are designed for one head-tracked user, and a number of passive viewers. Only the head tracked user gets to see the correct view of the scene, everybody else sees a distorted image. We reduce this problem by algorithmically democratizing the rendering view point among all tracked users. Researchers have proposed solutions for multiple tracked users, but most of them require major changes to the display hardware of the VR system, such as additional projectors or custom VR glasses. Our approach does not require additional hardware, except the ability to track each participating user. We propose three versions of our multi-viewer algorithm. Each of them balances image distortion and frame rate in different ways, making them more or less suitable for certain application scenarios. Our most sophisticated algorithm renders each pixel from its own, optimized camera perspective, which depends on all tracked users' head positions and orientations.","cites":"3","conferencePercentile":"48.38709677"},{"venue":"3DUI","id":"fdbdde6272c3b4ca8843b5c1187dfffed2b1e053","venue_1":"3DUI","year":"2010","title":"Extending the virtual trackball metaphor to rear touch input","authors":"Sven G. Kratz, Michael Rohs","author_ids":"3103005, 1876551","abstract":"Interaction with 3D objects and scenes is becoming increasingly important on mobile devices. We explore 3D object rotation as a fundamental interaction task. We propose an extension of the virtual trackball metaphor, which is typically restricted to a half sphere and single-sided interaction, to actually use a full sphere. The extension is enabled by a hardware setup called the \" iPhone Sandwich , \" which allows for simultaneous front-and-back touch input. This setup makes the rear part of the virtual trackball accessible for direct interaction and thus achieves the realization of the virtual trackball metaphor to its full extent. We conducted a user study that shows that a back-of-device virtual trackball is as effective as a front-of-device virtual trackball and that both outperform an implementation of tilt-based input.","cites":"6","conferencePercentile":"65.78947368"},{"venue":"3DUI","id":"5957ff8f0ea7ecb56fa8235c18b25c266490e36f","venue_1":"3DUI","year":"2015","title":"Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality","authors":"Mahdi Nabiyouni, Ayshwarya Saktheeswaran, Doug A. Bowman, Ambika Karanth","author_ids":"1804979, 3215903, 1729869, 3001550","abstract":"One of the goals of much virtual reality (VR) research is to increase realism. In particular, many techniques for locomotion in VR attempt to approximate real-world walking. However, it is not yet fully understood how the design of more realistic locomotion techniques affects user task performance. We performed an experiment to compare a semi-natural locomotion technique (based on the Virtusphere device) with a traditional, non-natural technique (based on a game controller) and a fully natural technique (real walking). We found that the Virtusphere technique was significantly slower and less accurate than both of the other techniques. Based on this result and others in the literature, we speculate that locomotion techniques with moderate interaction fidelity will often have performance inferior to both high-fidelity techniques and well-designed low-fidelity techniques. We argue that our experimental results are an effect of interaction fidelity, and perform a detailed analysis of the fidelity of the three locomotion techniques to support this argument.","cites":"4","conferencePercentile":"94.44444444"},{"venue":"3DUI","id":"b381b56bc1c5352fbeca73335f901e7df03bdf04","venue_1":"3DUI","year":"2015","title":"Design and evaluation of a visual acclimation aid for a semi-natural locomotion device","authors":"Mahdi Nabiyouni, Siroberto Scerbo, Vincent DeVito, Stefan Smolen, Patrick Starrin, Doug A. Bowman","author_ids":"1804979, 1795279, 2391870, 3330434, 1825317, 1729869","abstract":"One of the limitations of most virtual reality (VR) systems is that users cannot physically walk through large virtual environments. M any solutions have been proposed to this problem, including locomotion devices such as the Virtusphere. Such devices allow the user to employ moderately natural walking motions without physically moving through space, but may actually be difficult to use at first due to a lack of interaction fidelity. We designed and evaluated a visual aid that shows a virtual representation of the sphere to the user during an acclimation phase, reasoning that this would help users understand the forces they were feeling, plan their movements, and better control their movements. In a user study, we evaluated participants' walking performance both during and after an acclimation phase. Half of the participants used the visual aid during acclimation, while the other half had no visual aid. After acclimation, all participants performed more complex walking assessment tasks without any visual aid. The results demonstrate that use of the visual aid during acclimation was effective for improving task performance and decreasing perceived difficulty in the assessment tasks.","cites":"1","conferencePercentile":"61.11111111"},{"venue":"3DUI","id":"e20f45e0b06d9713be0c11afd79e8166294b5665","venue_1":"3DUI","year":"2014","title":"Slice-n-Swipe: A free-hand gesture user interface for 3D point cloud annotation","authors":"Felipe Bacim, Mahdi Nabiyouni, Doug A. Bowman","author_ids":"1720157, 1804979, 1729869","abstract":"Three-dimensional point clouds are generated by devices such as laser scanners and depth cameras, but their output is a set of unstructured, unlabeled points. Many scenarios require users to identify parts of the point cloud through manual annotation. Inspired by the current generation of \" natural user interface \" technologies, we present Slice-n-Swipe, a technique for 3D point cloud annotation based on free-hand gesture input. The technique is based on a chef's knife metaphor, and uses progressive refinement to allow the user to specify the points of interest. We demonstrate the Slice-n-Swipe concept with a prototype using the Leap Motion Controller for free-hand gesture input and a 3D mouse for virtual camera control.","cites":"5","conferencePercentile":"76.08695652"}]}