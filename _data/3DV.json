{"3DV.csv":[{"venue":"3DV","id":"cd35f65b8170c88d05286d93c332eaf318f14881","venue_1":"3DV","year":"2013","title":"Patch Volumes: Segmentation-Based Consistent Mapping with RGB-D Cameras","authors":"Peter Henry, Dieter Fox, Achintya Bhowmik, Rajiv Mongia","author_ids":"1791800, 1776234, 2961765, 2947899","abstract":"Recent advances have allowed for the creation of dense, accurate 3D maps of indoor environments using RGB-D cameras. Some techniques are able to create large-scale maps, while others focus on accurate details using GPU-accelerated volumetric representations. In this work we describe patch volumes, a novel multiple-volume representation which enables the creation of globally consistent maps of indoor environments beyond the capabilities of previous high-accuracy volumetric representations.","cites":"22","conferencePercentile":"91.42857143"},{"venue":"3DV","id":"09750ce4a8fa0a0fc596bdda8bf58db74fa9a0e1","venue_1":"3DV","year":"2016","title":"Synthesizing Training Images for Boosting Human 3D Pose Estimation","authors":"Wenzheng Chen, Huan Wang, Yangyan Li, Hao Su, Zhenhua Wang, Changhe Tu, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen","author_ids":"2924449, 1703731, 1920864, 2888806, 1715589, 3327879, 1684384, 1701009, 1748939","abstract":"Human 3D pose estimation from a single image is a challenging task with numerous applications. Convolutional Neural Networks (CNNs) have recently achieved superior performance on the task of 2D pose estimation from a single image, by training on images with 2D annotations collected by crowd sourcing. This suggests that similar success could be achieved for direct estimation of 3D poses. However , 3D poses are much harder to annotate, and the lack of suitable annotated training images hinders attempts towards end-to-end solutions. To address this issue, we opt to automatically synthesize training images with ground truth pose annotations. Our work is a systematic study along this road. We find that pose space coverage and texture diversity are the key ingredients for the effectiveness of synthetic training data. We present a fully automatic, scalable approach that samples the human pose space for guiding the synthesis procedure and extracts clothing textures from real images. Furthermore, we explore domain adaptation for bridging the gap between our synthetic training images and real testing photos. We demonstrate that CNNs trained with our synthetic images out-perform those trained with real photos on 3D pose estimation tasks.","cites":"7","conferencePercentile":"100"},{"venue":"3DV","id":"7705d1db0b2cdd45a75f9ead53cd39938a26706f","venue_1":"3DV","year":"2016","title":"Learning to Navigate the Energy Landscape","authors":"Julien P. C. Valentin, Angela Dai, Matthias Nießner, Pushmeet Kohli, Philip H. S. Torr, Shahram Izadi, Cem Keskin","author_ids":"2898574, 2208531, 2209612, 1685185, 7194119, 1699068, 7206941","abstract":"In this paper 1 , we present a novel, general, and efficient architecture for addressing computer vision problems that are approached from an 'Analysis by Synthesis' standpoint. Analysis by synthesis involves the minimization of reconstruction error, which is typically a non-convex function of the latent target variables. State-of-the-art methods adopt a hybrid scheme where discriminatively trained predictors like Random Forests or Convolutional Neural Networks are used to initialize local search algorithms. While these hybrid methods have been shown to produce promising results , they often get stuck in local optima. Our method goes beyond the conventional hybrid architecture by not only proposing multiple accurate initial solutions but by also defining a navigational structure over the solution space that can be used for extremely efficient gradient-free local search. We demonstrate the efficacy and generalizability of our approach by on tasks as diverse as Hand Pose Estimation , RGB Camera Relocalization, and Image Retrieval.","cites":"2","conferencePercentile":"76.92307692"},{"venue":"3DV","id":"f8be39c4ba8671672b06e8bc4cf81d3def412e7d","venue_1":"3DV","year":"2015","title":"Shadow Detection and Sun Direction in Photo Collections","authors":"Scott Wehrwein, Kavita Bala, Noah Snavely","author_ids":"3134916, 8261370, 1830653","abstract":"Modeling the appearance of outdoor scenes from photo collections is challenging because of appearance variation, especially due to illumination. In this paper we present a simple and robust algorithm for estimating illumination properties—shadows and sun direction—from photo collections. These properties are key to a variety of scene modeling applications, including outdoor intrinsic images, realistic 3D scene rendering, and temporally varying (4D) reconstruction. Our shadow detection method uses illumination ratios to analyze lighting independent of camera effects, and determines shadow labels for each 3D point in a reconstruction. These shadow labels can then be used to detect shadow boundaries and estimate sun direction, as well as to compute dense shadow labels in pixel space. We demonstrate our method on large Internet photo collections of scenes, and show that it outperforms prior multi-image shadow detection and sun direction estimation methods.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"3DV","id":"683f89af4317be90da7d759ea6daaff23308ba66","venue_1":"3DV","year":"2014","title":"Real-Time Face Reconstruction from a Single Depth Image","authors":"Vahid Kazemi, Cem Keskin, Jonathan Taylor, Pushmeet Kohli, Shahram Izadi","author_ids":"2626422, 7206941, 7356633, 1685185, 1699068","abstract":"Figure 1: Our method starts with estimating dense correspondences on an input depth image, using a discriminative model. A generative model parametrized by blend shapes is then utilized to further refine these correspondences. The final correspondence field is used for per-frame 3D face shape and expression reconstruction, allowing for texture unwrapping, retexturing or retargeting in real-time. Abstract This paper contributes a real time method for recovering facial shape and expression from a single depth image. The method also estimates an accurate and dense correspondence field between the input depth image and a generic face model. Both outputs are a result of minimizing the error in reconstructing the depth image, achieved by applying a set of identity and expression blend shapes to the model. Traditionally, such a generative approach has shown to be computationally expensive and non-robust because of the non-linear nature of the reconstruction error. To overcome this problem, we use a discriminatively trained prediction pipeline that employs random forests to generate an initial dense but noisy correspondence field. Our method then exploits a fast ICP-like approximation to update these correspondences , allowing us to quickly obtain a robust initial fit of our model. The model parameters are then fine tuned to minimize the true reconstruction error using a stochastic optimization technique. The correspondence field resulting from our hybrid generative-discriminative pipeline is accurate and useful for a variety of applications such as mesh deformation and retexturing. Our method works in real-time on a single depth image i.e. without temporal tracking, is free from per-user calibration, and works in low-light conditions.","cites":"3","conferencePercentile":"80.76923077"},{"venue":"3DV","id":"2350da8e3da215c6fddf149a081442ebabaf8a44","venue_1":"3DV","year":"2013","title":"Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion","authors":"Maik Keller, Damien Lefloch, Martin Lambers, Shahram Izadi, Tim Weyrich, Andreas Kolb","author_ids":"1970236, 3157468, 2722712, 1699068, 1784306, 1758212","abstract":"Real-time or online 3D reconstruction has wide applicability and receives further interest due to availability of consumer depth cameras. Typical approaches use a moving sensor to accumulate depth measurements into a single model which is continuously refined. Designing such systems is an intricate balance between reconstruction quality, speed, spatial scale, and scene assumptions. Existing online methods either trade scale to achieve higher quality reconstructions of small objects/scenes. Or handle larger scenes by trading real-time performance and/or quality, or by limiting the bounds of the active reconstruction. Additionally, many systems assume a static scene, and cannot robustly handle scene motion or reconstructions that evolve to reflect scene changes. We address these limitations with a new system for real-time dense reconstruction with equivalent quality to existing online methods, but with support for additional spatial scale and robustness in dynamic scenes. Our system is designed around a simple and flat point-based representation , which directly works with the input acquired from range/depth sensors, without the overhead of converting between representations. The use of points enables speed and memory efficiency, directly leveraging the standard graphics pipeline for all central operations; i.e., camera pose estimation , data association, outlier removal, fusion of depth maps into a single denoised model, and detection and update of dynamic objects. We conclude with qualitative and quantitative results that highlight robust tracking and high quality reconstructions of a diverse set of scenes at varying scales.","cites":"25","conferencePercentile":"94.28571429"},{"venue":"3DV","id":"d93e7a9acb3a727e7aeeb59dae7c29f189f155e9","venue_1":"3DV","year":"2015","title":"Unsupervised Temporal Segmentation of Repetitive Human Actions Based on Kinematic Modeling and Frequency Analysis","authors":"Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy","author_ids":"7417926, 2862376, 1727159, 1784213","abstract":"In this paper, we propose a method for temporal segmen-tation of human repetitive actions based on frequency analysis of kinematic parameters, zero-velocity crossing detection , and adaptive k-means clustering. Since the human motion data may be captured with different modalities which have different temporal sampling rate and accuracy (e.g., optical motion capture systems vs. Microsoft Kinect), we first apply a generic full-body kinematic model with an un-scented Kalman filter to convert the motion data into a unified representation that is robust to noise. Furthermore, we extract the most representative kinematic parameters via the primary frequency analysis. The sequences are segmented based on zero-velocity crossing of the selected parameters followed by an adaptive k-means clustering to identify the repetition segments. Experimental results demonstrate that for the motion data captured by both the motion capture system and the Microsoft Kinect, our proposed algorithm obtains robust segmentation of repetitive action sequences.","cites":"3","conferencePercentile":"85.18518519"},{"venue":"3DV","id":"6fb95e4c7a6c1f16fa8eacd5a222290321b6ff49","venue_1":"3DV","year":"2015","title":"Learning Hierarchical Semantic Segmentations of LIDAR Data","authors":"David Dohan, Brian Matejek, Thomas A. Funkhouser","author_ids":"2180770, 2712401, 1807080","abstract":"This paper investigates a method for semantic segmen-tation of small objects in terrestrial LIDAR scans in urban environments. The core research contribution is a hierarchical segmentation algorithm where potential merges between segments are prioritized by a learned affinity function and constrained to occur only if they achieve a significantly high object classification probability. This approach provides a way to integrate a learned shape-prior (the object classifier) into a search for the best semantic segmentation in a fast and practical algorithm. Experiments with LIDAR scans collected by Google Street View cars throughout ∼100 city blocks of New York City show that the algorithm provides better segmentations and classifications than simple alternatives for cars, vans, traffic lights, and street lights.","cites":"1","conferencePercentile":"59.25925926"},{"venue":"3DV","id":"e0eb766feea25f04418de22301dab42e8d8d9ddf","venue_1":"3DV","year":"2013","title":"SIFT-Realistic Rendering","authors":"Dominik Sibbing, Torsten Sattler, Bastian Leibe, Leif Kobbelt","author_ids":"2533661, 1959475, 1789756, 1763010","abstract":"—3D localization approaches establish correspondences between points in a query image and a 3D point cloud reconstruction of the environment. Traditionally, the database models are created from photographs using Structure-from-Motion (SfM) techniques, which requires large collections of densely sampled images. In this paper, we address the question how point cloud data from terrestrial laser scanners can be used instead to significantly reduce the data collection effort and enable more scalable localization. The key change here is that, in contrast to SfM points, laser-scanned 3D points are not automatically associated with local image features that could be matched to query image features. In order to make this data usable for image-based localization, we explore how point cloud rendering techniques can be leveraged to create virtual views from which database features can be extracted that match real image-based features as closely as possible. We propose different rendering techniques for this task, experimentally quantify how they affect feature repeatability, and demonstrate their benefit for image-based localization.","cites":"5","conferencePercentile":"68.57142857"},{"venue":"3DV","id":"80548f0dad8577b7487a6a1d4cad4b8facd5ed3d","venue_1":"3DV","year":"2013","title":"Accurate Georegistration of Point Clouds Using Geographic Data","authors":"Chun-Po Wang, Kyle Wilson, Noah Snavely","author_ids":"3213514, 4555425, 1830653","abstract":"The Internet contains a wealth of rich geographic information about our world, including 3D models, street maps, and many other data sources. This information is potentially useful for computer vision applications, such as scene understanding for outdoor Internet photos. However, leveraging this data for vision applications requires precisely aligning input photographs, taken from the wild, within a geographic coordinate frame, by estimating the position, orientation, and focal length. To address this problem, we propose a system for aligning 3D structure-from-motion point clouds, produced from Internet imagery, to existing geographic information sources, including Google Street View photos and Google Earth 3D models. We show that our method can produce accurate alignments between these data sources, resulting in the ability to accurately project geographic data into images gathered from the Internet, by \" Googling \" a depth map for an image using sources such as Google Earth.","cites":"8","conferencePercentile":"82.85714286"},{"venue":"3DV","id":"887e28f281c0b8c5dbfcf9c869bf565ad5a5f5f7","venue_1":"3DV","year":"2013","title":"Personalization and Evaluation of a Real-Time Depth-Based Full Body Tracker","authors":"Thomas Helten, Andreas Baak, Gaurav Bharaj, Meinard Müller, Hans-Peter Seidel, Christian Theobalt","author_ids":"1956300, 2951896, 3090007, 1775418, 1746884, 1680185","abstract":"Reconstructing a three-dimensional representation of human motion in real-time constitutes an important research topic with applications in sports sciences, human-computer-interaction, and the movie industry. In this paper, we contribute with a robust algorithm for estimating a personalized human body model from just two sequentially captured depth images that is more accurate and runs an order of magnitude faster than the current state-of-the-art procedure. Then, we employ the estimated body model to track the pose in real-time from a stream of depth images using a tracking algorithm that combines local pose optimization and a stabilizing database look-up. Together, this enables accurate pose tracking that is more accurate than previous approaches. As a further contribution, we evaluate and compare our algorithm to previous work on a comprehensive benchmark dataset containing more than 15 minutes of challenging motions. This dataset comprises calibrated marker-based motion capture data, depth data, as well as ground truth tracking results and is publicly available for research purposes. Tracking 3D human motion data constitutes an important strand of research with many applications to computer animation, medicine or human-computer-interaction. In recent years, the introduction of unexpensive depth cameras like Time-of-Flight cameras [1] or the Microsoft Kinect has boosted the research on monocular tracking since they constitute comparably cheap to obtain so-called 2.5 dimensional depth maps. Tracking from such depth input is especially appealing in home consumer scenarios, where a user controls an application only by using his own body as an input device and where complex hardware setups are not feasible. While depth data facilitates background subtraction compared to pure image based approaches, tracking still remains challenging because of the high dimensionality of the pose space and noise in the depth data. Currently, there exist two different strategies to harness depth data for tracking human motions. Bottom-up approaches detect body parts or joint-positions directly from the depth images. Such approaches often neglect the underlying skeletal topology of the human which may lead to improbable joint locations and jitter in the extracted motion. Top-down approaches fit a parametric model to the depth data using an optimization scheme. Here, the accuracy of the final tracking result is dependent on the degree to which the body model matches the true body shape of the person. In practice, such models are often obtained in a preprocessing step, e. g., using laser scanners which are not available in home consumer scenarios. Recently, first attempts have …","cites":"15","conferencePercentile":"88.57142857"},{"venue":"3DV","id":"3bf9888972cffdad27388e58790d0e528ce5942d","venue_1":"3DV","year":"2014","title":"Multi-view Photometric Stereo by Example","authors":"Jens Ackermann, Fabian Langguth, Simon Fuhrmann, Arjan Kuijper, Michael Goesele","author_ids":"1724781, 1877166, 1743966, 1738151, 1689293","abstract":"2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Abstract We present a novel multi-view photometric stereo technique that recovers the surface of textureless objects with unknown BRDF and lighting. The camera and light positions are allowed to vary freely and change in each image. We exploit orientation consistency between the target and an example object to develop a consistency measure. Motivated by the fact that normals can be recovered more reliably than depth, we represent our surface as both a depth map and a normal map. These maps are jointly optimized and allow us to formulate constraints on depth that take surface orientation into account. Our technique does not require the visual hull or stereo reconstructions for boot-strapping and solely exploits image intensities without the need for radiometric camera calibration. We present results on real objects with varying degree of specularity and show that these can be used to create globally consistent models from multiple views.","cites":"16","conferencePercentile":"100"},{"venue":"3DV","id":"580d2fd1b79e928242ca28e6d964af927c014ca7","venue_1":"3DV","year":"2016","title":"HDRFusion: HDR SLAM Using a Low-Cost Auto-Exposure RGB-D Sensor","authors":"Shuda Li, Ankur Handa, Yang Zhang, Andrew Calway","author_ids":"1690899, 2890624, 4449904, 3336943","abstract":"We describe a new method for comparing frame appearance in a frame-to-model 3-D mapping and tracking system using an low dynamic range (LDR) RGB-D camera which is robust to brightness changes caused by auto exposure. It is based on a normalised radiance measure which is invariant to exposure changes and not only robustifies the tracking under changing lighting conditions, but also enables the following exposure compensation perform accurately to allow online building of high dynamic range (HDR) maps. The latter facilitates the frame-to-model tracking to minimise drift as well as better capturing light variation within the scene. Results from experiments with synthetic and real data demonstrate that the method provides both improved tracking and maps with far greater dynamic range of luminosity.","cites":"0","conferencePercentile":"23.07692308"},{"venue":"3DV","id":"fa0abcbcddb728c123420e1489a8d176b5a09db1","venue_1":"3DV","year":"2014","title":"Calibration of Non-overlapping Cameras Using an External SLAM System","authors":"Esra Ataer Cansizoglu, Yuichi Taguchi, Srikumar Ramalingam, Yohei Miki","author_ids":"1781876, 2246577, 1699414, 2192947","abstract":"We present a simple method for calibrating a set of cameras that may not have overlapping field of views. We reduce the problem of calibrating the non-overlapping cameras to the problem of localizing the cameras with respect to a global 3D model reconstructed with a simultaneous localization and mapping (SLAM) system. Specifically, we first reconstruct such a global 3D model by using a SLAM system using an RGB-D sensor. We then perform localization and intrinsic parameter estimation for each camera by using 2D-3D correspondences between the camera and the 3D model. Our method locates the cameras within the 3D model, which is useful for visually inspecting camera poses and provides a model-guided browsing interface of the images. We demonstrate the advantages of our method using several indoor scenes. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Abstract—We present a simple method for calibrating a set of cameras that may not have overlapping field of views. We reduce the problem of calibrating the non-overlapping cameras to the problem of localizing the cameras with respect to a global 3D model reconstructed with a simultaneous localization and mapping (SLAM) system. Specifically, we first reconstruct such a global 3D model by using a SLAM system using an RGB-D sensor. We then perform localization and intrinsic parameter estimation for each camera by using 2D-3D correspondences between the camera and the 3D model. Our method locates the cameras within the 3D model, which is useful for visually inspecting camera poses and provides a model-guided browsing interface of the images. We demonstrate the advantages of our method using several indoor scenes.","cites":"1","conferencePercentile":"37.5"},{"venue":"3DV","id":"df2f474f9e682ac41a7e159aa9a85a05213b8eb8","venue_1":"3DV","year":"2014","title":"Photometric Stereo Using Internet Images","authors":"Boxin Shi, Kenji Inose, Yasuyuki Matsushita, Ping Tan, Sai-Kit Yeung, Katsushi Ikeuchi","author_ids":"2563079, 2585428, 1774618, 1911264, 3124660, 1739452","abstract":"—Photometric stereo using unorganized Internet images is very challenging, because the input images are captured under unknown general illuminations, with uncontrolled cameras. We propose to solve this difficult problem by a simple yet effective approach that makes use of a coarse shape prior. The shape prior is obtained from multi-view stereo and will be useful in twofold: resolving the shape-light ambiguity in uncalibrated photometric stereo and guiding the estimated normals to produce the high quality 3D surface. By assuming the surface albedo is not highly contrasted, we also propose a novel linear approximation of the nonlinear camera responses with our normal estimation algorithm. We evaluate our method using synthetic data and demonstrate the surface improvement on real data over multi-view stereo results.","cites":"11","conferencePercentile":"97.11538462"},{"venue":"3DV","id":"b3b262de503bd160ffcfd33e180d4e56fb41e53b","venue_1":"3DV","year":"2013","title":"The Visual Turing Test for Scene Reconstruction","authors":"Qi Shan, Riley Adams, Brian Curless, Yasutaka Furukawa, Steven M. Seitz","author_ids":"2141964, 2209945, 1810052, 1798912, 1679223","abstract":"We present the first large scale system for capturing and rendering relightable scene reconstructions from massive unstructured photo collections taken under different illumination conditions and viewpoints. We combine photos tak-en from many sources, Flickr-based ground-level imagery, oblique aerial views, and streetview, to recover models that are significantly more complete and detailed than previously demonstrated. We demonstrate the ability to match both the viewpoint and illumination of arbitrary input photos, enabling a Visual Turing Test in which photo and rendering are viewed side-by-side and the observer has to guess which is which. While we cannot yet fool human perception, the gap is closing.","cites":"31","conferencePercentile":"97.14285714"},{"venue":"3DV","id":"e0773ae92f130bc3c3f1f0b7bd591f5eab05682c","venue_1":"3DV","year":"2013","title":"3D Semantic Parameterization for Human Shape Modeling: Application to 3D Animation","authors":"Christian Rupprecht, Olivier Pauly, Christian Theobalt, Slobodan Ilic","author_ids":"2714036, 2178555, 1680185, 1695386","abstract":"Statistical human body models, like SCAPE, capture static 3D human body shapes and poses and are applied to many Computer Vision problems. Defined in a statistical context, their parameters do not explicitly capture semantics of the human body shapes such as height, weight, limb length, etc. Having a set of semantic parameters would allow users and automated algorithms to sample the space of possible body shape variations in a more intuitive way. Therefore, in this paper we propose a method for re-parameterization of statistical human body models such that shapes are controlled by a small set of intuitive semantic parameters. These parameters are learned directly from the available statistical human body model. In order to apply any arbitrary animation to our human body shape model we perform retargeting. From any set of 3D scans, a semantic parametrized model can be generated and animated with the presented methods using any animation data. We quantitatively show that our semantic parameter-ization is more reliable than standard semantic parameteri-zations, and show a number of animations retargeted to our semantic body shape model.","cites":"1","conferencePercentile":"21.42857143"},{"venue":"3DV","id":"c52ba3d22eab34a7f3dcb804377c52594511ec2b","venue_1":"3DV","year":"2016","title":"Dense Wide-Baseline Scene Flow from Two Handheld Video Cameras","authors":"Christian Richardt, Hyeongwoo Kim, Levi Valgaerts, Christian Theobalt","author_ids":"7240032, 3022958, 1797649, 1680185","abstract":"We propose a new technique for computing dense scene flow from two handheld videos with wide camera baselines and different photometric properties due to different sensors or camera settings like exposure and white balance. Our technique innovates in two ways over existing methods: (1) it supports independently moving cameras, and (2) it computes dense scene flow for wide-baseline scenarios. We achieve this by combining state-of-the-art wide-baseline correspondence finding with a variational scene flow formulation. First, we compute dense, wide-baseline correspondences using DAISY descriptors for matching between cameras and over time. We then detect and replace occluded pixels in the correspondence fields using a novel edge-preserving Laplacian correspondence completion technique. We finally refine the computed correspondence fields in a variational scene flow formulation. We show dense scene flow results computed from challenging datasets with independently moving, hand-held cameras of varying camera settings.","cites":"2","conferencePercentile":"76.92307692"},{"venue":"3DV","id":"435f1dfb8f4efe91e66eee0cf83973613ecf3571","venue_1":"3DV","year":"2015","title":"On Preserving Structure in Stereo Seam Carving","authors":"Kuo-Chin Lien, Matthew Turk","author_ids":"2656227, 1752714","abstract":"The major objective of image retargeting algorithms is to preserve the viewer's perception while adjusting the aspect ratio of an image. This means that an ideal retargeting algorithm has to be able to preserve high-level semantics and avoid generating low-level image distortion. Stereoscopic image retargeting poses a even more challenging problem in that the 3D perception has to be preserved as well. In this paper, we propose an algorithm based on high-order two-view co-labeling to simultaneously retarget a given stereo pair and preserve its 2D as well as 3D quality. Our experimental results qualitatively demonstrate the improved ability of preserving 2D image structures in both views. In addition , we show quantitatively that our algorithm improves upon the state-of-the-art up to 85% in terms of a measurement based on depth distortion.","cites":"0","conferencePercentile":"25.92592593"},{"venue":"3DV","id":"79ad644213e88ec14ef2197d23c9541629a29176","venue_1":"3DV","year":"2014","title":"Efficient Multiview Stereo by Random-Search and Propagation","authors":"Youngjung Uh, Yasuyuki Matsushita, Hyeran Byun","author_ids":"2847986, 1774618, 8555016","abstract":"We present an efficient multi-view 3D reconstruction method based on randomization and propagation scheme. Our method progressively refines 3D point estimates by randomly perturbing the initial guess of 3D points and propagates photo-consistent ones to their neighbors. In contrast to previous refinement methods that perform local optimization for a better photo-consistency, our randomization approach takes lucky matchings for reducing the computational complexity. Experiments show favorable efficiency of the proposed method with the accuracy that is close to the state-of-the-art methods.","cites":"2","conferencePercentile":"62.5"},{"venue":"3DV","id":"0d7ef9306bd25cf3ba1b5d309f75b9fbaf9d3fa8","venue_1":"3DV","year":"2016","title":"Large Scale SfM with the Distributed Camera Model","authors":"Chris Sweeney, Victor Fragoso, Tobias Höllerer, Matthew Turk","author_ids":"2224763, 2245862, 1743721, 1752714","abstract":"We introduce the distributed camera model, a novel model for Structure-from-Motion (SfM). This model describes image observations in terms of light rays with ray origins and directions rather than pixels. As such, the proposed model is capable of describing a single camera or multiple cameras simultaneously as the collection of all light rays observed. We show how the distributed camera model is a generalization of the standard camera model and describe a general formulation and solution to the absolute camera pose problem that works for standard or distributed cameras. The proposed method computes a solution that is up to 8 times more efficient and robust to rotation singularities in comparison with gDLS[20]. Finally, this method is used in an novel large-scale incremental SfM pipeline where distributed cameras are accurately and robustly merged together. This pipeline is a direct generalization of traditional incremental SfM; however, instead of incrementally adding one camera at a time to grow the reconstruction the reconstruction is grown by adding a distributed camera. Our pipeline produces highly accurate reconstructions efficiently by avoiding the need for many bundle adjustment iterations and is capable of computing a 3D model of Rome from over 15,000 images in just 22 minutes.","cites":"0","conferencePercentile":"23.07692308"},{"venue":"3DV","id":"3f06d445371c252d5a6ba977181987094148d6de","venue_1":"3DV","year":"2016","title":"Fast Single Shot Detection and Pose Estimation","authors":"Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka, Alexander C. Berg","author_ids":"3451188, 3468873, 2667317, 1722649, 1743020, 1743555","abstract":"For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or tracking parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for sliding-window detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose estimation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the offset for its location, and the approximate pose are all estimated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose estimation (42.4% 8 View mAVP on Pascal 3D+ [21]) and significantly faster (46 frames per second (FPS) on a TITAN X GPU). This approach to detection and rough pose estimation is fast and accurate enough to be widely applied as a pre-processing step for tasks including high-accuracy pose estimation, object tracking and localization, and vSLAM.","cites":"2","conferencePercentile":"76.92307692"},{"venue":"3DV","id":"93673e46ea112af9ced518a734c59531ec4368d7","venue_1":"3DV","year":"2014","title":"Interactive Mapping of Indoor Building Structures through Mobile Devices","authors":"Giovanni Pintore, Marco Agus, Enrico Gobbetti","author_ids":"1778838, 2544127, 1708999","abstract":"—We present a practical system to map and reconstruct multi-room indoor structures using the sensors commonly available in commodity smartphones. Our approach combines and extends state-of-the-art results to automatically generate floor plans scaled to real-world metric dimensions and to reconstruct scenes not necessarily limited to the Manhattan World assumption. In contrast to previous works, our method introduces an interactive method based on statistical indicators for refining wall orientations and a specialized merging algorithm for building the final rooms shape. The low CPU cost of the method makes it possible to support full execution by commodity smartphones, without the need of connecting them to a compute server. We demonstrate the effectiveness of our technique on a variety of multi-room indoor scenes, achieving remarkably better results than previous approaches.","cites":"1","conferencePercentile":"37.5"},{"venue":"3DV","id":"076812e6362d7d8f406be6d6d6de5c10dce574be","venue_1":"3DV","year":"2014","title":"GASP: Geometric Association with Surface Patches","authors":"Rahul Sawhney, Fuxin Li, Henrik I. Christensen","author_ids":"2964042, 3141988, 1723059","abstract":"— A fundamental challenge to sensory processing tasks in perception and robotics is the problem of obtaining data associations across views. We present a robust solution for ascertaining potentially dense surface patch (superpixel) associations , requiring just range information. Our approach involves decomposition of a view into regularized surface patches. We represent them as sequences expressing geometry invariantly over their superpixel neighborhoods, as uniquely consistent partial orderings. We match these representations through an optimal sequence comparison metric based on the Damerau-Levenshtein distance-enabling robust association with quadratic complexity (in contrast to hitherto employed joint matching formulations which are NP-complete). The approach is able to perform under wide baselines, heavy rotations, partial overlaps, significant occlusions and sensor noise. The technique does not require any priors – motion or otherwise, and does not make restrictive assumptions on scene structure and sensor movement. It does not require appearance – is hence more widely applicable than appearance reliant methods, and invulnerable to related ambiguities such as textureless or aliased content. We present promising qualitative and quantitative results under diverse settings, along with comparatives with popular approaches based on range as well as RGB-D data.","cites":"0","conferencePercentile":"12.5"},{"venue":"3DV","id":"aaed59716ff12c1c9650676d3c985dbde9f97e45","venue_1":"3DV","year":"2014","title":"Two Cameras and a Screen: How to Calibrate Mobile Devices?","authors":"Amaël Delaunoy, Jia Li, Bastien Jacquet, Marc Pollefeys","author_ids":"3241610, 1687366, 2217217, 1742208","abstract":"—We propose a new approach to estimate the geometric extrinsic calibration of all the elements of a smartphone or tablet (such as the screen, the front and the back cameras) by using a planar mirror. By moving a smartphone in front of a single static planar mirror, it is possible to establish correspondences between the images and a pattern displayed on the screen, and therefore estimate the geometric relationship between the non-overlapping cameras with respect to the screen location. The newly proposed setup (static mirror, moving smartphone) enables to both improve the state-of-the-art by working in the minimal case of two images, and improve the accuracy when more images are available. We analyze the minimal case for different calibration scenarios and evaluate the proposed approach on several data. We also show an application of this geometric calibration for specular surface reconstruction, by observing the reflection of a known pattern displayed on the screen.","cites":"1","conferencePercentile":"37.5"},{"venue":"3DV","id":"5ac76e4662df21c82eda90fab2b9b110bc5e6eef","venue_1":"3DV","year":"2013","title":"Assisted Multi-view Stereo Reconstruction","authors":"Matteo Dellepiane, Emanuele Cavarretta, Paolo Cignoni, Roberto Scopigno","author_ids":"2674152, 2073763, 1738697, 7980724","abstract":"—Multi-view stereo reconstruction methods can provide impressive results in a number of applications. Nevertheless , when trying to apply the state-of-the-art methods in the case of a more structured 3D acquisition, the lack of feedback on the quality of the reconstruction during the photo shooting can be problematic. In this paper we present a framework for the assisted reconstruction from images of real objects. The framework is able to provide, in quasi-realtime, a sparse reconstruction of the scene, so that the user is able to spot the missing or problematic parts. Moreover, the framework is able to separate the object of interest from the background and suggests missing points of view to the user, without any previous knowledge of the shape of the scene and the acquisition path. This is obtained by analyzing the sparse reconstruction and the connection between the reconstructed points and the input images. The framework has been tested on a variety of practical cases, and it has proved to be effective not only to obtain more complete reconstructions, but also to reduce the number of images needed and the processing time for dense reconstruction. I. INTRODUCTION Multi-view stereo reconstruction is a process concerning the automatic acquisition of objects and scenes models from multiple photographs. It aims at obtaining a 3D representation of a real object, in the form of a point cloud or a polygonal mesh, starting from a set of uncalibrated images. Potential applications of this technology include: construction of realistic object models for the movie, television, and video game industries; quantitative recovery of metric information for scientific and engineering data analysis; fast visualization via point-based rendering techniques; and object replication through fast prototyping technologies. The diffusion of high-quality, low-cost consumer digital cameras and the advancement in the state-of-the-art of this research topic made multi-view stereo reconstruction a very promising technology widely available to the public. A significant example of this is the Microsoft PhotoSynth online service, but several other freeware or low-cost solutions are also available. The structure of most multi-view stereo reconstruction solutions is based on an unsupervised strategy, where no previous information about the images is known. The reconstruction follows some steps which are usually fulfilled in a pipeline fashion (see also Figure 1-top). Although based on different steps, that could be driven by the user, the available systems","cites":"3","conferencePercentile":"44.28571429"},{"venue":"3DV","id":"008baae7037a47f69804c2eb8438d366a6e67486","venue_1":"3DV","year":"2016","title":"3D Human Pose Estimation via Deep Learning from 2D Annotations","authors":"Ernesto Brau, Hao Jiang","author_ids":"2048004, 4795925","abstract":"We propose a deep convolutional neural network for 3D human pose and camera estimation from monocular images that learns from 2D joint annotations. The proposed network follows the typical architecture, but contains an additional output layer which projects predicted 3D joints onto 2D, and enforces constraints on body part lengths in 3D. We further enforce pose constraints using an independently trained network that learns a prior distribution over 3D poses. We evaluate our approach on several benchmark datasets and compare against state-of-the-art approaches for 3D human pose estimation, achieving comparable performance. Additionally, we show that our approach significantly outperforms other methods in cases where 3D ground truth data is unavailable, and that our network exhibits good generalization properties.","cites":"1","conferencePercentile":"55.76923077"},{"venue":"3DV","id":"982278785a90c85d0ef51879b58d2ba990cccfb8","venue_1":"3DV","year":"2013","title":"A Learned Joint Depth and Intensity Prior Using Markov Random Fields","authors":"Daniel Herrera C., Juho Kannala, Peter F. Sturm, Janne Heikkilä","author_ids":"2533478, 1776374, 1713056, 1756505","abstract":"We present a joint prior that takes intensity and depth information into account. The prior is defined using a flexible Field-of-Experts model and is learned from a database of natural images. It is a generative model and has an efficient method for sampling. We use sampling from the model to perform inpainting and upsampling of depth maps when intensity information is available. We show that including the intensity information in the prior improves the results obtained from the model. We also compare to another two-channel inpainting approach and show superior results.","cites":"4","conferencePercentile":"57.14285714"},{"venue":"3DV","id":"d00e49f58771d34ece0be63fb4e1b7480351f3bb","venue_1":"3DV","year":"2014","title":"DT-SLAM: Deferred Triangulation for Robust SLAM","authors":"Daniel Herrera C., Kihwan Kim, Juho Kannala, Kari Pulli, Janne Heikkilä","author_ids":"2533478, 3736059, 1776374, 1704409, 1756505","abstract":"Obtaining a good baseline between different video frames is one of the key elements in vision-based monoc-ular SLAM systems. However, if the video frames contain only a few 2D feature correspondences with a good base-line, or the camera only rotates without sufficient translation in the beginning, tracking and mapping becomes unstable. We introduce a real-time visual SLAM system that incrementally tracks individual 2D features, and estimates camera pose by using matched 2D features, regardless of the length of the baseline. Triangulating 2D features into 3D points is deferred until keyframes with sufficient base-line for the features are available. Our method can also deal with pure rotational motions, and fuse the two types of measurements in a bundle adjustment step. Adaptive criteria for keyframe selection are also introduced for efficient optimization and dealing with multiple maps. We demonstrate that our SLAM system improves camera pose estimates and robustness, even with purely rotational motions.","cites":"4","conferencePercentile":"89.42307692"},{"venue":"3DV","id":"7bba2bc21e7b978d16b07f996c6e96640eb52e3a","venue_1":"3DV","year":"2016","title":"Video Depth-from-Defocus","authors":"Hyeongwoo Kim, Christian Richardt, Christian Theobalt","author_ids":"3022958, 7240032, 1680185","abstract":"Many compelling video post-processing effects, in particular aesthetic focus editing and refocusing effects, are feasible if per-frame depth information is available. Existing computational methods to capture RGB and depth either purposefully modify the optics (coded aperture, light-field imaging), or employ active RGB-D cameras. Since these methods are less practical for users with normal cameras, we present an algorithm to capture all-in-focus RGB-D video of dynamic scenes with an unmodified commodity video camera. Our algorithm turns the often unwanted defocus blur into a valuable signal. The input to our method is a video in which the focus plane is continuously moving back and forth during capture, and thus defocus blur is provoked and strongly visible. This can be achieved by manually turning the focus ring of the lens during recording. The core algorithmic ingredient is a new video-based depth-from-defocus algorithm that computes space-time-coherent depth maps, deblurred all-in-focus video, and the focus distance for each frame. We extensively evaluate our approach, and show that it enables compelling video post-processing effects, such as different types of refocusing.","cites":"0","conferencePercentile":"23.07692308"},{"venue":"3DV","id":"19a32d3569cb38f555682683419a97531596a41b","venue_1":"3DV","year":"2014","title":"Learning 3D Part Detection from Sparsely Labeled Data","authors":"Ameesh Makadia, Mehmet Ersin Yümer","author_ids":"2159982, 2396667","abstract":"—For large collections of 3D models, the ability to detect and localize parts of interest is necessary to provide search and visualization enhancements beyond simple high-level categorization. While current 3D labeling approaches rely on learning from fully labeled meshes, such training data is difficult to acquire at scale. In this work we explore learning to detect object parts from sparsely labeled data, i.e. we operate under the assumption that for any object part we have only one labeled vertex rather than a full region segmentation. Similarly, we also learn to output a single representative vertex for each detected part. Such localized predictions are useful for applications where visualization is important. Our approach relies heavily on exploiting the spatial configuration of parts on a model to drive the detection. Inspired by structured multi-class object detection models for images, we develop an algorithm that combines independently trained part classifiers with a structured SVM model, and show promising results on real-world textured 3D data.","cites":"3","conferencePercentile":"80.76923077"},{"venue":"3DV","id":"69261d39258a7d996044669264f68ea41ecf951e","venue_1":"3DV","year":"2014","title":"Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model","authors":"Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti Oulasvirta, Christian Theobalt","author_ids":"2961113, 2933543, 1746884, 2663734, 1680185","abstract":"Real-time marker-less hand tracking is of increasing importance in human-computer interaction. Robust and accurate tracking of arbitrary hand motion is a challenging problem due to the many degrees of freedom, frequent self-occlusions, fast motions, and uniform skin color. In this paper , we propose a new approach that tracks the full skeleton motion of the hand from multiple RGB cameras in real-time. The main contributions include a new generative tracking method which employs an implicit hand shape representation based on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is smooth and analytically differen-tiable making fast gradient based pose optimization possible. This shape representation, together with a full perspective projection model, enables more accurate hand mod-eling than a related baseline method from literature. Our method achieves better accuracy than previous methods and runs at 25 fps. We show these improvements both qualitatively and quantitatively on publicly available datasets.","cites":"11","conferencePercentile":"97.11538462"},{"venue":"3DV","id":"db0e9ba59b726dd60df4284bf07cb50aefc74dc3","venue_1":"3DV","year":"2013","title":"CrowdCam: Instantaneous Navigation of Crowd Images Using Angled Graph","authors":"Aydin Arpa, Luca Ballan, Rahul Sukthankar, Gabriel Taubin, Marc Pollefeys, Ramesh Raskar","author_ids":"2030244, 2565514, 1694199, 1690237, 1742208, 1717566","abstract":"We present a near real-time algorithm for interactively exploring a collectively captured moment without explicit 3D reconstruction. Our system favors immediacy and local coherency to global consistency. It is common to represent photos as vertices of a weighted graph, where edge weights measure similarity or distance between pairs of photos. We introduce Angled Graphs as a new data structure to organize collections of photos in a way that enables the construction of visually smooth paths. Weighted angled graphs extend weighted graphs with angles and angle weights which penalize turning along paths. As a result, locally straight paths can be computed by specifying a photo and a direction. The weighted angled graphs of photos used in this paper can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos. Ultimately, our system enables everyday people to take advantage of each others' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence. We believe that this type of application will greatly enhance shared human experiences spanning from events as personal as parents watching their children's football game to highly publicized red carpet galas.","cites":"6","conferencePercentile":"74.28571429"},{"venue":"3DV","id":"97d132c7765275707ee63dee5d597ab5e1b41dad","venue_1":"3DV","year":"2016","title":"Real-Time Halfway Domain Reconstruction of Motion and Geometry","authors":"Lucas Thies, Michael Zollhöfer, Christian Richardt, Christian Theobalt, Günther Greiner","author_ids":"3079262, 1699058, 7240032, 1680185, 1700516","abstract":"We present a novel approach for real-time joint reconstruction of 3D scene motion and geometry from binocular stereo videos. Our approach is based on a novel variational halfway-domain scene flow formulation, which allows us to obtain highly accurate spatiotemporal reconstructions of shape and motion. We solve the underlying optimization problem at real-time frame rates using a novel data-parallel robust non-linear optimization strategy. Fast convergence and large displacement flows are achieved by employing a novel hierarchy that stores delta flows between hierarchy levels. High performance is obtained by the introduction of a coarser warp grid that decouples the number of unknowns from the input resolution of the images. We demonstrate our approach in a live setup that is based on two commodity webcams, as well as on publicly available video data. Our extensive experiments and evaluations show that our approach produces high-quality dense reconstructions of 3D geometry and scene flow at real-time frame rates, and compares favorably to the state of the art.","cites":"1","conferencePercentile":"55.76923077"},{"venue":"3DV","id":"fbac11e46ed336b8b27544d6c661b8f9387c1187","venue_1":"3DV","year":"2013","title":"Single View Reconstruction of Piecewise Swept Surfaces","authors":"Avanish Kushal, Steven M. Seitz","author_ids":"3201468, 1679223","abstract":"We present a novel approach for the single view reconstruction (SVR) of piecewise swept scenes, exploiting the regular structure present in these man-made scenes. The parallelism of lines and its extension to curves are used as cues within a novel sequential algorithm propagating information across faces and their boundaries. Using this approach we are able to model a wide variety of architectural scenes and man-made objects. We show results generated both automatically as well as using user interaction, within a unified framework.","cites":"0","conferencePercentile":"7.142857143"},{"venue":"3DV","id":"28ef2c3d19460d719f0bbe93cf59335ba4dc60cb","venue_1":"3DV","year":"2014","title":"Multistage SFM: Revisiting Incremental Structure from Motion","authors":"Rajvi Shah, Aditya Deshpande, P. J. Narayanan","author_ids":"1962817, 2118997, 1729020","abstract":"In this paper, we present a new multistage approach for SfM reconstruction of a single component. Our method begins with building a coarse 3D reconstruction using high-scale features of given images. This step uses only a fraction of features and is fast. We enrich the model in stages by localizing remaining images to it and matching and trian-gulating remaining features. Unlike traditional incremental SfM, localization and triangulation steps in our approach are made efficient and embarrassingly parallel using geometry of the coarse model. The coarse model allows us to use 3D-2D correspondences based direct localization techniques to register remaining images. We further utilize the geometry of the coarse model to reduce the pair-wise image matching effort as well as to perform fast guided feature matching for majority of features. Our method produces similar quality models as compared to incremental SfM methods while being notably fast and parallel. Our algorithm can reconstruct a 1000 images dataset in 15 hours using a single core, in about 2 hours using 8 cores and in a few minutes by utilizing full parallelism of about 200 cores.","cites":"1","conferencePercentile":"37.5"},{"venue":"3DV","id":"d6166e3c21ed576a871c5e2442d0af8f1060f2f5","venue_1":"3DV","year":"2016","title":"Model-Based Outdoor Performance Capture","authors":"Nadia Robertini, Dan Casas, Helge Rhodin, Hans-Peter Seidel, Christian Theobalt","author_ids":"1958339, 8188478, 2933543, 1746884, 1680185","abstract":"Figure 1: From a set of multi-camera input images (left), we reconstruct the human performance as a temporally consistent 3D mesh that accurately matches the captured motion, here visualized textured (center) and untextured (right) to better aporeciate the results. Our novel formulation for model-based human performance capture enables reconstruction of outdoor performances without explicit silhouette segmentation. Abstract We propose a new model-based method to accurately reconstruct human performances captured outdoors in a multi-camera setup. Starting from a template of the actor model, we introduce a new unified implicit representation for both, articulated skeleton tracking and non-rigid surface shape refinement. Our method fits the template to unsegmented video frames in two stages – first, the coarse skeletal pose is estimated, and subsequently non-rigid surface shape and body pose are jointly refined. Particularly for surface shape refinement we propose a new combination of 3D Gaussians designed to align the projected model with likely silhouette contours without explicit segmentation or edge detection. We obtain reconstructions of much higher quality in outdoor settings than existing methods, and show that we are on par with state-of-the-art methods on indoor scenes for which they were designed.","cites":"1","conferencePercentile":"55.76923077"},{"venue":"3DV","id":"600f6c9f5f0a952302cb5b59c3a4e6b16a43d71f","venue_1":"3DV","year":"2014","title":"Efficient Multi-view Performance Capture of Fine-Scale Surface Detail","authors":"Nadia Robertini, Edilson de Aguiar, Thomas Helten, Christian Theobalt","author_ids":"1958339, 2049341, 1956300, 1680185","abstract":"new smooth energy with analytic derivatives for dense photo-consistency-based surface refinement that can be efficiently optimized in the continuous surface displacement k s using conditioned gradient ascent. í µí°¸(í µí±� í µí±) = í µí°¸í µí± í µí±�í µí±� (í µí±� í µí±) − í µí°¸í µí±�í µí±�í µí±� (í µí±� í µí±) • The similarity term measures the overlap of the projected collection of 3D Gaussians on the surface with the 2D image Gaussians. • We parameterize the means μ s of the surface Gaussians in terms of their 1D displacement k s along the correspondent surface normal: • The regularization term keeps the distribution of the 3D Gaussians on the surface geometrically smooth.","cites":"3","conferencePercentile":"80.76923077"},{"venue":"3DV","id":"e352e1157b114162eb3ef0baee73da9fd0b6fc37","venue_1":"3DV","year":"2014","title":"Solving for Relative Pose with a Partially Known Rotation is a Quadratic Eigenvalue Problem","authors":"Chris Sweeney, John Flynn, Matthew Turk","author_ids":"2224763, 4532653, 1752714","abstract":"We propose a novel formulation of minimal case solutions for determining the relative pose of perspective and generalized cameras given a partially known rotation, namely, a known axis of rotation. An axis of rotation may be easily obtained by detecting vertical vanishing points with computer vision techniques, or with the aid of sensor measurements from a smartphone. Given a known axis of rotation , our algorithms solve for the angle of rotation around the known axis along with the unknown translation. We formulate these relative pose problems as Quadratic Eigen-value Problems which are very simple to construct. We run several experiments on synthetic and real data to compare our methods to the current state-of-the-art algorithms. Our methods provide several advantages over alternatives methods , including efficiency and accuracy, particularly in the presence of image and sensor noise as is often the case for mobile devices.","cites":"5","conferencePercentile":"92.30769231"},{"venue":"3DV","id":"d8a705eb8af153bd01882a71832a0b477cfe8c89","venue_1":"3DV","year":"2014","title":"Efficient Colorization of Large-Scale Point Cloud Using Multi-pass Z-Ordering","authors":"Sunyoung Cho, Jizhou Yan, Yasuyuki Matsushita, Hyeran Byun","author_ids":"2474144, 4072680, 1774618, 8555016","abstract":"We present an efficient colorization method for a large scale point cloud using multi-view images. To address the practical issues of noisy camera parameters and color inconsistencies across multi-view images, our method takes an optimization approach for achieving visually pleasing point cloud colorization. We introduce a multi-pass Z-ordering technique that efficiently defines a graph structure to a large-scale and un-ordered set of 3D points, and use the graph structure for optimizing the point colors to be assigned. Our technique is useful for defining minimal but sufficient connectivities among 3D points so that the optimization can exploit the sparsity for efficiently solving the problem. We demonstrate the effectiveness of our method using synthetic datasets and a large-scale real-world data in comparison with other graph construction techniques.","cites":"0","conferencePercentile":"12.5"}]}