{"ACM_Multimedia.csv":[{"venue":"ACM Multimedia","id":"37f1966d4dfd2ea276074180e505147ba3dec218","venue_1":"ACM Multimedia","year":"2013","title":"Violence detection in hollywood movies by the fusion of visual and mid-level audio cues","authors":"Esra Acar, Frank Hopfgartner, Sahin Albayrak","author_ids":"2534957, 1759761, 1722170","abstract":"Detecting violent scenes in movies is an important video content understanding functionality e.g., for providing automated youth protection services. One key issue in designing algorithms for violence detection is the choice of discriminative features. In this paper, we employ mid-level audio features and compare their discriminative power against low-level audio and visual features. We fuse these mid-level audio cues with low-level visual ones at the decision level in order to further improve the performance of violence detection. We use Mel-Frequency Cepstral Coefficients (MFCC) as audio and average motion as visual features. In order to learn a violence model, we choose two-class support vector machines (SVMs). Our experimental results on detecting violent video shots in Hollywood movies show that mid-level audio features are more discriminative and provide more precise results than low-level ones. The detection performance is further enhanced by fusing the mid-level audio cues with low-level visual ones using an SVM-based decision fusion.","cites":"5","conferencePercentile":"72.66666667"},{"venue":"ACM Multimedia","id":"c90b5682073bc9f25683c044065c02bc76316706","venue_1":"ACM Multimedia","year":"2011","title":"Ubi-MUI 2011 ACM workshop summary","authors":"Ali A. Nazari Shirehjini, Sahin Albayrak, Abdulsalam Yassine","author_ids":"1804988, 1722170, 1784983","abstract":"Intelligent Environments have the vision of enhancing our everyday environment and interaction with its objects by sensing, computing, and communication capabilities. The major characteristics of such environments are the increasing number of embedded intelligent devices (ubiquity) into the background (transparency). These devices are expected to disappear or blend into the background and will be invisible to the user. However, because of this transparency, users fail to develop an adequate mental model for interaction with such environments. The Ubiquitous Meta User Interfaces (Ubi-MUI) ACM workshop provides a venue for the development of highly intuitive, multimedia supported meta user interfaces that bring transparency, predictability, and control into intelligent environments.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"762325ec61c899575136faf5296083778386e1d2","venue_1":"ACM Multimedia","year":"2016","title":"Are Safer Looking Neighborhoods More Lively?: A Multimodal Investigation into Urban Life","authors":"Marco De Nadai, Radu L. Vieriu, Gloria Zen, Stefan Dragicevic, Nikhil Naik, Michele Caraviello, César Augusto Hidalgo, Nicu Sebe, Bruno Lepri","author_ids":"7405787, 3069978, 2933565, 3451222, 3285462, 2517620, 3493596, 1703601, 1776476","abstract":"Policy makers, urban planners, architects, sociologists, and economists are interested in creating urban areas that are both lively and safe. But are the safety and liveliness of neighborhoods independent characteristics? Or are they just two sides of the same coin? In a world where people avoid unsafe looking places, neighborhoods that look unsafe will be less lively, and will fail to harness the natural surveillance of human activity. But in a world where the preference for safe looking neighborhoods is small, the connection between the perception of safety and liveliness will be either weak or nonexistent. In this paper we explore the connection between the levels of activity and the perception of safety of neighborhoods in two major Italian cities by combining mobile phone data (as a proxy for activity or liveliness) with scores of perceived safety estimated using a Convolutional Neural Network trained on a dataset of Google Street View images scored using a crowdsourced visual perception survey. We find that: (i) safer looking neighborhoods are more active than what is expected from their population density, employee density, and distance to the city centre; and (ii) that the correlation between appearance of safety and activity is positive, strong, and significant, for females and people over 50, but negative for people under 30, suggesting that the behavioral impact of perception depends on the demographic of the population. Finally, we use occlusion techniques to identify the urban features that contribute to the appearance of safety, finding that greenery and street facing windows contribute to a positive appearance of safety (in agreement with Oscar Newman's defensible space theory). These results suggest that urban appearance modulates levels of human activity and, consequently, a neighborhood's rate of natural surveillance.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"7ec7d60445a41d80366d1b317d48a547c0792307","venue_1":"ACM Multimedia","year":"2008","title":"Noisy video super-resolution","authors":"Feng Liu, Jinjun Wang, Shenghuo Zhu, Michael Gleicher, Yihong Gong","author_ids":"1734409, 1698291, 1682028, 1776507, 1710467","abstract":"Low-quality videos often not only have limited resolution, but also suffer from noise. Directly up-sampling a video without considering noise could deteriorate its visual quality due to magnifying noise. This paper addresses this problem with a unified framework that achieves simultaneous de-noising and super-resolution. This framework formulates noisy video super-resolution as an optimization problem, aiming to maximize the visual quality of the result. We consider a good quality result to be fidelity-preserving, detail-preserving and smooth. Accordingly, we propose measures for these qualities in the scenario of de-noising and super-resolution. The experiments on a variety of noisy videos demonstrate the effectiveness of the presented algorithm.","cites":"5","conferencePercentile":"48.85321101"},{"venue":"ACM Multimedia","id":"45f6ddd24f69fc1ecbbda48f8f8eee2c11ff0e2d","venue_1":"ACM Multimedia","year":"2007","title":"Video summarization by redundancy removing and content ranking","authors":"Tao Wang, Yue Gao, Patricia Peng Wang, Eric Li, Wei Hu, Yimin Zhang, Jun-Hai Yong","author_ids":"1685072, 1744619, 2349189, 4224970, 7004409, 1707720, 1724669","abstract":"In order to help the user to grasp the long video content quickly, this paper proposes a novel video summarization approach based on redundancy removal and content ranking. By video parsing and cast indexing, the approach first constructs a <i>story board</i> to let user know about the main scenes and the main actors in the video. Then it generates a <i>\"story-constraint summary\"</i> by key frame clustering and repetitive segment detection. To shorten the video summary length to a target length, our approach constructs a <i>\"time-constraint summary\"</i> by important factor based content ranking. Extensive experiments are carried out on TV series, movies, and cartoons. Good results demonstrate the effectiveness of the proposed method.","cites":"3","conferencePercentile":"34.63541667"},{"venue":"ACM Multimedia","id":"30c5000bd841388d426de26df678f567b75210c7","venue_1":"ACM Multimedia","year":"2015","title":"Predicting and Understanding Urban Perception with Convolutional Neural Networks","authors":"Lorenzo Porzi, Samuel Rota Bulò, Bruno Lepri, Elisa Ricci","author_ids":"3202308, 2145174, 1776476, 1878028","abstract":"Cities' visual appearance plays a central role in shaping human perception and response to the surrounding urban environment. For example, the visual qualities of urban spaces affect the psychological states of their inhabitants and can induce negative social outcomes. Hence, it becomes critically important to understand people's perceptions and evaluations of urban spaces. Previous works have demonstrated that algorithms can be used to predict high level attributes of urban scenes (<i>e.g.</i> safety, attractiveness, uniqueness), accurately emulating human perception. In this paper we propose a novel approach for predicting the perceived safety of a scene from Google Street View Images. Opposite to previous works, we formulate the problem of learning to predict high level judgments as a ranking task and we employ a Convolutional Neural Network (CNN), significantly improving the accuracy of predictions over previous methods. Interestingly, the proposed CNN architecture relies on a novel pooling layer, which permits to automatically discover the most important areas of the images for predicting the concept of perceived safety. An extensive experimental evaluation, conducted on the publicly available Place Pulse dataset, demonstrates the advantages of the proposed approach over state-of-the-art methods.","cites":"9","conferencePercentile":"96.48148148"},{"venue":"ACM Multimedia","id":"16d151025c65ccaf644ac8c8e5c71427226aedfd","venue_1":"ACM Multimedia","year":"2007","title":"Re-cinematography: improving the camera dynamics of casual video","authors":"Michael Gleicher, Feng Liu","author_ids":"1776507, 1734409","abstract":"This paper presents an approach to post-processing casually captured videos to improve apparent camera movement. <i>Re-cinematography</i> transforms each frame of a video such that the video better follows cinematic conventions. The approach breaks videos into shorter segments. For segments of the source video where the camera is relatively static, re-cinematography uses image stabilization to make the result look locked-down. For segments with camera motions, camera paths are keyframed automatically and interpolated with matrix logarithms to give velocity-profiled movements that appear intentional and directed. The approach automatically balances the tradeoff between motion smoothness and distortion to the original imagery. Results from our prototype show improvements to poor quality home videos.","cites":"17","conferencePercentile":"74.21875"},{"venue":"ACM Multimedia","id":"3756276aa7aa65d0b3a79867c1078e0ad3fd1854","venue_1":"ACM Multimedia","year":"2014","title":"The Workshop on Computational Personality Recognition 2014","authors":"Fabio Celli, Bruno Lepri, Joan-Isaac Biel, Daniel Gatica-Perez, Giuseppe Riccardi, Fabio Pianesi","author_ids":"2222709, 1776476, 3082345, 1698682, 1719162, 8029006","abstract":"The Workshop on Computational Personality Recognition aims to define the state-of-the-art in the field and to provide tools for future standard evaluations in personality recognition tasks. In the WCPR14 we released two different datasets: one of Youtube Vlogs and one of Mobile Phone interactions. We structured the workshop in two tracks: an open shared task, where participants can do any kind of experiment, and a competition. We also distinguished two tasks: A) personality recognition from multimedia data, and B) personality recognition from text only. In this paper we discuss the results of the workshop.","cites":"16","conferencePercentile":"94.17670683"},{"venue":"ACM Multimedia","id":"b09ebdcd143588efdf995da0d6e16f9127c2da62","venue_1":"ACM Multimedia","year":"2006","title":"Video retargeting: automating pan and scan","authors":"Feng Liu, Michael Gleicher","author_ids":"1734409, 1776507","abstract":"When a video is displayed on a smaller display than originally intended, some of the information in the video is necessarily lost. In this paper, we introduce <i>Video Retargeting</i> that adapts video to better suit the target display, minimizing the important information lost. We define a framework that measures the preservation of the source material, and methods for estimating the important information in the video. Video retargeting crops each frame and scales it to fit the target display. An optimization process minimizes information loss by balancing the loss of detail due to scaling with the loss of content and composition due to cropping. The cropping window can be moved during a shot to introduce virtual pans and cuts, subject to constraints that ensure cinematic plausibility. We demonstrate results of adapting a variety of source videos to small display sizes.","cites":"115","conferencePercentile":"98.44559585"},{"venue":"ACM Multimedia","id":"7f2dfb0776eff90744a65c5a7ce86b83bc26a0fa","venue_1":"ACM Multimedia","year":"1999","title":"MAVIS 2: an architecture for content and concept based multimedia information exploration","authors":"Robert Tansley, Mark R. Dobie, Paul H. Lewis, Wendy Hall","author_ids":"2709391, 2620443, 1773066, 1685385","abstract":"DEMONSTRATION ABSTRACT We are currently developing an open architecture that supports content based multimedia information exploration. MAVIS 2 (Multimedia Architecture for Video, Image and Sound, the second system by that name) aims to provide integrated content and concept based navigation, retrieval and browsing within and between multimedia objects in several media types[l]. It is designed to allow a user to make selections from media that they are viewing and form a query. The scope of the query governs the range of searching and the type of results that will be returned. The scope can include retrieval, in which case similar objects are returned to the user and it can include navigation, in which case links with similar source anchors to the selection are returned. All matching is content based and uses signatures (or feature vectors) that are derived from the media selections. The algorithms that are available are media dependent and many algorithms can be used to support one match. New algorithms can be transparently introduced into the system and will automatically be used due to the abstraction of functionality based process addressing. The current matching process exhaustively compares the query against objects within the query scope, but new approaches to matching can be introduced in a similar manner to individual comparison algorithms. A fundamental limitation of content based matching is that the system's ability to match is constrained by the degree to which measurable features can discriminate between objects the user wants and those they don't. Our approach to alleviating this problem is to introduce a semantic layer which stores a network of related concepts and multimedia representations of these concepts can be associated with nodes in the semantic layer. These may be different views of the same object or different examples of objects of the same type. The representations can be thought of as multimedia synonyms of concepts. The semantic layer can watch the query process and if the query selection matches well with a selection associated with a concept then other selections associated with that concept can be made Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on …","cites":"0","conferencePercentile":"7.983193277"},{"venue":"ACM Multimedia","id":"3f1b640533245a99296328876c2f9059bc5ea8f7","venue_1":"ACM Multimedia","year":"2000","title":"Towards virtual videography (poster session)","authors":"Michael Gleicher, James Masanz","author_ids":"1776507, 1894361","abstract":"Videographers have developed an art of conveying events in video. Through choices made in cinematography, editing, and post-processing, effective video presentations can be created from events recorded with little or no intrusion. In this paper, we explore systems that bring videography to situations where cost or time issues preclude application of the art. Our goal is to develop <i>virtual videography</i>, that is, systems that can help automate the process of creating an effective video presentation from given footage. In this paper, we discuss how virtual videography systems can be constructed by combining image-based rendering to synthetically generate shots with image understanding to help choose what should be shown to the viewer. To this, visual effects can be added to enhance the presentation, lessening the degradation caused by the medium.","cites":"3","conferencePercentile":"32.60869565"},{"venue":"ACM Multimedia","id":"242bb53bbfe0cb18298aa030134abc354b621121","venue_1":"ACM Multimedia","year":"2008","title":"Discovering panoramas in web videos","authors":"Feng Liu, Yu Hen Hu, Michael Gleicher","author_ids":"1734409, 1943030, 1776507","abstract":"While methods for stitching panoramas have been successful given proper source images, providing these source images still remains a burden. In this paper, we present a method to discover panoramic source images within widely available web videos. The challenge comes from the fact that many of these videos are not recorded intentionally for stitching panoramas. Our method aims to find segments within a video that work as panorama sources. Specifically, we determine a video segment to be a valid panorama source according to the following three criteria. First, its camera motion should cover a wide field-of-view of the scene. Second, its frames should be \"mosaicable\", which states that the inter-frame motion should observe the underlying conditions for stitching a panorama. Third, its frames should have good image quality. Based on these criteria, we formulate discovering panoramas in a video as an optimization problem that aims to find an optimal set of video segments as panorama sources. After discovering these panorama sources, we synthesize regular scene panoramas using them. When significant dynamics is detected in the sources, we fuse the dynamics into the scene panoramas to make activity synopses to convey the dynamics. Our experiment of querying panoramas from YouTube confirms the feasibility of using web videos as panorama sources and demonstrates the effectiveness of our method.","cites":"9","conferencePercentile":"65.36697248"},{"venue":"ACM Multimedia","id":"7a6d9f89e0925a220fe3dfba4f0d2745f8be6c9a","venue_1":"ACM Multimedia","year":"2014","title":"Learning Compact Face Representation: Packing a Face into an int32","authors":"Haoqiang Fan, Mu Yang, Zhimin Cao, Yujing Jiang, Qi Yin","author_ids":"1934546, 2726445, 8538782, 2956110, 7477615","abstract":"This paper addresses the problem of producing very compact representation of a face image for large-scale face search and analysis tasks. In tradition, the compactness of face representation is achieved by a dimension reduction step after representation extraction. However, the dimension reduction usually degrades the discriminative ability of the original representation drastically. In this paper, we present a deep learning framework which optimizes the compactness and discriminative ability jointly. The learnt representation can be as compact as 32 bit (same as the int32) and still produce highly discriminative performance (91.4% on LFW benchmark). Based on the extreme compactness, we show that traditional face analysis tasks (e.g. gender analysis) can be effectively solved by a Look-Up-Table approach given a large-scale face data set.","cites":"7","conferencePercentile":"84.93975904"},{"venue":"ACM Multimedia","id":"1cd36fedc999ae84bb405f237de278f1e8dc4732","venue_1":"ACM Multimedia","year":"2010","title":"Representative views re-ranking for 3D model retrieval with multi-bipartite graph reinforcement model","authors":"Yue Gao, You Yang, Qionghai Dai, Naiyao Zhang","author_ids":"1744619, 2807392, 1713158, 2178247","abstract":"In this paper, we propose a multi-bipartite graph reinforcement model for representative views re-ranking in 3D model retrieval. Given the views of one query 3D model, all query views are grouped into clusters to generate representative views and corresponding original weights. In the retrieval procedure, labeled positive retrieval results are employed to refine the query information. Each group of views from positive retrieval results and the group of representative query views are employed to construct a bipartite graph, and a multi-bipartite graph reinforcement algorithm is performed on these bipartite graphs to re-rank all views. Then the weights of all representative query views are updated. Experimental results on two 3D model databases are provided to justify the effectiveness of the proposed method.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"e56646512b276fa150551f4157092cb282070ada","venue_1":"ACM Multimedia","year":"2005","title":"A unified shot boundary detection framework based on graph partition model","authors":"Jinhui Yuan, Jianmin Li, Fuzong Lin, Bo Zhang","author_ids":"2422936, 8549039, 2025858, 1696318","abstract":"In this paper, we propose a unified shot boundary detection framework by extending the previous work of graph partition model with temporal constraints. To detect both the abrupt transitions (CUTs) and gradual transitions (GTs, excluding fade out/in) in a unified way, we incorporate temporal multi-resolution analysis into the model. Furthermore, instead of ad-hoc thresholding scheme, we construct a novel kind of feature to characterize shot transitions and employ support vector machine (SVM) with active leaning strategy to classify boundaries and non-boundaries. Extensive experiments have been carried out on the platform of TRECVID benchmark. The experimental results show that the proposed framework outperforms some others and achieves satisfactory results.","cites":"18","conferencePercentile":"74.5049505"},{"venue":"ACM Multimedia","id":"a8c13858586b2fb8562d109e5a7e12b2b65d55af","venue_1":"ACM Multimedia","year":"2004","title":"An online-optimized incremental learning framework for video semantic classification","authors":"Jun Wu, Xian-Sheng Hua, HongJiang Zhang, Bo Zhang","author_ids":"1714535, 1746102, 1718558, 1696318","abstract":"This paper considers the problems of feature variation and concept uncertainty in typical learning-based video semantic classification schemes. We proposed a new online semantic classification framework, termed OOIL (for Online-Optimized Incremental Learning), in which two sets of optimized classification models, local and global, are online trained by sufficiently exploiting both local and global statistic characteristics of videos. The global models are pre-trained on a relatively small set of pre-labeled samples. And the local models are optimized for the under-test video or video segment by checking a small portion of unlabeled samples in this video, while they are also applied to incrementally update the global models. Experiments have illustrated promising results on simulated data as well as real sports videos.","cites":"12","conferencePercentile":"65.93137255"},{"venue":"ACM Multimedia","id":"06d9332d16b56b0af115315272b57fdd7d0f2244","venue_1":"ACM Multimedia","year":"2002","title":"An effective region-based image retrieval framework","authors":"Feng Jing, Mingjing Li, HongJiang Zhang, Bo Zhang","author_ids":"7188215, 8392859, 1718558, 1696318","abstract":"We present a region-based image retrieval framework that integrates efficient region-based representation in terms of storage and retrieval and effective on-line learning capability. The framework consists of methods for image segmentation and grouping, indexing using modified inverted file, relevance feedback, and continuous learning. By exploiting a vector quantization method, a compact region-based image representation is achieved. Based on this representation, an indexing scheme similar to the inverted file technology is proposed. In addition, it supports relevance feedback based on the vector model with a weighting scheme. A continuous learning strategy is also proposed to enable the system to self improve. Experimental results on a database of 10,000 general-purposed images demonstrate the efficiency and effectiveness of the proposed framework.","cites":"36","conferencePercentile":"83.76068376"},{"venue":"ACM Multimedia","id":"b7aa194c63e61db0675383932cc6a88517a6dafb","venue_1":"ACM Multimedia","year":"2007","title":"VideoSense: a contextual video advertising system","authors":"Tao Mei, Linjun Yang, Xian-Sheng Hua, Hao Wei, Shipeng Li","author_ids":"1788123, 7866194, 1746102, 4567602, 4973820","abstract":"This demonstration presents a novel contextual advertising platform for online video service, called VideoSense. Unlike most current video-oriented sites that only display a video ad at the beginning or the end of a video, VideoSense aims to embed more contextually relevant ads at less intrusive positions within the video stream. Given an online video, VideoSense is able to detect a set of candidate ad insertion points based on content analysis, select a list of relevant candidate ads ranked according to textual relevance, and find the best match between insertion points and ads which maximizes the overall multimodal relevance. The effectiveness of VideoSense supporting contextually relevant and less intrusive advertising is validated by the user studies conducted on a variety of online video documents.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"0af2bf703c7f074abbc95bc914f34c5f802f08c5","venue_1":"ACM Multimedia","year":"2008","title":"Flickr distance","authors":"Lei Wu, Xian-Sheng Hua, Nenghai Yu, Wei-Ying Ma, Shipeng Li","author_ids":"1734221, 1746102, 1708598, 1705244, 4973820","abstract":"This paper presents Flickr distance, which is a novel measurement of the relationship between semantic concepts (objects, scenes) in visual domain. For each concept, a collection of images are obtained from Flickr, based on which the improved latent topic based visual language model is built to capture the visual characteristic of this concept. Then Flickr distance between different concepts is measured by the square root of Jensen-Shannon (JS) divergence between the corresponding visual language models. Comparing with WordNet, Flickr distance is able to handle far more concepts existing on the Web, and it can scale up with the increase of concept vocabularies. Comparing with Google distance, which is generated in textual domain, Flickr distance is more precise for visual domain concepts, as it captures the visual relationship between the concepts instead of their co-occurrence in text search results. Besides, unlike Google distance, Flickr distance satisfies triangular inequality, which makes it a more reasonable distance metric. Both subjective user study and objective evaluation show that Flickr distance is more coherent to human perception than Google distance. We also design several application scenarios, such as concept clustering and image annotation, to demonstrate the effectiveness of this proposed distance in image related applications.","cites":"70","conferencePercentile":"98.62385321"},{"venue":"ACM Multimedia","id":"4a59a73d2bc85b793463bba899d45e5a894223ec","venue_1":"ACM Multimedia","year":"2008","title":"A comprehensive human computation framework: with application to image labeling","authors":"Yang Yang, Bin B. Zhu, Rui Guo, Linjun Yang, Shipeng Li, Nenghai Yu","author_ids":"1708973, 1731015, 1691576, 7866194, 4973820, 1708598","abstract":"Image and video labeling is important for computers to understand images and videos and for image and video search. Manual labeling is tedious and costly. Automatically image and video labeling is yet a dream. In this paper, we adopt a Web 2.0 approach to labeling images and videos efficiently: Internet users around the world are mobilized to apply their \"common sense\" to solve problems that are hard for today's computers, such as labeling images and videos. We first propose a general human computation framework that binds problem providers, Web sites, and Internet users together to solve large-scale common sense problems efficiently and economically. The framework addresses the technical challenges such as preventing a malicious party from attacking others, removing answers from bots, and distilling human answers to produce high-quality solutions to the problems. The framework is then applied to labeling images. Three incremental refinement stages are applied. The first stage collects candidate labels of objects in an image. The second stage refines the candidate labels using multiple choices. Synonymic labels are also correlated in this stage. To prevent bots and lazy humans from selecting all the choices, trap labels are generated automatically and intermixed with the candidate labels. Semantic distance is used to ensure that the selected trap labels would be different enough from the candidate labels so that no human users would mistakenly select the trap labels. The last stage is to ask users to locate an object given a label from a segmented image. The experimental results are also reported in this paper. They indicate that our proposed schemes can successfully remove spurious answers from bots and distill human answers to produce high-quality image labels.","cites":"7","conferencePercentile":"58.48623853"},{"venue":"ACM Multimedia","id":"15ed4f031db4c23b49634e474e85168be32a5397","venue_1":"ACM Multimedia","year":"2007","title":"Using audio and video features to classify the most dominant person in a group meeting","authors":"Hayley Hung, Dinesh Babu Jayagopi, Chuohao Yeo, Gerald Friedland, Sileye O. Ba, Jean-Marc Odobez, Kannan Ramchandran, Nikki Mirghafori, Daniel Gatica-Perez","author_ids":"1756464, 1705782, 3101028, 1797144, 1684507, 1719610, 1740194, 1755677, 1698682","abstract":"The automated extraction of semantically meaningful information from multi-modal data is becoming increasingly necessary due to the escalation of captured data for archival. A novel area of multi-modal data labelling, which has received relatively little attention, is the automatic estimation of the most dominant person in a group meeting. In this paper, we provide a framework for detecting dominance in group meetings using different audio and video cues. We show that by using a simple model for dominance estimation we can obtain promising results.","cites":"47","conferencePercentile":"93.75"},{"venue":"ACM Multimedia","id":"b179a3906e0cb4a285dc9569b966c2c22b90a419","venue_1":"ACM Multimedia","year":"2004","title":"Designing experiential environments for management of personal multimedia","authors":"Rahul Singh, Rachel Knickmeyer, Punit Gupta, Ramesh Jain","author_ids":"2549072, 2626194, 2314047, 4521564","abstract":"With the increasing ubiquity of sensors and computational resources, it is becoming easier and increasingly common for people to electronically record, photographs, text, audio, and video gathered over their lifetime. Assimilating and taking advantage of such data requires recognition of its multimedia nature, development of data models that can represent semantics across different media, representation of complex relationships in the data (such as spatio-temporal, causal, or evolutionary), and finally, development of paradigms to mediate user-media interactions. There is currently a paucity of theoretical frameworks and implementations that allow management of diverse and rich multimedia data collections in context of the aforementioned requirements. This paper presents our research in designing an experiential Multimedia Electronic Chronicle system that addresses many of these issues in the concrete context of personal multimedia information. Central to our approach is the characterization and organization of media using the concept of an \"event\" for unified modeling and indexing. The event-based unified multimedia model underlies the experiential user interface, which supports direct interactions with the data within a unified presentation-exploration-query environment. In this environment, explicit facilities to model space and time aid in exploration and querying as well as in representation and reasoning with dynamic relationships in the data. Experimental and comparative studies demonstrate the promise of this research.","cites":"10","conferencePercentile":"62.00980392"},{"venue":"ACM Multimedia","id":"4cf439a2a8f038deb50414edea5be1c3f18fdd0f","venue_1":"ACM Multimedia","year":"2011","title":"Tag-based social image search with visual-text joint hypergraph learning","authors":"Yue Gao, Meng Wang, Huan-Bo Luan, Jialie Shen, Shuicheng Yan, Dacheng Tao","author_ids":"1744619, 1731598, 1696019, 1723020, 1698982, 7761803","abstract":"Tag-based social image search has attracted great interest and how to order the search results based on relevance level is a research problem. Visual content of images and tags have both been investigated. However, existing methods usually employ tags and visual content separately or sequentially to learn the image relevance. This paper proposes a tag-based image search with visual-text joint hypergraph learning. We simultaneously investigate the bag-of-words and bag-of-visual-words representations of images and accomplish the relevance estimation with a hypergraph learning approach. Each textual or visual word generates a hyperedge in the constructed hypergraph. We conduct experiments with a real-world data set and experimental results demonstrate the effectiveness of our approach.","cites":"25","conferencePercentile":"95.91836735"},{"venue":"ACM Multimedia","id":"19800d568ada500cb7607066bf6e82c378b1af56","venue_1":"ACM Multimedia","year":"2010","title":"W2Go: a travel guidance system by automatic landmark ranking","authors":"Yue Gao, Jinhui Tang, Richang Hong, Qionghai Dai, Tat-Seng Chua, Ramesh Jain","author_ids":"1744619, 8053308, 1739103, 1713158, 1684968, 4521564","abstract":"In this paper, we present a travel guidance system W2Go (Where to Go), which can automatically recognize and rank the landmarks for travellers. In this system, a novel Automatic Landmark Ranking (ALR) method is proposed by utilizing the tag and geo-tag information of photos in Flickr and user knowledge from Yahoo Travel Guide. ALR selects the popular tourist attractions (landmarks) based on not only the subjective opinion of the travel editors as is currently done on sites like WikiTravel and Yahoo Travel Guide, but also the ranking derived from popularity among tourists. Our approach utilizes geo-tag information to locate the positions of the tag-indicated places, and computes the probability of a tag being a landmark/site name. For potential landmarks, impact factors are calculated from the frequency of tags, user numbers in Flickr, and user knowledge in Yahoo Travel Guide. These tags are then ranked based on the impact factors. Several representative views for popular landmarks are generated from the crawled images with geo-tags to describe and present them in context of information derived from several relevant reference sources. The experimental comparisons to the other systems are conducted on eight famous cities over the world. User-based evaluation demonstrates the effectiveness of the proposed ALR method and the W2Go system.","cites":"36","conferencePercentile":"95.06849315"},{"venue":"ACM Multimedia","id":"4f2901836a3f14663a75a18249e6a2ca725e0614","venue_1":"ACM Multimedia","year":"2006","title":"Mapping learning in eigenspace for harmonious caricature generation","authors":"Junfa Liu, Yiqiang Chen, Wen Gao","author_ids":"5485341, 4070304, 3406319","abstract":"This paper proposes a mapping learning approach for caricature auto-generation. Simulating the artist's creativity based on the object's facial feature, our approach targets discovering what are the principal components of the facial features, and what's the difference between facial photograph and caricature measured by those components. In training phase, PCA approach is adopted to obtain the principal components. Then, machine learning of SVR (Support Vector Regression) is carried out to learn the mapping model in principal component space. With the mapping model, in application phase, users just need to input a frontal facial photograph for the caricature generation. The caricature is exaggerated based on the original face while reserving essential similar features. Experiments proved comparatively that our approach could generate more harmonious caricatures.","cites":"12","conferencePercentile":"74.87046632"},{"venue":"ACM Multimedia","id":"09c24618200203e3367b28c0dc12e82c09b34e2e","venue_1":"ACM Multimedia","year":"1999","title":"Visual digests for news video libraries","authors":"Michael G. Christel","author_ids":"7307726","abstract":"The Informedia Digital Video Library contains over 2000 hours of video, growing at a rate of 15 hours per week. A good query engine is not sufficient for information retrieval because often the candidate result sets grow in number as the library grows. Video digests summarize sets of stories from the library, providing users with a visual mechanism for interactive browsing and query refinement. These digests are generated dynamically under the direction of the user based on automatically derived metadata from the video library. Three types of digests are discussed: VIBE digests emphasizing word relationships, timelines showing trends against time, and maps showing geographic correlations. Multiple digests can be combined into a single view or animated into a temporal presentation.","cites":"30","conferencePercentile":"79.41176471"},{"venue":"ACM Multimedia","id":"002f697efabe3b8f0188e7a30383926e6d3e5c8e","venue_1":"ACM Multimedia","year":"1999","title":"tvDBMS: a video database management system incorporating a thematic indexing model","authors":"Shakeel A. Khoja, Wendy Hall","author_ids":"8296053, 1685385","abstract":"This demonstration presents a novel video database system, tvDBMS, which caters for complex and long videos, such as documentaries, educational videos, etc. As compared to relatively structured format videos like CNN news or commercial advertisements, this database system has the capacity to work with long and unstructured videos. tvDBMS metadata contains information about segments (combinations of frames) and scenes (collections of segments that represent the same content). This metadata is organized in such a way that it can be used to navigate a theme (concept or idea). Annotations describing scenes are linked in hierarchical manner to create a story line in the video, where thematic indexing is used to develop a video catalogue. Thematic indexing is a novel way to track a story in a video. A video contains many themes, which are implicitly related to each other. In order to resolve queries about particular scenes in a video, the scenes are stored hierarchically, which provides \" is-a \" or \" have-part \" relations between them. We have tested our model on one of the hour-long documentaries made for television by the Earl Mountbatten of Burma. The results show that a user can easily query for natural scenes or events such as Japanese Soldiers landing in Burma in World War 2, Allied forces defending the border, air bombings, soldiers marching in fields etc. tvDBMS is divided into three basic components. The Composer, which provides a user with tools to generate metadata for videos, create thematic indices and database connectivity of metadata and video data, as shown in figure (1). The Query Processor deals with all the queries to be performed and how to track a particular story line for an event or an object, whereas the Query Output Window provides twelve best searched portions of videos of a query. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish. to post on servers or to redistribute to lists, requires prior specific permwion and/or a fee.","cites":"0","conferencePercentile":"7.983193277"},{"venue":"ACM Multimedia","id":"36ed68e557334ce8115a7746339c3b1b55a3c75e","venue_1":"ACM Multimedia","year":"2007","title":"Aging in place: fall detection and localization in a distributed smart camera network","authors":"Adam Williams, Deepak Ganesan, Allen R. Hanson","author_ids":"6298085, 1742299, 1733922","abstract":"This paper presents the design, implementation and evaluation of a distributed network of smart cameras whose function is to detect and localize falls, an important application in elderly living environments. A network of overlapping smart cameras uses a decentralized procedure for computing inter-image homographies that allows the location of a fall to be reported in 2D world coordinates by calibrating only one camera. Also, we propose a joint routing and homography transformation scheme for multi-hop localization that yields localization errors of less than 2 feet using very low resolution images. Our goal is to demonstrate that such a distributed low-power system can perform adequately in this and related applications. A prototype implementation is given for low-power Agilent/UCLA Cyclops cameras running on the Crossbow MICAz platform. We demonstrate the effectiveness of the fall detection as well as the precision of the localization using a simulation of our sample implementation.","cites":"37","conferencePercentile":"89.58333333"},{"venue":"ACM Multimedia","id":"1d28c766c61fe0aebb1feeab2253ea8ff32214b1","venue_1":"ACM Multimedia","year":"2008","title":"Contextual in-image advertising","authors":"Tao Mei, Xian-Sheng Hua, Shipeng Li","author_ids":"1788123, 1746102, 4973820","abstract":"The community-contributed media contents over the Internet have become one of the primary sources for online advertising. However, conventional ad-networks such as Google AdSense treat image and video advertising as general text advertising without considering the inherent characteristics of visual contents. In this work, we propose an innovative contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense, represents the first attempt towards contextual in-image advertising. The relevant ads are selected based on not only textual relevance but also visual similarity so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image saliency to minimize intrusiveness to the user. We evaluate ImageSense on three photo-sharing sites with around one million images and 100 Web pages collected from several major sites, and demonstrate the effectiveness of ImageSense.","cites":"25","conferencePercentile":"87.1559633"},{"venue":"ACM Multimedia","id":"119d2a21567bb4d3a9834ef217b32fb92d0e3c50","venue_1":"ACM Multimedia","year":"2007","title":"Spectral regression: a unified subspace learning framework for content-based image retrieval","authors":"Deng Cai, Xiaofei He, Jiawei Han","author_ids":"1745280, 3945955, 1722175","abstract":"Relevance feedback is a well established and effective framework for narrowing down the gap between low-level visual features and high-level semantic concepts in content-based image retrieval. In most of traditional implementations of relevance feedback, a distance metric or a classifier is usually learned from user's provided negative and positive examples. However, due to the limitation of the user's feedbacks and the high dimensionality of the feature space, one is often confront with the issue of the <i>curse of the dimensionality</i>. Recently, several researchers have considered manifold ways to address this issue, such as Locality Preserving Projections, Augmented Relation Embedding, and Semantic Subspace Projection. In this paper, by using techniques from spectral graph embedding and regression, we propose a unified framework, called <i>spectral regression</i>, for learning an image subspace. This framework facilitates the analysis of the differences and connections between the algorithms mentioned above. And more crucially, it provides much faster computation and therefore makes the retrieval system capable of responding to the user's query more efficiently.","cites":"28","conferencePercentile":"87.5"},{"venue":"ACM Multimedia","id":"29aef14cc50be17b22b4dfc2240e17ea9e669df9","venue_1":"ACM Multimedia","year":"1996","title":"An Empirical Study of Attending and Comprehending Multimedia Presentations","authors":"Peter Faraday, Alistair G. Sutcliffe","author_ids":"2613856, 1683823","abstract":"The paper reports two studies concerning attention to and comprehension of Multimedia presentations. The MM sequence used was taken from a commercially produced CD-ROM, 'The Etiology of Cancer'. First, an eye tracking study of the presentation is reported. A second study was then cortductekl ml the memorisation of the materials used in the eye tracking study. The results of the studies are used to propose guidelines to improve design of MM presentations. INTRODUCTION One of the problems in the design of Multimedia (MM) interfaces is knowing whether the presentation will successfully deliver its content to its audience. For a user, understanding a multimedia presentation requires a series of","cites":"21","conferencePercentile":"44.44444444"},{"venue":"ACM Multimedia","id":"62691aae62afda3cd9339a4fb368b646b56b310f","venue_1":"ACM Multimedia","year":"1997","title":"Multimedia: Design for the ``Moment''","authors":"Peter Faraday, Alistair G. Sutcliffe","author_ids":"2613856, 1683823","abstract":"The paper reviews studies of attention and recall of expository The results of the studies are taken as a basis to justify the MM presentations, and summarises findings into key guidelines importance of moment by moment design. The problems found in for attentional design. An expert system based design advisor tool attentional design are used to inform a set of guidelines for is then described, which uses the guidelines to analyse MM presentation design. These are then extended to form a method for presentations. analysing MM presentations.","cites":"6","conferencePercentile":"22.61904762"},{"venue":"ACM Multimedia","id":"ae7c4c11b4fbc70e8a44a0456212cf6170126fef","venue_1":"ACM Multimedia","year":"1998","title":"Making Contact Points Between Text and Images","authors":"Peter Faraday, Alistair G. Sutcliffe","author_ids":"2613856, 1683823","abstract":"1. ABSTRACT Multimedia presentations and web pages make ever increasing use of combinations of text and image media. This paper expIores how 'contact points' or co-references between an image and text should be designed. A set of eye tracking results are reported which provide evidence for the effects of contact points. The results are used as the basis for a set of guidelines for successftd desi=m of image and teti A web page authoring tool is described which supports these requirements.","cites":"9","conferencePercentile":"26.92307692"},{"venue":"ACM Multimedia","id":"600dc8a4b70b4e058f17d0390bcf2ba8705c6e43","venue_1":"ACM Multimedia","year":"2006","title":"Transductive inference using multiple experts for brushwork annotation in paintings domain","authors":"Liza Leslie, Tat-Seng Chua, Ramesh Jain","author_ids":"2265789, 1684968, 4521564","abstract":"Many recent studies perform annotation of paintings based on brushwork. In these studies the brushwork is modeled indirectly as part of the annotation of high-level artistic concepts such as the artist name using low-level texture. In this paper, we develop a serial multi-expert framework for explicit annotation of paintings with brushwork classes. In the proposed framework, each individual expert implements transductive inference by exploiting both labeled and unlabelled data. To minimize the problem of noise in the feature space, the experts select appropriate features based on their relevance to the brushwork classes. The selected features are utilized to generate several models to annotate the unlabelled patterns. The experts select the best performing model based on Vapnik combined bound. The transductive annotation using multiple experts out-performs the conventional baseline method in annotating patterns with brushwork classes.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"405991a8396f6815bcd0c04bf829846a49fb0dd3","venue_1":"ACM Multimedia","year":"2010","title":"Mining people's trips from large scale geo-tagged photos","authors":"Yuki Arase, Xing Xie, Takahiro Hara, Shojiro Nishio","author_ids":"3043844, 1687677, 1697569, 1717916","abstract":"Photo sharing is one of the most popular Web services. Photo sharing sites provide functions to add tags and geo-tags to photos to make photo organization easy. Considering that people take photos to record something that attracts them, geo-tagged photos are a rich data source that reflects people's memorable events associated with locations. In this paper, we focus on geo-tagged photos and propose a method to detect people's frequent trip patterns, i.e., typical sequences of visited cities and durations of stay as well as descriptive tags that characterize the trip patterns. Our method first segments photo collections into trips and categorizes them based on their trip themes, such as visiting landmarks or communing with nature. Our method mines frequent trip patterns for each trip theme category. We crawled 5.7 million geo-tagged photos and performed photo trip pattern mining. The experimental result shows that our method outperforms other baseline methods and can correctly segment photo collections into photo trips with an accuracy of 78%. For trip categorization, our method can categorize about 80% of trips using tags and titles of photos and visited cities as features. Finally, we illustrate interesting examples of trip patterns detected from our dataset and show an application with which users can search frequent trip patterns by querying a destination, visit duration, and trip theme on the trip.","cites":"38","conferencePercentile":"95.61643836"},{"venue":"ACM Multimedia","id":"a79171fbb5d0823434822c00ddefeec633ae272d","venue_1":"ACM Multimedia","year":"2005","title":"Image clustering with tensor representation","authors":"Xiaofei He, Deng Cai, Haifeng Liu, Jiawei Han","author_ids":"3945955, 1745280, 1678964, 1722175","abstract":"We consider the problem of image representation and clustering. Traditionally, an <i>n</i><inf>1</inf> x <i>n</i><inf>2</inf> image is represented by a vector in the Euclidean space &Ropf; <sup><i>n</i>1 x <i>n</i>2</sup>. Some learning algorithms are then applied to these vectors in such a high dimensional space for dimensionality reduction, classification, and clustering. However, an image is intrinsically a matrix, or the second order tensor. The vector representation of the images ignores the spatial relationships between the pixels in an image. In this paper, we introduce a tensor framework for image analysis. We represent the images as points in the tensor space <i>R</i><sup><i>n</i>1</sup> mathcal <i>R</i><sup><i>n</i>2</sup> which is a tensor product of two vector spaces. Based on the tensor representation, we propose a novel image representation and clustering algorithm which explicitly considers the manifold structure of the tensor space. By preserving the local structure of the data manifold, we can obtain a tensor subspace which is optimal for data representation in the sense of local isometry. We call it <b>TensorImage</b> approach. Traditional clustering algorithm such as <i>k</i>-means is then applied in the tensor subspace. Our algorithm shares many of the data representation and clustering properties of other techniques such as Locality Preserving Projections, Laplacian Eigenmaps, and spectral clustering, yet our algorithm is much more computationally efficient. Experimental results show the efficiency and effectiveness of our algorithm.","cites":"17","conferencePercentile":"72.27722772"},{"venue":"ACM Multimedia","id":"eb625982081899f9c33e551847bdb54fce7d6239","venue_1":"ACM Multimedia","year":"2010","title":"Overview of ACM international workshop on connected multimedia","authors":"Zhongfei Zhang, Zhengyou Zhang, Ramesh Jain, Yueting Zhuang","author_ids":"1720488, 1732465, 4521564, 1755711","abstract":"Following the very first international workshop on connected multimedia held in Hangzhou, China, in October of 2009 jointly sponsored by US National Science Foundation and Zhejiang University of China, this is the very first ACM International Workshop on Connected Multimedia in conjunction with ACM International Conference on Multimedia held in Florence, Italy, in October of 2010. In this workshop overview, we first define what we mean by connected multimedia, and then briefly overview the program of this workshop.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"36c5e799152c6301b103a30d02fd25ee677b6699","venue_1":"ACM Multimedia","year":"2005","title":"Semantic knowledge extraction and annotation for web images","authors":"Zhigang Hua, Xiang-Jun Wang, Qingshan Liu, Hanqing Lu","author_ids":"2435347, 2872806, 1806361, 1694235","abstract":"Nowadays, images have become widely available on the World Wide Web (WWW). It's essential to develop effective ways for managing and retrieving such abundant images. Advantageously, compared to the traditional images where very little information is provided, the web images contain plentiful context data. This paper introduces a system that can automatically acquire semantic knowledge for web image annotation. By using a page layout analysis method that can precisely assign context to web images, we developed efficient algorithms to extract semantic knowledge for web images, such as description, people, temporal and geographic information. To validate the practicality and efficiency of this system, we applied it to about 6,500 images crawled from Web. Experiments demonstrated that our approach could achieve satisfactory results.","cites":"13","conferencePercentile":"65.59405941"},{"venue":"ACM Multimedia","id":"bebedc824cc73d2c660c02a52a8e9e8046d0715a","venue_1":"ACM Multimedia","year":"2005","title":"Providing on-demand sports video to mobile devices","authors":"Qingshan Liu, Zhigang Hua, Cunxun Zang, Xiaofeng Tong, Hanqing Lu","author_ids":"1806361, 2435347, 2949003, 1807645, 1694235","abstract":"This paper introduces a system for providing on-demand sports video to mobile devices, which has two main contributions. First, we construct an infrastructure for extracting and delivering the highlights instead of the whole sport videos to mobile clients, which can significantly reduce the bandwidth consumption. Second, we design an advanced UI for the mobile clients to effectively browse and interact with the video highlights. To validate the practicality and effectiveness of this system, we conduct the experiments on several real soccer videos. The results demonstrated that more than 65% of bandwidth consumption could be reduced. Moreover, the initial user study results show that the mobile users could interact effectively with the interface to seek or navigate sports videos.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"a1ee141c75a98618dc67b1802d8495e07ca8e749","venue_1":"ACM Multimedia","year":"2010","title":"3D object retrieval with bag-of-region-words","authors":"Yue Gao, You Yang, Qionghai Dai, Naiyao Zhang","author_ids":"1744619, 2807392, 1713158, 2178247","abstract":"View-based method becomes an essential approach to 3D object retrieval in recent years. In the view-based 3D object retrieval framework, each object is described by a set of views and representative features are extracted from these views to match the objects in database. In this paper, we propose a novel 3D multi-view representation method, Bag-of-Region-Words (BoRW). It first gridly selects points in each view and extracts local SIFT features. Each local feature is encoded into a visual word with a trained visual vocabulary. Then each view is split into several regions, and each region is represented by a bag-of-visual-words feature vector. All the obtained regions are further grouped into clusters based on the bag-of-visual-words feature, and one feature is selected from each cluster with corresponding weight. In this way, each object is described by a set of BoRW. The Earth Movers Distance is employed to estimate the distance between two BoRW feature vectors. Experimental results show that the proposed method can achieve better retrieval performance than existing methods.","cites":"9","conferencePercentile":"77.39726027"},{"venue":"ACM Multimedia","id":"c3053bb7e01a2ca0d2bba1de7570a18b543ca2f4","venue_1":"ACM Multimedia","year":"1996","title":"Teaching and Learning as Multimedia Authoring: The Classroom 2000 Project","authors":"Gregory D. Abowd, Christopher G. Atkeson, Ami Feinstein, Cindy E. Hmelo-Silver, Rob Kooper, Sue Long, Nitin Nick Sawhney, Mikiya Tani","author_ids":"1732524, 8483722, 2869352, 2327983, 2270272, 2665825, 3178155, 1822616","abstract":"We view college classroom teaching and learning as a multimedia authoring activity. The classroom provides a rich setting in which a number of diierent forms of communication co-exist, such as speech, writing and projected images. Much of the information in a lecture is poorly recorded or lost currently. Our hypothesis is that tools to aid in the capture and subsequent access of classroom information will enhance both the learning and teaching experience. To test that hypothesis , we initiated the Classroom 2000 project at Georgia Tech. The purpose of the project is to apply ubiquitous computing technology to facilitate automatic capture, integration and access of multimedia information in the educational setting of the university classroom. In this paper, we discuss various prototype tools we have created and used in a variety of courses and provide an initial evaluation of the acceptance and eeectiveness of the technology. We also share some lessons learned in applying ubiquitous computing technology in a real setting .","cites":"164","conferencePercentile":"94.44444444"},{"venue":"ACM Multimedia","id":"2ab2881fdd3da827d8d5240c79934eb1e35ba103","venue_1":"ACM Multimedia","year":"2010","title":"Intelligent query: open another door to 3d object retrieval","authors":"Yue Gao, Meng Wang, Jialie Shen, Qionghai Dai, Naiyao Zhang","author_ids":"1744619, 1731598, 1723020, 1713158, 2178247","abstract":"The increasing number of available 3D objects makes their efficient retrieval technology highly desired. Extensive research has been dedicated to view-based 3D object retrieval because of its advantage of 2D views for 3D object content representation. In this paradigm, typically the retrieval is accomplished based a set of different views of the query object, and focuses on the 3D object representation, matching and indexing. In this work, we present another aspect towards 3D object retrieval: intelligent query. Intelligent query includes query selection, query description and combination, and assistive query. We will show how this scheme is ideally suit for the 3D object retrieval problem. We conduct experiments on the National Taiwan University 3D Model database and results demonstrated that our approach can improve retrieval performance. Finally, we give insight into the future of the intelligent query for 3D object retrieval.","cites":"7","conferencePercentile":"70.82191781"},{"venue":"ACM Multimedia","id":"45284176fbf72d53584a02336e10b07225d583cd","venue_1":"ACM Multimedia","year":"2010","title":"The wisdom of social multimedia: using flickr for prediction and forecast","authors":"Xin Jin, Andrew C. Gallagher, Liangliang Cao, Jiebo Luo, Jiawei Han","author_ids":"1785303, 1759673, 2464399, 1717319, 1722175","abstract":"Social multimedia hosting and sharing websites, such as Flickr, Facebook, Youtube, Picasa, ImageShack and Photobucket, are increasingly popular around the globe. A major trend in the current studies on social multimedia is using the social media sites as a source of huge amount of labeled data for solving large scale computer science problems in computer vision, data mining and multimedia. In this paper, we take a new path to explore the global trends and sentiments that can be drawn by analyzing the sharing patterns of uploaded and downloaded social multimedia. In a sense, each time an image or video is uploaded or viewed, it constitutes an implicit vote for (or against) the subject of the image. This vote carries along with it a rich set of associated data including time and (often) location information. By aggregating such votes across millions of Internet users, we reveal the wisdom that is embedded in social multimedia sites for social science applications such as politics, economics, and marketing. We believe that our work opens a brand new arena for the multimedia research community with a potentially big impact on society and social sciences.","cites":"36","conferencePercentile":"95.06849315"},{"venue":"ACM Multimedia","id":"6ed1a99f5bc6c4d21246573e572190d36e0ad9c9","venue_1":"ACM Multimedia","year":"2005","title":"uPen: laser-based, personalized, multi-user interaction on large displays","authors":"Xiaojun Bi, Yuanchun Shi, Xiaojie Chen, Peifeng Xiang","author_ids":"1682293, 1732440, 3187743, 1707918","abstract":"We present the uPen, a laser pointer combined with a contact-pushed switch, three press buttons and a wireless communication module. This novel interaction device allows users to interact on large displays at a distance or directly on the surface with full-function of mouse. Onboard software enable the uPen system to identify different users and provide personalized services to them, such as associating users with corresponding privileges, giving access to each participant's private content (e.g., home pages, personal calendars). Additionally, with our two-step association method, the uPen system has the ability to distinguish strokes of different uPens working simultaneously and support multi-user simultaneous interaction. A prototype system has been implemented in our Smart Classroom [1]. And user studies show the benefit of using it.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"223d4f4ddcd9d9bbb9ba1fcf65c2c79113c771ed","venue_1":"ACM Multimedia","year":"2014","title":"Admission Control for Wireless Adaptive HTTP Streaming: An Evidence Theory Based Approach","authors":"Zhisheng Yan, Chang Wen Chen, Bin Liu","author_ids":"2656592, 7137737, 1702756","abstract":"In this research, we propose an evidence theory based admission control scheme for wireless cellular adaptive HTTP streaming systems. This novel scheme allows us to effectively address the uncertainty and inaccuracy in QoE management and network estimation, and seamlessly grant or deny the access requests. Specifically, based on recent work of QoE continuum model and QoE continuum driven adaptation algorithm, we utilize Dempster-Shafer evidence theory to assign proper degree of belief to admission, rejection and an uncertainty decision for each user's evidence. We then can strategically combine the weighted evidence of multiple users and make the final decision. The evaluation results show that the proposed scheme can provide satisfactory QoE for both existing and new users while still achieving comparable bandwidth efficiency.","cites":"6","conferencePercentile":"82.93172691"},{"venue":"ACM Multimedia","id":"6ecdf6dd6b956e60eda60cfcf4fb94127ac157ee","venue_1":"ACM Multimedia","year":"2014","title":"Daily Stress Recognition from Mobile Phone Data, Weather Conditions and Individual Traits","authors":"Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex Pentland","author_ids":"8674717, 1776476, 2378406, 8029006, 1682773","abstract":"Research has proven that stress reduces quality of life and causes many diseases. For this reason, several researchers devised stress detection systems based on physiological parameters. However, these systems require that obtrusive sensors are continuously carried by the user. In our paper, we propose an alternative approach providing evidence that daily stress can be reliably recognized based on behavioral metrics, derived from the user's mobile phone activity and from additional indicators, such as the weather conditions (data pertaining to transitory properties of the environment) and the personality traits (data concerning permanent dispositions of individuals). Our multifactorial statistical model, which is person-independent, obtains the accuracy score of 72.28% for a 2-class daily stress recognition problem. The model is efficient to implement for most of multimedia applications due to highly reduced low-dimensional feature space (32d). Moreover, we identify and discuss the indicators which have strong predictive power.","cites":"19","conferencePercentile":"95.3815261"},{"venue":"ACM Multimedia","id":"37e69a72b3df1e685c14357d29dd758af7db6600","venue_1":"ACM Multimedia","year":"2012","title":"Leveraging high-level and low-level features for multimedia event detection","authors":"Lu Jiang, Alexander G. Hauptmann, Guang Xiang","author_ids":"1697318, 7661726, 8720304","abstract":"This paper addresses the challenge of Multimedia Event Detection by proposing a novel method for high-level and low-level features fusion based on collective classification. Generally, the method consists of three steps: training a classifier from low-level features; encoding high-level features into graphs; and diffusing the scores on the established graph to obtain the final prediction. The final prediction is derived from multiple graphs each of which corresponds to a high-level feature. The paper investigates two graph construction methods using logarithmic and exponential loss functions, respectively and two collective classification algorithms, i.e. Gibbs sampling and Markov random walk. The theoretical analysis demonstrates that the proposed method converges and is computationally scalable and the empirical analysis on TRECVID 2011 Multimedia Event Detection dataset validates its outstanding performance compared to state-of-the-art methods, with an added benefit of interpretability.","cites":"32","conferencePercentile":"97.78481013"},{"venue":"ACM Multimedia","id":"391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b","venue_1":"ACM Multimedia","year":"2014","title":"Deep Learning for Content-Based Image Retrieval: A Comprehensive Study","authors":"Ji Wan, Dayong Wang, Steven C. H. Hoi, Pengcheng Wu, Jianke Zhu, Yongdong Zhang, Jintao Li","author_ids":"2622533, 1980038, 1741126, 8327053, 1704030, 1699819, 8722263","abstract":"Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.","cites":"46","conferencePercentile":"99.59839357"},{"venue":"ACM Multimedia","id":"10981279e85561adde1ecedabf218603874a4c00","venue_1":"ACM Multimedia","year":"1997","title":"An Open Architecture for Comic Actor Animation","authors":"Knut Manske, Max Mühlhäuser","author_ids":"3144350, 1725964","abstract":"The multimedia rapture has held out hopes for advancements in user-centric computing. At the same time, however, there is a move towards autonomous software (cf. intelligent filters, mobile and distributed agents, etc.), leaving users with an uncomfortable lack of knowledge about and control over 'what these components are doing behind their backs'. Visualization of both autonomous agent action and user-agent interaction becomes a crucial issue if these conflicting trends are to be harmonized. We present a system service for comic actor animation, which can be used as a representation of agents of all kinds. A second use case is for rapid authoring of animations which augment multimedia presentations or off-the-shelf software. Our focus is on the reuse of the necessary artwork, using a modular and flexible building-block approach. As a preliminary step, this approach requires a set of elementary animation sequences to be created by a professional graphic artist, once per character. These sequences can be repeatedly combined in custom animated cartoons by easy-to-use commands at runtime. Our Comic Actor Editor Engine CAeditEngine uses a sophisticated approach for combining the elementary building blocks to form complete animations. Our Comic Actor Playing Engine CAplayEngine uses a digital chroma keying technique in combination with layering to display the animations on top of any graphical user interface and any interactive software. The system runs under MS Windows NT, a first version was used in a public interactive exhibit of multimedia and animation techniques and showed excellent performance.","cites":"2","conferencePercentile":"11.9047619"},{"venue":"ACM Multimedia","id":"38ad7b1a1573d2ed43b2c5cc4fb033513b8321f4","venue_1":"ACM Multimedia","year":"1994","title":"Towards Usability Guidelines for Multimedia Systems","authors":"Mike Bearne, Sara Jones, John Sapsford-Francis","author_ids":"3091615, 4097146, 2199198","abstract":"The advent of technology which supports the concurrent presentation of information through a range of different media has raised new issues relating to the design of usable systems. While previous work in the areas of both Human-Computer Interaction (HCI) and hypermedia system usability can contribute a considerable amount to the development of such guidelines, we believe that the use of multiple output media demands an understanding of particular characteristics and limitations of users' attentional capabilities. This paper presents some initial guidelines for the design of usable multimedia systems. These guidelines are based on empirical findings regarding the nature of human attention derived from the field of experimental psychology. We believe that the provision of such guidelines for multimedia interface design will support designers in achieving the dual goals of maximising a user's flexibility in controlling the presentation of multiple concurrent media, while keeping cognitive load to an acceptable level.","cites":"21","conferencePercentile":"59.3220339"},{"venue":"ACM Multimedia","id":"3f2c53ce715e00d0874535429feac715f2cfcdc7","venue_1":"ACM Multimedia","year":"2013","title":"Online multimodal deep similarity learning with application to image retrieval","authors":"Pengcheng Wu, Steven C. H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, Chunyan Miao","author_ids":"8327053, 1741126, 1751367, 1714894, 1980038, 1679209","abstract":"Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.","cites":"32","conferencePercentile":"98.66666667"},{"venue":"ACM Multimedia","id":"c90467f12b957c858e3823f4839dafa6c18132fa","venue_1":"ACM Multimedia","year":"2011","title":"WSM2011: third ACM workshop on social media","authors":"Steven C. H. Hoi, Michal Jacovi, Yiannis Kompatsiaris, Jiebo Luo, Konstantinos Tserpes","author_ids":"1741126, 2697312, 1906503, 1717319, 1771972","abstract":"The Third Workshop on Social Media (WSM2011) continues the series of Workshops on Social Media in 2009 and 2010 and has been established as a platform for the presentation and discussion of the latest, key research issues in social media analysis, exploration, search, mining, and emerging new social media applications. It is held in conjunction with the ACM International Multimedia Conference (MM'11) at Scottsdale, Arizona, USA, 2011 and has attracted contributions on various aspects of social media including data mining from social media, content organization, geo-localization, personalization, recommendation systems, user experience, machine learning and social media approaches and architectures for large-scale data processing.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"6da5bb8b75d2710d530acdf93ab7b88e1410593c","venue_1":"ACM Multimedia","year":"2015","title":"ImmersiveMe'15: 3rd ACM International Workshop on Immersive Media Experiences","authors":"Teresa Chambel, Paula Viana, V. Michael Bove, Sharon Strover, Graham Thomas","author_ids":"1679943, 2171703, 2746450, 2388999, 8234385","abstract":"This ACM International Workshop on Immersive Media Experiences is in its 3rd edition. Since 2013 in Barcelona, it has been a meeting point of researchers, students, media producers, service providers and industry players in the area of immersive media environments, applications and experiences. After the successful first edition at ACM Multimedia 2013 and the consolidation of the theme and the team at Orlando in 2014, ImmersiveMe'15 aims at bringing to the stage new ideas and developments that keep this topic as appealing as in the previous editions. ImmersiveMe'15 will now take place in Brisbane and, again, it will be a platform to present interesting and out-of-the-box new work that contributes to make the world more interactive, immersive and engaging.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"2dc6edd3efde43a89c134e5ade674890180553b7","venue_1":"ACM Multimedia","year":"2011","title":"TapTell: understanding visual intents on-the-go","authors":"Ning Zhang, Tao Mei, Xian-Sheng Hua, Ling Guan, Shipeng Li","author_ids":"1729811, 1788123, 1746102, 1748180, 4973820","abstract":"This demonstration presents a mobile-based visual recognition and recommendation application on Windows Phone 7 called TapTell. This is different from other mobile-based visual search mechanisms which merely focus on the search process. TapTell firstly discovers and understands users' visual intents via a circle based natural user interaction called \"O\" gestures. Following, a Tap action is operated to choose the \"O\" gestured regions. The context-aware visual search mechanism is utilized for recognizing the intents and associating them with indexed metadata. Finally, the \"Tell\" action recommends relevant entities utilizing contextual information. The TapTell system has been evaluated at different scenarios on million scale images.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"40f715dcb54c47a0384c2e6dcf859f7fe18726e9","venue_1":"ACM Multimedia","year":"2011","title":"SIRE: a social image retrieval engine","authors":"Steven C. H. Hoi, Pengcheng Wu","author_ids":"1741126, 8327053","abstract":"With the explosive growth of social media applications on the internet, billions of social images have been made available in many social media web sites nowadays. This has presented an open challenge of web-scale social image search. Unlike existing commercial web search engines that often adopt text based retrieval, in this demo, we present a novel web-based multimodal paradigm for large-scale social image retrieval, termed \"Social Image Retrieval Engine\" (SIRE), which effectively exploits both textual and visual contents to narrow down the semantic gap between high-level concepts and low-level visual features. A relevance feedback mechanism is also equipped to learn with user's feedback to refine the search results interactively. Our live demo is available at http://msm.cais.ntu.edu.sg/SIRE, and a video is available athttp://www.youtube.com/user/msmntu.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"7429375255ae22d3e3b0120f4d61b9397328c5e8","venue_1":"ACM Multimedia","year":"2010","title":"WSM'10: 2nd ACM workshop on social media","authors":"Susanne Boll, Steven C. H. Hoi, Roelof van Zwol, Jiebo Luo","author_ids":"1714281, 1741126, 1747340, 1717319","abstract":"The ACM SIGMM International Workshop on Social Media (WSM'10) is the second workshop held in conjunction with the ACM International Multimedia Conference (MM'10) at Firenze, Italy, 2010. This workshop provides a forum for researchers and practitioners from all over the world to share information on their latest investigations on social media analysis, exploration, search, mining, and emerging new social media applications.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"a90aeb96464b9cfd0261db76ff3d0561845e88e4","venue_1":"ACM Multimedia","year":"2009","title":"First ACM SIGMM international workshop onsocial media (WSM'09)","authors":"Susanne Boll, Steven C. H. Hoi, Jiebo Luo, Rong Jin, Irwin King, Dong Xu","author_ids":"1714281, 1741126, 1717319, 1718400, 1706259, 1714390","abstract":"The ACM SIGMM International Workshop on Social Media (WSM'09) is the first workshop held in conjunction with the ACM International Multimedia Conference (MM'09) at Bejing, P.R. China, 2009. This workshop provides a forum for researchers and practitioners from all over the world to share information on their latest investigations on social media analysis, exploration, search, mining, and emerging new social media applications.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"7cc7c5a6eed5d1821f64bfc5d3b9be03fbc92bd1","venue_1":"ACM Multimedia","year":"2009","title":"Distance metric learning from uncertain side information with application to automated photo tagging","authors":"Lei Wu, Steven C. H. Hoi, Rong Jin, Jianke Zhu, Nenghai Yu","author_ids":"1734221, 1741126, 1718400, 1704030, 1708598","abstract":"<i>Automated photo tagging</i> is essential to make massive unlabeled photos searchable by text search engines. Conventional image annotation approaches, though working reasonably well on small testbeds, are either computationally expensive or inaccurate when dealing with large-scale photo tagging. Recently, with the popularity of social networking websites, we observe a massive number of user-tagged images, referred to as \"<i>social images</i>\", that are available on the web. Unlike traditional web images, social images often contain tags and other user-generated content, which offer a new opportunity to resolve some long-standing challenges in multimedia. In this work, we aim to address the challenge of large-scale automated photo tagging by exploring the social images. We present a retrieval based approach for automated photo tagging. To tag a test image, the proposed approach first retrieves <i>k</i> social images that share the largest visual similarity with the test image. The tags of the test image are then derived based on the tagging of the similar images. Due to the well-known semantic gap issue, a regular Euclidean distance-based retrieval method often fails to find semantically relevant images. To address the challenge of semantic gap, we propose a novel <i>probabilistic distance metric learning</i> scheme that (1) automatically derives constraints from the uncertain side information, and (2) efficiently learns a distance metric from the derived constraints. We apply the proposed technique to automated photo tagging tasks based on a social image testbed with over 200,000 images crawled from Flickr. Encouraging results show that the proposed technique is effective and promising for automated photo tagging.","cites":"45","conferencePercentile":"96.48760331"},{"venue":"ACM Multimedia","id":"58632b421de0d3d4214f3e6935351775783e0307","venue_1":"ACM Multimedia","year":"2008","title":"Near-duplicate keyframe retrieval by nonrigid image matching","authors":"Jianke Zhu, Steven C. H. Hoi, Michael R. Lyu, Shuicheng Yan","author_ids":"1704030, 1741126, 1681775, 1698982","abstract":"Near-duplicate image retrieval plays an important role in many real-world multimedia applications. Most previous approaches have some limitations. For example, conventional appearance-based methods may suffer from the illumination variations and occlusion issue, and local feature correspondence-based methods often do not consider local deformations and the spatial coherence between two point sets. In this paper, we propose a novel and effective Nonrigid Image Matching (NIM) approach to tackle the task of near-duplicate keyframe retrieval from real-world video corpora. In contrast to previous approaches, the NIM technique can recover an explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data effectively. To make our technique applicable to large-scale applications, we suggest an effective multi-level ranking scheme that filters out the irrelevant results in a coarse-to-fine manner. In our ranking scheme, to overcome the extremely small training size challenge, we employ a semi-supervised learning method for improving the performance using unlabeled data. To evaluate the effectiveness of our solution, we have conducted extensive experiments on two benchmark testbeds extracted from the TRECVID2003 and TRECVID2004 corpora. The promising results show that our proposed method is more effective than other state-of-the-art approaches for near-duplicate keyframe retrieval.","cites":"49","conferencePercentile":"95.87155963"},{"venue":"ACM Multimedia","id":"269e4b325ae331b7ddba491dbe34a2164edfd627","venue_1":"ACM Multimedia","year":"2004","title":"A novel log-based relevance feedback technique in content-based image retrieval","authors":"Steven C. H. Hoi, Michael R. Lyu","author_ids":"1741126, 1681775","abstract":"Relevance feedback has been proposed as an important technique to boost the retrieval performance in content-based image retrieval (CBIR). However, since there exists a semantic gap between low-level features and high-level semantic concepts in CBIR, typical relevance feedback techniques need to perform a lot of rounds of feedback for achieving satisfactory results. These procedures are time-consuming and may make the users bored in the retrieval tasks. For a long-term study purpose in CBIR, we notice that the users' feedback logs can be available and employed for helping the retrieval tasks in CBIR systems. In this paper, we propose a novel scheme to study the log-based relevance feedback (LRF) technique for improving retrieval performance and reducing the semantic gap in CBIR. In order to effectively incorporate the users' feedback logs, we propose a modified support vector machine (SVM) technique called soft label support vector machine (SLSVM) to construct the LRF algorithm in CBIR. We conduct extensive experiments to evaluate the performance of our proposed algorithm. Compared with the typical approach using query expansion (QEX) technique, we demonstrate that our proposed scheme can significantly improve the retrieval performance of semantic image retrieval from detailed experiments.","cites":"51","conferencePercentile":"90.68627451"},{"venue":"ACM Multimedia","id":"f7f2396d1fb55db32719954c744d61a78c4f3378","venue_1":"ACM Multimedia","year":"2010","title":"ReDi: an interactive virtual display system for ubiquitous devices","authors":"Wen Sun, Yan Lu, Shipeng Li","author_ids":"4537536, 1724084, 4973820","abstract":"In this paper, we present an interactive virtual display system to facilitate the ubiquitous user interaction with heterogeneous devices. By using small-size programmable hardware and wearable sensors, any display device (referred to as display surface) can act as a thin client for users to interact with the different remote devices. Under a flexible system architecture for local and remote devices' communication and collaboration, several techniques, such as adaptive screen compression, interactive ROI control, and accelerometer-based pointing input, are developed to improve the system performance and user experience. Evaluations show that the proposed system can efficiently utilize the remote computing resources and local display capabilities of ubiquitous devices, which will greatly benefit interactive multimedia applications.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"dc6153f154fa9b3b556f83e0b77883785bdc250f","venue_1":"ACM Multimedia","year":"2014","title":"ImmersiveMe'14: 2nd ACM international workshop on immersive media experiences","authors":"Teresa Chambel, Paula Viana, V. Michael Bove, Sharon Strover, Graham Thomas","author_ids":"1679943, 2171703, 2746450, 2388999, 8234385","abstract":"The 2nd ACM International Workshop on Immersive Media Experiences (ImmersiveMe'14) at ACM Multimedia aims at bringing together researchers, students, media producers, service providers and industry players in the emergent area of immersive media experiences, through the exploration of different scenarios, applications, and neighboring fields. This second edition, after a successful first edition at ACM Multimedia 2013, provides a platform for presenting on-going work, to consolidate and tie different research communities working on this engaging area, as well as to point directions for the future.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"820a49658b83b77b110f5b8738bc2c0e1ed61c97","venue_1":"ACM Multimedia","year":"2013","title":"Immersive media experiences: immersiveme 2013 workshop at ACM multimedia","authors":"Teresa Chambel, V. Michael Bove, Sharon Strover, Paula Viana, Graham Thomas","author_ids":"1679943, 2746450, 2388999, 2171703, 8234385","abstract":"Immersive media has the potential for strong impact on users' emotions and their sense of presence and engagement. The main objective of this workshop is to bring together researchers, students, media producers, service providers and industry players in the area of emergent immersive media. The workshop will provide a platform for a deep discussion on ongoing work, recent achievements and experiences. It is expected not only to consolidate experiences but also to identify aspects where strong collaboration among all the interested players is needed and to point towards future working directions.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"369c8a65ef7d9dcb14c962267b8b8c15bf92f215","venue_1":"ACM Multimedia","year":"2006","title":"Automatic image annotation by incorporating feature hierarchy and boosting to scale up SVM classifiers","authors":"Yuli Gao, Jianping Fan, Xiangyang Xue, Ramesh Jain","author_ids":"2026151, 4687732, 5507458, 4521564","abstract":"The performance of image classifiers largely depends on two inter-related issues:(1)suitable frameworks for image content representation and automatic feature extraction;(2) effective algorithms for image classifier training and feature subset selection. To address the first issue, a multiresolution grid-based framework is proposed for image content representation and feature extraction to bypass the time-consuming and erroneous process for image segmentation. To address the second issue, a hierarchical boosting algorithm is proposed by incorporating feature hierarchy and boosting to scale up SVM image classifier training in high-dimensional feature space. The high-dimensional multi-modal heterogeneous visual features are partitioned into multiple low-dimensional single-modal homogeneous feature subsets and each of them characterizes certain visual property of images. For each homogeneous feature subset, principal component analysis (PCA)is performed to exploit the feature correlations and a weak classifier is learned simultaneously. After the weak classifiers for different feature subsets and grid sizes are available, they are combined to boost an optimal classifier for the given object class or image concept, and the most representative feature subsets and grid sizes are selected. Our experiments on a specific domain of natural images have obtained very positive results.","cites":"36","conferencePercentile":"90.15544041"},{"venue":"ACM Multimedia","id":"5106c9cee5928283dd21e612805a79a92caafa91","venue_1":"ACM Multimedia","year":"2010","title":"Determining the sexual identities of prehistoric cave artists using digitized handprints: a machine learning approach","authors":"James Ze Wang, Weina Ge, Dean R. Snow, Prasenjit Mitra, C. Lee Giles","author_ids":"1699550, 1713529, 2347878, 1714911, 1749125","abstract":"The sexual identities of human handprints inform hypotheses regarding the roles of males and females in prehistoric contexts. Sexual identity has previously been manually determined by measuring the ratios of the lengths of the individual's fingers as well as by using other physical features. Most conventional studies measure the lengths manually and thus are often constrained by the lack of scaling information on published images. We have created a method that determines sex by applying modern machine-learning techniques to relative measures obtained from images of human hands. This is the known attempt at substituting automated methods for time-consuming manual measurement in the study of sexual identities of prehistoric cave artists. Our study provides quantitative evidence relevant to sexual dimorphism and the sexual division of labor in Upper Paleolithic societies. In addition to analyzing historical handprint records, this method has potential applications in criminal forensics and human-computer interaction.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"62d46bb4e0067773a275e30ef140ed9aa1fd7a61","venue_1":"ACM Multimedia","year":"2016","title":"Detecting Arbitrary Oriented Text in the Wild with a Visual Attention Model","authors":"Wenyi Huang, Dafang He, Xiao Yang, Zihan Zhou, Daniel Kifer, C. Lee Giles","author_ids":"1869202, 3493665, 1777526, 2519795, 1852261, 1749125","abstract":"Text embedded in images provides important semantic information about a scene and its content. Detecting text in an unconstrained environment is a challenging task because of the many fonts, sizes, backgrounds, and alignments of the characters. We present a novel attention model for detecting arbitrary oriented and curved scene text. Inspired by the attention mechanisms in the human visual system, our model utilizes a spatial glimpse network to processes the attended area and deploys a recurrent neural network that aggregates the information over time to determine the attention movement. Combining this with an off-the-shelf region proposal method, the model achieves the state-of-the-art performance on the highly cited ICDAR2013 dataset, and the MSRA-TD500 dataset which contains arbitrary oriented text.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"5a0d9333af1b4af1dc1a3041d4ab7d0aab595843","venue_1":"ACM Multimedia","year":"2000","title":"Automating the linking of content and concept","authors":"Robert Tansley, Colin Bird, Wendy Hall, Paul H. Lewis, Mark J. Weal","author_ids":"2709391, 2099999, 1685385, 1773066, 2806972","abstract":"In previous work we have described a multimedia system, MAVIS 2, supporting content and concept based retrieval and navigation. A central component of the system is a multimedia thesaurus in which media content is associated with appropriate concepts in a semantic layer. A major challenge is identifying and constructing these associations in a particular application without requiring a huge amount of manual effort. In this paper we propose a two phase approach to the problem. In the first phase, latent semantic analysis is used to associate metadata available for some media objects with concept class descriptions. This facilitate automatic associations to be made with the concept layer of the those media objects. In the second phase, media content matching is used to classify media objects without metadata through their similarity to media objects classified in phase 1.","cites":"16","conferencePercentile":"73.91304348"},{"venue":"ACM Multimedia","id":"8f9fcf7f43a5d7f668807efbb2a6770d78498a6a","venue_1":"ACM Multimedia","year":"2012","title":"Enabling 'togetherness' in high-quality domestic video","authors":"Ian Kegel, Pablo César, Jack Jansen, Dick C. A. Bulterman, Tim Stevens, Joke Kort, Nikolaus Färber","author_ids":"2883994, 1743507, 1724349, 1726923, 2461563, 8627284, 1745598","abstract":"Low-cost video conferencing systems have provided an existence proof for the value of video communication in a home setting. At the same time, current systems have a number of fundamental limitations that inhibit more general social interactions among multiple groups of participants. In our work, we describe the development, implementation and evaluation of a domestic video conferencing system that is geared to providing true 'togetherness' among conference participants. We show that such interactions require sophisticated support for high-quality audiovisual presentation, and processing support for person identification and localisation. In this paper, we describe user requirements for effective interpersonal interaction. We then report on a system that implements these requirements. We conclude with a systems and user evaluation of this work. We present results that show that participants in a video conference can be made feel as 'together' as collocated players of a board game.","cites":"8","conferencePercentile":"81.48734177"},{"venue":"ACM Multimedia","id":"33b6d42c0553ced7648046808b425dc21fe2b5b3","venue_1":"ACM Multimedia","year":"2013","title":"GeSoDeck: a geo-social event detection and tracking system","authors":"Xingyu Gao, Juan Cao, Zhiwei Jin, Xin Li, Jintao Li","author_ids":"1732399, 7468114, 2275324, 3375045, 8722263","abstract":"This demonstration presents a novel geo-social event detection and tracking system based on geographical pattern mining and content analysis, called \"GeSoDeck\". A user can capture what events happened by our system. Unlike most existing social event detection applications, GeSoDeck aims to detect events with high accuracy and efficiency, and track them as well. Given a geographical area, the system can not only detect diverse social events in this area using the geographical pattern mining and density-based K-means clustering, but also track the representative tweets of the detected event in real time, mining geographical diffusion trajectory on the map and temporal pattern of retweeting process. On a realistic dataset collected from Sina Weibo, the system can outperform the state-of-the-art methods.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"36c67609402dbf0da3731658e9d1fc87147859c0","venue_1":"ACM Multimedia","year":"2004","title":"Application of packet assembly technology to digital video and VoIP","authors":"Toshikatsu Kanda, Kazunori Shimamura","author_ids":"3165666, 2014282","abstract":"The Internet is composed of many kinds of networks and the networks are composed of network nodes such as routers. Routers use processor power for forwarding each packet with any size. At that time, node processor would be a bottleneck in respect to the high throughput if there would be too many packets to forward. Then, authors propose the packet assembly method. This aims to decrease the number of packets for the reduction of processor load, based on the fact that there are many packets much smaller than maximum transferable unit in backbone network.\n For the examination of the packet assembly, authors conducted two experiments. One is the experiment that conducts the packet assembly method for the traffic of digital video, and it provides the comparison of the image of digital video forwarded via routers without packet assembly with the one with packet assembly, and transition of edge router load and core router load. The other is the experiment that conducts the packet assembly method for the traffic of VoIP, and investigated about the influence on PSQM score, latency, and jitter.","cites":"0","conferencePercentile":"7.107843137"},{"venue":"ACM Multimedia","id":"751f078c7f4a11eb5ea2e98d8a2ebdf28c85b925","venue_1":"ACM Multimedia","year":"2000","title":"Mediacaptain - a Demo","authors":"Florian Mueller","author_ids":"1710544","abstract":"The mediacaptain is a system that facilitates indexing, browsing, summarizing and retrieval of video, on the Web with the support of supplementary material.\nThe demo is available over the Web at htt://www.mediacaptain.com. It is a presentation about the mediacaptain using the features of the mediacaptain.\nIn order to experience all the possibilities of the mediacaptain, it is advisable to watch the video at the given URL and make use of the provided functionality.","cites":"0","conferencePercentile":"7.065217391"},{"venue":"ACM Multimedia","id":"292c5012913b77250b2a241abc574bc99a155997","venue_1":"ACM Multimedia","year":"2000","title":"Mediacaptain - an interface for browsing streaming media","authors":"Florian Mueller","author_ids":"1710544","abstract":"The increase of bandwidth and streaming technology has made video on the Web the current &#8220;killer-app&#8221; of the dot-com world. However, users still face many problems. Users have to find the right video and the right segment within the video. Locally stored files provide easy (but still not very sophisticated) access to individual points in the video by utilizing a seek slider. If the video is streamed over the Internet, this slider loses much of its attraction. Every accessed point in the video requires the video player to buffer, which causes a time lag.\nThe mediacaptain is a system that addresses this issue by using supplementary material like text and graphics to provide indices. This time-aligned material is used to help the user make an informed decision on whether they want to watch a video and if so, what portions. This web-enabled prototype called mediacaptain emerged from user surveys and is demonstrated on several content types and represents an advanced experience with video on the Web.","cites":"0","conferencePercentile":"7.065217391"},{"venue":"ACM Multimedia","id":"22867087cc3e1698c9ff42786eade0d2d40416c0","venue_1":"ACM Multimedia","year":"1997","title":"An Evaluation of VBR Disk Admission Algorithms for Continuous Media File Servers","authors":"Dwight J. Makaroff, Gerald W. Neufeld, Norman C. Hutchinson","author_ids":"2678854, 1794384, 2925533","abstract":"In this paper, we address the problem of choosing a disk admission algorithm for continuous media streams where each stream may have a different bit rate, and more importantly, where the bit rate within a single stream may vary considerably. We evaluate several different Variable Bit Rate (VBR) disk admission control algorithms for continuous media. An algorithm which accepts too few streams under-utilizes the server resources, while an algorithm which accepts too many streams over-utilizes the resources resulting in inadequate service (i.e. missing or delayed data) to the clients. The evaluation process is based on a representative set of video streams encoded in MJPEG. We conclude that one particular algorithm, the VBR simulation algorithm, performs the best among realizable algorithms in terms of system utilization and delivery guarantees and performs close to an optimal algorithm,","cites":"12","conferencePercentile":"36.9047619"},{"venue":"ACM Multimedia","id":"3dc6405bfec3f1050b5386049f04f3cb987aaf37","venue_1":"ACM Multimedia","year":"2006","title":"Semi-supervised annotation of brushwork in paintings domain using serial combinations of multiple experts","authors":"Liza Leslie, Tat-Seng Chua, Ramesh Jain","author_ids":"2265789, 1684968, 4521564","abstract":"Many recent studies perform annotation of paintings based on brushwork. They model the brushwork indirectly as part of annotation of high-level artistic concepts such as artist name using low-level texture features and supervised inference methods. In this paper, we develop a framework for explicit annotation of paintings with brushwork classes. Brushwork classes serve as meta-level semantic concepts for artist names, paintings styles and periods of art and facilitate the incorporation of domain-specific ontologies. In particular, we employ the serial multi-expert framework with semi-supervised clustering methods to perform the annotation of brushwork patterns. Serial combination of multiple experts facilitates step-wise refinement of decisions based on the preferences of individual experts. Each individual expert performs focused subtasks using relevant feature set, which decreases the 'curse of dimensionality' and noise in the feature space. Each expert focuses on the annotation of the currently available samples from its unlabeled pool using semi-supervised agglomerative clustering. This approach is more appropriate as compared to the traditional classification methods since each brushwork class includes a variety of patterns and cannot be represented as a single distribution in the feature space. The experts exploit the distribution of unlabelled patterns and further minimize the annotation error. The multi-expert semi-supervised framework out-performs the conventional methods in annotation of patterns with brushwork classes. This framework will further be adopted to facilitate ontology-based annotation with higher-level semantic concepts such as the artist names, painting styles and periods of art.","cites":"18","conferencePercentile":"82.38341969"},{"venue":"ACM Multimedia","id":"e0aebea2a9a95895c3ebe1192c272356627c05a8","venue_1":"ACM Multimedia","year":"2010","title":"Living wall: programmable wallpaper for interactive spaces","authors":"Leah Buechley, David Mellis, Hannah Perner-Wilson, Emily Lovell, Bonifaz Kaufmann","author_ids":"1742691, 2643525, 1700601, 1712410, 1703220","abstract":"The Living Wall project explores the construction and application of interactive wallpaper. Using conductive, resistive, and magnetic paints we produced wallpaper that enables us to create dynamic, reconfigurable, programmable spaces. The wallpaper consists of circuitry that is painted onto a sheet of paper and a set of electronic modules that are attached to it with magnets. The wallpaper can be used for a multitude of functional and fanciful applications involving lighting, environmental sensing, appliance control, and ambient information display.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"1c8c20156fc85a9fd65d31494e6bca0bf225e50d","venue_1":"ACM Multimedia","year":"2011","title":"Contextual image search","authors":"Wenhao Lu, Jingdong Wang, Xian-Sheng Hua, Shengjin Wang, Shipeng Li","author_ids":"2282045, 1688516, 1746102, 1678689, 4973820","abstract":"In this paper, we propose a novel image search scheme, <i>contextual image search</i>. Different from conventional image search schemes that present a separate interface (e.g., text input box) to allow users to submit a query, the new search scheme enables users to search images by only masking a few words when they are reading through Web pages or other documents. Rather than merely making use of the explicit query input that is often not sufficient to express user's search intent, our approach explores the context information to better understand the search intent with two key steps: query augmenting and search results reranking using context, and expects to obtain better search results. Beyond contextual Web search, the context in our case is much richer and includes images besides texts. In addition to this type of search scheme, called <i>contextual image search with text input</i>, we also present another type of scheme, called <i>contextual image search with image input</i>, to allow users to select an image as the search query from Web pages or other documents they are reading. The key idea is to use the search-to-annotation technique and the contextual textual query mining scheme to determine the corresponding textual query, to finally get semantically similar search results. Experiments show that the proposed schemes make image search more convenient and the search results are more relevant to user intention.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"0134d7b44374c4110c18190f50e8e6904d65da25","venue_1":"ACM Multimedia","year":"2004","title":"Privacy protecting data collection in media spaces","authors":"Jehan Wickramasuriya, Mahesh Datt, Sharad Mehrotra, Nalini Venkatasubramanian","author_ids":"3243801, 2785610, 1686199, 1732742","abstract":"Around the world as both crime and technology become more prevalent, officials find themselves relying more and more on video surveillance as a cure-all in the name of public safety. Used properly, video cameras help expose wrongdoing but typically come at the cost of privacy to those not involved in any maleficent activity. What if we could design intelligent systems that are more selective in what video they capture, and focus on anomalous events while protecting the privacy of authorized personnel? This paper proposes a novel way of combining sensor technology with traditional video surveillance in building a privacy protecting framework that exploits the strengths of these modalities and complements their individual limitations. Our fully functional system utilizes off the shelf sensor hardware (i.e. RFID, motion detection) for localization, and combines this with a XML-based policy framework for access control to determine violations within the space. This information is fused with video surveillance streams in order to make decisions about how to display the individuals being surveilled. To achieve this, we have implemented several video masking techniques that correspond to varying user privacy levels. These results were achievable in real-time at acceptable frame rates, while meeting our requirements for privacy preservation.","cites":"68","conferencePercentile":"93.38235294"},{"venue":"ACM Multimedia","id":"5cf0f4f464608f8603361e689c81d9573e6d24b0","venue_1":"ACM Multimedia","year":"2011","title":"Video-based image retrieval","authors":"Linjun Yang, Yang Cai, Alan Hanjalic, Xian-Sheng Hua, Shipeng Li","author_ids":"7866194, 1801727, 6741141, 1746102, 4973820","abstract":"Likely variations in the capture conditions (e.g. light, blur, scale, occlusion) and in the viewpoint between the query image and the images in the collection are the factors due to which image retrieval based on the Query-by-Example (QBE) principle is still not reliable enough. In this paper, we propose a novel QBE-based image retrieval system where users are allowed to submit a short video clip as a query to improve the retrieval reliability. Improvement is achieved by integrating the information about different viewpoints and conditions under which object and scene appearances can be captured across different video frames. Rich information extracted from a video can be exploited to generate a more complete query representation than in the case of a single-image query and to improve the relevance of the retrieved results. Our experimental results show that video-based image retrieval (VBIR) is significantly more reliable than the retrieval using a single image as a query.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"1a44b31512bda7c69258b570942ba0df60b1d196","venue_1":"ACM Multimedia","year":"1993","title":"Panoramic Overviews for Navigating Real-World Scenes","authors":"Laura Teodosio, Michael Mills","author_ids":"1765546, 2236051","abstract":"This paper describes an interface which helps people maintain a sense of spatial context while navigating virtual real-world scenes. First, a single panoramic image of the entire space is constructed from the separate partial, but detailed, images which constitute the original video sampling of the scene. The user can then navigate through this real-world data by manipulating either the panoramic overview or the original detailed views appearing in a separate window. Clicking or dragging the cursor over regions in the panoramic overview updates the corresponding detailed view. Using the panorama in this way frees the user from the traditional linear modes of interacting with virtual real-word scenes. In addition, interacting with the detailed view highlights the corresponding region in the panoramic overview and leaves a \"trail\" of the user's path through the space. These methods of visualizing and interacting with digital video described in this paper can also be applied to collections of digital video which do not correspond to a physical space such as standard linear movies.","cites":"17","conferencePercentile":"39.53488372"},{"venue":"ACM Multimedia","id":"c9896769f5405658a0cc925b92b30399e199b512","venue_1":"ACM Multimedia","year":"2010","title":"A proxy-based mobile web browser","authors":"Huifeng Shen, Zhaotai Pan, Haicheng Sun, Yan Lu, Shipeng Li","author_ids":"1694963, 2672744, 2595960, 1724084, 4973820","abstract":"In this paper, we present a proxy-based mobile web browser with rich experiences. We use the server-side web parsing and rendering to leverage the browser computing logic. We use a composite screen format to represent the display of the web content, incorporating the web background screen and the dynamic web objects. And then we employ a slice-based screen encoding scheme to efficiently compress the web background screen. Besides the display screen of the web content, we also send the side information of the web objects to enable the designed object-level interaction mechanisms. The experimental results show that our browser can achieve the superior browsing speed, compared with the native browser and yield much better visual quality than the existing proxy-based browser","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"940e9d8db5e5cd7ba408583f0652e114f0729f4d","venue_1":"ACM Multimedia","year":"2010","title":"Mobile document scanning and copying","authors":"Jian Fan, Qian Lin, Jerry Liu","author_ids":"1762999, 5683660, 2040965","abstract":"In this paper, we show a multimedia system for processing mobile camera captured documents. Using a client application on a mobile phone, a user can capture a document image, and send the image to a processing server so that the document image can be restored using automatic perspective and illumination corrections. The restored document can then be sent to a web-connected printer to complete the copying task.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"0c2c53d71942ad3171b693f565812f1db43215e0","venue_1":"ACM Multimedia","year":"2009","title":"Descriptive visual words and visual phrases for image applications","authors":"Shiliang Zhang, Qi Tian, Gang Hua, Qingming Huang, Shipeng Li","author_ids":"1776581, 1724745, 1745420, 1689702, 4973820","abstract":"The Bag-of-visual Words (BoW) image representation has been applied for various problems in the fields of multimedia and computer vision. The basic idea is to represent images as visual documents composed of repeatable and distinctive visual elements, which are comparable to the words in texts. However, massive experiments show that the commonly used visual words are not as expressive as the text words, which is not desirable because it hinders their effectiveness in various applications. In this paper, Descriptive Visual Words (DVWs) and Descriptive Visual Phrases (DVPs) are proposed as the visual correspondences to text words and phrases, where visual phrases refer to the frequently co-occurring visual word pairs. Since images are the carriers of visual objects and scenes, novel descriptive visual element set can be composed by the visual words and their combinations which are effective in representing certain visual objects or scenes. Based on this idea, a general framework is proposed for generating DVWs and DVPs from classic visual words for various applications. In a large-scale image database containing 1506 object and scene categories, the visual words and visual word pairs descriptive to certain scenes or objects are identified as the DVWs and DVPs. Experiments show that the DVWs and DVPs are compact and descriptive, thus are more comparable with the text words than the classic visual words. We apply the identified DVWs and DVPs in several applications including image retrieval, image re-ranking, and object recognition. The DVW and DVP combination outperforms the classic visual words by 19.5% and 80% in image retrieval and object recognition tasks, respectively. The DVW and DVP based image re-ranking algorithm: DWPRank outperforms the state-of-the-art VisualRank by 12.4% in accuracy and about 11 times faster in efficiency.","cites":"118","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"0111300ff8d9a0d6717f2b473bb8338df5bd6872","venue_1":"ACM Multimedia","year":"2014","title":"Quality-adaptive Prefetching for Interactive Branched Video using HTTP-based Adaptive Streaming","authors":"Vengatanathan Krishnamoorthi, Niklas Carlsson, Derek L. Eager, Anirban Mahanti, Nahid Shahmehri","author_ids":"2020829, 1739691, 1736203, 1701370, 1723372","abstract":"Interactive branched video that allows users to select their own paths through the video, provides creative content designers with great personalization opportunities; however, such video also introduces significant new challenges for the system developer. For example, without careful prefetching and buffer management, the use of multiple alternative playback paths can easily result in playback interruptions. In this paper, we present a full implementation of an interactive branched video player using HTTP-based Adaptive Streaming (HAS) that provides seamless playback even when the users defer their branch path choices to the last possible moment. Our design includes optimized prefetching policies that we derive under a simple optimization framework, effective buffer management of prefetched data, and the use of parallel TCP connections to achieve efficient buffer workahead. Through performance evaluation under a wide range of scenarios, we show that our optimized policies can effectively prefetch data of carefully selected qualities along multiple alternative paths such as to ensure seamless playback, offering users a pleasant viewing experience without playback interruptions.","cites":"5","conferencePercentile":"80.72289157"},{"venue":"ACM Multimedia","id":"8a45ccdba0e60a3dfb34f90271358e8af3178af7","venue_1":"ACM Multimedia","year":"1995","title":"A Generalized Admissions Control Strategy for Heterogeneous, Distributed Multimedia Systems","authors":"Saurav Chatterjee, Jay K. Strosnider","author_ids":"1726894, 1997563","abstract":"This paper presents a generalized admissions control strategy for providing timing guarantees to multimedia applications executing over a set of distributed, heterogeneous system resources. This paper illustrates complications that arise in moving from resource-specific to generalized admissions control and introduces a strategy that can be used to solve some of these problems. Key elements of this Distributed Pipeline Admissions Control Strategy include (i) a resource-independent model for representing multimedia applications requiring access to an heterogeneous set of system resources, (ii) an uniform model for representing a set of heterogeneous system resources, (iii) a real-time heterogeneous resource allocation and routing algorithm, (iv) distributed pipeline scheduling policies that result in efficient and predictable resource usage by clients, and (v) a divide-and-conquer timing analysis technique for ascertaining whether client timing requirements are met. An audio/video example is provided to illustrate the application of this approach.","cites":"19","conferencePercentile":"42.5"},{"venue":"ACM Multimedia","id":"33a766ce3c711acf564f1c364a0763042c87a9d3","venue_1":"ACM Multimedia","year":"2006","title":"Interactive digital television and multimedia systems","authors":"Pablo César, Konstantinos Chorianopoulos","author_ids":"1743507, 1734991","abstract":"Interactive digital television is an emerging field with a high impact in our societies: it offers interactive services to the masses. This tutorial aims to establish a common framework by summarizing the most significant results in this multidisciplinary field. The review includes topics such as content distribution, system software of the receivers, and user interaction. In addition, we will discuss current commercial events such as the next generation of optical discs (e.g., blue-ray), BBC peer-to-peer service, and mobile television. Based on this discussion, we will formulate an agenda for further research. The agenda includes, for example, end-user enrichment of television content and social television. This half-day tutorial will provide the attendee a solid understanding of the technologies currently in use and an introduction of the open questions in the field.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"d03b17b73c529f924cb824bb3eea01537e31b88b","venue_1":"ACM Multimedia","year":"2006","title":"Variations 10b: a digital realization of cage's variations II","authors":"Nicholas Knouf","author_ids":"3074831","abstract":"Beginning in the middle of the twentieth century, composers of experimental music developed a number of new notational representations, most often falling under the category of <i>graphical scores</i>. John Cage's <i>Variations II</i> is a prime example, utilizing only dots and lines as its basis. I describe an interactive version of Cage's piece, called here <i>Variations 10b</i>, where a performer can change the score and receive immediate auditory feedback as to the results of the manipulation. This stands in contrast to the process of working through the analog score, where the aural output was not coincident with movement of the dots or lines. I suggest that creating and using digital versions of these early experimental music works radically changes the process of interacting with the pieces.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"37ef964720846b1ea6dcb0180d16c7a4b20a1e14","venue_1":"ACM Multimedia","year":"2007","title":"Annotation of paintings with high-level semantic concepts using transductive inference and ontology-based concept disambiguation","authors":"Liza Leslie, Tat-Seng Chua, Ramesh Jain","author_ids":"2265789, 1684968, 4521564","abstract":"Domain-specific knowledge of paintings defines a wide range of concepts for annotation and flexible retrieval of paintings. In this work, we employ the ontology of artistic concepts that includes visual (or atomic) concepts at the intermediate level and high-level concepts at the application level. Visual-level color and brushwork concepts are widely used by art historians to analyze paintings and serve as cues for annotating high-level concepts such as the artist names, painting styles and art periods for paintings. In this research we combine the color and brushwork concepts with low-level features and utilize the transductive inference framework to annotate high-level concepts to the image blocks. In order to resolve conflicting assignments of high-level concepts, we further employ the ontology-based concept disambiguation method and generate image-level annotations. This method performs global optimization of the block-level annotations using the linear constraints extracted from domain knowledge. Our experiments on annotating high-level concepts demonstrate that: a) the use of visual-level concepts significantly improves the accuracy as compared to using low-level features only; and b) the proposed transductive inference framework out-performs the conventional baseline methods and c) the proposed ontology-based disambiguation method generates superior results for several annotation scenarios.","cites":"13","conferencePercentile":"72.13541667"},{"venue":"ACM Multimedia","id":"75ea7e03c8403e5524f596011d1d3910adae39c6","venue_1":"ACM Multimedia","year":"1993","title":"Automatic Temporal Layout Mechanisms","authors":"M. Cecelia Buchanan, Polle Zellweger","author_ids":"2278953, 1877101","abstract":"A traditional static document has a spatial layout that indicates where objects in the document appear. Because multimedia documents incorporate time, they also require a temporal layout, or schedule, that indicates when events in the document occur. This paper argues that multimedia document systems should provide mechanisms for automatically producing temporal layouts for documents. The major advantage of this approach is that it makes it easier for authors to create and modify multimedia documents. This paper constructs a framework for understanding automatic temporal formatters and explores the basic issues surrounding them. It also describes the Firefly multimedia document system, which has been developed to test the potential of automatic temporal formatting.","cites":"95","conferencePercentile":"86.04651163"},{"venue":"ACM Multimedia","id":"7b6f5a4d4256ab76e0d131a3aad175edbbc40715","venue_1":"ACM Multimedia","year":"2008","title":"Multimodal observation systems","authors":"Mukesh Kumar Saini, Vivek K. Singh, Ramesh Jain, Mohan S. Kankanhalli","author_ids":"1760714, 4685302, 4521564, 1744045","abstract":"In recent years, we have seen a significant research interest in a number of multimodal sensing applications like surveillance, video ethnography, tele-presence, assisted living, life blogging etc. However, these applications are currently evolving as separate silos with no interconnection. Further, the individual application-centric architectures typically tend to focus on specific sensors, specific (hardwired) queries and deal with specific environments. We present a generic sensing architecture 'Observation System', which allows multiple users to undertake different applications through abstracted interaction with a common set of sensors. The observation system observes behavior of various objects in an environment and keeps a record of important events and activities in an eventbase. In this system, multifarious data collected from disparate sensors and other sources are correlated to understand and gain insights in the environment. The observation system has applications in many areas including but not limited to surveillance, traffic monitoring, ethnography, marketing, and healthcare. In this paper, we present the architecture and functionality of such a system and present details of activity detection using multiple sensor streams in a distributed sensing environment. We also present results of such an approach and potential extensions to the analysis of more complex activities and events.","cites":"2","conferencePercentile":"29.81651376"},{"venue":"ACM Multimedia","id":"0ba71a8c3cda08e62b69ee4c303dd5776819ce4d","venue_1":"ACM Multimedia","year":"2011","title":"A creation-tool for contemporary dance using multimodal video annotation","authors":"Diogo Cabral, João Valente, João M. F. Silva, Urândia Aragão, Carla Fernandes, Nuno Correia","author_ids":"2815588, 1806389, 3405823, 3329148, 2034871, 1717306","abstract":"This paper presents a video annotator that supports multimodal annotation and is applied to contemporary dance as a creation tool. The Creation-Tool was conceived and designed to assist the creative processes of choreographers, working as a digital notebook for personal annotations. The prototype, developed for Tablet PCs, is a real-time multimodal video annotator based on keyboard, pen and voice inputs. In addition, a remote control for mobile devices was added to the system. Two types of annotations were defined: annotation marks and regular annotations. The annotations marks are defined by a keyword and an icon, in contrast to regular annotations that do not have a pre-defined structure. Motion tracking defines the dynamic behavior of the annotations and voice input complements the other modalities.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"c828693ffc26736ff8a8f3cf230729d0c3547246","venue_1":"ACM Multimedia","year":"2009","title":"Events in multimedia","authors":"Ansgar Scherp, Ramesh Jain, Mohan S. Kankanhalli","author_ids":"1753135, 4521564, 1744045","abstract":"1. MOTIVATION Humans think in terms of events and entities. Events provide a natural abstraction of happenings in the real world. The concept of events has a long history in foundational sciences such as philosophy and linguistics. After first developing objects-based and entity-based approaches, computer science research is now addressing the concept of events and building many applications that consider events at least as important as objects. Consequently, we find many different solutions and approaches for modeling, detecting, and processing events. In addition, we find different applications that are based on events and make use of events. Conferences and workshops on events in computer science typically deal with the capturing, processing, and management of low-level events such as publish/subscribe systems and middleware solutions [3], complex event processing [1] and event stream processing [8], Semantic Web services [5], and reactivity for the Semantic Web [2]. Our understanding of events is different from the technical , low-level events above. Although this work is very essential for an efficient execution of the applications build on top of such approaches, the understanding of the concept of events is disconnected from the domain-level of events that the actual users of such applications have to deal with. How-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ever, considering multimedia data, its semantics is naturally closely tied to the event(s) it documents. We apply events to capture and represent human experience , i.e., to describe on a high-level the occurrences in which humans participate. These events are subject to discussions and interpretations by humans [4]. They may be very complex and linked to a variety of modeling aspects [7, 6], namely the participation of living and non-living objects in events, the temporal duration of events, and the spatial extension of objects. In addition, different kind of event relationships shall be supported like mereological relationships (composition of events), causal relationships, and correlation relationships. One also needs to consider the experiential aspect , i.e., the annotation of events with sensor data such as media data. As domain-level …","cites":"4","conferencePercentile":"42.97520661"},{"venue":"ACM Multimedia","id":"08272907c50e4af20b0e9b63a39950b410b595ce","venue_1":"ACM Multimedia","year":"2009","title":"Personal photo album summarization","authors":"Pinaki Sinha, Hamed Pirsiavash, Ramesh Jain","author_ids":"2174583, 2367683, 4521564","abstract":"Photo album summarization is the process of selecting a subset of photos from a larger collection which best preserves the information in the entire set and is semantically coherent. In this paper we propose a system which uses heterogeneous information sources associated with digital photos and generates a summary. Our algorithm adapts itself based on the type of event it is summarizing (Yearbook, Week or Single Day Event) We model the summarization problem as a retrieval problem based on different types of queries. We propose some evaluation metrics for the summary. We use an intuitive web based interface to present the results so that users can further explore the summary in an interactive way. This system is our submission to the CeWe Challenge for the Next Generation of Tangible Multimedia Products.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"1324e64544f3539074846bd784e088ba8dae7409","venue_1":"ACM Multimedia","year":"1997","title":"BubbleUp: Low Latency Fast-Scan for Media Servers","authors":"Edward Y. Chang, Hector Garcia-Molina","author_ids":"1686352, 1695250","abstract":"Interactive multimedia applications require fast response time. Traditional disk scheduling schemes can incur high latencies, and caching data in memory to reduce latency is usually not feasible, especially if fast-scans need to be supported. In this study we propose a disk-based solution called BubbleUp. It significantly reduces the initial la-tency for new requests, as well as for fast-scan requests. The throughput of the scheme is comparable to that of traditional schemes, and it may even provide better throughput than mechanisms based on elevator disk scheduling. BubbleUp incurs a slight disk storage overhead, but we argue that through effective allocation, this cost can be minimized.","cites":"21","conferencePercentile":"54.76190476"},{"venue":"ACM Multimedia","id":"37645b0da35ebeb7a334f33b0130912e08d010f6","venue_1":"ACM Multimedia","year":"2010","title":"Enriching audio-visual chat with conversation-based image retrieval and display","authors":"Jeroen Vanattenhoven, Christof van Nimwegen, Matthias Strobbe, Olivier Van Laere, Bart Dhoedt","author_ids":"2652653, 1984718, 3200385, 2219806, 1733741","abstract":"This paper presents the results of a user study carried out to evaluate an application prototype in which an audio-visual chat conversation between two users is augmented by pictures related to the topics of that conversation. The prototype analyses the conversation and deducts the topic of conversation by means of a keyword tree, augmented by an ontology. Then it retrieves pictures from Flickr based on this topic, after which the pictures are shown to the users. This mechanism is called conversation-based image retrieval. 15 participants were recruited for this user study; the duration of one session was approximately 30 minutes. Eye tracking and questionnaires were used to evaluate participants' experiences. We found that participants value the use of pictures to augment an audio-visual chat application. Furthermore, participants claimed they would use it in a social context: talking to family, friends and acquaintances. One significant improvement over the prototype would be to use their own pictures (personal user-generated content) instead of just random pictures.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"b2192ec7d7e80e7dea82f04e23343b4caeb1c812","venue_1":"ACM Multimedia","year":"2010","title":"Social pixels: genesis and evaluation","authors":"Vivek K. Singh, Mingyan Gao, Ramesh Jain","author_ids":"4685302, 1700498, 4521564","abstract":"Huge amounts of social multimedia is being created daily by a combination of globally distributed disparate sensors, including human-sensors (e.g. tweets) and video cameras. Taken together, this represents information about multiple aspects of the evolving world. Understanding the various events, patterns and situations emerging in such data has applications in multiple domains. We develop abstractions and tools to decipher various spatio-temporal phenomena which manifest themselves across such social media data. We describe an approach for aggregating social interest of users about any particular theme from any particular location into 'social pixels'. Aggregating such pixels spatio-temporally allows creation of social versions of images and videos, which then become amenable to various media processing techniques (like segmentation, convolution) to derive semantic situation information. We define a declarative set of operators upon such data to allow for users to formulate queries to visualize, characterize, and analyze such data. Results of applying these operations over an evolving corpus of millions of Twitter and Flickr posts, to answer situation-based queries in multiple application domains are promising.","cites":"34","conferencePercentile":"93.97260274"},{"venue":"ACM Multimedia","id":"33e8d6455e27fc7da02188bde81c32088ed18bf5","venue_1":"ACM Multimedia","year":"2009","title":"Automatic prediction of individual performance from \"thin slices\" of social behavior","authors":"Bruno Lepri, Nadia Mana, Alessandro Cappelletti, Fabio Pianesi","author_ids":"1776476, 1753458, 1791879, 8029006","abstract":"This paper targets the automatic detection of individual performances in group tasks by means of short sequences, \"thin slices\", of nonverbal behavior. We designed our task as a classification one. We also investigated the relevance of social context in our task and the effectiveness of our feature selection.","cites":"10","conferencePercentile":"67.97520661"},{"venue":"ACM Multimedia","id":"cc7475ab32698c247b7444337b8c66fd38524296","venue_1":"ACM Multimedia","year":"2005","title":"SensEye: a multi-tier camera sensor network","authors":"Purushottam Kulkarni, Deepak Ganesan, Prashant J. Shenoy, Qifeng Lu","author_ids":"1749860, 1742299, 1705052, 1705254","abstract":"This paper argues that a camera sensor network containing heterogeneous elements provides numerous benefits over traditional homogeneous sensor networks. We present the design and implementation of <i>senseye</i>---a multi-tier network of heterogeneous wireless nodes and cameras. To demonstrate its benefits, we implement a surveillance application using <i>senseye</i> comprising three tasks: object detection, recognition and tracking. We propose novel mechanisms for low-power low-latency detection, low-latency wakeups, efficient recognition and tracking. Our techniques show that a multi-tier sensor network can reconcile the traditionally conflicting systems goals of latency and energy-efficiency. An experimental evaluation of our prototype shows that, when compared to a single-tier prototype, our multi-tier <i>senseye</i> can achieve an order of magnitude reduction in energy usage while providing comparable surveillance accuracy.","cites":"169","conferencePercentile":"99.5049505"},{"venue":"ACM Multimedia","id":"5e12bf597a182b715e2ffe92aacdeb066ec23fcb","venue_1":"ACM Multimedia","year":"2011","title":"Automatic modeling of personality states in small group interactions","authors":"Jacopo Staiano, Bruno Lepri, Subramanian Ramanathan, Nicu Sebe, Fabio Pianesi","author_ids":"1767493, 1776476, 1742936, 1703601, 8029006","abstract":"In this paper, we target the automatic recognition of personality states in a meeting scenario employing visual and acoustic features. The social psychology literature has coined the name personality state to refer to a specific behavioral episode wherein a person behaves as more or less introvert/extrovert, neurotic or open to experience, etc. Personality traits can then be reconstructed as density distributions over personality states. Different machine learning approaches were used to test the effectiveness of the selected features in modeling the dynamics of personality states.","cites":"12","conferencePercentile":"85.86005831"},{"venue":"ACM Multimedia","id":"06d821f796a8ac198fe6465815fc4c9958dfd415","venue_1":"ACM Multimedia","year":"1999","title":"A multiagent system for content based navigation of music","authors":"David De Roure, Samhaa R. El-Beltagy, Steven Blackburn, Wendy Hall","author_ids":"1708123, 2214240, 4839398, 1685385","abstract":"We describe the integration of content based techniques for navigation of musical information (primarily in MIDI representation) into a multiagent system for distributed information management. This exercise illustrates the application of the multiagent approach to engineering distributed multimedia information systems and raises some design issues.","cites":"1","conferencePercentile":"19.32773109"},{"venue":"ACM Multimedia","id":"a4acd5c169361428c3280d8a848e6bf4bb757bda","venue_1":"ACM Multimedia","year":"2014","title":"Automatic Personality and Interaction Style Recognition from Facebook Profile Pictures","authors":"Fabio Celli, Elia Bruni, Bruno Lepri","author_ids":"2222709, 2552871, 1776476","abstract":"In this paper, we address the issue of personality and interaction style recognition from profile pictures in Facebook. We recruited volunteers among Facebook users and collected a dataset of profile pictures, labeled with gold standard self-assessed personality and interaction style labels. Then, we exploited a bag-of-visual-words technique to extract features from pictures. Finally, different machine learning approaches were used to test the effectiveness of these features in predicting personality and interaction style traits. Our good results show that this task is very promising, because profile pictures convey a lot of information about a user and are directly connected to impression formation and identity management.","cites":"6","conferencePercentile":"82.93172691"},{"venue":"ACM Multimedia","id":"1603c86701bc2f1f04b0d62240b5e63fbee3bc8a","venue_1":"ACM Multimedia","year":"2016","title":"Processing-Aware Privacy-Preserving Photo Sharing over Online Social Networks","authors":"Weiwei Sun, Jiantao Zhou, Ran Lyu, Shuyuan Zhu","author_ids":"8397572, 1735685, 3493927, 1719428","abstract":"With the ever-increasing popularity of mobile devices and online social networks (OSNs), sharing photos online has become extremely easy and popular. The privacy issues of shared photos and the associated protection schemes have received significant attention in recent years. In this work, we address the problem of designing privacy-preserving, high-fidelity, storage-efficient photo sharing solution over Facebook. We first conduct an in-depth study on the manipulations that Facebook performs to the uploaded images. With the awareness of such information, we suggest a DCT-domain image encryption scheme that is robust against these lossy operations. As validated by our experimental results, superior performance in terms of security, quality of the reconstructed images, and storage cost can be achieved.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"562e92078ea5b2579d7c34ec4726827965ef8c82","venue_1":"ACM Multimedia","year":"1997","title":"Semantic Analysis for Video Contents Extraction - Spotting by Association in News Nideo","authors":"Yuichi Nakamura, Takeo Kanade","author_ids":"5672176, 7642093","abstract":"Spotting by Association method for video analysis is a novel method to detect video segments with typical semantics. Video data contains various kinds of information through continuous images, natural language, and sound. For videos to be stored and retrieved in a Digital Libmry, it is essential to segment the video data into meaningful pieces. To detect meaningful segments, we need to identify the segment in each modality (video, language, and sound) that corresponds to the same story. For this purpose, we propose a new method for making correspondences between image clues detected by image analysis and language clues detected by natural language analysis. As a result, relevant video segments with sufficient information from every modality are obtained. We applied our method to closed-captioned CNN Headline News. Video segments with important events, such as a public speech, meeting , or visit, are detected fairly well.","cites":"0","conferencePercentile":"3.571428571"},{"venue":"ACM Multimedia","id":"1f7c84a629fc4cf16074d1395130d236466a5a68","venue_1":"ACM Multimedia","year":"2015","title":"Bandwidth-aware Prefetching for Proactive Multi-video Preloading and Improved HAS Performance","authors":"Vengatanathan Krishnamoorthi, Niklas Carlsson, Derek L. Eager, Anirban Mahanti, Nahid Shahmehri","author_ids":"2020829, 1739691, 1736203, 1701370, 1723372","abstract":"This paper considers the problem of providing users playing one streaming video the option of instantaneous and seamless playback of alternative videos. Recommendation systems can easily provide a list of alternative videos, but there is little research on how to best eliminate the startup time for these alternative videos. The problem is motivated by services that want to retain increasingly impatient users, who frequently watch the beginning of multiple videos, before viewing a video to the end. We present the design, implementation, and evaluation of an HTTP-based Adaptive Streaming (HAS) solution that provides careful prefetching and buffer management. We also present the design and evaluation of three fundamental policy classes that provide different tradeoffs between how aggressively new alternative videos are prefetched versus the importance of ensuring high playback quality. We show that our solution allows us to reduce the startup times of alternative videos by an order of magnitude and effectively adapt the quality such as to ensure the highest possible playback quality of the video being viewed. By improving the channel utilization we also address the discrimination problem that HAS clients often suffer from, allowing us to in some cases simultaneously improve the playback quality of the video being viewed and provide the value-added service of allowing instantaneous playback of the prefetched alternative videos.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"548d5d4f0f2801ab54a74b4c03710f3ce476de0e","venue_1":"ACM Multimedia","year":"2009","title":"Marking up a world: physical markup for virtual contentcreation","authors":"Eleanor G. Rieffel, Sagar Gattepally, Don Kimber, Jun Shingu, Jim Vaughan, John Doherty","author_ids":"1710276, 2344212, 2178004, 2565333, 2133761, 3881379","abstract":"The Pantheia system enables users to create virtual models by `marking up' the real world with pre-printed markers. The markers have predefined meanings that guide the system as it creates models. Pantheia takes as input user captured images or video of the marked up space. This video illustrates the workings of the system and shows it being used to create three models, one of a cabinet, one of a lab, and one of a conference room. As part of the Pantheia system, we also developed a 3D viewer that spatially integrates a model with images of the model.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"66e4f08c9c23b7aee799ecc3696e59ceac5434ba","venue_1":"ACM Multimedia","year":"2012","title":"Through the looking glass: mirror worlds for augmented awareness & capability","authors":"Don Kimber, Jun Shingu, Jim Vaughan, David Arendash, David Lee, Maribeth Back","author_ids":"2178004, 2565333, 2133761, 2044389, 2344788, 2131565","abstract":"We demonstrate a system for supporting mirror worlds - 3D virtual models of physical spaces that reflect the structure and activities of those spaces to help support context awareness and tasks such as planning and recollection of events. Through views on web pages, portable devices, or on `magic window' displays in the physical space, remote people may `look in' to the space, while people within the space are provided information that would not otherwise be obvious. For example, by looking at a mirror display, people can learn how long others have been present, or where they have been. People in one part of a building can get a sense of activities in the rest of the building. The system can be used to bridge across sites and help provide different parts of an organization with a shared awareness of each other's activities. We demonstrate viewers for several mirror worlds we have created, including for the ACM conference venue in Nara.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"cf0e22c8ac961aa8812bccbd0ef9b56cec947beb","venue_1":"ACM Multimedia","year":"2015","title":"Weak Labeled Multi-Label Active Learning for Image Classification","authors":"Shiquan Zhao, Jian Wu, Victor S. Sheng, Chen Ye, Pengpeng Zhao, Zhiming Cui","author_ids":"2166541, 1734550, 2858764, 3231041, 2927967, 1765993","abstract":"In order to achieve better classification performance with even fewer labeled images, active learning is suitable for these situations. Several active learning methods have been proposed for multi-label image classification, but all of them assume that all training images with complete labels. However, as a matter of fact, it is very difficult to get complete labels for each example, especially when the size of labels in a multi-label domain is huge. Usually, only partial labels are available. This is one kind of \"weak label\" problems. This paper proposes an ingeniously solution to this \"weak label\" problem on multi-label active learning for image classification (called WLMAL). It explores label correlation on the weak label problem with the help of input features, and then utilizes label correlation to evaluate the informativeness of each example-label pair in a multi-label dataset for active sampling. Our experimental results on three real-world datasets show that our proposed approach WLMAL consistently outperforms existing approaches significantly.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"2ef0748522a98f3bd06d7819410fd084e34031eb","venue_1":"ACM Multimedia","year":"2010","title":"Inter-ACT: an affective and contextually rich multimodal video corpus for studying interaction with robots","authors":"Ginevra Castellano, Iolanda Leite, André Pereira, Carlos Martinho, Ana Paiva, Peter W. McOwan","author_ids":"2710715, 1698537, 6725946, 1682637, 1738084, 2803283","abstract":"The Inter-ACT (INTEracting with Robots - Affect Context Task) corpus is an affective and contextually rich multimodal video corpus containing affective expressions of children playing chess with an iCat robot. It contains videos that capture the interaction from different perspectives and includes synchronised contextual information about the game and the behaviour displayed by the robot. The Inter-ACT corpus is mainly intended to be a comprehensive repository of naturalistic and contextualised, task-dependent data for the training and evaluation of an affect recognition system in an educational game scenario. The richness of contextual data that captures the whole human-robot interaction cycle, together with the fact that the corpus was collected in the same interaction scenario of the target application, make the Inter-ACT corpus unique in its genre.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"b8d2d984b97172b94d07d1df1213379ec1b216c8","venue_1":"ACM Multimedia","year":"2014","title":"Analysis/synthesis approaches for creatively processing video signals","authors":"Javier Villegas, Angus Graeme Forbes","author_ids":"2153336, 7574995","abstract":"This paper explores methods for the creative manipulation of video signals and the generation of animations through a process of analysis and synthesis. Our approach involves four distinct steps, and different creative outputs based on video inputs can be obtained by choosing different alternatives at each of the steps. First, we decide which features to extract from an input video sequence. Next, we choose a matching strategy to associate the features between a pair of video frames. Then, we choose a way to interpolate between corresponding features within these frames. Finally, we decide how to render these elements when resynthesizing the signal. We illustrate our approach with a range of different examples, including video manipulation experiments, animations, and real-time multimedia installations.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"584347cfd28701f6282ccc8fb81d7fd2d3341521","venue_1":"ACM Multimedia","year":"2012","title":"The new dunites","authors":"Andres E. Burbano Valdes, Danny Bazo, Sölen K. DiCicco, Angus Graeme Forbes","author_ids":"2342976, 3097045, 2038276, 7574995","abstract":"The New Dunites is an interdisciplinary media arts research project that investigates the archeological site where the set for Cecile B. DeMille's The Ten Commandments was buried in 1923 [1]. In particular, this multi-phase endeavor involved the gathering of geophysical and archeological data, the historical study of the dawn of cinema in California, and a series of novel interactive multimedia installations that explored new avenues in the representation of scientific and cultural data.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"2bb79bfd3d2f2ba11eee1c7ec7a8e9b48e1a22e6","venue_1":"ACM Multimedia","year":"2006","title":"An architecture for viewer-side enrichment of TV content","authors":"Dick C. A. Bulterman, Pablo César, Jack Jansen","author_ids":"1726923, 1743507, 1724349","abstract":"This paper presents a user interface model and implementation for exploiting next-generation interactive capabilities with the domain of television content. Our work studies capabilities that extend a user's potential impact over the consumption and sharing of television programs. The main capabilities of our environment include personalized viewing and navigation within a program fragment, and the ability to actively personalize content via various end-user content enrichments (such as line art, referrals and hyperlink insertions). In this paper, we present the implementation of a range of \"couch-top\" control and editing devices, including personal devices such as personal digital assistants and ad-hoc interactive devices. This paper also presents an architecture that decouples user actions into activators and handlers. We provide an overview of the interaction architecture and report on a series of deployment experiments on a wide range of consumer electronics devices.","cites":"6","conferencePercentile":"54.40414508"},{"venue":"ACM Multimedia","id":"904ed06e6a3066ef085049a37340f326afc007a1","venue_1":"ACM Multimedia","year":"2006","title":"Learning concepts from large scale imbalanced data sets using support cluster machines","authors":"Jinhui Yuan, Jianmin Li, Bo Zhang","author_ids":"2422936, 8549039, 1696318","abstract":"This paper considers the problem of using Support Vector Machines (SVMs) to learn concepts from large scale imbalanced data sets. The objective of this paper is twofold. Firstly, we investigate the effects of large scale and imbalance on SVMs. We highlight the role of linear non-separability in this problem. Secondly, we develop a both practical and theoretical guaranteed meta-algorithm to handle the trouble of scale and imbalance. The approach is named Support Cluster Machines (SCMs). It incorporates the <i>informative</i> and the <i>representative</i> under-sampling mechanisms to speedup the training procedure. The SCMs differs from the previous similar ideas in two ways, (a) the theoretical foundation has been provided, and (b) the clustering is performed in the feature space rather than in the input space. The theoretical analysis not only provides justification, but also guides the technical choices of the proposed approach. Finally, experiments on both the synthetic and the TRECVID data are carried out. The results support the previous analysis and show that the SCMs are efficient and effective while dealing with large scale imbalanced data sets.","cites":"22","conferencePercentile":"85.75129534"},{"venue":"ACM Multimedia","id":"3cd861bf2489dc0194ad41aa9e48232c2cb60e9b","venue_1":"ACM Multimedia","year":"2014","title":"A Multi-Touch DJ Interface with Remote Audience Feedback","authors":"Lasse Farnung Laursen, Masataka Goto, Takeo Igarashi","author_ids":"3083073, 1720652, 1717356","abstract":"Current DJ interfaces lack direct support for typical digital communication common in social media. We present a novel DJ interface for live internet broadcast performances with remote audience feedback integration. Our multi-touch interface is designed for a table top display, featuring a time-line based visualization. Two studies are presented involving seven DJs, culminating in four live broadcasts gathering and analyzing data to better understand both the DJ and audience perspective. This study is one of the first to look closer at DJs and remote audiences. We present useful insight for future interaction design between DJs and remote audiences, and interface integrated audience feedback.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"81544072580af527f00c03dad7efe687ebbc9ecc","venue_1":"ACM Multimedia","year":"2010","title":"Interactive visual object search through mutual information maximization","authors":"Jingjing Meng, Junsong Yuan, Yuning Jiang, Nitya Narasimhan, Venu Vasudevan, Ying Wu","author_ids":"1691016, 1746449, 1691963, 1714900, 1721863, 7133679","abstract":"Searching for small objects (e.g., logos) in images is a critical yet challenging problem. It becomes more difficult when target objects differ significantly from the query object due to changes in scale, viewpoint or style, not to mention partial occlusion or cluttered backgrounds. With the goal to retrieve and accurately locate the small object in the images, we formulate the object search as the problem of finding <i>subimages</i> with the largest mutual information toward the query object. Each image is characterized by a collection of local features. Instead of only using the query object for matching, we propose a discriminative matching using both positive and negative queries to obtain the mutual information score. The user can verify the retrieved subimages and improve the search results incrementally. Our experiments on a challenging logo database of 10,000 images highlight the effectiveness of this approach.","cites":"13","conferencePercentile":"81.50684932"},{"venue":"ACM Multimedia","id":"7c2458cd4dc35b63b74089e5339d6ad03a5c4f2b","venue_1":"ACM Multimedia","year":"2004","title":"Seeing sounds: exploring musical social networks","authors":"Piotr D. Adamczyk","author_ids":"1712162","abstract":"Information gathering from multimedia retrieval systems is aided by effective visualization, but the degree to which visualization is effective depends in part on the way the context of the results is presented. When relationships represent media rich connections, static visualization alone may not be enough. This work explores how to represent context and utilize multimedia to convey a more accurate sense of search results. As a representative case, we explore various presentations of social networks formed by expert opinions of musical artist similarity. Our work extends research in information visualization and music retrieval to create a multimedia search experience. Three interactive presentation styles are used; graph-based 2D, Desktop 3D (VRML), and CAVE (immersive Virtual Reality). Visual models are augmented with spatial audio in 3D, and hyperlinks to sound files in 2D. Results of a preliminary user study of these styles are discussed along with implications for recommender system design.","cites":"1","conferencePercentile":"19.11764706"},{"venue":"ACM Multimedia","id":"b7f729ec214a7539c6a7b3ea1913a575353bdf3e","venue_1":"ACM Multimedia","year":"2002","title":"Personalized advertisement-duration control for streaming delivery","authors":"Takashi Oshiba, Yuichi Koike, Masahiro Tabuchi, Tomonari Kamba","author_ids":"2406243, 1979541, 2439471, 2674045","abstract":"This paper describes the development of a streaming advertisement delivery system that controls the insertion of streaming advertisements into streaming content.Conventional personalization techniques lack a time-control function for advertisement insertion, so the advertisement exposure for each user access can become excessive, much to the annoyance of viewers. This could devalue streaming content by making it less attractive.In our technique, advertisement insertion control is based on the history of each viewer. This personalization method makes it possible to maintain a balanced ratio of the advertisement length to the content length. As a result, our technique should encourage the growth of Internet streaming services and enable more effective and less intrusive advertising.","cites":"3","conferencePercentile":"25.64102564"},{"venue":"ACM Multimedia","id":"836c487a84b14f67dfa5b7313324be8cdbc4e9b7","venue_1":"ACM Multimedia","year":"2010","title":"Interactive retrieval of targets for wide area surveillance","authors":"Saad Ali, Omar Javed, Niels Haering, Takeo Kanade","author_ids":"4333070, 1710121, 2085175, 7642093","abstract":"We address the problem of interactive search for a target of interest in surveillance imagery. Our solution consists of iteratively learning a distance metric for retrieval, based on user feedback. The approach employs (retrieval) rank based constraints and convex optimization to efficiently learn the distance metric. The algorithm uses both user labeled and unlabeled examples in the learning process. The method is fast enough for a new metric to be learned interactively for each target query. In order to reduce the burden on the user, a model-independent active learning method is used to select key examples, for response solicitation. This leads to a significant reduction in the number of user-interactions required for retrieving the target of interest. The proposed method is evaluated on challenging pedestrian and vehicle data sets, and compares favorably to the state of the art in target re-acquisition algorithms.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"2ddc7f0b26fd97d03d1ae9e7322465c0c1cae465","venue_1":"ACM Multimedia","year":"2006","title":"Event-centric multimedia data management for reconnaissance mission analysis and reporting","authors":"Utz Westermann, Srikanth Agaram, Bo Gong, Ramesh Jain","author_ids":"1713926, 2371136, 1764561, 4521564","abstract":"We demonstrate the concept of event-centric multimedia data management in the context of a multimedia eChronicle for the analysis, exploration, and reporting of events in military reconnaissance missions. Unlike the traditional media-centric approach, event-centricmultimedia data management focuses on the management of real-world events; documenting media are regarded as event metadata. For the detection of mission events, we apply simple but robust spatio-temporal clustering of basic soldier state and media events with good results considering the uncontrolled environment a military patrol constitutes. The core of the architecture is generic and applicable for the event-centric management of multimedia data in other domains as well.","cites":"2","conferencePercentile":"29.79274611"},{"venue":"ACM Multimedia","id":"00e328da2e6bc08b474c23427cb5daa431af568a","venue_1":"ACM Multimedia","year":"2009","title":"Tag refinement by regularized LDA","authors":"Hao Xu, Jingdong Wang, Xian-Sheng Hua, Shipeng Li","author_ids":"1742479, 1688516, 1746102, 4973820","abstract":"Tagging is nowadays the most prevalent and practical way to make images searchable. However, in reality many tags are irrelevant to image content. To refine the tags, previous solutions usually mine tag relevance relying on the tag similarity estimated right from the corpus to be refined. The calculation of tag similarity is affected by the noisy tags in the corpus, which is not conducive to estimate accurate tag relevance. In this paper, we propose to do tag refinement from the angle of topic modeling. In the proposed scheme, tag similarity and tag relevance are jointly estimated in an iterative manner, so that they can benefit from each other. Specifically, a novel graphical model, regularized Latent Dirichlet Allocation (rLDA), is presented. It facilitates the topic modeling by exploiting both the statistics of tags and visual affinities of images in the corpus. The experiments on tag ranking and image retrieval demonstrate the advantages of the proposed method.","cites":"37","conferencePercentile":"94.4214876"},{"venue":"ACM Multimedia","id":"7245206cdf02e839fb91716d684e534c62d0b712","venue_1":"ACM Multimedia","year":"2012","title":"Attribute feedback","authors":"Hanwang Zhang, Zheng-Jun Zha, Jingwen Bian, Yue Gao, Huan-Bo Luan, Tat-Seng Chua","author_ids":"5462268, 2355916, 3258963, 1744619, 1696019, 1684968","abstract":"This work presents a new interactive Content Based Image Retrieval (CBIR) scheme, termed Attribute Feedback (AF). Unlike traditional relevance feedback purely founded on low-level visual features, the Attribute Feedback system shapes users' information needs more precisely and quickly by collecting feedbacks on intermediate level semantic attributes. At each interactive iteration, AF first determines the most informative binary attributes for feedbacks, preferring the attributes that frequently (rarely) appear in current search results but are unlikely (likely) to be users' interest. The binary attribute feedbacks are then augmented by a new type of attributes, \"affinity attributes\", each of which is off-line learnt to describe the distance between user's envisioned image(s) and a retrieved image with respect to the corresponding affinity attribute. Based on the feedbacks on binary and affinity attributes, the images in corpus are further re-ranked towards better fitting the users' information needs. Extensive experiments on two real-world image datasets well demonstrate the superiority of the proposed scheme over other state-of-the-art relevance feedback based CBIR solutions.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"2dcd05d70c7c9d14cf7036d7d59a28110127e18a","venue_1":"ACM Multimedia","year":"2008","title":"Wearable forest-feeling of belonging to nature","authors":"Hill Hiroki Kobayashi, Ryoko Ueoka, Michitaka Hirose","author_ids":"2315764, 2805648, 1709460","abstract":"Wearable Forest is a clothing design that bio-acoustically interacts with distant wildlife in a remote forest through a remote-controlled speaker and microphone using a network. It expresses the bioacoustical beauty of nature in its unique aesthetic appeal for users and allows the users interact with a forest in real time through a network to acoustically experience a distant forest soundscape, thus merging man and nature without environmental destruction. This novel interactive sound system can create a sense of unity between users and a remote soundscape, enabling users to feel a sense of belonging to nature even in the midst of a city.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"afa91e5af5b315bd7dcb32a585a0b9c3e48e6eef","venue_1":"ACM Multimedia","year":"2006","title":"Automatic video annotation by semi-supervised learning with kernel density estimation","authors":"Meng Wang, Yan Song, Xun Yuan, HongJiang Zhang, Xian-Sheng Hua, Shipeng Li","author_ids":"1731598, 1804278, 8515902, 1718558, 1746102, 4973820","abstract":"Insufficiency of labeled training data is a major obstacle for automatically annotating large-scale video databases with semantic concepts. Existing semi-supervised learning algorithms based on parametric models try to tackle this issue by incorporating the information in a large amount of unlabeled data. However, they are based on a \"model assumption\" that the assumed generative model is correct, which usually cannot be satisfied in automatic video annotation due to the large variations of video semantic concepts. In this paper, we propose a novel semi-supervised learning algorithm, named Semi Supervised <i>Learning by Kernel Density Estimation</i> (SSLKDE), which is based on a non-parametric method, and therefore the \"model assumption\" is avoided. While only labeled data are utilized in the classical Kernel Density Estimation (KDE) approach, in SSLKDE both labeled and unlabeled data are leveraged to estimate class conditional probability densities based on an extended form of KDE. We also investigate the connection between SSLKDE and existing graph-based semi-supervised learning algorithms. Experiments prove that SSLKDE significantly outperforms existing supervised methods for video annotation.","cites":"26","conferencePercentile":"88.34196891"},{"venue":"ACM Multimedia","id":"44d576155256b41a446af954221e4928479a7d27","venue_1":"ACM Multimedia","year":"2011","title":"Context-based friend suggestion in online photo-sharing community","authors":"Ting Yao, Chong-Wah Ngo, Tao Mei","author_ids":"8543685, 1751681, 1788123","abstract":"With the popularity of social media, web users tend to spend more time than before for sharing their experience and interest in online photo-sharing sites. The wide variety of sharing behaviors generate different metadata which pose new opportunities for the discovery of communities. We propose a new approach, named context-based friend suggestion, to leverage the diverse form of contextual cues for more effective friend suggestion in the social media community. Different from existing approaches, we consider both visual and geographical cues, and develop two user-based similarity measurements, i.e., <i>visual similarity</i> and <i>geo similarity</i> for characterizing user relationship. The problem of friend suggestion is casted as a contextual graph modeling problem, where users are nodes and the edges between them are weighted by <i>geo similarity</i>. Meanwhile, the graph is initialized in a way that users with higher <i>visual similarity</i> to a given query have better chance to be recommended. Experimental results on a dataset of 13,876 users and ~1.5 million of their shared photos demonstrated that the proposed approach is consistent with human perception and outperforms other works.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"92bf716441c49bff898de989b2f57ed746677df7","venue_1":"ACM Multimedia","year":"2011","title":"Internet multimedia advertising: techniques and technologies","authors":"Tao Mei, Ruofei Zhang, Xian-Sheng Hua","author_ids":"1788123, 1694966, 1746102","abstract":"The explosive growth of multimedia data on the Internet creates huge opportunities for multimedia advertising. In this tutorial, we present the techniques and technologies for Internet multimedia advertising. The tutorial aims at bringing together recent insights from the research on multimedia advertising that addresses the theoretical fundamentals, solution concepts, and the issues related to the development of modern multimedia advertising schemes.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"244c4d3bcf0843408f3c9eec5497c4e2fb7eea0e","venue_1":"ACM Multimedia","year":"2009","title":"NLVS: a near-lossless video summarization system","authors":"Lin-Xie Tang, Tao Mei, Xian-Sheng Hua","author_ids":"1839622, 1788123, 1746102","abstract":"This video demonstration presents a new system for video summarization, called ``Near-Lossless Video Summarization\" (NLVS) to tackle with the challenges of storage and indexing brought by the current boom of videos for existing online video services. Unlike current techniques such as video compression and summarization which are still struggling to achieve the two often conflicting goals of low storage and high visual and semantic fidelity, NLVS is able to summarize a video stream with least information loss by using an extremely small piece of metadata. Although at a very low compression ratio (1/30 of H.264 baseline in average, where traditional compression techniques like H.264 fail to preserve the fidelity), the summary still can be used to reconstruct the original video (with the same duration) nearly without semantic information loss.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"1c448da6637f151ec55777ab4a1d5294bf36fc64","venue_1":"ACM Multimedia","year":"2009","title":"Visual query suggestion","authors":"Zheng-Jun Zha, Linjun Yang, Tao Mei, Meng Wang, Zengfu Wang","author_ids":"2355916, 7866194, 1788123, 1731598, 6130256","abstract":"Query suggestion is an effective approach to improve the usability of image search. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This paper proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. It provides a more effective query interface to formulate an intent-specific query by joint text and image suggestions. We show that VQS is able to more precisely and more quickly help users specify and deliver their search intents. When a user submits a text query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. If the user selects one of the suggestions, the corresponding keyword will be added to complement the initial text query as the new text query, while the image collection will be formulated as the visual query. VQS then performs image search based on the new text query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS with three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and search performance.","cites":"86","conferencePercentile":"99.17355372"},{"venue":"ACM Multimedia","id":"9be34171b10fab52e5dccca5955bfa6fb9117240","venue_1":"ACM Multimedia","year":"2006","title":"Towards content-based relevance ranking for video search","authors":"Wei Lai, Xian-Sheng Hua, Wei-Ying Ma","author_ids":"6064843, 1746102, 1705244","abstract":"Most existing web video search engines index videos by file names, URLs, and surrounding texts. These types of video metadata roughly describe the whole video in an abstract level without taking the rich content, such as semantic content descriptions and speech within the video, into consideration. Therefore the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored. In this paper we propose a novel relevance ranking approach for Web-based video search using both video metadata and the rich content contained in the videos. To leverage real content into ranking, the videos are segmented into shots, which are smaller and more semantic-meaningful retrievable units, and then more detailed information of video content such as semantic descriptions and speech of each shots are used to improve the retrieval and ranking performance. With video metadata and content information of shots, we developed an integrated ranking approach, which achieves improved ranking performance. We also introduce machine learning into the ranking system, and compare them with IR-model (information retrieval model) based method. The evaluation results demonstrate the effectiveness of the proposed ranking methods.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"3dcf7cdfe513db6385a4fcef14a26cf93f67bd3f","venue_1":"ACM Multimedia","year":"2009","title":"Near-lossless video summarization","authors":"Lin-Xie Tang, Tao Mei, Xian-Sheng Hua","author_ids":"1839622, 1788123, 1746102","abstract":"The daunting yet increasing volume of videos on the Internet brings the challenges of storage and indexing to existing online video services. Current techniques like video compression and summarization are still struggling to achieve the two often conflicting goals of low storage and high visual and semantic fidelity. In this work, we develop a new system for video summarization, called \"Near-Lossless Video Summarization\" (NLVS), which is able to summarize a video stream with the least information loss by using an extremely small piece of metadata. The summary consists of a set of synthesized mosaics and representative keyframes, a compressed audio stream, as well as the metadata about video structure and motion. Although at a very low compression ratio (i.e., 1/30 of H.264 baseline in average, where traditional compression techniques like H.264 fail to preserve the fidelity), the summary still can be used to reconstruct the original video (with the same duration) nearly without semantic information loss. We show that NLVS is a powerful tool for significantly reducing video storage through both objective and subjective comparisons with state-of-the-art video compression and summarization techniques.","cites":"12","conferencePercentile":"75.41322314"},{"venue":"ACM Multimedia","id":"23ab7a224b46e65a87722515ada2e3b9367e366c","venue_1":"ACM Multimedia","year":"2007","title":"Refining video annotation by exploiting pairwise concurrent relation","authors":"Zheng-Jun Zha, Tao Mei, Xian-Sheng Hua, Guo-Jun Qi, Zengfu Wang","author_ids":"2355916, 1788123, 1746102, 7266598, 6130256","abstract":"Video annotation is a promising and essential step for content-based video search and retrieval. Most of the state-of-the-art video annotation approaches detect multiple semantic concepts in an isolated manner, which neglect the fact that video concepts are usually correlated in semantic nature. In this paper, we propose to refine video annotation by leveraging the pairwise concurrent relation among video concepts. Such concurrent relation is explicitly modeled by a concurrent matrix and then a propagation strategy is adopted to refine the annotations. Through spreading the scores of all related concepts to each other iteratively, the detection results approach stable and optimal. In contrast with existing concept fusion methods, the proposed approach is computationally more efficient and easy to implement, not requiring to construct any contextual model. Furthermore, we show its intuitive connection with the PageRank algorithm. We conduct the experiments on TRECVID 2005 corpus and report superior performance compared to existing key approaches.","cites":"8","conferencePercentile":"59.375"},{"venue":"ACM Multimedia","id":"0ce8d407e5768f67d3b19a4f17a1d1c7863cab0e","venue_1":"ACM Multimedia","year":"2007","title":"Multi-layer multi-instance kernel for video concept detection","authors":"Zhiwei Gu, Tao Mei, Xian-Sheng Hua, Jinhui Tang, Xiuqing Wu","author_ids":"2582274, 1788123, 1746102, 8053308, 1787412","abstract":"In video concept detection, most existing methods have not well studied the <i>intrinsic hierarchical structure</i> of video content. However, unlike flat attribute-value data used in many existing methods, video is essentially a structured media with multi-layer representation. For example, a video can be represented by a hierarchical structure including, from large to small, <i>shot</i>, <i>key-frame</i>, and <i>region</i>. Moreover, it fits the typical Multi-Instance (MI) setting in which the \"bag-instance\" correspondence is embedded among contiguous layers. We call such multi-layer structure and the \"bag-instance\" relation embedded in the structure as Multi-Layer Multi-Instance (MLMI) setting in this paper. We formulate video concept detection as an MLMI learning problem in which a rooted tree with MLMI nature embedded is devised to represent a video segment. Furthermore, by fusing the information from different layers, we construct a novel MLMI kernel to measure the similarities between the instances in the same and different layers. In contrast to traditional MI learning, both the Multi-Layer structure and Multi-Instance relations are leveraged simultaneously in the proposed kernel. We applied MLMI kernel to concept detection task on TRECVID 2005 corpus and reported superior performance (+25% in Mean Average Precision) to standard Support Vector Machine based approaches.","cites":"8","conferencePercentile":"59.375"},{"venue":"ACM Multimedia","id":"14f35275ec1b5672fdbafb98be4c83642ba3e12f","venue_1":"ACM Multimedia","year":"2007","title":"Video annotation by graph-based learning with neighborhood similarity","authors":"Meng Wang, Tao Mei, Xun Yuan, Yan Song, Li-Rong Dai","author_ids":"1731598, 1788123, 8515902, 1804278, 1719546","abstract":"Graph-based semi-supervised learning methods have been proven effective in tackling the difficulty of training data insufficiency in many practical applications such as video annotation. These methods are all based on an assumption that the labels of similar samples are close. However, as a crucial factor of these algorithms, the estimation of pairwise similarity has not been sufficiently studied. Usually, the similarity of two samples is estimated based on the Euclidean distance between them. But we will show that similarities are not merely related to distances but also related to the structures around the samples. It is shown that distance-based similarity measure may lead to high classification error rates even on several simple datasets. In this paper we propose a novel <i>neighborhood similarity</i> measure, which simultaneously takes into account both thse distance between samples and the difference between the structures around the corresponding samples. Experiments on synthetic dataset and TRECVID benchmark demonstrate that the neighborhood similarity is superior to existing distance based similarity.","cites":"8","conferencePercentile":"59.375"},{"venue":"ACM Multimedia","id":"675932329bb06f73c5e3894ed18f40c71283411e","venue_1":"ACM Multimedia","year":"2012","title":"Personalized video recommendation through tripartite graph propagation","authors":"Bisheng Chen, Jingdong Wang, Qinghua Huang, Tao Mei","author_ids":"1959700, 1688516, 8304874, 1788123","abstract":"The rapid growth of the number of videos on the Internet provides enormous potential for users to find content of interest to them. Video search, such as Google, Youtube, Bing, is a popular way to help users to find desired videos. However, it is still very challenging to discover new video contents for users. In this paper, we address the problem of providing personalized video suggestions for users. Rather than only exploring the user-video graph that is formulated using the click-through information, we also investigate other two useful graphs, the user-query graph indicating if a user ever issues a query, and the query-video graph indicating if a video appears in the search result of a query. The two graphs act as a bridge to connect users and videos, and have a large potential to improve the recommendation as the queries issued by a user essentially imply his interest. As a result, we reach a tripartite graph over (user, video, query). We develop an iterative propagation scheme over the tripartite graph to compute the preference information of each user. Experimental results on a dataset of 2,893 users, 23,630 queries and 55,114 videos collected during Feb. 1-28, 2011 demonstrate that the proposed method outperforms existing state-of-the-art approaches, co-views and random walks on the user-video bipartite graph.","cites":"12","conferencePercentile":"88.76582278"},{"venue":"ACM Multimedia","id":"02c46fa7a9af2da5d8b74e16b779cdab135ac3ad","venue_1":"ACM Multimedia","year":"2007","title":"Video collage","authors":"Xueliang Liu, Tao Mei, Xian-Sheng Hua, Bo Yang, He-Qin Zhou","author_ids":"3076466, 1788123, 1746102, 3211345, 2457832","abstract":"In this work, we present Video Collage system, which automatically constructs a compact and visually appealing synthesized collage from a video sequence for efficient video browsing. Given a video, Video Collage is able to select the most representative images, extract salient regions of interest (ROI) from these images and resize ROI according to their saliencies, and seamlessly arrange them on a given canvas while preserving the temporal structure of video content. Furthermore, Video Collage provides a novel user interface that enables users to browse video content in a variety of more efficient ways in contrast to many existing approaches to video browsing.","cites":"10","conferencePercentile":"66.92708333"},{"venue":"ACM Multimedia","id":"c613107ee3509002517ec491f46e30db5cd834e7","venue_1":"ACM Multimedia","year":"2006","title":"To construct optimal training set for video annotation","authors":"Jinhui Tang, Yan Song, Xian-Sheng Hua, Tao Mei, Xiuqing Wu","author_ids":"8053308, 1804278, 1746102, 1788123, 1787412","abstract":"This paper exploits the criteria to optimize the training set construction for video annotation. Most existing learning-based semantic annotation approaches require a large training set to achieve good generalization capacity, in which a considerable amount of labor-intensively manual labeling is desirable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution rather than the size of the training data. We argue that a training set which includes most temporal and spatial distribution of the whole data will achieve a satisfying performance even in the case of limited size of training set. In order to capture the geometrical distribution characteristics of a given video collection, we propose the following four metrics for constructing an optimal training set, including <i>Salience Time Dispersiveness Spatial Dispersiveness</i> and <i>Diversity</i>. Moreover, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data for a training set with a given size. Experimental results demonstrate that these rules are effective for training set construction for video annotation, and significantly outperform random training set selection as well.","cites":"6","conferencePercentile":"54.40414508"},{"venue":"ACM Multimedia","id":"02a6e25a48cc085b21926c0050d79686f11f2b65","venue_1":"ACM Multimedia","year":"2005","title":"Natural video browsing","authors":"Cai-Zhi Zhu, Tao Mei, Xian-Sheng Hua","author_ids":"2989877, 1788123, 1746102","abstract":"In this demonstration, we show a novel system, Video Booklet, which enables nature personal video browsing and searching. Firstly representative thumbnails of video segments are selected and reshaped by a set of pre-trained personalized shape templates, and then printed out on a real booklet. When we want to watch the segment indicated by a certain thumbnail in the booklet, we are able to use camera phones or similar devices to capture the corresponding thumbnail, and send it to a computer via wireless network. Thereafter, the target thumbnail is accurately located by a Self-Trained Active Shape Model algorithm, and then the distortion of the captured image is corrected. Finally the Video Booklet system will automatically find the most similar thumbnail to the corrected one and begin to play the corresponding segment in the video library for us. Thereby, Video Booklet builds a seamless bridge between digital videos and analog albums.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"bb3503e948fcda8e40e2be1191a9e51d63e036bd","venue_1":"ACM Multimedia","year":"1994","title":"Video Tomography: An Efficient Method for Camerawork Extraction and Motion Analysis","authors":"Akihito Akutsu, Yoshinobu Tonomura","author_ids":"2523644, 2000148","abstract":"This paper proposes a new, efficient and practical way to extract lens zoom, camera pan and camera tilt information using modified motion analysis. The proposed method is called the Video Tomography Method (VTM), in which tomographic techniques are introduced into a motion estimation algorithm. By using the VTM, one is able to visualize motion as a spatiotemporal flow for motion analysis. The VTM is an extremely robust [resistant to noise] method for estimating camera operation due to its tomographic nature. The practicality of this type of motion estimation and analysis is confirmed by the results of our simulations and experiments in testing the prototype platform with a low quality video source. Other possible applications that might use extracted motion data are discussed. This method is targeted towards video handling applications that attribute extracted motion data into a video index. It will enhance the process of editing and browsing structured video, and will allow the visualization of scenes spatiotemporally so that video may be accessed intuitively and spatially in relation to its temporal location. This type of access is a new interface for structured video. Scene reconstruction techniques can be extended to apply to the problem of reconstructing occluded images and resolution enhancement.","cites":"46","conferencePercentile":"78.81355932"},{"venue":"ACM Multimedia","id":"2319db4b29270db9d1ac3802b24e9e3d402941c5","venue_1":"ACM Multimedia","year":"2005","title":"Spatio-temporal quality assessment for home videos","authors":"Tao Mei, Cai-Zhi Zhu, He-Qin Zhou, Xian-Sheng Hua","author_ids":"1788123, 2989877, 2457832, 1746102","abstract":"Compared with the video programs taken by professionals, home videos are always with low-quality content resulted from lack of professional capture skills. In this paper, we present a novel spatio-temporal quality assessment scheme in terms of low-level content features for home videos. In contrast to existing frame-level-based quality assessment approaches, a type of temporal segment of video, sub-shot, is selected as the basic unit for quality assessment. A set of spatio-temporal artifacts, regarded as the key factors affecting the overall perceived quality (i.e. <i>unstableness, jerkiness, infidelity, blurring, brightness</i> and <i>orientation</i>), are mined from each sub-shot based on the particular characteristics of home videos. The relationship between the overall quality metric and these factors are exploited by three different methods, including user study, factor fusion, and a learning-based scheme. To validate the proposed scheme, we present a scalable quality-based home video summarization system, aiming at achieving the best quality while simultaneously preserving the most informative content. A comparison user study between this system and the attention model based video skimming approach demonstrated the effectiveness of the proposed quality assessment scheme.","cites":"6","conferencePercentile":"45.2970297"},{"venue":"ACM Multimedia","id":"a58571963e961dd69ec41544a1f98c0bee15d44b","venue_1":"ACM Multimedia","year":"2005","title":"Tracking users' capture intention: a novel complementary view for home video content analysis","authors":"Tao Mei, Xian-Sheng Hua, He-Qin Zhou","author_ids":"1788123, 1746102, 2457832","abstract":"In this paper, we present a novel view to home video content analysis, which aims at tracking the <i>capture intention</i> of camcorder users. Based on the study of intention mechanism in psychology, a set of domain-specific capture intention concepts are defined. A comprehensive and extensible scheme consisting of video structuring, intention oriented feature analysis, as well as intention unit segmentation and classification is proposed to mine the users' capture intention. Experiments were carried on home video sequences of 90 hours in total, taken by 16 persons in recent 20 years. Both the user study and objective evaluations indicate that our proposed intention-based approach is an effective complement to existing home video content analysis schemes.","cites":"7","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"2aa9af7e2499a70e5efde27c403145b75286b9c7","venue_1":"ACM Multimedia","year":"2005","title":"Intention-based home video browsing","authors":"Tao Mei, Xian-Sheng Hua","author_ids":"1788123, 1746102","abstract":"This demonstration presents an efficient home video browsing system from a novel viewpoint -- <i>capture intention</i>. We extend our previous work to build up a comprehensive scheme to mine the capture intention, and based on this scheme, propose a novel home video browsing system. Such an intention based system assists both camcorder users and viewers to experience their personal home videos by providing two better manners of browsing -- by intention thumbnails and by intention curves. The user study indicates that our system offers users a more efficient way to search a given clip in a video than attention-based scheme.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"8208174a700c904eaee3ccabe4a09a275c914463","venue_1":"ACM Multimedia","year":"2001","title":"DEMAIS: designing multimedia applications with interactive storyboards","authors":"Brian P. Bailey, Joseph A. Konstan, John V. Carlis","author_ids":"1681836, 2478310, 1977396","abstract":"To create an innovative interactive multimedia application, a multimedia designer needs to rapidly explore numerous behavioral design ideas early in the design process, as creating innovative behavior is the cornerstone of creating innovative multimedia. Current tools and techniques do not support a designer's need for early behavior exploration, limiting her ability to rapidly explore and effectively communicate behavioral design ideas. To address this need, we have developed a sketch-based, interactive multimedia storyboard tool that uses a designer's ink strokes and textual annotations as an input design vocabulary. By operationalizing this vocabulary, the tool transforms an otherwise static sketch into a <i>working</i> example. The behavioral sketch can be quickly edited using gestures and an expressive visual language. By enabling a designer to explore and communicate behavioral design ideas using working examples early in the design process, our tool facilitates the creation of a more effective, compelling, and entertaining multimedia application.","cites":"63","conferencePercentile":"96.42857143"},{"venue":"ACM Multimedia","id":"198cc8e87af034f20faf95c72d2b752816aad56d","venue_1":"ACM Multimedia","year":"2002","title":"Duplicate detection in consumer photography and news video","authors":"Alejandro Jaimes, Shih-Fu Chang, Alexander C. Loui","author_ids":"1730325, 1735547, 1707590","abstract":"Consumers often make more than one photograph of the same scene, creating non-identical duplicates and near duplicates. In Kodak's consumer photography database, on average, 19% of the images, per roll, fall into this category. Automatic detection of duplicates, therefore, is extremely useful in applications that help users organize their image collections. We introduce the challenging problem of non-identical duplicate image detection in consumer photography, describe <i>STELLA</i> (a novel interactive personal image collection organization system), and give an overview of our novel framework for detecting duplicate and near duplicate consumer photographs and news videos.","cites":"18","conferencePercentile":"73.07692308"},{"venue":"ACM Multimedia","id":"3d62124d9908fbaae659887bb9dfe00dbd67f72b","venue_1":"ACM Multimedia","year":"1993","title":"Projection Detecting Filter for Video Cut Detection","authors":"Kiyotaka Otsuji, Yoshinobu Tonomura","author_ids":"3201720, 2000148","abstract":"This paper discusses a video cut detection method. Cut detection is an important basic technique for making videos easier to handle. First, this paper analyzes the distribution of the image difference V to clarify the characteristics that make V suitable for cut detection. Usually a motion insensitive parameter is used as V to avoid misdetection, but this makes it difficult to eliminate the loss of cuts. We propose a cut detection method that uses a projection detection filter. A motion sensitive V is used to stabilize V projections at cuts, and cuts are detected more reliably with this filter. The method can achieve high detection rates without increasing the rate of misdetection. Experimental results confirm the effectiveness of the filter. 3], and there has been no report about quantitative analysis [5, 6]. We consider that understanding the cut detection mechanism is important to resolve the problems experienced by all cut detection methods. In this paper, we propose a new cut detection method and report experimental results. In section 1, we study the cut detection mechanism by analyzing the distribution of image differences, and find out what is required for cut detection. In section 2, we discuss the practical problems of cut detection. A filtering method is proposed in section 3, and experimental results are shown in section 4. INTRODUCTION There have been many attempts at using video in computer and communication fields. However, there has been little improvement in the handling of video itself. Video cameras are ubiquitous but the information recorded on the tapes is useless until someone looks at it and indicates the tape's contents. Video information is difficult to handle because no machine can automatically \"recognize\" scenes with any degree of accuracy. Moreover, there are far more people with cameras producing video tapes than there are people capable of editing the tapes. We think it is necessary to convert video to make it easier to handle.","cites":"57","conferencePercentile":"76.74418605"},{"venue":"ACM Multimedia","id":"d6fb24b719fec0ffc285b4d3ed8d52d828f1760f","venue_1":"ACM Multimedia","year":"2007","title":"Structure-sensitive manifold ranking for video concept detection","authors":"Jinhui Tang, Xian-Sheng Hua, Guo-Jun Qi, Meng Wang, Tao Mei, Xiuqing Wu","author_ids":"8053308, 1746102, 7266598, 1731598, 1788123, 1787412","abstract":"Pairwise similarity of samples is an essential factor in graph propagation based semi-supervised learning methods. Usually it is estimated based on Euclidean distance. However, the <i>structural assumption</i>, which is a basic assumption in these methods, has not been taken into consideration in the normal pairwise similarity measure. In this paper, we propose a novel graph-based learning approach, named <i>Structure-Sensitive Manifold Ranking</i> (SSMR),based on a structure-sensitive similarity measure. Instead of using distance only, SSMR takes local distribution differences into account to more accurately measure pairwise similarity. Furthermore, we show that SSMR can also be deduced from a partial differential equation based anisotropic diffusion. Experiments conducted on the TRECVID dataset show that this approach significantly outperforms existing graph-based semi-supervised learning methods for video semantic concept detection.","cites":"24","conferencePercentile":"84.375"},{"venue":"ACM Multimedia","id":"bca196a52c967aa9cdef1b1d353b9f8278d3f192","venue_1":"ACM Multimedia","year":"2004","title":"A visuospatial memory cue system for meeting video retrieval","authors":"Takeshi Nagamine, Alejandro Jaimes, Kengo Omura, Kazutaka Hirata","author_ids":"2467521, 1730325, 2869396, 2263781","abstract":"We present a system based on a new, memory-cue paradigm for retrieving meeting video scenes. The system graically represents important memory retrieval cues such as room layout, participant's faces and sitting positions, etc.. Queries are formulated dynamically: as the user graically manipulates the cues, the query results are shown. Our system (1) helps users easily express the &#60;i>cues&#60;/i> they recall about a particular meeting; (2) helps users &#60;i>remember&#60;/i> new cues for meeting video retrieval. We discuss the experiments that motivate this new approach, implementation, and future work.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"69511f2cf0ff1892758d360e1416617987a2ed2c","venue_1":"ACM Multimedia","year":"2011","title":"Joint ACM workshop on human gesture and behavior understanding: (J-HGBU'11)","authors":"Maja Pantic, Alex Pentland, Alessandro Vinciarelli, Rita Cucchiara, Mohamed Daoudi, Alberto Del Bimbo","author_ids":"1694605, 1682773, 1719436, 1741922, 2909056, 8196487","abstract":"The ability to understand social signals of a person we are communicating with is the core of social intelligence. Social Intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. At the same time, human-centric multimedia applications for humans and about humans are becoming increasingly important. 3D modeled human-objects, like bodies, heads and faces are exploited for animation, security, and human computer interaction, while three dimensional motion of arms, legs and local body features is used for more complete human gesture, activity and behavior analysis. The Joint Human Gesture and Behavior Understanding (J-HGBU) workshop event consists of two parts focusing on these complementary challenges: the Workshop on Multimedia Access to 3D Human Objects (MA3HO'11) and the Workshop on Social Signal Processing (SSPW'11).","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"a8b2d190f9230d047b3d0e1f4223c4faf6fdfb95","venue_1":"ACM Multimedia","year":"2013","title":"Image search by graph-based label propagation with image representation from DNN","authors":"Yingwei Pan, Ting Yao, Kuiyuan Yang, Houqiang Li, Chong-Wah Ngo, Jingdong Wang, Tao Mei","author_ids":"3202968, 8543685, 2976163, 7179232, 1751681, 1688516, 1788123","abstract":"Our objective is to estimate the relevance of an image to a query for image search purposes. We address two limitations of the existing image search engines in this paper. First, there is no straightforward way of bridging the gap between semantic textual queries as well as users' search intents and image visual content. Image search engines therefore primarily rely on static and textual features. Visual features are mainly used to identify potentially useful recurrent patterns or relevant training examples for complementing search by image reranking. Second, image rankers are trained on query-image pairs labeled by human experts, making the annotation intellectually expensive and time-consuming. Furthermore, the labels may be subjective when the queries are ambiguous, resulting in difficulty in predicting the search intention. We demonstrate that the aforementioned two problems can be mitigated by exploring the use of click-through data, which can be viewed as the footprints of user searching behavior, as an effective means of understanding query. The correspondences between an image and a query are determined by whether the image was searched and clicked by users under the query in a commercial image search engine. We therefore hypothesize that the image click counts in response to a query are as their relevance indications. For each new image, our proposed graph-based label propagation algorithm employs neighborhood graph search to find the nearest neighbors on an image similarity graph built up with visual representations from deep neural networks and further aggregates their clicked queries/click counts to get the labels of the new image. We conduct experiments on MSR-Bing Grand Challenge and the results show consistent performance gain over various baselines. In addition, the proposed approach is very efficient, completing annotation of each query-image pair within just 15 milliseconds on a regular PC.","cites":"5","conferencePercentile":"72.66666667"},{"venue":"ACM Multimedia","id":"2d4efbaff4afe78a534a5145f417d4ba27ce63a3","venue_1":"ACM Multimedia","year":"2012","title":"GreenTube: power optimization for mobile videostreaming via dynamic cache management","authors":"Xin Li, Mian Dong, Zhan Ma, Felix C. A. Fernandes","author_ids":"1705796, 1926898, 1762531, 1679765","abstract":"Mobile video streaming has become one of the most popular applications in the trend of smartphone booming and the prevalence of 3G/4G networks, i.e., HSPA, HSPA+, and LTE. However, the prohibitively high power consumption by 3G/4G radios in smartphones reduces battery life significantly and thus severely hurts user experience. To tackle this challenge, we designed <i>GreenTube</i>, a system that optimizes power consumption for mobile video streaming by judiciously scheduling downloading activities to minimize unnecessary active periods of 3G/4G radio. GreenTube achieves this by dynamically managing the downloading cache based on user viewing history and network condition. We implemented GreenTube on Android-based smartphones. Experimental results show that GreenTube achieves large power reductions of more than 70% (on the 3G/4G radio) and 40% (for the whole system). We believe GreenTube is a desirable upgrade to the Android system, especially in the light of increasing LTE popularity.","cites":"22","conferencePercentile":"94.93670886"},{"venue":"ACM Multimedia","id":"82e7a3f11eccd043086220b6b39de9ec17f1d689","venue_1":"ACM Multimedia","year":"2014","title":"Rescue Tail Queries: Learning to Image Search Re-rank via Click-wise Multimodal Fusion","authors":"Xiaopeng Yang, Tao Mei, Yongdong Zhang","author_ids":"1851567, 1788123, 1699819","abstract":"Image search engines have achieved good performance for head (popular) queries by leveraging text information and user click data. However, there still remain a large number of tail (rare) queries with relatively unsatisfying search results, which are often overlooked in existing research. Image search for these tail queries therefore provides a grand challenge for research communities. Most existing re-ranking approaches, though effective for head queries, cannot be extended to tail. The assumption of these approaches that <i>the re-ranked list should not go far away from the initial ranked list</i> is not applicable to the tail queries. The challenge, thus, relies on how to leverage the possibly unsatisfying initial ranked results and the very limited click data to solve the search intent gap of tail queries.\n  To deal with this challenge, we propose to mine relevant information from the very few click data by leveraging click-wise-based image pairs and query-dependent multimodal fusion. Specifically, we hypothesize that images with more clicks are more relevant to the given query than the ones with no or relatively less clicks and the effects of different visual modalities to re-rank images are query-dependent. We therefore propose a novel query-dependent learning to re-rank approach for tail queries, called ``click-wise multimodal fusion.'' The approach can not only effectively expand training data by learning relevant information from the constructed click-wise-based image pairs, but also fully explore the effects of multiple visual modalities by adaptively predicting the query-dependent fusion weights. The experiments conducted on a real-world dataset with 100 tail queries show that our proposed approach can significantly improve initial search results by 10.88% and 9.12% in terms of NDCG@5 and NDCG@10, respectively, and outperform several existing re-ranking approaches.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"0b2f69314faab8548fd8fbd929eaef79a4514871","venue_1":"ACM Multimedia","year":"2004","title":"SenseWeb: collaborative image classification in a multi-user interaction environment","authors":"Roberto Lopez-Gulliver, Hiroko Tochigi, Tomohiro Sato, Masami Suzuki, Norihiro Hagita","author_ids":"2071712, 2332772, 1749000, 2549866, 1781078","abstract":"The SenseWeb system is a multi-user interactive information environment. Aimed at supporting the sharing of experiences and collaboration among multiple users allowing them to simultaneously interact with digital multimedia elements by using their bare hands. By sharing a common information space on a large screen, users can cooperatively search, filter, classify and interact with multimedia data in a natural and intuitive way. This paper introduces the system as well as preliminary experiment results to assess its effectiveness as a simultaneous multi-user interaction environment in collaborative image classification tasks. Results confirm our expectations of improvements in task completion time, ease of use and user satisfaction over multi-user but one-at-the-time interaction scenarios.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"6f2581d18cea9edcf0180e9bbfc89149edd50a23","venue_1":"ACM Multimedia","year":"1994","title":"Multimedia Databases and Information Systems (Panel)","authors":"Dragutin Petrovic, Farshid Arman, Charlie Judice, Alex Pentland, James O. Normile","author_ids":"2298852, 2338981, 2678959, 1682773, 2433073","abstract":"The role of computers in everyday usage is changing rapidly as Charlie Judice their ability to acquire and store images, video sequences, and audio clips has exploded in the recent past. These roles are APPLICATIONS AND PRACTICAL ISSUES: The success of expanded even further when one considers the possibilities this technology depends highly on its applications and the cost. presented by the networks of today and even more importantly What are the possible applications? Would people really pay for the networks that could emerge from merging cable and tele-them? How to bill someone? Per minute, per item, per search, phone companies in the near future. One of the most important by distance, combination? Copyright issues and how to enforce challenges for continued success and growth of multimedia is them? the ability to easily manage and access the information: storing gigabytes of data, organizing the information such that it is easily obtainable, searching capabilities to allow for convenient access, presenting and delivering the requestedinformation with appropriate quality of service, controlling access, and preserving copyrights. These functions are typically associated with databases which can easily be performed on text-based information; however, multimedia databases containing text, video, images, and audio present novel problems that must be addressed and resolved. This panel will discusss three of these issues that need to be resolved in order for multimedia databases and information systems to become a reality: information and data representation, query specification, and user interfaces. Other important issues. such as networking and storage, which will not be addressed. INFORMATION AND DATA REPRESENTATION: How is the information represented, what features are extracted, how are they extracted? How to compute and index these representa-tions? What is the level of user involvement in indexing? These represent the heart of the problem. [n addition, the ease with which users can brow se and navigate through the information space is critical and this is directly related to how the information is presented and indexed. HOW should the user specify what information he or she is looking at? Keywords as used today to describe contents of video and images are a good start, but they are also biased, labor intensive , error prone, incomplete, and could cause permanent loss of information. What types of user interfaces are available to allow non-database experts to gain access to the information by combination of searctthowse techniques? What kinds of interfaces and tools are needed …","cites":"0","conferencePercentile":"12.71186441"},{"venue":"ACM Multimedia","id":"25d0e335e0fe3e1707354bfccca9b534d7bb4420","venue_1":"ACM Multimedia","year":"2007","title":"Bipartite graph reinforcement model for web image annotation","authors":"Xiaoguang Rui, Mingjing Li, Zhiwei Li, Wei-Ying Ma, Nenghai Yu","author_ids":"3334955, 8392859, 2902955, 1705244, 1708598","abstract":"Automatic image annotation is an effective way for managing and retrieving abundant images on the internet. In this paper, a bipartite graph reinforcement model (BGRM) is proposed for web image annotation. Given a web image, a set of candidate annotations is extracted from its surrounding text and other textual information in the hosting web page. As this set is often incomplete, it is extended to include more potentially relevant annotations by searching and mining a large-scale image database. All candidates are modeled as a bipartite graph. Then a reinforcement algorithm is performed on the bipartite graph to re-rank the candidates. Only those with the highest ranking scores are reserved as the final annotations. Experimental results on real web images demonstrate the effectiveness of the proposed model.","cites":"41","conferencePercentile":"90.88541667"},{"venue":"ACM Multimedia","id":"307feea1e876d7c8c847bdeb50338c2d9243139e","venue_1":"ACM Multimedia","year":"2016","title":"LTA 2016: The First Workshop on Lifelogging Tools and Applications","authors":"Cathal Gurrin, Xavier Giró, Petia Radeva, Mariella Dimiccoli, Håvard D. Johansen, Hideo Joho, Vivek K. Singh","author_ids":"1737981, 3100480, 1724155, 2837527, 3042216, 1798501, 4685302","abstract":"The organisation of personal data is receiving increasing research attention due to the challenges we face in gathering, enriching, searching, and visualising such data. Given the increasing ease with which personal data being gathered by individuals, the concept of a lifelog digital library of rich multimedia and sensory content for every individual is fast becoming a reality. The LTA~2016 workshop aims to bring together academics and practitioners to discuss approaches to lifelog data analytics and applications; and to debate the opportunities and challenges for researchers in this new and challenging area.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2050c1bd348721544565fab63b260a7557e87388","venue_1":"ACM Multimedia","year":"2011","title":"Reliving on demand: a total viewer experience","authors":"Vivek K. Singh, Jiebo Luo, Dhiraj Joshi, Phoury Lei, Madirakshi Das, Peter O. Stubler","author_ids":"4685302, 1717319, 6163828, 2472012, 1697635, 2985249","abstract":"Billions of people worldwide use images and videos to capture various events in their lives. The primary purpose of the proposed media sharing application is digital re-living of those events by the photographers and their families and friends. The most popular tools for achieving this today are still static slide-shows (SSS) which primarily focus on visual effects rather than understanding the semantics of the media assets being used, or allowing different viewers (e.g. friends, family, who have different relationships, interests, time availabilities, and familiarities) any control over the flow of the show. We present a novel system that generates an aesthetically appealing and semantically drivable audio-visual media show based on several reliving dimensions of events, people, locations, and time. We allow each viewer to interact with the default presentation to 'on-the-fly' redirect the flow of reliving as desired from their individual perspectives. Moreover, each reliving session is logged and can be shared with other people over a wide array of platforms and devices, allowing sharing experience to go beyond the sharing of the media assets themselves. From a detailed analysis of the logged sessions across different user categories, we have obtained many interesting findings on the reliving needs, behaviors and patterns, which in turn validate our design motivations and principles.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"e2a90af93616be29e501cf0ac6c7666adfdbd917","venue_1":"ACM Multimedia","year":"2011","title":"Dynamic media show drivable by semantics","authors":"Vivek K. Singh, Jiebo Luo, Dhiraj Joshi, Madirakshi Das, Phoury Lei, Peter O. Stubler","author_ids":"4685302, 1717319, 6163828, 1697635, 2472012, 2985249","abstract":"We demonstrate a system to generate dynamic media shows that are significantly richer than static slide shows, which are currently the most popular form of photo playback. The goal is to enable media reliving experiences that are aesthetically pleasing, interactive, and semantically drivable as they center on people, locations, time, and events discovered in a media collection. Dynamic shows allow for better sharing of one's media collections in diverse social networks because people have different time availabilities and perspectives, and hence may want to interact, customize, and reroute the media flow per their individual needs.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"36706111c06bcc46ac660cb1ef1f1cd4e1940d61","venue_1":"ACM Multimedia","year":"2011","title":"From multimedia data to situation detection","authors":"Vivek K. Singh","author_ids":"4685302","abstract":"We are witnessing a phenomenal increase in multimodal human and device sensing to measure and report parameters such as temperature, vehicle speed, visual experiences, flu cases, and people happiness. Soon we expect these heterogeneous datasets (e.g. images, videos, weather sensors, check-ins and tweets) to become available in real-time in the Cloud for reasoning and decision making.\n Important human decisions however cannot be undertaken by piecemeal treatment of these individual data points. Rather we need computational tools to integrate and abstract these data points into higher level actionable representations.\n This underscores the need for computational tools to model and detect situations from large heterogeneous spatio-temporal data sets. In this thesis, we computationally define the notion of situations and propose a methodology to bridge the semantic gap between the widely available low level spatio-temporal data and actionable situation inference needed for decision making.\n We define a situation as: \"An actionable abstraction of observed spatio-temporal descriptors\".\n This definition underscores our viewpoint of computationally defining situations based on statistical descriptors (as opposed to say situation-calculus or recognition-by-parts), a focus on spatio-temporal data (which is indeed the most common connotation associated with situations), scoping of problem only to observable (via human/device sensors) data, and a focus on actionable abstractions (as defined explicitly by human domain experts).\n The problem of modeling and detecting situations from (STT) i.e. spatio-temporal-thematic data is relevant in multiple domains like traffic, weather, healthcare, business analysis, emergency response, and political decision making.\n Situation Modeling STT data spreads across very disparate application domains as well as data types. However, focusing on the commonalities and not the differences, we realize that there is a core set of operations which is central to defining spatio-temporal situations across different applications. Once a domain expert defines a situation of interest (e.g. a 'flu pandemic', 'hurricane advise') based on data sources, core operations, and user parameters, the same situation model can act as a standing query on realtime data streams and provide 'mass-personalization' to billions of end-users.\n Just like E/R modeling, or UML we merely provide the basic building blocks. It is each domain expert's responsibility to define actionable situations by combining these building blocks. These building blocks are designed to be computable, modular and explicit and hence translatable into executable code once the modeling is complete.\n Approach Our approach for integrating and characterizing heterogeneous spatio-temporal data is based on the concept of social pixels. We simply organize spatio-temporal values related to any theme on a two dimensional data grid. Such a grid provides heat-map like intuitive visualization, and also an image like computational data structure. Hence multiple spatio-temporal situational descriptors can be implemented as off-the-shelf image and video processing operations (Refer Fig 1).\n Current status We have made progress in terms of identifying the generic set of situation detection operations [1]. We have run multiple experiments with STT data sets to answer situational queries like 'what recommendation to give to user indicating flu-like symptoms' [2] and 'where to open a new iphone store'[1]. We are currently implementing the core STT analysis engine which will allow modeling and detection of multiple situation queries across applications. We are also finalizing a methodology to guide domain experts when they model situations in terms of building blocks like data sources, characterizations operators, and user parameters.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"2c0b5a70f62ebfd43770bfc2c35b486a7939ccd1","venue_1":"ACM Multimedia","year":"2013","title":"Automatic generation of social media snippets for mobile browsing","authors":"Wenyuan Yin, Tao Mei, Chang Wen Chen","author_ids":"8253274, 1788123, 7137737","abstract":"The ongoing revolution in media consumption from traditional PCs to the pervasiveness of mobile devices is driving the adoption of social media in our daily lives. More and more people are using their mobile devices to enjoy social media content while on the move. However, mobile display constraints create challenges for presenting and authoring the rich media content on screens with limited display size. This paper presents an innovative system to automatically generate magazine-like social media visual summaries, which is called \"snippet,\" for efficient mobile browsing. The system excerpts the most salient and dominant elements, i.e., a major picture element and a set of textual elements, from the original media content, and composes these elements into a text overlaid image by maximizing information perception. In particular, we investigate a set of aesthetic rules and visual perception principles to optimize the layout of the extracted elements by considering display constraints. As a result, browsing the snippet on mobile devices is just like quickly glancing at a magazine. To the best of our knowledge, this paper represents one of the first attempts at automatic social media snippet generation by studying aesthetic rules and visual perception principles. We have conducted experiments and user studies with social posts from news entities. We demonstrated that the generated snippets are effective at representing media content in a visually appealing and compact way, leading to a better user experience when consuming social media content on mobile devices.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"6d701b47fcc15fe2b4a8167c3f7fc1955ab10e18","venue_1":"ACM Multimedia","year":"2005","title":"Socially aware media","authors":"Alex Pentland","author_ids":"1682773","abstract":"Face-to-face communication conveys social context as well as words, and it is this social signaling that allows new information to be smoothly integrated into a shared, group-wide understanding. By building machines that understand social signaling and social context we can begin to make communication tools that keep remote users 'in the loop,' and can dramatically improve collective decision making.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"0e96c687bf2500344d88e45c8ca6ff2300d145a6","venue_1":"ACM Multimedia","year":"1997","title":"User Interface Evaluation of a Direct Manipulation Temporal Visual Query Language","authors":"Stacie Hibino, Elke A. Rundensteiner","author_ids":"1700871, 1715020","abstract":"As new query interfaces emerge for accessing multimedia data, formal user studies are needed to evaluate the usability of such interfaces. In this paper, we present results from a user interface evaluation of our temporal visual query language (TVQL). TVQL is a novel direct manipulation query interface for specifying temporal relationship queries over temporal events such as video data. In our user study, we compare TVQL to a forms-based temporal query language (TForms). Our results indicate that while subjects took longer to learn TVQL than TForrns, they were more efficient and more accurate in specifying temporal queries with the TVQL interface than with the TForms interface. KEYWORDS Temporal visual query language, temporal query filters, dynamic queries, user interface evaluation.","cites":"36","conferencePercentile":"78.57142857"},{"venue":"ACM Multimedia","id":"6c98e5fe4ff6f56aec2926b8071cd534a3e29325","venue_1":"ACM Multimedia","year":"1996","title":"MMVIS: Design and Implementation of a Multimedia Visual Information Seeking Environment","authors":"Stacie Hibino, Elke A. Rundensteiner","author_ids":"1700871, 1715020","abstract":"In our new paradigm for video analysis, we advocate the use of interactive visualizations where users can browse video data in search of temporal trends by specifying temporal queries via direct manipulation. In this paper, we describe the design and implementation of our MultiMedia Visual Information Seeking (MMVIS) system that successfully realizes this exploratory approach to temporal analysis. We present our design goals and decisions, including the design specifications of our subset selection query interface, our direct manipulation temporal visual query language (TVQL), and our temporal visualization (TViz) of results. We also present our strategies for implementing MMVIS—focusing in particular on our overall system architecture and the TVQL query processor. Finally, we briefly review a case study using real CSCW data and preliminary results of a user study, used to validate the utility of TVQL and MMVIS. 1. INTRODUCTION Previous approaches to video analysis have emphasized video annotation and coding over sophisticated analysis techniques (e.g., [6, 10, 21]). That is, by providing tools to simplify the process of creating annotations and more importantly for consistently coding relationships between objects or events in the data, these researchers have reduced the analysis process to specifying selection-type queries over the pre-coded data. For example, in a video of a design meeting setting, researchers could use previous video analysis systems to annotate all occurrences when \" P1 is digressing. \" Then, to find out how often P1 is digressing, they can pose the query, \" Select annotations where person=Pl A action=digressing \" and then count the number of results they retrieve. The problem with such an approach is that 1) it requires users to code relationships a","cites":"18","conferencePercentile":"36.11111111"},{"venue":"ACM Multimedia","id":"0b6082ce579225b6113c0984713a3cca8d350ff1","venue_1":"ACM Multimedia","year":"2016","title":"Summary for AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge","authors":"Michel F. Valstar, Jonathan Gratch, Björn W. Schuller, Fabien Ringeval, Roddy Cowie, Maja Pantic","author_ids":"1910575, 1730824, 1705602, 2124680, 1758549, 1694605","abstract":"The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood and Emotion\" will be the sixth competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and physiological depression and emotion analysis, with all participants competing under strictly the same conditions. The goal of the Challenge is to provide a common benchmark test set for multi-modal information processing and to bring together the depression and emotion recognition communities, as well as the audio, video and physiological processing communities, to compare the relative merits of the various approaches to depression and emotion recognition under well-defined and strictly comparable conditions and establish to what extent fusion of the approaches is possible and beneficial. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks.","cites":"9","conferencePercentile":"99.62962963"},{"venue":"ACM Multimedia","id":"4fd2a4b16b5d4e1ef2fb90c63a8d6229319de213","venue_1":"ACM Multimedia","year":"2008","title":"Social signal processing: state-of-the-art and future perspectives of an emerging domain","authors":"Alessandro Vinciarelli, Maja Pantic, Hervé Bourlard, Alex Pentland","author_ids":"1719436, 1694605, 1733733, 1682773","abstract":"The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for Social Signal Processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially-aware computing.","cites":"45","conferencePercentile":"94.49541284"},{"venue":"ACM Multimedia","id":"25dd83c3530bc8be063bf1148df3a601c1a1b042","venue_1":"ACM Multimedia","year":"2012","title":"Eyeke: what you hear is what you see","authors":"Takeshi Okunaka, Yoshinobu Tonomura","author_ids":"2090154, 2000148","abstract":"This demonstration shows an interactive visual-to-auditory scene sensing system called Eyeke (Eye Mike), which converts visual features of what it sees into sound output aiming at helping us to interact with our surroundings. Eyeke works robustly by restricting colors in its image processing and easy-and-effective calibration. Demonstrations of Eyeke as a music instrument and a camera-based scene sonar are shown.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"55b993a969ccf4404ddbc10a40112ef8e4e9d078","venue_1":"ACM Multimedia","year":"2013","title":"LAVES: an instant mobile video search system based on layered audio-video indexing","authors":"Wu Liu, Feibin Yang, Yongdong Zhang, Qinghua Huang, Tao Mei","author_ids":"1686917, 2068397, 1699819, 8304874, 1788123","abstract":"This demonstration presents an innovative instant mobile video search system based on layered audio-video indexing, called \"LAVES.\" Through the system, users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. Unlike most existing mobile video search applications which simply send the original video query to the cloud, the proposed mobile system is one of the first attempts towards instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is able to index large-scale video data using the layered audio-video indexing technique on the cloud, as well as extract light-weight joint audio-video signatures in real time and perform bipartite-graph-based progressive search process on the devices. On a 600 hours video dataset, the system can outperform the state-of-the-arts by achieving 90.79% precision when the query video is less than 10 seconds.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"6147aede0d4ab8ebbcb4c908c82b7dafec285b36","venue_1":"ACM Multimedia","year":"1999","title":"CyberCoaster: Polygonal line shaped slider interface to spatio-temporal media","authors":"Takashi Satou, Haruhiko Kojima, Akihito Akutsu, Yoshinobu Tonomura","author_ids":"8050581, 2350449, 2523644, 2000148","abstract":"In this demonstration, we propose CyberCoaster (Continuous Access by Spatio-Temporal slidER) as a new interface to time-varying visual data such as video and animation. CyberCoaster is a polygonal line shaped slider embedded in the visual data. CyberCoaster provides continuous, reversible, direct, and intuitive manipulation that has been independently offered by sliders and hot spots. Using CyberCoaster, users can manipulate the visual data, as if they were grabbing the objects directly. We demonstrate the applications of CyberCoaster including interactive video players, interactive animation, and 3D-image representation. As more and more video and animation data are available in digital form, it becomes more important not only to playback these time-varying visual data, but also to access them interactively. Temporal control of visual data is commonly realized by PLAY/STOP/FF/REW buttons, jog/shuttle dials, and sliders. To control video playback, a slider is often placed horizontally below the frame image. Users can scan the video back and forth by dragging the thumb. On the other hand, spatial control of time-varying visual data is realized by hot spots. The hot spot is an anchor region that accepts users' click and links to related topics. Conventionally, these two types of interfaces are used independently. Because a slider maps only a time-line from left to right, the slider operation is not concerned with the content. A hot spot responds to a click operation in an instant, therefore, it is difficult to control time continuously. Space and time are, however, related to each other in most visual data. For example, users sometimes need to scrub video frames to move objects in the video. Permission to make digital or hard copies ot all or part of this work for personal or classroom use is granted without tee provided that copies are not made or distributed for profit or commercial advantage and fhat copies bear this notice and the full citation on the first page. To copy otherwise. to republish, 10 post on servers or to redistribute to lists, requires prior specific permission and/or a fee. We have found that these features of both interfaces can be realized by a polygonal line shaped slider embedded in the time-varying visual data., We propose CyberCoaster as a new continuous, reversible, direct and intuitive interface to time-varying visual data. CyberCoaster represents camera-work or object trails in spatio-temporal data space, and accepts clicking and dragging operations of users as shown in Figure 1. Hot soot …","cites":"1","conferencePercentile":"19.32773109"},{"venue":"ACM Multimedia","id":"3c8c15da4334395803a08ee83e0b5e274e2d0502","venue_1":"ACM Multimedia","year":"1997","title":"PanoramaExcerpts: Extracting and Packing Panoramas for Video Browsing","authors":"Yukinobu Taniguchi, Akihito Akutsu, Yoshinobu Tonomura","author_ids":"2113938, 2523644, 2000148","abstract":"Browsing is a fundamental function in multimedia systems. This paper presents PanoramaExcerpts-a video browsing interface that shows a catalogue of two types of video icons: panoramic and keyframe icons. A panoramic icon is synthesized from a video segment taken with camera pan or tilt, and extracted using a camera operation estimation technique. A keyframe icon is extracted to supplement the panoramic icons; a shot-change detection algorithm is used. A panoramic icon represents the entire visible contents of a scene extended with camera pan or tilt, which is difficult to summarize using a single keyframe. For the automatic generation of PanoramaExcerpts, we propose an approach to integrate the following: (a) a shot-change detection method that detects instantaneous cuts as well as dissolves, with adaptive control over the sampling rate for efficient processing; (b) a method for locating segments that contain smooth camera pans or tilts, from which the panoramic icons can be synthesized; and (c) a layout method for packing icons in a space-efficient manner. We also describe the experimental results of the above three methods and the potential applications of PanoramaExcerpts. 1 INTRODUCTION Techniques for synthesizing panoramic images from an image sequence are attracting great attention. A panoramic image can be automatically created from multiple images by aligning and overlapping them using an image registration technique[6]. Panoramic image synthesis has many applications, including virtual reality[ll], model-based video compression[4], and video re-purposing[9] (e.g. creation of high-resolution images for publishing). Panoramic images have also been investigated as an intermediate representation for 3-D scene recovery and moving object recognition[l6]. In addition , panoramic images can be used as a synoptic view of a scene[4].","cites":"65","conferencePercentile":"85.71428571"},{"venue":"ACM Multimedia","id":"a9b47920055eb6d87a05b0e852f6ce62b08e0748","venue_1":"ACM Multimedia","year":"2010","title":"MM'10 workshop summary for SSPW: ACM workshop on social signal processing 2010","authors":"Maja Pantic, Alessandro Vinciarelli, Alex Pentland","author_ids":"1694605, 1719436, 1682773","abstract":"The Workshop on Social Signal Processing (SSPW) is the yearly event of the Social Signal Processing Network (EU-FP7 SSPNet project). This year's workshop programme consists of 4 premium Key Note Talks by Jeff Cohn, Alex Pentland. Justine Cassell, and Toyoaki Nishida, an oral session with 4 presentations, a poster session with 7 posters, and a panel session where the panelists will be the Key Note Speakers and the workshop organizers.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"c3d4e67a452f01c5e8f623fe601081ee1fa832fc","venue_1":"ACM Multimedia","year":"2006","title":"Human computing for interactive digital media","authors":"Alex Pentland, Jonathan Gips, Wen Dong, Will Stoltzman","author_ids":"1682773, 1731023, 2363243, 3250893","abstract":"Widespread adoption of interactive, peer-to-peer digital media will require a solution to the Privacy, Sharing, and Interest (PSI) problem: how can we know what the user wants to share with whom, and when, without burdening the user with constant updating of lists of approved users and sharing preferences? We argue that real-time analysis of user behavior provides an automatic PSI capability, allowing media to be automatically and proactively shared with a much lower user burden.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"c1d38483b61633d4bde52300c3215d50fdf08a45","venue_1":"ACM Multimedia","year":"2004","title":"ACM multimedia interactive art program: an introduction to the digital boundaries exhibition","authors":"Alejandro Jaimes, Pamela Jennings","author_ids":"1730325, 1902801","abstract":"The Digital Boundaries exhibition includes works that use multimedia to address issues of multiculturalism, identity, and awareness. By placing technology in new contexts to explore multimedia's impact on culture (and vice versa) we create a space for the discussion of new ideas and create an interdisciplinary impact by reinforcing a dialogue between the arts and multimedia communities. We discuss our motivation, the exhibition theme, the works selected, and their potential technical impact.","cites":"3","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"6357d3f168db4e65a217ceb4d410ccdbfaeda72c","venue_1":"ACM Multimedia","year":"2005","title":"Graph based multi-modality learning","authors":"Hanghang Tong, Jingrui He, Mingjing Li, Changshui Zhang, Wei-Ying Ma","author_ids":"8163721, 2770331, 8392859, 1700883, 1705244","abstract":"To better understand the content of multimedia, a lot of research efforts have been made on how to learn from multi-modal feature. In this paper, it is studied from a graph point of view: each kind of feature from one modality is represented as one independent graph; and the learning task is formulated as inferring from the constraints in every graph as well as supervision information (if available). For semi-supervised learning, two different fusion schemes, namely linear form and sequential form, are proposed. For each scheme, it is derived from optimization point of view; and further justified from two sides: similarity propagation and Bayesian interpretation. By doing so, we reveal the regular optimization nature, transductive learning nature as well as prior fusion nature of the proposed schemes, respectively. Moreover, the proposed method can be easily extended to unsupervised learning, including clustering and embedding. Systematic experimental results validate the effectiveness of the proposed method.","cites":"48","conferencePercentile":"92.57425743"},{"venue":"ACM Multimedia","id":"1e491d133f33b04d1f1f76631c07a63e5cd6174d","venue_1":"ACM Multimedia","year":"2011","title":"Honest signals: how social networks shape human behavior","authors":"Alex Pentland","author_ids":"1682773","abstract":"How did humans coordinate before we had sophisticated language capabilities? Pre-linguistic social species coordinate by signaling, and in particular 'honest signals' which actually cause changes in the listener. I will describe examples of human behaviors that are honest signals, and how they can be used to accurately predict and shape the outcomes of interactions (medical compliance, negotiation, trust assessment, depression screening, etc.). Understanding how human decision making is influenced by these pre-linguistic patterns of signaling also leads to very different ways to build incentives to change. In a recent trial we were able to change population behaviors using a social signaling strategy, and achieved twice the efficiency of standard behavior change schemes.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"8c49079113a3b6becbb834f3d82a948f69b3d8e4","venue_1":"ACM Multimedia","year":"2004","title":"An EPIC enhanced meeting environment","authors":"Qiong Liu, Frank Zhao, John Doherty, Don Kimber","author_ids":"1794500, 2204181, 3881379, 2178004","abstract":"ePic is an integrated presentation authoring and playback system that makes it easy to use a wide range of devices installed in one or multiple multimedia venues.","cites":"5","conferencePercentile":"45.58823529"},{"venue":"ACM Multimedia","id":"9704922d955fd5bf3a214e0438468924bd21b3b2","venue_1":"ACM Multimedia","year":"2004","title":"Locality preserving clustering for image database","authors":"Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, Xueyin Lin","author_ids":"1743055, 1745280, 3945955, 1705244, 2693354","abstract":"It is important and challenging to make the growing image repositories easy to search and browse. Image clustering is a technique that helps in several ways, including image data preprocessing, user interface designing, and search result representation. Spectral clustering method has been one of the most promising clustering methods in the last few years, because it can cluster data with complex structure, and the (near) global optimum is guaranteed. However, existing spectral clustering algorithms, like Normalized Cut, are difficult to handle data points out of training set. In this paper, we propose a clustering algorithm named Locality Preserving Clustering (LPC), which shares many of the data representation properties of nonlinear spectral method. Yet LPC provides an explicit mapping function which is defined everywhere, both on training data points and testing points. Experimental results show that LPC is more accurate than both \"direct Kmeans\" and \"PCA + Kmeans\". We also show that LPC produces in general comparable results with Normalized Cut, yet is more efficient than Normalized Cut.","cites":"35","conferencePercentile":"86.51960784"},{"venue":"ACM Multimedia","id":"40eab85dd9fcb166e73393e74ef6020299302750","venue_1":"ACM Multimedia","year":"2006","title":"Mobile camera supported document redirection","authors":"Qiong Liu, Paul McEvoy, Cheng-Jia Lai","author_ids":"1794500, 2387143, 2002056","abstract":"In this demonstration, we are going to illustrate how to use a mobile camera to redirect documents to various devices connected to the same network.","cites":"8","conferencePercentile":"62.95336788"},{"venue":"ACM Multimedia","id":"a24e135111db4d2efef04f1dc7b3580fc39665f1","venue_1":"ACM Multimedia","year":"1994","title":"From Generation to Generation: Multimedia, Community and Personal Stories (Panel)","authors":"Abbe Don, Laura Teodosio, Joe Lambert, Dana Atchley","author_ids":"2258864, 1765546, 1930302, 3096641","abstract":"Multimedia is rapidly proliferating into homes and schools. Tools for media creation are becoming more affordable and easier to use. Yet, the traditional models of centrally originated content — publishing and broadcasting — continue to dominate. Repurposed movie plots become the basis for adventure games on CD-ROM and television reruns have been promised \" on demand \" via 500-channel interactive cable systems, However, these panelists take an alternative view. The new medium enables the audience to create stories about themselves and their cultures and to share them through interactive networks as well as in face to face communities. Viewed in this fashion, we may be asking the wrong questions about the sources of content and ease of use in multimedia. Each panelist has participated in projects that embody personal and family stories, capture the experiences of endangered cultures, or provide a means for adults and chi Idren to create their own multimedia records, Drawing on examples from these projects, we will present the case that a new approach is necessary to unlock the potential of multimedia. Last year, Teodosio collaborated with Thomas Aguirre Smith and Barbara Woloch to create Multimedia Memory of Mayan Medicine, a mukimedia database that uses oral history and demonstrations by Mayan healers to document the use of medicinal plants in the Mayan Highlands of Chiapas Mexico. The multimedia system is explicitly designed for use by the Maya themselves and, as such, represents an important stimulus for the maintenance of traditional Maya herbal medicine, Ready access to the system is faci Iitated by its permanent location at the Ethnobotanical Herbarium of Chiapas (Herbario Etnobotnico de Chiapas) in San Cristobal de Las Casas. The database allows the Maya to explore their healing traditions in a form which is close to the original oraUvisual modes of learning, Users see where plants come from, how they are prepared to treat illness and how they are applied. If a Mayan Indian disagrees with the remedy for a particular illness, he can video tape his variation and add i[ to the database. The video databases will become a much used community resource in which younger Maya will explore the traditions of their elders and the surrounding Indian populations. As the film maker Richard Leacock has said, \" we must give cameras to the people who have stories to tell. \" Joe Lambert Amidst the debate on re-purposing the entertainment industry to service …","cites":"2","conferencePercentile":"29.66101695"},{"venue":"ACM Multimedia","id":"2cb9f8685912a82b5e53bfb7fb9a41af1a77dd3e","venue_1":"ACM Multimedia","year":"2008","title":"Content based automatic zooming: viewing documents on small displays","authors":"Patrick Chiu, Koichi Fujii, Qiong Liu","author_ids":"2895008, 1862528, 1794500","abstract":"We present an automatic zooming technique that leverages content analysis for viewing a document page on a small display such as a mobile phone or PDA. The page can come from a scanned document (bitmap image) or an electronic document (text and graphics data plus metadata). The page with text and graphics is segmented into regions. For each region, a scale-distortion function is constructed based on image analysis of the signal distortion that occurs at different scales. During interactive viewing of the document, as the user navigates by moving the viewport around the page, the zoom factor is automatically adjusted by optimizing the scale-distortion functions of the regions visible in the viewport.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"1a1d7d19d284409ff34587385ade97718c09555c","venue_1":"ACM Multimedia","year":"1994","title":"A Statistical Admission Control Algorithm for Multimedia Servers","authors":"Harrick M. Vin, Pawan Goyal, Alok Goyal","author_ids":"8734926, 1804259, 2622397","abstract":"A large-scale multimedia server, in practice, has to service a large number of clients simultaneously. Given the real-time requirements of each client and the fixed data transfer bandwidth of disks, a multimedia server must employ admission control algorithms to decide whether a new client can be admitted for service without violating the requirements of the clients already being serviced. In this paper, we present an admission control algorithm for multimedia servers which: (1) exploits the variation in access times of media blocks from disk as well as the variation in client load induced by variable rate compression schemes, and (2) provides statistical service guarantees to each client. The effectiveness of the algorithm is demonstrated through trace-driven simulations.","cites":"121","conferencePercentile":"94.91525424"},{"venue":"ACM Multimedia","id":"bee5ddbef483007752bc2ac122e1e9fd93745ffd","venue_1":"ACM Multimedia","year":"2009","title":"PACER: toward a cameraphone-based paper interface for fine-grained and flexible interaction with documents","authors":"Chunyuan Liao, Qiong Liu","author_ids":"2686363, 1794500","abstract":"Existing cameraphone-based interactive paper systems fall short of the flexibility of GUIs, partly due to their deficient fine-grained interactions, limited interaction styles and inadequate targeted document types. We present PACER, a platform for applications to interact with document details (e.g. individual words, East Asian characters, math symbols, music notes, and user-specified arbitrary image regions) of generic paper documents through a camera phone. With a see-through phone interface, a user can discover symbol recurrences in a document by pointing the phone's crosshair to a symbol within a printout. The user can also continuously move the phone over a printout for gestures to copy and email an arbitrary region, or play music notes on the printout.","cites":"4","conferencePercentile":"42.97520661"},{"venue":"ACM Multimedia","id":"2fa057a20a2b4a4f344988fee0a49fce85b0dc33","venue_1":"ACM Multimedia","year":"2013","title":"eHeritage of shadow puppetry: creation and manipulation","authors":"Min Lin, Zhenzhen Hu, Si Liu, Meng Wang, Richang Hong, Shuicheng Yan","author_ids":"4023598, 2428536, 2777248, 1731598, 1739103, 1698982","abstract":"To preserve the precious traditional heritage Chinese shadow puppetry, we propose the puppetry eHeritage, including a creator module and a manipulator module. The creator module accepts a frontal view face image and a profile face image of the user as input, and automatically generates the corresponding puppet, which looks like the original person and meanwhile has some typical characteristics of traditional Chinese shadow puppetry. In order to create the puppet, we first extract the central profile curve and warp the reference puppet eye and eyebrow to the shape of the frontal view eye and eyebrow. Then we transfer the puppet texture to the real face area. The manipulator module can accept the script provided by the user as input and automatically generate the motion sequences. Technically, we first learn atomic motions from a set of shadow puppetry videos. A scripting system converts the user's input to atomic motions, and finally synthesizes the animation based on the atomic motion instances. For better visual effects, we propose the sparsity optimization over simplexes formulation to automatically assemble weighted instances of different atomic actions into a smooth shadow puppetry animation sequence. We evaluate the performance of the creator module and the manipulator module sequentially. Extensive experimental results on the creation of puppetry characters and puppetry plays well demonstrate the effectiveness of the proposed system.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"3fb728d52c43f256518638eae360012507f08922","venue_1":"ACM Multimedia","year":"2012","title":"View-based 3D object retrieval by bipartite graph matching","authors":"Yue Wen, Yue Gao, Richang Hong, Huan-Bo Luan, Qiong Liu, Jialie Shen, Rongrong Ji","author_ids":"4953366, 1744619, 1739103, 1696019, 1794500, 1723020, 1725599","abstract":"Bipartite graph matching has been investigated in multiple view matching for 3D object retrieval. However, existing methods employ one-to-one vertex matching scheme while more than two views may share close semantic meanings in practice. In this work, we propose a bipartite graph matching method to measure the distance between two objects based on multiple views. In the proposed method, representative views are first selected by using view clustering for each object, and the corresponding weights are given based on the cluster results. A bipartite graph is constructed by using the two groups of representative views from two compared objects. To calculate the similarity between two objects, the bipartite graph is first partitioned to several subsets, and the views in the same sub-set are with high possibility to be with similar semantic meanings. The distances between two objects within individual subsets are then assembled through the graph to obtain the final similarity. Experimental results and comparison with the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"5928014e5d7cf62f05508a50008e25098134df5c","venue_1":"ACM Multimedia","year":"2004","title":"A web based multi-display presentation system","authors":"Frank Zhao, Qiong Liu","author_ids":"2204181, 1794500","abstract":"In this demonstration, we are going to illustrate how to give a presentation using multiple displays connected to the Internet.","cites":"1","conferencePercentile":"19.11764706"},{"venue":"ACM Multimedia","id":"e9c8f14269a46ae93c9e59d8ffbfb49026d51ce8","venue_1":"ACM Multimedia","year":"2012","title":"MixPad: augmenting interactive paper with mice & keyboards for cross-media and fine-grained interaction with documents","authors":"Xin Yang, Chunyuan Liao, Qiong Liu","author_ids":"6826800, 2686363, 1794500","abstract":"Existing interactive paper systems suffer from the disparate input devices for paper and computers. The finger-pen-only input on paper causes frequent devices switching (e.g. pen vs. mouse) during cross-media interactions, and may have issues of occlusion and precision. We propose MixPad, a novel interactive paper system, which allows users to exploit mice and keyboards to digitally manipulate fine-grained document content on paper, such as copying an arbitrary image region to a computer and clicking on a word for web search. With the combined input channels, MixPad enables richer digital functions on paper and facilitates bimanual operations cross different media. A preliminary user study shows positive feedback on this interaction technique.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"1384eaedd53648ebc9c9a104ba01c624d26f83f8","venue_1":"ACM Multimedia","year":"2012","title":"A fast video event recognition system and its application to video search","authors":"Yu-Gang Jiang, Qi Dai, Yingbin Zheng, Xiangyang Xue, Jie Liu, Dong Wang","author_ids":"1717861, 8678020, 3015119, 5507458, 1699746, 1726751","abstract":"Techniques for recognizing complex events in diverse Internet videos are important in many applications. State-of-the-art video event recognition approaches normally involve modules that demand extensive computation, which prevents their application to large scale problems. In this demonstration, we present a fast video event recognition system, which requires just a few seconds to process a general YouTube video with a few minutes of duration. The development of this system is grounded on several important findings from a large set of empirical studies, where we systematically evaluated many technical options for each critical module of a present-day video event recognition framework. Pooling the insights gained from this study leads to a speeded-up event recognition system that is 220-times faster than a decent baseline while still has a high degree of recognition accuracy. We also demonstrate the technical feasibility of using event recognition results as the sole clue for video search, where the similarity of videos is determined based on the consistency of the event recognition confidence scores. We showcase this capability using an Internet video dataset containing about 10 thousands of YouTube videos. Very promising results were observed.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"47628d640f2dcbe8cb89aac3c9bc9bc40ebbe9db","venue_1":"ACM Multimedia","year":"2003","title":"Knowing a tree from the forest: art image retrieval using a society of profiles","authors":"Kai Yu, Wei-Ying Ma, Volker Tresp, Zhao Xu, Xiaofei He, HongJiang Zhang, Hans-Peter Kriegel","author_ids":"1736727, 1705244, 1700754, 7814478, 3945955, 1718558, 1688561","abstract":"This paper aims to address the problem of art image retrieval (AIR), which aims to help users find their favorite painting images. AIR is of great interests to us because of its application potentials and interesting research challenges---the retrieval is not only based on painting contents or styles, but also heavily based on user <i>preference profiles</i>. This paper describes the collaborative ensemble learning, a novel statistical learning approach to this task. It at first applies probabilistic support vector machines (SVMs) to model each individual user's profile based on given examples, i.e. liked or disliked paintings. Due to the high complexity of profile modelling, the SVMs can be rather <i>weak</i> in predicting preferences for new paintings. To overcome this problem, we combine a society of users' profiles, represented by their respective SVM models, to predict a given user's preferences for painting images. We demonstrate that the combination scheme is embedded in a Bayesian framework and retains intuitive interpretations---like-minded users are likely to share similar preferences. We report extensive empirical studies based on two experimental settings. The first one includes some controlled simulations performed on 4533 painting images. In the second setting, we report evaluations based on user preferences collected through an online web-based survey. Both experiments demonstrate that the proposed approach achieves excellent performance in terms of capturing a user's diverse preferences.","cites":"11","conferencePercentile":"52.25225225"},{"venue":"ACM Multimedia","id":"39cc55356215fef3f975c74fd024441dcdc20b65","venue_1":"ACM Multimedia","year":"2015","title":"Summarization-based Video Caption via Deep Neural Networks","authors":"Guang Li, Shubo Ma, Yahong Han","author_ids":"5003981, 3003192, 2302512","abstract":"Generating appropriate descriptions for visual content draws increasing attention recently, where the promising progresses were obtained owing to the breakthroughs in deep neural networks. Different from the traditional SVO (subject, verb, object) based methods, in this paper, we propose a novel framework of video caption via deep neural networks. For each frame, we extract visual features by a fine-tuned deep Convulutional Neural Networks (CNN), which are then fed into a Recurrent Neural Networks (RNN) to generate novel sentences descriptions for each frame. In order to obtain the most representative and high-quality descriptions for target video, a well-devised automatic summarization process is incorporated to reduce the noises by ranking on the sentence-sequence graph. Moreover, our framework owns the merit of describing out-of-sample videos by transferring knowledge from pre-captioned images. Experiments on the benchmark datasets demonstrate our method has better performance than the state-of-the-art methods of video caption in language generation metrics as well as SVO accuracy.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"234cc247db13b0ecbc06917921ea78894dfe947f","venue_1":"ACM Multimedia","year":"2002","title":"Learning and inferring a semantic space from user's relevance feedback for image retrieval","authors":"Xiaofei He, Wei-Ying Ma, Oliver King, Mingjing Li, HongJiang Zhang","author_ids":"3945955, 1705244, 8375860, 8392859, 1718558","abstract":"As current methods for content-based retrieval are incapable of capturing the semantics of images, we experiment with using spectral methods to infer a semantic space from user's relevance feedback, so the system will gradually improve its retrieval performance through accumulated user interactions. In addition to the long-term learning process, we also model the traditional approaches to query refinement using relevance feedback as a short-term learning process. The proposed short- and long-term learning frameworks have been integrated into an image retrieval system. Experimental results on a large collection of images have shown the effectiveness and robustness of our proposed algorithms.","cites":"38","conferencePercentile":"85.47008547"},{"venue":"ACM Multimedia","id":"7e21438a1dbe0d352e3fcbe3bdfc071b170b63a2","venue_1":"ACM Multimedia","year":"2008","title":"Virtual physics circus","authors":"Don Kimber, Eleanor G. Rieffel, Jim Vaughan, John Doherty","author_ids":"2178004, 1710276, 2133761, 3881379","abstract":"This video shows the Virtual Physics Circus, a kind of playground for experimenting with simple physical models. The system makes it easy to create worlds with common physical objects such as swings, vehicles, ramps, and walls, and interactively play with those worlds. The system can be used as a creative art medium as well as to gain understanding and intuition about physical systems. The system can be controlled by a number of UI devices such as mouse, keyboard, joystick, and tags which are tracked in 6 degrees of freedom.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"2a1ee237e694542e4b5614888fb80b5c370eff3b","venue_1":"ACM Multimedia","year":"1995","title":"Commands as Media: Design and Implementation of a Command Stream","authors":"Jonathan L. Herlocker, Joseph A. Konstan","author_ids":"2658798, 2478310","abstract":"nized with other media streams, such as audio or video, to define presentations. The command stream was designed to help support the creation of multimedia presentations belonging to three domains. Choreographed Presentations: Presentations offering multiple synchronized media types, but are not limited to a pre-defined media types, nor are they limited to media directly playable by a computer. These presentations can control external devices such as room lighting, audio/visual equipment , and special effects such as stage fog or fireworks. An example of a choreographed presentation could be a interactive educational presentation on astronomy. The presentation would control a telescope attached to the computer. As part of the presentation, a student might select a point on map of the solar system. The presentation would move the telescope to the given point in the sky, then display the image on the screen. Enriched Interactive Presentations: Presentations that incorporate a greater interactivity and adaptability into a browsing environment. Such presentations require an extensive user interface component to allow a user to interact with the presentation, affecting its playback. These presentations can change media types or change presentation of media based on user input. These highly interactive presentations can also generate dynamic presentation elements based on user input or browsing environment. A multilingual, multi-background training presentation is an example of one such presentation. After determining the native language of the student, the presentation can select the media streams that are understandable by the student. The presentation will ask the user questions in order to determine the extent of the user's knowledge in the subject being taught. Then the presentation will present the user with training for knowledge and skills that the user can understand and has not already learned. Presentations with Simulations. Presentations containing interactive simulations of real processes. This requires the presentation to be able to display customized simulation runs based on input from a user, without prerecording simulation runs. A physics lab presentation might use simulations to help teach a student about conservation of momentum. A student could change parameters such as ABSTRACT We present a new medium composed of arbitrary commands. This command stream is a presentation medium that can be browsed at varying speeds, forwards and backwards, and with random access. Command streams can be synchronized with video, audio, and other media in multimedia presentations. We have used them to implement animation, timed user interaction, and device control. This …","cites":"9","conferencePercentile":"20"},{"venue":"ACM Multimedia","id":"6dac15df6545b883434ab18fbb21f8b956f897d1","venue_1":"ACM Multimedia","year":"2008","title":"Photo-based question answering","authors":"Tom Yeh, John J. Lee, Trevor Darrell","author_ids":"1704158, 3123113, 1753210","abstract":"Photo-based question answering is a useful way of finding information about physical objects. Current question answering (QA) systems are text-based and can be difficult to use when a question involves an object with distinct visual features. A photo-based QA system allows direct use of a photo to refer to the object. We develop a three-layer system architecture for photo-based QA that brings together recent technical achievements in question answering and image matching. The first, template-based QA layer matches a query photo to online images and extracts structured data from multimedia databases to answer questions about the photo. To simplify image matching, it exploits the question text to filter images based on categories and keywords. The second, information retrieval QA layer searches an internal repository of resolved photo-based questions to retrieve relevant answers. The third, human-computation QA layer leverages community experts to handle the most difficult cases. A series of experiments performed on a pilot dataset of 30,000 images of books, movie DVD covers, grocery items, and landmarks demonstrate the technical feasibility of this architecture. We present three prototypes to show how photo-based QA can be built into an online album, a text-based QA, and a mobile application.","cites":"27","conferencePercentile":"88.99082569"},{"venue":"ACM Multimedia","id":"2e13a77ca535aa0b92a3fb49e2ed7405be8886cc","venue_1":"ACM Multimedia","year":"2000","title":"A situated computing framework for mobile and ubiquitous multimedia access using small screen and composite devices","authors":"Thai-Lai Pham, Georg Schneider, Stuart Goose","author_ids":"2061509, 2070478, 2385465","abstract":"In recent years, small screen devices, such as cellular phones or Personal Digital Assistants (PDAs), enjoy phenomenal popularity. PDAs can be used to complement traditional computing systems to access personal multimedia information beyond the usage as digital organizers. However, due to the physical limitations accessing rich multimedia contents and diverse services using a single PDA is more difficult. Hence, the Situated Computing Framework (SCF) research project at Siemens Corporate Research (SCR) aims to develop a ubiquitous computing infrastructure that facilitates nomadic users to access rich multimedia contents using small screen devices. This paper describes a new distributed computing concept, the <i>Small Screen/Composite Device (SS/CD)</i> framework, which offers mobile users new classes of ubiquitous and mobile multimedia services without to limit the diversity and the richness of the provided services.","cites":"32","conferencePercentile":"85.86956522"},{"venue":"ACM Multimedia","id":"e13dfd4cb4bd1531838b49807139fc4de0b76990","venue_1":"ACM Multimedia","year":"2010","title":"Modeling, detecting, and processing events in multimedia","authors":"Ansgar Scherp, Ramesh Jain, Mohan S. Kankanhalli, Vasileios Mezaris","author_ids":"1753135, 4521564, 1744045, 1737436","abstract":"1. MOTIVATION Humans think in terms of events and entities. Events provide a natural abstraction of happenings in the real world. The concept of events has a long history in foundational sciences such as philosophy and linguistics. After first developing objects-based and entity-based approaches, computer science research is now addressing the concept of events and building many applications that consider events at least as important as objects. Consequently, we find many different solutions and approaches for modeling, detecting, and processing events. In addition, we find different applications that are based on events and make use of events. Conferences and workshops on events in computer science typically deal with the capturing, processing, and management of low-level events such as publish/subscribe systems and middleware solutions [3], complex event processing [1] and event stream processing [8], Semantic Web services [5], and reactivity for the Semantic Web [2]. Our understanding of events is different from the technical , low-level events above. Although this work is very essential for an efficient execution of the applications build on top of such approaches, the understanding of the concept of Copyright is held by the author/owner(s). events is disconnected from the domain-level of events that the actual users of such applications have to deal with. However , considering multimedia data, its semantics is naturally closely tied to the event(s) it documents. We apply events to capture and represent human experience , i.e., to describe on a high-level the occurrences in which humans participate. These events are subject to discussions and interpretations by humans [4]. They may be very complex and linked to a variety of modeling aspects [7, 6], namely the participation of living and non-living objects in events, the temporal duration of events, and the spatial extension of objects. In addition, different kind of event relationships shall be supported like mereological relationships (composition of events), causal relationships, and correlation relationships. One also needs to consider the experiential aspect , i.e., the annotation of events with sensor data such as media data. As domain-level events are subject to discussions and interpretations, different contextual points of view to events need to be supported [4]. Such domain-level events are important in a large variety of domains like emergency response, sports, news, law, and others. This year's International Workshop on Events in Multi-media 1 (EiMM10) is the second edition of the series of EiMM workshops, following the very successful first workshop …","cites":"16","conferencePercentile":"85.20547945"},{"venue":"ACM Multimedia","id":"6652413010811b414c37918ccfa15bc9026f9f53","venue_1":"ACM Multimedia","year":"2013","title":"Stereotime: a wireless 2D and 3D switchable video communication system","authors":"You Yang, Qiong Liu, Yue Gao, Binbin Xiong, Li Yu, Huan-Bo Luan, Rongrong Ji, Qi Tian","author_ids":"2807392, 1794500, 1744619, 2289810, 1720783, 1696019, 1725599, 1724745","abstract":"Mobile 3D video communication, especially with 2D and 3D compatible, is a new paradigm for both video communication and 3D video processing. Current techniques face challenges in mobile devices when bundled constraints such as computation resource and compatibility should be considered. In this work, we present a wireless 2D and 3D switchable video communication to handle the previous challenges, and name it as Stereotime. The methods of Zig-Zag fast object segmentation, depth cues detection and merging, and texture-adaptive view generation are used for 3D scene reconstruction. We show the functionalities and compatibilities on 3D mobile devices in WiFi network environment.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"0ef1383bf0a6d755ae55363b52f01501b5e3ff99","venue_1":"ACM Multimedia","year":"2007","title":"SMSBlogging: blog-on-the-street public art project","authors":"Archana Prasad, Sean Olin Blagsvedt, Kentaro Toyama","author_ids":"1779202, 1789487, 1769685","abstract":"In this paper we describe the experimental set-up and execution of a public art project. The aim was to explore the use of SMSBlogging for the purpose of community building and creative self-expression. We also discuss the results from this experiment and show our findings from six blog-on-the-street art acts in Bangalore, India, that introduce SMSBlogging technology in an iterative design process.","cites":"1","conferencePercentile":"19.27083333"},{"venue":"ACM Multimedia","id":"cf52545ed18be7288376063a0c9ab339d0e8ff78","venue_1":"ACM Multimedia","year":"2013","title":"Temporal encoded F-formation system for social interaction detection","authors":"Tian Gan, Yongkang Wong, Daqing Zhang, Mohan S. Kankanhalli","author_ids":"1684093, 3026404, 4368063, 1744045","abstract":"In the context of a social gathering, such as a cocktail party, the memorable moments are generally captured by professional photographers or by the participants. The latter case is often undesirable because many participants would rather enjoy the event instead of being occupied by the photo-taking task. Motivated by this scenario, we propose the use of a set of cameras to automatically take photos. Instead of performing dense analysis on all cameras for photo capturing, we first detect the occurrence and location of social interactions via F-formation detection. In the sociology literature, F-formation is a concept used to define social interactions, where each detection only requires the spatial location and orientation of each participant. This information can be robustly obtained with additional Kinect depth sensors. In this paper, we propose an extended F-formation system for robust detection of interactions and interactants. The extended F-formation system employs a heat-map based feature representation for each individual, namely Interaction Space (IS), to model their location, orientation, and temporal information. Using the temporally encoded IS for each detected interactant, we propose a best-view camera selection framework to detect the corresponding best view camera for each detected social interaction. The extended F-formation system is evaluated with synthetic data on multiple scenarios. To demonstrate the effectiveness of the proposed system, we conducted a user study to compare our best view camera ranking with human's ranking using real-world data.","cites":"12","conferencePercentile":"88.66666667"},{"venue":"ACM Multimedia","id":"0b577df03bbe17b07971030543b68054b16553b0","venue_1":"ACM Multimedia","year":"2008","title":"Haptics technologies: theory and applications from a multimedia perspective","authors":"Abdulmotaleb El-Saddik, Jongeun Cha, Kanav Kahol","author_ids":"1695990, 1690544, 3105442","abstract":"The desire for natural intuitive modes of interactions with digital media has led to development of multimodal interfaces that aim to engage the users through a confluence of modalities such as audio, video etc. Haptic systems that enable touch based interactions with digital environments are a recent addition to multimodal systems and have widespread applications; there is a need to develop a sound design, development and evaluation strategy to leverage the availability of the haptic modality. This tutorial aims to provide an initial impetus in the direction of enabling multimedia researchers to conduct research in the area of haptic user interfaces. The tutorial will present the audience with an introduction to the field of haptics. The presented material will be made accessible to the multimedia community, relating material from the haptics domain to multimedia algorithms, systems etc. At the completion of the tutorial, the students will be able to analyze and apply algorithms and strategies for design, development, and evaluation of touch based user interfaces.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"b04a8e95ba13e343291590d85575d77dfa77c44f","venue_1":"ACM Multimedia","year":"2005","title":"Web image clustering by consistent utilization of visual features and surrounding texts","authors":"Bin Gao, Tie-Yan Liu, Tao Qin, Xin Zheng, QianSheng Cheng, Wei-Ying Ma","author_ids":"1678646, 1744859, 8193913, 1743055, 1758097, 1705244","abstract":"Image clustering, an important technology for image processing, has been actively researched for a long period of time. Especially in recent years, with the explosive growth of the Web, image clustering has even been a critical technology to help users digest the large amount of online visual information. However, as far as we know, many previous works on image clustering only used either low-level visual features or surrounding texts, but rarely exploited these two kinds of information in the same framework. To tackle this problem, we proposed a novel method named consistent bipartite graph co-partitioning in this paper, which can cluster Web images based on the consistent fusion of the information contained in both low-level features and surrounding texts. In particular, we formulated it as a constrained multi-objective optimization problem, which can be efficiently solved by semi-definite programming (SDP). Experiments on a real-world Web image collection showed that our proposed method outperformed the methods only based on low-level features or surround texts.","cites":"72","conferencePercentile":"96.53465347"},{"venue":"ACM Multimedia","id":"3b9f492c9c594e77b205bd56e09f06a516f00566","venue_1":"ACM Multimedia","year":"2004","title":"Hierarchical clustering of WWW image search results using visual, textual and link information","authors":"Deng Cai, Xiaofei He, Zhiwei Li, Wei-Ying Ma, Ji-Rong Wen","author_ids":"1745280, 3945955, 2902955, 1705244, 3120972","abstract":"We consider the problem of clustering Web image search results. Generally, the image search results returned by an image search engine contain multiple topics. Organizing the results into different semantic clusters facilitates users' browsing. In this paper, we propose a hierarchical clustering method using visual, textual and link analysis. By using a vision-based page segmentation algorithm, a web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. By using block-level link analysis techniques, an image graph can be constructed. We then apply spectral techniques to find a Euclidean embedding of the images which respects the graph structure. Thus for each image, we have three kinds of representations, i.e. visual feature based representation, textual feature based representation and graph based representation. Using spectral clustering techniques, we can cluster the search results into different semantic clusters. An image search example illustrates the potential of these techniques.","cites":"170","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"2772c2bab5609f194cbb3e2cf4cf383d03f57987","venue_1":"ACM Multimedia","year":"2004","title":"Learning an image manifold for retrieval","authors":"Xiaofei He, Wei-Ying Ma, HongJiang Zhang","author_ids":"3945955, 1705244, 1718558","abstract":"We consider the problem of learning a mapping function from low-level feature space to high-level semantic space. Under the assumption that the data lie on a submanifold embedded in a high dimensional Euclidean space, we propose a relevance feedback scheme which is naturally conducted only on the image manifold in question rather than the total ambient space. While images are typically represented by feature vectors in Rn, the natural distance is often different from the distance induced by the ambient space Rn. The geodesic distances on manifold are used to measure the similarities between images. However, when the number of data points is small, it is hard to discover the intrinsic manifold structure. Based on user interactions in a relevance feedback driven query-by-example system, the intrinsic similarities between images can be accurately estimated. We then develop an algorithmic framework to approximate the optimal mapping function by a Radial Basis Function (RBF) neural network. The semantics of a new image can be inferred by the RBF neural network. Experimental results show that our approach is effective in improving the performance of content-based image retrieval systems.","cites":"88","conferencePercentile":"94.60784314"},{"venue":"ACM Multimedia","id":"0168fcd48510114b427be0f7d0c653d820e7304f","venue_1":"ACM Multimedia","year":"2005","title":"ACM multimedia interactive art program: an introduction to the presence/absence exhibition","authors":"Alejandro Jaimes, Andrew W. Senior, Wolfgang Muench","author_ids":"1730325, 1801333, 3022232","abstract":"The second ACM Multimedia Art program followed the successful formula used in ACM MM 2005, consisting of a session of long papers, a selection of posters and an art exhibition of multimedia works displayed at a gallery for a period encompassing the conference duration. \"Presence/Absence\" was selected as the central theme for the exhibition. In this paper, we discuss our motivations in organizing an art program at ACM MM, the exhibition theme, the works selected, and their potential impact in the technical community.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"af54c2ca5e497fe780639eae318d58da213c2b07","venue_1":"ACM Multimedia","year":"2004","title":"Video segmentation combining similarity analysis and classification","authors":"Matthew Cooper","author_ids":"4268667","abstract":"In this paper, we compare several recent approaches to video segmentation using pairwise similarity. We first review and contrast the approaches within the common framework of similarity analysis and kernel correlation. We then combine these approaches with non-parametric supervised classification for shot boundary detection. Finally, we discuss comparative experimental results using the 2002 TRECVID shot boundary detection test collection.","cites":"12","conferencePercentile":"65.93137255"},{"venue":"ACM Multimedia","id":"4438fc4b5e3c75052591fb815dadacff661f6d7b","venue_1":"ACM Multimedia","year":"2014","title":"Multi-modal Language Models for Lecture Video Retrieval","authors":"Huizhong Chen, Matthew Cooper, Dhiraj Joshi, Bernd Girod","author_ids":"2896700, 4268667, 6163828, 7811296","abstract":"We propose Multi-modal Language Models (MLMs), which adapt latent variable techniques for document analysis to exploring co-occurrence relationships in multi-modal data. In this paper, we focus on the application of MLMs to indexing text from slides and speech in lecture videos, and subsequently employ a multi-modal probabilistic ranking function for lecture video retrieval. The MLM achieves highly competitive results against well established retrieval methods such as the Vector Space Model and Probabilistic Latent Semantic Analysis. When noise is present in the data, retrieval performance with MLMs is shown to improve with the quality of the spoken text extracted from the video.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"cb4ba09dcdf9c299a0de8ebae59237d208cf3693","venue_1":"ACM Multimedia","year":"2015","title":"R2P: Recomposition and Retargeting of Photographic Images","authors":"Hui-Tang Chang, Po-Cheng Pan, Yu-Chiang Frank Wang, Ming-Syan Chen","author_ids":"2146380, 2757069, 2947229, 1691171","abstract":"In this paper, we propose a novel approach for performing joint recomposition and retargeting of photographic images (R2P). Given a reference image of interest, our method is able to automatically alter the composition of the input source image accordingly, while the recomposed output will be jointly retargeted to fit the reference. This is achieved by recomposing the visual components of the source image via graph matching, followed by solving a constrained mesh-warping based optimization problem for retargeting. As a result, the recomposed output image would fit the reference while suppressing possible distortion. Our experiments confirm that our proposed R2P method is able to achieve visually satisfactory results, without the need to use pre-collected labeled data or predetermined aesthetics rules.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"b97ac22258a07683b7b96490370e322d066d7d2e","venue_1":"ACM Multimedia","year":"2013","title":"Summary abstract for the 1st ACM international workshop on personal data meets distributed multimedia","authors":"Vivek K. Singh, Tat-Seng Chua, Ramesh Jain, Alex Pentland","author_ids":"4685302, 1684968, 4521564, 1682773","abstract":"Multimedia data are now created at a macro, public scale as well as individual personal scale. While distributed multimedia streams (e.g. images, microblogs, and sensor readings) have recently been combined to understand multiple spatio-temporal phenomena like epidemic spreads, seasonal patterns, and political situations; personal data (via mobile sensors, quantified-self technologies) are now being used to identify user behavior, intent, affect, social connections, health, gaze, and interest level in real time. An effective combination of the two types of data can revolutionize multiple applications ranging from healthcare, to mobility, to product recommendation, to content delivery. Building systems at this intersection can lead to better orchestrated media systems that may also improve users' social, emotional and physical well-being. For example, users trapped in risky hurricane situations can receive personalized evacuation instructions based on their health, mobility parameters, and distance to nearest shelter. This workshop bring together researchers interested in exploring novel techniques that combine multiple streams at different scales (macro and micro) to understand and react to each user's needs.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"c42034ca498bec0ac608279b9e8114e2a89dcce1","venue_1":"ACM Multimedia","year":"2008","title":"Connecting artists and scientists in multimedia research","authors":"Andruid Kerne, Ron Wakkary, Frank Nack, Amanda Steggell, Alejandro Jaimes, K. Selçuk Candan, Alberto Del Bimbo, Pamela Jennings, Aleksandra Dulic","author_ids":"1694380, 1783186, 1679840, 2576053, 1730325, 1720972, 8196487, 1902801, 2727530","abstract":"Historically, the ACM Multimedia Conference is split into a \"technical\" program and an \"arts\" program. These programs sometimes seem completely separate from one another, victims of a \"semantic gap\" between disciplines. The goal of this panel is to create a space in which scientists learn from artists, and arts from science. We need to discover new connections between modalities of research. In order to create the most exciting and powerful future forms of interactive multimedia systems, the ones that will create the most beneficial broader impact on humanity, we need to foster new collaborations between artists and scientists. This panel seeks to bridge the great divide of language and communities that has fragmented us, creating a new space for developing connections between the arts and sciences of multimedia research, as embodied through the artists and scientists of ACM Multimedia. The goal is to make this conference a premier site for catalyzing emergent connections.\n Among the ancient Greeks, the techne, which included the sciences, were based in the arts. Our modes of knowledge production have since separated and grown alienated. \"What weird stuff are those people doing?\" To bridge the divide, we must understand and acknowledge differences, and use this as a basis for discovering common ground. In the sciences, knowledge is constructed empirically, through hypotheses and validations. Aesthetics and concepts play essential roles in how works of art are formulated to stimulate experiences. Where do these aims intersect and how might they inform each other?\n In this conference, one track develops pattern recognition methods for content analysis and retrieval. Another develops network and system techniques. Applications invoke these methods in usable systems. Human-centered multimedia serves as a bridge connecting human experiences with algorithmic methods. The interactive art program develops new concepts of how multimedia can function culturally, amidst society, through tangible demonstrations of these concepts.\n Juxtaposing differences in methodologies and epistemologies is a method for provoking thought and identifying connections, which can lead to the development of new knowledge [1]. The goal of the panel is to catalyze discussions that build a foundation of mutual understanding and respect, and from this foundation, to build new ideas and human relationships that can lead to fruitful collaborations in the cycles to come.\n The panel will begin by asking each participant to characterize their approach to research, and to consider connections between the arts and sciences. \"How do you formulate research goals and objectives? What are the most significant methods that you use to carry them out?\" Significant contrasts in the epistemologies that underlie different modalities of research will be exposed. Artists will be asked, \"How can multimedia content analysis, processing, retrieval, networking, applications, and human-centered systems contribute to your art? How can your art contribute to multimedia research in content analysis, processing, retrieval, networking, applications, and human-centered systems?\" Scientists will be asked, \"How can conceptual and embodied components of interactive multimedia artworks, creativity support tools, and artbased media collections contribute to your research? How can your research contribute to interactive multimedia art?\"\n These individual statements will be followed by discussion. Panel and audience members will be asked to synthesize perspectives across disciplines, to reflect on what they have learned from each other and how this can influence future research.","cites":"2","conferencePercentile":"29.81651376"},{"venue":"ACM Multimedia","id":"40d45cad875e4c17c6c3f40284be743188e3294b","venue_1":"ACM Multimedia","year":"2014","title":"Transfer in Photography Composition","authors":"Hui-Tang Chang, Yu-Chiang Frank Wang, Ming-Syan Chen","author_ids":"2146380, 2947229, 1691171","abstract":"In this paper, we propose novel photography recomposition method, which aims at transferring the photography composition of a reference image to an input image automatically. Without any user interaction, our approach first identifies the salient foreground objects or image regions of interest, and the recomposition is performed by solving a graph-matching based optimization task. With additional post-processing step to preserve the locality and boundary information of the recomposed visual components, we can solve the task of photography recomposition without the uses of any prior knowledge on photography or predetermined image aesthetics rules. Experiments on a variety of images, including transferring the photography composition from real photos, sketches or even paintings, would confirm the effectiveness of our proposed method.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"1f0deaefcbee051040727abc818541534f1cb1cb","venue_1":"ACM Multimedia","year":"1995","title":"Using Rotational Mirrored Declustering for Replica Placement in a Disk-Array-Based Video Server","authors":"Ming-Syan Chen, Hui-I Hsiao, Chung-Sheng Li, Philip S. Yu","author_ids":"1691171, 2564072, 1755389, 1703117","abstract":"In a video-on-demand (VOD) environment, disk-arrays are often used to support the disk bandwidth requirement. This can pose serious problems on available disk bandwidth upon disk failure. In this paper, we explore the approach of replicating frequently accessed movies to provide high data bandwidth and fault-tolerance required in a disk-array-based video server. An isochronous continuous video stream imposes diierent requirements from a random access pattern on databases or les. Explicitly, we propose a new replica placement method, called rotational mirrored declustering (RMD), to support high data availability for disk arrays in a VOD environment. In essence, RMD is similar to the conventional mirrored declustering in that replicas are stored in diierent disk arrays, however diierent from the latter in that the replica placements in diierent disk arrays under RMD are properly rotated. Combining the merits of prior chained and mirrored declustering methods, RMD is particularly suitable for storing multiple movie copies to support VOD applications. To assess the performance of RMD, we conduct a series of experiments by emulating the storage and delivery of movies in a VOD system. Our results show that RMD consistently outperforms the conventional methods in terms of load balancing and fault-tolerance capability after disk failure, and is deemed a viable approach to supporting replica placement in a disk-array-based video server.","cites":"25","conferencePercentile":"55"},{"venue":"ACM Multimedia","id":"80ed3f568cbb0e77ac75f996ead9a26f951990aa","venue_1":"ACM Multimedia","year":"1994","title":"Support for Fully Interactive Playout in Disk-Array-Based Video Server","authors":"Ming-Syan Chen, Dilip D. Kandlur, Philip S. Yu","author_ids":"1691171, 1702213, 1703117","abstract":"In a video-on-demand (VOD) system, it is desirable to provide the user with interactive browsing functions such as &#8220;fast forward&#8221; and &#8220;fast backward.&#8221; However, these functions usually require a significant amount of additional resources from the VOD system in terms of storage space, retrieval throughput, network bandwidth, etc. Moreover, prevalent video compression techniques such as MPEG impose additional constraints on the process since they introduce inter-frame dependencies. In this paper, we devise methods to support variable rate browsing for MPEG-like video steams and minimize the additional resources required. Specifically, we consider retrieval for a disk-array-based video server and address the problem of distributing the retrieval requests across the disks.\nOur overall approach for interactive browsing comprises (1) a storage method, (2) placement and sampling methods, and (3) a playout method, where the placement and sampling methods are two alternatives for video segment selection. The segment sampling scheme supports browsing at any desired speed, while minimizing the variation on the number of video segments skipped between samplings. On the other hand, the segment placement scheme supports completely uniform segment sampling across the disk array for some specific speedup rates. Experiments for the visual effect of the proposed segment skipping approach have been conducted on MPEG data. It is shown that the proposed method is a viable approach to video browsing.","cites":"81","conferencePercentile":"89.83050847"},{"venue":"ACM Multimedia","id":"5237cabf216498d52fc6eb3af3d57027bae82a14","venue_1":"ACM Multimedia","year":"2012","title":"Predicting participants in public events using stock photos","authors":"Neil O'Hare, Luca Maria Aiello, Alejandro Jaimes","author_ids":"8471052, 2905635, 1730325","abstract":"Pictures taken by journalists for distribution and for inclusion in stock photo collections are often enriched with metadata. One key aspect of such photos is that they focus largely on events and feature celebrities and other public figures. They may provide interesting insights into how such public figures are related to each other in terms of the events they attend, and in their social proximity in terms of how often they are photographed together. In this paper, we study a corpus of approximately 9 million stock photographs taken over a 10 year period and, using their metadata, we extract a social network from co-appearance of public figures in events depicted in the photographs. We exploit this latent social information and combine it with the rich image metadata to explore the possibility of predicting attendees at future events, showing promising performance for this task.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"24a418cdee8750b38bc911d2c0d5ef4c95c5152c","venue_1":"ACM Multimedia","year":"2004","title":"Multi-model similarity propagation and its application for web image retrieval","authors":"Xin-Jing Wang, Wei-Ying Ma, Gui-Rong Xue, Xing Li","author_ids":"3349534, 1705244, 1701421, 7137486","abstract":"In this paper, we propose an iterative similarity propagation approach to explore the inter-relationships between Web images and their textual annotations for image retrieval. By considering Web images as one type of objects, their surrounding texts as another type, and constructing the links structure between them via webpage analysis, we can iteratively reinforce the similarities between images. The basic idea is that if two objects of the same type are both related to one object of another type, these two objects are similar; likewise, if two objects of the same type are related to two different, but similar objects of another type, then to some extent, these two objects are also similar. The goal of our method is to fully exploit the mutual reinforcement between images and their textual annotations. Our experiments based on 10,628 images crawled from the Web show that our proposed approach can significantly improve Web image retrieval performance.","cites":"51","conferencePercentile":"90.68627451"},{"venue":"ACM Multimedia","id":"00d9c87d45f20c459e07f3a1a0415422cae0518b","venue_1":"ACM Multimedia","year":"2006","title":"Instant archaeologies: digital lenses to probe and to perforate the urban fabric","authors":"Petra Gemeinboeck, Atau Tanaka, Andy Dong","author_ids":"2901718, 1904255, 1697102","abstract":"The paper discusses the digital artwork series <i>Impossible Geographies</i>, works that weave dynamic cartographies of invisible, fragile and hybrid spaces. <i>Impossible Geographies 01</i>: <i>Memory</i> invokes the memory space of a gallery, producing cracks through which past events seep into the physical present. <i>Net_D&#233;rive</i> negotiates the space between a gallery and its urban neighborhood, creating hybrid narratives along the grooves left by participants equipped with mobile phones. The playground of <i>Impossible Geographies 02: Urban Fiction</i> spreads across the city and beyond, mixing and shifting multiple urban geographies as participants sweep along the urban fabric with advanced mobile phones. The conceptual and technological methods of 'lens-making' and 'spacing,' developed in these artworks, explore the subjective, hybrid, and migrational nature of the geographies we belonging to. By re-sculpting the fluid nature of the urban fabric the works transform the everyday urbanscape into impossible imaginary geographies.","cites":"6","conferencePercentile":"54.40414508"},{"venue":"ACM Multimedia","id":"2e861971f7c4e727474bc95c3cb504ce7193e62c","venue_1":"ACM Multimedia","year":"2007","title":"Xface open source project and smil-agent scripting language for creating and animating embodied conversational agents","authors":"Koray Balci, Elena Not, Massimo Zancanaro, Fabio Pianesi","author_ids":"3283127, 2068569, 1775912, 8029006","abstract":"Xface is a set of open source tools for creation of embodied conversational agents (ECAs) using MPEG4 and keyframe based rendering driven by SMIL-Agent scripting language. Xface Toolkit, coupled with SMIL-Agent scripting serves as a full 3D facial animation authoring package. Xface project is initiated by Cognitive and Communication Technologies (TCC) division of FBK-irst (formerly ITC-irst). The toolkit is written in ANSI C++, and is open source and platform independent.","cites":"11","conferencePercentile":"69.27083333"},{"venue":"ACM Multimedia","id":"d7bc9e64ff7fcba8c068a38f53a9995368276bde","venue_1":"ACM Multimedia","year":"2005","title":"Facilitating collective musical creativity","authors":"Atau Tanaka, Nao Tokui, Ali Momeni","author_ids":"1904255, 2679064, 3157441","abstract":"We present two projects that facilitate collective music creativity over networks. One system is a participative social music system on mobile devices. The other is a collaborative music mixing environment that adheres to the Creative Commons license [1]. We discuss how network and community infrastructures affect the creative musical process, and the implications for artists creating new content for these formats. The projects described are real-world examples of collaborative systems as musical works.","cites":"17","conferencePercentile":"72.27722772"},{"venue":"ACM Multimedia","id":"e2cec4db9ca011ec81bf7529496fc79a5a788064","venue_1":"ACM Multimedia","year":"2006","title":"Measuring movement expertise in surgical tasks","authors":"Kanav Kahol, Narayanan Chatapuram Krishnan, Vineeth N. Balasubramanian, Sethuraman Panchanathan, Marshall L. Smith, John Ferrara","author_ids":"3105442, 2503137, 1699429, 1743991, 2723755, 5796143","abstract":"Surgical movement is composed of discrete gestures that are combined to perform complex surgical procedures. A promising approach to objective surgical skill evaluation systems is kinematics and kinetic analysis of hand movement that yields a gesture level analysis of proficiency of a performed movement. In this paper, we propose a novel system that combines surgical gesture segmentation, surgical gesture recognition, and expertise analysis of surgical profiles in minimally invasive surgery (MIS). Kinematic analysis was used to segment gestures from a continuous motion stream. Human anatomy driven Hidden Markov Models (HMMs) are adopted for gesture recognition and expertise identification. When the proposed system was tested on a library of 200 samples for every basic surgical gesture, the gesture recognition module reported a perfect accuracy rate for the basic gestures, while the expertise identification module showed 94.7% accuracy.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"fb44e706538d844c24cce7e49b6a5ae6ea720749","venue_1":"ACM Multimedia","year":"2012","title":"A human-centered perspective on multimedia data science: tutorial overview","authors":"Alejandro Jaimes","author_ids":"1730325","abstract":"This tutorial focuses on the analysis of user behavior in multimedia through large-scale data analysis. This includes discovering and leveraging search and navigation patterns, understanding how elements of interaction impact behavior, and how we can use controlled experiments in combination with user studies and other techniques to gain insights into human behavior with a particular emphasis on multimedia, particularly in the context of social media.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"9466dd414775a6e420af6b83f9b5db1919de939f","venue_1":"ACM Multimedia","year":"2004","title":"Grouping web image search result","authors":"Xin-Jing Wang, Wei-Ying Ma, Qi-Cai He, Xing Li","author_ids":"3349534, 1705244, 2941896, 7137486","abstract":"In this paper, we propose a Web image search result organizing method to facilitate user browsing. We formalize this problem as a salient image region pattern extraction problem. Given the images returned by Web search engine, we first segment the images into homogeneous regions and quantize the environmental regions into image codewords. The salient codeword \"phrases\" are then extracted and ranked based on a regression model learned from human labeled training data. According to the salient \"phrases\", images are assigned to different clusters, with the one nearest to the centroid as the entry for the corresponding cluster. Satisfying experimental results show the effectiveness of our proposed method.","cites":"14","conferencePercentile":"70.58823529"},{"venue":"ACM Multimedia","id":"b37a9cd92a0adb3b47ed4c0f31b789f64e35ff6b","venue_1":"ACM Multimedia","year":"2012","title":"PRiSMA: searching images in parallel","authors":"Pancho Tolchinsky, Luca Chiarandini, Alejandro Jaimes","author_ids":"2603465, 3315710, 1730325","abstract":"PRiSMA is an image search application for tablet and desktop devices intended to facilitate and promote the searching of images in parallel. With an intuitive user interface, users can branch their queries into multiple horizontal sliding strips to simultaneously explore different perspectives of large image collections (<i>e.g.</i>, colors, geographical location or topic). Strips can be easily created, tailored, merged, and removed, allowing users to effectively perform multiple queries and manage the results in a dynamic and orderly fashion. With PRiSMA we aim to explore the potential and limitations of parallel image search from a user perspective.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"02f94930219d2bb632d067ca2d31db61161ed5fb","venue_1":"ACM Multimedia","year":"2016","title":"Multi-Scale Triplet CNN for Person Re-Identification","authors":"Jiawei Liu, Zheng-Jun Zha, Q. I. Tian, Dong Liu, Ting Yao, Qiang Ling, Tao Mei","author_ids":"2074774, 2355916, 3493734, 5688961, 8543685, 2593172, 1788123","abstract":"Person re-identification aims at identifying a certain person across non-overlapping multi-camera networks. It is a fundamental and challenging task in automated video surveillance. Most existing researches mainly rely on hand-crafted features, resulting in unsatisfactory performance. In this paper, we propose a multi-scale triplet convolutional neural network which captures visual appearance of a person at various scales. We propose to optimize the network parameters by a comparative similarity loss on massive sample triplets, addressing the problem of small training set in person re-identification. In particular, we design a unified multi-scale network architecture consisting of both deep and shallow neural networks, towards learning robust and effective features for person re-identification under complex conditions. Extensive evaluation on the real-world Market-1501 dataset have demonstrated the effectiveness of the proposed approach.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"2cd6252dbe666b7c17957e396f31df5c766c426a","venue_1":"ACM Multimedia","year":"2007","title":"VideoSense: towards effective online video advertising","authors":"Tao Mei, Xian-Sheng Hua, Linjun Yang, Shipeng Li","author_ids":"1788123, 1746102, 7866194, 4973820","abstract":"With Internet delivery of video content surging to an unprecedented level, online video advertising is becoming increasingly pervasive. In this paper, we present a novel advertising system for online video service called <i>VideoSense</i>, which automatically associates the most relevant video ads with online videos and seamlessly inserts the ads at the most appropriate positions within each individual video. Unlike most current video-oriented sites that only display a video ad at the beginning or the end of a video, VideoSense aims to embed more contextually relevant ads at less intrusive positions within the video stream. Given an online video, VideoSense is able to detect a set of candidate ad insertion points based on content <i>discontinuity</i> and <i>attractiveness</i>, select a list of relevant candidate ads ranked according to <i>global textual relevance</i>, and compute <i>local visual-aural relevance</i> between each pair of insertion points and ads. To support contextually relevant and less intrusive advertising, the ads are expected to be inserted at the positions with highest discontinuity and lowest attractiveness, while the overall global and local relevance is maximized. We formulate this task as a nonlinear 0-1 integer programming problem and embed these rules as constraints. The experiments have proved the effectiveness of VideoSense for online video advertising.","cites":"28","conferencePercentile":"87.5"},{"venue":"ACM Multimedia","id":"b13f911e122aacfe5c14755915ad8682fa44bcf4","venue_1":"ACM Multimedia","year":"2009","title":"Understanding near-duplicate videos: a user-centric approach","authors":"Mauro Cherubini, Rodrigo de Oliveira, Nuria Oliver","author_ids":"1733184, 8641731, 1692808","abstract":"Popular content in video sharing web sites (e.g., YouTube) is usually duplicated. Most scholars define near-duplicate video clips (NDVC) based on non-semantic features (e.g., different image/audio quality), while a few also include semantic features (different videos of similar content). However, it is unclear what features contribute to the human perception of similar videos. Findings of two large scale online surveys (N = 1003) confirm the relevance of both types of features. While some of our findings confirm the adopted definitions of NDVC, other findings are surprising. For example, videos that vary in visual content - by overlaying or inserting additional information - may not be perceived as near-duplicate versions of the original videos. Conversely, two different videos with distinct sounds, people, and scenarios were considered to be NDVC because they shared the same semantics (none of the pairs had additional information). Furthermore, the exact role played by semantics in relation to the features that make videos alike is still an open question. In most cases, participants preferred to see only one of the NDVC in the search results of a video search query and they were more tolerant to changes in the audio than in the video tracks. Finally, we propose a user-centric NDVC definition and present implications for how duplicate content should be dealt with by video sharing websites.","cites":"17","conferencePercentile":"81.61157025"},{"venue":"ACM Multimedia","id":"0be80f51feb925ac17fc964f4fadef6f8738e788","venue_1":"ACM Multimedia","year":"2015","title":"Dissecting Urban Noises from Heterogeneous Geo-Social Media and Sensor Data","authors":"Hsun-Ping Hsieh, Rui Yan, Cheng-Te Li","author_ids":"2670863, 5495727, 2169355","abstract":"Geo-social media services, such as Foursquare and Flickr, provide rich data that sensors various urban activities of human beings from geographical, mobility, visual, and social aspects. While noise pollution in modern cities is getting worse and sound sensors are sparse and costly, it is highly demanded to infer and analyze the noise at any region in urban areas. In this paper, we aim to leverage heterogeneous geo-social sensor data on Foursquare, Flickr, and Gowalla, to dissect urban noises for every regions in a city. Using NYC 311 noise complaint records as the approximation of urban noises generated by regions, we propose a novel unsupervised framework that integrates the extracted geographical, mobility, visual, and social features to infer the noise composition for regions and time intervals of interest in NYC. Experimental results show that our system can achieve promising results with substantially few training data, compared to state-of-the-art methods.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"abc2b25b089c14e8d3eb375be883e54079fa871b","venue_1":"ACM Multimedia","year":"2001","title":"TeleExperience: communicating compelling experience","authors":"Ramesh Jain","author_ids":"4521564","abstract":"We experience our environment using our natural senses: sight, sound, touch, taste, and smell. These senses combined with the knowledge of the world allow us to experience and function in the world. Data are observed facts or measurements. Information is derived from data in a specific context. Experience is direct observation or participation in an event. The development of civilization is the story of the development of understanding of 'experience' and how to share it with fellow human beings of current and future generations. The desire to share experiences and desire to experience various events where one can not be present, will continue to be the motivating factor in the development of exciting technology in the future. A look at history shows how our society evolved to become an 'information society' and is on its way to becoming an 'experience society'.Compelling and engaging experiences require immersion in a rich set of multimedia data and information so that one can directly observe a subset of the data and information. TeleExperience is a natural major step in technology evolution. It will impact every aspect of our society including education, business, sexual behavior, and health care. TeleExperience will give rise to an experience society. In this presentation, we will examine the role of multimedia information systems, multimedia presentations, situated computing, perception systems, and personalization approaches to realize TeleExperience. Then we will discuss aspects of taking a promising and exciting technology from 'laboratory to popular practice' based on practical experience.","cites":"4","conferencePercentile":"40.81632653"},{"venue":"ACM Multimedia","id":"48c7afd55f620cc665cabb52755968c5c57d9eda","venue_1":"ACM Multimedia","year":"2002","title":"Semantic transcoding for live video server","authors":"Rita Cucchiara, Costantino Grana, Andrea Prati","author_ids":"1741922, 1705203, 1733945","abstract":"In this paper we present transcoding techniques for a video server architecture that enables the user to access live video streams by using different devices with different capabilities. For live videos, annotation methods cannot be exploited. Instead we propose methods of on-the-fly transcoding that adapt the video content with respect to the user resources and the video semantic. Thus we propose an object-based transcoding with \"classes of relevance\" (for instance People, Face and Background). To compare the different strategies we propose a metric based on the <i>Weighted Mean Square Error</i> that allows the analysis of different application scenarios by means of a class-wise distortion measure. The obtained results show that the use of semantic can improve the bandwidth to distortion ratio significantly.","cites":"12","conferencePercentile":"58.54700855"},{"venue":"ACM Multimedia","id":"de040b367676fc66300b87c8ff60c69fa6561189","venue_1":"ACM Multimedia","year":"2006","title":"MOM: multimedia ontology manager. A framework for automatic annotation and semantic retrieval of video sequences","authors":"Marco Bertini, Alberto Del Bimbo, Carlo Torniai, Rita Cucchiara, Costantino Grana","author_ids":"1801509, 8196487, 2277925, 1741922, 1705203","abstract":"Effective usage of multimedia digital libraries has to deal with the problem of building efficient content annotation and retrieval tools. MOM (Multimedia Ontology Manager) is a complete system that allows the creation of multimedia ontologies, supports automatic annotation and creation of extended text (and audio) commentaries of video sequences, and permits complex queries by reasoning on the ontology.","cites":"6","conferencePercentile":"54.40414508"},{"venue":"ACM Multimedia","id":"0209e7a60fe2c34b77b5a8de7a78a1fb1dd7dcfe","venue_1":"ACM Multimedia","year":"2006","title":"PEANO: pictorial enriched annotation of video","authors":"Costantino Grana, Roberto Vezzani, Daniele Bulgarelli, Giovanni Gualdi, Rita Cucchiara, Marco Bertini, Carlo Torniai, Alberto Del Bimbo","author_ids":"1705203, 1723285, 2611643, 1766510, 1741922, 1801509, 2277925, 8196487","abstract":"In this DEMO, we present a tool set for video digital library management that allows i) structural annotation of edited videos in MPEG-7 by automatically extracting shots and clips; ii) automatic semantic annotation based on perceptual similarity against a taxonomy enriched with pictorial concepts iii) video clip access and hierarchical summarization with stand-alone and web interface iv) access to clips from mobile platform in GPRS-UMTS video-streaming. The tools can be applied in different domain-specific Video Digital Libraries. The main novelty is the possibility to enrich the annotation with pictorial concepts that are added to a textual taxonomy in order to make the automatic annotation process more fast and often effective. The resulting multimedia ontology is described in the MPEG-7 framework. The PEANO (Perceptual Annotation of Video) tool has been tested over video art , sport (Soccer, Olimpic Games 2006, Formula 1) and news clips.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"6a82cfe1a807b11edbb720de2a3016c66bd4abc0","venue_1":"ACM Multimedia","year":"2009","title":"Multimedia in forensics","authors":"Marcel Worring, Rita Cucchiara","author_ids":"1717056, 1741922","abstract":"1. MOTIVATION Crime is and will always be an integral part of society. Prevention of crime and the forensic investigation of crimes which happened are therefore important tasks which deserve significant attention from governments and researchers alike. Forensic investigation of a crime is a complex process which starts at the crime scene, continues in the lab for in depth investigation, and ends in the court room where the final judgment is made. Investigators need support in all of these steps to make their jobs as effective and efficient as possible. Traces used to be fingerprints, fibres, documents and the like, but with the proliferation of multimedia data on the web, surveillance cameras in cities, and mobile phones in everyday life we see an enormous growth in multimedia data that needs to be analyzed by forensic investigators. For example hard disks can contain thousands of videos and millions of images potentially containing child abuse. Surveillance data can easily contain hundred to thousand cameras over a full 24 hour day in which the presence and whereabouts of a group of criminals has to be determined. The sheer volume of such data sets makes manual inspection of all data impossible. Tools are needed to support the investigator in their quest for relevant clues and evidence in mul-timedia data and in their strive towards preventing crime. Fortunately the omnipresence of multimedia data has also given a boost to research in handling of multimedia. Within the multimedia community tools have been developed for management of large collections of video footage, images, audio and other multimedia content, knowledge extraction and Copyright is held by the author/owner(s). simulation in various domains. Due to the inherent uncertainty and complexity of forensic data, applying those techniques to forensic data is not straightforward. However, the time is ripe to tailor these results for forensics. Like most research in multimedia, supporting the forensic process with multimedia tools is a multidisciplinary effort borrowing techniques from various disciplines studied in other fields. However, to be applicable for this particular application field the operating conditions of the algorithms and the fundamental assumptions underlying the techniques are challenged and need to be viewed from new perspectives. The multimedia community has proven to be effective in solving real world problems by developing new frameworks for integrating and adapting existing techniques into innovative solutions. So the multimedia research field should take the lead in this interesting application …","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"0d9c3dc5b3b85f030c2199c236879c096a02ab07","venue_1":"ACM Multimedia","year":"2004","title":"Speech, ink, and slides: the interaction of content channels","authors":"Richard J. Anderson, Crystal Hoyer, Craig Prince, Jonathan Su, Fred Videon, Steven A. Wolfman","author_ids":"2308531, 2224866, 1786208, 2510810, 3004136, 3162853","abstract":"In this paper, we report on an empirical exploration of digital ink and speech usage in lecture presentation. We studied the video archives of five Master's level Computer Science courses to understand how instructors use ink and speech together while lecturing, and to evaluate techniques for analyzing digital ink. Our interest in understanding how ink and speech are used together is to inform the development of future tools for supporting classroom presentation, distance education, and viewing of archived lectures. We want to make it easier to interact with electronic materials and to extract information from them. We want to provide an empirical basis for addressing challenging problems such as automatically generating full text transcripts of lectures, matching speaker audio with slide content, and recognizing the meaning of the instructor's ink. Our results include an evaluation of handwritten word recognition in the lecture domain, an approach for associating attentional marks with content, an analysis of linkage between speech and ink, and an application of recognition techniques to infer speaker actions.","cites":"25","conferencePercentile":"80.39215686"},{"venue":"ACM Multimedia","id":"839c65076e1e4bb8ea2761a99bed0c670f0c6ff9","venue_1":"ACM Multimedia","year":"2010","title":"Surfing on artistic documents with visually assisted tagging","authors":"Daniele Borghesani, Costantino Grana, Rita Cucchiara","author_ids":"3301949, 1705203, 1741922","abstract":"This paper describes a complete architecture for the interactive exploration and annotation of artistic collections. In particular the focus is on Renaissance illuminated manuscripts, which typically contain thousands of pictures, used to comment or embellish the manuscript Gothic text. The final aim is to create a human centered multimedia application allowing the non practitioners to enjoy these masterpieces and expert users to share their knowledge. The system is composed by a modern user interface for browsing, surfing and querying, an automatic segmentation module, to ease the initial picture extraction task, and a similarity based retrieval engine, used to provide visually assisted tagging capabilities. A relevance feedback procedure is included to further refine the results. Experiments are reported regarding the adopted visual features based on covariance matrices and the Mean Shift Feature Space Warping relevance feedback. Finally some hints on the user interface for museum installations are discussed.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"2037251a21243e6df2693907c0e2a2d54467e7b7","venue_1":"ACM Multimedia","year":"2010","title":"Rerum novarum: interactive exploration of illuminated manuscripts","authors":"Daniele Borghesani, Costantino Grana, Rita Cucchiara","author_ids":"3301949, 1705203, 1741922","abstract":"This paper describes an interactive application for the exploration and annotation of illuminated manuscripts, which typically contain thousands of pictures, used to comment or embellish the manuscript Gothic text. The system is composed by a modern user interface for browsing, surfing and querying, an automatic segmentation module, to ease the initial picture extraction task, and a similarity based retrieval engine, used to provide visually assisted tagging capabilities. A relevance feedback procedure is included to further refine the results.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"95aa80cf672771730393e1d7d263ab6f6d6e535d","venue_1":"ACM Multimedia","year":"2013","title":"Learning articulated body models for people re-identification","authors":"Davide Baltieri, Roberto Vezzani, Rita Cucchiara","author_ids":"3130046, 1723285, 1741922","abstract":"People re-identification is a challenging problem in surveillance and forensics and it aims at associating multiple instances of the same person which have been acquired from different points of view and after a temporal gap. Image-based appearance features are usually adopted but, in addition to their intrinsically low discriminability, they are subject to perspective and view-point issues. We propose to completely change the approach by mapping local descriptors extracted from RGB-D sensors on a 3D body model for creating a view-independent signature. An original bone-wise color descriptor is generated and reduced with PCA to compute the person signature. The virtual bone set used to map appearance features is learned using a recursive splitting approach. Finally, people matching for re-identification is performed using the Relaxed Pairwise Metric Learning, which simultaneously provides feature reduction and weighting. Experiments on a specific dataset created with the Microsoft Kinect sensor and the OpenNi libraries prove the advantages of the proposed technique with respect to state of the art methods based on 2D or non-articulated 3D body models.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"a1042f63521d3984fff598634d9995d940b7fbf6","venue_1":"ACM Multimedia","year":"2004","title":"Facial expression representation and recognition based on texture augmentation and topographic masking","authors":"Lijun Yin, Johnny Loi, Wei Xiong","author_ids":"8072251, 3352939, 6737623","abstract":"The variation of facial texture and surface due to the change of expression is an important cue for analyzing and modeling facial expressions. In this paper, we propose a new approach to represent the facial expression by using a so-called topographic feature. In order to capture the variation of facial surface structure, facial textures are processed by increasing the resolution. The topographical structure of human face is analyzed based on the resolution-enhanced textures. We investigate the relationship between the facial expression and its topographic features, and propose to represent the facial expression by the topographic labels. The detected topographic facial surface and the expressive regions reflect the status of facial skin movement. Based on the observation that the facial texture and its topographic features change along with facial expressions, we compare the disparity of these features between the neutral face and the expressive face to distinguish a number of universal expressions. The experiment demonstrates the feasibility of the proposed approach for facial expression representation and recognition.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"1cd8926b0ca68bc17732c306be6cb59cbf2cb0db","venue_1":"ACM Multimedia","year":"2013","title":"Modeling local descriptors with multivariate gaussians for object and scene recognition","authors":"Giuseppe Serra, Costantino Grana, Marco Manfredi, Rita Cucchiara","author_ids":"2275344, 1705203, 2420068, 1741922","abstract":"Common techniques represent images by quantizing local descriptors and summarizing their distribution in a histogram. In this paper we propose to employ a parametric description and compare its capabilities to histogram based approaches. We use the multivariate Gaussian distribution, applied over the SIFT descriptors, extracted with dense sampling on a spatial pyramid. Every distribution is converted to a high-dimensional descriptor, by concatenating the mean vector and the projection of the covariance matrix on the Euclidean space tangent to the Riemannian manifold. Experiments on Caltech-101 and ImageCLEF2011 are performed using the Stochastic Gradient Descent solver, which allows to deal with large scale datasets and high dimensional feature spaces.","cites":"3","conferencePercentile":"60"},{"venue":"ACM Multimedia","id":"092264ad1d0da542c9dc83ca63b85f6734ce6d76","venue_1":"ACM Multimedia","year":"2015","title":"A Deep Siamese Network for Scene Detection in Broadcast Videos","authors":"Lorenzo Baraldi, Costantino Grana, Rita Cucchiara","author_ids":"1843795, 1705203, 1741922","abstract":"We present a model that automatically divides broadcast videos into coherent scenes by learning a distance measure between shots. Experiments are performed to demonstrate the effectiveness of our approach by comparing our algorithm against recent proposals for automatic scene segmentation. We also propose an improved performance measure that aims to reduce the gap between numerical evaluation and expected results, and propose and release a new benchmark dataset.","cites":"5","conferencePercentile":"90.55555556"},{"venue":"ACM Multimedia","id":"3a1054be58a119a7f0f7244f95cfc431def111a9","venue_1":"ACM Multimedia","year":"2016","title":"A Browsing and Retrieval System for Broadcast Videos using Scene Detection and Automatic Annotation","authors":"Lorenzo Baraldi, Costantino Grana, Alberto Messina, Rita Cucchiara","author_ids":"1843795, 1705203, 1763274, 1741922","abstract":"This paper presents a novel video access and retrieval system for edited videos. The key element of the proposal is that videos are automatically decomposed into semantically coherent parts (called scenes) to provide a more manageable unit for browsing, tagging and searching. The system features an automatic annotation pipeline, with which videos are tagged by exploiting both the transcript and the video itself. Scenes can also be retrieved with textual queries; the best thumbnail for a query is selected according to both semantics and aesthetics criteria.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"425b5e866d657e9510d149b88db169cb6ba008d9","venue_1":"ACM Multimedia","year":"2016","title":"Motion Segmentation using Visual and Bio-mechanical Features","authors":"Stefano Alletto, Giuseppe Serra, Rita Cucchiara","author_ids":"2452552, 2275344, 1741922","abstract":"Nowadays, egocentric wearable devices are continuously increasing their widespread among both the academic community and the general public. For this reason, methods capable of automatically segment the video based on the recorder motion patterns are gaining attention. These devices present the unique opportunity of both high quality video recordings and multimodal sensors readings. Significant efforts have been made in either analyzing the video stream recorded by these devices or the bio-mechanical sensor information. So far, the integration between these two realities has not been fully addressed, and the real capabilities of these devices are not yet exploited. In this paper, we present a solution to segment a video sequence into motion activities by introducing a novel data fusion technique based on the covariance of visual and bio-mechanical features. The experimental results are promising and show that the proposed integration strategy outperforms the results achieved focusing solely on a single source.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"22c784b21dbb1802e7ab12105d4d48b3c4375a36","venue_1":"ACM Multimedia","year":"2005","title":"MultiPRE: a novel framework with multiple parallel retrieval engines for content-based image retrieval","authors":"Wei Xiong, Bo Qiu, Qi Tian, Changsheng Xu, Sim Heng Ong, Kelvin Weng Chiong Foong, Jean-Pierre Chevallet","author_ids":"6737623, 2821964, 1724745, 1688633, 1685644, 3307960, 1702093","abstract":"We propose a novel framework for content-based image retrieval with multiple parallel retrieval engines (MultiPRE) to achieve higher retrieval performance. Visual features, including both low-level features, such as color, texture and region features, and middle-level structure features, such as blob representation of objects are used to capture geometrical and statistical characteristics of images. Both clustering analysis and discrimination analysis are used as similarity measures in multiple retrieval engines, which are based on~principal component analysis (PCA) and support vector machines (SVM), respectively. Finally outputs of these engines are fused to determine ranking lists of retrieved images for given retrieval topics. The proposed framework has been evaluated based on the 26 image query topics over the CasImage database~with over 9000 medical images~used in ImageCLEF 2004, an international research effort for content-based image retrieval performance benchmark. Experiments show that the proposed framework achieved significantly better performance in terms of both the mean and the variance of average precision than the best run reported in ImageCLEF2004.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"f1dd778af01de993ba47858cdb6305c64f91288a","venue_1":"ACM Multimedia","year":"2015","title":"Automatic Image Dataset Construction from Click-through Logs Using Deep Neural Network","authors":"Yalong Bai, Kuiyuan Yang, Wei Yu, Chang Xu, Wei-Ying Ma, Tiejun Zhao","author_ids":"2643877, 2976163, 3389395, 1712753, 1705244, 3006844","abstract":"Labelled image datasets are the backbone for high-level image understanding tasks with wide application scenarios, and continuously drive and evaluate the progress of feature designing and supervised learning models. Recently, the million scale labelled image dataset further contributes to the rebirth of deep convolutional neural network and bypass manual designing handcraft features. However, the construction process of image dataset is mainly manual-based and quite labor intensive, which often take years' efforts to construct a million scale dataset with high quality. In this paper, we propose a deep learning based method to construct large scale image dataset in an automatic way. Specifically, word representation and image representation are learned in a deep neural network from large amount of click-through logs, and further used to define word-word similarity and image-word similarity. These two similarities are used to automatize the two labor intensive steps in manual-based image dataset construction: query formation and noisy image removal. With a new proposed cross convolutional filter regularizer, we can construct a million scale image dataset in one week. Finally, two image datasets are constructed to verify the effectiveness of the method. In addition to scale, the automatically constructed dataset has comparable accuracy, diversity and cross-dataset generalization with manually labelled image datasets.","cites":"4","conferencePercentile":"87.77777778"},{"venue":"ACM Multimedia","id":"1ab89f5f8f5d9d94d28ed43cb5cee62ba5e4f7b1","venue_1":"ACM Multimedia","year":"2010","title":"Toward more efficient user interfaces for mobile video browsing: an in-depth exploration of the design space","authors":"Jochen Huber, Jürgen Steimle, Max Mühlhäuser","author_ids":"1755184, 1790324, 1725964","abstract":"Increasingly powerful mobile devices enable users to access and watch videos in mobile settings. While some concepts for mobile video browsing have been presented, the field still lacks a general understanding of the design space and of the characteristics of interaction concepts. In order to improve user interfaces for mobile video browsing, this paper includes three contributions. First, we setup a design space for mobile video browsing and contribute seven novel interface concepts. They rely on GUI-based, on touch-gesture-based, and on physical interaction. Second, we present the results of an in-depth evaluation and comparison of these concepts. They are based on an ascriptive analysis of 18 hours of video observations from a controlled experiment with 44 participants. The results provide insights into common usability errors and misconceptions. Third, we derive implications for the design of mobile video browsers to minimize errors and to increase usability.","cites":"14","conferencePercentile":"83.01369863"},{"venue":"ACM Multimedia","id":"0d09033eadb43f8ec72b8cf2e928a473f64cb6d9","venue_1":"ACM Multimedia","year":"2014","title":"Bag-of-Words Based Deep Neural Network for Image Retrieval","authors":"Yalong Bai, Wei Yu, Tianjun Xiao, Chang Xu, Kuiyuan Yang, Wei-Ying Ma, Tiejun Zhao","author_ids":"2643877, 3389395, 2181001, 1712753, 2976163, 1705244, 3006844","abstract":"This work targets image retrieval task hold by MSR-Bing Grand Challenge. Image retrieval is considered as a challenge task because of the gap between low-level image representation and high-level textual query representation. Recently further developed deep neural network sheds light on narrowing the gap by learning high-level image representation from raw pixels. In this paper, we proposed a bag-of-words based deep neural network for image retrieval task, which learns high-level image representation and maps images into bag-of-words space. The DNN model is trained on the large scale clickthrough data, and the relevance between query and image is measured by the cosine similarity of query's bag-of-words representation and image's bag-of-words representation predicted by DNN, the visual similarity of images is computed by high-level image representation extracted via the DNN model too. Finally, PageRank algorithm is used to further improve the ranking list by considering visual similarity of images for each query. The experimental results achieved state-of-the-art performance and verified the effectiveness of our proposed method.","cites":"7","conferencePercentile":"84.93975904"},{"venue":"ACM Multimedia","id":"1313a31e1f7b97dc993e204e1085bda6ae9a07a2","venue_1":"ACM Multimedia","year":"2007","title":"The lens of ludic engagement: evaluating participation in interactive art installations","authors":"Ann Morrison, Peta Mitchell, Margot Brereton","author_ids":"2512884, 2256777, 1720522","abstract":"Designers and artists have integrated recent advances in interactive, tangible and ubiquitous computing technologies to create new forms of interactive environments in the domains of work, recreation, culture and leisure. Many designs of technology systems begin with the workplace in mind, and with function, ease of use, and efficiency high on the list of priorities. [1] These priorities do not fit well with works designed for an interactive art environment, where the aims are many, and where the focus on utility and functionality is to support a playful, ambiguous or even experimental experience for the participants. To evaluate such works requires an integration of art-criticism techniques with more recent Human Computer Interaction (HCI) methods, and an understanding of the different nature of engagement in these environments. This paper begins a process of mapping a set of priorities for amplifying engagement in interactive art installations. I first define the concept of ludic engagement and its usefulness as a lens for both design and evaluation in these settings. I then detail two fieldwork evaluations I conducted within two exhibitions of interactive artworks, and discuss their outcomes and the future directions of this research.","cites":"17","conferencePercentile":"74.21875"},{"venue":"ACM Multimedia","id":"10a942758c627ac8ee727eea28062d70d3d54c3f","venue_1":"ACM Multimedia","year":"2013","title":"Revisiting the VLAD image representation","authors":"Jonathan Delhumeau, Philippe Henri Gosselin, Hervé Jégou, Patrick Pérez","author_ids":"2610541, 2224820, 1681054, 1799777","abstract":"Recent works on image retrieval have proposed to index images by compact representations encoding powerful local descriptors, such as the closely related VLAD and Fisher vector. By combining such a representation with a suitable coding technique, it is possible to encode an image in a few dozen bytes while achieving excellent retrieval results. This paper revisits some assumptions proposed in this context regarding the handling of \"visual burstiness\", and shows that ad-hoc choices are implicitly done which are not desirable. Focusing on VLAD without loss of generality, we propose to modify several steps of the original design. Albeit simple, these modifications significantly improve VLAD and make it compare favorably against the state of the art.","cites":"59","conferencePercentile":"99.55555556"},{"venue":"ACM Multimedia","id":"02537545f31ecf4aff637daa97754fc462839cbc","venue_1":"ACM Multimedia","year":"2015","title":"Egocentric Video Summarization of Cultural Tour based on User Preferences","authors":"Patrizia Varini, Giuseppe Serra, Rita Cucchiara","author_ids":"2379129, 2275344, 1741922","abstract":"In this paper, we propose a new method to obtain customized video summarization according to specific user preferences. Our approach is tailored on Cultural Heritage scenario and is designed on identifying candidate shots, selecting from the original streams only the scenes with behavior patterns related to the presence of relevant experiences, and further filtering them in order to obtain a summary matching the requested user's preferences. Our preliminary results show that the proposed approach is able to leverage user's preferences in order to obtain a customized summary, so that different users may extract from the same stream different summaries.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"d9e141abc75fa15334653d627c354b19169be150","venue_1":"ACM Multimedia","year":"2015","title":"Distributed Optimal Datacenter Bandwidth Allocation for Dynamic Adaptive Video Streaming","authors":"Fanxin Kong, Xingjian Lu, Mingyuan Xia, Xue Liu, Haibing Guan","author_ids":"2035566, 1692694, 1778373, 1723807, 7203366","abstract":"Video streaming systems such as YouTube and Netflix are usually supported by the content delivery networks and datacenters that can consume many megawatts of power. Most existing works independently study the issues of improving quality of experience (QoE) for viewers and reducing the cost and emissions associated with the enormous energy usage of datacenters. By contrast, this paper addresses them both, and jointly optimizes the QoE, the energy cost and emissions by intelligently allocating datacenter bandwidth among different client groups. Specially, we propose a distributed algorithm for achieving the optimal bandwidth allocation. The algorithm novelly decomposes the optimization process into separate ones, which are solved iteratively across datacenters and clients. We demonstrate its convergence by both theoretical proof and experimental validation. The experimental results show that the proposed algorithm converges very fast and achieves much better QoE-cost balance than existing approaches.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"0c567c2a28b70e3fbb00914dda875fa9ec1ff906","venue_1":"ACM Multimedia","year":"1996","title":"A QoS Adaptive Transport System: Design, Implementation and Experience","authors":"Andrew T. Campbell, Geoff Coulson","author_ids":"1690035, 1696978","abstract":"Distributed audio and video applications need to adapt to fluctuations in delivered quality of service (QoS). By trading off temporal and spatial quality to available bandwidth, or manipulating the playout time of continuous media in response to variation in delay, audio and video flows can be made to adapt to fluctuating QoS with minimal perceptual distortion. In this paper we describe the implementation of a QoS adaptive transport system that incorporates a QoS oriented API and a range of mechanisms to assist applications in exploiting QoS and adapting to fluctuations in QoS. The system, which is an instantiation of the QoS Architecture (QoS-A), is implemented in a multi ATM switch network environment with Linux based PC end systems and continuous media file servers. A performance evaluation of the system configured to support Video-on-Demand application scenario is presented and discussed.","cites":"19","conferencePercentile":"38.88888889"},{"venue":"ACM Multimedia","id":"4dcff07f50f12e95e3b22d6d28ababa6385a4c64","venue_1":"ACM Multimedia","year":"2015","title":"Vision-enhanced Immersive Interaction and Remote Collaboration with Large Touch Displays","authors":"Zhengyou Zhang","author_ids":"1732465","abstract":"Large displays are becoming commodity, and more and more, they are touch-enabled. In this keynote, we describe a system called ViiBoard (Vision-enhanced Immersive Interaction with touch Board) that enables natural interaction and immersive remote collaboration with large touch displays by adding a commodity color plus depth sensor. It consists of two parts. The first part is called VTouch that augments touch input with visual understanding of the user to improve interaction with a large touch-sensitive display such as Microsoft Surface Hub. An RGBD sensor such as Microsoft Kinect adds the visual modality and enables new interactions beyond touch. Through visual analysis, the system understands where the user is, who the user is, and what the user is doing even before the user touches the display. Such information is used to enhance interaction in multiple ways. For example, a user can use simple gestures to bring up menu items such as color palette and soft keyboard; menu items can be shown where the user is and can follow the user; hovering can show information to the user before the user commits to touch; the user can perform different functions (for example writing and erasing) with different hands; and the user's preference profile can be maintained, distinct from other users. User studies are conducted and the users very much appreciate the value of these and other enhanced interactions.\n The second part is called ImmerseBoard. ImmerseBoard is a system for remote collaboration through a digital whiteboard that gives participants a 3D immersive experience, enabled by an RGBD sensor mounted on the side of a large touch display (the same setup as in VTouch). Using 3D processing of the depth images, life-sized rendering, and novel visualizations, ImmerseBoard emulates writing side-by-side on a physical whiteboard, or alternatively on a mirror. User studies involving three tasks show that compared to standard video conferencing with a digital whiteboard, ImmerseBoard provides participants with a quantitatively better ability to estimate their remote partners' eye gaze direction, gesture direction, intention, and level of agreement. Moreover, these quantitative capabilities translate qualitatively into a heightened sense of being together and a more enjoyable experience. ImmerseBoard's form factor is suitable for practical and easy installation in homes and offices.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"bf0cfffcd0263f2b5f06b4b3fc39098480358dc5","venue_1":"ACM Multimedia","year":"2007","title":"Exploiting spatial context constraints for automatic image region annotation","authors":"Jinhui Yuan, Jianmin Li, Bo Zhang","author_ids":"2422936, 8549039, 1696318","abstract":"In this paper we conduct a relatively complete study on how to exploit spatial context constraints for automated image region annotation. We present a straight forward method to regularize the segmented regions into 2D lattice layout, so that simple grid-structure graphical models can be employed to characterize the spatial dependencies. We show how to represent the spatial context constraints in various graphical models and also present the related learning and inference algorithms. Different from most of the existing work, we specifically investigate how to combine the classification performance of discriminative learning and the representation capability of graphical models. To reliably evaluate the proposed approaches, we create a moderate scale image set with region-level ground truth. The experimental results show that (i) spatial context constraints indeed help for accurate region annotation, (ii) the approaches combining the merits of discriminative learning and context constraints perform best, (iii) image retrieval can benefit from accurate region-level annotation.","cites":"44","conferencePercentile":"92.96875"},{"venue":"ACM Multimedia","id":"4fba6f20200180194b0a0c3834a2dc58f518eee7","venue_1":"ACM Multimedia","year":"2001","title":"SARI: self-authentication-and-recovery image watermarking system","authors":"Ching-Yung Lin, Shih-Fu Chang","author_ids":"1689953, 1735547","abstract":"In this project, we designed a novel image authentication system based on a our semi-fragile watermarking technique. The system, called SARI, can accept quantization-based lossy compression to a determined degree without any false alarm and can sensitively detect and locate malicious manipulations. It's the first system that has such capability in distinguishing malicious attacks from acceptable operations. Furthermore, the corrupted area can be approximately recovered by the information hidden in the other part of the contentimage. The amount of information embedded in our SARI system has nearly reached the theoretical maximum zero-error information hiding capacity of digital images. The software prototype includes two parts - the watermark embedder that's freely distributed and the authenticator that can be deployed online as a third-party service or used in the recipient side.","cites":"27","conferencePercentile":"85.20408163"},{"venue":"ACM Multimedia","id":"0a013a350aaafc0de25e632b5ab74f9b38841d44","venue_1":"ACM Multimedia","year":"2008","title":"Predicting the dominant clique in meetings through fusion of nonverbal cues","authors":"Dinesh Babu Jayagopi, Hayley Hung, Chuohao Yeo, Daniel Gatica-Perez","author_ids":"1705782, 1756464, 3101028, 1698682","abstract":"This paper addresses the problem of automatically predicting the dominant clique (i.e., the set of K-dominant people) in face-to-face small group meetings recorded by multiple audio and video sensors. For this goal, we present a framework that integrates automatically extracted nonverbal cues and dominance prediction models. Easily computable audio and visual activity cues are automatically extracted from cameras and microphones. Such nonverbal cues, correlated to human display and perception of dominance, are well documented in the social psychology literature. The effectiveness of the cues were systematically investigated as single cues as well as in unimodal and multimodal combinations using unsupervised and supervised learning approaches for dominant clique estimation. Our framework was evaluated on a five-hour public corpus of teamwork meetings with third-party manual annotation of perceived dominance. Our best approaches can exactly predict the dominant clique with 80.8% accuracy in four-person meetings in which multiple human annotators agree on their judgments of perceived dominance.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"174b5d5b2a0c6e9bdae3dc492c0a446a88c12246","venue_1":"ACM Multimedia","year":"2005","title":"LazyCut: content-aware template-based video authoring","authors":"Xian-Sheng Hua, Zengzhi Wang, Shipeng Li","author_ids":"1746102, 3196852, 4973820","abstract":"Though there are many commercial video authoring tools available today, video authoring remains as a tedious and extremely time consuming task that often requires trained professional skills. To tackle this issue, this demonstration presents a novel end-to-end system, called LazyCut, which enables fast, flexible and personalized video authoring and sharing. LazyCut provides a semi-automatic video authoring and sharing system that significantly reduces users' efforts in video editing while preserving sufficient flexibility and personalization.","cites":"6","conferencePercentile":"45.2970297"},{"venue":"ACM Multimedia","id":"977c8195ec77442fd0f8a0294921c8986d6c7b5d","venue_1":"ACM Multimedia","year":"2005","title":"Personal media sharing and authoring on the web","authors":"Xian-Sheng Hua, Shipeng Li","author_ids":"1746102, 4973820","abstract":"In this paper, we propose a novel system working on the Web for personal media sharing and authoring. Three primary technologies enable this end-to-end system, including scalable video coding, intelligent multimedia content analysis, and template-based media authoring. Scalable video codec tackles the issue of huge data transmission, multimedia content analysis facilitates automatic video editing, and template-based media authoring scheme further reduces the workload of media sharing and authoring. Experiment and a demo system on a real Internet environment show that this novel system is effective.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"3a0bb579a579cedc73bb045ec6521392779d54dc","venue_1":"ACM Multimedia","year":"2007","title":"Gradual transition detection with conditional random fields","authors":"Jinhui Yuan, Jianmin Li, Bo Zhang","author_ids":"2422936, 8549039, 1696318","abstract":"In this paper, we view gradual transition detection as a sequence labeling problem and propose to use Conditional Random Fields (CRFs) for this purpose. CRFs is a state-of-the-art sequence labeling approach. It provides a unified way to integrate various useful clues to form a decision system. Moreover, it has principled way for parameter estimation and inference. Compared to rule-based approaches, gradual transition detection with CRFs requires fewer human interactions while designing the system. The experiments on TRECVID platform show that CRFs can achieve comparable performance to that of the state-of-the-art approaches.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"8f1d8e4c31996d8e2435fdb795c0a92f7e4c382a","venue_1":"ACM Multimedia","year":"2007","title":"The importance of query-concept-mapping for automatic video retrieval","authors":"Dong Wang, Xirong Li, Jianmin Li, Bo Zhang","author_ids":"1726751, 7137848, 8549039, 1696318","abstract":"A new video retrieval paradigm of query-by-concept emerges recently. However, it remains unclear how to exploit the detected concepts in retrieval given a multimedia query. In this paper, we point out that it is important to map the query to a few relevant concepts instead of search with all concepts. In addition, we show that solving this problem through both text and image inputs are effective for search, and it is possible to determine the number of related concepts by a language modeling approach. Experimental evidence is obtained on the automatic search task of TRECVID 2006 using a large lexicon of 311 learned semantic concept detectors.","cites":"12","conferencePercentile":"70.57291667"},{"venue":"ACM Multimedia","id":"866386e908a882d132028ff0e84062b345e2ecbd","venue_1":"ACM Multimedia","year":"2009","title":"Vocabulary-based hashing for image search","authors":"Yingyu Liang, Jianmin Li, Bo Zhang","author_ids":"2621779, 8549039, 1696318","abstract":"This paper proposes a hash function family based on feature vocabularies and investigates the application in building indexes for image search. Each hash function is associated with a set of feature points, i.e. a vocabulary, and maps an input point to the ID of the nearest one in the vocabulary. The function family can be employed to build a high-dimensional index for approximate nearest neighbor search. Then we concentrate on its application in image search. Guiding rules for the construction of the vocabularies are derived, which improve the effectiveness of the approach in this context by taking advantage of the data distribution. The rules are applied to design an algorithm for vocabulary construction in practice. Experiments show promising performance of the approach and the effectiveness of the guiding rules. Comparison with the popular Euclidean locality-sensitive hashing also shows the advantage of our approach in image search.","cites":"4","conferencePercentile":"42.97520661"},{"venue":"ACM Multimedia","id":"4ab69672e1116427d685bf7c1edb5b1fd0573b5e","venue_1":"ACM Multimedia","year":"2012","title":"Spatial pooling of heterogeneous features for image applications","authors":"Lingxi Xie, Qi Tian, Bo Zhang","author_ids":"3041937, 1724745, 1696318","abstract":"The Bag-of-Features (BoF) model has played an important role for image representation in many multimedia applications. It has been extensively applied to many tasks including image classification, image retrieval, scene understanding, and so on. Despite the advantages of this model such as simplicity, efficiency and generality, there are also notable drawbacks for this model, including poor power of semantic expression of local descriptors, and lack of robust structures upon single visual words. To overcome these problems, various techniques have been proposed, such as multiple descriptors, spatial context modeling and interest region detection. Though they have been proven to improve the BoF model to some extent, there still lacks a coherent scheme to integrate each individual module.\n To address the problems above, we propose a novel framework with spatial pooling of heterogeneous features. Our framework differs from the traditional Bag-of-Features model on three aspects. First, we propose a new scheme for combining texture and edge based local features together at the descriptor extraction level. Next, we build geometric visual phrases to model spatial context upon heterogeneous features for mid-level representation of images. Finally, based on a smoothed edgemap, a simple and effective spatial weighting scheme is performed on our mid-level image representation. We test our integrated framework on several benchmark datasets for image classification and retrieval applications. The extensive results show the superior performance of our algorithm over state-of-the-art methods.","cites":"8","conferencePercentile":"81.48734177"},{"venue":"ACM Multimedia","id":"08464b1f103d5d9b6d825d2629fb2b24df3a0c3e","venue_1":"ACM Multimedia","year":"2014","title":"An Introduction to Arts and Digital Culture Inside Multimedia","authors":"David A. Shamma, Daragh Byrne","author_ids":"1760364, 2057853","abstract":"The Arts and Digital Culture program has offered a high quality forum for the presentation of interactive and arts-based multimedia applications at the annual ACM Multimedia conference for over a decade. This tutorial will explore the evolution of this program as a guide to new authors considering future participation in this program. By surveying both past technical and past exhibited contributions, this tutorial will offer guidance to artists, researchers and practitioners on success at this multifaceted, interdisciplinary forum at ACM Multimedia.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"2509d658df4eb97d99515845104bc2a9a00fe01a","venue_1":"ACM Multimedia","year":"2002","title":"The immersive cockpit","authors":"Wai-Kwan Tang, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"2429610, 1720633, 1714602","abstract":"Wide field-of-view (FOV) is necessary for many industrial applications, such as air traffic control, large vehicle driving and navigation. Unfortunately, the supporting structure/frame in most systems usually blocks part of the view, results in \"blind spot\" and raises the risk. In some cases, the working site is hazardous to the pilot. In this video demonstration, we introduce a video-based tele-immersive system, called the <i>Immersive Cockpit</i>. It captures live videos from the working site and recreates an immersive environment at the remote site where the pilot situates. It immerses the pilot at the remote site with a panoramic view of the environment, hence improves interactivity and safety. The design goals of our system are <i>real-time, live, low-cost</i> and <i>scalable</i>.We stitch multiple video streams captured from ordinary CCD cameras to generate a panoramic video. To avoid being blocked by the supporting frame, we allow a flexible placement of cameras. This approach trades the accuracy of the generated panorama image for a larger field-of-view. The panoramic video is presented on an immersive display which covers the field-of-view of the viewer.","cites":"3","conferencePercentile":"25.64102564"},{"venue":"ACM Multimedia","id":"9e1024c983839c423a45df8854259cd92204ae96","venue_1":"ACM Multimedia","year":"2014","title":"Exploring Principles-of-Art Features For Image Emotion Recognition","authors":"Sicheng Zhao, Yue Gao, Xiaolei Jiang, Hongxun Yao, Tat-Seng Chua, Xiaoshuai Sun","author_ids":"1755487, 1744619, 3134688, 1720100, 1684968, 1759841","abstract":"Emotions can be evoked in humans by images. Most previous works on image emotion analysis mainly used the elements-of-art-based low-level visual features. However, these features are vulnerable and not invariant to the different arrangements of elements. In this paper, we investigate the concept of principles-of-art and its influence on image emotions. Principles-of-art-based emotion features (PAEF) are extracted to classify and score image emotions for understanding the relationship between artistic principles and emotions. PAEF are the unified combination of representation features derived from different principles, including balance, emphasis, harmony, variety, gradation, and movement. Experiments on the International Affective Picture System (IAPS), a set of artistic photography and a set of peer rated abstract paintings, demonstrate the superiority of PAEF for affective image classification and regression (with about 5% improvement on classification accuracy and 0.2 decrease in mean squared error), as compared to the state-of-the-art approaches. We then utilize PAEF to analyze the emotions of master paintings, with promising results.","cites":"21","conferencePercentile":"96.38554217"},{"venue":"ACM Multimedia","id":"9a79e11b4261aa601d7f77f4ba5aed22ca1f6ad6","venue_1":"ACM Multimedia","year":"2014","title":"Perception-Guided Multimodal Feature Fusion for Photo Aesthetics Assessment","authors":"Luming Zhang, Yue Gao, Chao Zhang, Hanwang Zhang, Qi Tian, Roger Zimmermann","author_ids":"1763785, 1744619, 3671118, 5462268, 1724745, 1790974","abstract":"Photo aesthetic quality evaluation is a challenging task in multimedia and computer vision fields. Conventional approaches suffer from the following three drawbacks: 1) the deemphasized role of semantic content that is many times more important than low-level visual features in photo aesthetics; 2) the difficulty to optimally fuse low-level and high-level visual cues in photo aesthetics evaluation; and 3) the absence of a sequential viewing path in the existing models, as humans perceive visually salient regions sequentially when viewing a photo.\n  To solve these problems, we propose a new aesthetic descriptor that mimics humans sequentially perceiving visually/semantically salient regions in a photo. In particular, a weakly supervised learning paradigm is developed to project the local aesthetic descriptors (graphlets in this work) into a low-dimensional semantic space. Thereafter, each graphlet can be described by multiple types of visual features, both at low-level and in high-level. Since humans usually perceive only a few salient regions in a photo, a sparsity-constrained graphlet ranking algorithm is proposed that seamlessly integrates both the low-level and the high-level visual cues. Top-ranked graphlets are those visually/semantically prominent graphlets in a photo. They are sequentially linked into a path that simulates the process of humans actively viewing. Finally, we learn a probabilistic aesthetic measure based on such actively viewing paths (AVPs) from the training photos that are marked as aesthetically pleasing by multiple users. Experimental results show that: 1) the AVPs are 87.65% consistent with real human gaze shifting paths, as verified by the eye-tracking data; and 2) our photo aesthetic measure outperforms many of its competitors.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"290fb91615adc208ff79764f04eabc74be36936c","venue_1":"ACM Multimedia","year":"2016","title":"Predicting Personalized Emotion Perceptions of Social Images","authors":"Sicheng Zhao, Hongxun Yao, Yue Gao, Rongrong Ji, Wenlong Xie, Xiaolei Jiang, Tat-Seng Chua","author_ids":"1755487, 1720100, 1744619, 1725599, 2648498, 3134688, 1684968","abstract":"Images can convey rich semantics and induce various emotions to viewers. Most existing works on affective image analysis focused on predicting the dominant emotions for the majority of viewers. However, such dominant emotion is often insufficient in real-world applications, as the emotions that are induced by an image are highly subjective and different with respect to different viewers. In this paper, we propose to predict the personalized emotion perceptions of images for each individual viewer. Different types of factors that may affect personalized image emotion perceptions, including visual content, social context, temporal evolution, and location influence, are jointly investigated. Rolling multi-task hypergraph learning is presented to consistently combine these factors and a learning algorithm is designed for automatic optimization. For evaluation, we set up a large scale image emotion dataset from Flickr, named Image-Emotion-Social-Net, on both dimensional and categorical emotion representations with over 1 million images and about 8,000 users. Experiments conducted on this dataset demonstrate that the proposed method can achieve significant performance gains on personalized emotion classification, as compared to several state-of-the-art approaches.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"05bb649eb416cc21636b9af6853fa65990fbd485","venue_1":"ACM Multimedia","year":"2009","title":"Wearing a YouTube hat: directors, comedians, gurus, and user aggregated behavior","authors":"Joan-Isaac Biel, Daniel Gatica-Perez","author_ids":"3082345, 1698682","abstract":"While existing studies on YouTube's massive user-generated video content have mostly focused on the analysis of videos, their characteristics, and network properties, little attention has been paid to the analysis of users' long-term behavior as it relates to the roles they self-define and (explicitly or not) play in the site. In this paper, we present a novel statistical analysis of aggregated user behavior in YouTube from the novel perspective of user categories, a feature that allows people to ascribe to popular roles and to potentially reach certain communities. Using a sample of 270,000 users, we found that a high level of interaction and participation is concentrated on a relatively small, yet significant, group of users, following recognizable patterns of personal and social involvement. Based on our analysis, we also show that by using simple behavioral features from user profiles, people can be automatically classified according to their category with accuracy rates of up to 73%.","cites":"9","conferencePercentile":"64.04958678"},{"venue":"ACM Multimedia","id":"0bfe7347b68f403c2fd3cf8aef8ebf8a5c1f43d1","venue_1":"ACM Multimedia","year":"2011","title":"Innovating the multimedia experience","authors":"Khaled El-Maleh, Haohong Wang, Susie Wee, Hong Heather Yu, James D. Johnston, Zhengyou Zhang","author_ids":"2396778, 2642372, 2500676, 4344274, 2316912, 1732465","abstract":"In this panel, each panelist will present their view of the current state-of-the-art of research and product innovations in the three major areas of multimedia experience: visual, auditory and gaming. We will discuss examples of innovation that enhance the consumption and sharing of multimedia (video, audio, graphics etc.) and thus increase quality of user experience. Another major focus of this panel is to open the discussion on how to innovate new multimedia user experiences.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"270f74378d1e569742b535037c166d23790a4299","venue_1":"ACM Multimedia","year":"2011","title":"Robust hand gesture recognition based on finger-earth mover's distance with a commodity depth camera","authors":"Zhou Ren, Junsong Yuan, Zhengyou Zhang","author_ids":"1808906, 1746449, 1732465","abstract":"The recently developed depth sensors, e.g., the Kinect sensor, have provided new opportunities for human-computer interaction (HCI). Although great progress has been made by leveraging the Kinect sensor, e.g. in human body tracking and body gesture recognition, robust hand gesture recognition remains an open problem. Compared to the entire human body, the hand is a smaller object with more complex articulations and more easily affected by segmentation errors. It is thus a very challenging problem to recognize hand gestures. This paper focuses on building a robust hand gesture recognition system using the Kinect sensor. To handle the noisy hand shape obtained from the Kinect sensor, we propose a novel distance metric for hand dissimilarity measure, called Finger-Earth Mover's Distance (FEMD). As it only matches fingers while not the whole hand shape, it can better distinguish hand gestures of slight differences. The extensive experiments demonstrate the accuracy, efficiency, and robustness of our hand gesture recognition system.","cites":"88","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"353fd4036c169c69649d753104de1bb8a4cbc534","venue_1":"ACM Multimedia","year":"2003","title":"Geographic location tags on digital images","authors":"Kentaro Toyama, Ron Logan, Asta Roseway","author_ids":"1769685, 2372715, 2057918","abstract":"We describe an end-to-end system that capitalizes on geographic location tags for digital photographs. The World Wide Media eXchange (WWMX) database indexes large collections of image media by several pieces of metadata including timestamp, owner, and critically, location stamp. The location where a photo was shot is important because it says much about its semantic content, while being relatively easy to acquire, index, and search.The process of building, browsing, and writing applications for such a database raises issues that have heretofore been un- addressed in either the multimedia or the GIS community. This paper brings all of these issues together, explores different options, and offers novel solutions where necessary. Topics include acquisition of location tags for image media, data structures for location tags on photos, database optimization for location-tagged image media, and an intuitive UI for browsing a massive location-tagged image database. We end by describing an application built on top of the WWMX, a lightweight travelogue-authoring tool that automatically creates appropriate context maps for a slideshow of location-tagged photographs.","cites":"168","conferencePercentile":"98.1981982"},{"venue":"ACM Multimedia","id":"0951aa02500a8f10632de8a2c6db16d776be9cba","venue_1":"ACM Multimedia","year":"2011","title":"Robust hand gesture recognition with kinect sensor","authors":"Zhou Ren, Jingjing Meng, Junsong Yuan, Zhengyou Zhang","author_ids":"1808906, 1691016, 1746449, 1732465","abstract":"Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game.","cites":"59","conferencePercentile":"99.41690962"},{"venue":"ACM Multimedia","id":"110e36c12436aab43ce77765fe1413328e69f24c","venue_1":"ACM Multimedia","year":"2012","title":"Multi-view learning from imperfect tagging","authors":"Zhongang Qi, Ming Yang, Zhongfei Zhang, Zhengyou Zhang","author_ids":"2539841, 5015133, 1720488, 1732465","abstract":"In many real-world applications, tagging is imperfect: incomplete, inconsistent, and error-prone. Solutions to this problem will generate societal and technical impacts. In this paper, we investigate this arguably new problem: learning from imperfect tagging. We propose a general and effective learning scheme called the Multi-view Imperfect Tagging Learning (MITL) to this problem. The main idea of MITL lies in extracting the information of the imperfectly tagged training dataset from multiple views to differentiate the data points in the role of classification. Further, a novel discriminative classification method is proposed under the framework of MITL, which explicitly makes use of the given multiple labels simultaneously as an additional feature to deliver a more effective classification performance than the existing literature where one label is considered at a time as the classification target while the rest of the given labels are completely ignored at the same time. The proposed methods can not only complete the incomplete tagging but also denoise the noisy tagging through an inductive learning. We apply the general solution to the problem with a more specific context - imperfect image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods on solving the problem of learning from imperfect tagging in cross-media.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"2607f0093fecd4fee5244d56fcf3f53ff22e949e","venue_1":"ACM Multimedia","year":"2013","title":"Attribute-augmented semantic hierarchy: towards bridging semantic gap and intention gap in image retrieval","authors":"Hanwang Zhang, Zheng-Jun Zha, Yang Yang, Shuicheng Yan, Yue Gao, Tat-Seng Chua","author_ids":"5462268, 2355916, 3432893, 1698982, 1744619, 1684968","abstract":"This paper presents a novel Attribute-augmented Semantic Hierarchy (A<sup>2</sup> SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in Content-based Image Retrieval (CBIR). A<sup>2</sup> SH organizes the semantic concepts into multiple semantic levels and augments each concept with a set of related attributes, which describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. A hierarchical semantic similarity function is learnt to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedbacks on attributes and images. These feedbacks are then used to refine the search results based on A<sup>2</sup> SH. We develop a content-based image retrieval system based on the proposed A<sup>2</sup> SH. We conduct extensive experiments on a large-scale data set of over one million Web images. Experimental results show that the proposed A<sup>2</sup> SH can characterize the semantic affinities among images accurately and can shape user search intent precisely and quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.","cites":"21","conferencePercentile":"95.11111111"},{"venue":"ACM Multimedia","id":"8145acbdeddcca7166332c61ca8f823a2e238c6d","venue_1":"ACM Multimedia","year":"2016","title":"DRIVING: Distributed Scheduling for Video Streaming in Vehicular Wi-Fi Systems","authors":"Xi Chen, Lei Rao, Qiao Xiang, Xue Liu, Fan Bai","author_ids":"1714741, 2215639, 2955716, 1723807, 1735569","abstract":"Video streaming has been dominating the mobile bandwidth, and is still expanding drastically. Its tremendous economic benefits have driven the automobile industry to equip vehicles with video streaming capacity. As a result, the new in-cabin Wi-Fi systems have been deployed, enabling each vehicle as a streaming hotspot on the wheels. A built-in Access Point (AP) bridges the communications between Wi-Fi devices inside and cellular networks outside. Distinct advantages offered by this system include a more powerful antenna array to improve multimedia quality, a constant energy source to power the streaming, etc. However, there exist two challenging features that may jeopardize the system performance. (1) The in-cabin Wi-Fi hotspots are mostly deployed on private vehicles, and thus are completely decentralized. (2) Video packets need to be delivered before their deadlines with small delays. Due to these features, existing algorithms may fail to efficiently schedule the in-cabin Wi-Fi video streaming. To fill the gap, we propose the Delay-awaRe dIstributed Video schedulING (DRIVING) framework. Being fully distributed and delay-aware, DRIVING not only increases the streaming goodput, but also reduces the delivery latency and deadline missing ratio. %In order to optimize this new framework, we establish cross-layer analytical models, which help us tune the framework parameters for better performance. In a typical scenario, DRIVING increases the goodput by up to 27.0%, while reducing the queueing delay and the deadline missing ratio by up to 40.0% and 38.4%, respectively.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"d81d236dfe9aafceb6ddf635ef80ca322eca7dab","venue_1":"ACM Multimedia","year":"2012","title":"Joint semantic segmentation by searching for compatible-competitive references","authors":"Ping Luo, Xiaogang Wang, Liang Lin, Xiaoou Tang","author_ids":"1693209, 2868636, 1737218, 1741901","abstract":"This paper presents a framework for semantically segmenting a target image without tags by searching for references in an image database, where all the images are unsegmented but annotated with tags. We jointly segment the target image and its references by optimizing both <i>semantic consistencies</i> within individual images and <i>correspondences</i> between the target image and each of its references. In our framework, we first retrieve two types of references with a semantic-driven scheme: i) the compatible references which share similar global appearance with the target image; and ii) the competitive references which have distinct appearance to the target image but similar tags with one of the compatible references. The two types of references have complementary information for assisting the segmentation of the target image. Then we construct a novel graphical representation, in which the vertices are superpixels extracted from the target image and its references. The segmentation problem is posed as labeling all the vertices with the semantic tags obtained from the references. The method is able to label images without the pixel-level annotation and classifier training, and it outperforms the state-of-the-arts approaches on the MSRC-21 database.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"90198be8b40fc01924a1c3b4e4cee519570bf7c0","venue_1":"ACM Multimedia","year":"2010","title":"Large-scale music tag recommendation with explicit multiple attributes","authors":"Zhendong Zhao, Xinxi Wang, Qiaoliang Xiang, Andy M. Sarroff, Zhonghua Li, Ye Wang","author_ids":"2435606, 2348299, 3235642, 2122918, 2746295, 1681196","abstract":"Social tagging can provide rich semantic information for large-scale retrieval in music discovery. Such collaborative intelligence, however, also generates a high degree of tags unhelpful to discovery, some of which obfuscate critical information. Towards addressing these shortcomings, tag recommendation for more robust music discovery is an emerging topic of significance for researchers. However, current methods do not consider diversity of music attributes, often using simple heuristics such as tag frequency for filtering out irrelevant tags. Music attributes encompass any number of perceived dimensions, for instance vocalness, genre, and instrumentation. Many of these are underrepresented by current tag recommenders. We propose a scheme for tag recommendation using Explicit Multiple Attributes based on tag semantic similarity and music content. In our approach, the attribute space is explicitly constrained at the outset to a set that minimizes semantic loss and tag noise, while ensuring attribute diversity. Once the user uploads or browses a song, the system recommends a list of relevant tags in each attribute independently. To the best of our knowledge, this is the first method to consider Explicit Multiple Attributes for tag recommendation. Our system is designed for large-scale deployment, on the order of millions of objects. For processing large-scale music data sets, we design parallel algorithms based on the MapReduce framework to perform large-scale music content and social tag analysis, train a model, and compute tag similarity. We evaluate our tag recommendation system on CAL-500 and a large-scale data set ($N = 77,448$ songs) generated by crawling Youtube and Last.fm. Our results indicate that our proposed method is both effective for recommending attribute-diverse relevant tags and efficient at scalable processing.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"0ae92fcd6d8bc6d8c4fa5a35b372d2bd2cb62f03","venue_1":"ACM Multimedia","year":"1994","title":"Video Mosaic: Laying Out Time in a Physical Space","authors":"Wendy E. Mackay, Daniele Pagani","author_ids":"1732917, 2061046","abstract":"Paper video storyboards are still in use by even very experienced video producers with access to the most advanced video editing software. An analysis of the characteristics of paper and on-line editing provide an overlapping but distinct set of benefits (and problems). Paper provides the user with the ability to lay out various temporal sequences over a large spatial area and the ability to quickly sketch, annotate and rearrange the relevant video clips. On-line editing provides users with the ability to generate and store a variety of video arrangements. Video Mosaic provides users with the ability to combine the best of both worlds: elements of a paper video storyboard are used as input to an on-line video editing system to take advantage of the best aspects of each. We developed a Unix and a Macintosh version of Video Mosaic. This paper describes the design of Video Mosaic, compares alternative approaches to creating this type of application, and suggests directions for future work.","cites":"70","conferencePercentile":"88.13559322"},{"venue":"ACM Multimedia","id":"7d9f9a8e2803416d01e0bea9006efc58ecc5aaf7","venue_1":"ACM Multimedia","year":"2011","title":"Transfer tagging from image to video","authors":"Yang Yang, Yi Yang, Zi Huang, Heng Tao Shen","author_ids":"3432893, 1698559, 4778316, 1724393","abstract":"Nowadays massive amount of web video datum has been emerging on the Internet. To achieve an effective and efficient video retrieval, it is critical to automatically assign semantic keywords to the videos via content analysis. However, most of the existing video tagging methods suffer from the problem of lacking sufficient tagged training videos due to high labor cost of manual tagging. Inspired by the observation that there are much more well-labeled data in other yet relevant types of media (e.g. images), in this paper we study how to build a \"cross-media tunnel\" to transfer external tag knowledge from image to video. Meanwhile, the intrinsic data structures of both image and video spaces are well explored for inferring tags. We propose a <i>Cross-Media Tag Transfer</i> (CMTT) paradigm which is able to: 1) transfer tag knowledge between image and video by minimizing their distribution difference; 2) infer tags by revealing the underlying manifold structures embedded within both image and video spaces. We also learn an explicit mapping function to handle unseen videos. Experimental results have been reported and analyzed to illustrate the superiority of our proposal.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"348b0133bd5d0d885ce7e0f8e5d6bb394a03b2b2","venue_1":"ACM Multimedia","year":"2011","title":"Multiple feature hashing for real-time large scale near-duplicate video retrieval","authors":"Jingkuan Song, Yi Yang, Zi Huang, Heng Tao Shen, Richang Hong","author_ids":"1721410, 1698559, 4778316, 1724393, 1739103","abstract":"Near-duplicate video retrieval (NDVR) has recently attracted lots of research attention due to the exponential growth of online videos. It helps in many areas, such as copyright protection, video tagging, online video usage monitoring, etc. Most of existing approaches use only a single feature to represent a video for NDVR. However, a single feature is often insufficient to characterize the video content. Besides, while the accuracy is the main concern in previous literatures, the scalability of NDVR algorithms for large scale video datasets has been rarely addressed. In this paper, we present a novel approach - Multiple Feature Hashing (MFH) to tackle both the accuracy and the scalability issues of NDVR. MFH preserves the local structure information of each individual feature and also globally consider the local structures for all the features to learn a group of hash functions which map the video keyframes into the Hamming space and generate a series of binary codes to represent the video dataset. We evaluate our approach on a public video dataset and a large scale video dataset consisting of 132,647 videos, which was collected from YouTube by ourselves. The experiment results show that the proposed method outperforms the state-of-the-art techniques in both accuracy and efficiency.","cites":"80","conferencePercentile":"99.70845481"},{"venue":"ACM Multimedia","id":"58ba3cccc5a872e146c16634813d37c53b6a5cde","venue_1":"ACM Multimedia","year":"2009","title":"Retrieval based interactive cartoon synthesis via unsupervised bi-distance metric learning","authors":"Yi Yang, Yueting Zhuang, Dong Xu, Yunhe Pan, Dacheng Tao, Stephen J. Maybank","author_ids":"1698559, 1755711, 1714390, 1778259, 7761803, 1998180","abstract":"Cartoons play important roles in many areas, but it requires a lot of labor to produce new cartoon clips. In this paper, we propose a gesture recognition method for cartoon character images with two applications, namely content-based cartoon image retrieval and cartoon clip synthesis. We first define Edge Features (EF) and Motion Direction Features (MDF) for cartoon character images. The features are classified into two different groups, namely <i><b>intra-features</b></i> and <i><b>inter-features</b></i>. An Unsupervised Bi-Distance Metric Learning (UBDML) algorithm is proposed to recognize the gestures of cartoon character images. Different from the previous research efforts on distance metric learning, UBDML learns the optimal distance metric from the heterogeneous distance metrics derived from intra-features and inter-features. Content-based cartoon character image retrieval and cartoon clip synthesis can be carried out based on the distance metric learned by UBDML. Experiments show that the cartoon character image retrieval has a high precision and that the cartoon clip synthesis can be carried out efficiently.","cites":"12","conferencePercentile":"75.41322314"},{"venue":"ACM Multimedia","id":"8be7d2419c405a77d8bdad25bf7b2516056f339a","venue_1":"ACM Multimedia","year":"2008","title":"Heterogeneous multimedia data semantics mining using content and location context","authors":"Yi Yang, Yueting Zhuang, Wenhua Wang","author_ids":"1698559, 1755711, 4940203","abstract":"Because it is very common that the heterogeneous multimedia data of the same semantics always exist jointly in many domain and application specific databases, it is very helpful to consider the location information when analyzing multimedia data. In this paper we propose a method of integrating the content and location context for multimedia data mining to enable the cross-media retrieval, by which the query examples and the returned results can be of different modalities, e.g. to query audios by an example of image. We construct a graph model by combing the multimedia content and location information. The graph model is then refined according to different strategies. The semantic correlations among multimedia data are calculated by learning the high-order neighborhood structure of the graph and the Multimedia Correlation Space is constructed in which the cross-media retrieval can be performed. We also propose different methods of Relevance Feedback to improve the search results. Experiments demonstrate the promise of the proposed method.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"6bcac9dad004cb7f2ea3c6778ff6635793b26126","venue_1":"ACM Multimedia","year":"2003","title":"Creating touch-screens anywhere with interactive projected displays","authors":"Claudio S. Pinhanez, Rick Kjeldsen, Lijun Tang, Anthony Levas, Mark Podlaseck, Noi Sukaviriya, Gopal Sarma Pingali","author_ids":"1766240, 2739350, 4601982, 3250836, 2929348, 2745456, 1717077","abstract":"We demonstrate a system that combines steerable projection and computer vision technologies to create \"touch-screen\" style interactive displays on any flat surface in a space. A high-end version of the system -- the Everywhere Display (ED) -- combines an LCD projector with motorized focus and zoom, a computer controlled pan-tilt mirror, and a pan-tilt zoom camera to enable steering of interactive projections around space. A low-end version (ED-lite) enables creation of interactive displays using a portable projector and camera attached to a laptop computer. Unlike traditional augmented reality systems, the ED systems enable delivery of interactive multimedia content on ordinary objects without requiring users to wear head mounted displays or carry special input devices.","cites":"6","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"3af8636c4a45bdb94f00c3b4bbf41af27694a3af","venue_1":"ACM Multimedia","year":"2012","title":"Fast semantic image retrieval based on random forest","authors":"Hao Fu, Guoping Qiu","author_ids":"1690222, 8249242","abstract":"This paper introduces random forest as a computational and data structure paradigm for fusing low-level visual features and high-level semantic concepts for image retrieval. We use visual features to split the tree nodes and use the image labels to supervise the splitting to make images located at the same tree node share similar semantic concepts as well as visual similarities. We exploit such a random forest and define the semantic neighbor set (SNS) of a given image as the union of all images in the leaf nodes that this image falls onto. From SNS we further define the semantic similarity measure (SSM) between two images as the number of trees in which they share the same leaf nodes within a SNS. With SNS and SSM, example-based image retrieval becomes that of first finding the SNS of the querying image and then ranking the images according to the SSMs between the querying image and images in its SNS. We also show that the new technique can be adapted for keyword-based semantic image retrieval. The inherent efficient tree data structure leads to fast solutions. We will present experimental results to show the effectiveness of this new semantic image retrieval technique.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"79f4302b69df14a6517c21e5f1c985a91a921073","venue_1":"ACM Multimedia","year":"2010","title":"The idiap wolf corpus: exploring group behaviour in a competitive role-playing game","authors":"Hayley Hung, Gokul Chittaranjan","author_ids":"1756464, 2545632","abstract":"In this paper we present the Idiap Wolf Database. This is a audio-visual corpus containing natural conversational data of volunteers who took part in a competitive role-playing game. Four groups of 8-12 people were recorded. In total, just over 7 hours of interactive conversational data was collected. The data has been annotated in terms of the roles and outcomes of the game. There are 371 examples of different roles played over 50 games. Recordings were made with headset microphones, an 8-microphone array, and 3 video cameras and are fully synchronised. The novelty of this data is that some players have deceptive roles and the participants do not know what roles other people play.","cites":"13","conferencePercentile":"81.50684932"},{"venue":"ACM Multimedia","id":"6e24ae4864a9fee9bc953963574664c10e88717e","venue_1":"ACM Multimedia","year":"2010","title":"Encounter (resonances)","authors":"Hayley Hung, Christian Jacquemin","author_ids":"1756464, 1717038","abstract":"This work is about the remediation of one of Mark Rothko's Seagram murals through the composition of several online sources and additional digital rendering. Based on reproductions of Rothko's \"Red on Maroon\" found on the Internet, and using computer graphics compositing associated with moir&#233; and specular lighting effects, \"Encounter (Resonances)\" offers a new approach to the presentation of a piece of work that allows a viewer to perceive some of its very subtle nuances. The work echoes Rothko's mixed media layered painting technique by using reproductions of various color palettes and resolutions as metaphors for the layers of paint in his original works. While each of these copies may instantly remind us of the original work, the graphical rendering of \"Encounter (Resonances)\" combines them at three levels of representation (global shape, micro and macro structure), in an effort to encourage a level of prolonged engagement and gradual discovery in the artwork.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"eac947cdb3fa17d83810a383dd7b41d28d58c4ea","venue_1":"ACM Multimedia","year":"2012","title":"Robust cross-media transfer for visual event detection","authors":"Yang Yang, Yi Yang, Zi Huang, Jiajun Liu, Zhigang Ma","author_ids":"3432893, 1698559, 4778316, 8669089, 1727419","abstract":"In this paper, we present a novel approach, named <i>Robust Cross-Media Transfer </i> (RCMT), for visual event detection in social multimedia environments. Different from most existing methods, the proposed method can directly take different types of noisy social multimedia data as input and conduct robust event detection. More specifically, we build a robust model by employing an <i><sup>l</sup></i>2,1-norm regression model featuring noise tolerance, and also manage to integrate different types of social multimedia data by minimizing the distribution difference among them. Experimental results on real-life Flickr image dataset and YouTube video dataset demonstrate the effectiveness of our proposal, compared to state-of-the-art algorithms.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"559248171b3992b725c6f06c4dafaf3c8a21a6c2","venue_1":"ACM Multimedia","year":"2012","title":"Multi-view video contents viewing system by synchronized multi-view streaming architecture","authors":"Takafumi Marutani, Kenji Mase, Toshiaki Fujii, Tetsuya Kawamoto","author_ids":"2525313, 1722602, 1732969, 7867370","abstract":"We developed a novel networked video viewing system for multi-view video contents with video streaming technology. This work's contribution is that our developed system confirmed the validity of simultaneous multiple video streaming architecture that incorporates real-time channel switching and a target-oriented viewing interface. In addition, we newly introduced an inter-channel bandwidth management scheme to achieve cost-effective multi-view streaming.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"0e619f466a647b3faa08a839258a38f6e898ce65","venue_1":"ACM Multimedia","year":"2015","title":"Tutorial on Emotional and Social Signals for Multimedia Research","authors":"Hayley Hung, Hatice Gunes","author_ids":"1756464, 1781916","abstract":"1. MOTIVATION In 2006, Jaimes et al. [3] wrote about the role of human-centered computing (HCC) for Multimedia, finally devising a research agenda for Human-centered multimedia (HCM). Since then, the importance of humans in all aspects of multimedia research has increased significantly leading to an explosion in multimedia computing that exploits data coming from online social multimedia such as tags, social networks, etc. However, as the area grows and the research links between multimedia systems and humans become closer, theories and practices from other domains (namely social, behavioural, and emotional psychology, social signal processing, and affective computing) need to be more fully integrated into multimedia computing practices. In 2014, ACM MM launched a new area on emotional and social signals in multimedia that received many submissions and had a full session during the conference. Concurrently to this, a panel discussion was held at the conference organised by ourselves, Lex-ing Xie, and Bjoern Schuller to discuss the state of play for emotional and social signals in Multimedia research. By the end of the panel a number of clear challenges to integrate emotional and social signals into core multimedia research domains became apparent as clear multimedia concerns, which we summarised later in [1]. A challenge for human-centred multimedia is the analysis of human communicative behaviour in multimedia content when considering especially the spontaneous non-verbal signals that are gener-* H. Hung and H.Gunes contributed equally to this tutorial. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). ated by humans when interacting with each other. These signals require a slightly different approach to multimedia computing where the methods developed need findings from other disciplines such as social and behavioural psychology, affective computing or social signal processing. Considering the shift of much of the multimedia community towards the close-captioning of images and video, the next step is clearly to consider both the social and affective content of these videos or images. Moreover, as much of multimedia content creation shifts towards portable mobile devices, we must also start to consider the …","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"388e6b088fecea73e9365d5fb5b470e1ebb2217b","venue_1":"ACM Multimedia","year":"2009","title":"Integrating corrections into digital ink playback","authors":"Richard J. Anderson, Devy Pranowo, Craig Prince, Fred Videon","author_ids":"2308531, 2177237, 1786208, 3004136","abstract":"In this paper, we describe preliminary work on an ink editing application that allows an instructor to correct mistakes to digital ink written during a presentation that is to be archived. These corrections are then seamlessly reintegrated into the digital archive so that when the presentation is replayed the corrected ink is displayed instead of the original incorrect ink. We base our results on a system we have developed and prototype the work flow from initial presentation, through correction, updating the archive and playback. We show that a simple mechanism for correction is effective and low effort for the instructor. A key technical challenge that is addressed is the substitution of strokes by matching of the original and corrected ink.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"0afba9bd19b3cdf53fac07cd0470563c175b03bb","venue_1":"ACM Multimedia","year":"2002","title":"Pixie: a jukebox architecture to support efficient peer content exchange","authors":"Sami Rollins, Kevin C. Almeroth","author_ids":"1774595, 3131174","abstract":"Peer-to-peer (P2P) content exchange has recently gained attention from both the research and industrial communities. The dynamic nature of peer networks and the resource constraints of peer hosts have introduced a number of unique technical challenges that must be addressed to make large-scale P2P content exchange applications viable. In this work, we present and evaluate Pixie, an architecture that integrates one-to-many distribution of content and peer networks. Pixie provides a valuable data location service as well as a number of scalability properties both in terms of data location and content distribution. Our results indicate that, using a one-to-many scheme, we can significantly reduce the resources consumed in searching for and distributing content across peer networks. These scalability properties will become increasingly important as peer content exchange is extended to support more advanced applications.","cites":"7","conferencePercentile":"42.30769231"},{"venue":"ACM Multimedia","id":"493a5df94c3cd35fa7c452513435bead8c3a2340","venue_1":"ACM Multimedia","year":"2016","title":"Tracking Natural Events through Social Media and Computer Vision","authors":"Jingya Wang, Mohammed Korayem, Saul Blanco, David J. Crandall","author_ids":"2375091, 1775613, 2427382, 2821130","abstract":"Accurate, efficient, global observation of natural events is important for ecologists, meteorologists, governments, and the public. Satellites are effective but limited by their perspective and by atmospheric conditions. Public images on photo-sharing websites could provide crowd-sourced ground data to complement satellites, since photos contain evidence of the state of the natural world. In this work, we test the ability of computer vision to observe natural events in millions of geo-tagged Flickr photos, over nine years and an entire continent. We use satellites as (noisy) ground truth to train two types of classifiers, one that estimates if a Flickr photo has evidence of an event, and one that aggregates these estimates to produce an observation for given times and places. We present a web tool for visualizing the satellite and photo observations, allowing scientists to explore this novel combination of data sources.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"a397f3d749b94d87b14a8ebeb8f045ef914c75ef","venue_1":"ACM Multimedia","year":"2015","title":"How Was It?: Exploiting Smartphone Sensing to Measure Implicit Audience Responses to Live Performances","authors":"Claudio Martella, Ekin Gedik, Laura Cabrera Quiros, Gwenn Englebienne, Hayley Hung","author_ids":"3244826, 2282495, 1989524, 2654643, 1756464","abstract":"In this paper, we present an approach to understand the response of an audience to a live dance performance by the processing of mobile sensor data. We argue that exploiting sensing capabilities already available in smart phones enables a potentially large scale measurement of an audience's implicit response to a performance. In this work, we leverage both tri-axial accelerometers, worn by ordinary members of the public during a dance performance, to predict responses to a number of survey answers, comprising enjoyment, immersion, willingness to recommend the event to others, and change in mood. We also analyse how behaviour as a result of seeing a dance performance might be reflected in a people's subsequent social behaviour using proximity and acceleration sensing. To our knowledge, this is the first work where pervasive mobile sensing has been used to investigate spontaneous responses to predict the affective evaluation of a live performance. Using a single body worn accelerometer to monitor a set of audience members, we were able to predict whether they enjoyed the event with a balanced classification accuracy of 90\\%. The collective coordination of the audience's bodily movements also highlighted memorable moments that were reported later by the audience. The effective use of body movements to measure affective responses in such a setting is particularly surprising given that traditionally, physiological signals such as skin conductance or brain-based signals are the more commonly accepted methods to measure implicit affective response. Our experiments open interesting new directions for research on both automated techniques and applications for the implicit tagging of real world events via spontaneous and implicit audience responses during as well as after a performance.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"587027b0f722dd56ae5b0d4299b5e75934cd6fbd","venue_1":"ACM Multimedia","year":"2009","title":"TuVista: meeting the multimedia needs of mobile sports fans","authors":"Frank Bentley, Michael Groble","author_ids":"2634413, 1769016","abstract":"We describe the TuVista system, a service for viewing near-live sports content consisting of a multimedia editing/bundling station, a cloud-hosted metadata server, and a set of mobile clients. We begin by introducing TuVista I, a proof of concept experience prototype implemented quickly as a probe to understand multimedia needs at a live sporting event. After discussing the results of an initial field trial at Estadio Azteca in Mexico City, we describe the improvements in TuVista II to address the issues identified. These include rapid editing of multiple live video streams, push notifications of new content over XMPP, and an optimized metadata workflow for the content producer that reduced content publication time from fifteen minutes to less than 30 seconds. We conclude with a discussion of rapid prototyping and field deployments as a way to quickly identify user needs.","cites":"18","conferencePercentile":"82.85123967"},{"venue":"ACM Multimedia","id":"dc0e274c2114530bb305233c44636db290f5352f","venue_1":"ACM Multimedia","year":"2016","title":"Who is where?: Matching People in Video to Wearable Acceleration During Crowded Mingling Events","authors":"Laura Cabrera Quiros, Hayley Hung","author_ids":"1989524, 1756464","abstract":"We address the challenging problem of associating acceleration data from a wearable sensor with the corresponding spatio-temporal region of a person in video during crowded mingling scenarios. This is an important first step for multi-sensor behavior analysis using these two modalities. Clearly, as the numbers of people in a scene increases, there is also a need to robustly and automatically associate a region of the video with each person's device. We propose a hierarchical association approach which exploits the spatial context of the scene, outperforming the state-of-the-art approaches significantly. Moreover, we present experiments on matching from 3 to more than 130 acceleration and video streams which, to our knowledge, is significantly larger than prior works where only up to 5 device streams are associated.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"93e08c12c12990a46b423a38ef36ce56205ca4a9","venue_1":"ACM Multimedia","year":"2007","title":"Emotion-based impressionism slideshow with automatic music accompaniment","authors":"Cheng-Te Li, Man-Kwan Shan","author_ids":"2169355, 3030557","abstract":"In this paper, we propose the emotion-based Impressionism slideshow system with automatic music accompaniment. While conventional image slideshow systems accompany images with music manually, our proposed approach explores the affective content of painting to automatically recommend music based on emotions. This is achieved by association discovery between painting features and emotions, and between emotions and music features respectively. To generate more harmonic Impressionism presentation, a linear arrangement method is proposed based on modified traveling salesman algorithm. Moreover, some animation effects and synchronization issues for affective content of Impressionism fine arts are considered. Experimental result shows our emotion-based accompaniment brings better browsing experience of aesthetics.","cites":"19","conferencePercentile":"78.125"},{"venue":"ACM Multimedia","id":"dcbe1fa046044d76cd83bacc04ea52ad34194fec","venue_1":"ACM Multimedia","year":"2011","title":"PhotoFeel: feeling your photo collection with graph-based audiovisual flocking","authors":"Cheng-Te Li, Hsun-Ping Hsieh, Shou-De Lin","author_ids":"2169355, 2670863, 1724206","abstract":"This paper proposes an audiovisual presentation system, called PhotoFeel, to not only give users pleasant browsing atmosphere but also deliver a quick sense conveyed by given photo collection. While conventional photo display systems aim at improving the styles of presentation, we explore the visual and semantic feelings to create a space exhibiting the feelings from photographers. This is achieved by simulating the interactions among photos to emerge some flocking behaviors, where each photo is regarded as a simulated agent. To present the feelings of photos on the flocking, we construct two graphs by investigating the visual contents and tag semantics respectively. In addition, to enhance the diverse feelings of a photo collection, three audiovisual effects are composed to have rich presentation of feelings. Experimental results show that our PhotoFeel truly exhibits potential feelings for given photos and people comparatively favor our system.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"7847803d4c48f25b142ffc3b261a7e1366a1153d","venue_1":"ACM Multimedia","year":"2013","title":"Fourth international workshop on human behavior understanding (HBU 2013)","authors":"Albert Ali Salah, Hayley Hung, Oya Aran, Hatice Gunes","author_ids":"1764521, 1756464, 1685598, 1781916","abstract":"With advances in pattern recognition and multimedia computing, it became possible to analyze human behavior via multimodal sensors, at different time-scales and at different levels of interaction and interpretation. This ability opens up enormous possibilities for multimedia and multimodal interaction, with a potential of endowing the computers with a capacity to attribute meaning to users' attitudes, preferences, personality, social relationships, etc., as well as to understand what people are doing, the activities they have been engaged in, their routines and lifestyles. This workshop gathers researchers dealing with the problem of modeling human behavior under its multiple facets with particular attention to interactions in arts, creativity, entertainment and edutainment.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"dbb52fbeb514c5cdfb46665825e32216cc87d7ad","venue_1":"ACM Multimedia","year":"2009","title":"Visual speaker localization aided by acoustic models","authors":"Gerald Friedland, Chuohao Yeo, Hayley Hung","author_ids":"1797144, 3101028, 1756464","abstract":"The following paper presents a novel audio-visual approach for unsupervised speaker locationing. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the art audio-only speaker localization system (traditionally called speaker diarization) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the number of speakers and estimates \"who spoke when\", then, in a second step, the visual models are used to infer the location of the speakers in the video. The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker locationing at little incremental engineering and computation costs.","cites":"14","conferencePercentile":"79.33884298"},{"venue":"ACM Multimedia","id":"2f5ae832284c2f9fe2bba449b961745aef817bbf","venue_1":"ACM Multimedia","year":"2015","title":"Beyond Doctors: Future Health Prediction from Multimedia and Multimodal Observations","authors":"Liqiang Nie, Luming Zhang, Yi Yang, Meng Wang, Richang Hong, Tat-Seng Chua","author_ids":"1743245, 1763785, 1698559, 1731598, 1739103, 1684968","abstract":"Although chronic diseases cannot be cured, they can be effectively controlled as long as we understand their progressions based on the current observational health records, which is often in the form of multimedia data. A large and growing body of literature has investigated the disease progression problem. However, far too little attention to date has been paid to jointly consider the following three observations of the chronic disease progression: 1) the health statuses at different time points are chronologically similar; 2) the future health statuses of each patient can be comprehensively revealed from the current multimedia and multimodal observations, such as visual scans, digital measurements and textual medical histories; and 3) the discriminative capabilities of different modalities vary significantly in accordance to specific diseases. In the light of these, we propose an adaptive multimodal multi-task learning model to co-regularize the modality agreement, temporal progression and discriminative capabilities of different modalities. We theoretically show that our proposed model is a linear system. Before training our model, we address the data missing problem via the matrix factorization approach. Extensive evaluations on a real-world Alzheimer's disease dataset well verify our proposed model. It should be noted that our model is also applicable to other chronic diseases.","cites":"14","conferencePercentile":"97.40740741"},{"venue":"ACM Multimedia","id":"01e1c036795c9f131d1719c6e6e7efd6e1fd80b4","venue_1":"ACM Multimedia","year":"2004","title":"Incremental detection of text on road signs from video with application to a driving assistant system","authors":"Wen Wu, Xilin Chen, Jie Yang","author_ids":"5780816, 1710220, 1688428","abstract":"This paper proposes a fast and robust framework for incrementally detecting text on road signs from natural scene video. The new framework makes two main contributions. First, the framework applies a Divide-and-Conquer strategy to decompose the original task into two sub-tasks, that is, localization of road signs and detection of text. The algorithms for the two sub-tasks are smoothly incorporated into a unified framework through a real time tracking algorithm. Second, the framework provides a novel way for text detection from video by integrating 2D features in each video frame (e.g., color, edges, texture) with 3D information available in a video sequence (e.g., object structure). The feasibility of the proposed framework has been evaluated on the video sequences captured from a moving vehicle. The new framework can be applied to a driving assistant system and other tasks of text detection from video.","cites":"12","conferencePercentile":"65.93137255"},{"venue":"ACM Multimedia","id":"ab47aaced41665df8f7ba993c4ade561b4e749b0","venue_1":"ACM Multimedia","year":"2010","title":"Automated sleep quality measurement using EEG signal: first step towards a domain specific music recommendation system","authors":"Wei Zhao, Xinxi Wang, Ye Wang","author_ids":"1679099, 2348299, 1681196","abstract":"With the rapid pace of modern life, millions of people suffer from sleep problems. Music therapy, as a non-medication approach to mitigating sleep problems, has attracted increasing attention recently. However the adaptability of music therapy is limited by the time consuming task of choosing suitable music for users. Inspired by this observation, we discuss the concept of a domain specific music recommendation system, which automatically recommends music for users according to their sleep quality. The proposed system requires multidisciplinary efforts including automated sleep quality measurement and content-based music similarity measure. As a first step, we focus on the automated sleep quality measurement in this paper. An EEG-based approach is proposed to measure user's sleep quality. The advantages of our approach over standard Polysomnography (PSG) method are: 1) it measures sleep quality by recognizing three sleep categories rather than six sleep stages, thus higher accuracy can be expected; 2) three sleep categories are recognized by analyzing Electroencephalography (EEG) signal only, so the user experience is improved because he is attached with fewer sensors during sleep. We conduct experiments based on a standard data set. Our approach achieves high accuracy and shows promising potential for the music recommendation system.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"0b9af4e1ce320d7a8773ecb49e7f34bdb44eed88","venue_1":"ACM Multimedia","year":"2010","title":"MOGCLASS: a collaborative system of mobile devices forclassroom music education","authors":"Yinsheng Zhou, Graham Percival, Xinxi Wang, Ye Wang, Shengdong Zhao","author_ids":"3128975, 2307208, 2348299, 1681196, 2645457","abstract":"We introduce MOGCLASS: a system of networked mobile devices to amplify and extend children's capabilities to perceive, perform and produce music collaboratively in classroom context. MOGCLASS includes various features for students to enhance their motivation, interest, and collaboration in music class. It provides a wide-ranging palette of easy-to-use musical instruments for students to choose from, and supports both collaborative silent practice with headphones, and collaborative performance with loudspeakers. To facilitate classroom management, the teacher's interface is used to control students' activities. Our evaluation results indicate that MOGCLASS is effective in increasing students' motivation in learning music and in supporting teachers' classroom management","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"1a2eb5655f11a434e5d26809b6969b1b1c41dde8","venue_1":"ACM Multimedia","year":"2012","title":"A daily, activity-aware, mobile music recommender system","authors":"Xinxi Wang, Ye Wang, David S. Rosenblum","author_ids":"2348299, 1681196, 1889646","abstract":"Existing music recommender systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we demonstrate in this demo a novel system to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. In our system, contextual information is integrated with music content analysis to offer recommendation for daily activities.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"14bfa91774b48fd6a26a8b446881824a64c52b6a","venue_1":"ACM Multimedia","year":"2012","title":"Context-aware mobile music recommendation for daily activities","authors":"Xinxi Wang, David S. Rosenblum, Ye Wang","author_ids":"2348299, 1889646, 1681196","abstract":"Existing music recommendation systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we present in this paper a novel approach to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. We present a probabilistic model to integrate contextual information with music content analysis to offer music recommendation for daily activities, and we present a prototype implementation of the model. Finally, we present evaluation results demonstrating good accuracy and usability of the model and prototype.","cites":"38","conferencePercentile":"98.73417722"},{"venue":"ACM Multimedia","id":"8807cc1821d13b8d9fced8870415e81a96c90b99","venue_1":"ACM Multimedia","year":"2014","title":"Improving Content-based and Hybrid Music Recommendation using Deep Learning","authors":"Xinxi Wang, Ye Wang","author_ids":"2348299, 1681196","abstract":"Existing content-based music recommendation systems typically employ a \\textit{two-stage} approach. They first extract traditional audio content features such as Mel-frequency cepstral coefficients and then predict user preferences. However, these traditional features, originally not created for music recommendation, cannot capture all relevant information in the audio and thus put a cap on recommendation performance. Using a novel model based on deep belief network and probabilistic graphical model, we unify the two stages into an automated process that simultaneously learns features from audio content and makes personalized recommendations. Compared with existing deep learning based models, our model outperforms them in both the warm-start and cold-start stages without relying on collaborative filtering (CF). We then present an efficient hybrid method to seamlessly integrate the automatically learnt features and CF. Our hybrid method not only significantly improves the performance of CF but also outperforms the traditional feature mbased hybrid method.","cites":"14","conferencePercentile":"92.97188755"},{"venue":"ACM Multimedia","id":"2a4bbee0b4cf52d5aadbbc662164f7efba89566c","venue_1":"ACM Multimedia","year":"2014","title":"Pedestrian Attribute Recognition At Far Distance","authors":"Yubin Deng, Ping Luo, Chen Change Loy, Xiaoou Tang","author_ids":"2584434, 1693209, 1717179, 1741901","abstract":"The capability of recognizing pedestrian attributes, such as gender and clothing style, at far distance, is of practical interest in far-view surveillance scenarios where face and body close-shots are hardly available. We make two contributions in this paper. First, we release a new pedestrian attribute dataset, which is by far the largest and most diverse of its kind. We show that the large-scale dataset facilitates the learning of robust attribute detectors with good generalization performance. Second, we present the benchmark performance by SVM-based method and propose an alternative approach that exploits context of neighboring pedestrian images for improved attribute inference.","cites":"20","conferencePercentile":"95.98393574"},{"venue":"ACM Multimedia","id":"5539371e431915b31b051de9be7ff82da856ee4f","venue_1":"ACM Multimedia","year":"2008","title":"Attention-driven action retrieval with DTW-based 3d descriptor matching","authors":"Rongrong Ji, Xiaoshuai Sun, Hongxun Yao, Pengfei Xu, Tianqiang Liu, Xianming Liu","author_ids":"1725599, 1759841, 1720100, 8334787, 2414948, 8659574","abstract":"From visual perception viewpoint, actions in videos can capture high-level semantics for video content understanding and retrieval. However, action-level video retrieval meets great challenges, due to the interferences from global motions or concurrent actions, and the difficulties in robust action describing and matching. This paper presents a content-based action retrieval framework to enable effective search of near-duplicated actions in large-scale video database. Firstly, we present an attention shift model to distill and partition human-concerned saliency actions from global motions and concurrent actions. Secondly, to characterize each saliency action, we extract 3D-SIFT descriptor within its spatial-temporal region, which is robust against rotation, scale, and view point variances. Finally, action similarity is measured using Dynamic Time Warping (DTW) distance to offer tolerance for action duration variance and partial motion missing. Search efficiency in large-scale dataset is achieved by hierarchical descriptor indexing and approximate nearest-neighbor search. In validation, we present a prototype system VILAR to facilitate action search within \"Friends\" soap operas with excellent accuracy, efficiency, and human perception revealing ability.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"021c4d56a9253f3ac4b87ff065d1939dbc79a256","venue_1":"ACM Multimedia","year":"2007","title":"Dual cross-media relevance model for image annotation","authors":"Jing Liu, Bin Wang, Mingjing Li, Zhiwei Li, Wei-Ying Ma, Hanqing Lu, Songde Ma","author_ids":"5661757, 1726439, 8392859, 2902955, 1705244, 1694235, 1711796","abstract":"Image annotation has been an active research topic in recent years due to its potential impact on both image understanding and web image retrieval. Existing relevance-model-based methods perform image annotation by maximizing the joint probability of images and words, which is calculated by the expectation over training images. However, the semantic gap and the dependence on training data restrict their performance and scalability. In this paper, a dual cross-media relevance model (DCMRM) is proposed for automatic image annotation, which estimates the joint probability by the expectation over words in a pre-defined lexicon. DCMRM involves two kinds of critical relations in image annotation. One is the word-to-image relation and the other is the word-to-word relation. Both relations can be estimated by using search techniques on the web data as well as available training data. Experiments conducted on the Corel dataset and a web image dataset demonstrate the effectiveness of the proposed model.","cites":"44","conferencePercentile":"92.96875"},{"venue":"ACM Multimedia","id":"6ef2d918ec284b1c525d53d62fc9f92b93e41c26","venue_1":"ACM Multimedia","year":"2009","title":"What is a complete set of keywords for image description & annotation on the web","authors":"Xianming Liu, Hongxun Yao, Rongrong Ji, Pengfei Xu, Xiaoshuai Sun","author_ids":"8659574, 1720100, 1725599, 8334787, 1759841","abstract":"Does there exist a compact set of keywords that can completely and effectively cover the image annotation problem by expanding from it? In this paper, we answer this question by presenting a complete set framework for image annotation, which is motivated by the existence of semantic ontology. To generate this set, we propose a cross model optimization strategy from both textual and visual information for topic decomposition, based on a so-called Bipartite LSA model, which minimize multimodal error energy functions in a probabilistic Latent Semantic Analysis model. To achieve complete set based annotation, we present a Gaussian-Kernel-Generative process based keyword generation procedure, which analogizes keyword annotation in a probabilistic generative manner. A group of experiments is performed on Washington University image database and 80,000 Flickr images with comparisons to the state-of-the-arts. Finally, potential advantages and future improvements of our framework are discussed outside the scope of topic modeling.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"86f998342d159a8958dd8e2ed5ba4fa1e9a9c50c","venue_1":"ACM Multimedia","year":"2011","title":"Learning heterogeneous data for hierarchical web video classification","authors":"Xianming Liu, Hongxun Yao, Rongrong Ji, Pengfei Xu, Xiaoshuai Sun, Qi Tian","author_ids":"8659574, 1720100, 1725599, 8334787, 1759841, 1724745","abstract":"Web videos such as YouTube are hard to obtain sufficient precisely labeled training data and analyze due to the complex ontology. To deal with these problems, we present a hierarchical web video classification framework by learning heterogeneous web data, and construct a bottom-up semantic forest of video concepts by learning from meta-data. The main contributions are two-folds: firstly, analysis about middle-level concepts' distribution is taken based on data collected from web communities, and a concepts redistribution assumption is made to build effective transfer learning algorithm. Furthermore, an AdaBoost-Like transfer learning algorithm is proposed to transfer the knowledge learned from Flickr images to YouTube video domain and thus it facilitates video classification. Secondly, a group of hierarchical taxonomies named Semantic Forest are mined from YouTube and Flickr tags which reflect better user intention on the semantic level. A bottom-up semantic integration is also constructed with the help of semantic forest, in order to analyze video content hierarchically in a novel perspective. A group of experiments are performed on the dataset collected from Flickr and YouTube. Compared with state-of-the-arts, the proposed framework is more robust and tolerant to web noise.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"9345ec6249695f084f587fa4e519307beee33956","venue_1":"ACM Multimedia","year":"2011","title":"Augmented makeover based on 3D morphable model","authors":"Patricia Peng Wang, Xiaofeng Tong, Yangzhou Du, Jianguo Li, Wei Hu, Yimin Zhang","author_ids":"2349189, 1807645, 3165307, 1758942, 7004409, 1707720","abstract":"Avatar is the virtual representation of user's facial, body, and motion characteristics in computer game, social network, and augmented reality. Facial modeling needs enormous efforts to achieve immersive experience in applications like avatar chatting or online makeover. Great challenge exists in robust detection of 2D facial prominent points and mapping them to 3D models in a parameterized manner. Another challenge is how to characterize semantic components of eyes, mouth, nose, and cheek rather than low level mesh geometries. In this paper, we proposed an augmented makeover framework to deal with aforementioned challenges. Aiming to provide amateurs with flexible customizations, morphable model is constructed from a set of scanned 3D face data set. Appearance personalization is carried out in the offline phase where single image and multiple views are discussed respectively to generate deformative shape in a progressive manner. Augmentation is implemented in the online phase where a fast and robust 3D tracking is used to balance the tradeoff between accuracy and real-time requirements. By this means, immersive Human Computer Interaction such as virtual makeover and photo-realistic avatar chatting could be achieved.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"27c64de4abf0a162366761f38b59961e888f7ec0","venue_1":"ACM Multimedia","year":"2016","title":"Binary Optimized Hashing","authors":"Qi Dai, Jianguo Li, Jingdong Wang, Yu-Gang Jiang","author_ids":"8678020, 1758942, 1688516, 1717861","abstract":"This paper studies the problem of learning to hash, which is essentially a mixed integer optimization problem, containing both the binary hash code output and the (continuous) parameters forming the hash functions. Different from existing relaxation methods in hashing, which have no theoretical guarantees for the error bound of the relaxations, we propose binary optimized hashing (BOH), in which we prove that if the loss function is Lipschitz continuous, the binary optimization problem can be relaxed to a bound-constrained continuous optimization problem. Then we introduce a surrogate objective function, which only depends on unbinarized hash functions and does not need the slack variables transforming unbinarized hash functions to discrete functions, to approximate the relaxed objective function. We show that the approximation error is bounded and the bound is small when the problem is optimized. We apply the proposed approach to learn hash codes from either handcraft feature inputs or raw image inputs. Extensive experiments are carried out on three benchmarks, demonstrating that our approach outperforms state-of-the-arts with a significant margin on search accuracies.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"c9efcd8e32dced6efa2bba64789df8d0a8e4996a","venue_1":"ACM Multimedia","year":"2016","title":"Deep Convolutional Neural Network with Independent Softmax for Large Scale Face Recognition","authors":"Yue Wu, Jun Li, Yu Kong, Yun Fu","author_ids":"7136811, 1681426, 1679053, 1708679","abstract":"In this paper, we present our solution to the MS-Celeb-1M Challenge. This challenge aims to recognize 100k celebrities at the same time. The huge number of celebrities is the bottleneck for training a deep convolutional neural network of which the output is equal to the number of celebrities. To solve this problem, an independent softmax model is proposed to split the single classifier into several small classifiers. Meanwhile, the training data are split into several partitions. This decomposes the large scale training procedure into several medium training procedures which can be solved separately. Besides, a large model is also trained and a simple strategy is introduced to merge the two models. Extensive experiments on the MSR-Celeb-1M dataset demonstrate the superiority of the proposed method. Our solution ranks the first and second in two tracks of the final evaluation.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"ea8a8aa7e77e6e918d07b1aa3297dad384138969","venue_1":"ACM Multimedia","year":"2016","title":"Super Resolution of the Partial Pixelated Images With Deep Convolutional Neural Network","authors":"Haiyi Mao, Yue Wu, Jun Li, Yun Fu","author_ids":"3493678, 7136811, 1681426, 1708679","abstract":"The problem of super resolution of partial pixelated images is considered in this paper. Partial pixelated images are more and more common in nowadays due to public safety etc. However, in some special cases, for instance criminal investigation, some images are pixelated intentionally by criminals and partial pixelate make it hard to reconstruct images even a higher resolution images. Hence, a method is proposed to handle this problem based on the deep convolutional neural network, termed depixelate super resolution CNN(DSRCNN). Given the mathematical expression pixelates, we propose a model to reconstruct the image from the pixelation and map to a higher resolution by combining the adversarial autoencoder with two depixelate layers. This model is evaluated on standard public datasets in which images are pixelated randomly and compared to the state of arts methods, shows very exciting performance.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"9da63f089b8ee23120bfa8b4d9d9c8f605f421fc","venue_1":"ACM Multimedia","year":"2011","title":"Video indexing and recommendation based on affective analysis of viewers","authors":"Sicheng Zhao, Hongxun Yao, Xiaoshuai Sun, Pengfei Xu, Xianming Liu, Rongrong Ji","author_ids":"1755487, 1720100, 1759841, 8334787, 8659574, 1725599","abstract":"Most previous works on video indexing and recommendation were only based on the content of video itself, without considering the affective analysis of viewers, which is an efficient and important way to reflect viewers' attitudes, feelings and evaluations of videos. In this paper, we propose a novel method to index and recommend videos based on affective analysis, mainly on facial expression recognition of viewers. We first build a facial expression recognition classifier by embedding the process of building compositional Haar-like features into hidden conditional random fields (HCRFs). Then we extract viewers' facial expressions frame by frame through the videos, collected from the camera when viewers are watching videos, to obtain the affections of viewers. Finally, we draw the affective curve which tells the process of affection changes. Through the curve, we segment each video into affective sections, give the indexing result of the videos, and list recommendation points from views' aspect. Experiments on our collected database from the web show that the proposed method has a promising performance.","cites":"7","conferencePercentile":"74.9271137"},{"venue":"ACM Multimedia","id":"4f28df467220f629c538579f58017b2baa84a04b","venue_1":"ACM Multimedia","year":"2011","title":"Unsupervised fast anomaly detection in crowds","authors":"Xiaoshuai Sun, Hongxun Yao, Rongrong Ji, Xianming Liu, Pengfei Xu","author_ids":"1759841, 1720100, 1725599, 8659574, 8334787","abstract":"In this paper, we proposed a fast and robust unsupervised framework for anomaly detection and localization in crowed scenes. Our method avoids modeling the normal state of the crowds which is a very complex task due to the large within class variance of the normal target appearance and motion patterns. For each video frame, we extract the spatial temporal features of 3D blocks and generate the saliency map using a block-based center-surround difference operator. Then, motion vector matrix is obtained by adaptive rood pattern search block-matching algorithm and distance normalization. Attractive motion disorder descriptor is proposed to measure the global intensity of anomalies in the scene. Finally, we classify the frames into normal and anomalous ones by a binary classifier. In the experiments, we compared our method against several state-of-the-art approaches on UCSD dataset which is a widely used anomaly detection and localization benchmark. As the only unsupervised approach, our method outputs competitive results with near real-time processing speed","cites":"10","conferencePercentile":"82.94460641"},{"venue":"ACM Multimedia","id":"a897c55d5de82eb47c59796d5f6945843e69bfbd","venue_1":"ACM Multimedia","year":"1999","title":"A digital video authoring and publishing system designed for the Internet","authors":"Ronald Baecker","author_ids":"1715312","abstract":"We shall demonstrate a new implementation of the Movie Authoring and Design (MAD) system, originally reported at Multimedia'96 (Baecker, Rosenthal, et al. 1996). The new version, called CineKi? \" , has been created using Java and exploits the capabilities of QuickTime 4.","cites":"0","conferencePercentile":"7.983193277"},{"venue":"ACM Multimedia","id":"aa7046a0f10bfe0b002886702741e6f4de3d484c","venue_1":"ACM Multimedia","year":"2008","title":"Object fingerprints for content analysis with applications to street landmark localization","authors":"Wen Wu, Jie Yang","author_ids":"5780816, 1688428","abstract":"An object can be a basic unit for multimedia content analysis. Besides similarity among common objects, each object has its own unique characteristics which we cannot find in other surrounding objects in multimedia data. We call such unique characteristics object fingerprints. In this paper, we propose a novel approach to extract and match object fingerprints for multimedia content analysis. In particular, we focus on the problem of street landmark localization from images. Instead of modeling and matching a street landmark as a whole, our proposed approach extracts the landmark's object fingerprints in a given image and match to a new image or video in order to localize the landmark. We formulate matching the landmark's object fingerprints as a classification problem solved by a cascade of 1NN classifiers. We develop a street landmark localization system that combines salient region detection, segmentation, and object fingerprint extraction techniques for the purpose. To evaluate, we have compiled a novel dataset which consists of 15 U.S. street landmarks' images and videos. Our experiments on this dataset show superior performance to state-of-the-art recognition algorithms [20, 33]. The proposed approach can also be well generalized to other objects of interest and content analysis tasks. We demonstrate the feasibility through the application of our approach to refine web image search results and obtained encouraging results.","cites":"8","conferencePercentile":"61.69724771"},{"venue":"ACM Multimedia","id":"966ea7be1616057124cc09423d6f16962bc81f38","venue_1":"ACM Multimedia","year":"2012","title":"Scalable similar image search by joint indices","authors":"Jing Wang, Jingdong Wang, Xian-Sheng Hua, Shipeng Li","author_ids":"1697912, 1688516, 1746102, 4973820","abstract":"Text-based image search is able to return desired images for simple queries, but has limited capabilities in finding images with additional visual requirements. As a result, an image is usually used to help describe the appearance requirements. In this demonstration, we show a similar image search system that can support the joint textual and visual query. We present an efficient and effective indexing algorithm, neighborhood graph index, which is suitable for millions of images, and use it to organize joint inverted indices to search over billions of images.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"4cc5fb6cf48b2c58b283460b19f3beeb7e5b6a22","venue_1":"ACM Multimedia","year":"2013","title":"Clickage: towards bridging semantic and intent gaps via mining click logs of search engines","authors":"Xian-Sheng Hua, Linjun Yang, Jingdong Wang, Jing Wang, Ming Ye, Kuansan Wang, Yong Rui, Jin Li","author_ids":"1746102, 7866194, 1688516, 1697912, 3717648, 3163131, 1728806, 1695320","abstract":"The semantic gap between low-level visual features and high-level semantics has been investigated for decades but still remains a big challenge in multimedia. When \"search\" became one of the most frequently used applications, \"intent gap\", the gap between query expressions and users' search intents, emerged. Researchers have been focusing on three approaches to bridge the semantic and intent gaps: 1) developing more representative features, 2) exploiting better learning approaches or statistical models to represent the semantics, and 3) collecting more training data with better quality. However, it remains a challenge to close the gaps. In this paper, we argue that the massive amount of click data from commercial search engines provides a data set that is unique in the bridging of the semantic and intent gap. Search engines generate millions of click data (a.k.a. image-query pairs), which provide almost \"unlimited\" yet strong connections between semantics and images, as well as connections between users' intents and queries. To study the intrinsic properties of click data and to investigate how to effectively leverage this huge amount of data to bridge semantic and intent gap is a promising direction to advance multimedia research. In the past, the primary obstacle is that there is no such dataset available to the public research community. This changes as Microsoft has released a new large-scale real-world image click data to public. This paper presents preliminary studies on the power of large-scale click data with a variety of experiments, such as building large-scale concept detectors, tag processing, search, definitive tag detection, intent analysis, etc., with the goal to inspire deeper researches based on this dataset.","cites":"30","conferencePercentile":"97.77777778"},{"venue":"ACM Multimedia","id":"baf78a2e717d1f274a5f43f8420e8d9de3efd41c","venue_1":"ACM Multimedia","year":"2006","title":"SmartLabel: an object labeling tool using iterated harmonic energy minimization","authors":"Wen Wu, Jie Yang","author_ids":"5780816, 1688428","abstract":"Labeling objects in images is an essential prerequisite for many visual learning and recognition applications that depend on training data, such as image retrieval, object detection and recognition. Manually creating labels in images is not only time-consuming but also subject to human labeling errors, and eventually, becomes impossible for a large scale image database. Semi-supervised learning (SSL)algorithms such as Gaussian random field (GRF)can be applied to labeling objects in images since they have the ability to include a large amount of unlabeled data while requiring only a small amount of labeled data. However, the one-shot property of GRF prevents it from achieving good labeling performance. In this paper, we presents a novel object labeling tool, SmartLabel, to semi-automatically label objects in images. The algorithm of SmartLabel has four innovations over GRF:1)soft labeling,2)graph construction with spatial constraints, 3)iterated harmonic energy minimization, and 4)using relevance feedback to incorporate human interaction in the loop. As demonstrated in datasets of six object categories, the proposed SmartLabel not only works effectively even with a very small amount of user input (e.g., 1 .5%of image size)but also achieves significant improvement over GRF.","cites":"8","conferencePercentile":"62.95336788"},{"venue":"ACM Multimedia","id":"0ca46aa31bea9554713a54e034c065e494902f29","venue_1":"ACM Multimedia","year":"2015","title":"What Makes New York So Noisy?: Reasoning Noise Pollution by Mining Multimodal Geo-Social Big Data","authors":"Hsun-Ping Hsieh, Tzu-Chi Yen, Cheng-Te Li","author_ids":"2670863, 2446109, 2169355","abstract":"Noise pollution in modern cities is getting worse and sound sensors are sparse and costly, but it is highly demanded to have a system that can help reason and present the noise pollution at any region in urban areas. In this work, we leverage multimodal geo-social media data on Foursquare, Twitter, Flickr, and Gowalla in New York City, to infer and visualize the volume and the composition of noise pollution for every region in NYC. Using NYC 311 noise complaint records as the approximation of noise pollution for validation, we develop a joint inference and visualization system that integrates multimodal features, including geographical, mobility, visual, and social, with a graph-based learning model to infer the noise compositions of regions. Experimental results show that our model can achieve promising results with substantially few training data, compared to state-of-the-art methods. A NYC Urban Noise Diagnotor system is developed and allowed users to understand the noise composition of any region of NYC in an interactive manner.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"86a56313023e35460e7a18f95a108ee7884f2179","venue_1":"ACM Multimedia","year":"2009","title":"Interactive 3D caricature generation based on double sampling","authors":"Jinjing Xie, Yiqiang Chen, Junfa Liu, Chunyan Miao, Xingyu Gao","author_ids":"1735452, 4070304, 5485341, 1679209, 1732399","abstract":"Recently, 3D caricature generation and applications have attracted wide attention from both the research community and the entertainment industry. This paper proposes a novel interactive approach for various and interesting 3D caricature generation based on double sampling. Firstly, according to user's operation, we obtain a coarse 3D caricature with local features transformation by sampling in well-built principle component analysis (PCA) subspace. Secondly, to utilize information of the 2D caricature dataset, we sample in the local linear embedding (LLE) manifold subspace. Finally, we use the learned 2D caricature information to further refine the coarse caricature by applying Kriging interpolation. The experiments show that the 3D caricature generated by our method can preserve highly artistic styles and also reflect the user's intention.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"016bc42a8e957198e6e8d36a9d1b468c0abf9127","venue_1":"ACM Multimedia","year":"2007","title":"Beyond \"beyond being there\": towards multiscale communication systems","authors":"Nicolas Roussel, Sofiane Gueddana","author_ids":"1728921, 2005234","abstract":"Forty years after AT&amp;T's Picturephone, video is still mainly considered as a way to enhance audio communication in an attempt to reproduce face-to-face conditions. In a 1992 paper, Hollan and Stornetta argued that we should develop communication tools that go <i>beyond being there</i>. In this paper, we discuss two different interpretations of their analysis. We then propose the concept of <i>multiscale communication</i> as an alternative approach for motivating telecommunication research, an approach that aims at creating systems that support a variable degree of engagement, smooth transitions between degrees and integration with other media. Finally, we present three video systems from which the multiscale communication concept emerged and that partly illustrate it.","cites":"2","conferencePercentile":"28.125"},{"venue":"ACM Multimedia","id":"dc11456bdb2a58bce7969e1638b8962547d0d420","venue_1":"ACM Multimedia","year":"2015","title":"Harnessing Big Personal Data, with Scrutable User Modelling for Privacy and Control","authors":"Judy Kay","author_ids":"1736168","abstract":"My work aims to enable people to harness, control and manage their big personal data. This is challenging because people are generating vast, and growing, collections of personal data. That data is captured by a rich personal digital ecosystems of devices, some worn or carried, and others are fixed or embedded in the environment. Users explicitly store some data but systems also capture the user's digital footprints, ranging from simple clicks and touches, to images, audio and video. This personal data resides in a quite bewildering range of places, from personal devices to cloud stores, in multitudes of silos.\n Big personal data differs from the scientific big data in important ways. Because it is personal, it should be handled in ways that enable people to ensure it is managed and used as they wish. It may be of modest size compared with scientific big data, but people consider their data stores as big, because they are complex and hard to manage. A driving goal for my research has been to tackle the challenges of big personal data by creating infrastructures, representations and interfaces that enable a user to scrutinize and control their personal data in a scrutable user model.\n One important role for users models is personalisation, where the user model is a dynamic set of evidence-based beliefs about the user. This is the foundation for personalization, ranging from recommenders to teaching systems. User models may represent anything from the user's attributes to their knowledge, beliefs, goals, plans and preferences.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"d7db00b47c423422fb448b7ba6046a716c54eba4","venue_1":"ACM Multimedia","year":"1994","title":"No Multimedia Without Representation (Panel)","authors":"Marc Davis, Catherine Baudin, Smadar Kedar, Daniel M. Russell","author_ids":"1777964, 2775472, 2016284, 2006133","abstract":"Without represen(a[icsn multimedia will not happen. Computers tire basically deaf, blind, and ignorant, and will remain so for quite some time. Only by creating representations for media con[ent will we be able to construct large, robust multimedia sys[ems that support content-based manipulation of video, audio, and images. Yet the research community that deals with representation (A1 researchers) and the community that deals with media manipulation (multimedia researchers) have, for the most part. not come inro contact. The purpose of this panel is to incite debate about the role of representation of multimedia con-tenl by bringing these two communities together. Our intention IS to confront rese~rchers at ACM Multimedia'94 with a variety of positions al-rout the need for content representation in large-scalc multimedia systems We argue that work in knowledge reprcscn[ation IS essential to the construction of robust mul-timedia systcrns t{>manage media content. Mt)s(contemporary work In multimedia systems suffers from the Icgucy of traditional approaches to media production and usc Video has largely been used in \" single-use \" applications (Hollywood movies) as opposed to \" multiple reuse \" applications (video archives and stock footage houses). Because of this his-(or]cul bent. multimedia systems have not addressed the needs o! hui Iding up representations which can enable media content I{) he reused by people other than those who originally created Ihc conteo[ Ior purposes other than those for which the content was originally produced. The construction of large scale media w~hlvc~. which can become common resources in a global digital nc(work. demands that wc solve problems of representational design for sharing and reusing media content. Unfortunately, approaches to content representation which rely solely on signal-based parsing are inadequate to the task of representing mmii:) w that it can he effectively browsed, searched, retrieved, and reused What is needed is a hybrid approach of human and computational indexing of media content designed with sharable md reusable representation in mind. Without such content represcntatlon, multimedia will remain a toy application stuck on CD-ROMS and \" titles. \" In order for multimedia to become a medium for everyday communication we must develop a Ianguagc for the representation of multimedia content. This language reqoires that indexers be able to articulate a common substrate (~1'domain independent media representation that can hc shared so as to facilitate domain dependent reuse. The task of accessing the informtition in large repositories of multimedia documents and of building hypermedia networks relies …","cites":"0","conferencePercentile":"12.71186441"},{"venue":"ACM Multimedia","id":"1e6f09bc162bd94c3580331ddb7f766213ab9eb4","venue_1":"ACM Multimedia","year":"1994","title":"Supporting Real-Time Multimedia Behaviour in Open Distributed Systems: An Approach Based on Synchronous Languages","authors":"Gordon S. Blair, Michael Papathomas, Geoff Coulson, Philippe Robin, Laurent Hazard, Jean-Bernard Stefani, François Horn","author_ids":"1700173, 2428574, 1696978, 4349605, 2726373, 1715837, 1790921","abstract":"There is currently considerable interest in developing multimedia applications in open distributed systems. However, it is now becoming clear that existing architectures for open distributed systems do not support the particular requirements of continuous media types such as digital audio and video. This is particularly the case in the important areas of quality of service support and real-time synchronization. This paper presents results from the Sumo project which aims at supporting continuous media types within the framework defined by the draft Open Distributed Processing standard. The paper advocates the use of synchronous languages within this framework for specifying and implementing real-time synchronization and QoS monitoring. A computational model and the realization of an infrastructure supporting this view are presented.","cites":"4","conferencePercentile":"36.44067797"},{"venue":"ACM Multimedia","id":"001d23014c70dfca758b716d9c5b431ce26b4edd","venue_1":"ACM Multimedia","year":"2016","title":"Multi-Protocol Video Delivery with Late Trans-Muxing","authors":"Rufael Mekuria, Jelte Fennema, Dirk Griffioen","author_ids":"2086342, 3493609, 3493341","abstract":"In practice, video delivery using multiple protocols is needed to support the wide range of clients in the digital media ecosystem. Naive approaches increase backhaul traffic and cache storage requirements proportional to the number of protocols in use. To reduce this overhead, we present an efficient multi-protocol video delivery architecture. It exploits the fact that media segments in different protocols are often based on the same raw encoded media data. Instead of caching and distributing near duplicate media segments for each protocol throughout the content delivery network, we push their generation towards the edge. We call this Late Trans-Muxing (LTM). We implement LTM in a smart edge cache that can request and cache media byte ranges and generate protocol specific media segments on the fly. Experiments in an emulation testbed show that backhaul traffic and cache storage are reduced up to 85% and 50% respectively compared to CDN based approaches. This shows the benefit of LTM in multi-protocol DASH based video delivery.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"a584ec6df21024bee63bba296a648a6fc93f27ec","venue_1":"ACM Multimedia","year":"2012","title":"Mobile multimedia presentation in self-forming mobile device groups: ad-hoc networks in practice","authors":"Kevin Collins, Noel E. O'Connor, Gabriel-Miro Muntean","author_ids":"4532423, 1730463, 1742225","abstract":"This demo exhibits a new application of mobile ad-hoc networks, where, a group of mobile devices are connected to allow synchronized presentation of multimedia content. The demo is in the form of an interactive tour. Participants have a mobile device and the tour is led by a guide, who takes the group on an informative tour of a locale. The tour is augmented with the presentation of multimedia content on the devices, highlighting points of interest. Content presentation is controlled by the guide and is synchronized using the ad-hoc network. This is an edutainment application but the underlying technology could be applied elsewhere including educational settings and in entertainment.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"92741838ac46d8d2cbd0414b5b15395692a92a14","venue_1":"ACM Multimedia","year":"2007","title":"MusicSense: contextual music recommendation using emotional allocation modeling","authors":"Rui Cai, Chao Zhang, Chong Wang, Lei Zhang, Wei-Ying Ma","author_ids":"6306010, 3671118, 1782199, 1724298, 1705244","abstract":"In this paper, we present a novel contextual music recommendation approach, <i>MusicSense</i>, to automatically suggest music when users read Web documents such as Weblogs. <i>MusicSense</i> matches music to a document's content, in terms of the emotions expressed by both the document and the music songs. To achieve this, we propose a generative model - <i>Emotional Allocation Modeling</i> - in which a collection of word terms is considered as generated with a mixture of emotions. This model also integrates knowledge discovering from a Web-scale corpus and guidance from psychological studies of emotion. Music songs are also described using textual information extracted from their meta-data and relevant Web pages. Thus, both music songs and Web documents can be characterized as distributions over the emotion mixtures through the emotional allocation modeling. For a given document, the songs with the most matched emotion distributions are finally selected as the recommendations. Preliminary experiments on Weblogs show promising results on both emotion allocation and music recommendation.","cites":"38","conferencePercentile":"90.10416667"},{"venue":"ACM Multimedia","id":"53f6971b11b1b3d88dc60229705123d9941d0aa9","venue_1":"ACM Multimedia","year":"2006","title":"VirtualTour: an online travel assistant based on high quality images","authors":"Feng Jing, Lei Zhang, Wei-Ying Ma","author_ids":"7188215, 1724298, 1705244","abstract":"With the popularity of both travel and Web, more and more people use online travel services to facilitate their travel activities or share their travel experiences. Considering that existing services emphasize more on the textual content with the pictorial content only as supplement, we propose the VirtualTour system. It is an online travel service dedicated on high quality images, which helps travelers plan their trip. The images of VirtualTour are from photo forum sites. They have rich and accurate metadata which could be used to extract geographic location information of them and assess the quality of them. A representative sights identification algorithm is also proposed to automatically identify the possible related sights of a region. Based on the map services that seamlessly integrated into the system, a wieldy UI is designed to support several useful features, e.g. query by map, location or path.","cites":"17","conferencePercentile":"80.31088083"},{"venue":"ACM Multimedia","id":"210373ba3448dd277dced41880abd78263cacbd1","venue_1":"ACM Multimedia","year":"1998","title":"An Intelligent Media Browser Using Automatic Multimodal Analysis","authors":"Jonathan Foote, John S. Boreczky, Andreas Girgensohn, Lynn Wilcox","author_ids":"1797460, 2719487, 2195286, 1691319","abstract":"Many techniques can extract information from an multimedia stream, such as speaker identity or shot boundaries. We present a browser that uses this information to navigate through stored media. Because automatically-derived information is not wholly reliable, it is transformed into a time-dependent \" confidence score. \" When presented graphically, confidence scores enable users to make informed decisions about regions of interest in the media, so that non-interesting areas may be skipped. Additionally, index points may be determined automatically for easy navigation, selection, editing, and annotation and will support analysis types other than the speaker identification and shot detection used here.","cites":"29","conferencePercentile":"65.38461538"},{"venue":"ACM Multimedia","id":"9ca8c870bb00553984e774c36d2a55557c61b6ee","venue_1":"ACM Multimedia","year":"2006","title":"IGroup: web image search results clustering","authors":"Feng Jing, Changhu Wang, Yuhuan Yao, Kefeng Deng, Lei Zhang, Wei-Ying Ma","author_ids":"7188215, 1697065, 2922552, 1796862, 1724298, 1705244","abstract":"In this paper, we propose, IGroup, an efficient and effective algorithm that organizes Web image search results into clusters. IGroup is different from all existing Web image search results clustering algorithms that only cluster the top few images using visual or textual features. Our proposed algorithm first identifies several query-related semantic clusters based on a key phrases extraction algorithm originally proposed for clustering general Web search results. Then, all the resulting images are separated and assigned to corresponding clusters. As a result, all the resulting images are organized into a clustering structure with semantic level. To make the best use of the clustering results, a new user interface (UI) is proposed. Different from existing Web image search interfaces, which show only a limited number of suggested query terms or representative image thumbnails of some clusters, the proposed interface displays both representative thumbnails and appropriate titles of semantically coherent image clusters. Comprehensive user studies have been completed to evaluate both the clustering algorithm and the new UI.","cites":"51","conferencePercentile":"93.78238342"},{"venue":"ACM Multimedia","id":"8a4e8b78e9c16813e6c8f860cd051d24dbf7e145","venue_1":"ACM Multimedia","year":"2008","title":"What did you do today?: discovering daily routines from large-scale mobile data","authors":"Katayoun Farrahi, Daniel Gatica-Perez","author_ids":"3284542, 1698682","abstract":"We present a framework built from two Hierarchical Bayesian topic models to discover human location-driven routines from mobile phones. The framework uses location-driven bag representations of people's daily activities obtained from celltower connections. Using 68,000+ hours of real-life human data from the Reality Mining dataset, we successfully discover various types of routines. The first studied model, Latent Dirichlet Allocation (LDA), automatically discovers characteristic routines for all individuals in the study, including \"going to work at 10am\", \"leaving work at night\", or \"staying home for the entire evening\". In contrast, the second methodology with the Author Topic model (ATM) finds routines characteristic of a selected groups of users, such as \"being at home in the mornings and evenings while being out in the afternoon\", and ranks users by their probability of conforming to certain daily routines.","cites":"57","conferencePercentile":"96.33027523"},{"venue":"ACM Multimedia","id":"0b1a181f8b00b142ed110cb34daa2353dee3db2a","venue_1":"ACM Multimedia","year":"2006","title":"Scalable relevance feedback using click-through data for web image retrieval","authors":"En Cheng, Feng Jing, Lei Zhang, Hai Jin","author_ids":"1742005, 7188215, 1724298, 7949774","abstract":"Relevance feedback (RF) has been extensively studied in the content-based image retrieval community. However, no commercial Web image search engines support RF because of scalability, efficiency and effectiveness issues. In this paper we proposed a scalable relevance feedback mechanism using click-through data for web image retrieval. The proposed mechanism regards users' click-through data as implicit feedback which could be collected at lower cost, in larger quantities and without extra burden on the user. During RF process, both textual feature and visual feature are used in a sequential way. To seamlessly combine textual feature-based RF and visual feature-based RF, a query concept-dependent fusion strategy is automatically learned. Experimental results on a database consisting of nearly three million Web images show that the proposed mechanism is wieldy, scalable and effective.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"3d376aaa239b67d4a27646f65e428f27cbca1006","venue_1":"ACM Multimedia","year":"2006","title":"EnjoyPhoto: a vertical image search engine for enjoying high-quality photos","authors":"Lei Zhang, Le Chen, Feng Jing, Kefeng Deng, Wei-Ying Ma","author_ids":"1724298, 7628457, 7188215, 1796862, 1705244","abstract":"In this paper, we propose building a vertical image search engine called EnjoyPhoto that leverages rich metadata from various photo forum web sites to meet users' requirements for enjoying high-quality photos, which is virtually impossible in traditional image search engines. To solve the ranking problem when aggregating multiple photo forums, we propose a novel rank fusion algorithm that uses duplicate photos to normalize rating scores. To further improve user experiences in enjoying photos, we design an in-place image browsing interface, and compare it with several other interfaces in a user study. With rich metadata and rating information, more attractive user interfaces are enabled, including slideshow authoring and photo recommendations. We conducted experiments and user studies on a 2.5-million image database to evaluate the proposed rank fusion algorithm, investigate the rationale behind building a vertical image search engine, and study user interfaces and preferences for the purpose of enjoying high-quality photos. The experimental results demonstrate the effectiveness of the proposed ranking algorithm. The results also show that the 2.5-million high-quality image database in EnjoyPhoto performs comparably with Google's 1- billion image database for queries related to location, nature, and daily life categories. Finally, our results show that the in-place browsing interface-called Force-Transfer view-is much more convenient for users than traditional interfaces.","cites":"18","conferencePercentile":"82.38341969"},{"venue":"ACM Multimedia","id":"c0bfacb4d16f7d2aebb725ea252bebd56420b8e4","venue_1":"ACM Multimedia","year":"2005","title":"Multimodal affect recognition in learning environments","authors":"Ashish Kapoor, Rosalind W. Picard","author_ids":"7665582, 1719389","abstract":"We propose a multi-sensor affect recognition system and evaluate it on the challenging task of classifying interest (or disinterest) in children trying to solve an educational puzzle on the computer. The multimodal sensory information from facial expressions and postural shifts of the learner is combined with information about the learner's activity on the computer. We propose a unified approach, based on a mixture of Gaussian Processes, for achieving sensor fusion under the problematic conditions of missing channels and noisy labels. This approach generates separate class labels corresponding to each individual modality. The final classification is based upon a hidden random variable, which probabilistically combines the sensors. The multimodal Gaussian Process approach achieves accuracy of over 86%, significantly outperforming classification using the individual modalities, and several other combination schemes.","cites":"120","conferencePercentile":"99.00990099"},{"venue":"ACM Multimedia","id":"0d45baf91f2ec75ff82f6e443ad29e565e6e7e3b","venue_1":"ACM Multimedia","year":"2008","title":"Efficiently matching sets of features with random histograms","authors":"Wei Dong, Zhe Wang, Moses Charikar, Kai Li","author_ids":"4751916, 1748832, 1745732, 2359779","abstract":"As the commonly used representation of a feature-rich data object has evolved from a single feature vector to a set of feature vectors, a key challenge in building a content-based search engine for feature-rich data is to match feature-sets efficiently. Although substantial progress has been made during the past few years, existing approaches are still inefficient and inflexible for building a search engine for massive datasets. This paper presents a randomized algorithm to embed a set of features into a single high-dimensional vector to simplify the feature-set matching problem. The main idea is to project feature vectors into an auxiliary space using locality sensitive hashing and to represent a set of features as a histogram in the auxiliary space. A histogram is simply a high dimensional vector, and efficient similarity measures like L<sub>1</sub> and L<sub>2</sub> distances can be employed to approximate feature-set distance measures.\n We evaluated the proposed approach under three different task settings, i.e. content-based image search, image object recognition and near-duplicate video clip detection. The experimental results show that the proposed approach is indeed effective and flexible. It can achieve accuracy comparable to the feature-set matching methods, while requiring significantly less space and time. For object recognition with Caltech 101 dataset, our method runs 25 times faster to achieve the same precision as Pyramid Matching Kernel, the state-of-the-art feature-set matching method.","cites":"29","conferencePercentile":"89.90825688"},{"venue":"ACM Multimedia","id":"152bd0fbb9caf5cd2e612df02662f4180ac3376a","venue_1":"ACM Multimedia","year":"2010","title":"Assisted news reading with automated illustration","authors":"Diogo Delgado, João Magalhães, Nuno Correia","author_ids":"2435501, 1678306, 1717306","abstract":"We all had the problem of forgetting about what we just read a few sentences before. This comes from the problem of attention and is more common with children and elderly. People feel either bored or distracted by something more interesting. This paper proposes an application to help people reading news by illustrating the news story. The application provides mechanisms to (1) select the best illustration for each scene and (2) to select the set of illustrations to improve the story sequence. The application proposed in this technical demo aims at improving the user's attention when reading news articles. The application implements several information processing techniques to generate an audio-visual presentation of the text news article.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"952db89a56168439627a04289d5222b6e17c2f67","venue_1":"ACM Multimedia","year":"2010","title":"RTiVISS: real-time video interactive systems for sustainability","authors":"Mónica Mendes, Nuno Correia","author_ids":"2859556, 1717306","abstract":"RTiVISS is an exploratory project that proposes to investigate innovative concepts and design methods regarding environmental and sustainability issues. It is concerned with natural resources, specially forests, and their preservation, through critical research and experimental artistic approaches. The project proposes multiplatform access to real-time networked&#160;video and allows users to adopt selected forests under surveillance. The interactive system feeds a whole community that establishes connections by sharing \"the emotion of real-time\" and the challenge of uncertainty, while remotely monitoring natural environments for forests protection. This achievement enables artistic explorations with digital media in interactive installations that engage the audience senses in unconventional ways.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"cfbfba4f746cf4a02967f011a979abc8c7ddc740","venue_1":"ACM Multimedia","year":"1999","title":"Active video watching using annotation","authors":"Nuno Correia, Teresa Chambel","author_ids":"1717306, 1679943","abstract":"nmc@di.fct.unl.pt This paper describes the principles and a model for adding content and structure to existing video materials, based on annotation. Annotations in printed media promote active reading and, in a similar way, annotations in video promote active watching. The principles and model are illustrated by a prototype system for video annotation and browsing, named AntV (Annotations in Video).","cites":"19","conferencePercentile":"68.48739496"},{"venue":"ACM Multimedia","id":"f8e7734a1be4dbaf4a8de11f273534138f730783","venue_1":"ACM Multimedia","year":"2002","title":"Annotations as multiple perspectives of video content","authors":"Miguel Costa, Nuno Correia, Nuno Guimarães","author_ids":"1787103, 1717306, 5808712","abstract":"This paper describes a video annotation tool based on a new and flexible model, that gives several perspectives over the same video content. The model was designed in a way that allows having multiple views over the same video data, enabling users with different requirements to have the most appropriate interface. These views, video-lenses, highlight a specific aspect of the video content that is being annotated. Annotations are made using a timeline based interface with multiple tracks, where each track corresponds to a given video-lens. The format used to store and exchange the information is the MPEG-7 standard. The annotation tool (VAnnotator) is being developed in the scope of Vizard, an ambitious project that aims to define a new paradigm for video navigation, annotation, editing and retrieval. The Vizard project includes users, both from the production/archiving area and from the consumer electronics area, that help to define and validate the annotation requirements and functionality.","cites":"16","conferencePercentile":"68.8034188"},{"venue":"ACM Multimedia","id":"4f8057ba01b91a755d2d198f4f0382c21ff5a6c8","venue_1":"ACM Multimedia","year":"2008","title":"MobiToss: a novel gesture based interface for creating and sharing mobile multimedia art on large public displays","authors":"Jürgen Scheible, Timo Ojala, Paul Coulton","author_ids":"2150981, 7967584, 1693223","abstract":"This paper presents MobiToss, a novel application for creating and sharing mobile multimedia art with an off-the-shelf mobile phone equipped with built-in accelerometer sensors allowing gesture control. The user first takes a photo or captures a video with the phone and then using a 'throwing' gesture transfers the clip onto a large public display for instant viewing and manipulation by tilting the phone in different directions. The system augments the user-created clip with other items such as music or brand labels and the encoded clip is automatically sent back to the phone as a personal artefact of the event. The clip is also uploaded to a dedicated community website for sharing the created multimedia art with others. MobiToss could be deployed e.g. in clubs, pubs and concerts as a participatory VJ-tool. In addition to the design the paper presents the results of a preliminary user evaluation, which highlight the novel art experience provided by MobiToss.","cites":"21","conferencePercentile":"84.40366972"},{"venue":"ACM Multimedia","id":"58b996fb0aff8e61de39d7105a7da7d243739950","venue_1":"ACM Multimedia","year":"2010","title":"Sketch-based 3D model retrieval using diffusion tensor fields of suggestive contours","authors":"Sang Min Yoon, Maximilian Scherer, Tobias Schreck, Arjan Kuijper","author_ids":"1758071, 1741561, 1706471, 1738151","abstract":"The number of available 3D models in various areas increase steadily. Effective methods to search for those 3D models by content, rather than textual annotations, are crucial. For this purpose, we propose a new approach for content based 3D model retrieval by hand-drawn sketch images. This approach to retrieve visually similar mesh models from a large database consists of three major steps: (1) suggestive contour renderings from different viewpoints to compare against the user drawn sketches; (2) descriptor computation by analyzing diffusion tensor fields of suggestive contour images or the query sketch respectively; (3) similarity measurement to retrieve the models and the most probable view-point from which a model was sketched. Our proposed sketch based 3D model retrieval system is very robust against variations of shape, pose or partial occlusion of the user draw sketches. Experimental results are presented and indicate the effectiveness of our approach for sketch-based 3D mode retrieval.","cites":"30","conferencePercentile":"91.78082192"},{"venue":"ACM Multimedia","id":"3ac01c2fb2744767fdaf6fd9f5b35ad4f359934e","venue_1":"ACM Multimedia","year":"2006","title":"User authorship and creativity within interactivity","authors":"Karl D. D. Willis","author_ids":"2269914","abstract":"This paper tracks the development of the author's work entitled <i>Light Tracer</i>, and examines the surrounding issues of user authorship and creativity within interactivity.<i>Light Tracer</i> is an interactive system which invites the participant to write, draw and trace images in real physical space. The participant is situated in front of a screen reflecting their own image, and by manipulating a series of light sources, marks can be left onscreen such as drawings, messages, traces of physical objects such as faces, hands and bodies.It is the argument of the author that by allowing the user an optimum level of creative authorship within an interactive work, the user can be successfully engaged with the experience of the interaction and in turn produce and create themselves.","cites":"7","conferencePercentile":"58.29015544"},{"venue":"ACM Multimedia","id":"254285e6d3876f8301a0967209ee69973163093f","venue_1":"ACM Multimedia","year":"1993","title":"Optimization of the Grouped Sweeping Scheduling (GSS) with Heterogeneous Multimedia Streams","authors":"Mon-Song Chen, Dilip D. Kandlur, Philip S. Yu","author_ids":"3097465, 1702213, 1703117","abstract":"Grouped Sweeping Scheduling was proposed in [1] as a general formulation of a class of disk arm scheduling schemes. This class includes, for example, the fixed-order and SCAN scheduling schemes. An optimum design was presented for the homogeneous case, i.e., when all multi-media streams have the same characteristics. In this paper we examine the more general situation in which a mixture of different format multimedia streams, with different characteristics , coexist on the disk. The emphasis is to present and prove a simple procedure for optimizing GSS in this heterogeneous situation. Finally, we discuss how GSS can be used in dynamic settings to support heterogeneous request streams.","cites":"88","conferencePercentile":"81.39534884"},{"venue":"ACM Multimedia","id":"f810044e7396960b0da5d253b21cdf4632001745","venue_1":"ACM Multimedia","year":"2016","title":"PlaylistCreator: An Assisted Approach for Playlist Creation","authors":"Ricardo Dias, Daniel Gonçalves, Manuel J. Fonseca","author_ids":"5085530, 1751841, 1774352","abstract":"In this demo paper we describe PlaylistCreator, an assisted approach for supporting the creation of music playlists. Our solution allows creators to express song selection and browsing through a visual representation of their intents in a unified view, which relies on a set-based model for representing sources of songs. Creators can convey their purposes by seamlessly combining criteria for song selection from either manual or automatic sources, such as artists and albums, or similarity measures between artists or songs.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2a0ab0d439a72085234fee0137afeaecf9b33be3","venue_1":"ACM Multimedia","year":"2007","title":"SBIA: search-based image annotation by leveraging web-scale images","authors":"Xirong Li, Xin-Jing Wang, Changhu Wang, Lei Zhang","author_ids":"7137848, 3349534, 1697065, 1724298","abstract":"In this technical demonstration, we showcase the SBIA system - a search-based image annotation system. At the heart of the system lies a very large-scale image search engine which indexed three million Web images and supports both text and visual queries. Given an image (with initial annotations), SBIA first finds semantically/visually similar images via the search engine, and then mines representative keywords from the retrieved images. These keywords, after annotation rejection and relevance ranking, are finally used to annotate the query image.","cites":"4","conferencePercentile":"40.36458333"},{"venue":"ACM Multimedia","id":"51f58c420728833befd725d0309f013920b63269","venue_1":"ACM Multimedia","year":"2009","title":"A slide-ware application to support discursive presentations","authors":"Ryan P. Spicer, Yu-Ru Lin, Aisling Kelliher, Hari Sundaram","author_ids":"2977476, 2133006, 1820838, 8607462","abstract":"Transdisciplinary collaborations call for dynamic, responsive slide-ware presentations beyond the linear structure afforded by traditional tools. The NextSlidePlease application addresses this through a novel authoring and presentation interface. The application also features an innovative algorithm to enhance presentation time management. The cross-platform Java application is currently being evaluated in a variety of real-world presentation contexts.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"73944042356f44098f678af5e1d471cdb0ce38fd","venue_1":"ACM Multimedia","year":"2000","title":"New enhancements to cut, fade, and dissolve detection processes in video segmentation","authors":"Ba Tu Truong, Chitra Dorai, Svetha Venkatesh","author_ids":"2955704, 1719234, 1679520","abstract":"We present improved algorithms for cut, fade, and dissolve detection which are fundamental steps in digital video analysis. In particular, we propose a new adaptive threshold determination method that is shown to reduce artifacts created by noise and motion in scene cut detection. We also describe new two-step algorithms for fade and dissolve detection, and introduce a method for eliminating false positives from a list of detected candidate transitions. In our detailed study of these gradual shot transitions, our objective has been to accurately classify the type of transitions (fade-in, fade-out, and dissolve) and to precisely locate the boundary of the transitions. This distinguishes our work from other early work in scene change detection which tends to focus primarily on identifying the existence of a transition rather than its precise temporal extent. We evaluate our improved algorithms against two other commonly used shot detection techniques on a comprehensive data set, and demonstrate the improved performance due to our enhancements.","cites":"63","conferencePercentile":"94.56521739"},{"venue":"ACM Multimedia","id":"043682a2479d389b35cd74a1e1f7d45855f4a6c3","venue_1":"ACM Multimedia","year":"2001","title":"Affect computing in film through sound energy dynamics","authors":"Simon Moncrieff, Chitra Dorai, Svetha Venkatesh","author_ids":"2951873, 1719234, 1679520","abstract":"We develop an algorithm for the detection and classification of <i>affective</i> sound events underscored by specific patterns of sound energy dynamics. We relate the portrayal of these events to proposed high level <i>affect</i> or emotional coloring of the events. In this paper, four possible characteristic sound energy events are identified that convey well established meanings through their dynamics to portray and deliver certain affect, sentiment related to the horror film genre. Our algorithm is developed with the ultimate aim of automatically structuring sections of films that contain distinct shades of emotion related to horror themes for nonlinear media access and navigation. An average of 82% of the energy events, obtained from the analysis of the audio tracks of sections of four sample films corresponded correctly to the proposed affect. While the discrimination between certain sound energy event types was low, the algotithm correctly detected 71% of the occurrences of the sound energy events within audio tracks of the films analyzed, and thus forms a useful basis for determining affective scenes characteristic of horror in movies.","cites":"18","conferencePercentile":"76.53061224"},{"venue":"ACM Multimedia","id":"2d6a09cf251c7c7d8c500aa6c3451fba0f30d8b0","venue_1":"ACM Multimedia","year":"2002","title":"High level segmentation of instructional videos based on content density","authors":"Dinh Q. Phung, Svetha Venkatesh, Chitra Dorai","author_ids":"1749657, 1679520, 1719234","abstract":"Automatically partitioning instructional videos into topic sections is a challenging problem in e-learning environments for efficient content management and cataloging. This paper addresses this problem by proposing a novel density function to delineate sections underscored by changes in topics in instructional and training videos. The content density function draws guidance from the observation that topic boundaries coincide with the ebb and flow of the 'density' of content shown in these videos. Based on this function, we propose two methods for high-level segmentation by determining topic boundaries. We study the performance of the two methods on eight training videos, and our experimental results demonstrate the effectiveness and robustness of the two proposed high-level segmentation algorithms for learning media.","cites":"14","conferencePercentile":"62.39316239"},{"venue":"ACM Multimedia","id":"7a547353febf10f1e6fa2d7d2904bd476ed5383f","venue_1":"ACM Multimedia","year":"2002","title":"Application of computational media aesthetics methodology to extracting color semantics in film","authors":"Ba Tu Truong, Svetha Venkatesh, Chitra Dorai","author_ids":"2955704, 1679520, 1719234","abstract":"Using film grammar as the underpinning, we study the extraction of structures in video based on color using a wide configuration of clustering methods combined with existing and new similarity measures. We study the visualisation of these structures, which we call <i>Scene-Cluster Temporal Charts</i> and show how it can bring out the interweaving of different themes and settings in a film. We also extract color events that filmmakers use to draw/force a viewer's attention to a shot/scene. This is done by first extracting a set of colors used rarely in film, and then building a probabilistic model for color event detection. We demonstrate with experimental results from ten movies that our algorithms are effective in the extraction of both scene-cluster temporal charts and color events.","cites":"6","conferencePercentile":"37.17948718"},{"venue":"ACM Multimedia","id":"46cfa0e7832b1aba24479f1ffc185b4a2f10ca35","venue_1":"ACM Multimedia","year":"2009","title":"TravelScope: standing on the shoulders of dedicated travelers","authors":"Qiang Hao, Rui Cai, Jiang-Ming Yang, Rong Xiao, Like Liu, Shuo Wang, Lei Zhang","author_ids":"7825219, 6306010, 7788742, 2820959, 2150984, 5994097, 1724298","abstract":"In this paper, we propose a system called TravelScope that helps users experience virtual tours by presenting information mined from user-generated travelogues and photos. The system can (1) recommend popular places for a given region; (2) characterize comprehensive aspects (e.g., landmarks, styles, activities) for a location; and (3) show representative images for a landmark. A novel user interface is designed to provide a better user experience, by organizing both the textual and visual information generated by dedicated travelers in an attractive way.","cites":"6","conferencePercentile":"52.27272727"},{"venue":"ACM Multimedia","id":"1171fed0f024a51f7c4e9ecf7b1d4e0376191d71","venue_1":"ACM Multimedia","year":"2009","title":"Multimedia content analysis: model-based approaches vs. data-driven approaches","authors":"Lei Zhang, Qi Tian","author_ids":"1724298, 1724745","abstract":"The explosive growth of multimedia data on the Web has had a great impact on research, and opened a potentially controversial topic in the multimedia community about model-based approaches and data-driven approaches. In this panel, we expect intelligent discussions between the panelists and the audience on this topic. For example, which approach is more advantageous: model-based or data-driven? How can these two approaches learn from the latest developments of the other to further improve existing limitations? The debate in this panel and the insight shared by the panelists are highly anticipated and expected to inspire and drive researchers to answer these questions and push them into working on other, related problems.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"f5eac9d1f65f72b6d7296cfd04378c95f6796f7d","venue_1":"ACM Multimedia","year":"2009","title":"Argo: intelligent advertising made possible from users' photos","authors":"Xin-Jing Wang, Mo Yu, Lei Zhang, Wei-Ying Ma","author_ids":"3349534, 2482533, 1724298, 1705244","abstract":"Though monetizing user-generated photos has a great potential in image business, this topic is seldom touched due to the difficulties of both image understanding and ads-to-images vocabulary matching. In this technical demonstration, we show case the Argo system, which attempts to monetize UGC (user-generated content) photos by mining a user's interest from a group of his photos and advertising the photos accordingly. Given a page of photos, it first auto-tags each photo by a large-scale search-based image annotation method, then maps both image annotations and the textual descriptions of ads onto an ODP-based topic hierarchy. The mapping produces semantic features which are statistical distributions on ODP topics. Ads are ranked by their similarities to such topic distributions of the photos and the top-ranked ones are output.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"dc245547c3e3867e38fc86c7d92f006864731a7e","venue_1":"ACM Multimedia","year":"2003","title":"Hierarchical topical segmentation in instructional films based on cinematic expressive functions","authors":"Dinh Q. Phung, Svetha Venkatesh, Chitra Dorai","author_ids":"1749657, 1679520, 1719234","abstract":"In this paper, we propose a novel solution for segmenting an instructional video into hierarchical topical sections. Incorporating the knowledge of education-oriented film theory with our previous study of expressive functions namely the content density and the thematic functions, we develop an algorithm to effectively structuralize an instructional video into a two-tiered hierarchy of topical sections at the main and sub-topic levels. Our experimental results on a set of ten industrial instructional videos demonstrate the validity of the detection scheme.","cites":"6","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"8ee328292e9ba7011bd5c6e807659948ff8d0ca0","venue_1":"ACM Multimedia","year":"1996","title":"A JPEG Codec Adaptive to Region Importance","authors":"Jiying Zhao, Yoshihisa Shimazu, Koji Ohta, Rina Hayasaka, Yutaka Matsushita","author_ids":"1728718, 3143787, 3057247, 2483426, 1712012","abstract":"— This paper presents a novel standard base-line JPEG compatible still image compression method that allows to flexibly control the decompression quality of regions in image. Therefore, it allows both higher quality of important parts and lower bitrate as a whole, and it reflects the sense of human beings. We use fuzzy technique to determine important regions in the same manner as human beings do, allocate higher bitrate to important regions while lower to the rest parts of image. The paper introduces the important region determination , description, arbitrarily shaped region filling, importance adaptive image compression, decompression, and finally gives experimental results.","cites":"6","conferencePercentile":"11.11111111"},{"venue":"ACM Multimedia","id":"63f34db5c1de014ca138507d48c21cff76415e6c","venue_1":"ACM Multimedia","year":"2005","title":"Topic transition detection using hierarchical hidden Markov and semi-Markov models","authors":"Dinh Q. Phung, Thi V. Duong, Svetha Venkatesh, Hung Hai Bui","author_ids":"1749657, 2171721, 1679520, 1707085","abstract":"In this paper we introduce a probabilistic framework to exploit hierarchy, structure sharing and duration information for topic transition detection in videos. Our probabilistic detection framework is a combination of a shot classification step and a detection phase using hierarchical probabilistic models. We consider two models in this paper: the extended Hierarchical Hidden Markov Model (HHMM) and the Coxian Switching Hidden semi-Markov Model (S-HSMM) because they allow the natural decomposition of semantics in videos, including shared structures, to be modeled directly, and thus enable efficient inference and reduce the sample complexity in learning. Additionally, the S-HSMM allows the duration information to be incorporated, consequently the modeling of long-term dependencies in videos is enriched through both hierarchical and duration modeling. Furthermore, the use of Coxian distribution in the S-HSMM makes it tractable to deal with long sequences in video. Our experimentation of the proposed framework on twelve educational and training videos shows that both models outperform the baseline cases (flat HMM and HSMM) and performances reported in earlier work in topic detection. The superior performance of the S-HSMM over the HHMM verifies our belief that the duration information is an important factor in video content modeling.","cites":"11","conferencePercentile":"60.89108911"},{"venue":"ACM Multimedia","id":"bf91d62c2898915515ad33e8cd90853016df3fc0","venue_1":"ACM Multimedia","year":"2007","title":"Privacy and the access of information in a smart house environment","authors":"Simon Moncrieff, Svetha Venkatesh, Geoff A. W. West","author_ids":"2951873, 1679520, 7858062","abstract":"In this paper we present a framework for addressing privacy issues raised by the monitoring of assisted living smart house environments. In home environments, the conflict between the goals of the surveillance, and the private nature of the home, raises the issue of occupant privacy. This issue needs to be addressed if applications are to be accepted by the occupant. We identify four key properties required for the design of privacy sensitive ubiquitous computing applications. Subsequently, we develop a <i>dynamic</i> and <i>flexible</i> method for implementing privacy measures through controlling access to data, and an interface to provide <i>feedback</i> to the occupant, enabling them to <i>control</i> the implemented privacy measures. We form a generic framework for implementing privacy sensitive ubiquitous computing applications based on previous applications within the field. This framework was then extended and used to develop a specific framework for a privacy sensitive smart house. The approach proposed in the framework dynamically applies privacy measures to multi-modal data according to the situation, or context, of the environment. We further test an implementation of the privacy measures, and detail methods to implement feedback and control. The approach aims to decrease the invasiveness of the surveillance, while retaining the purpose of the assisted living environment.","cites":"10","conferencePercentile":"66.92708333"},{"venue":"ACM Multimedia","id":"519ee74f39b620466d4171afdb8d392c62b9d61d","venue_1":"ACM Multimedia","year":"2007","title":"Distributed query processing for mobile surveillance","authors":"Stewart Greenhill, Svetha Venkatesh","author_ids":"2965462, 1679520","abstract":"Addressing core issues in mobile surveillance, we present an architecture for querying and retrieving distributed, semi-permanent multi-modal data through challenged networks with limited connectivity. The system provides a rich set of queries for spatio-temporal querying in a surveillance context, and uses the network availability to provide best quality of service. It incrementally and adaptively refines the query, using data already retrieved that exists on static platforms and on-demand data that it requests from mobile platforms. We demonstrate the system using a real surveillance system on a mobile 20 bus transport network coupled with static bus depot infrastructure. In addition, we show the robustness of the system in handling different conditions in the underlying infrastructure by running simulations on a real, but historic dataset collected in an offline manner.","cites":"10","conferencePercentile":"66.92708333"},{"venue":"ACM Multimedia","id":"608720d62fc216234856a3d73bc371f70c525e52","venue_1":"ACM Multimedia","year":"2008","title":"Contextual navigation in a multimedia journal","authors":"Stewart Greenhill, Svetha Venkatesh","author_ids":"2965462, 1679520","abstract":"This work presents a framework for multimedia journaling, maintaining strong relationships between the document and embedded media. This enables media archives that are robust to changes in software environments, such as changes in web-sharing services, proprietary file formats and enables portability across operating system. We develop a journaling application using an existing multimedia framework, and show the power of the paradigm with specific case studies.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"26760e2b046a476d18e2b5a6284e47c2e1bb8a11","venue_1":"ACM Multimedia","year":"2008","title":"Delivering online advertisements inside images","authors":"Zhiwei Li, Lei Zhang, Wei-Ying Ma","author_ids":"2902955, 1724298, 1705244","abstract":"We present in this paper a new channel to deliver online advertisements along with Web images and show a new business model to monetize billions of Web images. The idea is intuitively inspired by image displaying processes on the Web, which typically require people to wait a few seconds before they see full resolution images. This is due to large file sizes and limited network bandwidth. To utilize idle time and the display area, we propose an innovative method for non-intrusively embedding ads into images in a visually pleasant manner. To maintain a smooth user experience, we utilize the thumbnail of the full-resolution image because it is small and visually similar to the full-resolution image. At the client side, a rendering engine first enlarges and blurs the thumbnail, and then blends the pre-chosen ads information into the enlarged image. Based on this idea, we propose three typical scenarios that can adopt the proposed image-advertising mode. More importantly, we can encourage providers of images or other users to participate in our online image ads service by tagging or annotating images. We envision revenue sharing with the providers participating in our service, and we expect that a large number of users will actively submit, tag and annotate images using the system. We have implemented a prototype image ads system, and conducted a series of experiments and user studies to evaluate such a new advertisement channel. The experimental results and user studies show that the proposed online image ad delivery is a non-intrusive ads mode, and the proposed solution is practical. This work also opens multiple new research directions ranging from multimedia to web data mining","cites":"5","conferencePercentile":"48.85321101"},{"venue":"ACM Multimedia","id":"af3d893af8a94c1f569d0f553bc7ab171159cba7","venue_1":"ACM Multimedia","year":"2007","title":"Scalable music recommendation by search","authors":"Rui Cai, Chao Zhang, Lei Zhang, Wei-Ying Ma","author_ids":"6306010, 3671118, 1724298, 1705244","abstract":"The growth of music resources on personal devices and Internet radio has increased the need for music recommendations. In this paper, aiming at providing an efficient and general solution, we present a search-based solution for scalable music recommendations. In this solution a music piece is first transformed to a music signature sequence in which each signature characterizes the timbre of a local music clip. Based on such signatures, a scale-sensitive method is then proposed to index the music pieces for similarity search, using the locality sensitive hashing (LSH). The scale-sensitive method can numerically find the appropriate parameters for indexing various scales of music collections, and thus can guarantee a proper number of nearest neighbors are found in search. In the recommendation stage, representative signatures from snippets of a seed piece are extracted as query terms, to retrieve pieces with similar melodies for suggestions. We also design a relevance-ranking function to sort the search results, based on the criteria that include <i>matching ratio, temporal order, term weight</i>, and <i>matching confidence</i>. Finally, with the search results, we propose a strategy to generate a dynamic playlist which can automatically expand with time. Evaluations of several music collections at various scales showed that our approach achieves encouraging results in terms of recommendation satisfaction and system scalability.","cites":"22","conferencePercentile":"81.51041667"},{"venue":"ACM Multimedia","id":"f6ed5992a17fc9830e79a8837d7d6b0263e82ba4","venue_1":"ACM Multimedia","year":"2008","title":"Audio Puzzler: piecing together time-stamped speech transcripts with a puzzle game","authors":"Nicholas Diakopoulos, Kurt Luther, Irfan A. Essa","author_ids":"2943892, 2427623, 1714295","abstract":"We have developed an audio-based casual puzzle game which produces a time-stamped transcription of spoken audio as a by-product of play. Our evaluation of the game indicates that it is both fun and challenging. The transcripts generated using the game are more accurate than those produced using a standard automatic transcription system and the time-stamps of words are within several hundred milliseconds of ground truth.","cites":"7","conferencePercentile":"58.48623853"},{"venue":"ACM Multimedia","id":"45c86a7718a6ed2ff17948eca090cfa060ed38bb","venue_1":"ACM Multimedia","year":"2008","title":"mTable: browsing photos and videos on a tabletop system","authors":"Patrick Chiu, Jeffrey Huang, Maribeth Back, Nicholas Diakopoulos, John Doherty, Wolfgang Polak, Xiaohua Sun","author_ids":"2895008, 6086666, 2131565, 2943892, 3881379, 2423252, 3237699","abstract":"In this video demo, we present mTable, a multimedia tabletop system for browsing photo and video collections. We have developed a set of applications for visualizing and exploring photos, a board game for labeling photos, and a 3D cityscape metaphor for browsing videos. The system is suitable for use in a living room or office lounge, and can support multiple displays by visualizing the collections on the tabletop and showing full-size images and videos on another flat panel display in the room.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"217478d6a95a5bceef11d7846895b57718d63e73","venue_1":"ACM Multimedia","year":"2001","title":"Automatic detection of 'Goal' segments in basketball videos","authors":"Surya Nepal, Uma Srinivasan, Graham J. Reynolds","author_ids":"1681657, 2569997, 1754678","abstract":"Advances in the media and entertainment industries, for example streaming audio and digital TV, present new challenges for managing large audio-visual collections. Efficient and effective retrieval from large content collections forms an important component of the business models for content holders and this is driving a need for research in audio-visual search and retrieval. Current content management systems support retrieval using low-level features, such as motion, colour, texture, beat and loudness. However, low-level features often have little meaning for the human users of these systems, who much prefer to identify content using high-level semantic descriptions or concepts. This creates a gap between the system and the user that must be bridged for these systems to be used effectively. The research presented in this paper describes our approach to bridging this gap in a specific content domain, sports video. Our approach is based on a number of automatic techniques for feature detection used in combination with heuristic rules determined through manual observations of sports footage. This has led to a set of models for interesting sporting events-goal segments-that have been implemented as part of an information retrieval system. The paper also presents results comparing output of the system against manually identified goals.","cites":"78","conferencePercentile":"97.95918367"},{"venue":"ACM Multimedia","id":"bfcc9a543b0785a5c6fff57bcf907c3f245773df","venue_1":"ACM Multimedia","year":"2003","title":"Making sense of video content","authors":"A. Viranga Ratnaike, Balasubramaniam Srinivasan, Surya Nepal","author_ids":"2045415, 2728392, 1681657","abstract":"Our aim in this research is to make sense of scenes in video. We expect this will enable us to identify different scenes sharing the same semantic, even if they do not share any multimedia cues. Our approach is based on emergence, and involves classification and reasoning. We use patterns of cues to synthesize semantic classifications. These classifications need to be consistent with the observed cues, and suggest other elements that might be present. The suggestions might be based on sets of related patterns, ontology, or other knowledge bases. With an idea of what we are looking for, we can re-examine the scenes for multimedia elements, which support our hypothesis, or at least are not inconsistent. We expect that sufficiently detailed semantic descriptions can be generated, by cycling through these steps.","cites":"0","conferencePercentile":"4.054054054"},{"venue":"ACM Multimedia","id":"4cba00262af070b52c2d488151302577661f2774","venue_1":"ACM Multimedia","year":"2006","title":"Virtual observers in a mobile surveillance system","authors":"Stewart Greenhill, Svetha Venkatesh","author_ids":"2965462, 1679520","abstract":"Conventional wide-area video surveillance systems use a network of fixed cameras positioned close to locations of interest. We describe an alternative and flexible approach to wide area surveillance based on observation streams collected from mobile cameras mounted on buses. We allow a \"virtual observer\" to be placed anywhere within the space covered by the sensor network, and reconstruct the scene at these arbitrary points. Use of such imagery is challenging because mobile cameras have variable position and orientation, and sample a large spatial area but at low temporal resolution. Additionally, the views of any particular place are distributed across many different video streams. Addressing this problem, we present a system in which views from an arbitrary perspective can be constructed by indexing, organising, and transforming images collected from multiple streams acquired from a network of mobile cameras. Our system supports retrieval of raw images based on constraints of space, time, and geometry (eg. visibility of landmarks). It also allows the synthesis of wide-angle panoramic views in situations where the camera motion produces suitable sampling of the scene and metaphors for query and presentation that overcome the complexity of the data.","cites":"11","conferencePercentile":"72.27979275"},{"venue":"ACM Multimedia","id":"3bc10fd4074787a6e9214988455d3b4d8f8c6708","venue_1":"ACM Multimedia","year":"2009","title":"NextSlidePlease: agile hyperpresentations","authors":"Ryan P. Spicer, Yu-Ru Lin, Aisling Kelliher","author_ids":"2977476, 2133006, 1820838","abstract":"In this video presentation, we introduce NextSlidePlease, a novel slide authoring and presentation application. The video begins with a dramatization illustrating the shortcomings of existing slide-ware tools identified through our prior research. We then describe our theoretical framework for addressing these identified problems and present a dramatization of the process by which our NextSlidePlease application can be used to overcome such issues in a business context. In addition, we illustrate the novel functional aspects of our application algorithm that enable effective time management and flexible presentations. Finally, we present promising results from two user studies.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"16f8cfbb5f6c6dfeb20b0d3432675471736d2c07","venue_1":"ACM Multimedia","year":"2002","title":"Leveraging non-relevant images to enhance image retrieval performance","authors":"T. V. Ashwin, Rahul Gupta, Sugata Ghosal","author_ids":"1827080, 4414000, 2754431","abstract":"Inherent subjectivity in user's perception of an image has motivated the use of relevance feedback (RF) in the image desigined output's retrieval process. RF techniques interactively determine the user's <i>query concept</i>, given the user's relevance judgments on a set of images. In this paper we propose a robust technique that utilizes non-relevant images to efficiently discover the relevant search region. A similarity metric, estimated using the relevant images is then used to rank and retrieve database images in the relevant region. The partitioning of the feature space is achieved by using a piecewise linear decision surface that separates the relevant and non-relevant images. Each of the hyperplanes constituting the decision surface is normal to the minimum distance vector from a non-relevant point to the convex hull of relevant points. Experimental results demonstrate significant improvement in retrieval performance for the small feedback size scenario over two well established RF algorithms.","cites":"1","conferencePercentile":"11.53846154"},{"venue":"ACM Multimedia","id":"329606a67954246d6e35ca66b0d4b8cd76f75db8","venue_1":"ACM Multimedia","year":"2006","title":"PartyPeer: a P2P massively multiplayer online game","authors":"Leslie S. Liu, Roger Zimmermann, Baoxuan Xiao, Jon Christen","author_ids":"2564698, 1790974, 3122037, 2796344","abstract":"Using peer-to-Peer (P2P) architectures for large scale <i>interactive</i> applications such as Massively Multiplayer Online Games (MMOG) is very challenging because of the difficulties to maintain a consistent game world in a distributed topology and exchange game state information in the P2P network without a central sever. In this demo proposal we present the innovative design and implementation of PartyPeer, an online social game which supports a massive number of users using our P2P based streaming network called ACTIVE+. We also discuss some of the implementation challenges when building this real-world P2P based game.","cites":"2","conferencePercentile":"29.79274611"},{"venue":"ACM Multimedia","id":"ed99e09a513a232a556ebc07d3cc8b81b1b0758c","venue_1":"ACM Multimedia","year":"2008","title":"Deadband-based offline-coding of haptic media","authors":"Julius Kammerl, Eckehard G. Steinbach","author_ids":"3328065, 7252930","abstract":"In this work, a novel perceptual coding approach for offline compression of haptic media is presented. Our scheme exploits the properties of human haptic perception and hides coding artifacts introduced by lossy compression below the human perception thresholds. We combine the concept of Just Noticeable Differences with predictive coding in order to achieve high coding efficiency without impairing user perception. In our experiments, we apply the proposed lossy compression scheme to haptic data that has been recorded during a telemanipulation session in a virtual environment. Our user studies reveal that our approach leads to strong data reduction up to 100:1 while preserving a high-quality haptic experience during playback of the compressed haptic data streams.","cites":"6","conferencePercentile":"53.89908257"},{"venue":"ACM Multimedia","id":"b534337da8e86539f3e14eedabad7d473fdf3bde","venue_1":"ACM Multimedia","year":"2010","title":"Qoe-based rate adaptation scheme selection for resource-constrained wireless video transmission","authors":"Srisakul Thakolsri, Wolfgang Kellerer, Eckehard G. Steinbach","author_ids":"2666205, 1749109, 7252930","abstract":"This paper proposes a Quality of Experience (QoE) based rate adaptation scheme selection approach for multi-user wireless video delivery. Transcoding and packet dropping are used as examples of rate adaptation schemes, and we investigate their impact on user perceived video quality. In the presence of constrained computation resources, the most suitable rate adaptation scheme is determined for each video stream such that the overall quality degradation is minimized. The proposed scheme selection approach is integrated with QoE-based resource allocation in presence of constrained transmission resources. Simulation results obtained from an emulated High Speed Downlink Packet Access (HSDPA) show that the QoE-based approach leads to significant improvements of user perceived quality compared to other approaches including a non-optimized HSDPA systems","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"b663cc3b4e64c6da69c057e5518eb19e5545207f","venue_1":"ACM Multimedia","year":"2010","title":"Error-resilient perceptual coding for networked haptic interaction","authors":"Fernanda Brandi, Julius Kammerl, Eckehard G. Steinbach","author_ids":"2550009, 3328065, 7252930","abstract":"The performance of haptic interaction across communication networks critically depends on the successful reconstruction of the bidirectionally transmitted haptic signals, and hence on the quality of the communication channel. We propose a novel error-resilient data reduction scheme for haptic communication which exploits known limits of human haptic perception. Particularly, we show that missing haptic information due to packet loss may strongly impair the user's experience during haptic interaction. We present and compare methods that eliminate the disturbing artifacts resulting out of packet loss. Our approach keeps the estimated impact of packet losses below human perception thresholds. A tree of possible cases (packets received or not received) and their respective occurrence probabilities is maintained at the sender side, and the system predicts unacceptable error cases to decide whether extra packets should be sent. We introduce different criteria that can be employed to trigger additional packets. In our experiments, we evaluate both the objective data reduction performance and the subjective system transparency by performing extensive tests using packet loss probability and round trip time as parameters. The proposed scheme shows excellent performances in terms of data reduction while sustaining good subjective ratings for a wide range of packet loss values and round trip times.","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"7568a060e91ef254ea19c36e2d83e36b0f609cf1","venue_1":"ACM Multimedia","year":"2011","title":"Perceptual coding of recorded telemanipulation sessions","authors":"Fernanda Brandi, Eckehard G. Steinbach","author_ids":"2550009, 7252930","abstract":"Efficient recording and replay of telemanipulation sessions require the compression of the occurring visual-haptic signals. To this end, we propose two distinct approaches which take into consideration known limitations of human visual and haptic perception. We compare both schemes to the state-of-the-art approaches and show that a substantial additional data reduction can be achieved while maintaining good overall haptic and visual performances.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"d4d282483b5ddff6072871ae48729bb9697f1e8f","venue_1":"ACM Multimedia","year":"2007","title":"Combining stroke-based and selection-based relevance feedback for content-based image retrieval","authors":"Jingyu Cui, Changshui Zhang","author_ids":"1680047, 1700883","abstract":"We propose a flexible interaction mechanism for CBIR by enabling relevance feedback inside images through drawling strokes. User's interest is obtained from an easy-to-use user interface, and fused seamlessly with traditional feedback information in a semi-supervised learning framework. Retrieval performance is boosted due to more precise description of the query concept. Region segmentation is also improved based on the collected strokes, and further enhances the retrieval precision. We implement our system Flexible Image Search Tool (<i>FIST</i>) based on the ideas above. Experiments on two real world data sets demonstrate the effectiveness of our approach.","cites":"6","conferencePercentile":"49.21875"},{"venue":"ACM Multimedia","id":"006268c93d5ed90f3f180209172b1c231727db7a","venue_1":"ACM Multimedia","year":"2013","title":"Lecture video segmentation by automatically analyzing the synchronized slides","authors":"Xiaoyin Che, Haojin Yang, Christoph Meinel","author_ids":"3358394, 1688587, 1708312","abstract":"In this paper we propose a solution which segments lecture video by analyzing its supplementary synchronized slides. The slides content derives automatically from OCR (Optical Character Recognition) process with an approximate accuracy of 90%. Then we partition the slides into different subtopics by examining their logical relevance. Since the slides are synchronized with the video stream, the subtopics of the slides indicate exactly the segments of the video. Our evaluation reveals that the average length of segments for each lecture is ranged from 5 to 15 minutes, and 45% segments achieved from test datasets are logically reasonable.","cites":"5","conferencePercentile":"72.66666667"},{"venue":"ACM Multimedia","id":"a825708c9f20cec5754809c0174f9c91a7abd6f1","venue_1":"ACM Multimedia","year":"1998","title":"Object-Oriented Retrieval Mechanism for Semistructured Image Collections","authors":"Guang-Ho Cha, Chin-Wan Chung","author_ids":"7873620, 1733783","abstract":"1. ~STMCT ~ls paper presents the object-oriented image retrieval mechanism ~vhlchprovides the content model, the indexing scheme, and tie query processing techniques m a whole. Three types of image content i.e., visual features, semantic features, and keywords, are define~ and they are represented using the object-oriented data model. Three types of index structures corresponding to the identied features are elaborated to facilitate the semch. The query processing techniques to process complex queries which use multiple types of indexes are also descriied Experiments have been carried out on large image collections to demonstrate the effectiveness of the proposed retrieval mechanism.","cites":"4","conferencePercentile":"14.42307692"},{"venue":"ACM Multimedia","id":"2d159b225e33a127d1cda541ca86800b8fd86b4a","venue_1":"ACM Multimedia","year":"2008","title":"On optimal scheduling for layered video streaming in heterogeneous peer-to-peer networks","authors":"Xin Xiao, Yuanchun Shi, Yuan Gao","author_ids":"7211018, 1732440, 1770050","abstract":"Layered video streaming in peer-to-peer networks has drawn great interests since not only it can accommodate a large number of users but also it handles heterogeneities of client networks. However, to our knowledge, there's still a lack of systematical study on the data scheduling (i.e. requesting and relaying data) for layered streaming, and previous works in this area just focus on maximizing the throughput and/or minimizing the packet delay. In this paper, firstly, according to the characteristics caused by layered coding, we propose the four objectives that should be achieved by data scheduling for layered streaming: high throughput, high layer delivery ratio, low useless packets ratio and low subscription jitter; then, we use a 3-stage scheduling approach to request missed blocks, where each stage has different scheduling objective but collaborate with each other. The min-cost network flow model, probability decision mechanism and multi-window remedy mechanism are employed in Free Stage, Decision Stage and Remedy Stage, respectively, to achieve the above four goals. Extensive experimental results indicate that our approach outperforms other schemes in both throughput and layer delivery ratio. Besides, the useless packets number and subscription jitters are kept low.","cites":"7","conferencePercentile":"58.48623853"},{"venue":"ACM Multimedia","id":"e1ef0a5cac93e43de662a6f0dd822982464ac0d3","venue_1":"ACM Multimedia","year":"2010","title":"3rd international workshop on affective interaction in natural environments (AFFINE)","authors":"Ginevra Castellano, Kostas Karpouzis, Jean-Claude Martin, Louis-Philippe Morency, Christopher E. Peters, Laurel D. Riek","author_ids":"2710715, 1715144, 1735094, 1767184, 2225789, 1707010","abstract":"The 3rd International Workshop on Affective Interaction in Natural Environments, AFFINE, follows a number of successful AFFINE workshops and events commencing in 2008.A key aim of AFFINE is the identification and investigation of significant open issues in real-time, affect-aware applications 'in the wild' and especially in embodied interaction, for example, with robots or virtual agents. AFFINE seeks to bring together researchers working on the real-time interpretation of user behaviour with those who are concerned with social robot and virtual agent interaction frameworks.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"5507060ad65205536d1361b93b45278955792707","venue_1":"ACM Multimedia","year":"2003","title":"Foreground object detection from videos containing complex background","authors":"Liyuan Li, Weimin Huang, Irene Y. H. Gu, Qi Tian","author_ids":"1746918, 1742173, 1761436, 1724745","abstract":"This paper proposes a novel method for detection and segmentation of foreground objects from a video which contains both stationary and moving background objects and undergoes both gradual and sudden \"once-off\" changes. A Bayes decision rule for classification of background and foreground from selected feature vectors is formulated. Under this rule, different types of background objects will be classified from foreground objects by choosing a proper feature vector. The stationary background object is described by the color feature, and the moving background object is represented by the color co-occurrence feature. Foreground objects are extracted by fusing the classification results from both stationary and moving pixels. Learning strategies for the gradual and sudden \"once-off\" background changes are proposed to adapt to various changes in background through the video. The convergence of the learning process is proved and a formula to select a proper learning rate is also derived. Experiments have shown promising results in extracting foreground objects from many complex backgrounds including wavering tree branches, flickering screens and water surfaces, moving escalators, opening and closing doors, switching lights and shadows of moving objects.","cites":"161","conferencePercentile":"97.2972973"},{"venue":"ACM Multimedia","id":"94f92c3ac53fd423780869c9be93dcf5657b19ad","venue_1":"ACM Multimedia","year":"2015","title":"MIL: Music Exploration and Visualization via Lyric and Image","authors":"Xixuan Wu, Yu Qiao, Xiaoou Tang","author_ids":"2499358, 1690077, 1741901","abstract":"In this paper, we introduce MIL: a music exploration prototype which integrates music (M), image (I), and lyrics (L), for efficiently visualizing and browsing music collections. MIL utilizes a novel structure, music semantic graph (MSG), to organize music collections in a hierarchical way by leveraging lyrics and acoustic cues of music. Each node of MSG corresponds to a music concept and is associated with a cluster of music tracks. MIL offers users a novel way to efficiently explore and scan music collections by using lyrics and image information. In addition, the proposed prototype supplies an easy-to-use interface to visualize MSG hierarchically. The user study shows that our prototype can effectively and efficiently help users to browse, search, and scan music collections.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"1ec7b3c10ee7f272b6301c42aa59b5006a79b7c5","venue_1":"ACM Multimedia","year":"2014","title":"Fusing Music and Video Modalities Using Multi-timescale Shared Representations","authors":"Bing Xu, Xiaogang Wang, Xiaoou Tang","author_ids":"8635361, 2868636, 1741901","abstract":"We propose a deep learning architecture to solve the problem of multimodal fusion of multi-timescale temporal data, using music and video parts extracted from Music Videos (MVs) in particular. We capture the correlations between music and video at multiple levels by learning shared feature representations with Deep Belief Networks (DBN). The shared representations combine information from multiple modalities for decision making tasks, and are used to evaluate matching degrees between modalities and to retrieve matched modalities using single or multiple modalities as input. Moreover, we propose a novel deep architecture to handle temporal data at multiple timescales. When processing long sequences with varying length, we propose to extract hierarchical shared representations by concatenating deep representations at different levels, and to perform decision fusion with a feed forward neural network, which takes input from predictions of local and global classifiers trained with shared representations at each level. The effectiveness of our method is demonstrated through MV classification and retrieval.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"3ac2f60427a3d5f4fa5196dca62bebfef91a946f","venue_1":"ACM Multimedia","year":"1993","title":"Where Were We: Making and Using Near-Synchronous, Pre-Narrative Video","authors":"Scott L. Minneman, Steve R. Harrison","author_ids":"2830501, 2418522","abstract":"\" Where Were We \" is a prototype system that exploits the capabilities of digital video to allow groups to include recordings of very recent events into real-time activities. The goals of the research are to see how playback-while-recording and random access capabilities might substantially alter the activity of group work, to discover what difficulties arise in using emerging digital multimedia hardware and software facilities for this kind of application, and to further explore how to base novel systems design on necessarily partial and fragmented observations of actual work activity. The implications of this work on systems architecture has been both to inform the design of digital multimedia tools and to suggest different factorings of computation, display, storage, and transmission. The prototype is now stable enough for use in exploratory settings, and we are poised to see how (and verify that) design groups can make use of these \" instant replay \" facilities in real working situations.","cites":"31","conferencePercentile":"65.11627907"},{"venue":"ACM Multimedia","id":"4ed4eb7c44f2d7e45d8df18b21bb3840320198d8","venue_1":"ACM Multimedia","year":"2013","title":"AdVisual: a visual-based advertising system","authors":"Chao Dong, Shifeng Chen, Xiaoou Tang","author_ids":"1804436, 2869725, 1741901","abstract":"In this work, we present a visual-based contextual advertising system, AdVisual. It is designed for content service providers to effectively select relevant ads for online videos. First, it will analyze each video and extract high level semantic visual information including specific objects, people, and significant scenes. Then ads highly related to the visual concepts are associated with the corresponding shots. As AdVisual is a user interaction system, it allows users to select favorite ads relevant to the video. By saliency detection, selected ads will be displayed as an overlay window embedded at the non-intrusive part of the shot.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"9bb790e607a7e99c77d43682dfd58db17a251020","venue_1":"ACM Multimedia","year":"2013","title":"Anchor concept graph distance for web image re-ranking","authors":"Shi Qiu, Xiaogang Wang, Xiaoou Tang","author_ids":"6321776, 2868636, 1741901","abstract":"Web image re-ranking aims to automatically refine the initial text-based image search results by employing visual information. A strong line of work in image re-ranking relies on building image graphs that requires computing distances between image pairs. In this paper, we present Anchor Concept Graph Distance (ACG Distance), a novel distance measure for image re-ranking. For a given textual query, an Anchor Concept Graph (ACG) is automatically learned from the initial text-based search results. The nodes of the ACG (i.e., anchor concepts) and their correlations well model the semantic structure of the images to be re-ranked. Images are projected to the anchor concepts. The projection vectors undergo a diffusion process over the ACG, and then are used to compute the ACG distance. The ACG distance reduces the semantic gap and better represents distances between images. Experiments on the MSRA-MM and INRIA datasets show that the ACG distance consistently outperforms existing distance measures and significantly improves start-of-the-art methods in image re-ranking.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"b868f7c8453acc72ac754713ad73732a007a2558","venue_1":"ACM Multimedia","year":"1999","title":"Multimedia, network protocols and users - bridging the gap","authors":"George Ghinea, Johnson P. Thomas, Robert S. Fish","author_ids":"1795412, 1701115, 1779667","abstract":"In this paper we present the case for using specifically configured protocol stacks geared towards human requirements in the delivery of distributed multimedia. We define <italic>Quality of Perception</italic> (QoP) as representing the user side of the more technical and traditional Quality of Service (QoS). QoP is a term which encompasses not only a user's satisfaction with the quality of multimedia presentations, but also his/her ability to analyse, synthesise and assimilate the informational content of multimedia displays. The <italic>Dynamically Reconfigurable Protocol Stacks</italic> (DRoPS) architecture supports low cost reconfiguration of individual protocol mechanisms in an attempt to best maintain QoP in connections where the provided QoS fluctuates unpredictably. Results show that DRoPS can be used to improve on the QoP provided by legacy protocol stacks (TCP/IP, UDP/IP), especially in the case of dynamic and complex sequences.","cites":"14","conferencePercentile":"62.18487395"},{"venue":"ACM Multimedia","id":"d1525c01613e01db6817a185b6292d723ba0f20c","venue_1":"ACM Multimedia","year":"1995","title":"A Confederation of Tools for Capturing and Accessing Collaborative Activity","authors":"Scott L. Minneman, Steve R. Harrison, Bill Janssen, Gordon Kurtenbach, Thomas P. Moran, Ian E. Smith, William van Melle","author_ids":"2830501, 2418522, 1784961, 1708940, 2909984, 3093905, 7870081","abstract":"This paper presents a confederation of tools, called Coral, that combine to support the real-time capture of and subsequent access to informal collaborative activities. The tools provide the means to initiate digital multimedia recordings, a variety of methods to index those recordings, and ways to retrieve the indexed material in other settings. The current system emerged from a convergence of the WhereWereWe multimedia work, the Tivoli LiveBoard application, and the Inter-Language Unification distributed-object programming infrastructure. We are working with a specific user community and application domain, which has helped us shape a particular, demonstrably useful, configuration of tools and to get extensive real-world experience with them. This domain involves frequent discussion and decision-making meetings and later access of the captured records of those meetings to produce accurate documentation. Several aspects of Coral-the application tools, the architecture of the confederation, and the multimedia infrastructure-are described.","cites":"94","conferencePercentile":"75"},{"venue":"ACM Multimedia","id":"2a7ff5f197d02f735c273846f445d4f88314b2de","venue_1":"ACM Multimedia","year":"2012","title":"Automatic music video generation: cross matching of music and image","authors":"Xixuan Wu, Bing Xu, Yu Qiao, Xiaoou Tang","author_ids":"2499358, 8635361, 1690077, 1741901","abstract":"Music and image are two most popular media on the Internet. Human perception of music and image are highly correlated. Music video is one of such products, in which music and image are complement to each other. In this paper, we present a system which can automatically generate music video for a given song. The challenge of such system comes from how to select relative images and align them with the song. This paper deals with this challenge by leveraging lyrics (if exists) and the semantic similarity between music and image. We retrieve related image in internet with lyrics keyword as query and use a learning based method to estimate a semantic score between an image and a music segment. Finally we construct a music video after quality filtering and refinement. Our system also allows users to upload their images and re-pick recommended images to personalize the music video.","cites":"6","conferencePercentile":"75"},{"venue":"ACM Multimedia","id":"c7bdb3b9fc05d952e4ad5d906a3b5754ce3b23f0","venue_1":"ACM Multimedia","year":"2012","title":"Cross matching of music and image","authors":"Xixuan Wu, Yu Qiao, Xiaogang Wang, Xiaoou Tang","author_ids":"2499358, 1690077, 2868636, 1741901","abstract":"Human perception of music and image are highly correlated. Both of them can inspire human sensation like emotion and power. This paper investigates how to model the relationship between music and image using 47,888 music-image pairs extracted from music videos. We have two basic observations for this relationship: 1) music space exhibits simpler cluster structure than image space, and 2) the relationship between the two spaces is complex and nonlinear. Based on these observations, we develop Multiple Ranking Canonical Correlation Analysis (MR-CCA) to learn such relationship. MR-CCA clusters the music-image pairs according to their music parts, and then conducts Ranking CCA (R-CCA) for each cluster. Compared with classical CCA, R-CCA takes account of the pairwise ranking information available in our dataset. MR-CCA improves performance and significantly reduce computational cost. Experiment results show that R-CCA outperforms CCA, and MR-CCA has the best performance with a consistency score of 84.52% with human labeling. The proposed method can be generalized to model cross media relationship and has potential applications in video generation, background music recommendation, and joint retrieval of music and image.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"4799fca0735418b03c1d0ee17deb1cfb5f9d3eb7","venue_1":"ACM Multimedia","year":"2007","title":"Hierarchical collaborative multicast","authors":"Francisco de Asís López-Fuentes, Eckehard G. Steinbach","author_ids":"2279848, 7252930","abstract":"In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on hierarchical clustering. Our delivery scheme involves cooperation among the participating peers. The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.","cites":"2","conferencePercentile":"28.125"},{"venue":"ACM Multimedia","id":"1cf7daa35e7236ea21fd9607b4e49484d8845d02","venue_1":"ACM Multimedia","year":"2001","title":"Real-time voice communication over the internet using packet path diversity","authors":"Yi J. Liang, Eckehard G. Steinbach, Bernd Girod","author_ids":"1771521, 7252930, 7811296","abstract":"The quality of real-time voice communication over best-effort networks is mainly determined by the delay and loss characteristics observed along the network path. Excessive playout buffering at the receiver is prohibitive and significantly delayed packets have to be discarded and considered as late loss. We propose to improve the tradeoff among delay, late loss rate, and speech quality using multi-stream transmission of real-time voice over the Internet, where multiple redundant descriptions of the voice stream are sent over independent network paths. Scheduling the playout of the received voice packets is based on a novel multi-stream adaptive playout scheduling technique that uses a Lagrangian cost function to trade delay versus loss. Experiments over the Internet suggest largely uncorrelated packet erasure and delay jitter characteristics for different network paths which leads to a noticeable path diversity gain. We observe significant reductions in mean end-to-end latency and loss rates as well as improved speech quality when compared to FEC protected single-path transmission at the same data rate. In addition to our Internet measurements, we analyze the performance of the proposed multi-path voice communication scheme using the <i>ns</i> network simulator for different network topologies, including shared network links.","cites":"54","conferencePercentile":"94.3877551"},{"venue":"ACM Multimedia","id":"f037862781dcaa6536a0a2c25149f5cce3c5b596","venue_1":"ACM Multimedia","year":"2007","title":"A web-based aggregated platform for user-contributed interactive media broadcasting","authors":"Jingjing Liu, Yalou Huang, Dong Li, Fanghao Wu, Bin Li","author_ids":"3889747, 1786087, 1678390, 3226969, 1740979","abstract":"In this paper, we present a web-based aggregated platform, DJ DreamFactory, which enables average users to effortlessly participate in and contribute to interactive media broadcasting over the Internet. The platform overcomes several shortcomings of existing Internet-based broadcasting systems, such as inconvenience in channel surfing and content browsing due to the scattering and isolating of broadcasting stations, difficulties in setting up a broadcasting station, lack of communications between broadcasters and audience, and little support for personalized experience. The proposed platform facilitates users' media access by seamlessly aggregating sporadic broadcasting stations run by individual hosts, and enables a virtual community where grassroots users can contribute to media broadcasting, sharing, organizing and annotating through social networking. In addition, it supports real-time multimodal interaction between audience and hosts, provides customized services for both broadcasters and audience, supports personalized media experiences by mining and managing audience's preferences, and facilitates the organization of unstructured media data collections as well as collective human intelligence on the Web.","cites":"2","conferencePercentile":"28.125"},{"venue":"ACM Multimedia","id":"587c8ffa558bd3c8a22589fd34cf2d51dea07ca3","venue_1":"ACM Multimedia","year":"2007","title":"Video search re-ranking via multi-graph propagation","authors":"Jingjing Liu, Wei Lai, Xian-Sheng Hua, Yalou Huang, Shipeng Li","author_ids":"3889747, 6064843, 1746102, 1786087, 4973820","abstract":"This paper<sup>1</sup> is concerned with the problem of multimodal fusion in video search. First, we employ an <i>object-sensitive</i> approach to query analysis to improve the baseline result of text-based video search. Then, we propose a <i>PageRank-like</i> graph-based approach to text-based search result re-ranking. To better exploit the underlying relationship between video shots, the proposed re-ranking scheme simultaneously leverages textual relevancy, semantic concept relevancy, and low-level-feature-based visual similarity. In this PageRank-like scheme, we construct a set of graphs with the video shots as vertexes, and the conceptual and visual similarity between video shots as \"hyperlinks\". A modified topic-sensitive PageRank algorithm is then applied on these graphs to propagate the relevance scores through all related video shots. Experimental results verify the effectiveness of the graph-based propagation approach combined with the object-sensitive query analysis approach, which brings significant improvement to the baseline of text-based video search. Our experimental analysis also indicates that the proposed re-ranking method is highly generic and independent of different query classes, training data, and human interference.","cites":"49","conferencePercentile":"94.53125"},{"venue":"ACM Multimedia","id":"287cdc25efdb5b7b4fd1f87e664f7f7844d56019","venue_1":"ACM Multimedia","year":"2001","title":"Visual query tools for uncertain spatio-temporal data","authors":"Katherine Malan, Gary Marsden, Edwin H. Blake","author_ids":"3200099, 1779264, 2829324","abstract":"Some multimedia archives contain data which have vague locations in time and space. By this we mean that, although there is some idea of when and where the entity is located, the precise information is unknown. In this paper, we present a novel approach to displaying and querying such uncertain data. We use the concepts of dynamic queries, add to this a 2D query tool for performing spatial queries and enable Boolean combinations of queries. We have implemented these ideas in a pilot system for querying African artwork. In this way, we show how it is possible for novice users to easily query large multimedia archives with complex uncertain attributes.","cites":"0","conferencePercentile":"4.591836735"},{"venue":"ACM Multimedia","id":"b80dbdfbcb3631e368ff348cac34c87c4052cd82","venue_1":"ACM Multimedia","year":"2003","title":"Automatic browsing of large pictures on mobile devices","authors":"Hao Liu, Xing Xie, Wei-Ying Ma, HongJiang Zhang","author_ids":"4087368, 1687677, 1705244, 1718558","abstract":"Pictures have become increasingly common and popular in mobile communications. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of large pictures on the small screen. In this paper, we propose a novel approach which is able to automate the scrolling and navigation of a large picture with a minimal amount of user interaction on mobile devices. An image attention model is employed to illustrate the information structure within an image. An optimal image browsing path is then calculated based on the image attention model to simulate the human browsing behaviors. Experimental evaluations of the proposed mechanism indicate that our approach is an effective way for viewing large images on small displays.","cites":"91","conferencePercentile":"94.59459459"},{"venue":"ACM Multimedia","id":"3def1bf233ad67e48b709697b7d6f97783241c94","venue_1":"ACM Multimedia","year":"2004","title":"K-BOX: a query-by-singing based music retrieval system","authors":"Dacheng Tao, Hao Liu, Xiaoou Tang","author_ids":"7761803, 4087368, 1741901","abstract":"In this paper, we present an efficient query-by-singing based musical retrieval system. We first combine multiple Support Vector Machines by classifier committee learning to segment the sentences from a song automatically. Many new methods in manipulating Mel-Frequency Cepstral Coefficient (MFCC) matrix are studied and compared for optimal feature selection. Experiments show that the 3rd coefficient is the most relevant to music comparison out of 13 coefficients and the proposed simplified MFCC feature is able to achieve a reasonable trade-off between accuracy and efficiency. To improve system efficiency, we re-organize the database by a new two-stage clustering scheme in both time space and feature space. We combine K-means algorithm and dynamic time wrapping similarity measurement for feature space clustering. We also propose a new method for model-selection of K-means algorithm. Experiments show that the proposed approach can achieve more than 30 percent increase in accuracy while speed up more than 16 times in average query time.","cites":"5","conferencePercentile":"45.58823529"},{"venue":"ACM Multimedia","id":"3cbb8ec4c45f81c0aacd44ff1241fb0316fa400f","venue_1":"ACM Multimedia","year":"2004","title":"Intuitive and effective interfaces for WWW image search engines","authors":"Zhiwei Li, Xing Xie, Hao Liu, Xiaoou Tang, Mingjing Li, Wei-Ying Ma","author_ids":"2902955, 1687677, 4087368, 1741901, 8392859, 1705244","abstract":"Web image search engine has become an important tool to organize digital images on the Web. However, most commercial search engines still use a list presentation while little effort has been placed on improving their usability. How to present the image search results in a more intuitive and effective way is still an open question to be carefully studied. In this demo, we present iFind, a scalable Web image search engine, in which we integrated two kinds of search result browsing interfaces. User study results have proved that our interfaces are superior to traditional interfaces.","cites":"3","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"1ba9d12f24ac04f0309e8ff9b0162c6e18d97dc3","venue_1":"ACM Multimedia","year":"2016","title":"Robust Face Recognition with Deep Multi-View Representation Learning","authors":"Jianshu Li, Jian Zhao, Fang Zhao, Hao Liu, Jing Li, Shengmei Shen, Jiashi Feng, Terence Sim","author_ids":"2757639, 7137086, 2013508, 4087368, 1742253, 3493398, 2752512, 1715286","abstract":"This paper describes our proposed method targeting at the MSR Image Recognition Challenge MS-Celeb-1M. The challenge is to recognize one million celebrities from their face images captured in the real world. The challenge provides a large scale dataset crawled from the Web, which contains a large number of celebrities with many images for each subject. Given a new testing image, the challenge requires an identify for the image and the corresponding confidence score. To complete the challenge, we propose a two-stage approach consisting of data cleaning and multi-view deep representation learning. The data cleaning can effectively reduce the noise level of training data and thus improves the performance of deep learning based face recognition models. The multi-view representation learning enables the learned face representations to be more specific and discriminative. Thus the difficulties of recognizing faces out of a huge number of subjects are substantially relieved. Our proposed method achieves a coverage of 46.1% at 95% precision on the random set and a coverage of 33.0% at 95% precision on the hard set of this challenge.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"4a8e4e637c7729a5418498efacee896f53e23bea","venue_1":"ACM Multimedia","year":"1993","title":"Salient Video Stills: Content and Context Preserved","authors":"Laura Teodosio, Walter Bender","author_ids":"1765546, 1682734","abstract":"A new class of images called salient stills is demonstrated and a software development platform for their creation is discussed. These images do not represent one discrete moment of time, as do a photograph or single video frame. Rather, one image reflects the aggregate of the temporal changes that occur in a moving image sequence with the salient features preserved. By the application of an affine transformation and non-linear temporal processing, multiple frames of an image sequence, which may include variations in focal-length or field-of-view, are combined to create a single still image. The still image may have multi-resolution patches, a larger field-of-view, or higher overall resolution than any individual frame in the original image sequence. It may also contain selected salient objects from any one of the sequence of video frames. The still can be created automatically or with user intervention. A by-product of the salient still process is a structured representation of moving image data.","cites":"128","conferencePercentile":"95.34883721"},{"venue":"ACM Multimedia","id":"b3af008337f881ee28930059c12f946681f5c7d3","venue_1":"ACM Multimedia","year":"2012","title":"Discovering areas of interest with geo-tagged images and check-ins","authors":"Jiajun Liu, Zi Huang, Lei Chen, Heng Tao Shen, Zhixian Yan","author_ids":"8669089, 4778316, 1685354, 1724393, 7833657","abstract":"Geo-tagged image is an ideal source for the discovery of popular travel places. However, the aspects of popular venues for daily-life purposes like dining and shopping are often missing in the mined locations from geo-tagged images. Fortunately check-in websites provide us a unique opportunity of analyzing people's preferences in their daily lives to complement the knowledge mined from geo-tagged images. This paper presents a novel approach for the discovery of Areas of Interest (AoI). By analyzing both geo-tagged images and check-ins, the approach exploits travelers' flavors as well as the preferences of daily-life activities of local residents to find AoI in a city. The proposed approach consists of two major steps. Firstly, we devise a density-based clustering method to discover AoI, mainly based on the image densities but also reinforced by the secondary densities from the images' neighboring venues. Then we propose a novel joint authority analysis framework to rank AoI. The framework simultaneously considers both the location-location transitions, and the user-location relations. An interactive presentation interface for visualizing AoI is also presented. The approach is tested with very large datasets for Shanghai city. They consist of 49,460 geo-tagged images from Panoramio.com, and 1,361,547 check-ins from the check-in website Qieke.com. By evaluating the ranking accuracy and quality of AoI, we demonstrate great improvements of our method over compared methods.","cites":"13","conferencePercentile":"90.66455696"},{"venue":"ACM Multimedia","id":"2f33fcc3ab9e5ab7da39231688de0ba63f757d53","venue_1":"ACM Multimedia","year":"2011","title":"Multimodal fusion for video copy detection","authors":"Xavier Anguera Miró, Juan Manuel Barrios, Tomasz Adamek, Nuria Oliver","author_ids":"3225775, 1795669, 1793790, 1692808","abstract":"Content-based video copy detection algorithms (CBCD) focus on detecting video segments that are identical or transformed versions of segments in a known video. In recent years some systems have proposed the combination of orthogonal modalities (e.g. derived from audio and video) to improve detection performance, although not always achieving consistent results. In this paper we propose a fusion algorithm that is able to combine as many modalities as available at the decision level. The algorithm is based on the weighted sum of the normalized scores, which are modified depending on how well they rank in each modality. This leads to a virtually parameter-free fusion algorithm. We performed several tests using 2010 TRECVID VCD datasets and obtain up to 46% relative improvement in min-NDCR while also improving the F1 metric on the fused results in comparison to just using the best single modality.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"26e425781e4090abfae65b5d68eac72282dd2e31","venue_1":"ACM Multimedia","year":"2016","title":"Image Captioning with Deep Bidirectional LSTMs","authors":"Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel","author_ids":"3879731, 1688587, 3393754, 1708312","abstract":"This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \"translate\" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"388bf923ef318975dd1be7291cedc1889b1dc0a0","venue_1":"ACM Multimedia","year":"2005","title":"Supporting multimedia streaming between mobile peers with link availability prediction","authors":"Min Qin, Roger Zimmermann, Leslie S. Liu","author_ids":"2756826, 1790974, 2564698","abstract":"Numerous types of mobile devices are now popular with end users, who increasingly use them to carry multimedia content on the go. As wireless connectivity is integrated with more handhelds, streaming multimedia content among mobile peers is becoming a popular application. One of the main challenges in mobile streaming is the requirement that the link must be continuously available for a period of time to enable uninterrupted data transmission and a smooth media performance. Hence, an accurate prediction of future link availability is very desirable and allows, for example, the selection of the most stable link when a multimedia object is available from multiple peers. In this paper, we present a novel iterative algorithm for predicting continuous link availability between two mobile ad-hoc peers. Our method can function without the support of GPS equipment. By a rough estimation of the distance between two peers, our approach is able to accurately predict link availability over a short period of time. Experimental results show that our algorithm can accurately estimate the future link status with an error margin lower than 7%. To demonstrate the feasibility of our approach we have integrated our link prediction algorithm into MStream: a pioneering mobile audio streaming application. Simulation results show that our link availability model can reduce the number of link breaks and achieve smooth streaming experiences among mobile peers.","cites":"12","conferencePercentile":"63.11881188"},{"venue":"ACM Multimedia","id":"14328fe86cb346dd4fa6ac2b798250786dcfbc6c","venue_1":"ACM Multimedia","year":"2010","title":"Social audio features for advanced music retrieval interfaces","authors":"Michael Kuhn, Roger Wattenhofer, Samuel Welten","author_ids":"1772075, 1716440, 2930885","abstract":"The size of personal music collections has constantly increased over the past years. As a result, the traditional metadata based lists to browse these collections have reached their limits. Interfaces that are based on music similarity offer an alternative and thus are increasingly gaining attention. Music similarity is typically either derived from audio-features (objective approach) or from user driven information sources, such as collaborative filtering or social tags (subjective approach). Studies show that the latter techniques outperform audio-based approaches when it comes to describe the perceived music similarity. However, subjective approaches typically only define pairwise relations as opposed to the global notion of similarity given by audio-feature spaces. Many of the proposed interfaces for similarity based music access inherently depend on this global notion and are thus not applicable to user driven music similarity measures. The first contribution of this paper is a high dimensional music space that is based on user driven similarity measures. It combines the advantages of audio-feature spaces (global view) with the advantages of subjective sources that better reflect the users' perception. The proposed space compactly represents similarity and therefore is well suited for offline use, such as in mobile applications. To demonstrate the practical applicability, the second contribution is a comprehensive mobile music player that incorporates several smart interfaces to access the user's music collection. Based on this application, we finally present a large-scale user study that underlines the benefits of the introduced interfaces and shows their great user acceptance.","cites":"11","conferencePercentile":"80.1369863"},{"venue":"ACM Multimedia","id":"dba0886b653d0628226e64d0024509f16a63c82b","venue_1":"ACM Multimedia","year":"2016","title":"SceneTextReg: A Real-Time Video OCR System","authors":"Haojin Yang, Cheng Wang, Christian Bartz, Christoph Meinel","author_ids":"1688587, 3879731, 3393754, 1708312","abstract":"We showcase a system for real-time video text recognition. The system is based on the standard workflow of text spotting system, which includes text detection and word recognition procedures. We apply deep neural networks in both procedures. In text localization stage, textual candidates are roughly captured by using a Maximally Stable Extremal Regions (MSERs) detector with high recall rate, false alarms are then eliminated by using Convolutional Neural Network (CNN ) verifier. For word recognition, we developed a skeleton based method for segmenting text region from its background, then a CNN based word recognizer is utilized for recognizing texts. Our current implementation demonstrates a real time performance for recognizing scene text by using a standard laptop with webcam. The word recognizer achieves competitive result to state-of-the-art methods by only using synthetical training data.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"0fa97c8676f945daf8c1e7a41c121be4496492c6","venue_1":"ACM Multimedia","year":"1997","title":"Supporting Similarity Queries in MARS","authors":"Michael Ortega-Binderberger, Yong Rui, Kaushik Chakrabarti, Sharad Mehrotra, Thomas S. Huang","author_ids":"2163619, 1728806, 1800885, 1686199, 1739208","abstract":"To address the emerging needs of applications that require access to and retrieval of multimedia objects, we are developing the Multimedia Analysis and Retrieval System (MARS) in our group at the University of Illi-nois 10]. In this paper, we concentrate on the retrieval subsystem of MARS and its support for content-based queries over image databases. Content-based retrieval techniques have been extensively studied for textual documents in the area of automatic information retrieval 19, 2]. This paper describes how these techniques can be adapted for ranked retrieval over image databases. Speciically, we discuss the ranking and retrieval algorithms developed in MARS based on the Boolean retrieval model and describe the results of our experiments that demonstrate the eeectiveness of the developed model for image retrieval.","cites":"110","conferencePercentile":"95.23809524"},{"venue":"ACM Multimedia","id":"2cc589806949dd03f18c9e596db744fa458a8a15","venue_1":"ACM Multimedia","year":"2008","title":"Real time google and live image search re-ranking","authors":"Jingyu Cui, Fang Wen, Xiaoou Tang","author_ids":"1680047, 1716835, 1741901","abstract":"Nowadays, web-scale image search engines (e.g. Google, Live Image Search) rely almost purely on surrounding text features. This leads to ambiguous and noisy results. We propose to use adaptive visual similarity to re-rank the text-based search results. A query image is first categorized into one of several predefined intention categories, and a specific similarity measure is used inside each category to combine image features for re-ranking based on the query image. Extensive experiments demonstrate that using this algorithm to filter output of Google and Live Image Search is a practical and effective way to dramatically improve the user experience. A real-time image search engine is developed for on-line image search with re-ranking: http://mmlab.ie.cuhk.edu.hk/intentsearch","cites":"59","conferencePercentile":"97.01834862"},{"venue":"ACM Multimedia","id":"9ff4d45f71994e230fe05993ee1dff3566a3f4a8","venue_1":"ACM Multimedia","year":"1999","title":"Audio-visual tracking for natural interactivity","authors":"Gopal Sarma Pingali, Gamze Tunali, Ingrid Carlbom","author_ids":"1717077, 3272155, 1716726","abstract":"The goal in user interfaces is natural interactivity unencumbered by sensor and display technology. In this paper, we propose that a multi-modal approach using inverse modeling techniques from computer vision, speech recognition, and acoustics can result in such interfaces. In particular, we demonstrate a system for audio-visual tracking, showing that such a system is more robust, more accurate, more compact, and yields more information than using a single modality for tracking. We also demonstrate how such a system can be used to find the talker among a group of individuals, and render 3D scenes to the user.","cites":"19","conferencePercentile":"68.48739496"},{"venue":"ACM Multimedia","id":"96472d8aac4c9a5744e87e6a434a87cce5f2386f","venue_1":"ACM Multimedia","year":"2001","title":"Multimedia retrieval through spatio-temporal activity maps","authors":"Gopal Sarma Pingali, Agata Opalach, Ingrid Carlbom","author_ids":"1717077, 3054971, 1716726","abstract":"As multiple video cameras and other sensors generate very large quantities of multimedia data in media productions and surveillance applications, a key challenge is to identify the relevant portions of the data and to rapidly retrieve the corresponding sensor data. Spatio-temporal activity maps serve as an efficient and intuitive graphical user interface for multimedia retrieval, particularly when the media streams are derived from multiple sensors observing a physical environment. We formulate the media retrieval problem in this context, and develop an architecture for interactive media retrieval by combining spatio-temporal \"activity maps\" with domain specific event information. Activity maps are computed from trajectories of motion of objects in the environment, which in turn are derived automatically by analysis of sensor data. We present an activity map based video retrieval system for the sport of tennis and demonstrate that the activity map based scheme significantly helps the user in a) <i>discovering</i> the relevant portions of the data, and b) non-linearly retrieving the corresponding media streams.","cites":"9","conferencePercentile":"55.6122449"},{"venue":"ACM Multimedia","id":"346cc65ad92d24f669736b3416a005dbbfb77d35","venue_1":"ACM Multimedia","year":"2002","title":"A multimodal speaker detection and tracking system for teleconferencing","authors":"Billibon H. Yoshimi, Gopal Sarma Pingali","author_ids":"1877763, 1717077","abstract":"A serious problem in both audio and video conferencing facilities available today is the difficulty in determining who is speaking among a large number of participants. There is a strong need for developing meeting room infrastructure and teleconference facilities that improve the sense of presence and participation experienced in remote meetings. We present a distributed multimodal tracking system that uses multiple cameras and microphones to automatically select the current speaker among multiple meeting participants. The system actively obtains and transmits video showing a good view of the selected speaker. The tracking system is integrated into a web-based video conferencing application that connects seven meeting rooms around the globe. An important part of designing such a system is to determine sensor placement and configuration through systematic experiments in the actual rooms where the system is deployed.","cites":"11","conferencePercentile":"56.83760684"},{"venue":"ACM Multimedia","id":"2688dc6a274a9e4bacf38adcf22d2823efc8508e","venue_1":"ACM Multimedia","year":"2001","title":"Intra-flow loss recovery and control for VoIP","authors":"Henning Sanneck, Nguyen Tuong, Long Le, Adam Wolisz, Georg Carle","author_ids":"1727184, 2271090, 2921850, 1778337, 2902874","abstract":"\"Best effort\" packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such as voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold: on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets.We first develop an end-to-end model based on loss run-lengths with which we can describe the loss distribution within a flow. These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs (PCM) with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs (G.729) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may \"break\" at transitions within the speech signal however.We then propose mechanisms which differentiate between packets within a voice data flow to minimize the impact of packet loss. We designate these methods as \"intra-flow\" loss recovery and control. At the end-to-end level, identification of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow to (statistically) trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both packets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can bem achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term.","cites":"13","conferencePercentile":"64.79591837"},{"venue":"ACM Multimedia","id":"be744c9d44413b3f227734041460d230586b3386","venue_1":"ACM Multimedia","year":"2008","title":"IntentSearch: interactive on-line image search re-ranking","authors":"Jingyu Cui, Fang Wen, Xiaoou Tang","author_ids":"1680047, 1716835, 1741901","abstract":"In this demo, we present IntentSearch, an interactive system for realtime web based image retrieval. IntentSearch works directly on top of Microsoft Live Image Search, and re-ranks its results according to user specified query image(s) and the automatically inferred user intention. Besides searching in the interface of Microsoft Live Image Search, we also design a more flexible interface to let users browse and play with all the images in the current search session, which makes web image search more efficient and interesting. Please visit http://mmlab.ie.cuhk.edu.hk/intentsearch for the experience.","cites":"26","conferencePercentile":"87.8440367"},{"venue":"ACM Multimedia","id":"c01db93e919a6311285354b5e161cfd91691ff2d","venue_1":"ACM Multimedia","year":"2007","title":"DJ DreamFactory","authors":"Jingjing Liu, Yalou Huang, Dong Li, Fanghao Wu, Bin Li","author_ids":"3889747, 1786087, 1678390, 3226969, 1740979","abstract":"DJ DreamFactory is a web-based integrated platform for interactive broadcasting over the Internet. In DJ DreamFactory, users are able to set up Internet-based broadcasting stations with minimal effort. Audience not only can receive broadcasting programs from multiple stations through this unified platform, but can also \"talk\" and \"write\" to the broadcasters as well as to other audience via real-time text and/or audio interactions.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"2297f1fef37313a79800b2783315bf8c63263c1b","venue_1":"ACM Multimedia","year":"1996","title":"Negotiation for Automated Generation of Temporal Multimedia Presentations","authors":"Mukesh Dalal, Steven K. Feiner, Kathleen McKeown, Shimei Pan, Michelle X. Zhou, Tobias Höllerer, James Shaw, Yong Feng, Jeanne Fromer","author_ids":"2772990, 1809403, 1727037, 2728986, 1705742, 1743721, 2320417, 1739720, 2974019","abstract":"Creating high-quality multimedia presentations requires much skill, time, and effort. This is particularly true when temporal media, such as speech and animation, are involved. We describe the design and implementation of a knowledge-based system that generates customized temporal multimedia presentations. We provide an overview of the system's architecture , and explain how speech, written text, and graphics are generated and coordinated. Our emphasis is on how temporal media are coordinated by the system through a multi-stage negotiation process. In negotiation, media-specific generation components interact with a novel coordination component that solves temporal constraints provided by the generators. We illustrate our work with a set of examples generated by the system in a testbed application intended to update hospital caregivers on the status of patients who have undergone a cardiac bypass operation.","cites":"43","conferencePercentile":"63.88888889"},{"venue":"ACM Multimedia","id":"37d57495f93f846033c10e9ba1125b8fbb9872a2","venue_1":"ACM Multimedia","year":"2015","title":"3D Printing and Camera Mapping: Dialectic of Virtual and Reality","authors":"He-Lin Luo, I-Chun Chen, Yi-Ping Hung","author_ids":"3001980, 3073794, 7312257","abstract":"Projection Mapping, the superimposing of virtual images upon actual objects, is already extensively used in performance arts. Applications of it are already quite mature, therefore, here we wish to achieve the opposite, or specifically speaking, the superimposing of actual objects into virtual images. This method of reverse superimposition is called \"camera mapping.\" Through cameras, camera mapping captures actual objects, and introduces them into a virtual world. Then using superimposition, this allows for actual objects to be rendered as virtual objects. However, the actual objects here must have refined shapes so that they may be superimposed back into the camera. Through the proliferation of 3D printing, virtual 3D models in computers can be created in reality, thereby providing a framework for the limits and demands of \"camera mapping.\" The new media artwork Digital Buddha combines 3D Printing and camera mapping. This work was created by 3-D deformable modeling through a computer, then transforming the model into a sculpture using 3D printing, and then remapping the materially produced sculpture back into the camera. Finally, it uses the already known algorithm to convert the model back into that of the original non-deformed sculpture. From this creation project, in the real world, audiences will see a deformed, abstract sculpture; and in the virtual world, through camera mapping, they will see a concrete sculpture (Buddha). In its representation, this piece of work pays homage to the work TV Buddha produced by video art master Nam June Paik. Using the influence television possesses over people, this work extends into the most important concepts of the digital era, \"coding\" and \"decoding,\" simultaneously addressing the shock and insecurity people in the digital era feel toward images.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"f95050856cd154ff1581ce4db232e34206dd8ec8","venue_1":"ACM Multimedia","year":"2013","title":"Augmented and interactive video playback based on global camera pose","authors":"Junsheng Fu, Lixin Fan, Yu You, Kimmo Roimela","author_ids":"6072601, 2034793, 8063525, 3318092","abstract":"This paper proposes a video playback system that allows user to expend the field of view to surrounding environments that are not visible in the original video frame, arbitrarily change the viewing angles, and see the superimposed point-of-interest (POIs) data in an augmented reality manner during the video playback. The processing consists of two main steps: in the first step, client uploads a video to the GeoVideo Engine, and then the GeoVideo Engine extracts the geo-metadata and returns them back to the client; in the second step, client requests POIs from server, and then the client renders the video with POIs.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"edc274a22bc9e8ffb3e0998af500c9690e7e043d","venue_1":"ACM Multimedia","year":"2010","title":"Structuring ordered nominal data for event sequence discovery","authors":"Chreston A. Miller, Francis K. H. Quek, Naren Ramakrishnan","author_ids":"3027468, 1740663, 1755938","abstract":"This work investigates using n-gram processing and a temporal relation encoding to providing relational information about events extracted from media streams. The event information is temporal and nominal in nature being categorized by a descriptive label or symbolic means and can be difficult to relationally compare and give ranking metrics. Given a parsed sequence of events, relational information pertinent to comparison between events can be obtained through the application of n-grams techniques borrowed from speech processing and temporal relation logic. The procedure is discussed along with results computed using a representative data set characterized by nominal event data.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"99ad4e64413d350be2f9d77add04745259ef4225","venue_1":"ACM Multimedia","year":"2003","title":"Detail-on-demand hypervideo","authors":"John Doherty, Andreas Girgensohn, Jonathan Helfman, Frank M. Shipman, Lynn Wilcox","author_ids":"3881379, 2195286, 1798257, 1749811, 1691319","abstract":"We demonstrate the use of detail-on-demand hypervideo in interactive training and video summarization. Detail-on-demand video allows viewers to watch short video segments and to follow hyperlinks to see additional detail. The player for detail-on-demand video displays keyframes indicating what links are available at each point in the video. The Hyper-Hitchcock authoring tool helps users create hypervideo by automatically dividing video into clips that can be combined in a direct manipulation interface. Clips can be grouped into composites and hyperlinks can be placed between clips and composites. A summarization algorithm creates multi-level hypervideo summaries from linear video by automatically selecting clips and placing links between them.","cites":"10","conferencePercentile":"48.1981982"},{"venue":"ACM Multimedia","id":"117551a64fbb3b3aea5d5c4d1504b9040cf0a019","venue_1":"ACM Multimedia","year":"2010","title":"Movie genre classification via scene categorization","authors":"Howard Zhou, Tucker Hermans, Asmita V. Karandikar, James M. Rehg","author_ids":"2401836, 8007160, 1846397, 1692956","abstract":"This paper presents a method for movie genre categorization of movie trailers, based on scene categorization. We view our approach as a step forward from using only low-level visual feature cues, towards the eventual goal of high-level seman- tic understanding of feature films. Our approach decom- poses each trailer into a collection of keyframes through shot boundary analysis. From these keyframes, we use state-of- the-art scene detectors and descriptors to extract features, which are then used for shot categorization via unsuper- vised learning. This allows us to represent trailers using a bag-of-visual-words (bovw) model with shot classes as vo- cabularies. We approach the genre classification task by mapping bovw temporally structured trailer features to four high-level movie genres: action, comedy, drama or horror films. We have conducted experiments on 1239 annotated trailers. Our experimental results demonstrate that exploit- ing scene structures improves film genre classification com- pared to using only low-level visual features.","cites":"15","conferencePercentile":"84.10958904"},{"venue":"ACM Multimedia","id":"4122cbf80a4a7c2a35559e2b89fb58b582326d6b","venue_1":"ACM Multimedia","year":"2016","title":"From Seed Discovery to Deep Reconstruction: Predicting Saliency in Crowd via Deep Networks","authors":"Yanhao Zhang, Lei Qin, Qingming Huang, Kuiyuan Yang, Jun Zhang, Hongxun Yao","author_ids":"7550220, 4758611, 1689702, 2976163, 1752214, 1720100","abstract":"Although saliency prediction in crowd has been recently recognized as an essential task for video analysis, it is not comprehensively explored yet. The challenges lie in that eye fixations in crowded scenes are inherently \"distinct\" and \"multi-modal\", which differs from those in regular scenes. To this end, the existing saliency prediction schemes typically rely on hand designed features with shallow learning paradigm, which neglect the underlying characteristics of crowded scenes. In this paper, we propose a saliency prediction model dedicated for crowd videos with two novelties: 1) Distinct units are discovered using deep representation learned by a Stacked Denoising Auto-Encoder (SDAE), considering perceptual properties of crowd saliency; 2) Contrast-based saliency is measured through deep reconstruction errors in the second SDAE trained on all units excluding distinct units. A unified model is integrated for online processing crowd saliency. Extensive evaluations on two crowd video benchmark datasets demonstrate that our approach can effectively explore crowd saliency mechanism in two-stage SDAEs and achieve significantly better results than state-of-the-art methods, with robustness to parameters.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"3f5bd92a26294b00ead3e020cd0fa9f1ce5ff3a4","venue_1":"ACM Multimedia","year":"2003","title":"Managing digital memories with the FXPAL photo application","authors":"John Adcock, Matthew L. Cooper, John Doherty, Jonathan Foote, Andreas Girgensohn, Lynn Wilcox","author_ids":"5529541, 6108444, 3881379, 1797460, 2195286, 1691319","abstract":"The FXPAL Photo Application is designed to faciliate the organization of digital images from digital cameras and other sources through automated organization and intuitive user interfaces.","cites":"3","conferencePercentile":"21.62162162"},{"venue":"ACM Multimedia","id":"ad049d74b7b1e6a12ca76ca5ef2a3abfb107a8d6","venue_1":"ACM Multimedia","year":"2004","title":"Between context-aware media capture and multimedia content analysis: where do we find the promised land?","authors":"Susanne Boll, Dick C. A. Bulterman, Ramesh Jain, Tat-Seng Chua, Rainer Lienhart, Lynn Wilcox, Marc Davis, Svetha Venkatesh","author_ids":"1714281, 1726923, 4521564, 1684968, 1803227, 1691319, 1777964, 1679520","abstract":"For well over the past decade, multimedia researchers have been trying to solve the problems of multimedia information retrieval and media asset management. The availability of high-quality highly descriptive descriptions of its content and structure– metadata–is a key factor in order for media to be retrievable and usable. The approaches we find here mostly generate metadata by an analysis of low-level features of multimedia content and the inference of high-level multimedia metadata. Different algorithms have been developed for different media types and applied to different application domains. Now, it is time to ask research if the expected results and contribution to multimedia information retrieval and media access have been achieved and the Promised Land has been found. In this panel, we would like to bring together researchers from different research directions and vividly discuss the panelists' different views on the following questions: What are the real results in the field of multimedia content analysis? Did we reach at feasible solutions for automatic signal-based analysis of media content and achieve workable solution to content-based multimedia access? Is it really true that media must be analyzed long after it has been captured and stored? Do the solutions we provide so far overcome the semantic gap? Or is it time to set off to new shores and try to integrate multimedia metadata earlier in the content creation process? Can the user be involved in the metadata creation process, how and when? Should we pre-pone the point in time where metadata is achieved by rather collecting it during capture than deriving it after it has been captured. Are content analysis and context-aware media capture two opposite research directions or can signal processing and context capturing meet and provide a synergy? What should the future end-to-end media production lifecycle be? Which applications are driving the creation and usage of context and metadata?","cites":"3","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"d822e16fa74fe4777ed159e3cc635e298af4dd5d","venue_1":"ACM Multimedia","year":"2008","title":"Determining activity patterns in retail spaces through video analysis","authors":"Andreas Girgensohn, Frank M. Shipman, Lynn Wilcox","author_ids":"2195286, 1749811, 1691319","abstract":"Retail establishments want to know about traffic flow and patterns of activity in order to better arrange and staff their business. A large number of fixed video cameras are commonly installed at these locations. While they can be used to observe activity in the retail environment, assigning personnel to this is too time consuming to be valuable for retail analysis. We have developed video processing and visualization techniques that generate presentations appropriate for examining traffic flow and changes in activity at different times of the day. Taking the results of video tracking software as input, our system aggregates activity in different regions of the area being analyzed, determines the average speed of moving objects in the region, and segments time based on significant changes in the quantity and/or location of activity. Visualizations present the results as heat maps to show activity and object counts and average velocities overlaid on the map of the space.","cites":"11","conferencePercentile":"70.18348624"},{"venue":"ACM Multimedia","id":"32327b1c0049da9833d4c31aabc06ef72ec1a760","venue_1":"ACM Multimedia","year":"2008","title":"Document finder","authors":"Qiong Liu, Patrick Chiu, Lynn Wilcox","author_ids":"1794500, 2895008, 1691319","abstract":"This demo introduces a tool for accessing an e-document by capturing one or more images of a real object or document hardcopy. This tool is useful when a file name or location of the file is unknown or unclear. It can save field workers and office workers from remembering/exploring numerous directories and file names. Frequently, it can convert tedious keyboard typing in a search box to a simple camera click. Additionally, when a remote collaborator cannot clearly see an object or a document hardcopy through remote collaboration cameras, this tool can be used to automatically retrieve and send the original e-document to a remote screen or printer.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"96ac8a9adca238317d5ad09bd8f158b37b0bca0c","venue_1":"ACM Multimedia","year":"2011","title":"A tool for authoring unambiguous links from printed content to digital media","authors":"Andreas Girgensohn, Frank M. Shipman, Lynn Wilcox, Qiong Liu, Chunyuan Liao, Yuichi Oneda","author_ids":"2195286, 1749811, 1691319, 1794500, 2686363, 2419952","abstract":"Embedded Media Markers (EMMs) are nearly transparent icons printed on paper documents that link to associated digital media. By using the document content for retrieval, EMMs are less visually intrusive than barcodes and other glyphs while still providing an indication for the presence of links. An initial implementation demonstrated good overall performance but exposed difficulties in guaranteeing the creation of unambiguous EMMs. We developed an EMM authoring tool that supports the interactive authoring of EMMs via visualizations that show the user which areas on a page may cause recognition errors and automatic feedback that moves the authored EMM away from those areas. The authoring tool and the techniques it relies on have been applied to corpora with different visual characteristics to explore the generality of our approach.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"40629608185e7bb3334978d6a19f9a235fa58ea4","venue_1":"ACM Multimedia","year":"2011","title":"OpenCast Matterhorn 1.1: reaching new heights","authors":"Christopher A. Brooks, Markus Ketterl, Adam Hochman, Josh Holtzman, Judy Stern, Tobias Wunden, Kristofor Amundson, Greg Logan, Kenneth Lui, Adam McKenzie, Denis Meyer, Markus Moormann, Matjaz Rihtar, Ruediger Rolf, Nejc Skofic, Micah Sutton, Ruben Perez Vazquez, Benjamin Wulff","author_ids":"2991107, 2647895, 2116894, 2844281, 8708751, 2633590, 3278571, 2613872, 2366809, 2385927, 2503368, 2582105, 2526356, 1831008, 1985730, 2034944, 2683004, 2798423","abstract":"This paper gives a short overview of the Opencast Matterhorn system. Built by an open community of individuals and institutions, Matterhorn provides a lecture capture platform for both research and production environments. Matterhorn is comprehensive and scalable, and includes components for the acquisition, processing, and playback of content. Matterhorn is licensed under the liberal Educational Community License (ECL 2.0), a flexible OSI approved open source license, and the Opencast community is free for all institutions, corporations, or individuals to join.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"12e66bfd6ddd53726e040316d1a9c73221a56aec","venue_1":"ACM Multimedia","year":"2002","title":"FlySPEC: a multi-user video camera system with hybrid human and automatic control","authors":"Qiong Liu, Don Kimber, Jonathan Foote, Lynn Wilcox, John S. Boreczky","author_ids":"1794500, 2178004, 1797460, 1691319, 2719487","abstract":"FlySPEC is a video camera system designed for real-time remote operation. A hybrid design combines the high resolution of an optomechanical video camera with the wide field of view always available from a panoramic camera. The control system integrates requests from multiple users so that each controls a virtual camera. The control system seamlessly integrates manual and fully automatic control. It supports a range of options from untended automatic to full manual control. The system can also learn control strategies from user requests. Additionally, the panoramic view is always available for an intuitive interface, and objects are never out of view regardless of the zoom factor. We present the system architecture, an information-theoretic approach to combining panoramic and zoomed images to optimally satisfy user requests, and experimental results that show the FlySPEC system significantly assists users in a remote inspection tasks.","cites":"43","conferencePercentile":"88.46153846"},{"venue":"ACM Multimedia","id":"67014e978659a89ac149f23867b3a5127efb9d4d","venue_1":"ACM Multimedia","year":"2002","title":"A hitchcock assisted video edited night at the opera","authors":"John Doherty, Lynn Wilcox, Andreas Girgensohn","author_ids":"3881379, 1691319, 2195286","abstract":"Hitchcock is a semi-automatic video editing system. This video shows users collaboratively authoring a home video.","cites":"2","conferencePercentile":"17.94871795"},{"venue":"ACM Multimedia","id":"2315b20bb3736c605cc3246afc904a2a7c7f941c","venue_1":"ACM Multimedia","year":"2012","title":"Modeling video viewing behaviors for viewer state estimation","authors":"Ryo Yonetani","author_ids":"1899753","abstract":"Human gaze behaviors when watching videos reflect their cognitive states as well as characteristics of the video scenes being watched. Our goal is to establish a method to estimate the viewer states from his/her eye movements toward general videos, such as TV news and commercials. The proposed method is based on a novel model of video viewing behaviors, which takes into account structural and statistical relationships between video dynamics, gaze dynamics and viewer states. This model realizes statistical learning of gaze information while considering dynamic characteristics of video scenes to achieve viewer-state estimation. In this paper, we present an overview of the viewer-state estimation method based on the model of video-viewing behaviors, including several past work done by the author's team.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"51463d7d66e2c703561225efc9ae2b9abd768db6","venue_1":"ACM Multimedia","year":"2015","title":"SINGA: A Distributed Deep Learning Platform","authors":"Beng Chin Ooi, Kian-Lee Tan, Sheng Wang, Wei Wang, Qingchao Cai, Gang Chen, Jinyang Gao, Zhaojing Luo, Anthony K. H. Tung, Yuan Wang, Zhongle Xie, Meihui Zhang, Kaiping Zheng","author_ids":"1693070, 1688848, 3774895, 3375911, 1701304, 1697506, 2504016, 3100448, 1699730, 1724874, 3063356, 1793178, 1977967","abstract":"Deep learning has shown outstanding performance in various machine learning tasks. However, the deep complex model structure and massive training data make it expensive to train. In this paper, we present a distributed deep learning system, called SINGA, for training big models over large datasets. An intuitive programming model based on the layer abstraction is provided, which supports a variety of popular deep learning models. SINGA architecture supports both synchronous and asynchronous training frameworks. Hybrid training frameworks can also be customized to achieve good scalability. SINGA provides different neural net partitioning schemes for training large models. SINGA is an Apache Incubator project released under Apache License 2.","cites":"8","conferencePercentile":"95"},{"venue":"ACM Multimedia","id":"8b37c5c363539b38d24bcb3dfbee9bf80cefbcd7","venue_1":"ACM Multimedia","year":"2011","title":"Semantic point detector","authors":"Kuiyuan Yang, Lei Zhang, Meng Wang, HongJiang Zhang","author_ids":"2976163, 1724298, 1731598, 1718558","abstract":"Local features are the building blocks of many visual systems, and local point detector is usually the first component for local feature extraction. Existing local point detector are designed with target for matching and it may not perform well when applied in image content representation. Actually many existing studies demonstrate that the simple dense sampling strategy can achieve better performance than many local point detection methods in image classification tasks. In this paper, we propose a novel point detector named semantic point detector, which detects a set of semantically meaningful patches from each image and yields more compact and complete image representation. It is learned from an set of images with concepts from a large ontology. We conduct extensive experiments based on the proposed detector, and the experimental results demonstrate the effectiveness of our approach.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"3f0cfc335cf62123b46de2dad02baecdde283a7c","venue_1":"ACM Multimedia","year":"2009","title":"Streaming HD H.264 encoder on programmable processors","authors":"Nan Wu, Mei Wen, Wei Wu, Ju Ren, Huayou Su, Changqing Xun, Chunyuan Zhang","author_ids":"2616589, 7446877, 2581478, 2567712, 3280898, 2599882, 6944626","abstract":"Programmable processors have great advantage over dedicated ASIC design under intense time-to-market pressure. However, real-time encoding of high-definition (HD) H.264 video (up to 1080p) is a challenge to most existing programmable processors. On the other hand, model-based design is widely accepted in developing complex media program. Stream model, an emerging model-based programming method, shows surprising efficiency on many compute-intensive domains especially for media processing. On the basis, this paper proposes a set of streaming techniques for H.264 encoding, and then develops all of the code based on the X264 reference code. Our streaming H.264 encoder is a pure software implementation completely written in high-level language without special hardware/algorithm support. Real execution results show that our encoder achieves significant speedup over the original X264 encoder on various programmable architectures: on X86 Core<sup>TM</sup>2 E8200 the speedup is 1.8x, on MIPS 4KEc the speedup is 3.7x, on TMS320 C6416 DSP the speedup is 5.5x, on stream processor STORM-SP16 G220 the speedup is 6.1x. Especially, on STORM processor, the streaming encoder achieves the performance of 30.6 frames per second for a 1080P HD sequence, satisfying the real-time requirement. These indicate that streaming is extremely efficient for this kind of media workload. Our work is also applicable for other media processing applications, and provides architecture insights into dedicated ASIC or FPGA HD H.264 encoders.","cites":"10","conferencePercentile":"67.97520661"},{"venue":"ACM Multimedia","id":"4a94b6d07250430bef4dbfab9dcd0a56ef1ab0e0","venue_1":"ACM Multimedia","year":"2011","title":"Contextual synonym dictionary for visual object retrieval","authors":"Wenbin Tang, Rui Cai, Zhiwei Li, Lei Zhang","author_ids":"2245033, 6306010, 2902955, 1724298","abstract":"In this paper, we study the problem of visual object retrieval by introducing a dictionary of contextual synonyms to narrow down the semantic gap in visual word quantization. The basic idea is to expand a visual word in the query image with its synonyms to boost the retrieval recall. Unlike the existing work such as soft-quantization, which only focuses on the Euclidean (<i>l</i><sub>2</sub>) distance in descriptor space, we utilize the visual words which are more likely to describe visual objects with the same semantic meaning by identifying the words with similar contextual distributions (<i>i.e.</i> contextual synonyms). We describe the contextual distribution of a visual word using the statistics of both co-occurrence and spatial information averaged over all the image patches having this visual word, and propose an efficient system implementation to construct the contextual synonym dictionary for a large visual vocabulary. The whole construction process is unsupervised and the synonym dictionary can be naturally integrated into a standard bag-of-feature image retrieval system. Experimental results on several benchmark datasets are quite promising. The contextual synonym dictionary-based expansion consistently outperforms the <i>l</i><sub>2</sub> distance-based soft-quantization, and advances the state-of-the-art performance remarkably.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"460d131e081486491af532604351a0691b52b11f","venue_1":"ACM Multimedia","year":"2014","title":"User-level psychological stress detection from social media using deep neural network","authors":"Huijie Lin, Jia Jia, Quan Guo, Yuanyuan Xue, Qi Li, Jie Huang, Lianhong Cai, Ling Feng","author_ids":"2917061, 1697300, 1972159, 1925515, 1682467, 1762049, 7239047, 6257025","abstract":"It is of significant importance to detect and manage stress before it turns into severe problems. However, existing stress detection methods usually rely on psychological scales or physiological devices, making the detection complicated and costly. In this paper, we explore to automatically detect individuals' psychological stress via social media. Employing real online micro-blog data, we first investigate the correlations between users' stress and their tweeting content, social engagement and behavior patterns. Then we define two types of stress-related attributes: 1) low-level content attributes from a single tweet, including text, images and social interactions; 2) user-scope statistical attributes through their weekly micro-blog postings, leveraging information of tweeting time, tweeting types and linguistic styles. To combine content attributes with statistical attributes, we further design a convolutional neural network (CNN) with cross autoencoders to generate user-scope content attributes from low-level content attributes. Finally, we propose a deep neural network (DNN) model to incorporate the two types of user-scope attributes to detect users' psychological stress. We test the trained model on four different datasets from major micro-blog platforms including Sina Weibo, Tencent Weibo and Twitter. Experimental results show that the proposed model is effective and efficient on detecting psychological stress from micro-blog data. We believe our model would be useful in developing stress detection tools for mental health agencies and individuals.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"49ed0c17f8090f56d13ec656c807a71d29373474","venue_1":"ACM Multimedia","year":"2012","title":"DisplayCast: a high performance screen sharing system for intranets","authors":"Surendar Chandra, Lawrence A. Rowe","author_ids":"1717644, 1723155","abstract":"DisplayCast is a many to many screen sharing system that is targeted towards Intranet scenarios. The capture software runs on all computers whose screens need to be shared. It uses an application agnostic screen capture mechanism that creates a sequence of pixmap images of the screen updates. It transforms these pixmaps to vastly improve the lossless Zlib compression performance. These algorithms were developed after an extensive analysis of typical screen contents. DisplayCast shares the processor and network resources required for screen capture, compression and transmission with host applications whose output needs to be shared. It balances the need for high performance screen capture with reducing its resource interference with user applications. DisplayCast uses Zeroconf for naming and asynchronous location. It provides support for Cisco WiFi and Bluetooth based localization. It also includes a HTTP/REST based controller for remote session initiation and control. DisplayCast supports screen capture and playback in computers running Windows 7 and Mac OS X operating systems. Remote screens can be archived into a H.264 encoded movie on a Mac. They can also be played back in real time on Apple iPhones and iPads. The software is released under a New BSD license.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"24f1678df744e356e08a99f9c94b775576f13700","venue_1":"ACM Multimedia","year":"2010","title":"TalkMiner: a search engine for online lecture video","authors":"John Adcock, Matthew Cooper, Laurent Denoue, Hamed Pirsiavash, Lawrence A. Rowe","author_ids":"5529541, 4268667, 1788661, 2367683, 1723155","abstract":"TalkMiner is a search engine for lecture webcasts. Lecture videos are processed to recover a set of distinct slide images and OCR is used to generate a list of indexable terms from the slides. On our prototype system, users can search and browse lists of lectures, slides in a specific lecture, and play the lecture video. Over 10,000 lecture videos have been indexed from a variety of sources. A public website will be published in mid 2010 that allows users to experiment with the search engine.","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"43b8f84f14315e3d8a49ba49b903864c8696e45e","venue_1":"ACM Multimedia","year":"2010","title":"Joint layered video and digital fountain coding for multi-channel video broadcasting","authors":"Wen Ji, Zhu Li","author_ids":"6359191, 4093066","abstract":"In this paper, we consider a scenario where multiple video content channels are broadcasted to a set of heterogeneous mobile users with diverse display devices and different channel conditions. The objective is to design a joint coding and rate allocation algorithm which achieves maximum overall receiving quality of the heterogeneous users, measured by broadcasting utility. We use hierarchy optimization to solve this problem, and decompose the problem into a two-tier solution: the inner loop aims at single content broadcasting, solved with a joint coding algorithm; while the outer loop focus on multiple contents broadcasting, using dynamic programming approach to find the optimal rate allocation policy. Numerical experiments demonstrate the effectiveness of the solution.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"5a2a6e26a27b78380bf3d2331d5d4ed681da06b0","venue_1":"ACM Multimedia","year":"2015","title":"Crowdsourced Multimedia Enhanced Spatio-temporal Constraint Based on-Demand Social Network for Group Mobility","authors":"Bilal Sadiq, Mohamed Abdur Rahman, Abdullah Murad, Muhammad Shahid, Faizan Ur Rehman, Ahmed Lbath, Akhlaq Ahmad, Ahmad M. Qamar","author_ids":"2829604, 1968371, 2813657, 5617594, 3300095, 1733279, 3248668, 2999776","abstract":"This paper presents a system that enables efficient and scalable real-time user and vehicle discovery using textual, audio and video mechanisms. The system allows users to group together for shared intra-city transportation with the aid of multimedia that helps individuals to 1) find community of common interest (CoCI), 2) locate individual users in a large crowd and 3) locate vehicles for mobility in an efficient and cost effective manner. The system is a pilot project and will be deployed during Hajj 2015 when over three million pilgrims from all over the world visit Makkah, Saudi Arabia.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"58e82bbcc15386f07f8a57d720e992d98c41d965","venue_1":"ACM Multimedia","year":"2015","title":"A Semantic Geo-Tagged Multimedia-Based Routing in a Crowdsourced Big Data Environment","authors":"Faizan Ur Rehman, Ahmed Lbath, Abdullah Murad, Mohamed Abdur Rahman, Bilal Sadiq, Akhlaq Ahmad, Ahmad M. Qamar, Saleh M. Basalamah","author_ids":"3300095, 1733279, 2813657, 1968371, 2829604, 3248668, 2999776, 3087877","abstract":"Traditional routing algorithms for calculating the fastest or shortest path become ineffective or difficult to use when both source and destination are dynamic or unknown. To solve the problem, we propose a novel semantic routing system that leverages geo-tagged rich crowdsourced multimedia information such as images, audio, video and text to add semantics to the conventional routing. Our proposed system includes a Semantic Multimedia Routing Algorithm (SMRA) that uses an indexed spatial big data environment to answer multimedia spatio-temporal queries in real-time. The results are customized to the users' smartphone bandwidth and resolution requirements. The system has been designed to be able to handle a very large number of multimedia spatio-temporal requests at any given moment. A proof of concept of the system will be demonstrated through two scenarios. These are 1) multimedia enhanced routing and 2) finding lost individuals in a large crowd using multimedia. We plan to test the system's performance and usability during Hajj 2015, where over four million pilgrims from all over the world gather to perform their rituals.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"1ea18eff7fa10938bc96b4c0f07469214ed4da6d","venue_1":"ACM Multimedia","year":"2011","title":"Job opportunities and career perspective for fresh graduates of the multimedia community","authors":"Yu-Ru Lin, Vincent Oria, K. Selçuk Candan, Lyndon Kennedy, Dulce B. Ponceleon, Hari Sundaram, Rong Yan, Roger Zimmermann","author_ids":"2133006, 1693176, 1720972, 4591648, 1802641, 8607462, 1767526, 1790974","abstract":"The future of multimedia community depends on how the community effectively and efficiently recruits, nurtures and retains young talents. Students tends to decide on their majors based on job opportunities and the main question in every student mind while finishing a degree is \"which jobs are out there for me?\" In this panel, we have gathered people from both academia and industry to discuss job opportunities and career perceptive. The panel will try to basically answer two main questions: (1) Which are the jobs for the fresh graduates of our community? (2) What are the carrier paths in both academia and industry?","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"4d4ebf24a532a3d20850498aa21eb7f8b34682a4","venue_1":"ACM Multimedia","year":"2011","title":"Consensus-based cross-correlation","authors":"Florian Schweiger, Georg Schroth, Michael Eichhorn, Eckehard G. Steinbach, Michael Fahrmair","author_ids":"2119331, 1701294, 2936409, 7252930, 1769863","abstract":"Cross-correlation is a classical similarity measure with broad applications in multimedia signal processing. While it is robust against uncorrelated noise in the input signals, it is severely affected by systematic disturbances which lead to biased results. To overcome this limitation, we propose in this paper consensus-based cross-correlation (ConCor) to deal with heavily corrupted signal parts that derail regular cross-correlation. ConCor builds upon the widely adopted RANSAC algorithm to reliably identify and eliminate corrupt signal parts at limited additional complexity. Our approach is universal in that it can be combined with existing cross-correlation variants. We apply ConCor in two example applications, namely video synchronization and template matching. Our experimental results demonstrate the improved robustness and accuracy when compared to classical cross-correlation.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"d03ac9942792b8f050a9284938eef1967c3656d2","venue_1":"ACM Multimedia","year":"2000","title":"A digital television navigator","authors":"Chengyuan Peng, Petri Vuorimaa","author_ids":"1713109, 1704368","abstract":"Digital television is a new, interesting, and rich platform for developing next generation multimedia services. One of the key digital television standards is Multimedia Home Platform (MHP) which includes hardware devices and software architecture. Navigator is the most important multimedia service of digital television. It acts the main index of all the services available in set-top box. In this paper we presented a Java Navigator developed in Future TV project.","cites":"12","conferencePercentile":"67.39130435"},{"venue":"ACM Multimedia","id":"2b33d44b5b633159dd91c220ba47553c5913d30b","venue_1":"ACM Multimedia","year":"2013","title":"Tele echo tube: beyond cultural and imaginable boundaries","authors":"Hill Hiroki Kobayashi, Michitaka Hirose, Akio Fujiwara, Kazuhiko Nakamura, Kaoru Sezaki, Kaoru Saito","author_ids":"2315764, 1709460, 1981056, 8320695, 1741395, 2837049","abstract":"Currently, human-computer interaction (HCI) is primarily focused on human-centric interactions; however, people experience many nonhuman-centric interactions during the course of a day. Interactions with nature, such as experiencing the sounds of birds or trickling water, can imprint the beauty of nature in our memories. In this context, this paper presents an interface of such nonhuman interactions to observe people's reaction to the interactions through an imaginable interaction with a mythological creature. Tele Echo Tube (TET) is a speaking tube interface that acoustically interacts with a deep mountain echo through the slightly vibrating lampshade-like interface. TET allows users to interact with the mountain echo in real time through an augmented echo-sounding experience with the vibration over a satellite data network. This novel interactive system can create an imaginable presence of the mythological creature in the undeveloped natural locations beyond our cultural and imaginable boundaries. The results indicate that users take the reflection of the sound as a cue that triggers the nonlinguistic believability in the form of the mythological metaphor of the mountain echo. This echo-like experience of believable interaction in an augmented reality between a human and nature gave the users an imaginable presence of the mountain echo with a high degree of excitement. This paper describes the development and integration of nonhuman-centric design protocols, requirements, methods, and context evaluation.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"65888dd86d62b95391478d671265aaa624d384c9","venue_1":"ACM Multimedia","year":"2008","title":"Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video","authors":"Gang Wang, Tat-Seng Chua, Ming Zhao","author_ids":"4148672, 1684968, 1681670","abstract":"In this paper, we present a model based on a multi-resolution, multi-source and multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for concept detection in news video. Because the characteristics and distributions of data in different sub-domains are different, we model and analyze the video in each sub-domain separately using a transductive framework. Along with this framework, we propose a \"pseudo-Vapnik combined error bound\" to tackle the problem of imbalanced distribution of training data in certain segments of sub-domains. For effective fusion of multi-modal features, we utilize multi-resolution inference and constraints to permit evidences from different modal features to support each other. Finally, we employ a bootstrapping technique to leverage unlabeled data to boost the overall system performance. We test our framework by detecting semantic concepts in the TRECVID 2004 dataset. Experimental results demonstrate that our approach is effective.","cites":"9","conferencePercentile":"65.36697248"},{"venue":"ACM Multimedia","id":"1a4ac68794e1b4daef9e6f0615a2c7af46ff4168","venue_1":"ACM Multimedia","year":"2008","title":"Snap and share your photobooks","authors":"Niels Henze, Susanne Boll","author_ids":"1801804, 1714281","abstract":"The sharing of photos with others, friends and family, has become even more popular with digital photography and Internet applications such as email or Web albums. At the same time, the physical touch of printed photos is still appreciated and customers use different services to print their photos on post cards, calendars or photobooks, often to give them as a present or to create a physical souvenir. Once printed, however, the sharing of photos with others becomes difficult as there is no link back from the now physical item to its digital counterpart. With Bookmarkr, we developed a system that employs a mobile camera phone to bridge the gap between the printed photo and its digital counterpart. The user takes an image of a photo in a photobook with the mobile phone's camera. The image is transmitted to a photobook server, which employs image analysis techniques to retrieve the corresponding photo. The photo is sent back to the user and establishes the digital-physical match. Bookmarkr allows the user to interact with printed photos in a similar way as interacting with digital photos by using a point-and-shoot metaphor. Our performance evaluation shows that the system is able to return the correct photo in up to 99% of all cases. A conducted user study revealed that the designed interaction is suitable for the participants and their printed photobooks.","cites":"17","conferencePercentile":"77.98165138"},{"venue":"ACM Multimedia","id":"9ae9805e53303861804fd681aec1f873955fa6c3","venue_1":"ACM Multimedia","year":"2011","title":"A system for reconstructing multiparty conversation field based on augmented head motion by dynamic projection","authors":"Kazuhiro Otsuka, Kamil Sebastian Mucha, Shiro Kumano, Dan Mikami, Masafumi Matsuda, Junji Yamato","author_ids":"1713258, 2095920, 3325574, 1993201, 2702771, 1710918","abstract":"A novel system is presented for reconstructing, in the real world, multiparty face-to-face conversation scenes; it uses dynamics projection to augment human head motion. This system aims to display and playback pre-recorded conversations to the viewers as if the remote people were taking in front of them. This system consists of multiple projectors and transparent screens. Each screen separately displays the life-size face of one meeting participant, and are spatially arranged to recreate the actual scene. The main feature of this system is dynamics projection, screen pose is dynamically controlled to emulate the head motions of the participants, especially rotation around the vertical axis, that are typical of shifts in visual attention, i.e. turning gaze from one to another. This recreation of head motion by physical screen motion, in addition to image motion, aims to more clearly express the interactions involving visual attention among the participants. The minimal design, frameless-projector-screen, with augmented head motion is expected to create a feeling that the remote participants are actually present in the same room. This demo presents our initial system and discusses its potential impact on future visual communications.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"041e102b11b6294119d3b71d3e6cfdf635db00e8","venue_1":"ACM Multimedia","year":"2009","title":"Routine classification through sequence alignment","authors":"Driss Choujaa, Naranker Dulay","author_ids":"1987068, 1933957","abstract":"In this paper we draw a methodological connection between human routine classification and the sequence alignment problem in bioinformatics. We first observe that human days exhibit important time shifts and therefore align them for comparison prior to classification. Our technique is evaluated on bimodal data including GSM and Bluetooth information collected on mobile phones. The introduction of new alignment features is found to significantly improve the accuracy of routine classification.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"ef57b2b689cb91e0c7f9f9a2691c340349d9b2d4","venue_1":"ACM Multimedia","year":"2014","title":"Affective media and wearables: surprising findings","authors":"Rosalind W. Picard","author_ids":"1719389","abstract":"Over a decade ago, I suggested that computers will need the skills of emotional intelligence in order to interact with regular people in ways that they perceive as intelligent. Our lab embarked on this journey of 'affective computing' with a focus on first enabling computers to better understand and communicate human emotion. Our main tools have been wearable sensors (several which we created), video, and audio, coupled with signal processing, machine learning and pattern analysis of multimodal human data. Along the way we encountered several surprises. This talk will highlight some of the challenges we have faced, some accomplishments, and the most surprising and rewarding findings. Our findings reveal the power of the human emotion system not only in intelligence, in social interaction, and in everyday media consumption, but also in autism, epilepsy, and sleep memory formation.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"d27997b7ce26331dc5a5da44b62e31c7833f81f4","venue_1":"ACM Multimedia","year":"2013","title":"MagicBrush: image search by color sketch","authors":"Xinghai Sun, Changhu Wang, Avneesh Sud, Chao Xu, Lei Zhang","author_ids":"1910318, 1697065, 1906449, 2857022, 1724298","abstract":"In this paper, we showcase the MagicBrush system, a novel painting-based image search engine. This system enables users to draw a color sketch as a query to find images. Different from existing works on sketch-based image retrieval, most of which focus on matching the shape structure without carefully considering other important visual modalities, MagicBrush takes into account the indispensable value of \"color\" related to \"shape\", and explores to make use of both the shape and color expectations that users usually have when they're imaging or searching for an image. To achieve this, we 1) develop a user-friendly interface to allow users to easily \"paint out\" their colorful visual expectations; 2) design a compact feature \"color-edge word\" to encode both shape and color information in a organic way; and 3) develop a novel matching and index structure to support a real-time response in 6.4 million images. By taking into account both shape and color information, the MagicBrush system helps users to vividly present what they are imagining, and retrieve images in a more natural way.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"db457650ed5e11dd50d2f77ec7a8e2038717f6bb","venue_1":"ACM Multimedia","year":"2010","title":"TalkMiner: a lecture webcast search engine","authors":"John Adcock, Matthew Cooper, Laurent Denoue, Hamed Pirsiavash, Lawrence A. Rowe","author_ids":"5529541, 4268667, 1788661, 2367683, 1723155","abstract":"The design and implementation of a search engine for lecture webcasts is described. A searchable text index is created allowing users to locate material within lecture videos found on a variety of websites such as YouTube and Berkeley webcasts. The index is created from words on the presentation slides appearing in the video along with any associated metadata such as the title and abstract when available.\n The video is analyzed to identify a set of distinct slide images, to which OCR and lexical processes are applied which in turn generate a list of indexable terms.\n Several problems were discovered when trying to identify distinct slides in the video stream. For example, picture-in-picture compositing of a speaker and a presentation slide, switching cameras, and slide builds confuse basic frame-differencing algorithms for extracting keyframe slide images. Algorithms are described that improve slide identification.\n A prototype system was built to test the algorithms and the utility of the search engine. Users can browse lists of lectures, slides in a specific lecture, or play the lecture video. Over 10,000 lecture videos have been indexed from a variety of sources. A public website will be published in mid 2010 that allows users to experiment with the search engine.","cites":"26","conferencePercentile":"90.4109589"},{"venue":"ACM Multimedia","id":"d817729f2fc8509d138f8db73ef0ca519615c8fe","venue_1":"ACM Multimedia","year":"2013","title":"Indexing billions of images for sketch-based retrieval","authors":"Xinghai Sun, Changhu Wang, Chao Xu, Lei Zhang","author_ids":"1910318, 1697065, 2857022, 1724298","abstract":"Because of the popularity of touch-screen devices, it has become a highly desirable feature to retrieve images from a huge repository by matching with a hand-drawn sketch. Although searching images via keywords or an example image has been successfully launched in some commercial search engines of billions of images, it is still very challenging for both academia and industry to develop a sketch-based image retrieval system on a billion-level database. In this work, we systematically study this problem and try to build a system to support query-by-sketch for two billion images. The raw edge pixel and Chamfer matching are selected as the basic representation and matching in this system, owning to the superior performance compared with other methods in extensive experiments. To get a more compact feature and a faster matching, a vector-like Chamfer feature pair is introduced, based on which the complex matching is reformulated as the crossover dot-product of feature pairs. Based on this new formulation, a compact shape code is developed to represent each image/sketch by projecting the Chamfer features to a linear subspace followed by a non-linear source coding. Finally, the multi-probe Kmedoids-LSH is leveraged to index database images, and the compact shape codes are further used for fast reranking. Extensive experiments show the effectiveness of the proposed features and algorithms in building such a sketch-based image search system.","cites":"10","conferencePercentile":"86.22222222"},{"venue":"ACM Multimedia","id":"805e975f4a77643889aa094077e948b9507d0eab","venue_1":"ACM Multimedia","year":"2012","title":"A rapid flower/leaf recognition system","authors":"Xianbiao Qi, Rong Xiao, Lei Zhang, Chun-Guang Li, Jun Guo","author_ids":"2689287, 2820959, 1724298, 1698277, 1724183","abstract":"In this work, we introduce a rapid and accurate flower/leaf recognition system. The system could process one query in less than 0.35s with users' simple interaction. Meanwhile, high accuracy and recall is achieved. Furthermore, low computational resource and memory cost are required by the system. Now, the system is demonstrated on 172 categories of flowers, the largest flower dataset until now, and 220 categories of leaves.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"b3384bc89474ec27a5436c743e5980be2b3ea5a8","venue_1":"ACM Multimedia","year":"2012","title":"Sketch2Tag: automatic hand-drawn sketch recognition","authors":"Zhenbang Sun, Changhu Wang, Liqing Zhang, Lei Zhang","author_ids":"2174694, 1697065, 7137826, 1724298","abstract":"In this work, we introduce the Sketch2Tag system for hand-drawn sketch recognition. Due to large variations presented in hand-drawn sketches, most of existing work was limited to a particular domain or limited predefined classes. Different from existing work, Sketch2Tag is a general sketch recognition system, towards recognizing any semantically meaningful object that a child can recognize. This system enables a user to draw a sketch on the query panel, and then provides real-time recognition results. To increase the recognition coverage, a web-scale clipart image collection is leveraged as the knowledge base of the recognition system. Better understanding a user's drawing will be of great value to a variety of applications, such as, improving the sketch-based image search by combining the recognition results as textual queries.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"3332ad5259d2c0175f1e6b4af5bea14dc199ca2d","venue_1":"ACM Multimedia","year":"1993","title":"Programming the Multimodal Interface","authors":"Ephraim P. Glinert, Meera Blattner","author_ids":"3072281, 2384375","abstract":"a z Abstract Fundamental problems will confront those who wish to take full advantage of the power of tomorrow's multi-modal environments. We argue that our recently introduced concept of metawidget, when embedded within a high level, networked user interface server, can support the effective implementation of complex multimedia applications. We develop algorithms which enable a multi-modal system to select the \" best \" combination of representations for the various \" information packets \" in a display at any moment. If no acceptable display can be generated from the available representations, methods are needed for creating new and useful, if not beautiful , representation(s) to resolve the impasse. A running example is provided to motivate and clarify the discussion. \" The computer : : : will find its most complete and powerful means of expression in modern graphics[, which] involves the","cites":"6","conferencePercentile":"23.25581395"},{"venue":"ACM Multimedia","id":"505f31948331f6572e43c980edde064bcbf9fbff","venue_1":"ACM Multimedia","year":"2003","title":"Generation of interactive multi-level video summaries","authors":"Frank M. Shipman, Andreas Girgensohn, Lynn Wilcox","author_ids":"1749811, 2195286, 1691319","abstract":"In this paper, we describe how a detail-on-demand representation for interactive video is used in video summarization. Our approach automatically generates a hypervideo composed of multiple video summary levels and navigational links between these summaries and the original video. Viewers may interactively select the amount of detail they see, access more detailed summaries, and navigate to the source video through the summary. We created a representation for interactive video that supports a wide range of interactive video applications and Hyper-Hitchcock, an editor and player for this type of interactive video. Hyper-Hitchcock employs methods to determine (1) the number and length of levels in the hypervideo summary, (2) the video clips for each level in the hypervideo, (3) the grouping of clips into composites, and (4) the links between elements in the summary. These decisions are based on an inferred quality of video segments and temporal relations those segments.","cites":"26","conferencePercentile":"73.42342342"},{"venue":"ACM Multimedia","id":"08c6e504dd497c1ac4d022eb34fd8f78e3b4b160","venue_1":"ACM Multimedia","year":"2003","title":"Shared interactive video for teleconferencing","authors":"Chunyuan Liao, Qiong Liu, Don Kimber, Patrick Chiu, Jonathan Foote, Lynn Wilcox","author_ids":"2686363, 1794500, 2178004, 2895008, 1797460, 1691319","abstract":"We present a system that allows remote and local participants to control devices in a meeting environment using mouse or pen based gestures \"through\" video windows. Unlike state-of-the-art device control interfaces that require interaction with text commands, buttons, or other artificial symbols, our approach allows users to interact with devices through live video of the environment. This naturally extends our video supported pan/tilt/zoom (PTZ) camera control system, by allowing gestures in video windows to control not only PTZ cameras, but also other devices visible in video images. For example, an authorized meeting participant can show a presentation on a screen by dragging the file on a personal laptop and dropping it on the video image of the presentation screen. This paper presents the system architecture, implementation tradeoffs, and various meeting control scenarios.","cites":"14","conferencePercentile":"58.10810811"},{"venue":"ACM Multimedia","id":"984dc26fe54664aaf4ac4fe930321996c6d79f14","venue_1":"ACM Multimedia","year":"2003","title":"Hyper-hitchcock: authoring interactive videos and generating interactive summaries","authors":"Andreas Girgensohn, Frank M. Shipman, Lynn Wilcox","author_ids":"2195286, 1749811, 1691319","abstract":"To simplify the process of editing interactive video, we developed the concept of \"detail-on-demand\" video as a subset of general hypervideo. Detail-on-demand video keeps the authoring and viewing interfaces relatively simple while supporting a wide range of interactive video applications. Our editor, Hyper-Hitchcock, provides a direct manipulation environment in which authors can combine video clips and place hyperlinks between them. To summarize a video, Hyper-Hitchcock can also automatically generate a hypervideo composed of multiple video summary levels and navigational links between these summaries and the original video. Viewers may interactively select the amount of detail they see, access more detailed summaries, and navigate to the source video through the summary.","cites":"6","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"0a42838fc154eba97a37917f099d06f786857bdb","venue_1":"ACM Multimedia","year":"2012","title":"Towards indexing representative images on the web","authors":"Xin-Jing Wang, Zheng Xu, Lei Zhang, Ce Liu, Yong Rui","author_ids":"3349534, 7814531, 1724298, 1681442, 1728806","abstract":"Even after 20 years of research on real-world image retrieval, there is still a big gap between what search engines can provide and what users expect to see. To bridge this gap, we present an image knowledge base, ImageKB, a graph representation of structured entities, categories, and representative images, as a new basis for practical image indexing and search. ImageKB is automatically constructed via a both bottom-up and top-down, scalable approach that efficiently matches 2 billion web images onto an ontology with millions of nodes. Our approach consists of identifying duplicate image clusters from billions of images, obtaining a candidate set of entities and their images, discovering definitive texts to represent an image and identifying representative images for an entity. To date, ImageKB contains 235.3M representative images corresponding to 0.52M entities, much larger than the state-of-the-art alternative ImageNet that contains 14.2M images for 0.02M synsets. Compared to existing image databases, ImageKB reflects the distributions of both images on the web and users' interests, contains rich semantic descriptions for images and entities, and can be widely used for both text to image search and image to text understanding.","cites":"8","conferencePercentile":"81.48734177"},{"venue":"ACM Multimedia","id":"34b7dde92f7f3f26be9d4fb9ea009cdbf87091c3","venue_1":"ACM Multimedia","year":"2012","title":"Query-adaptive shape topic mining for hand-drawn sketch recognition","authors":"Zhenbang Sun, Changhu Wang, Liqing Zhang, Lei Zhang","author_ids":"2174694, 1697065, 7137826, 1724298","abstract":"In this work, we study the problem of hand-drawn sketch recognition. Due to large intra-class variations presented in hand-drawn sketches, most of existing work was limited to a particular domain or limited pre-defined classes. Different from existing work, we target at developing a general sketch recognition system, to recognize any semantically meaningful object that a child can recognize. To increase the recognition coverage, a web-scale clipart image collection is leveraged as the knowledge base of the recognition system. To alleviate the problems of <i>intra-class shape variation</i> and <i>inter-class shape ambiguity</i> in this unconstrained situation, a query-adaptive shape topic model is proposed to mine object topics and shape topics related to the sketch, in which, multiple layers of information such as sketch, object, shape, image, and semantic labels are modeled in a generative process. Besides sketch recognition, the proposed topic model can also be used for related applications such as sketch tagging, image tagging, and sketch-based image search. Extensive experiments on different applications show the effectiveness of the proposed topic model and the recognition system.","cites":"12","conferencePercentile":"88.76582278"},{"venue":"ACM Multimedia","id":"a47f0b8bbeb34734ca4584ead35a96832d698435","venue_1":"ACM Multimedia","year":"2011","title":"Identifying authoritative sources of multimedia content: mining specificity and expertise from large-scale multimedia databases","authors":"Lyndon Kennedy, Malcolm Slaney","author_ids":"4591648, 1725788","abstract":"We present a framework for identifying authoritative sources (such as web sites or individual users) that are likely to produce high-quality or interesting images. We construct a directed graph across sources based on the propensity of one source to \"cite\" the content from another. A graph-centrality measure scores the authority for each source, which could then be applied for retrieval purposes. We apply this method to web image retrieval, where web sites are the sources, and citations are found via copy detection; and on a photo sharing site, where individuals are the sources and citations are users' favorites. We are able to identify primary or influential sources of media while avoiding the computational cost of other approaches.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"0c702f630782f95d95c5b95d41b62920f2976f45","venue_1":"ACM Multimedia","year":"2003","title":"Computation and performance issues In coliseum: an immersive videoconferencing system","authors":"H. Harlyn Baker, Nina T. Bhatti, Donald Tanguay, Irwin Sobel, Dan Gelb, Michael E. Goss, John MacCormick, Kei Yuasa, W. Bruce Culbertson, Thomas Malzbender","author_ids":"1939188, 2249002, 1993315, 2246837, 1730158, 2877539, 1698422, 1940661, 1774215, 1682399","abstract":"Coliseum is a multiuser immersive remote teleconferencing system designed to provide collaborative workers the experience of face-to-face meetings from their desktops. Five cameras are attached to each PC display and directed at the participant. From these video streams, view synthesis methods produce arbitrary-perspective renderings of the participant and transmit them to others at interactive rates, currently about 15 frames per second. Combining these renderings in a shared synthetic environment gives the appearance of having all participants interacting in a common space. In this way, Coliseum enables users to share a virtual world, with acquired-image renderings of their appearance replacing the synthetic representations provided by more conventional avatar-populated virtual worlds. The system supports virtual mobility--participants may move around the shared space--and reciprocal gaze, and has been demonstrated in collaborative sessions of up to ten Coliseum workstations, and sessions spanning two continents. This paper summarizes the technology, and reports on issues related to its performance.","cites":"19","conferencePercentile":"65.31531532"},{"venue":"ACM Multimedia","id":"aa09b39a40f8099a8f0604ac25e3ae038cc6b9b0","venue_1":"ACM Multimedia","year":"2010","title":"Ink jet olfactory display enabling instantaneous switches of scents","authors":"Sayumi Sugimoto, Daisuke Noguchi, Yuichi Bannai, Ken-ichi Okada","author_ids":"3349821, 2550027, 1968260, 1695617","abstract":"Trials on transmitting olfactory information together with audio/visual information are currently being conducted in the field of multimedia. However, continuous emission of scents creates problems of olfactory adaptations and scents lingering in the air. To overcome these problems, we developed an ink-jet olfactory display. This display has high emission control so that it can provide stable pulse emission of scents. Humans detect the scents when they breathe in and inhale scents molecules in the air. Therefore, it is important to synchronize the pulse ejection of scent presentation with the inspiration. With that, by using the pulse ejection of scents, we constructed the pulse ejection pattern what enables instantaneous switch of scents. We first measured the responding time to sense the shift of scents in order to achieve the pulse ejection pattern suited for switching the scents. Then, by using the pattern found in first experiment, we measured the shortest period of scent switching. At last, we measured the limit of switching scents. As a result, we constructed the presenting pattern of scent switching and the limit of scent switching when the scent switching was presented with its shortest period. It is expected that using this developed pattern with movies would raise the realistic sensations.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"87c73996726db8b0f3e3c2cec5c83daff3da7d73","venue_1":"ACM Multimedia","year":"2011","title":"Multi-feature pLSA for combining visual features in image annotation","authors":"Rui Zhang, Lei Zhang, Xin-Jing Wang, Ling Guan","author_ids":"1683234, 1724298, 3349534, 1748180","abstract":"We study in this paper the problem of combining low-level visual features for image region annotation. The problem is tackled with a novel method that combines texture and color features via a mixture model of their joint distribution. The structure of the presented model can be considered as an extension of the probabilistic latent semantic analysis (pLSA) in that it handles data from two different visual feature domains by attaching one more leaf node to the graphical structure of the original pLSA. Therefore, the proposed approach is referred to as multi-feature pLSA (MF-pLSA). The supervised paradigm is adopted to classify a new image region into one of a few pre-defined object categories using the MF-pLSA. To evaluate the performance, the VOC2009 and LabelMe databases were employed in our experiments, along with various experimental settings in terms of the number of visual words and mixture components. Evaluated based on the average recall and precision, the MF-pLSA is demonstrated superior to seven other approaches, including other schemes for visual feature combination.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"d7bcdef54f58ed6d9fc1fbb9d97dd6e05da43bd6","venue_1":"ACM Multimedia","year":"1998","title":"Exploiting Temporal Parallelism for Software-Only Video Effects Processing","authors":"Ketan Mayer-Patel, Lawrence A. Rowe","author_ids":"3715598, 1723155","abstract":"Internet video is emerging as an important multime-dia application area. Although development and use of video applications is increasing, the ability to manipulate and process video is missing within this application area. Current video eeects processing solutions are not well matched for the Internet video environment. A software-only solution, however, provides enough exibility to match the constraints and needs of a particular video application. The key to a software solution is exploiting parallelism. This paper presents the design of a parallel software-only video eeects processing system. Preliminary experimental results exploring the use of temporal parallelism are presented.","cites":"13","conferencePercentile":"41.34615385"},{"venue":"ACM Multimedia","id":"0e05fd4e501a920080c854442bbd11743fc9cb6c","venue_1":"ACM Multimedia","year":"2006","title":"Talk2Me: the art of augmenting conversations","authors":"Ann Morrison, Peta Mitchell, Ralf Mühlberger","author_ids":"2512884, 2256777, 2151843","abstract":"This paper describes an interactive installation work set in a large dome space. The installation is an audio and physical re-rendition of an interactive writing work. In the original work, the user interacted via keyboard and screen while online. This rendition of the work retains the online interaction, but also places the interaction within a physical space, where the main 'conversation' takes place by the participant-audience speaking through microphones and listening through headphones. The work now also includes voice and SMS input, using speech-to-text and text-to-speech conversion technologies, and audio and displayed text for output. These additions allow the participant-audience to co-author the work while they participate in audible conversation with keyword-triggering characters (bots). Communication in the space can be person-to-computer via microphone, keyboard, and phone; person-to-person via machine and within the physical space; computer-to-computer; and computer-to-person via audio and projected text.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"0865acb900aefa2a7659cc80b679475af3f70e3b","venue_1":"ACM Multimedia","year":"2004","title":"A reversible color transform for 16-bit-color picture coding","authors":"Na Li, Jiajun Bu, Chun Chen","author_ids":"1748656, 8475311, 5371645","abstract":"This paper proposes a reversible color transform for 16-bit-color (hicolor) picture coding. The work is motivated by the increasing needs of multimedia applications on low-end devices such as mobile phones and PDAs. They have limited resources and up to 16-bit displays. Current image/video coding systems can hardly manage this case effectively. To enhance coding efficiency on this condition, a reversible color transform customized for hicolor systems is derived from Y'CrCb and JPEG2000 Reversible Component Transformation (RCT). The transform proves simple but highly-decorrelating, and able to reduce the computation time of decoding. Comparison experiment demonstrates the effectiveness of this transform with equal or even higher coding efficiency on low-end devices with 16-bit display mode.","cites":"0","conferencePercentile":"7.107843137"},{"venue":"ACM Multimedia","id":"7c36afc9828379de97f226e131390af719dbc18d","venue_1":"ACM Multimedia","year":"2012","title":"Unsupervised face-name association via commute distance","authors":"Jiajun Bu, Bin Xu, Chenxia Wu, Chun Chen, Jianke Zhu, Deng Cai, Xiaofei He","author_ids":"8475311, 3448335, 2484982, 5371645, 1704030, 1745280, 3945955","abstract":"Recently, the task of unsupervised face-name association has received a considerable interests in multimedia and information retrieval communities. It is quite different with the generic facial image annotation problem because of its unsupervised and ambiguous assignment properties. Specifically, the task of face-name association should obey the following three constraints: (1) a face can only be assigned to a name appearing in its associated caption or to null; (2) a name can be assigned to at most one face; and (3) a face can be assigned to at most one name. Many conventional methods have been proposed to tackle this task while suffering from some common problems, eg, many of them are computational expensive and hard to make the null assignment decision. In this paper, we design a novel framework named face-name association via commute distance (FACD), which judges face-name and face-null assignments under a unified framework via commute distance (CD) algorithm. Then, to further speed up the on-line processing, we propose a novel anchor-based commute distance (ACD) algorithm whose main idea is using the anchor point representation structure to accelerate the eigen-decomposition of the adjacency matrix of a graph. Systematic experiment results on a large scale and real world image-caption database with a total of 194,046 detected faces and 244,725 names show that our proposed approach outperforms many state-of-the-art methods in performance. Our framework is appropriate for a large scale and real-time system.","cites":"5","conferencePercentile":"70.88607595"},{"venue":"ACM Multimedia","id":"504844223c6f89f48a3a58b73add73ca854a6479","venue_1":"ACM Multimedia","year":"2008","title":"EasyToon: cartoon personalization using face photos","authors":"Fang Wen, Shifeng Chen, Xiaoou Tang","author_ids":"1716835, 2869725, 1741901","abstract":"In this demo, we present a family photo album based cartoon personalization system, EasyToon. Using the family photo album as the candidate pool, a personalized cartoon image is obtained in two main steps. First, the best face candidate is selected from the album interactively. Then a personalized cartoon image is automatically synthesized by lending the selected face into the target cartoon image. By integrating state of the art computer vision and graphics technologies and effective UI design EasyToon can generate a personalized cartoon storyboard easily and quickly.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"207bb4db8cbf324912f5490f9a476b70549967f7","venue_1":"ACM Multimedia","year":"2002","title":"M-Studio: an authoring application for context-aware multimedia","authors":"Pengkai Pan, Carly Kastner, David Crowe, Glorianna Davenport","author_ids":"2002873, 2874757, 8007147, 8295627","abstract":"Broadband wireless networks coupled with handheld computers and appropriate sensing technologies provide a channel for the delivery of mobile cinema. Mobile cinema changes the consumer experience of motion picture stories in that discrete cinematic sequences are delivered based on the consumer's location and the a story-real-time metric. The M-Studio authoring tool helps mobile story creators design, simulate and adjust mobile narratives. The tool provides the author with a graphical manipulation interface for linking content with a specific geographical space and a simulator allows the author to evaluate and iterate the content for continuity of story threads as they may be presented. The tool directly generates the code that is required for the server to deliver the cinematic sequences appropriately. This tool is discussed in the context of the two mobile narratives that have been created.","cites":"10","conferencePercentile":"53.84615385"},{"venue":"ACM Multimedia","id":"0cd29fbdb577ee59535cf6ca1f68cc520a7211ed","venue_1":"ACM Multimedia","year":"2006","title":"Sensitivity analysis: unexpected outcomes in art and engineering","authors":"Kenneth Y. Goldberg","author_ids":"1733062","abstract":"Contemporary art and engineering research are both at their best when things don't turn out as planned. I'll present selected examples based on artworks developed with students and other collaborators involving robots and networks over the past 20 years. These projects set out to investigate intersections of technology and nature, such as the Telegarden, a robot installation that allowed online participants to remotely tend a living garden; Ballet Mori, a classical dance performed to sounds triggered by live seismic data; and Demonstrate, where an ultra high-resolution video camera raised eyebrows at the 40th anniversary of the Free Speech Movement. Every project led to unexpected twists and complications...I'll also argue that the languages of contemporary art and engineering research are complex, dynamic, and often frustratingly impenetrable to outsiders. In art, a blue disk can be a cliche, or, in the right place at the right time, profound. In engineering, analogous contexts determine the beauty of a coordinate frame or mathematical equation. In both spheres, aesthetic interpretation is based on knowledge of prior art and contemporary dialogues. Being so similar, it is not surprising that unexpected forces arise when these two spheres are brought together.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"961dd9de7e901149e4d6f395430e8ae3e4b71cda","venue_1":"ACM Multimedia","year":"2009","title":"Generating location overviews with images and tags by mining user-generated travelogues","authors":"Qiang Hao, Rui Cai, Xin-Jing Wang, Jiang-Ming Yang, Yanwei Pang, Lei Zhang","author_ids":"7825219, 6306010, 3349534, 7788742, 2286218, 1724298","abstract":"Automatically generating location overviews in the form of both visual and textual descriptions is highly desired for online services such as travel planning, to provide attractive and comprehensive outlines of travel destinations. Actually, user-generated content (e.g., travelogues) on the Web provides abundant information to various aspects (e.g., landmarks, styles, activities) of most locations in the world. To leverage the experience shared by Web users, in this paper we propose a location overview generation approach, which first mines location-representative tags from travelogues and then uses such tags to retrieve web images. The learnt tags and retrieved images are finally presented via a novel user interface which provides an informative overview for a given location. Experimental results based on 23,756 travelogues and evaluation over 20 locations show promising results on both travelogue mining and location overview generation.","cites":"21","conferencePercentile":"86.57024793"},{"venue":"ACM Multimedia","id":"fa15f1e7d84d5cc4ff0fcfe5be650384041ebc33","venue_1":"ACM Multimedia","year":"2003","title":"The co-opticon: shared access to a robotic streaming video camera","authors":"Dezhen Song, Kenneth Y. Goldberg","author_ids":"2302416, 1733062","abstract":"The \"co-opticon\" is a robotic pan, tilt, and zoom streaming video camera controlled by simultaneous frame requests from remote users. Robotic webcameras are commercially available but currently restrict control to only one user at a time. The co-opticon introduces a new interface that allows simultaneous control by many users. We will demonstrate the implemented system using a Java-based interface at the conference linked via the Internet to a camera on the UC Berkeley campus. We will also discuss system architecture and several new algorithms we've developed to compute optimal camera paramters based on user frame requests. The co-opticon can be tested online at: www.tele-actor.net/co-opticon.","cites":"1","conferencePercentile":"11.26126126"},{"venue":"ACM Multimedia","id":"b96444d36a15cf3314c621b7197f4c41f5b5a579","venue_1":"ACM Multimedia","year":"2010","title":"Supporting children's social communication skills through interactive narratives with virtual characters","authors":"Mary Ellen Foster, Katerina Avramides, Sara Bernardini, Jingying Chen, Christopher Frauenberger, Oliver Lemon, Kaska Porayska-Pomsta","author_ids":"2644188, 1715292, 2282728, 4118522, 2043489, 1782798, 1788253","abstract":"The development of social communication skills in children relies on multimodal aspects of communication such as gaze, facial expression, and gesture. We introduce a multimodal learning environment for social skills which uses computer vision to estimate the children's gaze direction, processes gestures from a large multi-touch screen, estimates in real time the affective state of the users, and generates interactive narratives with embodied virtual characters. We also describe how the structure underlying this system is currently being extended into a general framework for the development of interactive multimodal systems.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"4c7d539d8c53dad46fc1b672962eda3923fd5ee0","venue_1":"ACM Multimedia","year":"2003","title":"Synchronization of lecture videos and electronic slides by video text analysis","authors":"Feng Wang, Chong-Wah Ngo, Ting-Chuen Pong","author_ids":"1745827, 1751681, 1681882","abstract":"An essential goal of structuring lecture videos captured in live presentation is to provide a synchronized view of video clips and electronic slides. This paper presents an automatic approach to match video clips and slides based on the analysis of text embedded in lecture videos. We describe a method to reconstruct high-resolution video texts from multiple keyframes for robust OCR recognition. A two-stage matching algorithm based on the title and content similarity measures between video clips and slides is also proposed.","cites":"28","conferencePercentile":"74.77477477"},{"venue":"ACM Multimedia","id":"7d33c78ed39c99ccaf80c9d0f79edd486e9ac2d5","venue_1":"ACM Multimedia","year":"2005","title":"Exploiting self-adaptive posture-based focus estimation for lecture video editing","authors":"Feng Wang, Chong-Wah Ngo, Ting-Chuen Pong","author_ids":"1745827, 1751681, 1681882","abstract":"Head pose plays a special role in estimating a presenter's focuses and actions for lecture video editing. This paper presents an efficient and robust head pose estimation algorithm to cope with the new challenges arising in the content management of lecture videos. These challenges include speed requirement, low video quality, variant presenting styles and complex settings in modern classrooms. Our algorithm is based on a robust hierarchical representation of skin color clustering and a set of pose templates that are automatically trained. Contextual information is also considered to refine pose estimation. Most importantly, we propose an online learning approach to deal with different presenting styles, which has not been addressed before. We show that the proposed approach can significantly improve the performance of pose estimation. In addition, we also describe how posture is used in focus estimation for lecture video editing by integrating with gesture.","cites":"3","conferencePercentile":"28.96039604"},{"venue":"ACM Multimedia","id":"2ff5624baf529b194727dfa5abaeaf614a55fda6","venue_1":"ACM Multimedia","year":"2006","title":"Interactive mosaic generation for video navigation","authors":"Kihwan Kim, Irfan A. Essa, Gregory D. Abowd","author_ids":"3736059, 1714295, 1732524","abstract":"Navigation through large multimedia collections that include videos and images still remains cumbersome. In this paper, we introduce a novel method to visualize and navigate through the collection by creating a mosaic image that visually represents the compilation. This image is generated by a labeling-based layout algorithm using various sizes of sample tile images from the collection. Each tile represents both the photographs and video files representing scenes selected by matching algorithms. This generated mosaic image provides a new way for thematic video and visually summarizes the videos. Users can generate these mosaics with some predefined themes and layouts, or base it on the results of their queries. Our approach supports automatic generation of these layouts by using meta-information such as color, time-line and existence of faces or manually generated annotated information from existing systems (e.g., the Family Video Archive).","cites":"8","conferencePercentile":"62.95336788"},{"venue":"ACM Multimedia","id":"18d0a46f6a9314cc79286bb08dd5fa59ae5329cc","venue_1":"ACM Multimedia","year":"1998","title":"Avatar Creation Using Automatic Face Processing","authors":"Michael J. Lyons, Andre Plante, Sebastien Jehan, Seiki Inoue, Shigeru Akamatsu","author_ids":"1709339, 1968344, 2978018, 1776120, 7322751","abstract":"In tie context of multimedia, an avatar is the visual representation of the self in a virtual world. It is desirable to incorporate personal information, such as an image of fac~ about the user into the avatar. To Wi end we have developed an algorithm which can autom3tid1y extract a face from an imagq modify it characterize it in terms of high-level propertiw, and apply it to the creation of a persontized avatar. The algorithm h= been tested on several hundred facial imag=, including many taken under uncontro~ed acquisition conditions, and found to efilbit satisfactory performmce for immedi3te practid use.","cites":"4","conferencePercentile":"14.42307692"},{"venue":"ACM Multimedia","id":"f3c5777acc68092e816208abbdb25a5c0040b4a4","venue_1":"ACM Multimedia","year":"2006","title":"PhotoArcs: Ludic tools for sharing photographs","authors":"Morgan Ames, Lilia Manguy","author_ids":"2165036, 1832333","abstract":"PhotoArcs is a framework for flexible creation of timeline displays of photographs, photo-narratives, and consistent, collaboratively-created metadata. The interface aims to enable easy and fun manipulation and sharing of digital photographs and stories, organized around chronological \"arcs\" of photographs, to encourage remote sharing and interaction. We describe the design of the interface and how the design has changed across three iterations. We also report on the results of an exploratory lowfidelity usability study with five participants and an expert evaluation and critique with eleven user experience researchers, and outline future directions for the PhotoArcs project.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"070ec5d14ae7bf5749d590c5e5da387c7f4dbe9f","venue_1":"ACM Multimedia","year":"2008","title":"Video event detection using motion relativity and visual relatedness","authors":"Feng Wang, Yu-Gang Jiang, Chong-Wah Ngo","author_ids":"1745827, 1717861, 1751681","abstract":"Event detection plays an essential role in video content analysis. However, the existing features are still weak in event detection because: i) most features just capture what is involved in an event or how the event evolves separately, and thus cannot completely describe the event; ii) to capture event evolution information, only motion distribution over the whole frame is used which proves to be noisy in unconstrained videos; iii) the estimated object motion is usually distorted by camera movement. To cope with these problems, in this paper, we propose a new motion feature, namely Expanded Relative Motion Histogram of Bag-of-Visual-Words (ERMH-BoW) to employ motion relativity and visual relatedness for event detection. In ERMH-BoW, by representing what aspect of an event with Bag-of-Visual-Words (BoW), we construct relative motion histograms between visual words to depict the object activities or how aspect of the event. ERMH-BoW thus integrates both what and how aspects for a complete event description. Instead of motion distribution features, local motion of visual words is employed which is more discriminative in event detection. Meanwhile, we show that by employing relative motion, ERMH-BoW is able to honestly describe object activities in an event regardless of varying camera movement. Besides, to alleviate the visual word correlation problem in BoW, we propose a novel method to expand the relative motion histogram. The expansion is achieved by diffusing the relative motion among correlated visual words measured by visual relatedness. To validate the effectiveness of the proposed feature, ERMH-BoW is used to measure video clip similarity with Earth Mover's Distance (EMD) for event detection. We conduct experiments for detecting LSCOM events in TRECVID 2005 video corpus, and performance is improved by 74% and 24% compared with existing motion distribution feature and BoW feature respectively.","cites":"59","conferencePercentile":"97.01834862"},{"venue":"ACM Multimedia","id":"3babc3a78da87f68cf8c9b884205cf7e3f1f977a","venue_1":"ACM Multimedia","year":"2014","title":"Cuteness Recognition and Localization in the Photos of Animals","authors":"Yu Bao, Jing Yang, Liangliang Cao, Haojie Li, Jinhui Tang","author_ids":"2887710, 1796912, 2464399, 7179327, 8053308","abstract":"Among the flourishing amount of photos in the social media websites, \"cute\" images of animals are particularly attractive to the Internet users. This paper considers building an automatic model which can distinguish cute images from non-cute ones. To make the recognition results more interpretable, a lot of efforts are made to find which part of the animal appears attractive to the human users. To validate the success of our proposed method, we collect three new datasets of different animals, i.e., cats, dogs, and rabbits with both cute and non-cute images. Our model obtains promising performance in distinguishing cute images from non-cute ones. Moreover, it outperforms the classical models with not only better recognition accuracy, but also more intuitive localization of the cuteness in the images. The contribution of this paper is three-fold: (1) We collect new datasets for cuteness recognition, (2) We extend the powerful Fisher Vector representation to localize cute part in the animal recognition, and (3) Extensive experimental results show that our proposed method can recognize cute animals of cats, dogs, and rabbits.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"472ecaea9faf1c018727bca32fb7d33de8df4b8e","venue_1":"ACM Multimedia","year":"2012","title":"Interactive data-driven discovery of temporal behavior models from events in media streams","authors":"Chreston A. Miller, Francis K. H. Quek","author_ids":"3027468, 1740663","abstract":"This paper investigates a technique for the discovery of temporal behavior models within multimedia event data. Advancements in both technology and the marketplace present us the opportunity for research in analysis of situated human behavior using video and other sensor data (media streams). By situated analysis, we mean the study of behavior <i>in time</i> as opposed to looking at behavior in the form of aggregated data divorced from how they occur in context. Human and social scientists seek to model behavior captured in media, and these data may be represented in a multi-dimensional event data space derived from media streams. The knowledge of these scientists (experts) is a valuable resource which can be leveraged to search this space. We propose a solution that incorporates the expert in an iteratively, interactive data-driven discovery process to evolve a desired behavior model. We test our solution's accuracy on a multimodal meeting corpus with a progressive three tiered approach.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"3b5a0bc73a92fd940db45f9a5c751059fa829693","venue_1":"ACM Multimedia","year":"1999","title":"MBase: indexing, browsing, and playback of media at FXPAL","authors":"Lynn Wilcox, Shingo Uchihashi, Andreas Girgensohn, Jonathan Foote, John S. Boreczky","author_ids":"1691319, 1713254, 2195286, 1797460, 2719487","abstract":"As video is used more and more as the official record of meetings and teleconferences, the ability to locate relevant passages or even entire meetings becomes important. To this end, we want to give users visual summaries and help them locate specific video passages quickly. Such a system is useful in any application that requires a quick overview of many videos, such as a video library or the results of a search engine. We present techniques that use automatic feature analysis, such as slide detection and applause detection, to help locate a specific video and to navigate to regions of interest within it. As part of the FXPAL MBase system, we have implemented a web-based interface (Figure 1) that graphically presents information about the contents of each video in a collection such as its keyframes and the distribution of a particular feature over time. Comic book style summarjes (Figure 2), called manga, provide single-page overviews of videos suitable for printing or web access. A media player is tightly integrated with the web interface. It supports navigation within a selected file by visualizing confidence scores for the presence of features and by using them as index points. Further descriptions of some of the aspects of this system are available in [ 1,2] and on our external web site: Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without tee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prmr specific permission and/or a fee.","cites":"4","conferencePercentile":"37.39495798"},{"venue":"ACM Multimedia","id":"525bd88d3cf1f781e52d44909073b52ef80b146e","venue_1":"ACM Multimedia","year":"1999","title":"NoteLook: taking notes in meetings with digital video and ink","authors":"Patrick Chiu, Ashutosh Kapuskar, Sarah Reitmeier, Lynn Wilcox","author_ids":"2895008, 2472902, 3319203, 1691319","abstract":"NoteLook is a client-server system designed and built to support multimedia note taking in meetings with digital video and ink. It is integrated into a conference room equipped with computer controllable video cameras, video conference camera, and a large display rear video projector. The NoteLook client application runs on wireless pen-based notebook computers. Video channels containing images of the room activity and presentation material are transmitted by the NoteLook servers to the clients, and the images can be interactively and automatically incorporated into the note pages. Users can select channels, snap in large background images and sequences of thumbnails, and write freeform ink notes. A smart video source management component enables the capture of high quality images of the presentation material from a variety of sources. For accessing and browsing the notes and recorded video, NoteLook generates Web pages with links from the images and ink strokes correlated to the video.","cites":"60","conferencePercentile":"92.43697479"},{"venue":"ACM Multimedia","id":"8f84ff8921dcaf1a2682a759f1e8e515ab0eae4a","venue_1":"ACM Multimedia","year":"2010","title":"Understanding multimedia content using web scale social media data","authors":"Dong Xu, Lei Zhang, Jiebo Luo","author_ids":"1714390, 1724298, 1717319","abstract":"Nowadays, increasingly rich and massive social media data (such as texts, images, audios, videos, blogs, and so on) are being posted to the web, including social networking websites (e.g., MySpace, Facebook), photo and video sharing websites (e.g., Flickr, YouTube), and photo forums (e.g., Photosig.com and Photo.net). Recently, researchers from multidisciplinary areas have proposed to use data-driven approaches for multimedia content understanding by leveraging such unlimited web images and videos as well as their associated rich contextual information (e.g., tag, comments, category, title and metadata). In this three hour tutorial, we plan to introduce the important general concepts and themes of this timely topic. We will also review and summarize the recent multimedia content analysis methods using web-scale social media data as well as present insight into the challenges and future directions in this area. Moreover, we will also show extensive demos on image annotation and retrieval by using rich social media data.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"05a163fe153504bec3fc4b61a1d7bb9da924f8cd","venue_1":"ACM Multimedia","year":"2006","title":"Improving the experience of controlling avatars in camera-based games using physical input","authors":"Na Li, Neema Moraveji, Hiroaki Kimura, Eyal Ofek","author_ids":"1748656, 3106937, 2408817, 1735652","abstract":"This paper investigates two methods of improving the user experience of camera-based interaction. First, problems that arise when avatars are designed to mimic a user's physical actions are presented. Second, a solution is proposed: adding a layer of separation between user and avatar while retaining intuitive user control. Two methods are proposed for this separation: spatially and temporally. Implementations of these methods are then presented in the context of a simple game and evaluate their effect on performance and satisfaction. Results of a human subject experiment are presented, showing that reducing the amount of user control can maintain, and even improve, user satisfaction if the design of such a reduction is appropriate. This is followed by a discussion of how the findings inform camera-based game design.","cites":"2","conferencePercentile":"29.79274611"},{"venue":"ACM Multimedia","id":"0ecb364d801f506cdb5210b6f9a6c5be7197e81e","venue_1":"ACM Multimedia","year":"2016","title":"Leveraging Contextual Cues for Generating Basketball Highlights","authors":"Vinay Bettadapura, Caroline Pantofaru, Irfan A. Essa","author_ids":"3115428, 2997956, 1714295","abstract":"The massive growth of sports videos has resulted in a need for automatic generation of sports highlights that are comparable in quality to the hand-edited highlights produced by broadcasters such as ESPN. Unlike previous works that mostly use audio-visual cues derived from the video, we propose an approach that additionally leverages contextual cues derived from the environment that the game is being played in. The contextual cues provide information about the excitement levels in the game, which can be ranked and selected to automatically produce high-quality basketball highlights. We introduce a new dataset of 25 NCAA games along with their play-by-play stats and the ground-truth excitement data for each basket. We explore the informativeness of five different cues derived from the video and from the environment through user studies. Our experiments show that for our study participants, the highlights produced by our system are comparable to the ones produced by ESPN for the same games.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"4f35198e161879ffbf3bd5e9b1f9f493e1ff8300","venue_1":"ACM Multimedia","year":"2015","title":"A Cross-media Sentiment Analytics Platform For Microblog","authors":"Chao Chen, Fuhai Chen, Donglin Cao, Rongrong Ji","author_ids":"2067320, 2642638, 8181007, 1725599","abstract":"In this demo, a cross-media public sentiment analysis system is presented. The system presents and visualizes the sentiments of microblog data by organizing the results by region, topic, and content, respectively. Such sentiment is obtained by fusing of sentiment classification scores from both visual and textual channel. In such a way, social multimedia sentiment is shown in a multi-level and user-friendly form.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"57979a240ee7d28f85f4ba8b59ddfe9617478d7c","venue_1":"ACM Multimedia","year":"1998","title":"Layered Transmission and Caching for the Multicast Session Directory service","authors":"Andrew Swan, Steven McCanne, Lawrence A. Rowe","author_ids":"2066909, 2887819, 1723155","abstract":"The recent advent of the Internet Multicast service has enabled a number of successful real-time multimedia applications , yet the scalability of these applications remains challenged by the inherent heterogeneity of the underlying In-ternet. One promising approach for taming this heterogene-ity is to encode each media ow as a layered signal that is striped across multiple multicast groups, thereby allowing a receiver to tune its individual reception rate by modulating its subscription to multicast groups. Though signiicant progress had been made on media transport protocols and congestion control strategies for adjusting multicast groups in this fashion, comparatively little work has been devoted to extending the session directory service and address allocation architecture to meet the needs and requirements of layered media. Moreover, the large-scale deployment of lay-ered media formats is hindered by the lack of support for layered formats in existing session directory tools. To overcome these limitations, we propose a new architecture for session advertisement and caching that exploits multicast \\administrative scope\" through protocol proxies to admit layered media formats and reduce the start-up latency of a directory-service client by an order of magnitude or more. Our architecture is fully compatible with the existing directory service allowing our implementation, which is split across a new session directory tool and network proxy, to be incrementally deployed within the current Internet mul-timedia conferencing architecture. 1 Introduction The Internet Multicast service and its realization in the public Internet | the Multicast Backbone or MBone | form the cornerstone of a new model for multipoint communication called lightweight sessions 15]. In lightweight sessions, a multimedia application disseminates its media ow simply by framing the ow as a sequence of packets and multicas-ting those packets to a multicast group address. Receivers interested in a certain ow simply join the multicast group corresponding to the ow in question and the network auto","cites":"15","conferencePercentile":"48.07692308"},{"venue":"ACM Multimedia","id":"c11aed82d14cbb7d38c5eb238339109159da10be","venue_1":"ACM Multimedia","year":"2011","title":"Sketch2Cartoon: composing cartoon images by sketching","authors":"Changhu Wang, Jun Zhang, Bruce Yang, Lei Zhang","author_ids":"1697065, 1752214, 1862829, 1724298","abstract":"In this paper, we introduce the Sketch2Cartoon system, which is an automatic cartoon making system by leveraging a novel sketch-based clipart image search engine. Different from existing work, most of which either limited users to the pre-prepared characters or only used keyword queries to search materials, Sketch2Cartoon enables users to sketch major curves of characters and props in their mind, and real-time search results from millions of clipart images could be selected to compose the cartoon images. The selected components are vectorized and thus could be further edited. By enabling sketch-based input, the cartoon image making process becomes more natural, and even a child who is too young to read or write can draw whatever he/she imagines and get interesting cartoon images.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"d8f8e62465a5f7a6c31e27b52769f07ff8a1c8d6","venue_1":"ACM Multimedia","year":"2010","title":"MindFinder: interactive sketch-based image search on millions of images","authors":"Yang Cao, Hai Wang, Changhu Wang, Zhiwei Li, Liqing Zhang, Lei Zhang","author_ids":"4918470, 1680829, 1697065, 2902955, 7137826, 1724298","abstract":"In this paper, we showcase the MindFinder system, which is an interactive sketch-based image search engine. Different from existing work, most of which is limited to a small scale database or only enables single modality input, MindFinder is a sketch-based multimodal search engine for million-level database. It enables users to sketch major curves of the target image in their mind, and also supports tagging and coloring operations to better express their search intentions. Owning to a friendly interface, our system supports multiple actions, which help users to flexibly design their queries. After each operation, top returned images are updated in real time, based on which users could interactively refine their initial thoughts until ideal images are returned. The novelty of the MindFinder system includes the following two aspects: 1) A multimodal searching scheme is proposed to retrieve images which meet users' requirements not only in structure, but also in semantic meaning and color tone. 2) An indexing framework is designed to make MindFinder scalable in terms of database size, memory cost, and response time. By scaling up the database to more than two million images, MindFinder not only helps users to easily present whatever they are imagining, but also has the potential to retrieve the most desired images in their mind.","cites":"31","conferencePercentile":"92.46575342"},{"venue":"ACM Multimedia","id":"01579416712b90d75a0cd9fd72af0cde1490598e","venue_1":"ACM Multimedia","year":"1997","title":"QuickSet: Multimodal Interaction for Distributed Applications","authors":"Philip R. Cohen, Michael Johnston, David McGee, Sharon L. Oviatt, Jay Pittman, Ira A. Smith, Liang Chen, Josh Clow","author_ids":"2305444, 6494527, 2565317, 2807460, 2809213, 1989199, 1692551, 2414661","abstract":"This paper presents an emerging application of multimodal interface research to distributed applications. We have developed the QuickSet prototype, a pen/voice system running on a hand-held PC, communicating via wireless LAN through an agent architecture to a number of systems, including NRaD's' LeatherNet system, a distributed interactive training simulator built for the US Marine Corps. The paper describes the overall system architecture, a novel multimodal integration strategy offering mutual compensation among modalities, and provides examples of multimodal simulation setup. Finally, we discuss our applications experience and evaluation.","cites":"308","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"e20582a68463f68965b65958e1ffbdc3b73db624","venue_1":"ACM Multimedia","year":"1997","title":"Middleware for Distributed Multimedia: Need a New Direction? (Panel)","authors":"Guru M. Parulkar, Lawrence A. Rowe, David Hutchison, Jonathan Walpole, Rajendra Yavatkar","author_ids":"2164347, 1723155, 7374135, 3210222, 8086646","abstract":"This panel is motivated by three emerging trends: (1) multimedia applications represent an important class 'of distributed and networking applications; (2) middleware has become a valuable software layer/system which allows users to develop large complex distributed applications without having to deal with details of underlying networking and operating system; and (3) object-oriented methodology has matured to the point that it has become a De-facto standard for software design and development. The last two trends also explain rapidly increasing commercial interest in OMG's proposed middleware standard Common Object Request Broker Architecture (CORBA) and associated Object Request Broker (ORB) implementations. It is claimed that distributed object oriented middleware eliminates many tedious, error-prone and non-portable aspects of developing and maintaining distributed applications by automating common network programming tasks such as object location, object activation, parameter marshaling, fault recovery, and security. These three trends together motivate a need for an Object-Oriented middleware that can especially support distributed and networked multimedia applications. Several groups have already initiated efforts aimed at extending CORBA and associated ORBS to make them more suitable for multimedia applications. Others argue that multimedia applications have different requirements in that they require periodic processing and transmission of continuous streams of data, and that CORBA is the wrong middleware to extend for distributed multimedia applications. The purpose of this panel is to (1) argue for a standard middleware layer which can facilitate development of multimedia applications; (2) explore desirable features of the middleware for multimedia; (3) explore how (distributed) objects can play a role in defining the multimedia middleware standard; and (4) argue suitability of CORBA (and associated ORBS) and their proposed extensions for multimedia.","cites":"0","conferencePercentile":"3.571428571"},{"venue":"ACM Multimedia","id":"973909185cf5e3e5279085b8dda00bb03bd7f7d8","venue_1":"ACM Multimedia","year":"1997","title":"Floor Control for Large-Scale MBone Seminars","authors":"Radhika Malpani, Lawrence A. Rowe","author_ids":"2895610, 1723155","abstract":"The development of MBone videoconferencing tools has led to a large number of seminars being broadcast over the Internet. Most MBone seminars have little remote participation. One reason might be the absence of goodfloor control mechanisms. This paper describes a tool we developed, called the Questionboard, that implements floor control to facilitate question asking in large-scale MBone seminars. The qb protocol is described including the mechanisms used for reliable transmission of data and the floor control and crash recovery protocols. The MBone is a virtual network deployed over the Internet that supports routing of IP multicast packets [D91]. The deployment of the MBone and the development of MBone conferencing tools, such as vie [MJ95], vat [JM], ivs rzT94], sd [Jl, and wb [M92] have lead to a wide variety of events like seminars and conferences being multicast over the Internet. Since early 1995, we have multi-cast the Berkeley Multimedia and Graphics Seminar, a regularly scheduled weekly seminar over the open Internet /R95]. A speaker is invited to talk about her area of research. The presentation is multicast over the MBone. We produce audio and video streams and use wb to display slides. We run a second wb session called the control whiteboard, as a back channel to fix transmission errors and allow MBone participants to ask questions. The presentation is attended by Berkeley faculty and students, giving us a local audience in addition to the remote MBone audience. Local attendance ranges between 15 and 50 people. Remote attendance ranges between 10 and 200 people. Our experience with this seminar series is that while many questions are asked by the local audience, few remote participants ask 1. Penuission to malie digitnlflwd topics ol'nll or pnfl oT1llis mn~erinl for personal or clawroom use is gmmcd willlou kc provided tl1a1 IIN copies ore not made or distribukd Ibr prcli1 or commercinl ndvantny, 11~ copyright nolice. 1lla title ol'lhe publicn1ion and ils dale appear. and nolice is given thrill copyright is by pcnnission cl'llle ACM. Inc. To copy otherwise, to republisli, lo post on servers or lo redis1ribuk lo lists, requires specilic permission and/or ke. questions. One possible reason is the burden of setting up microphones and cameras to allow the remote participant to transmit his question. Another possible reason is the absence of a floor control mechanism. This problem is especially important for seminars with several hundred remote participants. Currently, the only way …","cites":"52","conferencePercentile":"80.95238095"},{"venue":"ACM Multimedia","id":"f2da1242c41754ba273c18e3369c314980d1b6e3","venue_1":"ACM Multimedia","year":"2014","title":"Representing Musical Patterns via the Rhythmic Style Histogram Feature","authors":"Matthew Prockup, Jeffrey J. Scott, Youngmoo E. Kim","author_ids":"2227419, 2538988, 1730150","abstract":"When listening to music, humans often focus on melodic and rhythmic elements to identify specific songs or genres. While these representations may be quite simple, they still capture and differentiate higher level aspects of music such as expressive intent and musical style. In this work we seek to extract and represent rhythmic patterns from a polyphonic corpus of audio encompassing a number of styles. A compact feature is designed that probabilistically models rhythmic activations within musical beat divisions through histograms of Inter-Onset-Intervals (IOI). Onset detection functions are calculated from multiple frequency bands of a perceptually motivated filter bank. This allows for patterns of lower pitched and higher pitched onsets to be described separately. Through a set of supervised and unsupervised experiments, we show that this feature is well suited for a variety of tasks in which quantifying rhythmic style is necessary.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"a689a4e6874b8ecbe79214ddb2955a166ddc9e82","venue_1":"ACM Multimedia","year":"2009","title":"Interacting with a personal cubic 3D display","authors":"Billy Lam, Ian Stavness, Ryan Barr, Sidney Fels","author_ids":"2451385, 2351671, 3010087, 1749457","abstract":"We describe a demonstration of four novel interaction techniques for a cubic head-coupled 3D display. The interactions illustrated include: viewing a static scene, navigating through a large landscape, playing with colliding objects inside a box, and stylus-based manipulation of objects. Users experience new interaction techniques for 3D scene manipulation in a cubic display.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"5293bf49cc457497ce04f152619acc636f22d660","venue_1":"ACM Multimedia","year":"2009","title":"T-IRS: textual query based image retrieval system for consumer photos","authors":"Yiming Liu, Dong Xu, Ivor W. Tsang, Jiebo Luo","author_ids":"2055258, 1714390, 1807998, 1717319","abstract":"In this demonstration, we present a (quasi) real-time textual query based image retrieval system (T-IRS) for consumer photos by leveraging millions of web images and their associated rich textual descriptions (captions, categories, etc.). After a user provides a textual query (e.g., \"boat\"), our system automatically finds the positive web images that are related to the textual query \"boat\" as well as the negative web images which are irrelevant to the textual query. Based on these automatically retrieved positive and negative web images, we employ the decision stump ensemble classifier to rank personal consumer photos. To further improve the photo retrieval performance, we also develop a novel relevance feedback method, referred to as Cross-Domain Regularized Regression (CDRR), which effectively utilizes both the web images and the consumer images. Our system is inherently not limited by any predefined lexicon.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"9385ff997c149380a87ba1882f975ba2e1e20db7","venue_1":"ACM Multimedia","year":"1999","title":"A multicast scheme for parallel software-only video effects processing","authors":"Ketan Mayer-Patel, Lawrence A. Rowe","author_ids":"3715598, 1723155","abstract":"We have developed a parallel software-only processing system for creating real-time video effects such as titling and compositing (e.g., picture-in-picture) using compressed Internet video sources. The system organizes processors into a hierarchy of levels. Processes at each level of the hierarchy can exploit different types of parallelism and coordinate the actions of lower levels. To control the effect, control messages must be distributed to processors in the hierarchy while preserving the independence of each level. This requires a control mechanism that supports efficient delivery of messages to groups of processors, tunable reliability semantics, and recoverable state information. We describe a mechanism that meets these requirements that uses IP-Multicast, the Scalable Reliable Multicast protocol, and the Scalable Naming and Announcement Protocol. We also describe an optimization that provides a flexible framework for linking the control of different aspects of one or more related video effects.","cites":"5","conferencePercentile":"41.17647059"},{"venue":"ACM Multimedia","id":"98607a02da18eca4b3648f71c96427a3d6997df9","venue_1":"ACM Multimedia","year":"2001","title":"A new foreground extraction scheme for video streams","authors":"Zhengping Wu, Chun Chen","author_ids":"1705526, 5371645","abstract":"The MPEG-4 video coding standard consists of object based coding schemes for multimedia and enables content based functionalities. Video objects in still pictures or video sequences should be first identified before the encoding process starts. An algorithm based on information fusion, which can be used for the extraction of foreground objects in video streams in real time is proposed in this paper. The method efficiently integrates image and motion information of video streams. The thresholding technique operated in the HSV space provides a better use of the color information than that in the traditional RGB space. Using enhanced boundary extracted from the motion region for contour adaptation offers an original method to refine the contour.","cites":"4","conferencePercentile":"40.81632653"},{"venue":"ACM Multimedia","id":"c8e1c1ac60d391c47dfdecd0aa10e757aa396aea","venue_1":"ACM Multimedia","year":"2010","title":"Photo2Trip: generating travel routes from geo-tagged photos for trip planning","authors":"Xin Lu, Changhu Wang, Jiang-Ming Yang, Yanwei Pang, Lei Zhang","author_ids":"1706990, 1697065, 7788742, 2286218, 1724298","abstract":"Travel route planning is an important step for a tourist to prepare his/her trip. As a common scenario, a tourist usually asks the following questions when he/she is planning his/her trip in an unfamiliar place: 1) Are there any travel route suggestions for a one-day or three-day trip in Beijing? 2) What is the most popular travel path within the Forbidden City? To facilitate a tourist's trip planning, in this paper, we target at solving the problem of automatic travel route planning. We propose to leverage existing travel clues recovered from 20 million geo-tagged photos collected from www.panoramio.com to suggest customized travel route plans according to users' preferences. As the footprints of tourists at memorable destinations, the geo-tagged photos could be naturally used to discover the travel paths within a destination (attractions/landmarks) and travel routes between destinations. Based on the information discovered from geo-tagged photos, we can provide a customized trip plan for a tourist, i.e., the popular destinations to visit, the visiting order of destinations, the time arrangement in each destination, and the typical travel path within each destination. Users are also enabled to specify personal preference such as visiting location, visiting time/season, travel duration, and destination style in an interactive manner to guide the system. Owning to 20 million geo-tagged photos and 200,000 travelogues, an online system has been developed to help users plan travel routes for over 30,000 attractions/landmarks in more than 100 countries and territories. Experimental results show the intelligence and effectiveness of the proposed framework.","cites":"67","conferencePercentile":"98.08219178"},{"venue":"ACM Multimedia","id":"6364caca4e0140277b83ceb8359b8464ddf7e510","venue_1":"ACM Multimedia","year":"2010","title":"Photo2Trip: an interactive trip planning system based on geo-tagged photos","authors":"Huagang Yin, Xin Lu, Changhu Wang, Nenghai Yu, Lei Zhang","author_ids":"3244944, 1706990, 1697065, 1708598, 1724298","abstract":"In this technical demonstration, we present a novel interactive trip planning system, i.e. Photo2Trip, by leveraging existing travel clues recovered from 20 million geo-tagged photos. Compared with the most common ways of trip planning, such as surveying travelogues and resorting to travel forums, Photo2Trip enables users to plan their trips in a more effective way. To meet users' diverse travel requirements, the system considers the following preferences: travel location (e.g. Beijing, Paris, or New York), travel duration (e.g. a two-day trip or a five-day trip), visiting time (e.g. summer, winter, March, or October), and travel style preference (e.g. prefer historic or prefer scenery sites). According to user requirements, Photo2Trip can automatically recommend popular travel routes among multiple destinations (attractions/ landmarks), and suggest typical internal paths within each destination. Moreover, users are allowed to interactively adjust the suggested plans by adding or removing destinations to get more customized travel routes from the system. Owning to 20 million geo-tagged photos and 200,000 travelogues, Photo2Trip is capable of supporting users plan travel routes for over 30,000 attractions/landmarks in more than 100 countries and territories.","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"3aa9d042e977f99d1124d6ce6cd83d3feb975ec2","venue_1":"ACM Multimedia","year":"2009","title":"Using large-scale web data to facilitate textual query based retrieval of consumer photos","authors":"Yiming Liu, Dong Xu, Ivor W. Tsang, Jiebo Luo","author_ids":"2055258, 1714390, 1807998, 1717319","abstract":"The rapid popularization of digital cameras and mobile phone cameras has lead to an explosive growth of consumer photo collections. In this paper, we present a (quasi) real-time textual query based personal photo retrieval system by leveraging millions of web images and their associated rich textual descriptions (captions, categories, etc.). After a user provides a textual query (<i>e.g.</i>, \"pool\"), our system exploits the inverted file method to automatically find the positive web images that are related to the textual query \"pool\" as well as the negative web images which are irrelevant to the textual query. Based on these automatically retrieved relevant and irrelevant web images, we employ two simple but effective classification methods, <i>k</i> Nearest Neighbor (kNN) and decision stumps, to rank personal consumer photos. To further improve the photo retrieval performance, we propose three new relevance feedback methods via cross-domain learning. These methods effectively utilize both the web images and the consumer images. In particular, our proposed cross-domain learning methods can learn robust classifiers with only a very limited amount of labeled consumer photos from the user by leveraging the pre-learned decision stumps at interactive response time. Extensive experiments on both consumer and professional stock photo datasets demonstrated the effectiveness and efficiency of our system, which is also inherently not limited by any predefined lexicon.","cites":"21","conferencePercentile":"86.57024793"},{"venue":"ACM Multimedia","id":"16aa32b9b5f175a0f0f8486697b2fa83b8e3f09e","venue_1":"ACM Multimedia","year":"2010","title":"Discriminative codeword selection for image representation","authors":"Lijun Zhang, Chun Chen, Jiajun Bu, Zhengguang Chen, Shulong Tan, Xiaofei He","author_ids":"1707675, 5371645, 8475311, 8157596, 2054844, 3945955","abstract":"Bag of features (BoF) representation has attracted an increasing amount of attention in large scale image processing systems. BoF representation treats images as loose collections of local invariant descriptors extracted from them. The visual codebook is generally constructed by using an unsupervised algorithm such as K-means to quantize the local descriptors into clusters. Images are then represented by the frequency histograms of the codewords contained in them. To build a compact and discriminative codebook, codeword selection has become an indispensable tool. However, most of the existing codeword selection algorithms are supervised and the human labeling may be very expensive. In this paper, we consider the problem of unsupervised codeword selection, and propose a novel algorithm called Discriminative Codeword Selection (DCS). Motivated from recent studies on discriminative clustering, the central idea of our proposed algorithm is to select those codewords so that the cluster structure of the image database can be best respected. Specifically, a multi-output linear function is fitted to model the relationship between the data matrix after codeword selection and the indicator matrix. The most discriminative codewords are thus defined as those leading to minimal fitting error. Experiments on image retrieval and clustering have demonstrated the effectiveness of the proposed method.","cites":"7","conferencePercentile":"70.82191781"},{"venue":"ACM Multimedia","id":"1f25d891663e109d7e2cbb8c2916a992daa907cd","venue_1":"ACM Multimedia","year":"1997","title":"Move-to-Rear List Scheduling: A New Scheduling Algorithm for Providing QoS Guarantees","authors":"John L. Bruno, Eran Gabber, Banu Özden, Abraham Silberschatz","author_ids":"3229338, 2580394, 1799649, 1688159","abstract":"In order to support multiple real-time applications on a single platform, ,the operating system must provide Quality of Service (&OS) guarantees so that the system resources can be provisioned among applications to achieve desired levels of predictable performance. The traditional QoS parameters include fairness, delay, and throughput. In this paper we introduce a new QoS criterion called cumulative service. The cumulative service criterion relates the total service obtained by a process under a scheduling policy to the ideal service that the process would have accumulated by executing on each resource at a reserued rate. We say that a scheuling policy provides a cumulative service guarantee if the performance of the real system differs from the ideal system by at most a constant amount. A cumulative service guarantee is vital for applications (e.g., a continous media file service) that require multiple resources and demand predictable aggregated throughput over aII these resources. E.xisting scheduling algorithms that guarantee traditional QoS paramaters do not provide cumulative service guarantees. We present a new scheduling algorithm called Move-To-Rear List Scheduling which provides a cumulative service guarantee as well as the traditional guarantees such as fairness (proportional sharing) and bounded delay. The complexity of MTR-LS is o(ln(n)) where n is the number of processes.","cites":"18","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"08ca67ccb7b37a19e380cd98b257294d0b8079c8","venue_1":"ACM Multimedia","year":"2016","title":"A Perceptual Quality Metric for Videos Distorted by Spatially Correlated Noise","authors":"Chao Chen, Mohammad Izadi, Anil C. Kokaram","author_ids":"2067320, 1698705, 2058053","abstract":"Assessing the perceptual quality of videos is critical for monitoring and optimizing video processing pipelines. In this paper, we focus on predicting the perceptual quality of videos distorted by noise. Existing video quality metrics are tuned for \"white\", i.e., spatially uncorrelated noise. However, white noise is very rare in real videos. Based on our analysis of the noise correlation patterns in a broad and comprehensive video set, we build a video database that simulates the commonly encountered noise characteristics. Using the database, we develop a perceptual quality assessment algorithm that explicitly incorporates the noise correlations. Experimental results show that, for videos with spatially correlated noises, the proposed algorithm presents high accuracy in predicting perceptual qualities.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"15084667178296ecc7fdd8fb26febdf9d611cb4a","venue_1":"ACM Multimedia","year":"2010","title":"SCOTT: set cover tracing technology","authors":"Dulce B. Ponceleon, Jeff Lostpiech, Hongxia Jin, Eric Wilcox","author_ids":"1802641, 2385195, 1705713, 8493409","abstract":"In this paper, we describe SCOTT: a demonstration system that uses the Set Cover Tracing algorithm for determining the source of pirate content. This algorithm is very efficient in dealing with collusion attacks - the performance is close to linear in the number of colluders. However, the algorithm is based on the Set Cover Problem, which is known to be NP hard. SCOTT confirms the assertion in the original paper that a set cover algorithm is efficient in this particular application. The SCOTT system is suitable for use in a commercial application; the most notable of which is tracing the source of pirate Blu-ray movies. (Blu-ray players contain a built-in tracing traitors key assignment.) It also contains a visualization of the tracing process. After each pirate movie, SCOTT displays the universal of all players and its estimate of guilt for each player.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"6e885a982cb097e003b0a7c2ab7bd47cfc0cf864","venue_1":"ACM Multimedia","year":"2009","title":"Feature selection for fast speech emotion recognition","authors":"Luming Zhang, Mingli Song, Na Li, Jiajun Bu, Chun Chen","author_ids":"1763785, 1727111, 1748656, 8475311, 5371645","abstract":"In speech based emotion recognition, both acoustic features extraction and features classification are usually time consuming,which obstruct the system to be real time. In this paper, we proposea novel feature selection (FSalgorithm to filter out the low efficiency features towards fast speech emotion recognition.Firstly, each acoustic feature's discriminative ability, time consumption and redundancy are calculated. Then, we map the original feature space into a nonlinear one to select nonlinear features,which can exploit the underlying relationship among the original features. Thirdly, high discriminative nonlinear feature with low time consumption is initially preserved. Finally, a further selection is followed to obtain low redundant features based on these preserved features. The final selected nonlinear features are used in features' extraction and features' classification in our approach, we call them qualified features. The experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase. Moreover, a competitive of recognition accuracy has been observed in the speech emotion recognition.","cites":"7","conferencePercentile":"55.78512397"},{"venue":"ACM Multimedia","id":"45ed0adeb0be2577a157d4445f0a90bf2d3d0392","venue_1":"ACM Multimedia","year":"2009","title":"Visual attention analysis by pseudo gravitational field","authors":"Yezhou Yang, Mingli Song, Na Li, Jiajun Bu, Chun Chen","author_ids":"7607499, 1727111, 1748656, 8475311, 5371645","abstract":"As a crucial step of the visual cognition and perception, visual attention analysis shows great importance in many research or application areas. In this paper, we treat this problem from a new angle, inspired by the classic gravitational field theory. By defining \"mass\" of each pixel, we compute \"force\" between them to obtain a so-called pseudo gravitational field over an image. Then, we propose an iteration algorithm to simulate the movement of the fixation points affected by this field. Finally, stable visual attention points or areas are obtained when those fixation points finally aggregate around some special pixels or areas. The main contributions are threefold: (1) by introducing classic gravitational field theory into visual attention analysis, a new point of view is proposed; (2) a competition scheme is constructed and the meaning of attraction can be applied into visual attention analysis intuitively; (3) by using pixels into computation through down sampling, a faster analysis method is achieved. The experimental result shows that our method is effective and consistent with the generally accepted definition of visual attention.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"d1bb44ccfa3e66816334cbc999e6da7635f29e94","venue_1":"ACM Multimedia","year":"2009","title":"Convex experimental design using manifold structure for image retrieval","authors":"Lijun Zhang, Chun Chen, Wei Chen, Jiajun Bu, Deng Cai, Xiaofei He","author_ids":"1707675, 5371645, 3207592, 8475311, 1745280, 3945955","abstract":"Content Based Image Retrieval (CBIR) has become one of the most active research areas in computer science. Relevance feedback is often used in CBIR systems to bridge the semantic gap. Typically, users are asked to make relevance judgements on some query results, and the feedback information is then used to re-rank the images in the database. An effective relevance feedback algorithm must provide the users with the most informative images with respect to the ranking function. In this paper, we propose a novel active learning algorithm, called Convex Laplacian Regularized Ioptimal Design (CLapRID), for relevance feedback image retrieval. Our algorithm is based on a regression model which minimizes the least square error on the labeled images and simultaneously preserves the intrinsic geometrical structure of the image space. It selects the most informative images which minimize the average predictive variance. The optimization problem of CLapRID can be cast as a semidefinite programming (SDP) problem, and solved via interior-point methods. Experimental results on COREL database have demonstrate the effectiveness of the proposed algorithm for relevance feedback image retrieval.","cites":"9","conferencePercentile":"64.04958678"},{"venue":"ACM Multimedia","id":"da8617706c9a520eaa264172872d0817a63b3a5e","venue_1":"ACM Multimedia","year":"2015","title":"A Multimodal Predictive Model of Successful Debaters or How I Learned to Sway Votes","authors":"Maarten Brilman, Stefan Scherer","author_ids":"2615820, 1770312","abstract":"Interpersonal skills such as public speaking are essential assets for a large variety of professions and in everyday life. The ability to communicate in social environments often greatly influences a person's career development, can help resolve conflict, gain the upper hand in negotiations, or sway the public opinion. We focus our investigations on a special form of public speaking, namely public debates of socioeconomic issues that affect us all. In particular, we analyze performances of expert debaters recorded through the Intelligence Squared U.S. (IQ2US) organization. IQ2US collects high-quality audiovisual recordings of these debates and publishes them online free of charge. We extract audiovisual nonverbal behavior descriptors, including facial expressions, voice quality characteristics, and surface level linguistic characteristics. Within our experiments we investigate if it is possible to automatically predict if a debater or his/her team are going to sway the most votes after the debate using multimodal machine learning and fusion approaches. We identify unimodal nonverbal behaviors that characterize successful debaters and our investigations reveal that multimodal machine learning approaches can reliably predict which individual (~75% accuracy) or team (85% accuracy) is going to win the most votes in the debate. We created a database consisting of over 30 debates with four speakers per debate suitable for public speaking skill analysis and plan to make this database publicly available for the research community.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"050323d550a59d23c8658251eebc60a0bf1b82e0","venue_1":"ACM Multimedia","year":"2011","title":"Automatic motion-guided video stylization and personalization","authors":"Chen Cao, Shifeng Chen, Wei Zhang, Xiaoou Tang","author_ids":"2863559, 2869725, 8660372, 1741901","abstract":"Video stylization transfers a source video into an artistic version while maintaining temporal coherence between adjacent frames. In this paper, we formulate the unsupervised example-based video stylization with Markov random field model. In our algorithm, we implement an improved optical flow algorithm to maintain temporal coherence while improve the accuracy of estimation along motion boundaries. We also extend our algorithm to the application of video personalization, in which human faces keep clear and distinguishable. A series of techniques are fused in video personalization, including face detection and alignment, motion flow, skin detection, and illumination blending. Given a source video and a style template image, our algorithm produces the stylized and/or personalized video(s) automatically. Experimental results demonstrate that our algorithm performs excellently in both video stylization and personalization.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"d4b87e2ac04769c2ef3168274435803263329a16","venue_1":"ACM Multimedia","year":"2009","title":"Performance driven face animation via non-rigid 3d tracking","authors":"Wei Zhang, Qiang Wang, Xiaoou Tang","author_ids":"8660372, 1715161, 1741901","abstract":"In this demo, a performance driven 3D face animation system is proposed. The proposed system consists of two key components: a robust non-rigid 3D tracking module and a MPEG4 compliant facial animation module. Firstly, the facial motion is tracked from source videos which contain both the rigid 3D head motion (6 DOF) and the non-rigid expression variations. Afterward, the tracked facial motion is parameterized via estimating a set of MPEG4 facial animation parameters(FAP). As the final step, these FAP values are transferred to the MPEG4-compliant face model for the animation purpose. The proposed tracking and animation system has a strong generalization ability and can be used in the indoor environment with no additional assumptions.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"cd62a3eb2793daba9454dfc7ee439b877d13643d","venue_1":"ACM Multimedia","year":"2011","title":"Reading between the tags to predict real-world size-class for visually depicted objects in images","authors":"Martha Larson, Christoph Kofler, Alan Hanjalic","author_ids":"1748247, 2517570, 6741141","abstract":"Multimedia information retrieval stands to benefit from the availability of additional information about tags and how they relate to the content visually depicted in images. We propose a generic approach that contributes to improving the informativeness of image tags by combining generalizations about the distributional tendencies of physical objects in the real world and statistics of natural language use patterns that have been mined from the Web. The approach, which we refer to as 'Reading between the Tags,' provides for each tag associated with an image, first, a prediction concerning <i>corporeality</i>, i.e., whether or not the tag denotes a physical entity, and, then, concerning the real-world size of that entity, i.e., large, medium or small. Mining takes place using a set of Language Use Frames (LUFs) that are composed of natural language neighborhoods characteristic of tag classes. We validate our approach with a series of experiments on a set of images from the MIRFLICKR data set using ground truth created with standard crowdsourcing techniques. The main experiments demonstrate the effectiveness of our approach for size-class prediction. A further experiment shows that size-class prediction can be improved and made image-specific using general and relatively small sets of visual concepts. A final experiment confirms that the set of LUFs can also be chosen automatically via statistical feature selection.","cites":"10","conferencePercentile":"82.94460641"},{"venue":"ACM Multimedia","id":"0047cae0233ebf8201b4fd9819e6a40dab25a703","venue_1":"ACM Multimedia","year":"2011","title":"Finding representative and diverse community contributed images to create visual summaries of geographic areas","authors":"Stevan Rudinac, Alan Hanjalic, Martha Larson","author_ids":"2815960, 6741141, 1748247","abstract":"This paper presents an automatic approach that uses community-contributed images to create representative and diverse visual summaries of specific geographic areas. Complex relations between images, extracted visual features, text associated with the images as well as users and their social network are modeled using a multimodal graph. To compute affinities between nodes in the graph we rely on the proven concept of random walk with restarts. The novelty of our approach lies in its use of the multimodal graph to create a diverse, yet representative, image set. Further, we introduce an edge-weighting mechanism for the fusion of heterogeneous modalities. We evaluate our summaries with a new protocol that tests for representativeness and diversity using image geo-coordinates and is independent of the need for human evaluators. The experiments, performed on a set of Flickr images, demonstrate the effectiveness of our approach.","cites":"10","conferencePercentile":"82.94460641"},{"venue":"ACM Multimedia","id":"9abff170cf4e897d2be90536d4835b94bb5e22bc","venue_1":"ACM Multimedia","year":"2014","title":"Salable Image Search with Reliable Binary Code","authors":"Guangxin Ren, Junjie Cai, Shipeng Li, Nenghai Yu, Qi Tian","author_ids":"2887950, 2086802, 4973820, 1708598, 1724745","abstract":"In many existing image retrieval algorithms, Bag-of-Words (BoW) model has been widely adopted for image representation. To achieve accurate indexing and efficient retrieval, local features such as the SIFT descriptor are extracted and quantized to visual words. One of the most popular quantization scheme is scalar quantization, which generates binary signature with an empirical threshold value. However, such binarization strategy inevitably suffers from the quantization loss induced by each quantized bit and impairs the effectiveness of search performance. In this paper, we investigate the reliability of each bit in scalar quantization and propose a novel reliable binary SIFT feature. We move one step ahead to incorporate the reliability in both index word expansion and feature similarity. Our proposed approach not only accelerates the search speed by narrowing search space, but also improves the retrieval accuracy by alleviating the impact of unreliable quantized bits. Experimental results demonstrate that the proposed approach achieves significant improvement in retrieval efficiency and accuracy.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"2a41c1c45c5b33d5a9cea521c93946d93aee9eb0","venue_1":"ACM Multimedia","year":"2014","title":"Optimized Distances for Binary Code Ranking","authors":"Jianfeng Wang, Heng Tao Shen, Shuicheng Yan, Nenghai Yu, Shipeng Li, Jingdong Wang","author_ids":"5040264, 1724393, 1698982, 1708598, 4973820, 1688516","abstract":"Binary encoding on high-dimensional data points has attracted much attention due to its computational and storage efficiency. While numerous efforts have been made to encode data points into binary codes, how to calculate the effective distance on binary codes to approximate the original distance is rarely addressed. In this paper, we propose an effective distance measurement for binary code ranking. In our approach, the binary code is firstly decomposed into multiple sub codes, each of which generates a query-dependent distance lookup table. Then the distance between the query and the binary code is constructed as the aggregation of the distances from all sub codes by looking up their respective tables. The entries of the lookup tables are optimized by minimizing the misalignment between the approximate distance and the original distance. Such a scheme is applied to both the symmetric distance and the asymmetric distance. Extensive experimental results show superior performance of the proposed approach over state-of-the-art methods on three real-world high-dimensional datasets for binary code ranking.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"ad971193830096bf9820724f12a58d9af3e5524a","venue_1":"ACM Multimedia","year":"2013","title":"Listen, look, and gotcha: instant video search with mobile phones by layered audio-video indexing","authors":"Wu Liu, Tao Mei, Yongdong Zhang, Jintao Li, Shipeng Li","author_ids":"1686917, 1788123, 1699819, 8722263, 4973820","abstract":"Mobile video is quickly becoming a mass consumer phenomenon. More and more people are using their smartphones to search and browse video content while on the move. In this paper, we have developed an innovative instant mobile video search system through which users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. The system is able to index large-scale video data using a new layered audio-video indexing approach in the cloud, as well as extract light-weight joint audio-video signatures in real time and perform progressive search on mobile devices. Unlike most existing mobile video search applications that simply send the original video query to the cloud, the proposed mobile system is one of the first attempts at instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is characterized by four unique properties: 1) a joint audio-video signature to deal with the large aural and visual variances associated with the query video captured by the mobile phone, 2) layered audio-video indexing to holistically exploit the complementary nature of audio and video signals, 3) light-weight fingerprinting to comply with mobile processing capacity, and 4) a progressive query process to significantly reduce computational costs and improve the user experience---the search process can stop anytime once a confident result is achieved. We have collected 1,400 query videos captured by 25 mobile users from a dataset of 600 hours of video. The experiments show that our system outperforms state-of-the-art methods by achieving 90.79% precision when the query video is less than 10 seconds and 70.07% even when the query video is less than 5 seconds.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"3d092467770b7175b8409e072d4941bc3ba21842","venue_1":"ACM Multimedia","year":"2013","title":"Order preserving hashing for approximate nearest neighbor search","authors":"Jianfeng Wang, Jingdong Wang, Nenghai Yu, Shipeng Li","author_ids":"5040264, 1688516, 1708598, 4973820","abstract":"In this paper, we propose a novel method to learn similarity-preserving hash functions for approximate nearest neighbor (NN) search. The key idea is to learn hash functions by maximizing the alignment between the similarity orders computed from the original space and the ones in the hamming space. The problem of mapping the NN points into different hash codes is taken as a classification problem in which the points are categorized into several groups according to the hamming distances to the query. The hash functions are optimized from the classifiers pooled over the training points. Experimental results demonstrate the superiority of our approach over existing state-of-the-art hashing techniques.","cites":"23","conferencePercentile":"96"},{"venue":"ACM Multimedia","id":"a86f5f7d76117cd329cdb3bec5e6c96ba6fae45d","venue_1":"ACM Multimedia","year":"2013","title":"Annotation for free: video tagging by mining user search behavior","authors":"Ting Yao, Tao Mei, Chong-Wah Ngo, Shipeng Li","author_ids":"8543685, 1788123, 1751681, 4973820","abstract":"The problem of tagging is mostly considered from the perspectives of machine learning and data-driven philosophy. A fundamental issue that underlies the success of these approaches is the visual similarity, ranging from the nearest neighbor search to manifold learning, to identify similar instances of an example for tag completion. The need to searching for millions of visual examples in high-dimensional feature space, however, makes the task computationally expensive. Moreover, the results can suffer from robustness problem, when the underlying data, such as online videos, are rich of semantics and the similarity is difficult to be learnt from low-level features. This paper studies the exploration of user searching behavior through click-through data, which is largely available and freely accessible by search engines, for learning video relationship and applying the relationship for economic way of annotating online videos. We demonstrated that, by a simple approach using co-click statistics, promising results were obtained in contrast to feature-based similarity measurement. Furthermore, considering the long tail effect that few videos dominate most clicks, a new method based on~polynomial~semantic indexing is proposed to learn a latent space~for alleviating the sparsity problem of click-through data. The proposed approaches are then applied for three major tasks in tagging: tag assignment, ranking, and enrichment. On~a bipartite graph constructed from click-through data with~over 15 million queries and 20 million video URL clicks,~we showed that annotation can be performed for free with competitive performance and minimum computing resource, representing a new and promising paradigm for video tagging in addition to machine learning and data-driven methodologies.","cites":"17","conferencePercentile":"93.55555556"},{"venue":"ACM Multimedia","id":"8de146e498338cd8250941bef88d3bec4d7fc301","venue_1":"ACM Multimedia","year":"2012","title":"SocialTransfer: cross-domain transfer learning from social streams for media applications","authors":"Suman Deb Roy, Tao Mei, Wenjun Zeng, Shipeng Li","author_ids":"3213827, 1788123, 1702938, 4973820","abstract":"The usage and applications of social media have become pervasive. This has enabled an innovative paradigm to solve multimedia problems (e.g., recommendation and popularity prediction), which are otherwise hard to address purely by traditional approaches. In this paper, we investigate how to build a mutual connection among the disparate social media on the Internet, using which cross-domain media recommendation can be realized. We accomplish this goal through <i>SocialTransfer</i>---a novel cross-domain real-time transfer learning framework. While existing transfer learning methods do not address how to utilize the real time social streams, our proposed <i>SocialTransfer</i> is able to effectively learn from social streams to help multimedia applications, assuming an intermediate topic space can be built across domains. It is characterized by two key components: 1) a topic space learned in real time from social streams via Online Streaming Latent Dirichlet Allocation (OSLDA), and 2) a real-time cross-domain graph spectra analysis based transfer learning method that seamlessly incorporates learned topic models from social streams into the transfer learning framework. We present as use cases of \\emph{SocialTransfer} two video recommendation applications that otherwise can hardly be achieved by conventional media analysis techniques: 1) socialized query suggestion for video search, and 2) socialized video recommendation that features socially trending topical videos. We conduct experiments on a real-world large-scale dataset, including 10.2 million tweets and 5.7 million YouTube videos and show that \\emph{SocialTransfer} outperforms traditional learners significantly, and plays a natural and interoperable connection across video and social domains, leading to a wide variety of cross-domain applications.","cites":"30","conferencePercentile":"97.31012658"},{"venue":"ACM Multimedia","id":"1ce75a7d7dc056a89e08c4b3aaa18982aae52511","venue_1":"ACM Multimedia","year":"2014","title":"Boosting cross-media retrieval via visual-auditory feature analysis and relevance feedback","authors":"Hong Zhang, Junsong Yuan, Xingyu Gao, Zhenyu Chen","author_ids":"1734058, 1746449, 1732399, 1760274","abstract":"Different types of multimedia data express high-level semantics from different aspects. How to learn comprehensive high-level semantics from different types of data and enable efficient cross-media retrieval becomes an emerging hot issue. There are abundant statistical and semantic correlations among heterogeneous low-level media content, which makes it challenging to query cross-media data effectively. In this paper, we propose a new cross-media retrieval method based on short-term and long-term relevance feedback. Our method mainly focuses on two typical types of media data, i.e. image and audio. First, we build multimodal representation via statistical canonical correlation between image and audio feature matrices, and define cross-media distance metric for similarity measure; then we propose optimization strategy based on relevance feedback, which fuses short-term learning results and long-term accumulated knowledge into the objective function. Experiments on image-audio dataset have demonstrated the superiority of our method over several existing algorithms.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"5bf866c4f120eba409a97a0d55d353361a87700c","venue_1":"ACM Multimedia","year":"2012","title":"Color filter for image search","authors":"Peng Wang, Dongqing Zhang, Jingdong Wang, Zhong Wu, Xian-Sheng Hua, Shipeng Li","author_ids":"1722767, 2991610, 1688516, 1782391, 1746102, 4973820","abstract":"Image search relying on surrounding texts can return reliably relevant images to some extent. Most recent efforts are focusing on utilizing visual contents to help users find images with specific visual requirements. In this demonstration, we show a color filter scheme for image search, which enables users to find images containing objects or scenes with their interested color. Color is one of the most crucial cues in describing visual contents and has been frequently used in various applications. The key components in developing our demo include salient object detection and perceptual color naming. The color filter in Microsoft Bing image search is developed using these techniques.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"b717d84d551de252300b9f161a5551162a936119","venue_1":"ACM Multimedia","year":"2012","title":"Query-driven iterated neighborhood graph search for large scale indexing","authors":"Jingdong Wang, Shipeng Li","author_ids":"1688516, 4973820","abstract":"In this paper, we address the approximate nearest neighbor (ANN) search problem over large scale visual descriptors. We investigate a simple but very effective approach, neighborhood graph search, which constructs a neighborhood graph to index the data points and conducts a local search, expanding neighborhoods with a best-first manner, for ANN search. Our empirical analysis shows that neighborhood expansion is very efficient, with <i>O(1)</i> cost, for a new NN candidate location, and has high chances to locate true NNs and hence it usually performs well. However, it often gets sub-optimal solutions since local search only checks the neighborhood of the current solution, or conducts exhaustive and continuous neighborhood expansions to find better solutions, which deteriorates the query efficiency.\n In this paper, we propose a query-driven iterated neighborhood graph search approach to improve the performance. We follow the iterated local search (ILS) strategy, widely-used in combinatorial optimization, to find a solution beyond a local optimum. We handle the key challenge in making neighborhood graph search adapt to ILS, Perturbation, which generates a new pivot to restart a local search. To this end, we present a criterion to check if the local search over a neighborhood graph arrives at the local solution. Moreover, we exploit the query and search history to design the perturbation scheme, resulting in a more effective search. The major benefit is avoiding unnecessary neighborhood expansions and hence more efficiently finding true NNs. Experimental results on large scale SIFT matching, similar image search, and shape retrieval with non-metric distance measures, show that our approach performs much better than previous state-of-the-art ANN search approaches.","cites":"16","conferencePercentile":"93.03797468"},{"venue":"ACM Multimedia","id":"510b7daf6cc18b16d78b117ce1702fd0148ebec2","venue_1":"ACM Multimedia","year":"2012","title":"A bag-of-objects retrieval model for web image search","authors":"Yang Yang, Linjun Yang, Gangshan Wu, Shipeng Li","author_ids":"1708973, 7866194, 1748584, 4973820","abstract":"Image search reranking has been an active research topic in recent years to boost the performance of the existing web image search engine which is mostly based on textual metadata of images. Various approaches have been proposed to rerank images for general queries and argue that, they may not necessarily be optimal for queries in specific domain, e.g., object queries, since the reranking algorithms are operated on whole images, instead of the relevant parts of images. In this paper, we propose a novel bag-of-objects retrieval model for image search reranking of object queries. Firstly, we employ a common object discovery algorithm to discover query-relevant objects from the search results returned by text-based image search engine. Then, the query and its result images are represented as a language model on the query relevant object vocabulary, based on which the ranking function can be derived. As the common object discovery is unreliable and may introduce noises, we propose to incorporate the attributes of the discovered objects, e.g., size, position, etc., into the ranking function through a linear model, and the weights on the object attributes can be learned. The experiments on two subsets of Web Queries dataset comprising object queries demonstrate that our approach can significantly outperform the existing reranking methods on object queries.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"12ff2d7f1e3456f57eb5f6c9def450531a8b4b58","venue_1":"ACM Multimedia","year":"2011","title":"Frontiers in multimedia search","authors":"Alan Hanjalic, Martha Larson","author_ids":"6741141, 1748247","abstract":"This outline summarizes our tutorial \"Frontiers in Multimedia Search\", whose goal is to provide insights into the most recent developments in the field of multimedia retrieval and to identify the issues and bottlenecks that could determine the directions of research focus for the coming years. We present an overview of new algorithms and techniques, particularly concentrating on those innovative approaches that are informed by neighboring fields including information retrieval, speech and language processing and network analysis. We also discuss evaluation of new algorithms, in particular, making use of crowdsourcing for the development of the necessary data sets.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"646725e4480a1f4e04c00377d8aac08fee10f7e6","venue_1":"ACM Multimedia","year":"2012","title":"Finding perfect rendezvous on the go: accurate mobile visual localization and its applications to routing","authors":"Heng Liu, Tao Mei, Jiebo Luo, Houqiang Li, Shipeng Li","author_ids":"1780798, 1788123, 1717319, 7179232, 4973820","abstract":"While on the go, more and more people are using their phones to enjoy ubiquitous location-based services (LBS). One of the fundamental problems of LBS is localization. Researchers are now investigating ways to use a phone-captured image for localization as it contains more scene context information than the embedded sensors. In this paper, we present a novel approach to mobile visual localization that accurately senses geographic scene context according to the current image (typically associated with a rough GPS position). Unlike most existing visual localization methods, the proposed approach is capable of providing a complete set of more accurate parameters about the scene geo---including the actual locations of both the mobile user and perhaps more importantly the captured scene along with the viewing direction. Our approach takes advantage of advanced techniques for large-scale image retrieval and 3D model reconstruction from photos. Specifically, we first perform joint geo-visual clustering in the cloud to generate scene clusters, with each scene represented by a 3D model. The 3D scene models are then indexed using a visual vocabulary tree structure. The phone-captured image is used to retrieve the relevant scene models, then aligned with the models, and further registered to the real-world map. Our approach achieves an estimation accuracy of user location within 14 meters, viewing direction within 9 degrees, and scene location within 21 meters. Such a complete set of accurate geo-parameters can lead to various LBS applications for routing that cannot be achieved with most existing methods. In particular, we showcase three novel applications: 1) accurate self-localization, 2) collaborative localization for rendezvous routing, and 3) routing for photographing. The evaluations through user studies indicate these applications are effective for facilitating the perfect rendezvous for mobile users.","cites":"24","conferencePercentile":"95.88607595"},{"venue":"ACM Multimedia","id":"c920cc6782136aea178ab65043cd59cdf7321080","venue_1":"ACM Multimedia","year":"2012","title":"When video search goes wrong: predicting query failure using search engine logs and visual search results","authors":"Christoph Kofler, Linjun Yang, Martha Larson, Tao Mei, Alan Hanjalic, Shipeng Li","author_ids":"2517570, 7866194, 1748247, 1788123, 6741141, 4973820","abstract":"The recent increase in the volume and variety of video content available online presents growing challenges for video search. Users face increased difficulty in formulating effective queries and search engines must deploy highly effective algorithms to provide relevant results. Although lately much effort has been invested in optimizing video search engine results, relatively little attention has been given to predicting for which queries results optimization is most useful, i.e., predicting which queries will fail. Being able to predict when a video search query would fail is likely to make the video search result optimization more efficient and effective, improve the search experience for the user by providing support in the query formulation process and in this way boost the development of video search engines in general. While insight about a query's performance in general could be obtained using the well-known concept of query performance prediction (QPP), we propose a novel approach for predicting a failure of a video search query in the specific context of a search session. Our 'context-aware query failure' prediction approach uses a combination of 'user indicators' and 'engine indicators' to predict whether a particular query is likely to fail in the context of a particular search session. User indicators are derived from the search log and capture the patterns of query (re)formulation behavior and the click-through data of a user during a typical video search session. Engine indicators are derived from the video search results list and capture the visual variance of search results that would be offered to the user for the given query. We validate our approach experimentally on a test set containing 1+ million video search queries and show its effectiveness compared to a set of conventional QPP baselines. Our approach achieves a 13% relative improvement over the baseline.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"72647916bb258662e9b620be235cb484cffba07b","venue_1":"ACM Multimedia","year":"2012","title":"Local visual words coding for low bit rate mobile visual search","authors":"Yue Wu, Shiyang Lu, Tao Mei, Jian Zhang, Shipeng Li","author_ids":"7136811, 1762181, 1788123, 1741109, 4973820","abstract":"Mobile visual search has attracted extensive attention for its huge potential for numerous applications. Research on this topic has been focused on two schemes: sending query images, and sending compact descriptors extracted on mobile phones. The first scheme requires about 30-40KB data to transmit, while the second can reduce the bit rate by 10 times. In this paper, we propose a third scheme for extremely low bit rate mobile visual search, which sends compressed visual words consisting of vocabulary tree histogram and descriptor orientations rather than descriptors. This scheme can further reduce the bit rate with few extra computational costs on the client. Specifically, we store a vocabulary tree and extract visual descriptors on the mobile client. A light-weight pre-retrieval is performed to obtain the visited leaf nodes in the vocabulary tree. The orientation of each local descriptor and the tree histogram are then encoded to be transmitted to server. Our new scheme transmits less than 1KB data, which reduces the bit rate in the second scheme by 3 times, and obtains about 30% improvement in terms of search accuracy over the traditional Bag-of-Words baseline. The time cost is only 1.5 secs on the client and 240 msecs on the server.","cites":"5","conferencePercentile":"70.88607595"},{"venue":"ACM Multimedia","id":"52b57012249ee210346802db4149ecfdb5569d87","venue_1":"ACM Multimedia","year":"2010","title":"Gesture and touch controlled video player interface for mobile devices","authors":"Shelley Buchinger, Ewald Hotop, Helmut Hlavacs, Francesca De Simone, Touradj Ebrahimi","author_ids":"2980017, 2657887, 1743771, 2315642, 1681498","abstract":"Today, mobile communication devices allow users to access a wide variety of multimedia contents and services. In order to improve user experience and device usability, the design of interfaces and interaction techniques for mobile devices have focused on new modalities, other than those used for desktop computers. In this paper, we describe a novel gesture controlled video player interface for mobile devices. The results of a usability study confirm that users would definitely like to adopt the major part of the proposed features. Furthermore, the responsiveness and reliability of the interface has been studied. Measured response times have been found to be within acceptable boundaries and the number of unrecognised haptic controls is limited.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"32f287cb5e1aa27bf1a61acf56476768c38e25f6","venue_1":"ACM Multimedia","year":"2011","title":"JIGSAW: interactive mobile visual search with multimodal queries","authors":"Yang Wang, Tao Mei, Jingdong Wang, Houqiang Li, Shipeng Li","author_ids":"1747927, 1788123, 1688516, 7179232, 4973820","abstract":"The traditional text-based visual search has not been sufficiently improved over the years to accommodate the new emerging demand of mobile users. While on the go, searching on one's phone is becoming pervasive. This paper presents an innovative application for mobile phone users to facilitate their visual search experience. By taking advantage of smart phone functionalities such as multi-modal and multi-touch interactions, users can more conveniently formulate their search intent, and thus search performance can be significantly improved. The system, called JIGSAW (Joint search with ImaGe, Speech, And Words), represents one of the first attempts to create an interactive and multi-modal mobile visual search application. The key of JIGSAW is the composition of an exemplary image query generated from the raw speech via multi-touch user interaction, as well as the visual search based on the exemplary image. Through JIGSAW, users can formulate their search intent in a natural way like playing a jigsaw puzzle on the phone screen: 1) a user speaks a natural sentence as the query, 2) the speech is recognized and transferred to text which is further decomposed to keywords through entity extraction, 3) the user selects preferred exemplary images that can visually represent his/her intent and composes a query image via multi-touch, and 4) the composite image is then used as a visual query to search similar images. We have deployed JIGSAW on a real-world phone system, evaluated the performance on one million images, and demonstrated that it is an effective complement to existing mobile visual search applications.","cites":"10","conferencePercentile":"82.94460641"},{"venue":"ACM Multimedia","id":"20471a9b43a7344a947159a10a20933e9b86aefc","venue_1":"ACM Multimedia","year":"2011","title":"Million-scale near-duplicate video retrieval system","authors":"Yang Cai, Linjun Yang, Wei Ping, Fei Wang, Tao Mei, Xian-Sheng Hua, Shipeng Li","author_ids":"1801727, 7866194, 2352591, 1723185, 1788123, 1746102, 4973820","abstract":"In this paper, we present a novel near-duplicate video retrieval system serving one million web videos. To achieve both the effectiveness and efficiency, a visual word based approach is proposed, which quantizes each video frame into a word and represents the whole video as a bag of words. The system can respond to a query in 41ms with 78.4% MAP on average.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"91a0c3a1a741da0cdec618483ac7e4b89bebed21","venue_1":"ACM Multimedia","year":"2011","title":"Alice's worlds of wonder: exploiting tags to understand images in terms of size and scale","authors":"Christoph Kofler, Martha Larson, Alan Hanjalic","author_ids":"2517570, 1748247, 6741141","abstract":"The 'Wonderlands of Size and Scale' system extends Flickr search functionality with a mechanism that exploits information implicit in image tag sets to filter images on the basis of characteristics related to size and scale. The innovative contribution of the 'Wonderlands' system is threefold: first, its use of an understanding of real-world physical entities depicted in images in terms of size and scale to filter images for display to the user; second, its application of our recently proposed 'Reading between the Tags' approach, which infers the real-world size of physical objects depicted in images by combining user-assigned image tags and natural language statistics mined from the Web; and, third, its trim realization of an engaging application that is implemented in a light-weight manner such that it can be executed as a live extension to the Flickr search engine. Results of a simple system-oriented evaluation support the conclusion that 'Wonderlands' reaches its aim of presenting users with images sorted with respect to the size- and scale-characteristics of their visually depicted content.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"420d4fb6954c58e378cabe4302162e8740efab3f","venue_1":"ACM Multimedia","year":"2003","title":"DOVE: drawing over video environment","authors":"Jiazhi Ou, Xilin Chen, Susan R. Fussell, Jie Yang","author_ids":"1962473, 1710220, 1692772, 1688428","abstract":"We demonstrate a multimedia system that integrates pen-based gesture and live video to support collaboration on physical tasks. The system combines network IP cameras, desktop PCs, and tablet PCs (or PDAs) to allow a remote helper to draw on a video feed of a workspace as he/she provides task instructions. A gesture recognition component enables the system both to normalize freehand drawings to facilitate communication with remote partners and to use pen-based input as a camera control device. The system also embeds some tools, such as controlled video delay, gesture delay, and remote camera pan-tilt-zoom control. The system provides a software environment for studying multimodal/multimedia communication for remote collaborative physical tasks.","cites":"14","conferencePercentile":"58.10810811"},{"venue":"ACM Multimedia","id":"1dc74a5d8f401c2a43c586ff5681fee43d69afe5","venue_1":"ACM Multimedia","year":"2012","title":"GeoMM'12: ACM international workshop on geotagging and its applications in multimedia","authors":"Liangliang Cao, Gerald Friedland, Martha Larson","author_ids":"2464399, 1797144, 1748247","abstract":"Geotagging is the process of adding geographical identification metadata to various media files such as photos, videos, websites, messages, and tweets. It is not limited to GPS sensor data but an extension of current multimedia files with a wide variety of location-specific information. The GeoMM'12 workshop presents research on recent research on geotagging within the context of multimedia analysis. This workshop aims to not only provide more cutting edge algorithms, but also motivate novel applications in this promising field.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"3ae1e19df7626caeeb73bdcecba3186f53a6f224","venue_1":"ACM Multimedia","year":"2007","title":"Introducing tangerine: a tangible interactive natural environment","authors":"Stefano Baraldi, Alberto Del Bimbo, Lea Landucci, Nicola Torpei, Omar Cafini, Elisabetta Farella, Augusto Pieracci, Luca Benini","author_ids":"2251862, 8196487, 1769560, 1813987, 3199667, 1777488, 2064130, 1710649","abstract":"In this paper we describe TANGerINE, a tangible tabletop environment in which users can interact with digital contents manipulating tangible smart objects. Such objects provide continuous data about their status through the embedded wireless sensors, while an overhead computer vision module tracks their position and orientation. Merging sensing data, the system is able to detect a richer language of gestures and manipulations both on the tabletop and in its surroundings, enabling for a more expressive interaction language across different contexts.","cites":"9","conferencePercentile":"63.54166667"},{"venue":"ACM Multimedia","id":"06e669f6ef3110654c1227f61d323e69fc133957","venue_1":"ACM Multimedia","year":"2006","title":"Tabletop community: artwork for visualization of social interactions using a bipartite network","authors":"Noriyuki Fujimura, Satoshi Fujiyoshi, Tom Hope, Takuichi Nishimura","author_ids":"2716672, 2606900, 2760903, 1765625","abstract":"\"Tabletop Community\" is an artwork that records interactions among users around a table. The artwork also visualizes a social network of users from data that show these interactions; the piece can present that network to other users as well. Its theme is self-recognition, which emerges from social interactions with others in a human network. It is intended to offer experiences of looking back at a user's past interactions with others within the perspective of a group (social network) in a visual manner. We have shown the artwork at Ubicomp2005. The paper shows the types of interactions, inferred from collected data and comments from the audience, which have delivered some improvements in the artwork since its original version.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"bb0b9001aff0aa353b65e0922d76bbeef00c3ce4","venue_1":"ACM Multimedia","year":"2006","title":"Tabletop community: visualization of real world oriented social network","authors":"Noriyuki Fujimura, Satoshi Fujiyoshi, Tom Hope, Takuichi Nishimura","author_ids":"2716672, 2606900, 2760903, 1765625","abstract":"We have undertaken a research project that visualizes a community, especially for events such as academic conferences. As research progresses, we have noticed that small gatherings of a few persons happen during events as a vital component of forming a community. We call these happenings Social Interactions. Typical situations that foster Social Interactions include gathering around a table. Therefore, we think it is possible to visualize larger communities through obtaining and processing Social Interaction data via table-like interfaces.As one part of group research project, here we introduce an art piece, named \"Tabletop Community\", that enables the visualization of Social Interactions around the table. Through this artwork system, users/participants easily record the state and atmosphere of each Interaction. The system visualizes the state of the entire community as an interactive network visualization. Here we introduce past results along with the current progress of the system.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"7129b1b37932758d3ad3b53d4f6354c7084f8b09","venue_1":"ACM Multimedia","year":"2015","title":"HyperMeeting: Supporting Asynchronous Meetings with Hypervideo","authors":"Andreas Girgensohn, Jennifer Marlow, Frank M. Shipman, Lynn Wilcox","author_ids":"2195286, 1680630, 1749811, 1691319","abstract":"While synchronous meetings are an important part of collaboration, it is not always possible for all stakeholders to meet at the same time. We created the concept of hypermeetings for meetings with asynchronous attendance. Such hypermeetings consist of a chain of video-recorded meetings with hyperlinks for navigating through them. Our HyperMeeting system supports the viewing of prior meetings during a videoconference. Natural viewing behavior such as pausing video generates hyperlinks between previous and current meetings. During playback, automatic link-following guided by playback plans present the relevant content to users. Playback plans take into account the user's meeting attendance and viewing history and match them with features such as topic and speaker segmentation. A user study showed that participants found hyperlinks useful but did not always understand where the links would take them. Experiences from longer-term use and the study results provide a good basis for future system improvements.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"4da2b83c58dc4693173cf3e91eeba80c88330166","venue_1":"ACM Multimedia","year":"2010","title":"User driven audio content navigation for spoken web","authors":"Ketki A. Dhanesha, Nitendra Rajput, Kundan Srivastava","author_ids":"2995322, 3304776, 3068571","abstract":"It is a common practice for us to skim textual content on a web page. While skimming, we usually skip words or phrases that are not of interest to us and we slow down our speed when the content seems to be of relevance to us. But when we listen to audio content, which is not persistent and is sequential, such skimming is not possible. In developing countries, cell-phone penetration is much higher than Internet penetration. Moreover, due to low literacy, voice is a convenient modality to access information. The skimming techniques are therefore more critical in the audio domain. In this paper, we describe the technique for navigating audio content while interacting with information systems in a client server environment, where a dumb phone is the client device. The paper presents techniques for skimming audio content and for placing markers in audio. The user studies conducted with 18 users for more than 1 month, in a live setting substantiates the usability and usefulness of the navigation techniques.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"56cbddd689b227b562e77db9ddf0e850b3bd3e4c","venue_1":"ACM Multimedia","year":"2005","title":"Scalable media streaming to interactive users","authors":"Marcus V. M. Rocha, Marcelo Maia, Ítalo S. Cunha, Jussara M. Almeida, Sérgio Vale Aguiar Campos","author_ids":"3257590, 3596974, 2033985, 8118988, 1808802","abstract":"Recently, a number of scalable stream sharing protocols have been proposed with the promise of great reductions in the server and network bandwidth required for delivering popular media content. Although the scalability of these protocols has been evaluated mostly for sequential user accesses, a high degree of interactivity has been observed in the accesses to several <i>real</i> media servers. Moreover, some studies have indicated that user interactivity can severely penalize the scalability of stream sharing protocols.This paper investigates alternative mechanisms for scalable streaming to interactive users. We first identify a set of workload aspects that are determinant to the scalability of classes of streaming protocols. Using real workloads and a new interactive media workload generator, we build a rich set of realistic synthetic workloads. We evaluate Bandwidth Skimming and Patching, two state-of-the-art streaming protocols, covering, with our workloads, a larger region of the design space than previous work. Finally, we propose and evaluate five optimizations to Bandwidth Skimming, the most scalable of the two protocols. Our best optimization reduces the average server bandwidth required for interactive workloads in up to 54%, for unlimited client buffers, and 29%, if buffers are constrained to 25% of media size.","cites":"22","conferencePercentile":"79.45544554"},{"venue":"ACM Multimedia","id":"c75774fa8437c8efb63a3cfff3930fbc42bc49b9","venue_1":"ACM Multimedia","year":"2013","title":"Efficient image and tag co-ranking: a bregman divergence optimization method","authors":"Lin Wu, Yang Wang, John Shepherd","author_ids":"1719109, 1747927, 3551303","abstract":"Ranking on image search has attracted considerable attentions. Many graph-based algorithms have been proposed to solve this problem. Despite their remarkable success, these approaches are restricted to their separated image networks. To improve the ranking performance, one effective strategy is to work beyond the separated image graph by leveraging fruitful information from manual semantic labeling (i.e., tags) associated with images, which leads to the technique of co-ranking images and tags, a representative method that aims to explore the reinforcing relationship between image and tag graphs. The idea of co-ranking is implemented by adopting the paradigm of random walks. However, there are two problems hidden in co-ranking remained to be open: the high computational complexity and the problem of out-of-sample. To address the challenges above, in this paper, we cast the co-ranking process into a Bregman divergence optimization framework under which we transform the original random walk into an equivalent optimal kernel matrix learning problem. Enhanced by this new formulation, we derive a novel extension to achieve a better performance for both in-sample and out-of-sample cases. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of our approach.","cites":"8","conferencePercentile":"83.11111111"},{"venue":"ACM Multimedia","id":"3d23d52c3699bb72e89a9ae3099bff9eb924ed78","venue_1":"ACM Multimedia","year":"2014","title":"Exploiting Correlation Consensus: Towards Subspace Clustering for Multi-modal Data","authors":"Yang Wang, Xuemin Lin, Lin Wu, Wenjie Zhang, Qing Zhang","author_ids":"1747927, 1720352, 1719109, 8031033, 1681771","abstract":"Often, a data object described by many features can be decomposed as multi-modalities, which always provide complementary information to each other. In this paper, we study subspace clustering for multi-modal data by effectively exploiting data correlation consensus across modalities, while keeping individual modalities well encapsulated. Our technique can yield a more ideal data similarity matrix, which encodes strong data correlations for the cross-modal data objects in the same subspace.\n To these ends, we propose a novel angular based regularizer coupled with our objective function, which is aided by trace lasso and minimized to yield sparse representation vectors encoding data correlations in multiple modalities. As a result, the sparse code vectors of the same cross-modal data have small angular difference so as to achieve the data correlation consensus simultaneously. This can generate a compatible data similarity matrix for multi-modal data. The final subspace clustering result is obtained by applying spectral clustering on such data similarity matrix. The effectiveness of our approach is validated by experiments conducted on real-world image datasets.","cites":"8","conferencePercentile":"87.34939759"},{"venue":"ACM Multimedia","id":"340f9e587346a22ef95a148327484e82ec69a264","venue_1":"ACM Multimedia","year":"2015","title":"Effective Multi-Query Expansions: Robust Landmark Retrieval","authors":"Yang Wang, Xuemin Lin, Lin Wu, Wenjie Zhang","author_ids":"1747927, 1720352, 1719109, 8031033","abstract":"Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users may convey different geometry information depending on the viewpoints and/or angles, and may subsequently yield very different results. In fact, dealing with the landmarks with shapes caused by the photography of q-users is often nontrivial and has never been studied.\n Motivated by this, in this paper we propose a novel framework, namely multi-query expansions, to retrieve semantically robust landmarks by two steps. Firstly, we identify the top-<i>k</i> photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible shape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Secondly, we propose a novel technique to generate the robust yet compact pattern set from the multi-query photos. To ensure redundancy-free and enhance the efficiency, we adopt the existing minimum-description-length-principle based pattern mining techniques to remove similar query photos from the (<i>k</i>+1) selected query photos. Then, a landmark retrieval rule is developed to calculate the ranking scores between mined pattern set and each photo in the database, which are ranked to serve as the final ranking list of landmark retrieval. Extensive experiments are conducted on real-world landmark datasets, validating the significantly higher accuracy of our approach.","cites":"4","conferencePercentile":"87.77777778"},{"venue":"ACM Multimedia","id":"8a61396ab3eae09d5a6b6bd020bb4433dec52473","venue_1":"ACM Multimedia","year":"2015","title":"Evento 360: Social Event Discovery from Web-scale Multimedia Collection","authors":"Jaeyoung Choi, Eungchan Kim, Martha Larson, Gerald Friedland, Alan Hanjalic","author_ids":"1692966, 2265122, 1748247, 1797144, 6741141","abstract":"We present Evento 360 (URL: http://evento360.info), an online interactive social event browser, which allows the user to explore events detected within a web-scale multimedia corpus. The system addresses five key aspects of social multimedia event detection and summarization: multimodality, scale, diversity of representations, noise of multimedia items, and missing metadata. The detection algorithm uses unsupervised clustering approach that exploits temporal, spatial and textual metadata. For each detected event cluster, to choose the best subset of photos that meet both relevance and diversity criteria, the system uses hierarchical clustering that exploits both visual and audio information. Evento 360's user interface provides a search feature that is not limited to a certain set of events, but rather can handle an arbitrary event query. It allows the user to retrieve and explore relevant events. The system scales well and is effective in producing high-quality summaries of the detected events.","cites":"4","conferencePercentile":"87.77777778"},{"venue":"ACM Multimedia","id":"1805fc34db66733ad1bdc3a9610dba7324e6ea2b","venue_1":"ACM Multimedia","year":"2010","title":"Exploiting noisy visual concept detection to improve spoken content based video retrieval","authors":"Stevan Rudinac, Martha Larson, Alan Hanjalic","author_ids":"2815960, 1748247, 6741141","abstract":"In this paper, we present a technique for unsupervised construction of concept vectors, concept-based representations of complete video units, from the noisy shot-level output of a set of visual concept detectors. We deploy these vectors to improve spoken-content-based video retrieval using Query Expansion Selection (QES). Our QES approach analyzes results lists returned in response to several alternative query expansions, applying a coherence indicator calculated on top-ranked items to choose the appropriate expansion. The approach is data driven, does not require prior training and relies solely on the analysis of the collection being queried and the results lists produced for the given query text. The experiments, performed on two datasets, TRECVID 2007/2008 and TRECVID 2009, demonstrate the effectiveness of our approach and show that a small set of well-selected visual concept detectors is sufficient to improve retrieval performance.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"1ec43ccf5e400ddc3f1d2720760d343de22dbe8c","venue_1":"ACM Multimedia","year":"2015","title":"Overview of the 2015 Workshop on Speech, Language and Audio in Multimedia","authors":"Guillaume Gravier, Gareth J. F. Jones, Martha Larson, Roeland Ordelman","author_ids":"1708671, 1750400, 1748247, 7461581","abstract":"The Workshop on Speech, Language and Audio in Multimedia (SLAM) positions itself at at the crossroad of multiple scientific fields (music and audio processing, speech processing, natural language processing and multimedia) to discuss and stimulate research results, projects, datasets and benchmarks initiatives where audio, speech and language are applied to multimedia data. While the first two editions were collocated with major speech events, SLAM'15 is deeply rooted in the multimedia community, opening up to computer vision and multimodal fusion. To this end, the workshop emphasizes video hyperlinking as an showcase where computer vision meets speech and language. Such techniques provide a powerful illustration of how multimedia technologies incorporating speech, language and audio can make multimedia content collections better accessible, and thereby more useful, to users.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"7179c11d32f241965130cfa506d467f2b0722881","venue_1":"ACM Multimedia","year":"2013","title":"ACM multimedia 2013 workshop on crowdsourcing for multimedia","authors":"Kuan-Ta Chen, Wei-Ta Chu, Martha Larson","author_ids":"6270307, 1766628, 1748247","abstract":"The topic \"Crowdsourcing for Multimedia\" encompasses the full range of techniques that combine human intelligence and a large number of individual contributors to advance the state of the art in multimedia research. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provided a forum for presenting new crowdsourcing techniques, exchanging innovative crowdsourcing ideas, and discussing crowdsourcing best practices for multimedia. The workshop program consisted of presented papers, a keynote speech and a panel discussion. A special feature of this year's workshop was the \"Crowdsourcing for Multimedia Ideas Competition\", the results of which were presented at the workshop.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"b198cb4c739c36868d7c800ef472cdb9b9e82a9d","venue_1":"ACM Multimedia","year":"2013","title":"Blending the physical and the virtual in music technology: from interface design to multi-modal signal processing","authors":"George Tzanetakis, Sidney Fels, Michael J. Lyons","author_ids":"1693065, 1749457, 1709339","abstract":"Recent years have seen a significant increase of interest in rich multi-modal user interfaces going beyond conventional mouse/keyboard/screen interaction. The new interface technologies are broadly impacting music technology and culture. New musical interfaces use a variety of sensing (and actuating) modalities to receive and present information to users, and often require techniques from signal processing and machine learning in order to extract and fuse high level information from noisy, high dimensional signals over time. Hence they pose many interesting signal processing challenges while offering fascinating possibilities for new research. At the same time the richness of possibilities for new forms of musical interaction requires a new approach to the design of musical technologies and has implications for performance aesthetics and music pedagogy. This tutorial begins with a general and gentle introduction to the theory and practice of the design of new technologies for musical creation and performance. It continues with an overview of signal processing and machine learning methods which are needed for more advanced work in new musical interface design.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"ec4b4068503a5ffcc115d2096e90c10238aaab4e","venue_1":"ACM Multimedia","year":"2010","title":"Flow: an interactive public artwork","authors":"Fiona Bowie, Sidney Fels, Morgan Hibbert","author_ids":"2256312, 1749457, 2883534","abstract":"This paper describes the conceptual, aesthetic, hardware, and software design of Flow, a photo/media-based permanent public interactive artwork in Vancouver, Canada. The work is located at street level in a new local community centre at one of the city's oldest intersections. In addition to the community centre location, it has a related interactive web component. It involves the animation and projection of continually recombining photographic images onto a large, interactive 4x4 array of electronically controlled switch glass windows. Over the course of the day and night, these photographic tableaux appear on the glass in combinations that depend upon image-to-image relationships, time of day, season and weather. In addition, lighting elements including water effect gobos are integrated at selected times of day. Images disappear when viewers inside the building come within close proximity to the work: the interactive windows respond to movement by changing from translucent to clear. In the daytime, when the projected image is off at the site, the work continues on the project's website, offering the visitor an interaction with the work. The work aims to provide an experience of the flux of people, animals, landscape and urban environment over time. It addresses the way landscape has transformed in response to colonialism, capital and local pressures, where change is rapid and histories are lost and rewritten.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"462836cf13085140bd7f2b8cd1f3fda12d0888c8","venue_1":"ACM Multimedia","year":"2013","title":"Crowdsourcing for multimedia research","authors":"Mohammad Soleymani, Martha Larson","author_ids":"2463695, 1748247","abstract":"Crowdsourcing techniques make use of intelligent contributions of large number of human crowdmembers. This tutorial introduces researchers to the applications of crowdsourcing to multimedia analysis with the aim of allowing them to understand the potentials and limitations of crowdsourcing tools and techniques. We emphasize the fact that crowdsourcing represents a further development along a pre-existing continuum of techniques, and discuss the added advantages that new developments offer. We provide a basic overview of human computation, with an emphasis on example cases in which crowdsourcing has been applied to generate data sets, to improve automatic multimedia content analysis, and to elicit user needs or multimedia system requirements. Different techniques and considerations in using human computation methods to acquire high-quality data and annotations are discussed and demonstrated.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"81e3a67cf470e85d74236ce9cb061aaf089fd1ba","venue_1":"ACM Multimedia","year":"2005","title":"Automatic generating detail-on-demand hypervideo using MPEG-7 and SMIL","authors":"Tina T. Zhou, Tamás D. Gedeon, Jesse S. Jin","author_ids":"2641330, 1768305, 1763037","abstract":"Detail-on-demand hypervideo will provide a powerful mechanism to allow viewers to see additional information of video segments through hyperlinks. A large number of tools are devoted to the identification of selectable video objects and the synchronization mechanisms for linking additional information to selectable video objects. We focus here on the automatic generation of additional information and the integration of the additional information to its corresponding selectable video object. We demonstrate a method using the Multimedia Content Description Interface defined in MPEG-7 and the Synchronized Multimedia Integration Language (SMIL) to automatically generate detail-on-demand hypervideos.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"78a0684f183dfdd6a0dd67fe961609995b408428","venue_1":"ACM Multimedia","year":"2013","title":"How do we deep-link?: leveraging user-contributed time-links for non-linear video access","authors":"Raynor Vliegendhart, Babak Loni, Martha Larson, Alan Hanjalic","author_ids":"3285093, 1689471, 1748247, 6741141","abstract":"This paper studies a new way of accessing videos in a non-linear fashion. Existing non-linear access methods allow users to jump into videos at points that depict specific visual concepts or that are likely to elicit affective reactions. We believe that deep-link comments, which occur unprompted on social video sharing platforms, offer a new opportunity beyond existing methods. With deep-link comments, viewers express themselves about a particular moment in a video by including a time-code. Deep-link comments are special because they reflect viewer perceptions of noteworthiness, that include, but extend beyond depicted conceptual content and induced affective reactions. Based on deep-link comments collected from YouTube, we develop a Viewer Expressive Reaction Variety (VERV) taxonomy that captures how viewers deep-link. We validate the taxonomy with a user study on a crowdsourcing platform and discuss how it extends conventional relevance criteria. We carry out experiments which show that deep-link comments can be automatically filtered and sorted into VERV categories.","cites":"3","conferencePercentile":"60"},{"venue":"ACM Multimedia","id":"1d52f948ead7af53aabcff26ca95101b932c7ebe","venue_1":"ACM Multimedia","year":"2012","title":"ACM multimedia 2012 workshop on crowdsourcing for multimedia","authors":"Kuan-Ta Chen, Wei-Ta Chu, Martha Larson, Wei Tsang Ooi","author_ids":"6270307, 1766628, 1748247, 1678873","abstract":"Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing. Through presented papers, invited talks and a panel, the workshop will promote interactive discussion on the scope and research potentials of crowdsourcing. The goal is to provide information to the multimedia research community on the principles of crowdsourcing and to inspire researchers to address the limitations of current studies by innovative use of human computation and collective intelligence. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"309649bd77ad66f66d22f38353ca9c3cef5e6869","venue_1":"ACM Multimedia","year":"2012","title":"LikeLines: collecting timecode-level feedback for web videos through user interactions","authors":"Raynor Vliegendhart, Martha Larson, Alan Hanjalic","author_ids":"3285093, 1748247, 6741141","abstract":"Conventional online video players do not make the inner structure of the video apparent, making it hard to jump straight to the interesting parts. Our LikeLines system provides its users with a navigable heat map of interesting regions for the videos they are watching. Its novelty lies in its combination of content analysis and both explicit and implicit user interactions. The system can be readily used and deployed to collect large amounts of interaction data needed for in-depth research on timecode-level feedback.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"415c629c0e0891f597940700a0f91780e47a3b68","venue_1":"ACM Multimedia","year":"2012","title":"Intent and its discontents: the user at the wheel of the online video search engine","authors":"Alan Hanjalic, Christoph Kofler, Martha Larson","author_ids":"6741141, 2517570, 1748247","abstract":"We embrace the position of the user in the driver's seat of the video search engine by proposing a principled framework for multimedia retrieval that moves beyond <i>what</i> users are searching for also to encompass <i>why</i> they search. This 'why' is understood as the reason, purpose or immediate goal behind a user information need, which we identify as the underlying user 'intent'. Breaking information needs down into a topical dimension representing 'what' and an intent dimension representing 'why' will allow online video search engines to provide users with more satisfying search results. Until now, research on intent has remained small scale, limited by the lack of a systematic method for arriving at possible dimensions of user intent that provide productive areas of inquiry for multimedia research. We demonstrate how mining information from user descriptions of video information needs on the social Web makes it possible to identify useful intent categories for online video search and carry out validation experiments showing that these categories display enough invariance to be successfully modeled by a video search engine. In a final experiment, we demonstrate the potential for these categories to improve video retrieval with a large user study confirming that users associate salient differences within topically homogenous video search engine results lists with these intent categories. This reveals the potential to refine video results list using user intent.","cites":"14","conferencePercentile":"92.08860759"},{"venue":"ACM Multimedia","id":"22b1fc329b25cd961497e9f1f70d41b94c2dcbc2","venue_1":"ACM Multimedia","year":"2014","title":"How 'How' Reflects What's What: Content-based Exploitation of How Users Frame Social Images","authors":"Michael Riegler, Martha Larson, Mathias Lux, Christoph Kofler","author_ids":"1763552, 1748247, 6517033, 2517570","abstract":"In this paper, we introduce the concept of intentional framing, defined as the sum of the choices that a photographer makes on how to portray the subject matter of an image. We carry out analysis experiments that demonstrate the existence of a correspondence between image similarity that is calculated automatically on the basis of global feature representations, and image similarity that is perceived by humans at the level of intentional frames. Intentional framing has profound implications: The existence of a fundamental image-interpretation principle that explains the importance of global representations in capturing human-perceived image semantics reaches beyond currently dominant assumptions in multimedia research. The ability of fast global-feature approaches to compete with more `sophisticated' approaches, which are computationally more complex, is demonstrated using a simple search method (SimSea) to classify a large (2M) collection of social images by tag class. In short, intentional framing provides a principled connection between human interpretations of images and lightweight, fast image processing methods. Moving forward, it is critical that the community explicitly exploits such approaches, as the social image collections that we tackle, continue to grow larger.","cites":"8","conferencePercentile":"87.34939759"},{"venue":"ACM Multimedia","id":"ab9c130d7f51fd06e31d9d6d6d2db935d1dccecd","venue_1":"ACM Multimedia","year":"1996","title":"Vibrotactile Feedback in Delicate Virtual Reality Operations","authors":"Li-Te Cheng, Rick Kazman, John A. Robinson","author_ids":"1690296, 1699020, 2195760","abstract":"Virtual environments are often unsuitable for delicate operations because of the poverty of feedback, particularly tactile and force feedback. However, force feedback done properly requires large, heavy, expensive equipment. We have experimented with a particular form of tactile feedback using vibration as a substitute for force feedback. Substituting vibration for force feedback is intuitively appealing because it is cheap and low cost. But is it effective? Unfortunately, the answer is both \" yes \" and \" no' \". We describe an experiment evaluating the effect of vibrotactile sensory substitution on user performance during a grasping task with delicate virtual objects. We found that adding vibrotactile feedback to visual and audio feedback improved task completion time for novice users, but led to increased grasp pressure over repeated uses of the system.","cites":"14","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"d26dc3ce0a9d7c9e63c0cec7f801fb7d0d88f93f","venue_1":"ACM Multimedia","year":"2010","title":"Advances in multimedia retrieval, part i: frontiers in multimedia search","authors":"Alan Hanjalic, Martha Larson","author_ids":"6741141, 1748247","abstract":"This paper presents an overview of the Part I of the one-day tutorial on Advances in Multimedia Retrieval. The material in this paper should be considered in combination with the Part II (Video Search Engines) of this tutorial.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"7af3084aa104760d16bb380d4d37ddac0442b850","venue_1":"ACM Multimedia","year":"2014","title":"What are the Fashion Trends in New York?","authors":"Shintami Chusnul Hidayati, Kai-Lung Hua, Wen-Huang Cheng, Shih-Wei Sun","author_ids":"3413443, 3082412, 1711298, 1689158","abstract":"Fashion is a reflection of the society of a period. Given that New York City is one of the world's fashion capitals, understanding its change in fashion becomes a way to know the society and the times. To keep up with fashion trends, it is important to know what's \" in\" and what's \"out\" for a season. Though the fashion trends have been analyzed by fashion designers and fashion analysts for a long time, this issue has been ignored in multimedia science. In this paper, we present a novel algorithm that automatically discovers visual style elements representing fashion trends for a certain season. The visual style elements are discovered based on the stylistic coherent and unique characteristics. The experimental results demonstrate the effectiveness of our proposed method through a large number of catwalk show videos.","cites":"8","conferencePercentile":"87.34939759"},{"venue":"ACM Multimedia","id":"11a47001daa4125dc5e1ba60e57b5e9048c28ff5","venue_1":"ACM Multimedia","year":"2007","title":"Establishing the utility of non-text search for news video retrieval with real world users","authors":"Michael G. Christel","author_ids":"7307726","abstract":"TRECVID participants have enjoyed consistent success using storyboard interfaces for shot-based retrieval, as measured by TRECVID interactive search mean average precision (MAP). However, much is lost by only looking at MAP, and especially by neglecting to bring in representatives of the target user communities to conduct such tasks. This paper reports on the use of within-subjects experiments to reduce subject variability and emphasize the examination of specific video search interface features for their effectiveness in interactive retrieval and user satisfaction. A series of experiments is surveyed to illustrate the gradual realization of getting non-experts to utilize non-textual query features through interface adjustments. Notably, the paper explores the use of the search system by government intelligence analysts, concluding that a variety of search methods are useful for news video retrieval and lead to improved satisfaction. This community, dominated by text search system expertise but still new to video and image search, performed better with and favored a system with image and concept query capabilities over an exclusive text-search system. The user study also found that sports topics mean nothing for this user community and tens of relevant shots collected into the answer set are considered enough to satisfy the information need. Lessons learned from these user interactions are reported, with recommendations on both interface improvements for video retrieval systems and enhancing the ecological validity of video retrieval interface evaluations.","cites":"18","conferencePercentile":"76.30208333"},{"venue":"ACM Multimedia","id":"33fdd0d410e4fa1149888e203fbc68b3aebe8de7","venue_1":"ACM Multimedia","year":"2011","title":"Interactive digital scrapbook generation for travel photos based on design principles of typography","authors":"Jung-Yu Yeh, Min-Chun Hu, Wen-Huang Cheng, Ja-Ling Wu","author_ids":"2618888, 2345240, 1711298, 1686629","abstract":"To facilitate the photo management and sharing tasks, many application tools have been developed to generate pleasant photo slideshows, collages, or scrapbooks by applying simple templates/layouts and visual effects. In this work, we propose a convenient digital scrapbook generating system for travel photos, named as IS-Scrapbook, which keeps the virtues while dismisses the drawbacks of conventional digital photo presentation styles. The IS-Scrapbook system has three friendly attributes compared to other photo presentation tools. First, aiming to attract the audience, we highlight objects more meaningful or familiar to the viewer, e.g. the landmark and the protagonist in the photos. Second, five basic design principles of typography, i.e. proximity, contrast, balance, color harmony and repetition, are applied to produce more vivacious layouts. Third, the system automatically generates a digital scrapbook with a default layout, and the user can further adjust photo positions and enrich each page by sketching or inserting dialog bubbles with the aid of the developed user interface. User study shows that the proposed work enhances the experiences of photo browsing and gives a brand-new way of photo sharing.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"489ccbf56cf9e0b518558a90396e3ec79eb0b0a1","venue_1":"ACM Multimedia","year":"2008","title":"Photo navigator","authors":"Chi-Chang Hsieh, Wen-Huang Cheng, Chia-Hu Chang, Yung-Yu Chuang, Ja-Ling Wu","author_ids":"2147143, 1711298, 2900551, 3032320, 1686629","abstract":"Nowadays, travel has become a popular activity for people to relax their body and mind. Taking photos is then often an inevitable and frequent event during one's trip for recording the enjoyable experience. To help people to relive the wonderful travel experience they had recorded in photos, this paper presents a system, Photo Navigator, for enhancing the photo browsing experience by creating a new browsing style with a realistic feel to users as being into the scenes and taking a trip back in time to revisit the place. The proposed system is characterized by two main features. First, it better reveals the spatial relations among photos and offers a strong sense of space by taking users to fly into the scenes. Second, it is fully automatic and makes plausible for novice users to utilize the 3D technologies that are traditionally complex to manipulate. The proposed system is compared with two other photo browsing tools, ACDSee's photo slideshow and Microsoft's PhotoStory. User studies show that people would comparatively favor the browsing style we offer and appreciate the ease to create such a style.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"c78139300995bbee819fa3e6ea9901243751f42f","venue_1":"ACM Multimedia","year":"2001","title":"Automated authoring of coherent multimedia discourse in conversation systems","authors":"Michelle X. Zhou, Shimei Pan","author_ids":"1705742, 2728986","abstract":"We are building a full-fledged multimedia conversation framework called Responsive Information Architect (RIA), using a combination of AI and multimedia techniques. Here we describe RIA's capability of automated authoring of a coherent multimedia discourse, which is used by RIA to express itself when conversing with a user. Specifically, we focus on explaining three unique features of our automated authoring approach: automated authoring of multimedia inter¿action acts, dynamic insertion of multimedia punctuation acts, and systematic design of cross-media acts.","cites":"6","conferencePercentile":"48.46938776"},{"venue":"ACM Multimedia","id":"016b13a9e16e4a5c88a9e44ce293072cd17fdbd5","venue_1":"ACM Multimedia","year":"2004","title":"Finding the right shots: assessing usability and performance of a digital video library interface","authors":"Michael G. Christel, Neema Moraveji","author_ids":"7307726, 3106937","abstract":"The authors developed a system in which visually dense displays of thumbnail imagery in storyboard views are used for shot-based video retrieval. The views allow for effective retrieval, as evidenced by the success achieved by expert users with the system in interactive query for NIST TRECVID 2002 and 2003. This paper demonstrates that novice users also achieve comparatively high retrieval performance with these views using the TRECVID 2003 benchmarks. Through an analysis of the user interaction logs, heuristic evaluation, and think-aloud protocol, the usability of the video information retrieval system is appraised with respect to shot-based retrieval. Design implications are presented based on these TRECVID usability evaluations regarding efficient, effective information retrieval interfaces to locate visual information from video corpora.","cites":"24","conferencePercentile":"79.65686275"},{"venue":"ACM Multimedia","id":"a054e7e500fb898cdef03cb1ddfb570d26a48e07","venue_1":"ACM Multimedia","year":"2012","title":"Making use of eye tracking information in image collection creation and region annotation","authors":"Tina Walber","author_ids":"3244997","abstract":"The goal of this work is to implicitly gain information about images from human eye movements and to use this information to improve the handling of images. Users' points of gaze are measured with an eye tracker while they are viewing or tagging images. By means of the gaze data, image selections will be created automatically and tags will be assigned to specific image regions. So far, we have shown that one can assign given tags to regions in a manually segmented image.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"a38a185ba9965198d2d5363e2e42b41503fdf609","venue_1":"ACM Multimedia","year":"2015","title":"Evolution of a Tabletop Telepresence System through Art and Technology","authors":"Anthony Dunnigan, John Doherty, Daniel Avrahami, Jacob T. Biehl, Patrick Chiu, Chelhwon Kim, Qiong Liu, Henry Tang, Lynn Wilcox","author_ids":"2374899, 3881379, 2667384, 7457467, 2895008, 2263279, 1794500, 2334519, 1691319","abstract":"New technologies arise in a number of ways. They may come from advances in scientific research, through new combinations of existing technologies, or by simply imagining what might be possible in the future. This video describes the evolution of Tabletop Telepresence, a system for remote collaboration through desktop videoconferencing combined with a digital desk. Tabletop Telepresence began as a collection of camera, projector, videoconferencing and user interaction technologies. Working together; artists and research scientists combined these technologies into a means of sharing paper documents between remote desktops, interacting with those documents, requesting services (such as translation), and communicating through a videoconference.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"729bfc2cfce4686824235342d626e3a7744b61f0","venue_1":"ACM Multimedia","year":"2001","title":"SVG for navigating digital news video","authors":"Michael G. Christel, Chang Huang","author_ids":"7307726, 1750913","abstract":"Scalable Vector Graphics (SVG) is a language for describing two-dimensional graphics in XML, specifically vector graphic shapes, images, and text. SVG is a new World Wide Web Consortium (W3C) Candidate Recommendation as of November 2000, and this paper describes how SVG provides an ideal framework for presenting manipulable, interactive summarizations into a multimedia information repository. Specifically, we present VIBE and map SVG interfaces into a digital news video library for delivery through web browsers. Pan-and-zoom visualizations of video through SVG are discussed.","cites":"2","conferencePercentile":"26.02040816"},{"venue":"ACM Multimedia","id":"02d16cb1694191a9d49aadbf1bb522537d2051bd","venue_1":"ACM Multimedia","year":"2011","title":"Mixing remote locations using shared screen as virtual stage","authors":"Shingo Uchihashi, Tsutomu Tanzawa","author_ids":"1713254, 2885416","abstract":"This paper introduces a visual communication system that integrates images displayed on a shared screen and people in front of the screen using depth information obtained from a stereo camera. Conventional video conferencing systems convey PC screen contents and video of local and remote participants in different channels, making interactions among them difficult. The proposed system maps persons in the foreground onto the screen preserving relative position and size. A series of preliminary experiments is conducted to compare the performance of the proposed system against conventional systems.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"73e49942a3535dfcb3376433ef5663dad9673ae3","venue_1":"ACM Multimedia","year":"2001","title":"Improvising camera control for capturing meeting activities using a floor plan","authors":"Shingo Uchihashi","author_ids":"1713254","abstract":"This paper describes camera control interfaces for capturing meetings and presentations into multimedia documents. While technologies are maturing to deliver multimedia documents over network, skilled human hands are still required to create the contents. We had dug into the problem and found that some portion of it derives from current camera control systems, which only provide interfaces for incremental navigations. Presets are provided for some systems to avoid cumbersome manipulations, but the difficulty to improvise controls remains untouched. We introduced an interface using floor plan for selecting an arbitrary area to be captured. We also conducted a study to compare our method with other two typical camera control interfaces. The results revealed that our method was significantly better than a typical joystick-metaphor interface. Although an interface with presets resulted superior for completing tasks in limited conditions, the participants judge the use of a floor plan to be equally good in respect to ease of camera navigation as they intended. They also indicated possible improvement for our interface to close in on the one with presets.","cites":"6","conferencePercentile":"48.46938776"},{"venue":"ACM Multimedia","id":"226f308cb4ef5ca08b012521c5641a8f16d22d9b","venue_1":"ACM Multimedia","year":"1995","title":"Model-Based Notion Estimation for Synthetic Animations","authors":"Maneesh Agrawala, Andrew C. Beers, Navin Chaddha","author_ids":"1820412, 2680929, 1693680","abstract":"One approach to performing motion estimation on synthetic animations is to treat them as video sequences and use standard image-based motion estimation methods. Alternatively, we can take advantage of information used in rendering the animation to guide the motion estimation algorithm. This information includes the 3D movements of the objects in the scene and the projection transformations from 3D world space into screen space. In this paper we examine how to use this high level object motion information to perform fast, accurate block-based motion estimation for synthetic anima-tions. The optical ow eld is a 2D vector eld describing the translational motion of each pixel from frame to frame. Our motion estimation algorithm rst computes the optical ow eld, based on the object motion information. We then combine the per-pixel motion information for a block of pixels to create a single 2D projective matrix that best encodes the motion of all the pixels in the block. The entries of the 2D matrix are determined using a least squares formulation. Our algorithms are more accurate and much faster in algorithmic complexity than many image-based motion estimation algorithms.","cites":"8","conferencePercentile":"10"},{"venue":"ACM Multimedia","id":"72d1df784b867d894f22d54c0375018ad41ef0b4","venue_1":"ACM Multimedia","year":"1997","title":"VideoTrails: Representing and Visualizing Structure in Video Sequences","authors":"Vikrant Kobla, David S. Doermann, Christos Faloutsos","author_ids":"2926946, 1679490, 1702392","abstract":"The problem of determining the physical and semantic structure of an extended video sequence is essential for providing appropriate processing, indexing and retrieval capabilities for video databases. In this paper, we describe a novel technique which reduces a sequence of MPEG encoded video frames to a trail of points in a low dimensional space. In this space, we can cluster frames, analyze transitions between clusters and compute properties of the resulting trail. By classifying portions of the trail as either stationary or transitional, we are able to detect gradual edits between shots. Furthermore, tracking the interaction of clusters over time, we l a y the groundwork for the complete analysis and representation of the video's physical and semantic structure.","cites":"31","conferencePercentile":"73.80952381"},{"venue":"ACM Multimedia","id":"1a6c346d1f24d41c73f86c4f52a93be5651c8647","venue_1":"ACM Multimedia","year":"2011","title":"ARA: the active reading application","authors":"Gene Golovchinsky, Scott Carter, Anthony Dunnigan","author_ids":"2724097, 1804229, 2374899","abstract":"The Active Reading Application (ARA) brings the familiar experience of writing on paper to the tablet. The application augments paper-based practices with audio, the ability to review annotations, and sharing. It is designed to make it easier to review, annotate, and comment on documents by individuals and groups. ARA incorporates several patented technologies and draws on several years of research and experimentation.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"56c699e8c96d83010a85f6843944e642e223c4a8","venue_1":"ACM Multimedia","year":"2010","title":"Embedded media marker: linking multimedia to paper","authors":"Qiong Liu, Chunyuan Liao, Lynn Wilcox, Anthony Dunnigan, Bee Liew","author_ids":"1794500, 2686363, 1691319, 2374899, 1961723","abstract":"An Embedded Media Marker (EMM) is a transparent mark printed on a paper document that signifies the availability of additional media associated with that part of the document. Users take a picture of the EMM using a camera phone, and the media associated with that part of the document is displayed on the phone. Unlike bar codes, EMMs are nearly transparent and thus do not interfere with the document appearance. Retrieval of media associated with an EMM is based on image features of the document within the EMM boundary. Unlike other feature-based retrieval methods, the EMM clearly indicates to the user the existence and type of media associated with the document location. A semi-automatic authoring tool is used to place an EMM at a location in a document, in such a way that it encompasses sufficient identification features with minimal disturbance to the original document. We will demonstrate how to create an EMM-enhanced document, and how the EMM enables access to the associated media on a cell phone.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"7a9e5f89fb634e759c49e1be124851f5648506ed","venue_1":"ACM Multimedia","year":"2010","title":"The virtual chocolate factory: mixed reality industrial collaboration and control","authors":"Maribeth Back, Don Kimber, Eleanor G. Rieffel, Anthony Dunnigan, Bee Liew, Sagar Gattepally, Jonathan Foote, Jun Shingu, Jim Vaughan","author_ids":"2131565, 2178004, 1710276, 2374899, 1961723, 2344212, 1797460, 2565333, 2133761","abstract":"We show several aspects of a complex mixed reality system that we have built and deployed in a real-world factory setting. In our system, virtual worlds, augmented realities, and social and mobile applications are all fed from the same infrastructure. In collaboration with TCHO[1], a chocolate maker in San Francisco, we built a virtual \"mirror\" world of a real-world chocolate factory and its processes. Sensor data is imported into the multi-user 3D environment from hundreds of sensors on the factory floor. The resulting virtual factory is used for simulation, visualization, and collaboration, using a set of interlinked, real-time layers of information. Another part of our infrastructure is designed to support appropriate industrial uses for mobile devices such as cell phones and tablet computers. We deployed this system at the real-world factory in 2009, and it is now is daily use there. By simultaneously developing mobile, virtual, and web-based display and collaboration environments, we aimed to create an infrastructure that did not skew toward one type of application but that could serve many at once, interchangeably. Through this mixture of mobile, social, mixed and virtual technologies, we hope to create systems for enhanced collaboration in industrial settings between physically remote people and places, such as factories in China with managers in the US.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"2e6e808ec248454857e099daff1055923aacd541","venue_1":"ACM Multimedia","year":"2007","title":"DOTS: support for effective video surveillance","authors":"Andreas Girgensohn, Don Kimber, Jim Vaughan, Tao Yang, Frank M. Shipman, Thea Turner, Eleanor G. Rieffel, Lynn Wilcox, Francine Chen, Anthony Dunnigan","author_ids":"2195286, 2178004, 2133761, 1720792, 1749811, 2729644, 1710276, 1691319, 1800347, 2374899","abstract":"DOTS (Dynamic Object Tracking System) is an indoor, real-time, multi-camera surveillance system, deployed in a real office setting. DOTS combines video analysis and user interface components to enable security personnel to effectively monitor views of interest and to perform tasks such as tracking a person. The video analysis component performs feature-level foreground segmentation with reliable results even under complex conditions. It incorporates an efficient greedy-search approach for tracking multiple people through occlusion and combines results from individual cameras into multi-camera trajectories. The user interface draws the users. attention to important events that are indexed for easy reference at a later time. Different views within the user interface provide spatial information for easier navigation. Our system, with over twenty video cameras installed in hallways and other public spaces in our office building, has been in constant use for almost a year.","cites":"28","conferencePercentile":"87.5"},{"venue":"ACM Multimedia","id":"6184db6401b77df1af73e4224ac82f70f9df392b","venue_1":"ACM Multimedia","year":"1994","title":"Digital Video Segmentation","authors":"Arun Hampapur, Terry E. Weymouth, Ramesh Jain","author_ids":"1690709, 3164501, 4521564","abstract":"The data driven, bottom up approach to video segmentation has ignored the inherent structure that exists in video. This work uses the model driven approach to digital video segmentation. Mathematical models of video based on video production techniques are formulated. These models are used to classify the edit effects used in video and film production. The classes and models are used to systematically design the feature detectors for detecting edit effects in digital video. Digital video segmentation is formulated as a feature based classification problem. Experimental results from segmenting cable television programming with cuts, fades, dissolves and page translate edits are presented.","cites":"126","conferencePercentile":"97.45762712"},{"venue":"ACM Multimedia","id":"4b75513400884854a8cb89728dffb6e5f571913f","venue_1":"ACM Multimedia","year":"2005","title":"Post-bit: embodied video contents on tiny stickies","authors":"Takashi Matsumoto, Anthony Dunnigan, Maribeth Back","author_ids":"1749267, 2374899, 2131565","abstract":"Post-Bit is a small e-paper device modeled after paper Post-Its&#174;<sup>1</sup>. We explored and designed interfaces to handle multi-media contents with paper-like manipulations using this e-paper device. The functions of each Post-Bit combined the affordance of physical tiny sticky memos and digital handling of information. At this stage of the design, we have prototyped two features of the interface: connecting computer-based workspaces and physical workspaces (using a function called Drop-Beyond-Drag), and tangible and tactile operation of multi-media contents. In this paper, we present the integrated design and functionality of the Post-Bit system's main components as shown in the video scenario.","cites":"1","conferencePercentile":"14.10891089"},{"venue":"ACM Multimedia","id":"494d19952421ffed9d94e0eaba35ebb7a2ae59f2","venue_1":"ACM Multimedia","year":"1997","title":"Designing Interactive Multimedia (Panel)","authors":"Lori L. Scarlatos, Rudy Darken, Komei Harada, Carrie Heeter, Richard Muller, Ben Shneiderman","author_ids":"3131135, 1738071, 3096475, 2181452, 3760547, 1740403","abstract":"This paper presents contrasting metaphors and paradigms for designing interactive media interfaces. Multimedia interface designers and researchers with diverse backgrounds discuss their own design approaches and important design 'issues. Discussion of these issues is continued beyond this paper through a web site:","cites":"6","conferencePercentile":"22.61904762"},{"venue":"ACM Multimedia","id":"068c7cf1c7a13cfa6ce17fa00d814ec634acafa2","venue_1":"ACM Multimedia","year":"2011","title":"Augmenting mobile city-view image retrieval with context-rich user-contributed photos","authors":"Yin-Hsi Kuo, Wen-Yu Lee, Winston H. Hsu, Wen-Huang Cheng","author_ids":"1692811, 2595525, 1716836, 1711298","abstract":"With the growth of mobile devices, the needs for location-based services are emerging. Taking the advantage of the GPS information, we can roughly estimate a user's location. However, it is necessary to leverage extra information (e.g., photos) to precisely locate the object of interest through mobile devices for further applications such as mobile search. Users can simply take a picture (with GPS enabled) of an interesting target to retrieve the building information. Therefore, the raise of real-time building recognition or retrieval system becomes a challenging problem. The most recent approaches are to recognize buildings by the street-view images; however, the query photos from mobile devices usually contain different lighting conditions. In order to provide a more robust city-view image retrieval system, we propose to augment the visual diversity of database images by integrating the context-rich user-contributed photos from social media. Preliminary experimental results show that the street-view images can provide different angles of the target whereas the user-contributed photos can enhance the diversity of the target. Besides, for the real-time retrieval system, we also combine both visual and GPS constraints in the retrieval process on inverted indexing so that we can achieve a real-time retrieval system.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"b4f45be105631c7e87eac4464fd3c1de04e9c1b4","venue_1":"ACM Multimedia","year":"2012","title":"Actions speak louder than words: searching human action video based on body movement","authors":"Yan-Ching Lin, Min-Chun Hu, Wen-Huang Cheng, Yung-Huan Hsieh, Hong-Ming Chen","author_ids":"2692074, 2345240, 1711298, 2096888, 3578661","abstract":"Human action video search is a frequent demand in multimedia applications, and conventional video search schemes based on keywords usually fail to correctly find relevant videos due to noisy video tags. Observing the widespread use of Kinect-like depth cameras, we propose to search human action videos by directly performing the target action with body movements. Human actions are captured by Kinect and the recorded depth information is utilized to measure the similarity between the query action and each human action video in the database. We use representative depth descriptors without learning optimization to achieve real-time and promising performance as compatible as those of the leading methods based on color images and videos. Meanwhile, a large Depth-included Human Action video dataset, namely DHA, is collected to prove the effectiveness of the proposed video search system.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"60432ee6d6a5e3787d0643f799b65522bc01aac1","venue_1":"ACM Multimedia","year":"2015","title":"Predicting Image Memorability by Multi-view Adaptive Regression","authors":"Houwen Peng, Kai Li, Bing Li, Haibin Ling, Weihua Xiong, Weiming Hu","author_ids":"2484788, 2359779, 7137749, 1805398, 2214855, 1696704","abstract":"The images we encounter throughout our lives make different impressions on us: Some are remembered at first glance, while others are forgotten. This phenomenon is caused by the intrinsic memorability of images revealed by recent studies [5,6]. In this paper, we address the issue of automatically estimating the memorability of images by proposing a novel <i>multi-view adaptive regression</i> (MAR) model. The MAR model provides an effective mapping of visual features to memorability scores by taking advantage of robust feature selection and multiple feature integration. It consists of three major components: an adaptive loss function, an adaptive regularization and a multi-view modeling strategy. Moreover, we design an alternating direction method (ADM) optimization algorithm to solve the proposed objective function. Experimental results on the MIT benchmark dataset show the superiority of the proposed model compared with existing image memorability prediction methods.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"cf5de0ea6451cc56b4fb91a81cb79c01c5b3fb23","venue_1":"ACM Multimedia","year":"2012","title":"Clothing genre classification by exploiting the style elements","authors":"Shintami Chusnul Hidayati, Wen-Huang Cheng, Kai-Lung Hua","author_ids":"3413443, 1711298, 3082412","abstract":"This paper presents a novel approach to automatically classify the upperwear genre from a full-body input image with no restrictions of model poses, image backgrounds, and image resolutions. Five style elements, that are crucial for clothing recognition, are identified based on the clothing design theory. The corresponding features of each of these style elements are also designed. We illustrate the effectiveness of our approach by showing that the proposed algorithm achieved overall precision of 92.04%, recall of 92.45%, and F score of 92.25% with 1,077 clothing images crawled from popular online stores.","cites":"6","conferencePercentile":"75"},{"venue":"ACM Multimedia","id":"bbada73ab4e97f61274fc2d8ac8c7b624e10e847","venue_1":"ACM Multimedia","year":"2013","title":"Physiognomy master: a novel personality analysis system based on facial features","authors":"Che-Hao Hsu, Kai-Lung Hua, Wen-Huang Cheng","author_ids":"8143468, 3082412, 1711298","abstract":"In this demo, we present the proposed \"Physiognomy Master.\" It is a novel practical personality analysis system based on facial features. We first design five facial features that are essential for face reading. We then construct a database to record the facial features' values from a number of volunteers. In the meantime, the volunteers are also invited to fill out a professional personality test. The relations between the facial features and the personality traits are then learned. Given a test subject or an input frontal face image, the proposed system will produce the associated personality report by fusing the personality scores from the people who have similar facial features in the constructed database. The fusing mechanism is based on the idea that people with similar facial features possess similar personality characteristics. The proposed system is a powerful tool in numerous kinds of social interactions, such as personnel selection, team composition, and marriage matching.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"9c1fdde1156bb77443d26eb5072c830dbc7e30af","venue_1":"ACM Multimedia","year":"1995","title":"Multimedia Documents with Elastic Time","authors":"Michelle Y. Kim, Junehwa Song","author_ids":"2003203, 1789470","abstract":"Time is an essential component i n i n terac-tive m ultimedia documents (or systems). We present the elastic time model for multimedia documents. Using the metaphor of a spring system , it allows authors to associate with each multimedia object a minimum and a maximum length and a length at rest. Authors can connect the (elastic) objects by dening temporal relationships among them. If the given speci-cation is consistent, a document is produced which is also elastic, with a minimum, a maximum , and an optimal length. As such, our elastic model associates with a document a range of feasible solutions in addition to an optimal one. The author can then select from the acceptable range an alternative length for the document, and the system will compute a revised solution that takes the additional global constraint i n to eect. The system can answer questions, such as: \\Can I show this multimedia presentation in 10 minutes? If so, how should all the objects be scheduled?\" Furthermore, the system also takes fairness into consideration and distributes any necessary stretching or shrinking across multimedia objects contained in a document. As such, the elastic time model provides expressive p o w er and exibility in document au-thoring and browsing. The proposed approach has been implemented in Smalltalk/OS2.","cites":"47","conferencePercentile":"70"},{"venue":"ACM Multimedia","id":"41a1d3958c2f56770861fe0c750d3f042e047d82","venue_1":"ACM Multimedia","year":"2004","title":"Narrative abstraction model for story-oriented video","authors":"Byunghee Jung, Tae-Yeong Kwak, Junehwa Song, Yoon-Joon Lee","author_ids":"3334567, 3080242, 1789470, 1751540","abstract":"TV program review services, especially drama review services, are one of the most popular video on demand services on the Web. In this paper, we propose a novel video abstraction model for a review service of story-oriented video such as dramas. In a drama review service, viewers want to understand the story in a short time and service providers want to provide video abstracts at minimum cost. The proposed model enables the automatic creation of a video abstract that still allows viewers to understand the overall story of the source video. Also, the model has a flexible structure so that the duration of an abstract can be adjusted depending on the requirements given by viewers. We get clues for human understanding of a story from scenario writing rules and editorial techniques which are popularly used in the process of video producing. We have implemented the proposed model and successfully applied it to several TV dramas.","cites":"10","conferencePercentile":"62.00980392"},{"venue":"ACM Multimedia","id":"159d529d321514e454f4cb1ee9533a2634ba12fe","venue_1":"ACM Multimedia","year":"2011","title":"Hybrid image summarization","authors":"Hao Xu, Jingdong Wang, Xian-Sheng Hua, Shipeng Li","author_ids":"1742479, 1688516, 1746102, 4973820","abstract":"In this paper, we address a problem of managing tagged images with hybrid summarization. We formulate this problem as finding a few image exemplars to represent the image set semantically and visually and solve it in a hybrid way by exploiting both visual and textual information associated with images. We propose a novel approach, called Homogeneous and Heterogeneous Message Propagation (H<sup>2</sup>MP), which extends affinity propagation that only works over homogeneous relations to heterogeneous relations. The summary obtained by our approach is both visually and semantically satisfactory. The experimental results demonstrate the effectiveness and efficiency of the proposed approach.","cites":"8","conferencePercentile":"77.8425656"},{"venue":"ACM Multimedia","id":"093542181342c1ab4f59201804f665e90f9d0468","venue_1":"ACM Multimedia","year":"2011","title":"The role of attractiveness in web image search","authors":"Bo Geng, Linjun Yang, Chao Xu, Xian-Sheng Hua, Shipeng Li","author_ids":"7712150, 7866194, 2857022, 1746102, 4973820","abstract":"Existing web image search engines are mainly designed to optimize topical relevance. However, according to our user study, attractiveness is becoming a more and more important factor for web image search engines to satisfy users' search intentions. Important as it can be, web image attractiveness from the search users' perspective has not been sufficiently recognized in both the industry and the academia. In this paper, we present a definition of web image attractiveness with three levels according to the end users' feedback, including perceptual quality, aesthetic sensitivity and affective tune. Corresponding to each level of the definition, various visual features are investigated on their applicability to attractiveness estimation of web images. To further deal with the unreliability of visual features induced by the large variations of web images, we propose a contextual approach to integrate the visual features with contextual cues mined from image EXIF information and the associated web pages. We explore the role of attractiveness by applying it to various stages of a web image search engine, including the online ranking and the interactive reranking, as well as the offline index selection. Experimental results on three large-scale web image search datasets demonstrate that the incorporation of attractiveness can bring more satisfaction to 80% of the users for ranking/reranking search results and 30.5% index coverage improvement for index selection, compared to the conventional relevance based approaches.","cites":"15","conferencePercentile":"89.79591837"},{"venue":"ACM Multimedia","id":"a9706bb650568d58c182fbb024bc39907b26bc42","venue_1":"ACM Multimedia","year":"2011","title":"Modeling social strength in social media community via kernel-based learning","authors":"Jinfeng Zhuang, Tao Mei, Steven C. H. Hoi, Xian-Sheng Hua, Shipeng Li","author_ids":"2146277, 1788123, 1741126, 1746102, 4973820","abstract":"Modeling continuous social strength rather than conventional binary social ties in the social network can lead to a more precise and informative description of social relationship among people. In this paper, we study the problem of social strength modeling (SSM) for the users in a social media community, who are typically associated with diverse form of data. In particular, we take Flickr---the most popular online photo sharing community---as an example, in which users are sharing their experiences through substantial amounts of multimodal contents (e.g., photos, tags, geo-locations, friend lists) and social behaviors (e.g., commenting and joining interest groups). Such heterogeneous data in Flickr bring opportunities yet challenges to the research community for SSM. One of the key issues in SSM is how to effectively explore the heterogeneous data and how to optimally combine them to measure the social strength. In this paper, we present a kernel-based learning to rank framework for inferring the social strength of Flickr users, which involves two learning stages. The first stage employs a kernel target alignment algorithm to integrate the heterogeneous data into a holistic similarity space. With the learned kernel, the second stage rectifies the pair-wise learning to rank approach to estimating the social strength. By learning the social strength graph, we are able to conduct collaborative recommendation and collective classification. The promising results show that the learning-based approach is effective for SSM. Despite being focused on Flickr, our technique can be applied to model social strength of users in any other social media community.","cites":"24","conferencePercentile":"95.33527697"},{"venue":"ACM Multimedia","id":"422b5555ae51440de257ad51534d59dea5313ae8","venue_1":"ACM Multimedia","year":"2015","title":"Multi-Sensor Cello Recordings for Instantaneous Frequency Estimation","authors":"Fabian-Robert Stöter, Michael Müller, Bernd Edler","author_ids":"2206907, 3664398, 2243485","abstract":"Estimating the fundamental frequency (F0) of a signal is a well studied task in audio signal processing with many applications. If the F0 varies over time, the complexity increases, and it is also more difficult to provide ground truth data for evaluation. In this paper we present a novel dataset of cello recordings addressing the lack of reference annotations for musical instruments. Besides audio data, we include sensor recordings capturing the finger position on the fingerboard which is converted into an instantaneous frequency estimate. In speech processing, the electroglottograph (EGG) is able to capture the excitation signal of the vocal tract, which is then used to generate a reference instantaneous F0. Inspired by this approach, we included high speed video camera recordings to extract the excitation signal originating from the moving string. The derived data can be used to analyze vibratos --- a very commonly used playing style. The dataset is released under a Creative Commons license.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"c79178a47403f317f837e4a8aa9fd03bfed1dfc7","venue_1":"ACM Multimedia","year":"2011","title":"Multiclass object detection by combining local appearances and context","authors":"Limin Wang, Yirui Wu, Tong Lu, Kang Chen","author_ids":"1716238, 3065505, 2285916, 1723702","abstract":"In this paper, we present a novel approach for multiclass object detection by combining local appearances and contextual constraints. We first construct a multiclass Hough forest of local patches, which can well deal with multiclass object deformations and local appearance variations, due to randomization and discrimination of the forest. Then, in the object hypothesis space, a new multiclass context model is proposed to capture relative location constraints, disambiguating appearance inputs in multiclass object detection. Finally, multiclass objects are detected with a greedy search algorithm efficiently. Experimental evaluations on two image data sets show that the combination of local appearances and context achieves state-of-the-art performance in multiclass object detection.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"a7b70353762d62ff80cf1c4210b38beaf0e69c34","venue_1":"ACM Multimedia","year":"2011","title":"Environmental sound classification for scene recognition using local discriminant bases and HMM","authors":"Feng Su, Li Yang, Tong Lu, Gongyou Wang","author_ids":"7018836, 5151236, 2285916, 2378235","abstract":"Analysis and classification of auditory scenes or contexts play important roles in content-based indexing and retrieval of multimedia databases and context-aware applications. In this paper, we propose an environmental sound and auditory scene recognition scheme that focuses on efficient feature representation and classfication of the unstructured composition of a scene (for example, restaurant, street, beach, etc.). We propose to use the local discriminant bases (LDB) technique to identify the discriminatory time-frequency subspace for environmental sounds and then use it for corresponding feature extraction. Based on LDB, we present two recognition models, with or without explicit sound event modeling, for auditory scenes, in which the hidden Markov model (HMM) is used to depict the characteristics and correlations among various events that constitute the scene. The experimental results demonstrate the effectiveness of the proposed approach for auditory scene classification.","cites":"7","conferencePercentile":"74.9271137"},{"venue":"ACM Multimedia","id":"0036c2a84a0b1a39d008c8dee93b6e763323b7da","venue_1":"ACM Multimedia","year":"2016","title":"Time Matters: Multi-scale Temporalization of Social Media Popularity","authors":"Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Tao Mei","author_ids":"1746414, 1711298, 1699819, 1788123","abstract":"The evolution of social media popularity exhibits rich temporality, i.e., popularities change over time at various levels of temporal granularity. This is influenced by temporal variations of public attentions or user activities. For example, popularity patterns of street snap on Flickr are observed to depict distinctive fashion styles at specific time scales, such as season-based periodic fluctuations for Trench Coat or one-off peak in days for Evening Dress. However, this fact is often overlooked by existing research of popularity modeling. We present the first study to incorporate multiple time-scale dynamics into predicting online popularity. We propose a novel computational framework in the paper, named Multi-scale Temporalization, for estimating popularity based on multi-scale decomposition and structural reconstruction in a tensor space of user, post, and time by joint low-rank constraints. By considering the noise caused by context inconsistency, we design a data rearrangement step based on context aggregation as preprocessing to enhance contextual relevance of neighboring data in the tensor space. As a result, our approach can leverage multiple levels of temporal characteristics and reduce the noise of data decomposition to improve modeling effectiveness. We evaluate our approach on two large-scale Flickr image datasets with over 1.8 million photos in total, for the task of popularity prediction. The results show that our approach significantly outperforms state-of-the-art popularity prediction techniques, with a relative improvement of 10.9%-47.5% in terms of prediction accuracy.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"60946e101b4e34967198ecf3ce846f59707dc130","venue_1":"ACM Multimedia","year":"2004","title":"Interactive retrieval of 3D shape models using physical objects","authors":"Hiroyasu Ichida, Yuichi Itoh, Yoshifumi Kitamura, Fumio Kishino","author_ids":"2895181, 1707181, 1690228, 3289052","abstract":"We present a novel method for interactive retrieval of 3D shapes using ysical objects. Our method is based on simple ysical 3D interaction with a set of tangible blocks. As the user connects blocks, the system automatically recognizes the shape of the constructed ysical structure and picks similar 3D shape models from a preset model database, in real time. Our system fully supports interactive retrieval of 3D shape models in an extremely simple fashion, which is completely non-verbal and cross-cultural. These advantages make it an ideal interface for inexperienced users, previously barred from many applications that include 3D shape retrieval tasks.","cites":"14","conferencePercentile":"70.58823529"},{"venue":"ACM Multimedia","id":"c4e692651e13b51d4143d3426ea5fe0f153543e7","venue_1":"ACM Multimedia","year":"2006","title":"GVU-PROCAMS: enabling novel projected interfaces","authors":"Jay Summet, Matthew Flagg, James M. Rehg, Gregory D. Abowd, Neil Weston","author_ids":"2585396, 2990157, 1692956, 1732524, 8150887","abstract":"Front projection allows large displays to be deployed relatively easily. However, it is sometimes difficult to find a location to place a projector, especially for ad-hoc installations. Additionally, front projection suffers from shadows and occlusions, making it ill-suited for interactive displays. The GVU-PROCAMS system allows programmers to deploy projectors and displays easily in arbitrary locations by enabling enhanced keystone correction via warping on 3D hardware. In addition, it handles the calibration of multiple projectors using computer vision to produce a redundantly illuminated surface. Redundant illumination offers robustness in the face of occlusions, providing a user with the experience of a rear-projected surface. This paper presents a stand-alone application (WinPVRP) and a programming system (GVU-PROCAMS) that easily allows others to create projected displays with enhanced warping and redundant illumination.","cites":"2","conferencePercentile":"29.79274611"},{"venue":"ACM Multimedia","id":"ff31230838fe359da204412d236b16679b91aa49","venue_1":"ACM Multimedia","year":"2006","title":"Enabling secure distribution of digital media to SD-cards","authors":"Dulce B. Ponceleon, Stefan Nusser, Vladimir Zbarsky, Julian A. Cerruti, Sigfredo I. Nin","author_ids":"1802641, 1772974, 2272350, 2802890, 2379243","abstract":"As the marketplace for digital media increases we witness the rise of new media distribution models where timely delivery, convenience, privacy and personalization are essential features of competitive offerings. Consumers are looking for innovative ways to access content in a service-oriented manner that suits their mobile life style. This paper describes a prototype standard-based system that allows the secure and fast download of content to SD-Card-enabled consumer devices. Content is protected by Content Protection for Recordable Media (CPRM) technology; specifically we use CPRM SD-Video. Our prototype shows Digital Media Terminals (DMT, also denoted kiosks) that enable innovative rental and purchase models. Our solution combines IBM's plug-in web services, Panasonic's leading-edge devices and Porto Media's high-speed data transfer to Secure Digital Memory Flash Cards (SD-Cards).","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"d60a823015087f35e9cdfb5ab7afed5d6be833cc","venue_1":"ACM Multimedia","year":"2006","title":"3D model metrieval based on volumetric extended gaussian image and hierarchical self organizing map","authors":"Jiqi Zhang, Hau-San Wong, Zhiwen Yu","author_ids":"3304367, 3238615, 1705015","abstract":"In this paper, we introduce a novel shape signature, called Volumetric Extended Gaussian Image (VEGI). It captures the volumetric distribution of a 3D mesh model along the latitude-longitude direction without conventional pose normalization and is translation and scaling invariant. Rotation invariance is accomplished by further calculating the spherical harmonic transform of this directional distribution. Due to the completeness and orthonormality properties of spherical harmonics, the VEGI also provides multi-resolution description of a model so that a multi-level indexing scheme based on Hierarchical Self Organizing Map (HSOM) can be established to improve retrieval efficiency. Experimental results show that our retrieval architecture has high discriminative power and outperforms many existing methods.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"f53dedb219bcc227c1fc8628350f283cc8843ff7","venue_1":"ACM Multimedia","year":"2009","title":"Learning distance metric for regression by semidefinite programming with application to human age estimation","authors":"Bo Xiao, Xiaokang Yang, Yi Xu, Hongyuan Zha","author_ids":"1719582, 1795291, 1734114, 1750350","abstract":"A good distance metric for the input data is crucial in many pattern recognition and machine learning applications. Past studies have demonstrated that learning a metric from labeled samples can significantly improve the performance of classification and clustering algorithms. In this paper, we investigate the problem of learning a distance metric that measures the semantic similarity of input data for regression problems. The particular application we consider is human age estimation. Our guiding principle for learning the distance metric is to preserve the local neighborhoods based on a specially designed distance as well as to maximize the distances between data that are not in the same neighborhood in the semantic space.Without any assumption about the structure and the distribution of the input data, we show that this can be done by using semidefinite programming. Furthermore, the low-level feature space can be mapped to the high-level semantic space by a linear transformation with very low computational cost. Experimental results on the publicly available FG-NET database show that 1) the learned metric correctly discovers the semantic structure of the data even when the amount of training data is small and 2) significant improvement over the traditional Euclidean metric for regression can be obtained using the learned metric. Most importantly, simple regression methods such as <i>k</i> nearest neighbors (kNN), combined with our learned metric, become quite competitive (and sometimes even superior) in terms of accuracy when compared with the state-of-the-art human age estimation approaches.","cites":"20","conferencePercentile":"85.12396694"},{"venue":"ACM Multimedia","id":"0c201e633c23f1b69d095067315ff457bb227ac8","venue_1":"ACM Multimedia","year":"2005","title":"Cooking navi: assistant for daily cooking in kitchen","authors":"Reiko Hamada, Jun Okabe, Ichiro Ide, Shin'ichi Satoh, Shuichi Sakai, Hidehiko Tanaka","author_ids":"2400207, 3133361, 1679187, 1700567, 1720879, 3264865","abstract":"We are developing a cooking navigation system, which helps even a novice user to cook several recipes in parallel without failure, while improving an advanced user's skill further. To realize this, the system optimizes the cooking procedure considering the following restrictions: (1) Duration of cooking, (2) Accuracy of cooking, and (3) Learning effect, by providing appropriate instructions to user's at the right timing, making full use of multimedia information. The users should be able to cook perfectly and comfortably just by following the text, video and audio provided by the system. According to the result of a preliminary experiment, all users from novice to experienced cooks could finish two dishes in parallel while enjoyeing the cooking very much. The result of a questionnaire shows the effectiveness of the multimedia navigation that we propose.","cites":"30","conferencePercentile":"84.15841584"},{"venue":"ACM Multimedia","id":"6475f90fe42ace8dc5d53059580db25cd148a04f","venue_1":"ACM Multimedia","year":"2012","title":"AttachedShock: facilitating moving targets acquisition on augmented reality devices using goal-crossing actions","authors":"Chuang-Wen You, Yung-Huan Hsieh, Wen-Huang Cheng","author_ids":"1702472, 2096888, 1711298","abstract":"The prevalence of augmented reality devices in our daily lives offers increasing opportunities for users to navigate the real world. However, as users move, on-screen targets move unpredictably, and eventually disappear from the screen in mobile navigation scenarios. The changing target movement pattern creates difficulty for users in selecting the targets on time before targets escape from the screen. This study proposes a novel target selecting technique, AttachedShock, for easing target selection tasks on augmented reality devices by crossing a naturally expanding wave pattern that is attached to targets. We evaluated the effectiveness of the proposed technique by conducting comparative studies on measuring the performance of four techniques under various mobile navigation scenarios. The results indicate that the proposed technique assists users in selecting moving targets to improve the error rate substantially, by a minimum of 61.75%, and incurs acceptable distractions to users, compared to other techniques.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"6f382a9513a04fab248d82cb3faed91afca916fb","venue_1":"ACM Multimedia","year":"2012","title":"Human action recognition and retrieval using sole depth information","authors":"Yan-Ching Lin, Min-Chun Hu, Wen-Huang Cheng, Yung-Huan Hsieh, Hong-Ming Chen","author_ids":"2692074, 2345240, 1711298, 2096888, 3578661","abstract":"Observing the widespread use of Kinect-like depth cameras, in this work, we investigate into the problem of using sole depth data for human action recognition and retrieval in videos. We proposed the use of simple depth descriptors without learning optimization to achieve promising performances as compatible to those of the leading methods based on color images and videos, and can be effectively applied for real-time applications. Because of the infrared nature of depth cameras, the proposed approach will be especially useful under poor lighting conditions, e.g. the surveillance environments without sufficient lighting. Meanwhile, we proposed a large Depth-included Human Action video dataset, namely DHA, which contains 357 videos of performed human actions belonging to 17 categories. To the best of our knowledge, the DHA is one of the largest depth-included video datasets of human actions.","cites":"8","conferencePercentile":"81.48734177"},{"venue":"ACM Multimedia","id":"47612cc60f0f8bd69cad139b9e181df6191413ba","venue_1":"ACM Multimedia","year":"2010","title":"Enhanced exploration of oral history archives through processed video and synchronized text transcripts","authors":"Michael G. Christel, Scott M. Stevens, Bryan Maher, Julieanna Richardson","author_ids":"7307726, 1755566, 2971380, 3046888","abstract":"A digital video library of over 900 hours of video and 18000 stories from The HistoryMakers was used by 266 students, faculty, librarians, and life-long learners interacting with a system providing multiple search and viewing capabilities over a trial period of several months. User demographics and actions were logged with this multimedia collection, providing quantitative and qualitative metrics on system use. These transaction logs were complemented with heuristic evaluation, interviews, and contextual inquiry with representative users. Collectively, these mixed methods informed the development of the next generation web-based interface for the HistoryMakers video oral histories to improve access to and dissemination of this rich cultural resource. In particular, the feature of a synchronized text transcript in the video player for the narratives merited further investigation. Such an interface has not seen widespread use in digital video players available on the web, yet was valued highly by oral history archive viewers. A user study with 27 participants measured the utility of the HistoryMakers web interface incorporating the synchronized transcript video player for stated fact-finding and open-ended tasks. For life oral histories, an aligned text transcript is valued for both tasks, with the video rated significantly more useful for open-ended tasks over fact-finding. These results suggest a task-dependent role of modality in presentation of oral histories, with synchronized transcripts rated highly across tasks.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"8ba868573c307f89eedef342cbf72d54d9de2f22","venue_1":"ACM Multimedia","year":"2012","title":"Action tutor: real-time exemplar-based sequential movement assessment with kinect sensor","authors":"Chi-Wen Chen, Min-Chun Hu, Wen-Huang Cheng, Che-Han Chang, Jui-Hsin Lai, Ja-Ling Wu","author_ids":"2847370, 2345240, 1711298, 3106354, 2653162, 1686629","abstract":"With the aid of depth camera, such as Microsoft Kinect, the difficulty of vision-based posture estimation is greatly decreased, and human action analysis has achieved a wide range of applications. However, there is still much to do to develop effective movement assessment technique, which bridges the results of human posture estimation and the understanding of human action performance. In this work, we propose an action tutor system which enables the user to interactively retrieve the learning exemplar of the target action movement and to immediately acquire motion instructions while learning it in front of the Kinect. In the retrieval stage, non-linear time warping algorithms are designed to retrieve video segments similar to the query movement roughly performed by the user. In the learning stage, the user learns according to the selected video exemplar, and the motion assessment including both static and dynamic differences is presented to the user in a more effective and organized way, helping him/her to perform the action movement correctly.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"71f134f6841f3850057948e7374af068e7703171","venue_1":"ACM Multimedia","year":"2010","title":"Bateau ivre: an artistic markerless outdoor mobile augmented reality installation on a riverboat","authors":"Christian Jacquemin, Wai Kit Chan, Matthieu Courgeon","author_ids":"1717038, 2302369, 3237926","abstract":"<i>Bateau Ivre</i> is a project presented on the Seine River to make a large audience aware of the possible developments of Augmented Reality through an artistic installation in a mobile outdoor environment. The installation could be viewed from a ship by a large audience without specific equipment, through nightly video-projection on the River banks. The augmentation of the physical world was implemented through real-time image processing for live special effects such as contouring, particles, or non-realistic rendering. The artistic purpose of the project was to immerge the audience into a non-realistic view of the River banks that would differ from the traditional tourist tours that highlight the main landmarks of Paris classical architecture.\n The implemented software applied standard algorithms for special effects to a live video stream and reprojected these effects on the captured scenes to combine the physical world with its modified image. An analysis of the project output reveals that the impact of the effects in mobile SAR varies a lot, and does not correspond to the visual impact on a standard desktop screen.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"5a0d45f31a8e1403518596fb38a319a588f5178a","venue_1":"ACM Multimedia","year":"2014","title":"An Event Driven Fusion Approach for Enjoyment Recognition in Real-time","authors":"Florian Lingenfelser, Johannes Wagner, Elisabeth André, Gary McKeown, William Curran","author_ids":"2565410, 1788048, 4843293, 2228246, 5448294","abstract":"Social signals and interpretation of carried information is of high importance in Human Computer Interaction. Often used for affect recognition, the cues within these signals are displayed in various modalities. Fusion of multi-modal signals is a natural and interesting way to improve automatic classification of emotions transported in social signals. Throughout most present studies, uni-modal affect recognition as well as multi-modal fusion, decisions are forced for fixed annotation segments across all modalities. In this paper, we investigate the less prevalent approach of event driven fusion, which indirectly accumulates asynchronous events in all modalities for final predictions. We present a fusion approach, handling short-timed events in a vector space, which is of special interest for real-time applications. We compare results of segmentation based uni-modal classification and fusion schemes to the event driven fusion approach. The evaluation is carried out via detection of enjoyment-episodes within the audiovisual Belfast Story-Telling Corpus.","cites":"5","conferencePercentile":"80.72289157"},{"venue":"ACM Multimedia","id":"293a0c383a42550d640b6af141c5c10822900606","venue_1":"ACM Multimedia","year":"2013","title":"The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time","authors":"Johannes Wagner, Florian Lingenfelser, Tobias Baur, Ionut Damian, Felix Kistler, Elisabeth André","author_ids":"1788048, 2565410, 3314405, 3048626, 2844803, 4843293","abstract":"Automatic detection and interpretation of social signals carried by voice, gestures, mimics, etc. will play a key-role for next-generation interfaces as it paves the way towards a more intuitive and natural human-computer interaction. The paper at hand introduces Social Signal Interpretation (SSI), a framework for real-time recognition of social signals. SSI supports a large range of sensor devices, filter and feature algorithms, as well as, machine learning and pattern recognition tools. It encourages developers to add new components using SSI's C++ API, but also addresses front end users by offering an XML interface to build pipelines with a text editor. SSI is freely available under GPL at http://openssi.net.","cites":"29","conferencePercentile":"97.33333333"},{"venue":"ACM Multimedia","id":"038e1b9826d1fc0887dbd0c87bb6a1983393874b","venue_1":"ACM Multimedia","year":"2008","title":"Mitigating the impact of hardware defects on multimedia applications: a cross-layer approach","authors":"Kyoungwoo Lee, Aviral Shrivastava, Minyoung Kim, Nikil D. Dutt, Nalini Venkatasubramanian","author_ids":"1731941, 1726554, 6089942, 1781660, 1732742","abstract":"Increasing exponentially with each technology generation, hardware-induced soft errors pose a significant threat for the reliability of mobile multimedia devices. Since traditional hardware error protection techniques incur significant power and performance overheads, this paper proposes a cooperative cross-layer approach that exploits existing error control schemes at the application layer to mitigate the impact of hardware defects. Specifically, we propose error detection codes in hardware, drop and forward recovery in middleware, and error-resilient video encoding at the application level to effectively and efficiently combat soft errors with minimal overheads. Experimental evaluation on standard test video streams demonstrates that our cooperative error-aware method for video encoding improves performance by 60% and energy consumption by 58% with even better reliability at the cost of only 3% quality degradation on average, as compared to an error correction code based hardware protection technique. Combining intelligent schemes to select a recovery mechanism can guide system designers to trade off multiple constraints such as performance, power, reliability, and QoS.","cites":"8","conferencePercentile":"61.69724771"},{"venue":"ACM Multimedia","id":"2b84630680e2c906f8d7ac528e2eb32c99ef203a","venue_1":"ACM Multimedia","year":"2014","title":"We are not All Equal: Personalizing Models for Facial Expression Analysis with Transductive Parameter Transfer","authors":"Enver Sangineto, Gloria Zen, Elisa Ricci, Nicu Sebe","author_ids":"1716310, 2933565, 1878028, 1703601","abstract":"Previous works on facial expression analysis have shown that person specific models are advantageous with respect to generic ones for recognizing facial expressions of new users added to the gallery set. This finding is not surprising, due to the often significant inter-individual variability: different persons have different morphological aspects and express their emotions in different ways. However, acquiring person-specific labeled data for learning models is a very time consuming process. In this work we propose a new transfer learning method to compute personalized models without labeled target data Our approach is based on learning multiple person-specific classifiers for a set of <i>source</i> subjects and then directly transfer knowledge about the parameters of these classifiers to the <i>target</i> individual. The transfer process is obtained by learning a regression function which maps the data distribution associated to each source subject to the corresponding classifier's parameters. We tested our approach on two different application domains, Action Units (AUs) detection and spontaneous pain recognition, using publicly available datasets and showing its advantages with respect to the state-of-the-art both in term of accuracy and computational cost.","cites":"15","conferencePercentile":"93.57429719"},{"venue":"ACM Multimedia","id":"43799a4760719aedbc8d5aadd851742594e60016","venue_1":"ACM Multimedia","year":"2014","title":"You Talkin' to Me?: Recognizing Complex Human Interactions in Unconstrained Videos","authors":"Bo Zhang, Yan Yan, Nicola Conci, Nicu Sebe","author_ids":"1696318, 1703972, 3058987, 1703601","abstract":"Nowadays, due to the exponential growth of the user generated videos and the prevailing videos sharing communities such as YouTube and Hulu, recognizing complex human activities in the wild becomes increasingly important in the research community. These videos are hard to study due to the frequent changes of camera viewpoint, multiple people moving in the scene, fast body movements, and varied lengths of video clips. In this paper, we propose a novel framework to analyze human interactions in TV shows. Firstly, we exploit the motion interchange pattern (MIP) to detect camera viewpoint changes in a video, and extract the salient motion points in the bounding box that covers the region of interest (ROI) in each frame. Then, we compute the large displacement optical flow for the salient pixels in the bounding box, and build the histogram of oriented optical flow as the motion feature vector for each frame. Finally, the self-similarity matrix (SSM) is adopted to capture the global temporal correlation of frames in a video. After extracting the SSM descriptors, the video feature vector can be constructed through different encoding approaches. The proposed framework works well in practice for unconstrained videos. We validate our approach on the TV human interaction (TVHI) dataset, and the experimental results demonstrate the efficacy of our strategy.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"d14c1bf8ce40212336c41db66ebbd1ec2d9d6deb","venue_1":"ACM Multimedia","year":"2014","title":"Multiple Features But Few Labels?: A Symbiotic Solution Exemplified for Video Analysis","authors":"Zhigang Ma, Yi Yang, Nicu Sebe, Alexander G. Hauptmann","author_ids":"1727419, 1698559, 1703601, 7661726","abstract":"Video analysis has been attracting increasing research due to the proliferation of internet videos. In this paper, we investigate how to improve the performance on internet quality video analysis. Particularly, we work on the scenario of few labeled training videos being provided, which is less focused in multimedia. To being with, we consider how to more effectively harness the evidences from the low-level features. Researchers have developed several promising features to represent videos to capture the semantic information. However, as videos usually characterize rich semantic contents, the analysis performance by using one single feature is potentially limited. Simply combining multiple features through early fusion or late fusion to incorporate more informative cues is doable but not optimal due to the heterogeneity and different predicting capability of these features. For better exploitation of multiple features, we propose to mine the importance of different features and cast it into the learning of the classification model. Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance. On the other hand, to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way. The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos. We perform extensive experiments on video action recognition and multimedia event recognition and the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"2b6e0e5d4e860dae56ef3a5e4f90987bcaf9fda4","venue_1":"ACM Multimedia","year":"2014","title":"Temporal Dropout of Changes Approach to Convolutional Learning of Spatio-Temporal Features","authors":"Dubravko Culibrk, Nicu Sebe","author_ids":"2876527, 1703601","abstract":"The paper addresses the problem of learning features that can account for temporal dynamics present in videos. Although deep convolutional learning methods revolutionized several areas of multimedia and computer vision, there have been relatively few proposals dealing with ways in which these methods can be enabled to make use of motion information, critical to the extraction of useful information from video. We propose a temporal dropout of changes approach for this, which allows us to consider temporal information over a series of frames without increasing the number of training parameters of the network.\n To illustrate the potential of the proposed methodology, we focus on the problem of dynamic texture classification. Dynamic textures represent an important form of dynamics present in video data, so far not considered within the framework of deep learning.\n Initial results presented in the paper show that the proposed approach, based on a well-known deep convolutional neural network, can achieve state-of-the-art performance on two well-known and challenging dynamic texture classification data sets (DynTex++ and UCLA dynamic texture).","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"70781d3829501206ffb896f39328d1b63bde05d1","venue_1":"ACM Multimedia","year":"2013","title":"Time matters!: capturing variation in time in video using fisher kernels","authors":"Ionut Mironica, Jasper R. R. Uijlings, Negar Rostamzadeh, Bogdan Ionescu, Nicu Sebe","author_ids":"1759152, 1823362, 2599281, 1796198, 1703601","abstract":"In video global features are often used for reasons of computational efficiency, where each global feature captures information of a single video frame. But frames in video change over time, so an important question is: how can we meaningfully aggregate frame-based features in order to preserve the variation in time? In this paper we propose to use the Fisher Kernel to capture variation in time in video. While in this approach the temporal order is lost, it captures both subtle variation in time such as the ones caused by a moving bicycle and drastic variations in time such as the changing of shots in a documentary. Our work should not be confused with a Bag of Local Visual Features approach, where one captures the visual variation of local features in both time and space indiscriminately. Instead, each feature measures a complete frame hence we capture variation in time only.\n We show that our framework is highly general, reporting improvements using frame-based visual features, body-part features, and audio features on three diverse datasets: We obtain state-of-the-art results on the UCF50 human action dataset and improve the state-of-the-art on the MediaEval 2012 video-genre benchmark and on the ADL daily activity recognition dataset.","cites":"11","conferencePercentile":"87.33333333"},{"venue":"ACM Multimedia","id":"ace31210e4b6428926e39756e43c302442d21789","venue_1":"ACM Multimedia","year":"2015","title":"Cross-Modal Image-Tag Relevance Learning for Social Images","authors":"Yong Cheng, Zhengxiang Cai, Rui Feng, Cheng Jin, Yuejie Zhang, Tao Zhang","author_ids":"2570492, 2748211, 1908561, 3506220, 7550713, 1743147","abstract":"A new algorithm is developed in this paper to support more effective cross-modal image-tag relevance learning for large-scale social images, which integrates the multimodal feature representation, multimodal relevance measurement, and cross- modal relevance fusion. The main contribution of our work is that we provide a more reasonable base to learn cross-modal relevance among social images, which can be acquired from integrating multimodal image and tag relevance with multiple features in different modalities. Very positive results were obtained in our experiments using a large quantity of public social image data.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"1d0884818b22af95a4078f17f2f8ad58a97c1cd2","venue_1":"ACM Multimedia","year":"2013","title":"GLocal structural feature selection with sparsity for multimedia data understanding","authors":"Yan Yan, Zhongwen Xu, Gaowen Liu, Zhigang Ma, Nicu Sebe","author_ids":"1703972, 2351434, 3056447, 1727419, 1703601","abstract":"The selection of discriminative features is an important and effective technique for many multimedia tasks. Using irrelevant features in classification or clustering tasks could deteriorate the performance. Thus, designing efficient feature selection algorithms to remove the irrelevant features is a possible way to improve the classification or clustering performance. With the successful usage of sparse models in image and video classification and understanding, imposing structural sparsity in \\emph{feature selection} has been widely investigated during the past years. Motivated by the merit of sparse models, we propose a novel feature selection method using a sparse model in this paper. Different from the state of the art, our method is built upon $\\ell _{2,p}$-norm and simultaneously considers both the global and local (GLocal) structures of data distribution. Our method is more flexible in selecting the discriminating features as it is able to control the degree of sparseness. Moreover, considering both global and local structures of data distribution makes our feature selection process more effective. An efficient algorithm is proposed to solve the $\\ell_{2,p}$-norm sparsity optimization problem in this paper. Experimental results performed on real-world image and video datasets show the effectiveness of our feature selection method compared to several state-of-the-art methods.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"6c510b56b468303adf9ea2fe94bd93d98347f07c","venue_1":"ACM Multimedia","year":"2013","title":"We are not equally negative: fine-grained labeling for multimedia event detection","authors":"Zhigang Ma, Yi Yang, Zhongwen Xu, Nicu Sebe, Alexander G. Hauptmann","author_ids":"1727419, 1698559, 2351434, 1703601, 7661726","abstract":"Multimedia event detection (MED) is an effective technique for video indexing and retrieval. Current classifier training for MED treats the negative videos equally. However, many negative videos may resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not accurate enough to characterize the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations, which brings mutual reciprocality. Our model obtains two kinds of classifiers, one from the attributes and one from the features, which incorporate the informative cues from the fine-grained labels. The outputs of both classifiers on the testing videos are fused for detection. Extensive experiments on the challenging TRECVID MED 2012 development set have validated the efficacy of our proposed approach.","cites":"8","conferencePercentile":"83.11111111"},{"venue":"ACM Multimedia","id":"bad0a668f4d640eecfc79a0e129af6882b68e11d","venue_1":"ACM Multimedia","year":"2012","title":"Knowledge adaptation for ad hoc multimedia event detection with few exemplars","authors":"Zhigang Ma, Yi Yang, Yang Cai, Nicu Sebe, Alexander G. Hauptmann","author_ids":"1727419, 1698559, 1801727, 1703601, 7661726","abstract":"Multimedia event detection (MED) has a significant impact on many applications. Though video concept annotation has received much research effort, video event detection remains largely unaddressed. Current research mainly focuses on sports and news event detection or abnormality detection in surveillance videos. Our research on this topic is capable of detecting more complicated and generic events. Moreover, the curse of reality, <i>i.e.</i>, precisely labeled multimedia content is scarce, necessitates the study on how to attain respectable detection performance using only limited positive examples. Research addressing these two aforementioned issues is still in its infancy. In light of this, we explore Ad Hoc MED, which aims to detect complicated and generic events by using few positive examples. To the best of our knowledge, our work makes the first attempt on this topic. As the information from these few positive examples is limited, we propose to infer knowledge from other multimedia resources to facilitate event detection. Experiments are performed on real-world multimedia archives consisting of several challenging events. The results show that our approach outperforms several other detection algorithms. Most notably, our algorithm outperforms SVM by 43% and 14% comparatively in Average Precision when using Gaussian and &#935;<sup>2</sup> kernel respectively.","cites":"34","conferencePercentile":"98.41772152"},{"venue":"ACM Multimedia","id":"419550e7b918c64785f087b17f7fde6c94bc6d4e","venue_1":"ACM Multimedia","year":"2012","title":"Distributional semantics with eyes: using image analysis to improve computational representations of word meaning","authors":"Elia Bruni, Jasper R. R. Uijlings, Marco Baroni, Nicu Sebe","author_ids":"2552871, 1823362, 1689044, 1703601","abstract":"The current trend in image analysis and multimedia is to use information extracted from text and text processing techniques to help vision-related tasks, such as automated image annotation and generating semantically rich descriptions of images. In this work, we claim that image analysis techniques can \"return the favor\" to the text processing community and be successfully used for a general-purpose representation of word meaning. We provide evidence that simple low-level visual features can enrich the semantic representation of word meaning with information that cannot be extracted from text alone, leading to improvement in the core task of estimating degrees of semantic relatedness between words, as well as providing a new, perceptually-enhanced angle on word semantics. Additionally, we show how distinguishing between a concept and its context in images can improve the quality of the word meaning representations extracted from images.","cites":"24","conferencePercentile":"95.88607595"},{"venue":"ACM Multimedia","id":"11ad9cdc8c022159edf0f0f2d23f364a1cd47b60","venue_1":"ACM Multimedia","year":"2012","title":"In the eye of the beholder: employing statistical analysis and eye tracking for analyzing abstract paintings","authors":"Victoria Yanulevskaya, Jasper R. R. Uijlings, Elia Bruni, Andreza Sartori, Elisa Zamboni, Francesca Bacci, David Melcher, Nicu Sebe","author_ids":"3180583, 1823362, 2552871, 2136723, 3169119, 2926719, 2357492, 1703601","abstract":"Most artworks are explicitly created to evoke a strong emotional response. During the centuries there were several art movements which employed different techniques to achieve emotional expressions conveyed by artworks. Yet people were always consistently able to read the emotional messages even from the most abstract paintings. Can a machine learn what makes an artwork emotional? In this work, we consider a set of 500 abstract paintings from Museum of Modern and Contemporary Art of Trento and Rovereto (MART), where each painting was scored as carrying a positive or negative response on a Likert scale of 1-7. We employ a state-of-the-art recognition system to learn which statistical patterns are associated with positive and negative emotions. Additionally, we dissect the classification machinery to determine which parts of an image evokes what emotions. This opens new opportunities to research why a specific painting is perceived as emotional. We also demonstrate how quantification of evidence for positive and negative emotions can be used to predict the way in which people observe paintings.","cites":"21","conferencePercentile":"94.62025316"},{"venue":"ACM Multimedia","id":"5fc230bada7638022acd6c1b2dec6aeaca750790","venue_1":"ACM Multimedia","year":"2011","title":"Can computers learn from humans to see better?: inferring scene semantics from viewers' eye movements","authors":"Subramanian Ramanathan, Victoria Yanulevskaya, Nicu Sebe","author_ids":"1742936, 3180583, 1703601","abstract":"This paper describes an attempt to bridge the <b>semantic gap</b> between computer vision and scene understanding employing eye movements. Even as computer vision algorithms can efficiently detect scene objects, discovering semantic relationships between these objects is as essential for scene understanding. Humans understand complex scenes by rapidly moving their eyes (saccades) to selectively focus on <i>salient</i> entities (fixations). For 110 social scenes, we compared verbal descriptions provided by observers against eye movements recorded during a free-viewing task. Data analysis confirms (i) a strong correlation between task-explicit linguistic descriptions and task-implicit eye movements, both of which are influenced by underlying scene semantics and (ii) the ability of eye movements in the form of <i>fixations</i> and <i>saccades</i> to indicate salient <i>entities</i> and <i>entity relationships</i> mentioned in scene descriptions.\n We demonstrate how eye movements are useful for inferring the meaning of <b>social</b> (everyday scenes depicting human activities) and <b>affective</b> (emotion-evoking content like <i>expressive faces, nudes</i>) scenes. While saliency has always been studied through the prism of fixations, we show that saccades are particularly useful for (i) distinguishing mild and high-intensity facial expressions and (ii) discovering interactive actions between scene entities.","cites":"15","conferencePercentile":"89.79591837"},{"venue":"ACM Multimedia","id":"b1b25164c3a2c08cf6cb89bb6899706a21cab1a0","venue_1":"ACM Multimedia","year":"1994","title":"Argo: A System for Distributed Collaboration","authors":"Hania Gajewska, James J. Kistler, Mark S. Manasse, David D. Redell","author_ids":"2481955, 3040083, 2083378, 2737515","abstract":"The goal of the Argo system is to allow medium-sized groups of users to collaborate remotely from their desktops in a way that approaches as closely as possible the effectiveness of face-to-face meetings. In support of this goal, Argo combines high quality multi-party digital video and full-duplex audio with telepointers, shared applications, and whiteboards in a uniform and familiar environment. The shared applications can be unmodified X programs shared via a proxy server, unmodified groupware applications, and applications written using our toolkit. Workers can contact each other as easily as making a phone call, and can easily bring into a conference any material they are working on. They do so by interacting with an object-oriented, client/server conference control system. The same conference control system is used to support teleporting, i.e. moving the desktop environment from one workstation's display to another (for example, from office to home). This paper describes the system we have built to test the hypothesis that the effectiveness of remote collaboration can be substantially impacted by the responsiveness of the interaction media.","cites":"26","conferencePercentile":"63.55932203"},{"venue":"ACM Multimedia","id":"5408f9841ef7f3ed58b64d1b401370fe401ffac3","venue_1":"ACM Multimedia","year":"2011","title":"Exploiting the entire feature space with sparsity for automatic image annotation","authors":"Zhigang Ma, Yi Yang, Feiping Nie, Jasper R. R. Uijlings, Nicu Sebe","author_ids":"1727419, 1698559, 1688370, 1823362, 1703601","abstract":"The explosive growth of digital images requires effective methods to manage these images. Among various existing methods, automatic image annotation has proved to be an important technique for image management tasks, e.g., image retrieval over large-scale image databases. Automatic image annotation has been widely studied during recent years and a considerable number of approaches have been proposed. However, the performance of these methods is yet to be satisfactory, thus demanding more effort on research of image annotation. In this paper, we propose a novel semi supervised framework built upon feature selection for automatic image annotation. Our method aims to jointly select the most relevant features from all the data points by using a sparsity-based model and exploiting both labeled and unlabeled data to learn the manifold structure. Our framework is able to simultaneously learn a robust classifier for image annotation by selecting the discriminating features related to the semantic concepts. To solve the objective function of our framework, we propose an efficient iterative algorithm. Extensive experiments are performed on different real-world image datasets with the results demonstrating the promising performance of our framework for automatic image annotation.","cites":"19","conferencePercentile":"93.14868805"},{"venue":"ACM Multimedia","id":"b2f5c9e9e6701c54aa38893f0faee6897769dc64","venue_1":"ACM Multimedia","year":"2010","title":"Pervasive video analysis: workshop overview","authors":"Hamid K. Aghajan, Marco Cristani, Vittorio Murino, Nicu Sebe","author_ids":"1768752, 1723008, 1727204, 1703601","abstract":"This workshop aims at tackling the novel challenging scenarios in pervasive video analysis which require not only to address specific problems (e.g., tracking, recognition) on a single view, but to deal with a set of distributed observations, eventually integrated with subjective mobile video streams. Accepted papers cover a wide range of subjects going from the joint analysis of video sequences, taken from fixed location and mobile cameras, to situation awareness and understanding.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"4bd8c5eb70de170e34a411a47a8abc68651a382c","venue_1":"ACM Multimedia","year":"2004","title":"ChucK: a programming language for on-the-fly, real-time audio synthesis and multimedia","authors":"Ge Wang, Perry R. Cook","author_ids":"6624472, 1716507","abstract":"In this paper, we describe ChucK - a programming language and programming model for writing precisely timed, concurrent audio synthesis and multimedia programs. Precise concurrent audio programming has been an unsolved (and ill-defined) problem. ChucK provides a concurrent programming model that solves this problem and significantly enhances designing, developing, and reasoning about programs with complex audio timing. ChucK employs a novel &#60;i>data-driven&#60;/i> timing mechanism and a related &#60;i>time-based synchronization&#60;/i> model, both implemented in a virtual machine. We show how these features enable precise, concurrent audio programming and provide a high degree of programmability in writing real-time audio and multimedia programs. As an extension, programmers can use this model to write code &#60;i>on-the-fly&#60;/i> -- while the program is running. These features provide a powerful programming tool for building and experimenting with complex audio synthesis and multimedia programs.","cites":"27","conferencePercentile":"81.12745098"},{"venue":"ACM Multimedia","id":"d08a81231259ea5a60d382d63bd37d7803b9b62f","venue_1":"ACM Multimedia","year":"1994","title":"An Automatic Lip-Synchronization Algorithm for Synthetic Faces","authors":"Keith Waters, Thoms M. Levergood","author_ids":"1744126, 1736506","abstract":"This paper addresses the problem of automatically synchronizing computer-generated faces with synthetic speech. The complete process provides a novel form of face-to-face communication and the ability to create a new range of talking <italic>personable</italic> synthetic characters. Based on plain ASCII text input, a synthetic speech segment is generated and synchronized in real-time to a graphical display of an articulating mouth and face. The key component of the algorithm is the run-time facility that adaptively synchronizes the graphical display of the face to the audio.","cites":"37","conferencePercentile":"72.88135593"},{"venue":"ACM Multimedia","id":"2e3d5418bd1ab75806cf4af899983d0ef5b196e0","venue_1":"ACM Multimedia","year":"2009","title":"TAPESTREA: a new way to design sound","authors":"Ananya Misra, Ge Wang, Perry R. Cook","author_ids":"1951179, 6624472, 1716507","abstract":"TAPESTREA is a sound design and composition framework that facilitates the creation of new sound from existing digital audio recordings, through interactive analysis, transformation and re-synthesis. During analysis, sound templates of diffierent types are extracted using a variety of techniques. Each extracted template is transformed and synthesized independently, allowing specialized transformations on each template based on its type. The user interacts with TAPESTREA via a set of graphical interfaces that offer parametric control over every stage of analysis, transformation and re-synthesis. Synthesis is further controlled through ChucK scripts. These combined techniques form a workbench for completely transforming a sound scene, dynamically generating soundscapes, or creating musical tapestries by weaving together transformed elements from different recordings. Thus, TAPESTREA introduces a new paradigm for sound design, composition and sonic sculpting tasks.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"062217df8d0db724d35c0a8fc872549bb4872474","venue_1":"ACM Multimedia","year":"2010","title":"Putting the pieces together: multimodal analysis of social attention in meetings","authors":"Subramanian Ramanathan, Jacopo Staiano, Kyriaki Kalimeri, Nicu Sebe, Fabio Pianesi","author_ids":"1742936, 1767493, 1799135, 1703601, 8029006","abstract":"This paper presents a multimodal framework employing eye-gaze, head-pose and speech cues to explain observed social attention patterns in meeting scenes. We first investigate a few hypotheses concerning social attention and characterize meetings and individuals based on ground-truth data. This is followed by replication of ground-truth results through automated estimation of eye-gaze, head-pose and speech activity for each participant. Experimental results show that combining eye-gaze and head-pose estimates decreases error in social attention estimation by over 26%.","cites":"13","conferencePercentile":"81.50684932"},{"venue":"ACM Multimedia","id":"65753c742491c6db3963a961562e8735f77119d3","venue_1":"ACM Multimedia","year":"2015","title":"Analyzing Free-standing Conversational Groups: A Multimodal Approach","authors":"Xavier Alameda-Pineda, Yan Yan, Elisa Ricci, Oswald Lanz, Nicu Sebe","author_ids":"1780201, 1703972, 1878028, 1717522, 1703601","abstract":"During natural social gatherings, humans tend to organize themselves in the so-called free-standing conversational groups. In this context, robust head and body pose estimates can facilitate the higher-level description of the ongoing interplay. Importantly, visual information typically obtained with a distributed camera network might not suffice to achieve the robustness sought. In this line of thought, recent advances in wearable sensing technology open the door to multimodal and richer information flows. In this paper we propose to cast the head and body pose estimation problem into a matrix completion task. We introduce a framework able to fuse multimodal data emanating from a combination of distributed and wearable sensors, taking into account the temporal consistency, the head/body coupling and the noise inherent to the scenario. We report results on the novel and challenging SALSA dataset, containing visual, auditory and infrared recordings of 18 people interacting in a regular indoor environment. We demonstrate the soundness of the proposed method and the usability for higher-level tasks such as the detection of F-formations and the discovery of social attention attractors.","cites":"9","conferencePercentile":"96.48148148"},{"venue":"ACM Multimedia","id":"21dd400e194245b4f829f3a38bc42ca96a5283be","venue_1":"ACM Multimedia","year":"2015","title":"Who's Afraid of Itten: Using the Art Theory of Color Combination to Analyze Emotions in Abstract Paintings","authors":"Andreza Sartori, Dubravko Culibrk, Yan Yan, Nicu Sebe","author_ids":"2136723, 2876527, 1703972, 1703601","abstract":"Color plays an essential role in everyday life and is one of the most important visual cues in human perception. In abstract art, color is one of the essential means to convey the artist's intention and to affect the viewer emotionally. However, colors are rarely experienced in isolation, rather, they are usually presented together with other colors. In fact, the expressive properties of two-color combinations have been extensively studied by artists. It is intriguing to try to understand how color combinations in abstract paintings might affect the viewer emotionally, and to investigate if a computer algorithm can learn this mechanism.\n In this work, we propose a novel computational approach able to analyze the color combinations in abstract paintings and use this information to infer whether a painting will evoke positive or negative emotions in an observer. We exploit art theory concepts to design our features and the learning algorithm. To make use of the color-group information, we propose inferring the emotions elicited by paintings based on the sparse group lasso approach. Our results show that a relative improvement of between 6% and 8% can be achieved in this way. Finally, as an application, we employ our method to generate Mondrian-like paintings and do a prospective user study to evaluate the ability of our method as an automatic tool for generating abstract paintings able to elicit positive and negative emotional responses in people.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"82383526c4f871e695702472749c1c076b38d898","venue_1":"ACM Multimedia","year":"2015","title":"Supervised Hashing with Pseudo Labels for Scalable Multimedia Retrieval","authors":"Jingkuan Song, Lianli Gao, Yan Yan, Dongxiang Zhang, Nicu Sebe","author_ids":"1721410, 2671321, 1703972, 3021351, 1703601","abstract":"There is an increasing interest in using hash codes for efficient multimedia retrieval and data storage. The hash functions are learned in such a way that the hash codes can preserve essential properties of the original space or the label information. Then the Hamming distance of the hash codes can approximate the data similarity. Existing works have demonstrated the success of many supervised hashing models. However, labeling data is time and labor consuming, especially for scalable datasets. In order to utilize the supervised hashing models to improve the discriminative power of hash codes, we propose a Supervised Hashing with Pseudo Labels (SHPL) which uses the cluster centers of the training data to generate pseudo labels, based on which the hash codes can be generated using the criteria of supervised hashing. More specifically, we utilize linear discriminant analysis (LDA) with trace ratio criterion as a showcase for hash functions learning and during the optimization, we prove that the pseudo labels and the hash codes can be jointly learned and iteratively updated in an unified framework. The learned hash functions can harness the discriminant power of trace ratio criterion, and thus can achieve better performance. Experimental results on three large-scale unlabeled datasets (i.e., SIFT1M, GIST1M, and SIFT1B) demonstrate the superior performance of our SHPL over existing hashing methods.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"11980be16282eccbadfe8988984ae81833a7b4b3","venue_1":"ACM Multimedia","year":"2015","title":"2nd Workshop on Computational Models of Social Interactions: Human-Computer-Media Communication (HCMC2015)","authors":"Mohamed R. Amer, Ajay Divakaran, Shih-Fu Chang, Nicu Sebe","author_ids":"1683287, 1696401, 1735547, 1703601","abstract":"Communicating ideas and information from and to humans is a very important subject. In our daily life, human interact with variety of entities, such as, other humans, machines, media. Constructive interactions are needed for good communication, which would result in successful outcomes, such as answering a query, learning a new skill, getting a service done, and communicating emotions. Each of these entities invokes a set of signals. Current research has focused on analyzing one entity's signals with no respect to the other entities in a unidirectional manner. The computer vision community focused on detection, classification and recognition of humans and their poses and gestures progressing onto actions, activities, and events but it does not go beyond that. The signal processing community focused on emotion recognition from facial expressions or audio or both combined. The HCI community focused on making easier interfaces for machines to ease their usage. The goal of this workshop is to bring multiple disciplines together, to process human directed signals holistically, in a bidirectional manner, rather than isolation. This workshop is positioned to display this rich domain of applications, which will provide the necessary next boost for these technologies. At the same time, it seeks to ground computational models on theory that would help achieve the technology goals. This would allow us to leverage decades of research in different fields and to spur interdisciplinary research thereby opening up new problem domains for the multimedia community.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"ad0cb9c59d923f08848fc34e42b9a1be3002e952","venue_1":"ACM Multimedia","year":"2006","title":"Maximum unfolded embedding: formulation, solution, and application for image clustering","authors":"Huan Wang, Shuicheng Yan, Thomas S. Huang, Xiaoou Tang","author_ids":"1703731, 1698982, 1739208, 1741901","abstract":"In this paper, we present a novel spectral analysis algorithm for image clustering. First, the image manifold is embedded onto a low-dimensional feature space with dual objectives, i.e., maximizing the distances of faraway sample pairs meanwhile preserving the local manifold structure, which essentially results in a <i>Trace Ratio</i> optimization problem. Then an efficient iterative procedure is proposed to directly optimize the trace ratio and finally the clustering process is implemented on the derived low-dimensional embedding. Moreover, the linear approximation is also presented for handling the out-of-sample data. Experimental results show that our algorithm, referred to as Maximum Unfolded Embedding, brings an encouraging improvement in clustering accuracy over the state-of-the-art algorithms, such as K-Means, PCA-Kmeans, normalized cut \\cite shi00normalized, and Locality Preserving Clustering [13].","cites":"9","conferencePercentile":"67.35751295"},{"venue":"ACM Multimedia","id":"2da46cc04b4fc01f9722e406da9ea8bc3858bcd3","venue_1":"ACM Multimedia","year":"2013","title":"AirTouch panel: a re-anchorable virtual touch panel","authors":"Shih-Yao Lin, Chuen-Kai Shie, Shen-Chi Chen, Yi-Ping Hung","author_ids":"1794068, 2837744, 1762602, 7312257","abstract":"To achieve maximum mobility, device-less approaches for home appliance remote control have received increasing attention in recent years. In this paper, we propose a screen-less virtual touch panel, called AirTouch Panel, which can be positioned at any place with various orientations around users. The proposed virtual touch panel provides a potential ability to remotely control the home appliances, such as television, air conditioner, and so on. The proposed system allows users to anchor the panel at the place with comfortable poses. If the users want to change panel's position or orientation, they only need to re-anchor it, and then the panel will be reset. In this paper, our main contribution is to design a re-anchorable virtual panel for digital home remote control. Most importantly, we explore the design of such imaginary interface through two user studies. In our user studies, we analyze task completion time, satisfaction rate, and the number of miss-clicks. We are interested in the feasibility issues, for example, proper click gesture, panel size and button size, etc. Moreover, based on the AirTouch Panel, we also developed an intelligent TV to demonstrate the usability for controlling home appliance.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"92ed8beefb1b2f5e9f3c95f7ce5813a99de482df","venue_1":"ACM Multimedia","year":"2005","title":"Learning an image-word embedding for image auto-annotation on the nonlinear latent space","authors":"Wei Liu, Xiaoou Tang","author_ids":"3406404, 1741901","abstract":"Latent Semantic Analysis (LSA) has shown encouraging performance for the problem of unsupervised image automatic annotation. LSA conducts annotation by keywords propagation on a linear Latent Space, which accounts for the underlying semantic structure of word and image features. In this paper, we formulate a more general nonlinear model, called Nonlinear Latent Space model, to reveal the latent variables of word and visual features more precisely. Instead of the basic propagation strategy, we present a novel inference strategy for image annotation via Image-Word Embedding (IWE). IWE simultaneously embeds images and words and captures the dependencies between them from a probabilistic viewpoint. Experiments show that IWE-based annotation on the nonlinear latent space outperforms previous unsupervised annotation methods.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"ee4a8a37e16bf2cd032fdc77a07ad12123db77ec","venue_1":"ACM Multimedia","year":"2003","title":"Visualizing the pulse of a classroom","authors":"Milton Chen","author_ids":"1713071","abstract":"Effective classroom teaching often requires an instructor to be acutely aware of every student. The instructor must rapidly look from student to student to catch fleeting gestures or facial expressions. To facilitate the tracking of communicative actions in a remote classroom, we built a multiparty videoconferencing system that automatically determine whether students are speaking, making gestures, or moving in their seats. These activity indicators are displayed over the video such that the instructor can see into the recent past. The activity indicators are also grouped into a visualization of the classroom interaction dynamics, thereby providing a measure of the pulse of the classroom.We conducted a user study where teachers used our system in a simulated class. The teachers found that the activity indicators to be a useful teaching aid during class; however, the indicators are most useful as a record of the class. In a student survey, we found that if audio, video, or activity indicators must be recorded, students overwhelmingly prefer activity indicators since the indicators mask the content of the communication and thus are less intrusive to the students' privacy.","cites":"19","conferencePercentile":"65.31531532"},{"venue":"ACM Multimedia","id":"602d49f2a477370541ae52e14e16219e4054438b","venue_1":"ACM Multimedia","year":"2002","title":"Achieving effective floor control with a low-bandwidth gesture-sensitive videoconferencing system","authors":"Milton Chen","author_ids":"1713071","abstract":"Multiparty videoconferencing with even a small number of people is often infeasible due to the high network bandwidth required. Bandwidth can be significantly reduced if most of the advantages of using full-motion video can be achieved with low-frame-rate video; unfortunately, the impact of low-frame-rate video on communication is relatively unexplored. We implemented a multiparty videoconferencing system that supports full-motion video, low-frame-rate video where the video is updated only once every few seconds, and a hybrid scheme where full-motion video is transmitted when the system detects that a user is making a gesture and low-frame-rate video is transmitted at all other times. We studied people using our system for small-group discussions and found that low-frame-rate video limited people's ability to request to speak or judge when to stop speaking. The hybrid scheme, conversely, was as effective as full-motion video for floor control, resulting in a similar number of speaker changes, while using only ten percent of the bandwidth.","cites":"26","conferencePercentile":"76.92307692"},{"venue":"ACM Multimedia","id":"33a11aae0a9fd4fcd50156f7a636e6faf62deaa5","venue_1":"ACM Multimedia","year":"2012","title":"Action recognition for human-marionette interaction","authors":"Shih-Yao Lin, Chuen-Kai Shie, Shen-Chi Chen, Yi-Ping Hung","author_ids":"1794068, 2837744, 1762602, 7312257","abstract":"In this paper, we propose a human-marionette interaction system based on a human action recognition approach for applications to <b>interactive artistic puppetry</b> and a <b>mimicking-marionette game</b>. We developed an intelligent marionette called \"i-marionette\" that is controlled by a sophisticated control device to achieve various human actions. Moreover, we utilized an action recognition approach to enable the i-marionette to learn and recognize complex dance movements. The idea of artistic puppetry is to present a conflict scenario between two different cultural worlds: the <b>performer</b> is active and represents the culture of modern technology based in the real world. In contrast, the <i>i</i>-<b>marionette</b> represents traditional culture and is passive and based in a virtual world. The active performer guides the passive <i>i</i>-marionette to form a space-time connection between the real world and the virtual world. The <i>i</i>-marionette mimics the performer's action, while the performer also mimics the i-marionette's action. The performance represents an artistic conception in which humans invent technology and the <i>i</i>-marionette is manipulated by human control. However, in this interactive circle, the human is implicitly affected by the i-marionette. In our mimicking-marionette game, a player mimics the <i>i</i>-marionette's action. Subsequently, our human action recognition system measures the action similarity between the player and the <i>i</i>-marionette, and our system provides a similarity score.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"bdfb1e44f2e1855e5a081b20a7d3b73343112581","venue_1":"ACM Multimedia","year":"2013","title":"Multimedia framed","authors":"Elizabeth F. Churchill","author_ids":"1801572","abstract":"Multimedia is the combination of several media forms, More typically, the word implies sound and full-motion video. While multimedia technologists concern themselves with the production and distribution of the multimedia artifacts themselves, information designers, educationalists and artists are more concerned with the reception of the artifact, and consider multimedia to be another representational format for multimodal information presentation. Such a perspective leads to questions such as: Is text, or audio or video, or a combination of all three, the best format for the message? Should another modality (e.g., haptics/touch, olfaction) be invoked instead or in addition? How does the setting affect perception/reception? Is the artifact interactive? Is it changed by audience members? Understanding how an artifact is perceived, received and interacted with is central to understanding what multimedia is, opening up possibilities and issuing technical challenges as we imagine new forms and formats of multimedia experience. In this talk, I will illustrate how content understanding is modulated by context, by the \" framing \" of the content. I will discuss audience participatory production of multimedia and multimodal experiences. I will conclude with some technical excitements, design/development challenges and experiential possibilities that lie ahead. 1. A Summary The beginnings of multimedia vary depending your perspective [2]. One could set the beginnings to be when humans first painted on cave walls in the Grotte de Lascaux, France. Or perhaps it was 1455 when the Gutenberg Press or in the 1800 with the birth of photography and the first 'movies'. Perhaps it was when the term was coined in the 1960's or more specifically with the Xanadu project in 1965 and the development of hypertext. Or maybe Pong heralded the beginning with its launch in 1972. Whatever your personal view, research developments in computer technologies (hardware and software), audio and telecommunications are the component pieces of the our everyday multimedia experience. In this talk, I will focus on the experience of multimedia in general and more specifically on how the framing of multimedia artifacts affects how we experience them. Framing can be intentional–scripted creations produced with clear intent by technologists, film-makers, archivists, documentarians, designers, architects and media artists. Framing can also be unintentional. Everyday acts of interest and consumption make us, as viewers, co-producers in the experiences of multimedia artifacts as we view, download, annotate, comment and share multimedia artifacts online, we change how they are interpreted and understood by others. I will …","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"a20933600f53d3324954b8b4e06e230dd6d8b407","venue_1":"ACM Multimedia","year":"2001","title":"Design of a virtual auditorium","authors":"Milton Chen","author_ids":"1713071","abstract":"We built a videoconference system called the Virtual Auditorium to support dialog-based distance learning. The instructor can see dozens of students on a tiled wall-sized display and establish eye contact with any student. Telephone-quality audio and television-quality video can be streamed using commodity codecs such as wavelet and MPEG-4. Support for stream migration allows a seamless user interface to span the multiple computers driving the display wall..We performed user studies on the auditorium parameters. We found that the optimal display wall size must balance two contradictory requirements: subjects prefer larger videos for seeing facial expressions and smaller videos for seeing everyone without head movement. Ideally, each video should have a field of view that spans 14 degrees, which corresponds to a slightly larger than life-size image. At the very least, each video should have a field of view of 6 degrees. We found that a video window should be less than 2.7 degrees horizontally and 9 degrees vertically from the camera in order to maintain the appearance of eye contact for the remote viewer. In addition, we describe a previously unreported gaze phenomenon: a person's expectation determines his perception of eye contact under ambiguous conditions.","cites":"0","conferencePercentile":"4.591836735"},{"venue":"ACM Multimedia","id":"42801dc3e59a21f6e4e8815f8554a080a459d35b","venue_1":"ACM Multimedia","year":"2011","title":"Dynamic social network for narrative video analysis","authors":"Tsung-Hung Tsai, Wen-Huang Cheng, Yung-Huan Hsieh","author_ids":"3057243, 1711298, 2096888","abstract":"Narrative video analysis has attracted much research attention, for narrative scenes can provide meaningful representations of multimedia contents. To go beyond the limitations of content based appraoches, social network techniques was introduced in the literature to explore the high-level narrative structures by mining the relations between video characters. Taking into account the fact that such a social network is not static but changes over time as the video narrative evolves, in this work, we develop a novel social network model, namely dynamic social network, for capturing the spatiotemporal dynamics in the social network of video characters so as to enable the automatic segmentation of a video into a sequence of narrative scenes. The proposed approach is experimented with various genres of movies and the results demonstrate our effectiveness.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"0c018bc65acbb80392345cea83c73b01cce6ff67","venue_1":"ACM Multimedia","year":"2010","title":"Toward an automatically generated soundtrack from low-level cross-modal correlations for automotive scenarios","authors":"Marco Cristani, Anna Pesarin, Carlo Drioli, Vittorio Murino, Antonio Rodà, Michele Grapulin, Nicu Sebe","author_ids":"1723008, 3274174, 7623979, 1727204, 2394919, 3098689, 1703601","abstract":"In this paper, we propose a novel recommendation policy for driving scenarios. While driving a car, listening to an audio track may enrich the atmosphere, conveying emotions that let the driver sense a more arousing experience. Here, we are introducing a recommendation policy that, given a video sequence taken by a camera mounted onboard a car, chooses the most suitable audio piece from a predetermined set of melodies. The mixing mechanism takes inspiration from a set of generic qualitative aesthetical rules for cross-modal linking, realized by associating audio and video features. The contribution of this paper is to translate such qualitative rules into quantitative terms, learning from an extensive training dataset cross-modal statistical correlations, and validating them in a thoroughly way. In this way, we are able to define what are the audio and video features that correlate at best (i.e., promoting or rejecting some aesthetical rules), and what are their correlation intensities. This knowledge is then employed for the realization of the recommendation policy. A set of user studies illustrate and validate the policy, thus encouraging further developments toward a real implementation in an automotive application.","cites":"9","conferencePercentile":"77.39726027"},{"venue":"ACM Multimedia","id":"db3bd229899b96a463a99295d68af9c63228dd5e","venue_1":"ACM Multimedia","year":"2005","title":"The multimedia challenges raised by pervasive games","authors":"Mauricio Capra, Milena Radenkovic, Steve Benford, Leif Oppermann, Adam Drozd, Martin Flintham","author_ids":"2899536, 1688358, 1738239, 3246801, 2111372, 1795102","abstract":"Pervasive gaming is a new form of multimedia entertainment that extends the traditional computer gaming experience out into the real world. Through a combination of personal devices, positioning systems and other multimedia sensors, combined with wireless networking, a pervasive game can respond to a player's movements and context and enable them to communicate with a game server and other players. We review recent examples of pervasive games in order to explain their distinctive characteristics as multimedia applications. We then consider the challenge of scaling pervasive games to include potentially very large numbers of players. We propose a new approach based upon a campaign model in which individuals, local groups and experts draw on a combination of pervasive games, online services and broadcasting to take part in national or even global events. We discuss the challenges that this raises for further research.","cites":"22","conferencePercentile":"79.45544554"},{"venue":"ACM Multimedia","id":"2a218c17944d72bfdc7f078f0337cab67536e501","venue_1":"ACM Multimedia","year":"2012","title":"Detection bank: an object detection based video representation for multimedia event recognition","authors":"Tim Althoff, Hyun Oh Song, Trevor Darrell","author_ids":"1745524, 2133680, 1753210","abstract":"While low-level image features have proven to be effective representations for visual recognition tasks such as object recognition and scene classification, they are inadequate to capture complex semantic meaning required to solve high-level visual tasks such as multimedia event detection and recognition. Recognition or retrieval of events and activities can be improved if specific discriminative objects are detected in a video sequence. In this paper, we propose an image representation, called <i>Detection Bank</i>, based on the detection images from a large number of windowed object detectors where an image is represented by different statistics derived from these detections. This representation is extended to video by aggregating the key frame level image representations through mean and max pooling. We empirically show that it captures complementary information to state-of-the-art representations such as Spatial Pyramid Matching and Object Bank. These descriptors combined with our Detection Bank representation significantly outperforms any of the representations alone on TRECVID MED 2011 data.","cites":"9","conferencePercentile":"84.01898734"},{"venue":"ACM Multimedia","id":"f2d13538f9eb811d6db0f27c5890958a9a6fd534","venue_1":"ACM Multimedia","year":"2012","title":"Interactive art \"maelstrom&vortex\": the body's speed - a race between digital and analog speeds","authors":"He-Lin Luo, Yi-Ping Hung","author_ids":"3001980, 7312257","abstract":"With an explosion of accessible information, the digital era has been integrated into our daily lives. At any time, people can operate handheld computers (cell phones) to connect to the online world. The thrill of speed is slowly replacing the analog speed of the human body. Even as the human body attempts to catch up with the speed of a computer, it tries to flee from it at the same time. This type of hope returns us to an original and natural sense of ambivalence, so that, in post-digital times, people will attempt to find the divine light of the analog era. The work, \"Maelstrom&#38;Vortex\", utilizes interactive art to try to explain analog and digital speeds. Through the interaction, people can experience differences in speed between physical sensations and computer data as it re-combines them with a sense of space. In the work, the analog movement of mechanical motors, the digital capture of cameras, and digital computation of computers are combined, and then intervened with the analog-like human body. This allows participants to discover the relationship amongst analog, digital, and the body.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"40bb091179136c7ce5ffad89d0ac2e96e054be17","venue_1":"ACM Multimedia","year":"2010","title":"Multimodal location estimation","authors":"Gerald Friedland, Oriol Vinyals, Trevor Darrell","author_ids":"1797144, 1689108, 1753210","abstract":"In this article we define a multimedia content analysis problem, which we call multimodal location estimation: Given a video/image/audio file, the task is to determine where it was recorded. A single indication, such as a unique landmark, might already pinpoint a location precisely. In most cases, however, a combination of evidence from the visual and the acoustic domain will only narrow down the set of possible answers. Therefore, approaches to tackle this task should be inherently multimedia. While the task is hard, in fact sometimes unsolvable, training data can be leveraged from the Internet in large amounts. Moreover, even partially successful automatic estimation of location opens up new possibilities in video content matching, archiving, and organization. It could revolutionize law enforcement and computer-aided intelligence agency work, especially since both semi-automatic and fully automatic approaches would be possible. In this article, we describe our idea of growing multimodal location estimation as a research field in the multimedia community. Based on examples and scenarios, we propose a multimedia approach to leverage cues from the visual and the acoustic portions of a video as well as from given metadata. We also describe experiments to estimate the amount of available training data that could potentially be used as publicly available infrastructure for research in this field. Finally, we present an initial set of results based on acoustic and visual cues and discuss the massive challenges involved and some possible paths to solutions.","cites":"27","conferencePercentile":"90.68493151"},{"venue":"ACM Multimedia","id":"37d45a03fb977e8e2cd0d2558c4225a2487986e8","venue_1":"ACM Multimedia","year":"2016","title":"Joint Graph Learning and Video Segmentation via Multiple Cues and Topology Calibration","authors":"Jingkuan Song, Lianli Gao, Mihai Marian Puscas, Feiping Nie, Fumin Shen, Nicu Sebe","author_ids":"1721410, 2671321, 3366004, 1688370, 2731972, 1703601","abstract":"Video segmentation has become an important and active research area with a large diversity of proposed approaches. Graph-based methods, enabling top performance on recent benchmarks, usually focus on either obtaining a precise similarity graph or designing efficient graph cutting strategies. However, these two components are often conducted in two separated steps, and thus the obtained similarity graph may not be the optimal one for segmentation and this may lead to suboptimal results. In this paper, we propose a novel framework, joint graph learning and video segmentation (JGLVS)}, which learns the similarity graph and video segmentation simultaneously. JGLVS learns the similarity graph by assigning adaptive neighbors for each vertex based on multiple cues (appearance, motion, boundary and spatial information). Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the similarity graph, such that the connected components in the resulted similarity graph are exactly equal to the number of segmentations. Furthermore, JGLVS can automatically weigh multiple cues and calibrate the pairwise distance of superpixels based on their topology structures. Most noticeably, empirical results on the challenging dataset VSB100 show that JGLVS achieves promising performance on the benchmark dataset which outperforms the state-of-the-art by up to 11% for the BPR metric.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"eef4750618d8952aa53f18c10a0a878d151705af","venue_1":"ACM Multimedia","year":"2016","title":"Emerging Topics in Learning from Noisy and Missing Data","authors":"Xavier Alameda-Pineda, Timothy M. Hospedales, Elisa Ricci, Nicu Sebe, Xiaogang Wang","author_ids":"1780201, 1697755, 1878028, 1703601, 2868636","abstract":"While vital for handling most multimedia and computer vision problems, collecting large scale fully annotated datasets is a resource-consuming, often unaffordable task. Indeed, on the one hand datasets need to be large and variate enough so that learning strategies can successfully exploit the variability inherently present in real data, but on the other hand they should be small enough so that they can be fully annotated at a reasonable cost. With the overwhelming success of (deep) learning methods, the traditional problem of balancing between dataset dimensions and resources needed for annotations became a full-fledged dilemma. In this context, methodological approaches able to deal with partially described data sets represent a one-of-a-kind opportunity to find the right balance between data variability and resource-consumption in annotation. These include methods able to deal with noisy, weak or partial annotations. In this tutorial we will present several recent methodologies addressing different visual tasks under the assumption of noisy, weakly annotated data sets.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"163ba08b33eb3b0e43e48f866df22e7a60101e9b","venue_1":"ACM Multimedia","year":"2015","title":"Unsupervised Extraction of Human-Interpretable Nonverbal Behavioral Cues in a Public Speaking Scenario","authors":"Md. Iftekhar Tanveer, Ji Liu, Mohammed E. Hoque","author_ids":"2497319, 3256884, 1828739","abstract":"We present a framework for unsupervised detection of nonverbal behavioral cues---hand gestures, pose, body movements, etc.---from a collection of motion capture (MoCap) sequences in a public speaking setting. We extract the cues by solving a sparse and shift-invariant dictionary learning problem, known as <i>shift-invariant sparse coding</i>. We find that the extracted behavioral cues are human-interpretable in the context of public speaking. Our technique can be applied to automatically identify the common patterns of body movements and the time-instances of their occurrences, minimizing time and efforts needed for manual detection and coding of nonverbal human behaviors.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"640058d40118d1400d92a5df70f2db6caf8c12f6","venue_1":"ACM Multimedia","year":"1997","title":"A Visual Approach to Multimedia Querying and Presentation","authors":"Isabel F. Cruz, Wendy T. Lucas","author_ids":"1696790, 7911576","abstract":"Multimedia data has become readily available from a variety of resources, such as the Web, to users (ranging from naive to sophisticated) who need to select and to present the data in a way that is meaningful to their particular applications. Delaunay \" \" is our framework for querying and presenting multimedia data stored in distributed data repositories, including the Web. It is unique in combining user-defined layouts with ad hoc querying capabilities, thereby enabling users to tailor, in a simple way, the layout of virtual documents composed of retrieved mdimedia objects. In this paper, we focus on the object-oriented data models, on the declarative query languages, and on how the results of the queries to disparate resources are integrated to form coherent user-defined documents.","cites":"28","conferencePercentile":"64.28571429"},{"venue":"ACM Multimedia","id":"8a8954bb9f4f09ce2fb79c8c22aadad80105a491","venue_1":"ACM Multimedia","year":"2008","title":"E-tree: emotionally driven augmented reality art","authors":"Stephen W. Gilroy, Marc Cavazza, Rémi Chaignon, Satu-Marja Mäkelä, Markus Niiranen, Elisabeth André, Thurid Vogt, Jérôme Urbain, Mark Billinghurst, Hartmut Seichter, Maurice Benayoun","author_ids":"1784425, 1696638, 2556405, 2280759, 2893186, 4843293, 1717432, 2262210, 1684805, 3211403, 2762819","abstract":"In this paper, we describe an Augmented Reality Art installation, which reacts to user behaviour using Multimodal analysis of affective signals. The installation features a virtual tree, whose growth is influenced by the perceived emotional response from spectators. The system implements a 'magic mirror' paradigm (using a large-screen display or projection system) and is based on the ARToolkit with extended representations for scene graphs. The system relies on a PAD dimensional model of affect to support the fusion of different affective modalities, while also supporting the representation of affective responses that relate to aesthetic impressions. The influence of affective input on the visual component is achieved by mapping affective data to an L-System governing virtual tree behaviour. We have performed an early evaluation of the system, both from the technical perspective and in terms of user experience. Post-hoc questionnaires were generally consistent with data from multimodal affective processing, and users rated the overall experience as positive and enjoyable, regardless of how proactive they were in their interaction with the installation.","cites":"12","conferencePercentile":"71.78899083"},{"venue":"ACM Multimedia","id":"257f31140b02f158c16c2911f30bc0ed7f715a6b","venue_1":"ACM Multimedia","year":"2016","title":"Academic Coupled Dictionary Learning for Sketch-based Image Retrieval","authors":"Dan Xu, Xavier Alameda-Pineda, Jingkuan Song, Elisa Ricci, Nicu Sebe","author_ids":"1751941, 1780201, 1721410, 1878028, 1703601","abstract":"In the last few years, the query-by-visual-example paradigm gained popularity, specially for content based retrieval systems. As sketches represent a natural way of expressing a synthetic query, recent research efforts focused on developing algorithmic solutions to address the sketch-based image retrieval (SBIR) problem. Within this context, we propose a novel approach for SBIR that, unlike previous methods, is able to exploit the visual complexity inherently present in sketches and images. We introduce academic learning, a paradigm in which the sample learning order is constructed both from the data, as in self-paced learning, and from partial curricula. We propose an instantiation of this paradigm within the framework of coupled dictionary learning to address the SBIR task. We also present an efficient algorithm to learn the dictionaries and the codes, and to pace the learning combining the reconstruction error, the prior knowledge suggested by the partial curricula and the cross-domain code coherence. In order to evaluate the proposed approach, we report an extensive experimental validation showing that the proposed method outperforms the state-of-the-art in coupled dictionary learning and in SBIR on three different publicly available datasets.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"722fcc35def20cfcca3ada76c8dd7a585d6de386","venue_1":"ACM Multimedia","year":"2014","title":"Caffe: Convolutional Architecture for Fast Feature Embedding","authors":"Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadarrama, Trevor Darrell","author_ids":"2717320, 1782282, 2718899, 3049736, 7343063, 2983898, 1687120, 1753210","abstract":"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.\n Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.","cites":"1720","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"318567451fc86c9a02afd1172aba6d0ab2d5d814","venue_1":"ACM Multimedia","year":"2006","title":"Image annotation refinement using random walk with restarts","authors":"Changhu Wang, Feng Jing, Lei Zhang, HongJiang Zhang","author_ids":"1697065, 7188215, 1724298, 1718558","abstract":"Image annotation plays an important role in image retrieval and management. However, the results of the state-of-the-art image annotation methods are often unsatisfactory. Therefore, it is necessary to refine the imprecise annotations obtained by existing annotation methods. In this paper, a novel approach to automatically refine the original annotations of images is proposed. On the one hand, for Web images, textual information, e.g. file name and surrounding text, is used to retrieve a set of candidate annotations. On the other hand, for non-Web images that are lack of textual information, a relevance model-based algorithm using visual information is used to decide the candidate annotations. Then, candidate annotations are re-ranked and only the top ones are reserved as the final annotations. To re-rank the annotations, an algorithm using Random Walk with Restarts (RWR) is proposed to leverage both the corpus information and the original confidence information of the annotations. Experimental results on both non-Web images of Corel dataset and Web images of photo forum sites demonstrate the effectiveness of the proposed method.","cites":"94","conferencePercentile":"97.92746114"},{"venue":"ACM Multimedia","id":"7213754685c5b7fbb759fae83ae52f4b6d54251d","venue_1":"ACM Multimedia","year":"2010","title":"Making computers look the way we look: exploiting visual attention for image understanding","authors":"Harish Katti, Subramanian Ramanathan, Mohan S. Kankanhalli, Nicu Sebe, Tat-Seng Chua, K. R. Ramakrishnan","author_ids":"2478739, 1742936, 1744045, 1703601, 1684968, 8224156","abstract":"Human Visual attention (HVA) is an important strategy to focus on specific information while observing and understanding visual stimuli. HVA involves making a series of <i>fixations</i> on select locations while performing tasks such as object recognition, scene understanding, <i>etc</i>. We present one of the first works that combines fixation information with automated concept detectors to (i) infer <i>abstract image semantics</i>, and (ii) enhance performance of object detectors.\n We develop visual attention-based models that sample fixation distributions and fixation transition distributions in <i>regions-of-interest</i> (ROI) to infer abstract semantics such as <i>expressive</i> faces and interactions (such as <i>look</i>, <i>read</i>, <i>etc.</i>). We also exploit eye-gaze information to deduce possible locations and scale of <i>salient</i> concepts and aid state-of-art detectors. A 18% performance increase with over 80% reduction in computational time for a state-of-art object detector [4].","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"97755741814e650f34a52c387c2ddd351df5a2ab","venue_1":"ACM Multimedia","year":"2006","title":"IGroup: a web image search engine with semantic clustering of search results","authors":"Feng Jing, Changhu Wang, Yuhuan Yao, Kefeng Deng, Lei Zhang, Wei-Ying Ma","author_ids":"7188215, 1697065, 2922552, 1796862, 1724298, 1705244","abstract":"In this demo, we present IGroup, a Web image search engine that organizes the search results into semantic clusters. Different from all existing Web image search results clustering algorithms that only cluster the top few images using visual or textual features, IGroup first identifies several query-related semantic clusters based on a key phrases extraction algorithm originally proposed for clustering general Web search results. Then, all the resulting images are separated and assigned to corresponding clusters. To make the best use of the clustering results, a new user interface is proposed. Please go to http://igroup.msra.cn for real experience.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"9146319b702042c0e2f9f6615674655c863b627b","venue_1":"ACM Multimedia","year":"2010","title":"FACT: fine-grained cross-media interaction with documents via a portable hybrid paper-laptop interface","authors":"Chunyuan Liao, Hao Tang, Qiong Liu, Patrick Chiu, Francine Chen","author_ids":"2686363, 3524342, 1794500, 2895008, 1800347","abstract":"FACT is an interactive paper system for fine-grained interaction with documents across the boundary between paper and computers. It consists of a small camera-projector unit, a laptop, and ordinary paper documents. With the camera-projector unit pointing to a paper document, the system allows a user to issue pen gestures on the paper document for selecting fine-grained content and applying various digital functions. For example, the user can choose individual words, symbols, figures, and arbitrary regions for keyword search, copy and paste, web search, and remote sharing. FACT thus enables a computer-like user experience on paper. This paper interaction can be integrated with laptop interaction for cross-media manipulations on multiple documents and views. We present the infrastructure, supporting techniques and interaction design, and demonstrate the feasibility via a quantitative experiment. We also propose applications such as document manipulation, map navigation and remote collaboration.","cites":"21","conferencePercentile":"88.76712329"},{"venue":"ACM Multimedia","id":"2e3748a0207bcba03fed1e761deebad89ff35ef6","venue_1":"ACM Multimedia","year":"2008","title":"Audio privacy: reducing speech intelligibility while preserving environmental sounds","authors":"Francine Chen, John Adcock, Shruti Krishnagiri","author_ids":"1800347, 5529541, 2152572","abstract":"Audio monitoring has many applications but also raises privacy concerns. In an attempt to help alleviate these concerns, we have developed a method for reducing the intelligibility of speech while preserving intonation and the ability to recognize most environmental sounds. The method is based on identifying vocalic regions and replacing the vocal tract transfer function of these regions with the transfer function from prerecorded vowels, where the identity of the replacement vowel is independent of the identity of the spoken syllable. The audio signal is then re-synthesized using the original pitch and energy, but with the modified vocal tract transfer function. We performed an intelligibility study which showed that environmental sounds remained recognizable but speech intelligibility can be dramatically reduced to a 7% word recognition rate.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"780557daaa39a445b24c41f637d5fc9b216a0621","venue_1":"ACM Multimedia","year":"2015","title":"Large Video Event Ontology Browsing, Search and Tagging (EventNet Demo)","authors":"Hongliang Xu, Guangnan Ye, Yitong Li, Dong Liu, Shih-Fu Chang","author_ids":"2368325, 2860010, 2664705, 5688961, 1735547","abstract":"In this demo we present PITAGORA\\footnote{Demo video available at http://bit.ly/1GgtUrN}: a mobile web contextual social network designed for the check-in area of an airport. The app provides recommendation of potential friends, local experts and targeted services. Recommendation is hybrid and combines social media analysis and collaborative filtering techniques. Users' recommendation has been evaluated through a user study with good results.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"22dada4a7ba85625824489375184ba1c3f7f0c8f","venue_1":"ACM Multimedia","year":"2015","title":"EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video","authors":"Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, Shih-Fu Chang","author_ids":"2860010, 2664705, 2368325, 5688961, 1735547","abstract":"Event-specific concepts are the semantic concepts specifically designed for the events of interest, which can be used as a mid-level representation of complex events in videos. Existing methods only focus on defining event-specific concepts for a small number of pre-defined events, but cannot handle novel unseen events. This motivates us to build a large scale event-specific concept library that covers as many real-world events and their concepts as possible. Specifically, we choose WikiHow, an online forum containing a large number of how-to articles on human daily life events. We perform a coarse-to-fine event discovery process and discover 500 events from WikiHow articles. Then we use each event name as query to search YouTube and discover event-specific concepts from the tags of returned videos. After an automatic filter process, we end up with 95,321 videos and 4,490 concepts. We train a <i>Convolutional Neural Network</i> (CNN) model on the 95,321 videos over the 500 events, and use the model to extract deep learning feature from video content. With the learned deep learning feature, we train 4,490 binary SVM classifiers as the event-specific concept library. The concepts and events are further organized in a hierarchical structure defined by WikiHow, and the resultant concept library is called EventNet. Finally, the EventNet concept library is used to generate concept based representation of event videos. To the best of our knowledge, EventNet represents the first video event ontology that organizes events and their concepts into a semantic structure. It offers great potential for event retrieval and browsing. Extensive experiments over the zero-shot event retrieval task when no training samples are available show that the proposed EventNet concept library consistently and significantly outperforms the state-of-the-art (such as the 20K ImageNet concepts trained with CNN) by a large margin up to 207%. We will also show that EventNet structure can help users find relevant concepts for novel event queries that cannot be well handled by conventional text based semantic analysis alone. The unique two-step approach of first applying event detection models followed by detection of event-specific concepts also provides great potential to improve the efficiency and accuracy of Event Recounting since only a very small number of event-specific concept classifiers need to be fired after event detection.","cites":"28","conferencePercentile":"99.25925926"},{"venue":"ACM Multimedia","id":"6958a2deeb7b88675a3da350b5e429703ea4fa5d","venue_1":"ACM Multimedia","year":"2013","title":"Towards a comprehensive computational model foraesthetic assessment of videos","authors":"Subhabrata Bhattacharya, Behnaz Nojavanasghari, Tao Chen, Dong Liu, Shih-Fu Chang, Mubarak Shah","author_ids":"1729765, 2974242, 4725884, 5688961, 1735547, 1745480","abstract":"In this paper we propose a novel aesthetic model emphasizing psycho-visual statistics extracted from multiple levels in contrast to earlier approaches that rely only on descriptors suited for image recognition or based on photographic principles. At the lowest level, we determine dark-channel, sharpness and eye-sensitivity statistics over rectangular cells within a frame. At the next level, we extract Sentibank features (1,200 pre-trained visual classifiers) on a given frame, that invoke specific sentiments such as \"colorful clouds\", \"smiling face\" etc. and collect the classifier responses as frame-level statistics. At the topmost level, we extract trajectories from video shots. Using viewer's fixation priors, the trajectories are labeled as foreground, and background/camera on which statistics are computed. Additionally, spatio-temporal local binary patterns are computed that capture texture variations in a given shot. Classifiers are trained on individual feature representations independently. On thorough evaluation of 9 different types of features, we select the best features from each level -- dark channel, affect and camera motion statistics. Next, corresponding classifier scores are integrated in a sophisticated low-rank fusion framework to improve the final prediction scores. Our approach demonstrates strong correlation with human prediction on 1,000 broadcast quality videos released by NHK as an aesthetic evaluation dataset.","cites":"20","conferencePercentile":"94.66666667"},{"venue":"ACM Multimedia","id":"c8c5f565f0b6b5d8f69daf71d6fab9313dfd045d","venue_1":"ACM Multimedia","year":"2012","title":"Hybrid social media network","authors":"Dong Liu, Guangnan Ye, Ching-Ting Chen, Shuicheng Yan, Shih-Fu Chang","author_ids":"5688961, 2860010, 2833615, 1698982, 1735547","abstract":"Analysis and recommendation of multimedia information can be greatly improved if we know the interactions between the content, user, and concept, which can be easily observed from the social media networks. However, there are many heterogeneous entities and relations in such networks, making it difficult to fully represent and exploit the diverse array of information. In this paper, we develop a <i>hybrid social media network</i>, through which the heterogeneous entities and relations are seamlessly integrated and a joint inference procedure across the heterogeneous entities and relations can be developed. The network can be used to generate personalized information recommendation in response to specific targets of interests, e.g., personalized multimedia albums, target advertisement and friend/topic recommendation. In the proposed network, each node denotes an entity and the multiple edges between nodes characterize the diverse relations between the entities (e.g., friends, similar contents, related concepts, favorites, tags, etc). Given a query from a user indicating his/her information needs, a propagation over the hybrid social media network is employed to infer the utility scores of all the entities in the network while learning the edge selection function to activate only a sparse subset of relevant edges, such that the query information can be best propagated along the activated paths. Driven by the intuition that much redundancy exists among the diverse relations, we have developed a robust optimization framework based on several sparsity principles. We show significant performance gains of the proposed method over the state of the art in multimedia retrieval and recommendation using data crawled from social media sites. To the best of our knowledge, this is the first model supporting not only aggregation but also judicious selection of heterogeneous relations in the social media networks.","cites":"7","conferencePercentile":"78.32278481"},{"venue":"ACM Multimedia","id":"3f9341ca11d776f923fe6046be0352d60532b34a","venue_1":"ACM Multimedia","year":"2011","title":"Next photo please: towards visually consistent sequential photo browsing","authors":"Dong Liu, Shuicheng Yan, HongJiang Zhang","author_ids":"5688961, 1698982, 1718558","abstract":"Sequential photo browsing has become the most important function in the desktop and online image repository management systems, where existing systems typically display the photos in default orders such as the lexicographic order of the photo filename or the chronological order of the photo taken time. However, these browsing orders, especially when the browsing speed is fast, ignore the vision persistency characteristic of human visual systems, which results in inconsistent visual experience for photo viewers. To address this issue, we construct a photo relationship graph based on various kinds of visual features that complementarily reflect human visual perception. Then the seeking of visually consistent photo browsing sequence is cast into a traveling salesman problem which seeks an optimal path with minimum visual distance within the graph structure. Experiment results on sequential browsing of Flickr photo groups indicate that the proposed method clearly beats the other sequential photo browsing methods in terms of visual consistency.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"7c82e4a9c48944dc9c24ac0c9b695244e74c109c","venue_1":"ACM Multimedia","year":"2010","title":"Unsupervised object category discovery via information bottleneck method","authors":"Zhengzheng Lou, Yangdong Ye, Dong Liu","author_ids":"2467033, 2382085, 5688961","abstract":"We present a novel approach to automatically discover object categories from a collection of unlabeled images. This is achieved by the Information Bottleneck method, which finds the optimal partitioning of the image collection by maximally preserving the relevant information with respect to the latent semantic residing in the image contents. In this method, the images are modeled by the Bag-of-Words representation, which naturally transforms each image into a visual document composed of visual words. Then the sIB algorithm is adopted to learn the object patterns by maximizing the semantic correlations between the images and their constructive visual words. Extensive experimental results on 15 benchmark image datasets show that the Information Bottleneck method is a promising technique for discovering the hidden semantic of images, and is superior to the state-of-the-art unsupervised object category discovery methods.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"0a70401d161c6c180d84e8139ee8bfbaadb2baad","venue_1":"ACM Multimedia","year":"2010","title":"Image retagging","authors":"Dong Liu, Xian-Sheng Hua, Meng Wang, HongJiang Zhang","author_ids":"5688961, 1746102, 1731598, 1718558","abstract":"Online social media repositories such as Flickr and Zooomr allow users to manually annotate their images with freely-chosen tags, which are then used as indexing keywords to facilitate image search and other applications. However, these tags are frequently imprecise and incomplete, though they are provided by human beings, and many of them are almost only meaningful for the image owners (such as the name of a dog). Thus there is still a gap between these tags and the actual content of the images, and this significantly limits tag-based applications, such as search and browsing. To tackle this issue, this paper proposes a social image \"retagging\" scheme that aims at assigning images with better content descriptors. The refining process, including denoising and enriching, is formulated as an optimization framework based on the consistency between \"visual similarity\" and \"semantic similarity\" in social images, that is, the visually similar images tend to have similar semantic descriptors, and vice versa. An effective iterative bound optimization algorithm is applied to learn the improved tag assignment. In addition, as many tags are intrinsically not closely-related to the visual content of the images, we employ knowledge based method to differentiate visual content related tags from unrelated ones and then constrain the tagging vocabulary of our automatic algorithm within the content related tags. Finally, to improve the coverage of the tags, we further enrich the tag set with appropriate synonyms and hypernyms based on an external knowledge base. Experimental results on a Flickr image collection demonstrate the effectiveness of this approach. We will also show the remarkable performance improvements brought by retagging via two applications, i.e., tag-based search and automatic annotation.","cites":"40","conferencePercentile":"96.02739726"},{"venue":"ACM Multimedia","id":"ba7b63fd964fae7ff30954d07a20728a3f65cca1","venue_1":"ACM Multimedia","year":"2002","title":"A pluggable service-to-service communication mechanism for home multimedia networks","authors":"Jin Nakazawa, Hideyuki Tokuda","author_ids":"1703867, 1700496","abstract":"This paper proposes a pluggable service-to-service (S2S) communication mechanism in a middleware for home networks, called Virtual Networked Appliance (VNA) architecture. In the architecture, service description method and the plug-gable S2S communication mechanism are separated in an orthogonal way. Through the separation, VNA architecture solved problems of home networks on which users have to operate multiple heterogeneous middleware technologies simultaneously: middleware fragmentation problem, due to complexity of realizing heterogeneous services on one middle-ware technology: aspect realization violation problem. The pluggable S2S communication mechanism provides service programmers with a simple aspect representation method to define a service-specific protocol concern apart from the service's implementation. It also provides off-the-shelf protocol modules of such well-known communication protocols as RTP, RTSP, HTTP, and SMTP for an inter-service communication, and dynamically loads them based on the aspects defined by the programmer. This reduces the complexity of implementing heterogeneous services on the VNA architecture, thereby addressing the problems. In this paper, we first clarify the two problems. Then, we describe the proposed mechanism with an overview of the middleware architecture referring to a composite service: \"Follow-You-and-Me Video.\"","cites":"3","conferencePercentile":"25.64102564"},{"venue":"ACM Multimedia","id":"add1833641164fcea412a70a02856a7a0e332126","venue_1":"ACM Multimedia","year":"2009","title":"Smart batch tagging of photo albums","authors":"Dong Liu, Meng Wang, Xian-Sheng Hua, HongJiang Zhang","author_ids":"5688961, 1731598, 1746102, 1718558","abstract":"As one of the emerging Web 2.0 activities, tagging becomes a popular approach to manage personal media data, such as photo albums. However, exhaustively tagging all photos in an album is a labor-intensive and time-consuming task, and simply entering tags for the whole album will significantly degrade the tagging accuracy. In this paper, we propose a smart batch tagging scheme that aims at facilitating users in album tagging. For a given album, it selects a set of representative exemplars for manual tagging, where the number of exemplars is dependent on the content of the photos.Then the tags of the rest photos are automatically inferred.In this way, the number of tagged photos is significantly reduced and we will show that high tagging accuracy can still be maintained. Therefore, a good trade-off between manual efforts and tagging performance can be achieved. Experimental results have demonstrated the effectiveness and usefulness of the proposed approach.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"058306d1de3c1a3de83e883c52b6f380709dc17f","venue_1":"ACM Multimedia","year":"2000","title":"A unified framework for semantics and feature based relevance feedback in image retrieval systems","authors":"Ye Lu, Chunhui Hu, Xingquan Zhu, HongJiang Zhang, Qiang Yang","author_ids":"2362714, 7403011, 1694121, 1718558, 1733090","abstract":"The relevance feedback approach to image retrieval is a powerful technique and has been an active research direction for the past few years. Various ad hoc parameter estimation techniques have been proposed for relevance feedback. In addition, methods that perform optimization on multi-level image content model have been formulated. However, these methods only perform relevance feedback on the low-level image features and fail to address the images' semantic content. In this paper, we propose a relevance feedback technique, <i>iFind</i>, to take advantage of the semantic contents of the images in addition to the low-level features. By forming a semantic network on top of the keyword association on the images, we are able to accurately deduce and utilize the images' semantic contents for retrieval purposes. The accuracy and effectiveness of our method is demonstrated with experimental results on real-world image collections.","cites":"132","conferencePercentile":"97.82608696"},{"venue":"ACM Multimedia","id":"9d164c719b276d9a04056b473a64eadcbec5876c","venue_1":"ACM Multimedia","year":"2005","title":"Face to face: a media-art using a face detection system and its exhibition","authors":"Yasuto Nakanishi","author_ids":"2533881","abstract":"\"Face to face\" is a media-art that only takes pictures of a profile or a blurring face, etc. those might be thought as failure pictures generally. Its theme is sameness and difference between camera and mirror, and it aims to offer an experience that people come across oneself whom he/she might not know after he/she sees oneself whom oneself usually see in a mirror. We had an exhibition at NTT ICC (Inter-Communication Center) in Tokyo. In seeing stored images, people seemed to find various activities thorough interacting with this work.","cites":"0","conferencePercentile":"4.950495049"},{"venue":"ACM Multimedia","id":"86ff576c2fa8d4acbce75093459f0bf6c915c7db","venue_1":"ACM Multimedia","year":"2003","title":"Automated annotation of human faces in family albums","authors":"Lei Zhang, Longbin Chen, Mingjing Li, HongJiang Zhang","author_ids":"1724298, 1679242, 8392859, 1718558","abstract":"Automatic annotation of photographs is one of the most desirable needs in family photograph management systems. In this paper, we present a learning framework to automate the face annotation in family photograph albums. Firstly, methodologies of content-based image retrieval and face recognition are seamlessly integrated to achieve automated annotation. Secondly, face annotation is formulated in a Bayesian framework, in which the face similarity measure is defined as <i>maximum a posteriori</i> (MAP) estimation. Thirdly, to deal with the missing features, marginal probability is used so that samples which have missing features are compared with those having the full feature set to ensure a non-biased decision. The experimental evaluation has been conducted within a family album of few thousands of photographs and the results show that the proposed approach is effective and efficient in automated face annotation in family albums.","cites":"78","conferencePercentile":"91.44144144"},{"venue":"ACM Multimedia","id":"0949071bbb5f79cf5a018572911823a127621086","venue_1":"ACM Multimedia","year":"2008","title":"UCam: direct manipulation using handheld camera for 3d gesture interaction","authors":"Liang Zhang, Yuanchun Shi, Mingming Fan","author_ids":"2257164, 1732440, 1868611","abstract":"This paper presents UCam a novel approach in 3D Gesture Interaction based on handheld camera movement. UCam reflects hand's movement and directly maps it to the movement of 3D object based on visual tracking of feature-like points on incoming frames. Only one button is needed to differentiate rotation and translation. The advantages of this technique lie in the popularity and low cost of handheld cameras, low requirement and no need of adjustment of background and easy to use for beginners. To evaluate UCam, it is compared with mouse in some 3D controlling tasks. The results show that UCam is more flexible and easier to use and master in most cases. Even for complicated tasks, UCam has comparable performance as mouse.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"132642afede580aba6aaabce90f7bc0d559d7b3b","venue_1":"ACM Multimedia","year":"2002","title":"MyPhotos: a system for home photo management and processing","authors":"Yanfeng Sun, HongJiang Zhang, Lei Zhang, Mingjing Li","author_ids":"8166022, 1718558, 1724298, 8392859","abstract":"<i>MyPhotos</i> is a prototype system for home photo management and processing. Several home user orientated image processing and analysis tools are provided. And several auto grouping methods can help user to organize photos. The system also provides a natural user interface and a workflow for easy browsing and searching.","cites":"29","conferencePercentile":"78.63247863"},{"venue":"ACM Multimedia","id":"e76f15eb0b849c21a771713c50ed9a211ae19fd5","venue_1":"ACM Multimedia","year":"2004","title":"Web services selection for distributed composition of multimedia content","authors":"Matthias Wagner, Wolfgang Kellerer","author_ids":"1768595, 1749109","abstract":"Growing numbers of pervasive devices are gaining access to the Internet. However, much of the existing rich multimedia content cannot be handled by mobile client devices with limited communication, processing, storage and display capabilities. In this paper, we propose new ways to enhance the universal access to multimedia content through Web Services and Semantic Web concepts. A semantic-based personalized delivery concept is drafted that makes use of these emerging technologies together with rather classical multimedia transcoding ideas. Instead of large, monolithic portal applications designed for multi-purpose adaptation and a single-source delivery, we propose to shift multimedia adaptation functionality to a portfolio of adequately selected Web Services. Web Services accessible through standard interfaces that allow for multimedia format conversion and composition can allow for a more flexible, application-independent adaptation and thus ease multimedia service provisioning essentially.","cites":"15","conferencePercentile":"72.54901961"},{"venue":"ACM Multimedia","id":"08561c9922bdce3d9045b50184ff361b256bdd5d","venue_1":"ACM Multimedia","year":"2013","title":"Creation of individual photo selections: read preferences from the users' eyes","authors":"Tina Walber, Chantal Neuhaus, Steffen Staab, Ansgar Scherp, Ramesh Jain","author_ids":"3244997, 3178039, 1752093, 1753135, 4521564","abstract":"The automated selection of satisfying subsets from large collections of photos is a central challenge in multimedia research. Objective criteria like the depiction of persons or the photo quality are met by existing approaches. But it is difficult to know the users' personal interest, which plays an important role in the selection process. The expected spread of devices with eye tracking support in the near future allows us to measure this interest in a new way. In an experiment with 12 participants, we derive the most interesting photos of a collection for every person from gaze information recorded during the free viewing of the photos. We can show that the eye tracking information delivers valuable information about the users' preferences by comparing the results to a manual selection. The selection based on gaze information significantly outperforms baseline approaches and improves the results by up to 17%. For photo sets of personal interest this improvement is even up to 23%.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"1fa5f3c6398d3ee37b67ddf75b0e971da36dea45","venue_1":"ACM Multimedia","year":"2010","title":"Integrated mobile visualization and interaction of events and POIs","authors":"Daniel Schmeiß, Ansgar Scherp, Steffen Staab","author_ids":"3245799, 1753135, 1752093","abstract":"We propose a new approach for mobile visualization and interaction of temporal information by integrating support for time with today's most prevalent visualization of spatial information the map. Our approach allows for an easy and precise selection of the time that is of interest and provides immediate feedback to the users when interacting with it. It has been developed in an evolutionary process gaining formative feedback from end users.","cites":"17","conferencePercentile":"86.30136986"},{"venue":"ACM Multimedia","id":"50a8a6fdbb9e133c2b1b0ae0fe752dde91e521eb","venue_1":"ACM Multimedia","year":"2014","title":"Music Emotion Recognition by Multi-label Multi-layer Multi-instance Multi-view Learning","authors":"Bin Wu, Erheng Zhong, Andrew Horner, Qiang Yang","author_ids":"6023931, 1762818, 1718185, 1733090","abstract":"Music emotion recognition, which aims to automatically recognize the affective content of a piece of music, has become one of the key components of music searching, exploring, and social networking applications. Although researchers have given more and more attention to music emotion recognition studies, the recognition performance has come to a bottleneck in recent years. One major reason is that experts' labels for music emotion are mostly song-level, while music emotion usually varies within a song. Traditional methods have considered each song as a single instance and have built models based on song-level features. However, they ignored the dynamics of music emotion and failed to capture accurate emotion-feature correlations. In this paper, we model music emotion recognition as a novel multi-label multi-layer multi-instance multi-view learning problem: music is formulated as a hierarchical multi-instance structure (e.g., song-segment-sentence) where multiple emotion labels correspond to at least one of the instances with multiple views of each layer. We propose a Hierarchical Music Emotion Recognition model (HMER) -- a novel hierarchical Bayesian model using sentence-level music and lyrics features. It captures music emotion dynamics with a song-segment-sentence hierarchical structure. HMER also considers emotion correlations between both music segments and sentences. Experimental results show that HMER outperforms several state-of-the-art methods in terms of $F_1$ score and mean average precision.","cites":"5","conferencePercentile":"80.72289157"},{"venue":"ACM Multimedia","id":"50ac4d883b5f41e6c94a14051fedc957d40c4c3e","venue_1":"ACM Multimedia","year":"2000","title":"A prediction system for multimedia pre-fetching in Internet","authors":"Zhong Su, Qiang Yang, HongJiang Zhang","author_ids":"1703625, 1733090, 1718558","abstract":"The rapid development of Internet has resulted in more and more multimedia in Web content. However, due to the limitation in the bandwidth and huge size of the multimedia data, users always suffer from long time waiting. On the other hand, if we can predict the web object or page that the user most likely will view next while the user is viewing the current page, and pre-fetch the content, then the perceived network latency can be significantly reduced. In this paper, we present an n-gram based model to utilize path profiles of users from very large web log to predict the users' future requests. Our model is based on a simple extension of existing point-based models for such predictions, but our results show that by sacrificing the applicability somewhat one can gain a great deal in prediction precision. Also we present an efficient method to compress the prediction model size so that it can be fitted into the main memory. Our result can potentially be applied to a wide range of applications on the web, including pro-fetching, enhancement of recommendation systems as well as web caching policies. The experiments based on three realistic web logs have proved the effectiveness of the proposed scheme.","cites":"28","conferencePercentile":"82.60869565"},{"venue":"ACM Multimedia","id":"2237b251f1114431105bb7880fef9c5f3d7187ea","venue_1":"ACM Multimedia","year":"2012","title":"QoE-based opportunistic transmission for video broadcasting in heterogeneous circumstance","authors":"Wen Ji, Zhu Li, Yiqiang Chen","author_ids":"6359191, 4093066, 4070304","abstract":"This paper presents an opportunistic transmission scheme for layered video broadcasting to multiple heterogeneous devices. In contrast to conventional wireless video broadcasting system, the main ideas proposed here include: (i) exploit the quality of heterogeneous user experience (QoE) metric under wireless broadcasting scenario, with consideration of various channel state, device capability, video content urgency and the number of demanding users. (ii) formulate reliable multiple video streams broadcasting to heterogeneous devices as an aggregate maximum utility achieving problem. (iii) use opportunistic scheduling to select suitable users in each transmission interval so as to improve the broadcasting utility. (iv) use parallel-pipe structure to transmit the layered video with Fountain coding protection, which provide reliable and low-latency transmission in heterogeneous circumstance. Numerical experiments demonstrate that the proposed scheme outperforms conventional methods.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"6de839f607a55b39f23c22eaa48e21a4851ad567","venue_1":"ACM Multimedia","year":"2008","title":"Understanding video interactions in youtube","authors":"Fabrício Benevenuto, Fernando Duarte, Tiago Rodrigues, Virgílio A. F. Almeida, Jussara M. Almeida, Keith W. Ross","author_ids":"2810330, 2329285, 5033919, 7360316, 8118988, 4053633","abstract":"This paper seeks understanding the user behavior in a social network created essentially by video interactions. We present a characterization of a social network created by the video interactions among users on YouTube, a popular social networking video sharing system. Our results uncover typical user behavioral patterns as well as show evidences of anti-social behavior such as self-promotion and other types of content pollution.","cites":"23","conferencePercentile":"86.23853211"},{"venue":"ACM Multimedia","id":"6482dcb3d1b9798039b56395db86eb1965cd9060","venue_1":"ACM Multimedia","year":"2004","title":"The relative effectiveness of concept-based versus content-based video retrieval","authors":"Meng Yang, Barbara M. Wildemuth, Gary Marchionini","author_ids":"1764252, 3094359, 1791487","abstract":"Three video search systems were compared in the interactive search task at the TRECVID 2003 workshop: a &#60;i>text-only&#60;/i> system, which searched video shots through transcripts; a &#60;i>features-only&#60;/i> system, which searched video shots through 16 video content features (e.g., airplanes and people); and a &#60;i>combined&#60;/i> system, which searched through both transcripts and content features. 36 participants each completed 12 video search tasks. The hypothesis that the combined system would perform better than both the text-only and the features-only systems was not supported, and large topic effects were found. Further analysis showed that concept-based video retrieval worked best for &#60;i>specific&#60;/i> topics, whereas the hybrid retrieval techniques which combine both concept- and content-based video retrieval showed some advantage when searching for &#60;i>generic&#60;/i> topics. The results have implications for topic/task analysis for video retrieval research, and also for the implementation of hybrid video retrieval systems.","cites":"10","conferencePercentile":"62.00980392"},{"venue":"ACM Multimedia","id":"f3e4d0f1da79aac75ec62e15623ba67be3182432","venue_1":"ACM Multimedia","year":"2010","title":"The use of non-conventional methods for content analysis and understanding: panel overview","authors":"Nicu Sebe, Qi Tian","author_ids":"1703601, 1724745","abstract":"This panel will enable the participants to understand key concepts, state-of-the-art techniques, and open issues in content analysis and understanding that make use of non-conventional methods. As such we will cover aspects such as (1) eye gaze for multimodal interaction and content analysis; (2) multimodal interaction for affective retrieval and in affective interfaces: approaches to multimedia content analysis and interaction that use multiple channels of information, new interaction paradigms and physiological signals; (3) the use of brain signals for advanced brain-computer interaction and interfaces; and (4) applications: traditional and emerging application areas.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"5e0799a3ed10b76083173e2cadeac4c5a0e04749","venue_1":"ACM Multimedia","year":"2006","title":"Image annotation by large-scale content-based image retrieval","authors":"Xirong Li, Le Chen, Lei Zhang, Fuzong Lin, Wei-Ying Ma","author_ids":"7137848, 7628457, 1724298, 2025858, 1705244","abstract":"Image annotation has been an active research topic in recent years due to its potentially large impact on both image understanding and Web image search. In this paper, we target at solving the automatic image annotation problem in a novel <i>search</i> and <i>mining</i> framework. Given an uncaptioned image, first in the search stage, we perform content-based image retrieval (CBIR) facilitated by high-dimensional indexing to find a set of visually similar images from a large-scale image database. The database consists of images crawled from the World Wide Web with rich annotations, e.g. titles and surrounding text. Then in the <i>mining</i> stage, a search result clustering technique is utilized to find most representative keywords from the annotations of the retrieved image subset. These keywords, after salience ranking, are finally used to annotate the uncaptioned image. Based on search technologies, this framework does not impose an explicit training stage, but efficiently leverages large-scale and well-annotated images, and is potentially capable of dealing with unlimited vocabulary. Based on 2.4 million real Web images, comprehensive evaluation of image annotation on Corel and U. Washington image databases show the effectiveness and efficiency of the proposed approach.","cites":"57","conferencePercentile":"94.30051813"},{"venue":"ACM Multimedia","id":"0f2b3973e6100d3d92df03210c6ad2bc1a9ec191","venue_1":"ACM Multimedia","year":"2005","title":"Iteratively clustering web images based on link and attribute reinforcements","authors":"Xin-Jing Wang, Wei-Ying Ma, Lei Zhang, Xing Li","author_ids":"3349534, 1705244, 1724298, 7137486","abstract":"Image clustering is an important research topic which contributes to a wide range of applications. Traditional image clustering approaches are based on image content features only, while content features alone can hardly describe the semantics of the images. In the context of Web, images are no longer assumed homogeneous and \"flatdistributed but are richly structured. There are two kinds of reinforcements embedded in such data: 1) the reinforcement between attributes of different data types (intra-type links reinforcements); and 2) the reinforcement between object attributes and the inter-type links (inter-type links reinforcements). Unfortunately, most of the previous works addressing relational data failed to fully explore the reinforcements. In this paper, we propose a reinforcement clustering framework to tackle this problem. It reinforces images and texts' attributes via inter-type links and inversely uses these attributes to update these links. The iterative reinforcing nature of this framework promises the discovery of the semantic structure of images, which is the basis of image clustering. Experimental results show the effectiveness of our proposed framework.","cites":"13","conferencePercentile":"65.59405941"},{"venue":"ACM Multimedia","id":"046bc0e9e76315f1b7d4a1c4754c890b1d43756d","venue_1":"ACM Multimedia","year":"2004","title":"Efficient propagation for face annotation in family albums","authors":"Lei Zhang, Yuxiao Hu, Mingjing Li, Wei-Ying Ma, HongJiang Zhang","author_ids":"1724298, 7741920, 8392859, 1705244, 1718558","abstract":"In this paper, we propose and investigate a new user scenario for face annotation, in which users are allowed to multi-select a group of otogras and assign names to these otogras. The system will then attempt to propagate names from otogra level to face level, i.e. to infer the correspondence between name and face. Given the face similarity measure which combines methodologies from face recognition and content-based image retrieval, we formulate name propagation as an optimization problem. We define the objective function as the sum of similarities between each pair of faces of the same individual in different otogras, and propose an iterative optimization algorithm to infer the optimal correspondence. To make the propagation result reliable, a reject scheme is adopted to reject those with low confidence scores. Furthermore, we investigate the combination and alternation of browsing mode for propagation and viewer mode for annotation, so that each mode can benefit from additional inputs from the other mode. The experimental evaluation has been conducted within a typical family album of over one thousand otogras and the results show that the proposed approach is effective and efficient in automated face annotation in family albums.","cites":"41","conferencePercentile":"88.48039216"},{"venue":"ACM Multimedia","id":"3ec7f9226d4300ee87d4c7d60daf383353d57175","venue_1":"ACM Multimedia","year":"2010","title":"Sonify your face: facial expressions for sound generation","authors":"Roberto Valenti, Alejandro Jaimes, Nicu Sebe","author_ids":"1958930, 1730325, 1703601","abstract":"We present a novel visual creativity tool that automatically recognizes facial expressions and tracks facial muscle movements in real time to produce sounds. The facial expression recognition module detects and tracks a face and outputs a feature vector of motions of specific locations in the face. The feature vector is used as input to a Bayesian network which classifies facial expressions into several categories (e.g., angry, disgusted, happy, etc.). The classification results are used along with the feature vector to generate a combination of sounds that change in real time depending on the person's facial expressions. We explain the artistic motivation behind the work, the basic components of our tool, and possible applications in the arts (performance, installation) and in the medical domain. Finally, we report on the experience of approximately 25 users of our system at a conference demonstration session, of 9 participants in a pilot study to assess the system's usability, and discuss our experience installing the work at an important digital arts festival (RE-NEW 2009).","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"2013655e505eba3ff48928ee13ce6964c4bf4ac2","venue_1":"ACM Multimedia","year":"2010","title":"Behavior and properties of spatio-temporal local features under visual transformations","authors":"Julian Stöttinger, Bogdan Tudor Goras, Nicu Sebe, Allan Hanbury","author_ids":"3245731, 2727937, 1703601, 1699657","abstract":"Successful state-of-the-art video retrieval and classification applications are predominantly carried out by means of spatio-temporal features. Typically, the evaluation of these tasks is exclusively done based on their final performance but no systematic analysis of feature robustness, invariance and stability has been done yet for large scale video retrieval. In this work, we analyze the impact of visual transformation on spatio-temporal features in large scale experiments. Following the recipe of recent state of the art evaluations, we choose the best performing approaches, namely the spatio-temporal Harris3D, Hessian3D, and Cuboid detectors and the HOG/HOF, SURF3D, and HOG3D descriptors. We show that these features have different properties and behave differently under varying transformations (challenges). This helps researchers to justify the choice of features for new applications and helps to optimize the choice of input video in terms of resolution, compression, frames per second or noise suppression. We make the extracted features accessible on-line for further independent evaluation and applications.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"8144ab63a5529fb003432c5858986f74e20afaea","venue_1":"ACM Multimedia","year":"2008","title":"3rd international workshop on human-centered computing (HCC '08)","authors":"Alejandro Jaimes, Daniela Nicklas, Nicu Sebe","author_ids":"1730325, 1696621, 1703601","abstract":"In this workshop summary we describe the motivation for continued discussion in Human-Centered Computing, giving an outline of the articles presented at the workshop, its expected outcomes, and future activities. We emphasize the reasoning behind a non-traditional format for the workshop, which builds on the previous workshops on \"Human-Centered Multimedia\" held in conjunction with ACM Multimedia 2007 and 2006.","cites":"1","conferencePercentile":"23.62385321"},{"venue":"ACM Multimedia","id":"943c74c068ad70db1503e8cdaffadb85e21bcfa9","venue_1":"ACM Multimedia","year":"2007","title":"Human-centered multimedia systems: tutorial overview","authors":"Nicu Sebe, Alejandro Jaimes, Hamid K. Aghajan","author_ids":"1703601, 1730325, 1768752","abstract":"Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. This tutorial takes a holistic view on the research issues and applications of Human-Centered Multimedia Systems focusing on three main areas: (1)multimodal interaction: visual (body, gaze, gesture) and audio (emotion)analysis; (2) image databases, indexing, and retrieval: context modeling, cultural issues, and machine learning for user-centric approaches; (3)multimedia data: conceptual analysis at different levels (feature, cognitive, and affective). This paper gives a brief overview of the areas covered in the tutorial.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"38c6b1319eabba8b519930559907aae9c1e6e37e","venue_1":"ACM Multimedia","year":"2006","title":"Human-centered computing: a multimedia perspective","authors":"Alejandro Jaimes, Nicu Sebe, Daniel Gatica-Perez","author_ids":"1730325, 1703601, 1698682","abstract":"Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. In this paper, we give an overview of HCC from a Multimedia perspective. We describe what we consider to be the three main areas of Human-Centered Multimedia (HCM): media production, analysis, and interaction. In addition, we identify the core characteristics of HCM, describe example applications, and propose a research agenda for HCM.","cites":"60","conferencePercentile":"94.81865285"},{"venue":"ACM Multimedia","id":"1a7f1685e4c9a200b0c213060e203137279142d6","venue_1":"ACM Multimedia","year":"2009","title":"Ranking with local regression and global alignment for cross media retrieval","authors":"Yi Yang, Dong Xu, Feiping Nie, Jiebo Luo, Yueting Zhuang","author_ids":"1698559, 1714390, 1688370, 1717319, 1755711","abstract":"Rich multimedia content including images, audio and text are frequently used to describe the same semantics in E-Learning and Ebusiness web pages, instructive slides, multimedia cyclopedias, and so on. In this paper, we present a framework for cross-media retrieval, where the query example and the retrieved result(s) can be of different media types. We first construct Multimedia Correlation Space (MMCS) by exploring the semantic correlation of different multimedia modalities, during which multimedia content and co-occurrence information is utilized. We propose a novel ranking algorithm, namely ranking with Local Regression and Global Alignment (LRGA), which learns a robust Laplacian matrix for data ranking. In LRGA, for each data point, a local linear regression model is used to predict the ranking values of its neighboring points. We propose a unified objective function to globally align the local models from all the data points so that an optimal ranking value can be assigned to each data point. LRGA is insensitive to parameters, making it particularly suitable for data ranking. A relevance feedback algorithm is proposed to improve the retrieval performance. Comprehensive experiments have demonstrated the effectiveness of our methods.","cites":"69","conferencePercentile":"97.52066116"},{"venue":"ACM Multimedia","id":"426add1b779a8efc56a7ee99f01f216b68a8d32d","venue_1":"ACM Multimedia","year":"2008","title":"Localization and mapping of surveillance cameras in city map","authors":"Wee Kheng Leow, Cheng-Chieh Chiang, Yi-Ping Hung","author_ids":"1787377, 1721143, 7312257","abstract":"Many large cities have installed surveillance cameras to monitor human activities for security purposes. An important surveillance application is to track the motion of an object of interest, e.g., a car or a human, using one or more cameras, and plot the motion path in a city map. To achieve this goal, it is necessary to localize the cameras in the city map and to determine the correspondence mappings between the positions in the city map and the camera views. Since the view of the city map is roughly orthogonal to the camera views, there are very few common features between the two views for a computer vision algorithm to correctly identify corresponding points automatically. This paper proposes a method for camera localization and position mapping that requires minimum user inputs. Given approximate corresponding points between the city map and a camera view identified by a user, the method computes the orientation and position of the camera in the city map, and determines the mapping between the positions in the city map and the camera view. Both quantitative tests and practical application test have been performed. It can obtain the best-fit solutions even though the user-specified correspondence is inaccurate. The performance of the method is assessed in both quantitative tests and practical application. Quantitative test results show that the method is accurate and robust in camera localization and position mapping. Application test results are very encouraging, showing the usefulness of the method in real applications.","cites":"1","conferencePercentile":"23.62385321"},{"venue":"ACM Multimedia","id":"03ff6a3942e538be959118e2c399bb6c62d37276","venue_1":"ACM Multimedia","year":"1999","title":"Multi-scale sub-image search","authors":"Nicu Sebe, Michael S. Lew, Dionysius P. Huijsmans","author_ids":"1703601, 1731570, 2901762","abstract":"If an image should be retrieved by its subregions from a large image database, an immense number of possible queries will appear. Therefore, the index which encodes the spatial information of an image, should make only few assumptions about possible queries. In addition , this index has to consider different scales of objects in the image. In this paper, we propose a novel approach using a hierarchical index encoding image regions , gained by a fixed partition. The suggested index uses color features and is easy to implement. The index is tested on a database with more than 12,000 images.","cites":"21","conferencePercentile":"71.00840336"},{"venue":"ACM Multimedia","id":"19cf2e25b7ca2e0300cf16d2ef0d8492cce3a188","venue_1":"ACM Multimedia","year":"1999","title":"Robust color indexing","authors":"Nicu Sebe, Michael S. Lew","author_ids":"1703601, 1731570","abstract":"In content based image retrieval, color indexing is one of the most prevalent retrieval methods. In literature, most of the attention has been focussed on the color model with little or no consideration of the noise models. In this paper we investigate the problem of color indexing from a maximum likelihood perspective. We take into account the color model, the noise distribution, and the quantization of the color features. Furthermore, from the real noise distribution we derive a distortion measure, which consistently provides improved accuracy. Our investigation concludes with results on a real stock photography database, consisting of 11,000 color images.","cites":"6","conferencePercentile":"44.53781513"},{"venue":"ACM Multimedia","id":"bcbd96b64e0a745710da7d0e191af651ac541921","venue_1":"ACM Multimedia","year":"2009","title":"SMALLab: a mixed-reality environment for embodied and mediated learning","authors":"Aisling Kelliher, David Birchfield, Ellen Campana, Sarah Hatton, Mina Johnson-Glenberg, Christopher Martinez, Loren Olson, Philippos Savvides, Lisa M. Tolentino, Kelly Phillips, Sibel Uysal","author_ids":"1820838, 1731986, 1720935, 2890622, 2860127, 2171410, 2827994, 2627166, 2504681, 2114122, 1961645","abstract":"In this video presentation, we introduce the Situated Multimedia Arts Learning Lab [SMALLab], a mixed-reality learning environment that supports interactive engagement through full body 3D movements and gestures within a collaborative, computationally mediated space. The video begins by describing the holistic approach to embodied and mediated learning developed by our transdisciplinary research team, grounded in understandings derived from research in the learning sciences, digital media and human computer interaction. We then outline the three core tenets of effective learning exemplified by our research -- embodiment, multimodality and collaboration. The video next demonstrates the design and functionality of the physical and digital components of SMALLab. We conclude by illustrating our partner collaborations with K12 teachers and students with four scenarios depicting Geography, Physics, Language Arts and Chemistry learning modules.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"0f4a462bf14e3a36dda7e0a9aa41d039efaa1e45","venue_1":"ACM Multimedia","year":"2010","title":"Building with a memory: responsive color interventions","authors":"Andreea Danielescu, Ryan P. Spicer, David Tinapple, Aisling Kelliher, Ellen Campana","author_ids":"1922984, 2977476, 3260223, 1820838, 1720935","abstract":"Building with a Memory is a subtle responsive intervention that aims to provide cohesion and community awareness through the use of light and color. The installation delivers thought-provoking information by capturing, analyzing and rendering real-time and archived human activity in a workplace setting. The installation senses movement in the space through an IR camera and computer vision techniques. Two custom lighting fixtures and a video monitor render the aggregated movements. The visually simple aesthetic of the piece aims to balance active engagement and passive contribution, providing a rewarding experience for both occasional passersby and regular users of the space. This paper describes the motivations and contributions of the installation, together with insights gained from an informal evaluation and directions for future explorations.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"8a16c692acdaac9cd405ced6307cf1d30a0ccb05","venue_1":"ACM Multimedia","year":"2011","title":"Abstract rendering of human activity in a dynamic distributed learning environment","authors":"Andreea Danielescu, Ryan P. Spicer, David Tinapple, Aisling Kelliher, Shawn Nikkila, Sean Burdick","author_ids":"1922984, 2977476, 3260223, 1820838, 3000790, 2699876","abstract":"Contemporary distributed enterprises present challenges in terms of demonstrating community activity awareness and coherence across individuals and teams in collaborating networks. Building with a Memory is an experiential media system that captures and represents human activity in a distributed workplace over time. The system senses and analyzes movement in two workspaces in a mixed-use building with the results rendered in an informative ambient display in the building entryway. We describe the design and development of the system, together with insights from two studies of the installation and promising future directions.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"1cb5547c3f5ff42746bf9c4e083795aed3c8c609","venue_1":"ACM Multimedia","year":"2016","title":"Capped Lp-Norm Graph Embedding for Photo Clustering","authors":"Mengfan Tang, Feiping Nie, Ramesh Jain","author_ids":"3057568, 1688370, 4521564","abstract":"Photos are a predominant source of information on a global scale. Cluster analysis of photos can be applied to situation recognition and understanding cultural dynamics. Graph-based learning provides a current approach for modeling data in clustering problems. However, the performance of this framework depends heavily on initial graph construction by input data. Data outliers degrade graph quality, leading to poor clustering results. We designed a new capped lp-norm graph-based model to reduce the impact of outliers. This is accomplished by allowing the data graph to self adjust as part of the graph embedding. Furthermore, we derive an iterative algorithm to solve the objective function optimization problem. Experiments on four real-world benchmark data sets and Yahoo Flickr Creative Commons data set show the effectiveness of this new graph-based capped lp-norm clustering method.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"135b5d7c2dfc1639dc4d1ab16c9d4d0629c39229","venue_1":"ACM Multimedia","year":"2011","title":"Learning frame relevance for video classification","authors":"Hua Wang, Feiping Nie, Heng Huang, Yi Yang","author_ids":"6098612, 1688370, 1748032, 1698559","abstract":"Traditional video classification methods typically require a large number of labeled training video frames to achieve satisfactory performance. However, in the real world, we usually only have sufficient labeled video clips (such as tagged online videos) but lack labeled video frames. In this paper, we formalize the video classification problem as a Multi-Instance Learning (MIL) problem, an emerging topic in machine learning in recent years, which only needs bag (video clip) labels. To solve the problem, we propose a novel Parameterized Class-to-Bag (P-C2B) Distance method to learn the relative importance of a training instance with respect to its labeled classes, such that the instance level labeling ambiguity in MIL is tackled and the frame relevances of training video data with respect to the semantic concepts of interest are given. Promising experimental results have demonstrated the effectiveness of the proposed method.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"13cd82d236506bf1a0e4fd7bb529071cbac3cacc","venue_1":"ACM Multimedia","year":"2008","title":"MultiPresenter: a presentation system for (very) large display surfaces","authors":"Joel Lanir, Kellogg S. Booth, Anthony Tang","author_ids":"2211260, 1800617, 3151023","abstract":"We introduce MultiPresenter, a novel presentation system designed to work on very large display spaces (multiple displays or physically large high-resolution displays). MultiPresenter allows presenters to organize and present pre-made and dynamic presentations that take advantage of a very large display space accessed from a personal laptop. Presenters can use the extra space to provide long-term persistency of information to the audience. Our design deliberately separates content generation (authoring) from the presentation of content. We focus on supporting presentation flow and a variety of presentation styles, ranging from automated, scripted sequences of pre-made slides to highly dynamic ad-hoc, and non-linear content. By providing smooth transition between these styles, presenters can easily alter the flow of content during a presentation to adapt to an audience or to change emphasis in response to emerging interests. We describe our goals, rationale and the design process, providing a detailed description of the current version of the system, and discuss our experience using it throughout a one-semester first year computer science course.","cites":"17","conferencePercentile":"77.98165138"},{"venue":"ACM Multimedia","id":"b94a1a9b50560e38da10b87d5493fd029c26513c","venue_1":"ACM Multimedia","year":"2004","title":"Mining emergent structures from mixed media For content retrieval","authors":"Jamie Ng, Kanagasabai Rajaraman, Edward Altman","author_ids":"3125770, 1994514, 2783723","abstract":"In this paper we present a novel approach for retrieval of thematic video content from mixed media. Based on the principles of conceptual blending, information from different media is mined for emergent structures from mixed media. We have built a system, called OntoMedia, to test the efficacy of this approach over traditional methods for media retrieval. The system employs an ontology as a unified indexing scheme for associated text documents for the mixed media content. By applying graph theoretic path finding operations to the ontology to process queries for video content, we show that the system is able to synchronize information from multiple media sources and retrieve semantically related content across media types. A trial experiment was carried out using mixed media courseware from the Singapore-MIT Alliance (SMA) distance education course. Results showed that information retrieval along a path performs significantly better than keyword-based and ontology-focused searches.","cites":"1","conferencePercentile":"19.11764706"},{"venue":"ACM Multimedia","id":"102b968d836177f9c436141e382915a4f8549276","venue_1":"ACM Multimedia","year":"2005","title":"Affective multimodal human-computer interaction","authors":"Maja Pantic, Nicu Sebe, Jeffrey F. Cohn, Thomas S. Huang","author_ids":"1694605, 1703601, 1737918, 1739208","abstract":"Social and emotional intelligence are aspects of human intelligence that have been argued to be better predictors than IQ for measuring aspects of success in life, especially in social interactions, learning, and adapting to what is important. When it comes to machines, not all of them will need such skills. Yet to have machines like computers, broadcast systems, and cars, capable of adapting to their users and of anticipating their wishes, endowing them with the ability to recognize user's affective states is necessary. This article discusses the components of human affect, how they might be integrated into computers, and how far are we from realizing affective multimodal human-computer interaction.","cites":"96","conferencePercentile":"97.52475248"},{"venue":"ACM Multimedia","id":"d2315ad0b9bff26cbdf95021e7b90a2e1651dbd5","venue_1":"ACM Multimedia","year":"2007","title":"Cross-modal correlation learning for clustering on image-audio dataset","authors":"Hong Zhang, Yueting Zhuang, Fei Wu","author_ids":"1734058, 1755711, 1695826","abstract":"It is interesting and challenging to explore correlations between different datasets and utilize such correlations for the clustering on these datasets. Cross-modal correlation between images and audios can help identify images (or audios) of certain semantics. However, the heterogeneous problem makes it difficult to learn cross-modal correlation between visual and auditory features. In this paper, we analyze canonical correlation between feature matrices of images and audios during subspace mapping; then we design correlation-based similarity reinforcement for images and audios; thirdly we implement image clustering and audio clustering with affinity propagation. Experiment results on image-audio dataset are encouraging and show that the performance of our approach is effective. We give an interesting application of querying images by audio examples.","cites":"18","conferencePercentile":"76.30208333"},{"venue":"ACM Multimedia","id":"75dc7690ea966436677dad8fca3abab86650c57b","venue_1":"ACM Multimedia","year":"2005","title":"Ere be dragons: an interactive artwork","authors":"Stephen Boyd Davis, Magnus Moar, John Cox, Chris Riddoch, Karl Cooke, Rachel Jacobs, Matt Watkins, Richard Hull, Tom Melamed","author_ids":"3340571, 2791126, 1786109, 1974430, 3322547, 4455402, 2247096, 2639761, 2155752","abstract":"The paper introduces a pervasive digital artwork which harnesses live heart-rate and GPS data to create a novel experience on a Pocket PC. The aims of the project, the technologies employed and the results of a preliminary trial are briefly described.","cites":"7","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"5bcd183a6b408c5dcc3b17574607964a4dd54b8b","venue_1":"ACM Multimedia","year":"2016","title":"Multilayer and Multimodal Fusion of Deep Neural Networks for Video Classification","authors":"Xiaodong Yang, Pavlo Molchanov, Jan Kautz","author_ids":"5177497, 2824500, 1690538","abstract":"This paper presents a novel framework to combine multiple layers and modalities of deep neural networks for video classification. We first propose a multilayer strategy to simultaneously capture a variety of levels of abstraction and invariance in a network, where the convolutional and fully connected layers are effectively represented by our proposed feature aggregation methods. We further introduce a multimodal scheme that includes four highly complementary modalities to extract diverse static and dynamic cues at multiple temporal scales. In particular, for modeling the long-term temporal information, we propose a new structure, FC-RNN, to effectively transform pre-trained fully connected layers into recurrent layers. A robust boosting model is then introduced to optimize the fusion of multiple layers and modalities in a unified way. In the extensive experiments, we achieve state-of-the-art results on two public benchmark datasets: UCF101 and HMDB51.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2c5d75fc70f84c6c51bdc9d7c70d234747bff889","venue_1":"ACM Multimedia","year":"2011","title":"From images to 3d models made easy","authors":"Isaac Esteban, Judith Dijk, Frans C. A. Groen","author_ids":"2945200, 3239849, 8397760","abstract":"FIT3D is a Toolbox built for Matlab that aims at unifying and distributing a set of tools that will allow the researcher to obtain a complete 3D model from a set of calibrated images. In this paper we motivate and present the structure of the toolbox in a tutorial and example based approach. Given its exibility and scope we believe that FIT3D represents an exciting opportunity for researchers that want to develop or test one particular method with real data without the need for extensive additional programming.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"0c38c33d9e0debe6bc4ffe130da5a41465b827a3","venue_1":"ACM Multimedia","year":"2011","title":"Interactive rich reading: enhanced book reading experience with a conversational agent","authors":"Koichi Mori, Rafael Ballagas, Glenda Revelle, Hayes Raffle, Hiroshi Horii, Mirjana Spasojevic","author_ids":"3101800, 1685500, 1752066, 3038806, 1789153, 2968608","abstract":"In this work, we introduce Interactive Rich Reading, a new enhanced book experience designed to run on smartphones and a tablet device. Interactive Rich Reading is characterized by a video-based conversational agent that asks questions or makes comments about the current page and is specifically designed to promote engagement with the contents of children's books. We use video compositing techniques to overlay the conversational agent directly over the book contents, creating a magical experience for children by bringing the book to life. We describe technical issues related to enabling this experience on mobile platforms for easier adoption of this technique by other researchers and practitioners.","cites":"9","conferencePercentile":"80.4664723"},{"venue":"ACM Multimedia","id":"a102f897e6c63eff59e1c6ae800d12fb3f7156f3","venue_1":"ACM Multimedia","year":"2011","title":"Cloud download: using cloud utilities to achieve high-quality content distribution for unpopular videos","authors":"Yan Huang, Zhenhua Li, Gang Liu, Yafei Dai","author_ids":"1752621, 3543872, 1697913, 1692303","abstract":"Video content distribution dominates the Internet traffic. The state-of-the-art techniques generally work well in distributing popular videos, but do not provide satisfactory content distribution service for unpopular videos due to low data health or low data transfer rate. In recent years, the worldwide deployment of cloud utilities provides us with a novel perspective to consider the above problem. We propose and implement the cloud download scheme, which achieves high-quality video content distribution by using cloud utilities to guarantee the data health and enhance the data transfer rate. Specifically, a user sends his video request to the cloud which subsequently downloads the video from the Internet and stores it in the cloud cache. Then the user can usually retrieve his requested video (whether popular or unpopular) from the cloud with high data rate in any place at any time, via the intra-cloud data transfer acceleration. Running logs of our real deployed commercial system (named VideoCloud) confirm the effectiveness and efficiency of cloud download. The users' average data transfer rate of unpopular videos exceeds 1.6 Mbps, and over 80% of the data transfer rates are more than 300 Kbps which is the basic playback rate of online videos. Our study provides practical experiences and valuable heuristics for making use of cloud utilities to achieve efficient Internet services.","cites":"14","conferencePercentile":"88.7755102"},{"venue":"ACM Multimedia","id":"473fcf84484f56c18d54354940fc055066642b31","venue_1":"ACM Multimedia","year":"1995","title":"Dealing with Synchronization and Timing Variability in the Playback of Interactive Session Recordings","authors":"Nelson R. Manohar, Atul Prakash","author_ids":"2560629, 1704708","abstract":"In this paper, we describe scheduling and synchronization support for a novel multimedia document, referred to as a session object. The session object captures a voice-annotated, interactive session with an application | it contains audio and window streams. This paper addresses media scheduling and synchronization issues for the support of faithful replay of session objects when subject to timing variability at the replay workstation. The replay is supported by an adaptive scheduling algorithm. The algorithm preserves relative inter-stream synchronization between window and audio streams. Run-time temporal deformations are applied over the schedule of the window stream. We show that the inter-stream asynchrony oats under statistical control as a function of the scheduling interval. The mechanisms could be generalized to the replay of streams that are subject to timing variability. Our object-oriented toolkit, ReplayKit, enables an application to become replay-aware through access to session capture and replay functionality.","cites":"15","conferencePercentile":"30"},{"venue":"ACM Multimedia","id":"05fe5942892a3508162157e2c2f59a7e651d74e7","venue_1":"ACM Multimedia","year":"1994","title":"Protocols for Integrated Audio and Shared Windows in Collaborative Systems","authors":"Amit G. Mathur, Atul Prakash","author_ids":"7405290, 1704708","abstract":"This paper describes the architecture and protocols for integrating real-time audio and shared windows in computer-supported cooperative work (CSCW) environments. Such applications require that actions on shared windows be synchronized with accompanying audio. We give a characterization of this synchronization problem and propose an architecture for handling audio-enhanced cooperative work. We present a protocol for synchronizing the audio stream and window-event streams and evaluate its performance.","cites":"5","conferencePercentile":"39.83050847"},{"venue":"ACM Multimedia","id":"8393adbbd88e3e7c164e41d8d74b56d1b4430728","venue_1":"ACM Multimedia","year":"2008","title":"Bi-layer video segmentation with foreground and background infrared illumination","authors":"Qiong Wu, Pierre Boulanger, Walter F. Bischof","author_ids":"1771305, 1798278, 1766762","abstract":"In this paper, we investigate two ways of employing infrared video with color video for automatic foreground-background video segmentation: foreground infrared (IR) illumination and background IR illumination. Foreground IR illumination gives an initial foreground template, which is combined with image segmentation to complete foreground segmentation. Two algorithms are explored, Graph Cut and Relaxation Labeling. The disadvantage of foreground IR illumination can be compensated by background illumination.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"4db900f3f911086cb6be1027caa5e8bad2f9d31c","venue_1":"ACM Multimedia","year":"2014","title":"Real-time summarization of user-generated videos based on semantic recognition","authors":"Xi Wang, Yu-Gang Jiang, Zhenhua Chai, Zichen Gu, Xinyu Du, Dong Wang","author_ids":"6172485, 1717861, 1790258, 2650085, 3343733, 1726751","abstract":"User-generated contents play an important role in the Internet video-sharing activities. Techniques for summarizing the user-generated videos (UGVs) into short representative clips are useful in many applications. This paper introduces an approach for UGV summarization based on semantic recognition. Different from other types of videos like movies or broadcasting news, where the semantic contents may vary greatly across different shots, most UGVs have only a single long shot with relatively consistent high-level semantics. Therefore, a few semantically representative segments are generally sufficient for a UGV summary, which can be selected based on the distribution of semantic recognition scores. In addition, due to the poor shooting quality of many UGVs, factors such as camera shaking and lighting condition are also considered to achieve more pleasant summaries. Experiments on over 100 UGVs with both subjective and objective evaluations show that our approach clearly outperforms several alternative methods and is highly efficient. Using a regular laptop, it can produce a summary for a 2-minute video in just 10 seconds.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"797aa634327b60efb09641b2cb3e037bdc3cb468","venue_1":"ACM Multimedia","year":"1999","title":"A novel relevance feedback technique in image retrieval","authors":"Yong Rui, Thomas S. Huang","author_ids":"1728806, 1739208","abstract":"The relevance feedback based approach to image retrieval has been an active research direction in the past few years. Many parameter estimation techniques have been proposed for relevance feedback. However, most of them are either based on ad-hoc heuristics or only partial solutions. In this paper, we introduce the first technique that not only has a solid theoretical framework but also takes into account the multi-level image content model. This technique formulates a vigorous optimization problem. By using Lagrange multipliers, we have derived the explicit optimal solutions for both the query vectors and the weights associated with the two-level image model. Experimental results on real-world image collections have shown the effectiveness and robustness of our proposed algorithm.","cites":"68","conferencePercentile":"94.95798319"},{"venue":"ACM Multimedia","id":"6df52f98ac89ef1928fb55848e32cc7ddeecbec5","venue_1":"ACM Multimedia","year":"2002","title":"Creating music videos using automatic media analysis","authors":"Jonathan Foote, Matthew L. Cooper, Andreas Girgensohn","author_ids":"1797460, 6108444, 2195286","abstract":"We present methods for automatic and semi-automatic creation of music videos, given an arbitrary audio soundtrack and source video. Significant audio changes are automatically detected; similarly, the source video is automatically segmented and analyzed for suitability based on camera motion and exposure. Video with excessive camera motion or poor contrast is penalized with a high unsuitability score, and is more likely to be discarded in the final edit. High quality video clips are then automatically selected and aligned in time with significant audio changes. Video clips are adjusted to match the audio segments by selecting the most suitable region of the desired length. Besides a fully automated solution, our system can also start with clips manually selected and ordered using a graphical interface. The video is then created by truncating the selected clips (preserving the high quality portions) to produce a video digest that is synchronized with the soundtrack music, thus enhancing the impact of both.","cites":"43","conferencePercentile":"88.46153846"},{"venue":"ACM Multimedia","id":"9dc3635bfca447a615f3e8b90309a5fe5a167bfb","venue_1":"ACM Multimedia","year":"2016","title":"Multimedia for personal health and health care","authors":"Susanne Boll, Kiyo Aizawa, Alexia Briasouli, Cathal Gurrin, Laleh Jalali, Jochen Meyer","author_ids":"1714281, 3493627, 3397434, 1737981, 1841383, 3176192","abstract":"Ever since the emergence of digitization, the term multimedia has been used to represent a combination of different kinds of media types, such images, audio, and videos. As new sensing technologies emerge and are now becoming omnipresent in daily lives, the definition, role and significance of multimedia is changing. Multimedia now represents the means for communicating, cooperating, and also for monitoring numerous aspects of daily life, at various levels of granularity and application, ranging from personal to societal. With this shift, we have since moved from comprehending single media and its state toward comprehending media in terms of its use context. Multimedia is thus no longer confined to documentation and preservation, entertainment or personal media collections; rather, it has become an integral part of the tools and systems that are providing solutions to today's societal challenges-including challenges related to health care and personal health , aging, education, societal participation, sustainable energy, and intelligent transportation. Multimedia has thus evolved into a core enabler for future interactive and cooperative applications at the heart of society. In this workshop we explore the relevance and contribution of multimedia to health care and personal media.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"5dcc79637a4d7ea952784609a39b16238c32cf38","venue_1":"ACM Multimedia","year":"2016","title":"SuperSelect: An Interactive Superpixel-Based Segmentation Method for Touch Displays","authors":"Christoph Korinke, Tim Claudius Stratmann, Tim Laue, Susanne Boll","author_ids":"3217758, 2431272, 2519223, 1714281","abstract":"We present the concept of an interactive image segmentation method, which allows a fast and precise extraction of foreground objects from natural images. The method is especially suited for mobile, touch-based devices. The approach combines automatic image segmentation with interactive refinement. In a first step, the user extracts an object using the first stage of the GrabCut algorithm. Any errors in the resulting segmentation can then be corrected via a novel correction step in which the underlying image is over-segmented into superpixels using the SLIC algorithm. The user can select individual superpixels in order to adjust the image segmentation. This image segmentation method is implemented as a full working demo on a Microsoft Surface Pro 3.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"b6c82cb8d5a09bf30991090293ab57ac81ea7431","venue_1":"ACM Multimedia","year":"2004","title":"Music artist style identification by semi-supervised learning from both lyrics and content","authors":"Tao Li, Mitsunori Ogihara","author_ids":"1726351, 1774705","abstract":"Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of identifying \"similar\" artists using both lyrics and acoustic data. The approach for using a small set of labeled samples for the seed labeling to build classifiers that improve themselves using unlabeled data is presented. This approach is tested on a data set consisting of 43 artists and 56 albums using artist similarity provided by All Music Guide. Experimental results show that using such an approach the accuracy of artist similarity classifiers can be significantly improved and that artist similarity can be efficiently identified.","cites":"17","conferencePercentile":"74.75490196"},{"venue":"ACM Multimedia","id":"b7ec9dbda6383cf793f2a40fe22b47852e1d166e","venue_1":"ACM Multimedia","year":"2005","title":"MediaMetro: browsing multimedia document collections with a 3D city metaphor","authors":"Patrick Chiu, Andreas Girgensohn, Surapong Lertsithichai, Wolfgang Polak, Frank M. Shipman","author_ids":"2895008, 2195286, 2543736, 2423252, 1749811","abstract":"The MediaMetro application provides an interactive 3D visualization of multimedia document collections using a city metaphor. The directories are mapped to city layouts using algorithms similar to treemaps. Each multimedia document is represented by a building and visual summaries of the different constituent media types are rendered onto the sides of the building. From videos, Manga storyboards with keyframe images are created and shown on the fatade; from slides and text, thumbnail images are produced and subsampled for display on the building sides. The images resemble windows on a building and can be selected for media playback. To support more facile navigation between high overviews and low detail views, a novel swooping technique was developed that combines altitude and tilt changes with zeroing in on a target.","cites":"11","conferencePercentile":"60.89108911"},{"venue":"ACM Multimedia","id":"4d2060376aa557e50dea5a5f44c10fbbf32ed78f","venue_1":"ACM Multimedia","year":"2013","title":"A tool for catching back your preferred videos from physical collages","authors":"Christoph Korinke, Mohammad Rabbath, Dennis Lamken, Susanne Boll","author_ids":"3217758, 2316024, 2650986, 1714281","abstract":"Images and videos are easy to create by inexperienced users with intuitive devices like smartphones. We propose a system which enables users to extract panoramas from videos for the creation of collages which can be printed. Single frames used for panoramas are indexed on the basis of SURF features. As a result users can make a photo with the smartphone from a panorama within the printed collage or zoom into the digital version of a collage to select an area of interest and retrieve the related video using the SURF features index. Moreover users can pick a photo from a location where a video was previously recorded and retrieve a related video as well. Our approach supports the maintenance of a large number of videos.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"4c1add6909091cc49091ebdfbff73c983eb96286","venue_1":"ACM Multimedia","year":"2010","title":"Blog2Book: transforming blogs into photo books employing aesthetic principles","authors":"Philipp Sandhaus, Mohammad Rabbath, Ilja Erbis, Susanne Boll","author_ids":"2581283, 2316024, 1923325, 1714281","abstract":"For many people web blogs are the preferred means to document important moments of their lifes, e.g. a holiday trip or the year abroad. Such blogs contain photos and textual descriptions of events in a well-structured form. However, while being a perfect means to share such important moments with friends and family, blogs remain digital and thus do not provide the valuable experience of a physical souvenir of the documented event, such as a photo book. In this paper we therefore propose a solution to combine the advantages of both web blogs and printed photo books. We bridge the gap between the digital and physical world by providing a system to automatically transform a blog into a photo book. For this we employ our system for automatic page layout following aesthetic principles. The structure of the blog is reflected in the overall layout of the photo book. We also enrich the resulting photo book with additional content from the web by employing links and location information present in the blog entries. The result is a physical counterpart to the original blog in a nice layout. Our approach is implemented as a web-based rich media application.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"ea03c9d940d67372272cce807787c9cf27a868ed","venue_1":"ACM Multimedia","year":"2004","title":"First-year students' paper chase: a mobile location-aware multimedia game","authors":"Palle Klante, Jens Krösche, Daniela Ratt, Susanne Boll","author_ids":"3256948, 3166722, 2393746, 1714281","abstract":"The latest achievements in the field of mobile networks and ubiquitous computing enable the integration and combination of technologies like Internet, Java, and multimedia in a new kind of application that employs distribution, wireless networking, localisation, and movement---mobile location-aware multimedia games. The approach presented is an innovative re-invention of a paper chase/scavenger hunt as a mobile location-aware game. The video demonstration illustrates the technical foundation and the system design of the mobile game. We also show the fun and the action of the game following some students when they play the game on our university campus.","cites":"5","conferencePercentile":"45.58823529"},{"venue":"ACM Multimedia","id":"faa086d15178bed9ef645923dac0dfddb2c14454","venue_1":"ACM Multimedia","year":"2011","title":"Why did the prime minister resign?: generation of event explanations from large news repositories","authors":"Frank Nack, Ichiro Ide","author_ids":"1679840, 1679187","abstract":"One of the common parts of news is to provide the background for a current event, such as the resignation of a Prime Minister. This paper addresses a framework that facilitates semi-automated authoring of explanatory audio-visual news topics in a retrospective style for the domain of politics based on already edited new stories available in the repository of the news corporation. The aim is to facilitate a journalist with an audio-visual body based on which he/she can finalize the explanatory piece. The proposed framework enhances current state of the art video summarization by allowing the combination of different news stories into one coherent explanation about a topic of the current news. The framework introduces techniques that exploit demoscopic data in form of polls for the development of the general story outline; the automatic retrieval of relevant material by using a combination of event templates and automatic news summarization over topic threads; and the generation of the final video by applying a set of trimming rules. Example generations are presented and discussed and an outline of future work is presented.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"0349bf4cb182b60e461de921066f199fc2a43cac","venue_1":"ACM Multimedia","year":"2000","title":"Automatically extracting highlights for TV Baseball programs","authors":"Yong Rui, Anoop Gupta, Alex Acero","author_ids":"1728806, 1725517, 1723644","abstract":"In today's fast-paced world, while the number of channels of television programming available is increasing rapidly, the time available to watch them remains the same or is decreasing. Users desire the capability to watch the programs time-shifted (on-demand) and/or to watch just the highlights to save time. In this paper we explore how to provide for the latter capability, that is the ability to extract highlights automatically, so that viewing time can be reduced.\nWe focus on the sport of baseball as our initial target&#8212;it is a very popular sport, the whole game is quite long, and the exciting portions are few. We focus on detecting highlights using audio-track features alone without relying on expensive-to-compute video-track features. We use a combination of generic sports features and baseball-specific features to obtain our results, but believe that may other sports offer the same opportunity and that the techniques presented here will apply to those sports. We present details on relative performance of various learning algorithms, and a probabilistic framework for combining multiple sources of information. We present results comparing output of our algorithms against human-selected highlights for a diverse collection of baseball games with very encouraging results.","cites":"216","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"00786983986e33ba6182517ec8f5f3693048a258","venue_1":"ACM Multimedia","year":"2004","title":"Generic support for personalized mobile multimedia tourist applications","authors":"Ansgar Scherp, Susanne Boll","author_ids":"1753135, 1714281","abstract":"Mobile applications such as mobile tourist guides that provide tourists with location-based information today mostly aim to adapt the multimedia content to the different end user devices. More and more, these applications also exploit positioning information like GPS to guide the user on the trip. What is still lacking, however, is a &#60;i>personalization&#60;/i> of the content to the interests and preferences of the individual tourist and the characteristics of the used end device. However, such a personalization increases the application's complexity since every individual alternatives have to be considered and implemented. To provide substantial support for the development of personalized (mobile) multimedia applications, we developed a domain independent software framework for an efficient and cost-effective development of personalized mobile multimedia applications. We illustrate the framework in the specific domain of personalized mobile tourist information.","cites":"10","conferencePercentile":"62.00980392"},{"venue":"ACM Multimedia","id":"a350642f50fbe5ddafd429f361fbbb4803c8d8fd","venue_1":"ACM Multimedia","year":"1999","title":"A 3D audio only interactive Web browser: using spatialization to convey hypermedia document structure","authors":"Stuart Goose, Carsten Möller","author_ids":"2385465, 8470456","abstract":"Interactive audio browsers provide both sighted and visually impaired users with access to the WWW. In addition to the desktop PC, audio browsing technology can be deployed that enable users to browse the WWW using a telephone or while driving a car. This paper describes a new conceptual model of the HTML document structure and its mapping to a 3D audio space. Novel features are discussed that provide information such as: an audio structural survey of the HTML document; accurate positional audio feedback of the source and destination anchors when traversing both inter-and intra-document links; a linguistic progress indicator; the announcement of destination document meta-information as new links are encountered. These new features can improve both the user's comprehension of the HTML document structure and their orientation within it. These factors, in turn, can improve the effectiveness of the browsing experience.","cites":"34","conferencePercentile":"84.45378151"},{"venue":"ACM Multimedia","id":"d312362dc530dce6308ac093a102ff4903394d3d","venue_1":"ACM Multimedia","year":"2016","title":"Data Aesthetics: The Ethics and Aesthetics of Big Data Gathering seen from the Artists Eye","authors":"Lucas Evers, Frank Nack","author_ids":"3493741, 1679840","abstract":"Art can reflect in depth about data (vertical) and at the same time has an aesthetical, ontological, intentional, and conscious realm (horizontal) that can communicate cross-dimensionally to the general public. In that way data can be contextualized in its being, can be made palpable and hence facilitate the communication of data into communication about data. The art exhibition of the 24th ACM MM conference will be presented at the Amsterdam Public Library (http://www.oba.nl/oba/english.html) and aims to provide art works that critically reflect on the use of data in procedural contexts. The presented art works were established in collaborations between artists and researchers that, in cases, have access to large data volumes. The presented art works make use of non-motivated and motivated functions art is valued for, as it is art in its act of resembling, expression and the presentation of the self that facilitates the representation of reality.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"a2a270e94740b1615672f3f89fcbd519b8286386","venue_1":"ACM Multimedia","year":"2011","title":"Eventscapes: visualizing events over time with emotive facets","authors":"Brett Adams, Dinh Q. Phung, Svetha Venkatesh","author_ids":"1760106, 1749657, 1679520","abstract":"The scale and dynamicity of social media, and interaction between traditional news sources and online communities, has created challenges to information retrieval approaches. Users may have no clear information need or be unable to express it in the appropriate idiom, requiring instead to be oriented in an unfamiliar domain, to explore and learn. We present a novel data-driven visualization, termed Eventscape, that combines time, visual media, mood, and controversy. Formative evaluation highlights the value of emotive facets for rapid evaluation of mixed news and social media topics, and a role for such visualizations as pre-cursors to deeper search.","cites":"10","conferencePercentile":"82.94460641"},{"venue":"ACM Multimedia","id":"1fefde6507e78ba46ddd2d1456f6d8910c447439","venue_1":"ACM Multimedia","year":"2011","title":"Cognitive intervention in autism using multimedia stimulus","authors":"Svetha Venkatesh, Stewart Greenhill, Dinh Q. Phung, Brett Adams","author_ids":"1679520, 2965462, 1749657, 1760106","abstract":"We demonstrate an open multimedia-based system for delivering early intervention therapy for autism. Using flexible multi-touch interfaces together with principled ways to access rich content and tasks, we show how a syllabus can be translated into stimulus sets for early intervention. Media stimuli are able to be presented agnostic to language and media modality due to a semantic network of concepts and relations that are fundamental to language and cognitive development, which enable stimulus complexity to be adjusted to child performance. Being open, the system is able to assemble enough media stimuli to avoid children over-learning, and is able to be customised to a specific child which aids with engagement. Computer-based delivery enables automation of session logging and reporting, a fundamental and time-consuming part of therapy.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"d0bd5a39578990629ebbda60d8986aac2c359e45","venue_1":"ACM Multimedia","year":"2000","title":"Curricula and resources for courses about multimedia (panel session)","authors":"Edward A. Fox, Wolfgang Effelsberg, Nicolas D. Georganas, Rachelle S. Heller, Ralf Steinmetz","author_ids":"1705950, 1750165, 1723057, 2436280, 1725298","abstract":"This panel will discuss a recommendation for curricula guidelines for courses about multimedia. Based on conference input, the guidelines will be modified for review prior to future publication by ACM (with other groups). The recommendation will be one of the main results of a June 2000 Dagstuhl workshop led by 4 of the panelists. Dr. Heller, the 5<sup>th</sup> panelist, also attended that workshop, and is co-PI with the panel moderator on a directly related NSF project, Curriculum Resources in Interactive Multimedia, CRIM, see http://ei.cs.vt.edu/~crim. Each of the panelists has taught courses about multimedia. Each is interested in developing tools/demonstrations/resources to help in those course. Each has a particular area of interest in the multimedia field. Together they constitute a representative group among those who will contribute to and benefit from curricula resources in multimedia.","cites":"0","conferencePercentile":"7.065217391"},{"venue":"ACM Multimedia","id":"fe532862ef62071e6da2c4ddfc02798fd78d407f","venue_1":"ACM Multimedia","year":"2007","title":"Automatic classification of didactic functions of e-learning resources","authors":"Marek Meyer, Alexander Hannappel, Christoph Rensing, Ralf Steinmetz","author_ids":"2342665, 2534285, 1713844, 1725298","abstract":"Re-use of digital resources is an important issue in e-Learning scenarios, because only intensive re-use can make e-Learning cost efficient. Besides reusing whole courses, authors often desire to re-use fine grained parts of courses for creating new Learning Resources. The granularity which appears to be most promising for this kind of re-use is the level of information objects. Information objects each have a dedicated didactic function; a set of information objects with different didactic functions are combined into Learning Objects. This paper analyzes how didactic functions of existing information objects can be automatically classified using machine learning technology. The results of such classification methods on a set of Learning Resources from medical science are discussed.","cites":"4","conferencePercentile":"40.36458333"},{"venue":"ACM Multimedia","id":"43a9a158f7c0780b90bf049332351360024c086b","venue_1":"ACM Multimedia","year":"2010","title":"Serious games for health: personalized exergames","authors":"Stefan Göbel, Sandro Hardy, Viktor Wendel, Florian Mehm, Ralf Steinmetz","author_ids":"1829765, 2522082, 1836113, 2399911, 1725298","abstract":"In this paper, we describe a set of personalized exergames which combine methods and concepts of serious games, adaptation and personalization, authoring and sensor technologies. Compared to existing systems, the set of games does not only keep track of the user's vital state, but also directly integrates vital parameters into the gameplay and supports the training and motivation for sustainable physical activity in a playful manner.","cites":"33","conferencePercentile":"93.56164384"},{"venue":"ACM Multimedia","id":"0aa4768fb25d9c08ce453443eabf4a338bfad07a","venue_1":"ACM Multimedia","year":"2011","title":"Collaborative authoring of serious games for health","authors":"Florian Mehm, Sandro Hardy, Stefan Göbel, Ralf Steinmetz","author_ids":"2399911, 2522082, 1829765, 1725298","abstract":"The efficient production of Serious Games typically requires the collaboration of technical and game development experts, i.e. game developers and domain experts such as pedagogues or sports experts. For the use case of exergames with educational aspects, we demonstrate how an authoring tool for Serious Games can be specialized for collaborative authoring by defining roles of users and providing different views on the created game to each user group carrying out different tasks in the production of the game.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"37e54c4211ca71af425adb9a017caf9f87b6bb01","venue_1":"ACM Multimedia","year":"2013","title":"Adaptable and personalized game-based training system for fall prevention","authors":"Sandro Hardy, Stefan Göbel, Ralf Steinmetz","author_ids":"2522082, 1829765, 1725298","abstract":"Digital Games which incorporate movements of the player`s body in their gameplay are becoming more and more popular. An increasing number of doctors and physical therapists use such games for training exercises, although these games are not designed to achieve predefined training goals. Various studies show that the training effects of these games are small in comparison with classic exercises. Therapists request more accessible and more flexible games. In this paper we present an adaptive game for fall prevention based on the adaptation and exergame analysis framework StoryTecRT which allows the adaptation of parameters which impact accessibility, acceptance and training load of a game. This paper includes an insight into the framework and the implementation as well as first evaluation results.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"cbff56026ae67a1a5ad8258c9a6a6f8f00dbecac","venue_1":"ACM Multimedia","year":"2000","title":"Rapid modeling of animated faces from video images","authors":"Zicheng Liu, Zhengyou Zhang, Chuck Jacobs, Michael F. Cohen","author_ids":"1691128, 1732465, 2671772, 1694613","abstract":"Generating realistic 3D human face models and facial animations has been a persistent challenge in computer graphics. We have developed a system that constructs textured 3D face models from videos with minimal user interaction. Our system takes images and video sequences of a face with an ordinary video camera. After five manual clicks on two images to tell the system where the eye corners, nose top and mouth corners are, the system automatically generates a realistic looking 3D human head model and the constructed model can be animated immediately. A user, with a PC and an ordinary camera, can use our system to generate his/her face model in a few minutes. We will demonstrate the system at the conference.","cites":"3","conferencePercentile":"32.60869565"},{"venue":"ACM Multimedia","id":"2bf37dd0ebaac003e67d11cc869e5e93b13dbd00","venue_1":"ACM Multimedia","year":"2015","title":"Opportunities and Challenges of Industry-Academic Collaborations in Multimedia Research","authors":"Shih-Fu Chang, Matthew Cooper, Denver Dash, Funda Kivran-Swaine, Jia Li, David A. Shamma","author_ids":"1735547, 4268667, 8515079, 3060424, 1687366, 1760364","abstract":"This ACM MM panel aims to redefine the state of research between Academia and Industry.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"a118bb386a0062e56018cf1437ecad060fd690c3","venue_1":"ACM Multimedia","year":"2003","title":"Excuse me, but are you human?","authors":"Yong Rui, Zicheng Liu","author_ids":"1728806, 1691128","abstract":"Web services designed for human users are being abused by computer programs (bots). The bots steal thousands of free email accounts in a minute; participate in online polls to skew results; and irritate people by joining online chat rooms. These real-world issues have recently generated a new research area called Human Interactive Proofs (HIP), whose goal is to defend services from malicious attacks by differentiating bots from human users. We propose a new HIP algorithm based on detecting human face and facial features. Human faces are the most familiar object to humans, rendering it possibly the best candidate for HIP. We conducted user studies and showed the ease of use of our system to human users. We designed attacks using the best existing face detectors and demonstrated the difficulty to bots.","cites":"6","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"7a0c3c5677ec5134ba3ae4faf2ae2998ede6984b","venue_1":"ACM Multimedia","year":"2008","title":"Requirements and recommendations for an enhanced meeting viewing experience","authors":"Sasa Junuzovic, Rajesh Hegde, Zhengyou Zhang, Philip A. Chou, Zicheng Liu, Cha Zhang","author_ids":"2999736, 6466598, 1732465, 1720816, 1691128, 1706673","abstract":"We have found that viewing recorded meetings using traditional meeting viewers whose interfaces consist of an automatic speaker and a fixed context view does not provide sufficient information and control to the users. In particular, a survey of users who watch meeting recordings on a regular basis revealed that it is also useful to provide (1) speaker-related information, including who the speaker is talking to, looking at, and being interrupted by, and (2) more control of the interface, including changing the relative sizes of the speaker and context views and navigating within the context view. We present a 3D interface prototype designed specifically to meet these requirements when viewing recorded meetings. We describe in detail the results of a user study comparing the effectiveness of the new and traditional style interfaces with respect to these requirements. Based on this study, we present a set of guidelines for future interfaces.","cites":"10","conferencePercentile":"68.34862385"},{"venue":"ACM Multimedia","id":"10720fa474b1cc7dfb95df3c2b5210ae6095179d","venue_1":"ACM Multimedia","year":"2011","title":"Real-time human action search using random forest based hough voting","authors":"Gang Yu, Junsong Yuan, Zicheng Liu","author_ids":"5605714, 1746449, 1691128","abstract":"Many existing techniques in content based video retrieval treat a video sequence as a whole to match it against a query video or to assign a text label. Such an approach has serious limitations when applied to human action retrieval because an action may occur only in a sub-region and last for a small portion of the video length. In situations like this, we essentially need to match the subvolumes of the video sequences against the query video. A naive exhaustive search is impractical due to large number of possible subvolumes for each video sequence. In this paper, we propose a novel framework for action retrieval which performs pattern matching at subvolume level and is very efficient in handling large corpus of videos. We construct an unsupervised random forest to index the video database, generate a score volume with Hough voting and then employ a max sub-path strategy to quickly search for the temporal and spatial positions of all the video sequences in the database. We present action search experiments on challenging datasets to validate the efficiency and effectiveness of our system.","cites":"9","conferencePercentile":"80.4664723"},{"venue":"ACM Multimedia","id":"3cac9ea58db2288eb267bb65034dcece9e998be8","venue_1":"ACM Multimedia","year":"2012","title":"Predicting human activities using spatio-temporal structure of interest points","authors":"Gang Yu, Junsong Yuan, Zicheng Liu","author_ids":"5605714, 1746449, 1691128","abstract":"Early recognition and prediction of human activities are of great importance in video surveillance, e.g., by recognizing a criminal activity at its beginning stage, it is possible to avoid unfortunate outcomes. We address early activity recognition by developing a Spatial-Temporal Implicit Shape Model (STISM), which characterizes the space-time structure of the sparse local features extracted from a video. The early recognition of human activities is accomplished by pattern matching through STISM. To enable efficient and robust matching, we propose a new random forest structure, called multi-class balanced random forest, which makes a good trade-off between the balance of the trees and the discriminative abilities. The prediction is done simultaneously for multiple classes, which saves both the memory and computational cost. The experiments show that our algorithm significantly outperforms the state of the arts for the human activity prediction problem.","cites":"12","conferencePercentile":"88.76582278"},{"venue":"ACM Multimedia","id":"84b454bd1dda40eb5c427e7f536ed362c9e5884d","venue_1":"ACM Multimedia","year":"2005","title":"Playas: homeland mirage","authors":"Jack Stenner, Andruid Kerne, Yauger Williams","author_ids":"2236655, 1694380, 1829226","abstract":"This paper describes an interactive installation that addresses issues of presence and absence by creating a virtualized representation of the abandoned town, Playas, New Mexico. This town is slated for conversion into an anti-terrorism training facility by New Mexico Tech University in conjunction with the United States Department of Homeland Security. Using the metaphor of the mirage, the work functions as a critique of our understanding of \"reality.","cites":"1","conferencePercentile":"14.10891089"},{"venue":"ACM Multimedia","id":"65b75fe7be12e6904667f6a22cd034d1fd005358","venue_1":"ACM Multimedia","year":"2005","title":"Censor chair: exploring censorship and social presence through psychophysiological sensing","authors":"Eric Aley, Trina Cooper, Ross Graeber, Andruid Kerne, Kyle Overby, Zachary O. Toups","author_ids":"2169419, 1867594, 2487369, 1694380, 1945162, 1720900","abstract":"In this paper, we describe <i>Censor Chair</i>, an art installation that creates a shared experience addressing forms of censorship including self-censorship, censorship of a group upon an individual, visual and auditory censorship in digital media, and censorship in society. We are taking a playful position in considering relationships between censorship and sensors that monitor physiology. <i>Censor Chair</i> makes use of a galvanic skin response (GSR) sensor, live video feeds, and a barcode reader to drive the presentation of a digital media library.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"020dfd1bcca603b8b3d59b64a4b0d756fc57591b","venue_1":"ACM Multimedia","year":"1996","title":"Enhancing Network Services through Multimedia Data Analysers","authors":"Ferdinando Samaria, Harold Syfrig, Alan Jones, Andy Hopper","author_ids":"2782553, 3178479, 6663515, 1769174","abstract":"This paper summarises our experience of using data analysers to enhance network multimedia services. By analysers we mean simple processing modules that extract information contained in various multimedia streams. They are categorised based on their location with respect to the network, the location being determined by balancing bandwidth requirements and computational complexity. Various applications are described where analysers are used to enhance aspects of the service provided. Details of the multimedia environment are given, followed by an overview of the analyser architecture and examples of analyser-enhanced applications. The paper is concluded by indicating directions of future development.","cites":"4","conferencePercentile":"8.333333333"},{"venue":"ACM Multimedia","id":"d2db32935e8ef96d29fc9b86840cfd2ca08994fd","venue_1":"ACM Multimedia","year":"1998","title":"Preparing for the Digital Media Monsoons","authors":"Andy Hopper","author_ids":"1769174","abstract":"Digiti media are Wady pushing their way hto more and more aspects of our personal ad corporate Eves. Audio CDs are we~ entrench~ digiti-eras, for boths~ and moviw, are beginning to displace their mdo~e cousins; DWS are s-g to app-9 digiti teletilon is on the immediate horkon; and digiti mass storage continues to become cheaper, faster, and more plenti by leaps md bounds. ~ese trends done wodd soon force us to dd with the problem of mana~g largq diverse co~ections of digiti mdthnedia objects, but StititiWUS advanc~ in sentient computing and broadband wirelws networtig codd tie the problem to an entirely different level. Effective indefig, annotation, and retrieval of dgiti asse6 are set to become some of the preeminent cha~enges in computing. OM md the University of Cambridge have long been at the forefront of research in the mess of wired and wireless networtig, mtitirnedi~ information retrieval, and sentient computing. k this * I W present =pects of our recent research in th~e areas md &cuss how they might combine to shape tie future role of digiti media in the home =d wor&lac~ 1.1 Keywords hfultimdl% digiti assets, information retrieval, wireless nefivorks, sentient computing Permission10make digitd or hard copies of Al Orpart of IMi v,ork for personal or classroom ose ~ g~~ed ~lthout fee prot,ided lhal copies are not made or distributed for profit or ammertial adt'anmge,and that copies bear th~ notim and the full titation on the firs: page. To mpy othewk% 10repub~sk to post on sewen or lo redistribute to fists, requir= prior specific-. 2.-ODUC~ON me world is going digiti. Digiti media are gradudly replacing tieir anrdogue counterparts, and tiIs replacement spans eve~lng born personal photograph and home movie collations to pre-recorded and home-rworded teletilon and movie archives. Furthermore, the transition to digiti media is enabling applications without existing anrdogue counte~arts-video mail, for example. As these trends continue, the ability to organize and search through very large coll~tions of digiti media assets will bwome incraingly important And the world is going wirelas. Radio-based networks are becoming faster and cheaper and this trend is set to continue. men high-bandwidth wireless connativity becomes the norm, the volume of digiti media generated by an individud W no longer be limited by the amount of z non-volatile storage he or she can physically carry, but will be essentidy limitiess. h this world, the current fanciful proposals for video-and audio-based \" corporate memories …","cites":"0","conferencePercentile":"2.884615385"},{"venue":"ACM Multimedia","id":"8a702a71b6023651492e12ba85cb4162e4f25052","venue_1":"ACM Multimedia","year":"2003","title":"Human + agent: creating recombinant information","authors":"Andruid Kerne, Vikram Sundaram, Jin Wang, Madhur Khandelwal, J. Michael Mistrot","author_ids":"1694380, 2832449, 7138241, 2763850, 2131503","abstract":"combinFormation is a tool that enables browsing and collecting information elements in a generative space. By generative, we mean that the tool is an agent that automatically retrieves information elements and visually composes them. A combinFormation session presents a dynamic, evolving recombination of information elements from different sources. The elements are manipulable in the information space. Recombination is the process of taking previously unconnected elements, and combining them to create new configurations.One purpose of this space is to support the formation of ideas, through more and less focused processes of foraging. While ideas are forming, the criteria that underlie information foraging activities may not be well defined. Collecting the specific subset of related information elements is challenging. Cognitive scientists have established that combinations of images and textual elements are examples of preinventive structures that can lead to the emergence of new ideas. These preinventive structures often combine existing representations.Our program generates recombinant visualizations that develop interrelationships between the information elements. The generative visualization is based on a procedural model of the information, and the user's interests. The user model reflects interactions in which s/he explicitly expresses interest. The agent retrieves information based on the evolving model. The visual composition is also developed to emphasize the user's evolving sense of what is important. This involves solving problems in the dynamic visualization of dynamic, heterogeneous collections. In our novel interaction model, the human being shares control of the evolving information space with the agent. The user can express interest in information elements as they stream in, and design the visual space, using interactive tools. Expressions feed back through the model to drive the program's retrieval and visual composition decisions.","cites":"0","conferencePercentile":"4.054054054"},{"venue":"ACM Multimedia","id":"4083c9d267fea613a9682cc721e3c243a9fa06ae","venue_1":"ACM Multimedia","year":"2001","title":"MPEG-L/MRP: implementing adaptive streaming of MPEG videos for interactive internet applications","authors":"Susanne Boll, Wolfgang Klas, Michael Menth, Christian Heinlein","author_ids":"1714281, 1719183, 1759515, 2197283","abstract":"Existing multimedia streaming technologies offer no specific support for user interaction like jumping to bookmarks in a video, or switching to reverse play. When the users, e.g., jump to a bookmark, the player requests frames for the new presentation point from the server and resumes playing only when the data has arrived. Our solution for this problem is the client prefetching the buffering strategy, <i>MPEG-L/MRP</i>, that ensures that the frames which are needed for a response to a possible user interaction are already in the client's buffer, which leads to qucik and smooth reaction to user interactions. In case of variable bandwidth, our MPEG-1 streaming approach selects only a subset of all frames to be fetched from the server and supports a smooth presentation at a reduced frame rate with correct timelines.The technical demonstration shows the interactive streaming of MPEG-videos and illustrates our buffering and prefetching strategy.","cites":"1","conferencePercentile":"15.30612245"},{"venue":"ACM Multimedia","id":"0ff30910fbd50dad9dd54bf076f16238544e1733","venue_1":"ACM Multimedia","year":"1999","title":"A cross-media adaptation strategy for multimedia presentations","authors":"Susanne Boll, Wolfgang Klas, Jochen Wandel","author_ids":"1714281, 1719183, 2345344","abstract":"Adaptation techniques for multimedia presentations are mainly concerned with switching between different qualities of single media elements to reduce the data volume and by this to adapt to limited presentation resources. This kind of adaptation, however, is limited to an inherent lower bound, i.e., the lowest acceptable technical quality of the respective media type. To overcome this limitation, we propose <italic>cross-media</italic> adaptation in which the presentation alternatives can be media elements of different media type, even different fragments. Thereby, the alternatives can extremely vary in media type and data volume and this enormously widens the possibilities to efficiently adapt to the current presentation resources. However, the adapted presentation must still convey the same content as the original one, hence, the substitution of media elements and fragments must preserve the presentation semantics. Therefore, our cross-media adaptation strategy provides models for the automatic <italic>augmentation</italic> of multimedia documents by semantically equivalent presentation alternatives. Additionally, during presentation, <italic>substitution</italic> models enforce a semantically correct information flow in case of dynamic adaptation to varying presentation resources. The cross-media adaptation strategy allows for flexible reuse of multimedia content in many different environments and, at the same time, maintains a semantically correct information flow of the presentation.","cites":"53","conferencePercentile":"90.75630252"},{"venue":"ACM Multimedia","id":"2301d5aa7f47fef0d23ad0323e2ac1a72cbbf21d","venue_1":"ACM Multimedia","year":"2009","title":"Design and deployment of a hybrid CDN-P2P system for live video streaming: experiences with LiveSky","authors":"Hao Yin, Xuening Liu, Tongyu Zhan, Vyas Sekar, Feng Qiu, Chuang Lin, Hui Zhang, Bo Li","author_ids":"8542346, 8016832, 3318598, 1732751, 1734833, 1735376, 1732258, 1716576","abstract":"We present our design and deployment experiences with LiveSky, a commercially deployed hybrid CDN-P2P live streaming system. CDNs and P2P systems are the common techniques used for live streaming, each having its own set of advantages and disadvantages. LiveSky inherits the best of both worlds: the quality control and reliability of a CDN and the inherent scalability of a P2P system. We address several key challenges in the system design and implementation including (a) dynamic resource scaling while guaranteeing stream quality, (b) providing low startup latency, (c) ease of integration with existing CDN infrastructure, and (d) ensuring network-friendliness and upload fairness in the P2P operation. LiveSky has been commercially deployed and used for several large-scale live streaming events serving more than ten million users in China. We evaluate the performance of LiveSky using data from these real-world deployments. Our results indicate that such a hybrid CDN-P2P system provides quality and user performance comparable to a CDN and effectively scales the system capacity when the user volume exceeds the CDN capacity.","cites":"70","conferencePercentile":"98.14049587"},{"venue":"ACM Multimedia","id":"085e44d4213ea04de40d9af18264c5f0fcdc496e","venue_1":"ACM Multimedia","year":"2015","title":"Ubii: Towards Seamless Interaction between Digital and Physical Worlds","authors":"Zhanpeng Huang, Weikai Li, Pan Hui","author_ids":"2969693, 2255625, 2665582","abstract":"We present Ubii (Ubiquitous interface and interaction), an interface system that aims to expand people's perception and interaction from the digital space to the physical world. The centralized user interface is broken into pieces woven in the domain environment. Augmented user interface is paired to the physical objects, where physical and digital presentations are displayed in the same context. The augmented interface and physical affordance respond as one control to provide seamless interaction. By connecting digital interface with physical objects, the system presents a nearby embodiment to afford users sense of awareness to interact with domain objects. Integrated on wearable devices as Google Glass, a less intrusive and more convenient interaction is afforded. Our research illustrates the great potential of direct mapping of interaction between digital interfaces and physical affordance by converging wearable devices and augmented reality (AR) technology.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"0fde1108b730fe123af00156fa17cfb06b847e0c","venue_1":"ACM Multimedia","year":"2003","title":"Colour picking: the pecking prder of form and function","authors":"Frank Nack, Amit Manniesing, Lynda Hardman","author_ids":"1679840, 2260721, 1701116","abstract":"Multimedia presentation generation has to be able to balance the functional aspects of a presentation that address the information needs of the user and its aesthetic form. We demonstrate our approach using automatic colour design for which we integrate relevant aspects of colour theory. We do not provide a definition of the relative importance of form versus function, but seek to explore the roles of subjective elements in the generation process.","cites":"1","conferencePercentile":"11.26126126"},{"venue":"ACM Multimedia","id":"2d1260c466d75dacf6f7fc264749acec562bc0e6","venue_1":"ACM Multimedia","year":"2013","title":"Query-dependent visual dictionary adaptation for image reranking","authors":"Jialong Wang, Cheng Deng, Wei Liu, Rongrong Ji, Xiangyu Chen, Xinbo Gao","author_ids":"2193940, 1715156, 3406279, 1725599, 2572474, 1717818","abstract":"Although text-based image search engines are popular for ranking images of user's interest, the state-of-the-art ranking performance is still far from satisfactory. One major issue comes from the visual similarity metric used in the ranking operation, which depends solely on visual features. To tackle this issue, one feasible method is to incorporate semantic concepts, also known as image attributes, into image ranking. However, the optimal combination of visual features and image attributes remains unknown. In this paper, we propose a query-dependent image reranking approach by leveraging the higher level attribute detection among the top returned images to adapt the dictionary built over the visual features to a query-specific fashion. We start from offline learning transposition probabilities between visual codewords and attributes, then utilize the probabilities to online adapt the dictionary, and finally produce a query-dependent and semantics-induced metric for image ranking. Extensive evaluations on several benchmark image datasets demonstrate the effectiveness and efficiency of the proposed approach in comparison with state-of-the-arts.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"c4585a02acc9b53fef8b8e20cda772e94d5a08b4","venue_1":"ACM Multimedia","year":"2014","title":"Anahita: A System for 3D Video Streaming with Depth Customization","authors":"Kiana Calagari, Krzysztof Templin, Tarek Elgamal, Khaled M. Diab, Piotr Didyk, Wojciech Matusik, Mohamed Hefeeda","author_ids":"2397259, 2668907, 1940000, 2455396, 3307078, 1752521, 1711116","abstract":"Producing high-quality stereoscopic 3D content requires significantly more effort than preparing regular video footage. In order to assure good depth perception and visual comfort, 3D videos need to be carefully adjusted to specific viewing conditions before they are shown to viewers. While most stereoscopic 3D content is designed for viewing in movie theaters, where viewing conditions do not vary significantly, adapting the same content for viewing on home TV-sets, desktop displays, laptops, and mobile devices requires additional adjustments. To address this challenge, we propose a new system for 3D video streaming that provides automatic depth adjustments as one of its key features. Our system takes into account both the content and the display type in order to customize 3D videos and maximize their perceived quality. We propose a novel method for depth adjustment that is well-suited for videos of field sports such as soccer, football, and tennis. Our method is computationally efficient and it does not introduce any visual artifacts. We have implemented our 3D streaming system and conducted two user studies, which show: (i) adapting stereoscopic 3D videos for different displays is beneficial, and (ii) our proposed system can achieve up to 35% improvement in the perceived quality of the stereoscopic 3D content.","cites":"8","conferencePercentile":"87.34939759"},{"venue":"ACM Multimedia","id":"6bd80d52eecb5c44e1244b1afbac140a2c9adf44","venue_1":"ACM Multimedia","year":"1996","title":"A Quality Planning Model for Distributed Multimedia in the Virtual Cockpit","authors":"Mark Claypool, John Riedl","author_ids":"1905185, 8497382","abstract":"Tomorrow's multimedia applications will stress all parts of a computer system. To determine the computer resources needed to meet application demands we have developed a new capacity planning model that is based on application quality as perceived by the user. We have applied our model to a Distributed Interactive Simulation flight simulator called the Viiual Cockpit. We investigate the quality of the Vkttral Cockpit on existing networks and processors and predict the effects of high-speed networks and high-performance processors on Virtual Cockpit quality. We find processor performance is the current bottleneck in application quality for the Virtual Cockpit, but that higher-speed networks, such as ATM, will be needed to meet network requirements after two to three generations of processor improvement.","cites":"7","conferencePercentile":"13.88888889"},{"venue":"ACM Multimedia","id":"5d262897142815fc15f9b1ce95f72d9811f35b7b","venue_1":"ACM Multimedia","year":"1994","title":"Synchronization of Distributed Multimedia Data in an Application-Specific Manner","authors":"Nipun Agarwal, Sang Hyuk Son","author_ids":"2109926, 1806873","abstract":"One of the distinctive features of multimedia systems is the wide range of applications they intend to cover, stretching the gamut from entertainment to life-critical applications such as real-time remote surgery. In the face of such a wide spectrum of applications, protocols used to deal with various issues in multimedia systems should be adaptable to the application. Synchronization is one of the key characteristics of a multimedia system. In this paper, we propose a mechanism for synchronization of distributed media data, the accuracy of which can be tailored to the intended application. The model is extended for both stored and live multimedia data. The synchronization mechanism we propose ensures synchronization for all possible temporal relations. The performance of our scheme is evaluated and some implementation issues are discussed.","cites":"6","conferencePercentile":"43.22033898"},{"venue":"ACM Multimedia","id":"a3d2d934b4e21cc86682c424e3c3aa7111b61f11","venue_1":"ACM Multimedia","year":"2007","title":"Display pre-filtering for multi-view video compression","authors":"Matthias Zwicker, Sehoon Yea, Anthony Vetro, Clifton Forlines, Wojciech Matusik, Hanspeter Pfister","author_ids":"1796846, 2147785, 7810517, 1694854, 1752521, 1701371","abstract":"Multi-view 3D displays are preferable to other stereoscopic display technologies because they provide autostereoscopic viewing from any viewpoint without special glasses. However, they require a large number of pixels to achieve high image quality. Therefore, data compression is a major issue for this approach. In this paper, we present a framework for efficient compression of multi-view video streams for multi-view 3D displays. Our goal is to optimize image quality without increasing the required data bandwidth. We achieve this by taking into account a precise notion of the multi-dimensional display bandwidth. The display bandwidth implies that scene elements that appear at a given distance from the display become increasingly blurry as the distance grows. Our main contribution is to enhance conventional multi-view compression pipelines with an additional pre-filtering step that bandlimits the multi-view signal to the display bandwidth. This imposes a shallow depth of field on the input images, thereby removing high frequency content. We show that this pre-filtering step leads to increased image quality compared to state-of-the-art multi-view coding at equal bitrate. We present results of an extensive user study that corroborate the benefits of our approach. Our work suggests that display pre-filtering will be a fundamental component in signal processing for 3D displays, and that any multi-view compression scheme will benefit from our pre-filtering technique.","cites":"7","conferencePercentile":"53.90625"},{"venue":"ACM Multimedia","id":"cff5d7201adba72c4c33822b9b5995c288e1a12e","venue_1":"ACM Multimedia","year":"1994","title":"Artists in Multimedia: Creating Meaningful Roles (Panel)","authors":"Rich Gold, Char Davies, Michael Naimark, Mark Petrakis, Stephen Wilson, Sara Roberts","author_ids":"2005439, 3188255, 2946428, 2172395, 6743981, 2583688","abstract":"There was a time not too long ago when artists in the software businessconsisted of people who drew pretty pictures or composed nice music at the tail end of the production process. Their participation was rarely invited in the process of design, and in order to make even cosmetic contributions in implementation they had to learn, master, and repttrpose perversely arcane tools. In some regions, companies, and segments of the intftts-try, artists are still marginalized in these ways, This panel will look at several artists-each with a unique role in multimedia-who are making contributions and differences of an entirely different order: defining and directing research, inventing new media forms and structures, designing tools, and feeding technological innovation back into the performing arts. We will explore the topic of how artists can create and assume meaningful new roles in relation to the business of multimedia. Rich Gold is a composer who co-founded the League of Automatic Music Composers, the first network computer band. As an internationally known artist he invented the field of Algo-rithmic Symbolism, an example of which, The Party Planner, was featured in Scientific American, He was head of the sound and music department of Sega USA's coin-op video game division and the inventor of the award winning \" Lhtle Computer People \" (Activision), the first fully autonomous computerized person you could buy. For five years he headed the electronic and computer toy research area at Mattel Toys and was the manager of, among other interactive gizmos, the Mattel Power Glove. After working as a consultant in vimsal reality he joined Xerox PARC, where he is now a researcher working on Ubiquitous Computing, the director of PAIR (an artist in residence program that brings in artists to work one on one with researchers), tiny handheld devices and the philosophy of stuff. Char Davies was born in Toronto, Canada. She studied at Ben-nington College (Vermont) and received a Bachelor of Fhte Arts from the University of Victoria (British Columbia) in 1978. For the next decade she pursued a career in painting and worked as a freelance filmmaker with the National Film Board of Canada. In the mid-80s, she became interested in 3D computer technology as a means of creating in virtual space; to this end she joined SOFTfMAGE in 1987 when the company was still in its infancy. Char was instrumental in building SOITIMAGE into one of the world's leading …","cites":"0","conferencePercentile":"12.71186441"},{"venue":"ACM Multimedia","id":"ff79608187bd28436a98305d7ffa97fc98cb13f8","venue_1":"ACM Multimedia","year":"2005","title":"Toward emergent representations for video","authors":"Ryan Shaw, Marc Davis","author_ids":"2723628, 1777964","abstract":"Advanced systems for finding, using, sharing, and remixing video require high-level representations of video content. A number of researchers have taken top-down, analytic approaches to the specification of representation structures for video. The resulting schemes, while showing the potential of high-level representations for aiding the retrieval and resequencing of video, have generally proved too complex for mainstream use. In this paper, we propose a bottom-up, emergent approach to developing video representation structures by examining retrieval requests and annotations made by a community of video remixers. Our initial research has found a useful degree of convergence between user-generated indexing terms and query terms, with the salient exception of descriptions of characters' corporeal characteristics.","cites":"11","conferencePercentile":"60.89108911"},{"venue":"ACM Multimedia","id":"e2c45bd5ad6890899bcb36e47c458b1b2f5b10d3","venue_1":"ACM Multimedia","year":"2002","title":"Media semantics: who needs it and why?","authors":"Chitra Dorai, Andreas Mauthe, Frank Nack, Lloyd Rutledge, Thomas Sikora, Herbert Zettl","author_ids":"1719234, 1749658, 1679840, 1696744, 1733384, 2607489","abstract":"Introduction As pointed out in the keynote address at the 2001 ACM Multime-dia Conference [3], the current major goal of multimedia research is directed towards provisioning information for pervasive access and use. To achieve this, what will become important are technologies that help sift useful nuggets of information from torrents of media data, which can be turned into valuable knowledge just in time and need, and tools that help provide access to these nuggets in anytime-anywhere-any device mode to everyone ranging from enterprise customers to independent consumers. Further we need to treat various media on an equal basis in environments that provide multimedia-based interactions, where they ultimately add value to users, whatever the nature of the interactions may be and whatever the preferred mode of media access may be. Thus, there is a fundamental need to investigate the means to elucidate, subli-mate, or rationalize information and knowledge from media data. However, current user expectations are far from being met owing to generic low-level content metadata available from automated processing that deal only with representing perceived content, and not the semantics of it. The challenges in modeling and extracting media-intrinsic as well as extrinsic semantics are as complicated as the attendant problem of matching them with user needs in various domains in which we anticipate robust media usage [4]. Moreover, once information is gathered from various repositories in a federated fashion, we also need mechanisms to automatically process the disparate data for generating visually pleasing media presentations, and for need-oriented and device-appropriate media playback. The tools for designing and developing such technologies are in their infancy and their development depends very much on a better understanding of user requirements, domain needs, objective measurements of media items as well as subjective interpretations. One promising approach at bridging the semantic gap and building high-level semantic descriptions for reliable content location, access, and navigation services is founded on an understanding of media elements and their roles in synthesizing meaning, manipulating perceptions, and crafting messages, with a systematic study of media productions [1]. The two broadest attempts for a standardized description of content, the Semantic Web from the W3C [5], and MPEG-7 from ISO [2], also provide some insight into these problems but only briefly touch on the problem of underlying semantics needs in different domains. The goal of this panel is to explore, discuss and come to a better understanding of the following issues: …","cites":"11","conferencePercentile":"56.83760684"},{"venue":"ACM Multimedia","id":"0b2aa5c1a2d1a5bb6c545874f93a3d3c085395d6","venue_1":"ACM Multimedia","year":"2016","title":"Contextual Enrichment of Remote-Sensed Events with Social Media Streams","authors":"Benjamin Bischke, Damian Borth, Christian Schulze, Andreas Dengel","author_ids":"3457221, 1772549, 2096479, 1703343","abstract":"The availability of satellite images for academic or commercial purpose is increasing rapidly due to efforts made by governmental agencies (NASA, ESA) to publish such data openly or commercial startups (PlanetLabs) to provide real-time satellite data. Beyond many commercial application, satellite data is helpful to create situation awareness in disaster recovery and emergency situations such as wildfires, earthquakes, or flooding. To fully utilize such data sources, we present a scalable system for the contextual enrichment of satellite images by crawling and analyzing multimedia content from social media. This information stream can provide vital information from the ground and help to complement remote sensing in situations. We use Twitter as main data source and analyze its textual, visual, temporal, geographical and social dimensions. Visualizations show different aspects of the event allowing high-level comprehension and provide deeper insights into the event as complemented by social media.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"85117e2a2fa9267c54a5e0017c9c5b78f87d7fa3","venue_1":"ACM Multimedia","year":"2016","title":"High-speed Depth Stream Generation from a Hybrid Camera","authors":"Xinxin Zuo, Sen Wang, Jiangbin Zheng, Ruigang Yang","author_ids":"2407738, 6646390, 3104013, 1682766","abstract":"High-speed video has been commonly adopted in consumer-grade cameras, augmenting these videos with a corresponding depth stream will enable new multimedia applications, such as 3D slow-motion video. In this paper, we present a hybrid camera system that combines a high-speed color camera with a depth sensor, e.g. Kinect depth sensor, to generate a depth stream that can produce both high-speed and high-resolution RGB+depth stream. Simply interpolating the low-speed depth frames is not satisfactory, where interpolation artifacts and lose in surface details are often visible. We have developed a novel framework that utilizes both shading constraints within each frame and optical flow constraints between neighboring frames. More specifically we present (a) an effective method to find the intrinsics images to allow more accurate normal estimation; and (b) an optimization-based framework to estimate the high-resolution/high-speed depth stream, taking into consideration temporal smoothness and shading/depth consistency. We evaluated our holistic framework with both synthetic and real sequences, it showed superior performance than previous state-of-the-art.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"3dbda087b8d3ef1311329ea1dc740f7c2ebe56a6","venue_1":"ACM Multimedia","year":"2005","title":"PhotoRouter: destination-centric mobile media messaging","authors":"Shane Ahern, Simon King, Hong Qu, Marc Davis","author_ids":"2796251, 1721016, 2624067, 1777964","abstract":"The number of people using cameraphones is growing by tens of millions every month. Yet the majority of cameraphone users have difficulty transferring photos off their phone and sharing them with others. <i>PhotoRouter</i> is a software application for cameraphones that makes the photo sharing process destination-centric by allowing users to focus on who the photo should go to, not how it needs to get there. Attempting to produce an application which meets user needs better than current, technology-centric cameraphone photo sharing applications, we designed PhotoRouter. In this paper we describe PhotoRouter's user interface innovations that we will show in our technical demonstration.","cites":"1","conferencePercentile":"14.10891089"},{"venue":"ACM Multimedia","id":"de09cb5eeff5e7a752567c009f7621dfa6ebb2da","venue_1":"ACM Multimedia","year":"2016","title":"Generating Affective Captions using Concept And Syntax Transition Networks","authors":"Tushar Karayil, Philipp Blandfort, Damian Borth, Andreas Dengel","author_ids":"2675822, 3429472, 1772549, 1703343","abstract":"The area of image captioning i.e. the automatic generation of short textual descriptions of images has experienced much progress recently. However, image captioning approaches often only focus on describing the content of the image without any emotional or sentimental dimension which is common in human captions. This paper presents an approach for image captioning designed specifically to incorporate emotions and feelings into the caption generation process. The presented approach consists of a Deep Convolutional Neural Network (CNN) for detecting Adjective Noun Pairs in the image and a graphical network architecture called \"Concept And Syntax Transition (CAST)\" network for generating sentences from these detected concepts.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"55634fd8066ac2cd1bba5a6ce690ebb2056b44b6","venue_1":"ACM Multimedia","year":"2005","title":"MMM2: mobile media metadata for photo sharing","authors":"Shane Ahern, Simon King, Marc Davis","author_ids":"2796251, 1721016, 1777964","abstract":"Though cameraphones are rapidly becoming the dominant platform for consumer digital photography, users still face difficulties in transferring, managing, and sharing photos captured with cameraphones. The Mobile Media Metadata 2 (MMM2) system removes the difficulty in transferring photos from the device by providing an automatic upload capability and uses metadata about the context in which a photo was captured to simplify photo management and streamline the sharing process. In our MMM2 system, we have leveraged collaborative filtering techniques to infer the likely sharing recipients for photos based on contextual metadata, which allows the system to accurately guess likely share recipients for a photo and present them to the photographer at the time of capture.","cites":"10","conferencePercentile":"59.15841584"},{"venue":"ACM Multimedia","id":"87447cdcf6fe2d1238f0e9c933e8032816f579e0","venue_1":"ACM Multimedia","year":"2004","title":"From context to content: leveraging context to infer media metadata","authors":"Marc Davis, Simon King, Nathaniel Good, Risto Sarvas","author_ids":"1777964, 1721016, 6201430, 3261489","abstract":"The recent popularity of mobile camera phones allows for new opportunities to gather important metadata at the point of capture. This paper describes a method for generating metadata for photos using spatial, temporal, and social context. We describe a system we implemented for inferring location information for pictures taken with camera phones and its performance evaluation. We propose that leveraging contextual metadata at the point of capture can address the problems of the semantic and sensory gaps. In particular, combining and sharing spatial, temporal, and social contextual metadata from a given user and across users allows us to make inferences about media content.","cites":"90","conferencePercentile":"95.58823529"},{"venue":"ACM Multimedia","id":"b449ee21cfd3fae76b685a201122f928726e2780","venue_1":"ACM Multimedia","year":"2007","title":"Semantics, content, and structure of many for the creation of personal photo albums","authors":"Susanne Boll, Philipp Sandhaus, Ansgar Scherp, Utz Westermann","author_ids":"1714281, 2581283, 1753135, 1713926","abstract":"Photos are often a means to remember personal events, and the creation of photo albums is the attempt to preserve our memories in a nice book. For a long time people have been creating such photo albums on the basis of prints from analog photos arranged in an album book with scissors and glue and annotated with comments and captions - a tedious task which in these days is getting support by authoring tools and digitally mastered photo books. Relying on the content of others such as printed travel guides, news papers, leaflets, but also friends and family the personal content often has been enriched, enhanced, and completed. This is the starting point of our work: with digital photography and the increasing amount of content-based and contextual metadata of personal photos we can now use this metadata to actually support the targeted and semi-automatic inclusion of interesting, related information from content of others, e.g., from Web 2.0 communities, and offer and add it at the right spot in the personal album. In this paper, we show how photo album creation can benefit from leveraging information learned from many users in regard of the album's content, structure, and semantics.","cites":"21","conferencePercentile":"80.46875"},{"venue":"ACM Multimedia","id":"293ab69b54a9dc625b2683b717e7497d7d264380","venue_1":"ACM Multimedia","year":"2015","title":"Offloading Guidelines for Augmented Reality Applications on Wearable Devices","authors":"Bowen Shi, Ji Yang, Zhanpeng Huang, Pan Hui","author_ids":"2011039, 6766376, 2969693, 2665582","abstract":"As Augmented Reality (AR) gets popular on wearable devices such as Google Glass, various AR applications have been developed by leveraging synergetic benefits beyond the single technologies. However, the poor computational capability and limited power capacity of current wearable devices degrade runtime performance and sustainability. Computational offloading strategy has been proposed to outsource computation to remote cloud for improving performance. Nevertheless, comparing with mobile devices, the wearable devices have their specific limitations, which induce additional problems and require new thoughts of computational offloading. In this paper, we propose several guidelines of computational offloading for AR applications on wearable devices based on our practical experiences of designing and developing AR applications on Google Glass. The guidelines have been adopted and proved by our application prototypes.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"60b90c02ebcc1f73ee6247a24c8e5e0036176290","venue_1":"ACM Multimedia","year":"2016","title":"ReadMe: A Real-Time Recommendation System for Mobile Augmented Reality Ecosystems","authors":"Dimitris Chatzopoulos, Pan Hui","author_ids":"8608653, 2665582","abstract":"We introduce ReadMe, a real-time recommendation system (RS) and an online algorithm for Mobile Augmented Reality (MAR) ecosystems. A MAR ecosystem is the one that contains mobile users and virtual objects. The role of ReadMe is to detect and present the most suitable virtual objects on the mobile user's screen. The selection of the proper virtual objects depends on the mobile users' context. We consider the user's context as a set of variables that can be either drawn directly by user's device or can be inferred by it or can be collected in collaboration with other mobile devices.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"6d4d0675d031257656f940a72f1c1cf57c08d7f7","venue_1":"ACM Multimedia","year":"2008","title":"Second international workshop on story representation, mechanism and context (SRMC 2008)","authors":"Kevin M. Brooks, Aisling Kelliher, Frank Nack","author_ids":"2786523, 1820838, 1679840","abstract":"Stories are one of the primary forms we use to organize our lived experiences into patterned narratives that aspire to communicate that which is memorable and valuable. SRMC 2008 provides a forum for multimedia researchers and practitioners to share their novel insights and understandings of the human and machine storytelling abilities necessary for the development of engaging, participatory and sustainable multimedia systems.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"1e1754f940aa6a5ec439eedd27ad94890e36c666","venue_1":"ACM Multimedia","year":"2012","title":"X-large virtual workspaces for projector phones through peephole interaction","authors":"Bonifaz Kaufmann, Martin Hitz","author_ids":"1703220, 1737056","abstract":"In peephole interaction a window to a virtual workspace is moved in space to reveal additional content. It is a promising interaction technique for mobile projector phones to display large workspaces which contain more information than can be appropriately displayed on a small smartphone screen. In this paper we describe a projector phone prototype that implements peephole pointing without instrumenting the environment or using any additional hardware besides a smartphone and a handheld projector. This device allows for the first time to perform peephole interaction in the wild. Moreover, we demonstrate some applications we have built to exploit and investigate the full potential of peephole interaction with projector phones.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"4027419e5619de8fcbc75a728462c49f8c0206ec","venue_1":"ACM Multimedia","year":"2008","title":"Image annotation using personal calendars as context","authors":"Andrew C. Gallagher, Carman Neustaedter, Liangliang Cao, Jiebo Luo, Tsuhan Chen","author_ids":"1759673, 1688400, 2464399, 1717319, 1746230","abstract":"In this paper, we introduce the idea of using the context of a personal calendar for labeling photo collections. Calendar event annotations are matched to images based on image capture time, and a Na&#239;ve Bayes model considers features from the calendar events as well as from computer vision-based image analysis to determine if the image actually matches the calendar event. This approach has the benefit that it requires no extra annotation from the consumer, since most people already keep calendars. In our test collections, 36% of personal images could be tagged with a label from a personal calendar. Note that our preliminary results represent a lower bound on the performance that is possible because all the system components are expected to improve over time. As people migrate toward digital calendars, we can also expect more consistency in their calendar labels, which should improve the annotation accuracy.","cites":"19","conferencePercentile":"82.33944954"},{"venue":"ACM Multimedia","id":"01904a2ed5872cac57b02ec8ad5e7a4f1802b6fb","venue_1":"ACM Multimedia","year":"1995","title":"An Application Level Video Gateway","authors":"Elan Amir, Steven McCanne, Hui Zhang","author_ids":"2680247, 2887819, 1732258","abstract":"The current model for multicast transmission of video over the Internet assumes that a fixed average bandwidth is uniformly present throughout the network. Consequently, sources limit their transmission rates to accommodate the lowest bandwidth links, even though high-bandwidth con-nectivity might be available to many of the participants. We propose an architecture where a video transmission can be decomposed into multiple sessions with different bandwidth requirements using an application-level gateway. Our video gateway transparently connects pairs of sessions into a single logical conference by manipulating the data and control information of the video streams. In particular, the gateway performs bandwidth adaptation through transcoding and rate-control. We describe an efficient algorithm for transcod-ing Motion-JPEG to H.261 that runs in real-time on standard workstations. By making the Real-time Transport Protocol (RTP) an integral component of our architecture, the video gateway interoperates with the current Internet video tools in a transparent fashion. We have built a prototype of the video gateway and used it to redistribute multi-megabit JPEG video seminars from the Bay Area Gigabit Network as 128 kb/s H.261 MBone sessions.","cites":"210","conferencePercentile":"85"},{"venue":"ACM Multimedia","id":"0cccd0ef1993b013326febe4044afd19e37ffd00","venue_1":"ACM Multimedia","year":"1994","title":"Experiments with the Tenet Real-Time Protocol Suite on the Sequoia 2000 Wide Area Network","authors":"Anindo Banerjea, Edward W. Knightly, Fred Templin, Hui Zhang","author_ids":"2889298, 1747574, 2323977, 1732258","abstract":"Emerging distributed multimedia applications have stringent performance requirements in terms of bandwidth, delay, delay-jitter, and loss rate. The Tenet real-time protocol suite provides the services and mechanisms for delivering such performance guarantees, even during periods of high network load and congestion. The protocols achieve this by using resource management, connection admission control, and appropriate packet service disciplines inside the network. The Sequoia 2000 network employs the Tenet Protocol Suite at each of its hosts and routers making it one of the first wide area packet-switched networks to provide end-to-end per-connection performance guarantees. This paper presents experiments with the Tenet protocols on the Sequoia 2000 network including measurements of the performance of the protocols, the service recieved by real multimedia applications using the protocols, and comparisons with the service received by applications that use the Internet protocols (UDP/IP). We conclude that the Tenet protocols successfully protect the real-time channels from other traffic in the network, including other real-time channels, and allow channels to continue to meet their performance guarantees, even when the network is highly loaded.","cites":"24","conferencePercentile":"61.01694915"},{"venue":"ACM Multimedia","id":"37d60626bd21404a1799ce44184d367376e50a44","venue_1":"ACM Multimedia","year":"2005","title":"Media gallery TV: view and shop your photos on interactive digital television","authors":"Sabine Thieme, Ansgar Scherp, Melanie Albrecht, Susanne Boll","author_ids":"1867155, 1753135, 4035696, 1714281","abstract":"In this paper, we present the <i>Media Gallery</i>, a MHP-based interactive multimedia application on digital TV. This application allows customers to view and order their digital photos and to order physical prints and fun products from these digital photos directly from TV. The Media Gallery opens a new distribution channel and market opportunity for the photo finisher and a platform to comfortably view and order their digital images directly on their TV.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"a19394533bdb03cb76f06b98036f99424b667454","venue_1":"ACM Multimedia","year":"2010","title":"An immersive system for browsing and visualizing surveillance video","authors":"Philip DeCamp, George Shaw, Rony Kubat, Deb Roy","author_ids":"1810605, 8612845, 2160011, 4719423","abstract":"HouseFly is an interactive data browsing and visualization system that synthesizes audio-visual recordings from multiple sensors, as well as the meta-data derived from those recordings, into a unified viewing experience. The system is being applied to study human behavior in both domestic and retail situations grounded in longitudinal video recordings. HouseFly uses an immersive video technique to display multiple streams of high resolution video using a realtime warping procedure that projects the video onto a 3D model of the recorded space. The system interface provides the user with simultaneous control over both playback rate and vantage point, enabling the user to navigate the data spatially and temporally. Beyond applications in video browsing, this system serves as an intuitive platform for visualizing patterns over time in a variety of multi-modal data, including person tracks and speech transcripts.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"4d7661a69491dcde3b43859c018365fd66899c3b","venue_1":"ACM Multimedia","year":"2005","title":"An ambient intelligence platform for physical play","authors":"Ron Wakkary, Marek Hatala, Robb Lovell, Milena Droumeva","author_ids":"1783186, 3034061, 2162846, 2585571","abstract":"This paper describes an ambient intelligent prototype known as socio-ec(h)o. socio-ec(h)o explores the design and implementation of a system for sensing and display, user modeling, and interaction models based on a game structure. The game structure includes, word puzzles, levels, body states, goals and game skills. Body states are body movements and positions that players must discover in order to complete a level and in turn represent a learned game skill. The paper provides an overview of background concepts and related research. We describe the prototype and game structure, provide a technical description of the prototype and discuss technical issues related to sensing, reasoning and display. The paper contributes by providing a method for constructing group parameters from individual parameters with real-time motion capture data; and a model for mapping the trajectory of participant's actions in order to determine an <i>intensity</i> level used to manage the experience flow of the game and its representation in audio and visual display. We conclude with a discussion of known and outstanding technical issues, and future research.","cites":"14","conferencePercentile":"67.82178218"},{"venue":"ACM Multimedia","id":"1c9472ca3adfb71bc12ee89d91f9e58fe328fcdc","venue_1":"ACM Multimedia","year":"2005","title":"Context-driven smart authoring of multimedia content with xSMART","authors":"Ansgar Scherp, Susanne Boll","author_ids":"1753135, 1714281","abstract":"In recent years, many highly sophisticated multimedia authoring tools have been developed. Up to today, these system's integration of the targeted user context, however, is limited. With our <i>Context-aware Smart Multimedia Authoring Tool</i> (<i>xSMART</i>) we developed a semi-automatic authoring tool that integrates the targeted user context into the different authoring steps and exploits this context to guide the author through the content authoring process. The design of xSMART allows that it can be extended and customized to the requirements of a specific domain by domain-specific wizards. These wizards realize the user interface that meets best the domain-specific requirements and effectively supports the domain experts in creating their content targeted at a specific user context.","cites":"17","conferencePercentile":"72.27722772"},{"venue":"ACM Multimedia","id":"67b38b88f3b3acb4ebba3c1941cbab7290bf59fa","venue_1":"ACM Multimedia","year":"2014","title":"Object-Based Visual Sentiment Concept Analysis and Application","authors":"Tao Chen, Felix X. Yu, Jiawei Chen, Yin Cui, Yan-Ying Chen, Shih-Fu Chang","author_ids":"4725884, 1815972, 3214237, 2163853, 1809974, 1735547","abstract":"This paper studies the problem of modeling object-based visual concepts such as \"crazy car\" and \"shy dog\" with a goal to extract emotion related information from social multimedia content. We focus on detecting such adjective-noun pairs because of their strong co-occurrence relation with image tags about emotions. This problem is very challenging due to the highly subjective nature of the adjectives like \"crazy\" and \"shy\" and the ambiguity associated with the annotations. However, associating adjectives with concrete physical nouns makes the combined visual concepts more detectable and tractable. We propose a hierarchical system to handle the concept classification in an object specific manner and decompose the hard problem into object localization and sentiment related concept modeling. In order to resolve the ambiguity of concepts we propose a novel classification approach by modeling the concept similarity, leveraging on online commonsense knowledgebase. The proposed framework also allows us to interpret the classifiers by discovering discriminative features. The comparisons between our method and several baselines show great improvement in classification performance. We further demonstrate the power of the proposed system with a few novel applications such as sentiment-aware music slide shows of personal albums.","cites":"17","conferencePercentile":"94.77911647"},{"venue":"ACM Multimedia","id":"2738c04169577fdf2107787d777fc4b961e55d13","venue_1":"ACM Multimedia","year":"2014","title":"Automatic Image Cropping using Visual Composition, Boundary Simplicity and Content Preservation Models","authors":"Chen Fang, Zhe L. Lin, Radomír Mech, Xiaohui Shen","author_ids":"7596126, 2547020, 2008027, 1720987","abstract":"Cropping is one of the most common tasks in image editing for improving the aesthetic quality of a photograph. In this paper, we propose a new, aesthetic photo cropping system which combines three models: <i>visual composition, boundary simplicity</i>, and <i>content preservation.</i> The visual composition model measures the quality of composition for a given crop. Instead of manually defining rules or score functions for composition, we learn the model from a large set of well-composed images via discriminative classifier training. The boundary simplicity model measures the clearness of the crop boundary to avoid object cutting-through. The content preservation model computes the amount of salient information kept in the crop to avoid excluding important content. By assigning a hard lower bound constraint on the content preservation and linearly combining the scores from the visual composition and boundary simplicity models, the resulting system achieves significant improvement over recent cropping methods in both quantitative and qualitative evaluation.","cites":"7","conferencePercentile":"84.93975904"},{"venue":"ACM Multimedia","id":"e18caa6818ba1cf4629a89e2b8ade5965db067d0","venue_1":"ACM Multimedia","year":"2003","title":"Identifying audio clips with RARE","authors":"Christopher J. C. Burges, John C. Platt, Jonathan Goldstein","author_ids":"3158445, 1842887, 2621731","abstract":"In this paper, we describe RARE (Robust Audio Recognition Engine): a system for identifying audio streams and files. RARE can be used in a variety of applications: from enhancing the consumer listening experience to cleaning large audio databases. RARE was designed with two key qualities in mind: robustness to distortion of the audio, and lookup speed. RARE identifies audio clips in a stream against a database of 1/4 million songs in real time using approximately 10% CPU on an 850 MHz P3, and with a measured false positive rate of 1.5x10-8 per clip, per database entry, at a false negative rate of 0.2% per clip. We demo RARE in real-time on a stream and on distorted files.","cites":"7","conferencePercentile":"38.73873874"},{"venue":"ACM Multimedia","id":"0c4f736bb1126757ad4412e131fe2c9e99878edc","venue_1":"ACM Multimedia","year":"2005","title":"Towards context-aware face recognition","authors":"Marc Davis, Michael Smith, John F. Canny, Nathaniel Good, Simon King, Rajkumar Janakiraman","author_ids":"1777964, 7522504, 1729041, 6201430, 1721016, 2548551","abstract":"In this paper, we focus on the use of context-aware, collaborative filtering, machine-learning techniques that leverage automatically sensed and inferred contextual metadata together with computer vision analysis of image content to make accurate predictions about the human subjects depicted in cameraphone photos. We apply Sparse-Factor Analysis (SFA) to both the contextual metadata gathered in the MMM2 system and the results of PCA (Principal Components Analysis) of the photo content to achieve a 60% face recognition accuracy of people depicted in our cameraphone photos, which is 40% better than media analysis alone. In short, we use context-aware media analysis to solve the face recognition problem for cameraphone photos.","cites":"34","conferencePercentile":"85.89108911"},{"venue":"ACM Multimedia","id":"422461bbe014ce639074b9db684c69daa7a703db","venue_1":"ACM Multimedia","year":"2005","title":"Photo LOI: browsing multi-user photo collections","authors":"Rahul Nair, Nick Reid, Marc Davis","author_ids":"1799464, 1833448, 1777964","abstract":"The number of digital photographs is growing beyond the abilities of individuals to easily manage and understand their own photo collections. Photo LOI (Level of Interest) is a technique that filters, aggregates, and visualizes photographs taken by multiple users who shared temporal, spatial, and/or social context at the point of photo capture. Photo LOI enables groups of photographers to see and manipulate visualizations of their photographic activities over time and social space in order to help cluster and select photos, and enables researchers to study contextual patterns in the phototaking habits of different users and groups of users. In this paper we give a brief overview of Photo LOI's features and describe some of its applications.","cites":"9","conferencePercentile":"56.68316832"},{"venue":"ACM Multimedia","id":"010f2591f27f5dfac816a302af8a7d74ff62a3fe","venue_1":"ACM Multimedia","year":"2016","title":"Social and Affective Robotics Tutorial","authors":"Maja Pantic, Vanessa Evers, Marc Peter Deisenroth, Luis Merino, Björn W. Schuller","author_ids":"1694605, 1778867, 3274725, 1738621, 1705602","abstract":"Social and Affective Robotics is a growing multidisciplinary field encompassing computer science, engineering, psychology, education, and many other disciplines. It explores how social and affective factors influence interactions between humans and robots, and how affect and social signals can be sensed and integrated into the design, implementation, and evaluation of robots. With talks by renowned researchers in this area, Social and Affective Robotics Tutorial will help both new and experienced researchers to identify trends, concepts, methodologies and applications in this field, identified as a technological megatrend driving the fourth industrial revolution.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"665fd13ba886b8fb28df07f0413a3da6e4f8f418","venue_1":"ACM Multimedia","year":"2015","title":"SkyStitch: A Cooperative Multi-UAV-based Real-time Video Surveillance System with Stitching","authors":"Xiangyun Meng, Wei Wang, Ben Leong","author_ids":"2818758, 1706612, 3038892","abstract":"Recent advances in unmanned aerial vehicle (UAV) technologies have made it possible to deploy an aerial video surveillance system to provide an unprecedented aerial perspective for ground monitoring in real time. Multiple UAVs would be required to cover a large target area, and it is difficult for users to visualize the overall situation if they were to receive multiple disjoint video streams. To address this problem, we designed and implemented <i>SkyStitch</i>, a multiple-UAV video surveillance system that provides a single and panoramic video stream to its users by stitching together multiple aerial video streams. SkyStitch addresses two key design challenges: (i) the high computational cost of stitching and (ii) the difficulty of ensuring good stitching quality under dynamic conditions. To improve the speed and quality of video stitching, we incorporate several practical techniques like distributed feature extraction to reduce workload at the ground station, the use of hints from the flight controller to improve stitching efficiency and a Kalman filter-based state estimation model to mitigate jerkiness. Our results show that SkyStitch can achieve a stitching rate that is 4 times faster than existing state-of-the-art methods and also improve perceptual stitching quality. We also show that SkyStitch can be easily implemented using commercial off-the-shelf hardware.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"0830a5a9e2e495564eee8e54debab292c625b7a3","venue_1":"ACM Multimedia","year":"2002","title":"An MPEG performance model and its application to adaptive forward error correction","authors":"Ketan Mayer-Patel, Long Le, Georg Carle","author_ids":"3715598, 2921850, 2902874","abstract":"We present a general analytical model for predicting the reconstructed frame rate of an MPEG stream. Our model captures the temporal relationships between I-, P, and B-frames but is independent of the channel and media characteristics. We derive an adaptive FEC scheme from the general model and verify it by comparing it to the results of a simulation. The prediction error of the model compared to the simulation for a wide array of parameter values is less than 5%. We then use the derived adaptive FEC scheme to study the optimal rate allocation (i.e., between generating a higher frame rate or increasing the protection for a lower frame rate) when equation-based TCP rate control is used to couple packet rates to channel characteristics such as round trip time and packet loss probabilities. Surprisingly, we find that optimal protection levels for I- and P-frames are relatively static as loss rates increase from 1% to 4% while changes in the frame type pattern are used to ameliorate the effects of the increased loss. The study demonstrates how our model can be used to reveal joint source/channel coding tradeoffs and how they relate to encoding and transmission parameters.","cites":"16","conferencePercentile":"68.8034188"},{"venue":"ACM Multimedia","id":"cfab5fae8ce3e19ef932211d74a73e7bd9baef88","venue_1":"ACM Multimedia","year":"2016","title":"Jockey Time: Making Video Playback to Enhance Emotional Effect","authors":"Kyeong-Ah Jeong, Hyeon-Jeong Suk","author_ids":"3204144, 1866213","abstract":"In order to effectively and easily deliver the affective quality of a video, this study investigated the emotional manifests induced by the playback design of the video. In designing the playback, we articulated speed, direction, and continuity of the video and surveyed observers' responses. Based on the results, we propose seven categories of playback design, and each appeals cheerful, happy, relaxed, funny, urgent, angry, and sad emotion. For an easy use, we offer an online video editing service, \"Jockey Time.\" A beta version was operated for a month for monitoring purpose, and finally, Jockey Time v.1.0 is now launched for anybody to easily enhance the emotional effect of one's video.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"68e81d226dae0d5074b4d9caf99d1752ebe00b98","venue_1":"ACM Multimedia","year":"2015","title":"Gradient-based 2D-to-3D Conversion for Soccer Videos","authors":"Kiana Calagari, Mohamed A. Elgharib, Piotr Didyk, Alexandre Kaspar, Wojciech Matusik, Mohamed Hefeeda","author_ids":"2397259, 1854465, 3307078, 3035284, 1752521, 1711116","abstract":"A wide spread adoption of 3D videos and technologies is hindered by the lack of high-quality 3D content. One promising solution to address this problem is to use automated 2D-to-3D conversion. However, current conversion methods, while general, produce low-quality results with artifacts that are not acceptable to many viewers. We address this problem by showing how to construct a high-quality, domain-specific conversion method for soccer videos. We propose a novel, data-driven method that generates stereoscopic frames by transferring depth information from similar frames in a database of 3D stereoscopic videos. Creating a database of 3D stereoscopic videos with accurate depth is, however, very difficult. One of the key findings in this paper is showing that computer generated content in current sports computer games can be used to generate high-quality 3D video reference database for 2D-to-3D conversion methods. Once we retrieve similar 3D video frames, our technique transfers depth gradients to the target frame while respecting object boundaries. It then computes depth maps from the gradients, and generates the output stereoscopic video. We implement our method and validate it by conducting user-studies that evaluate depth perception and visual comfort of the converted 3D videos. We show that our method produces high-quality 3D videos that are almost indistinguishable from videos shot by stereo cameras. In addition, our method significantly outperforms the current state-of-the-art method. For example, up to 20% improvement in the perceived depth is achieved by our method, which translates to improving the mean opinion score from Good to Excellent.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"5c7723ed3ed7ef6c3849f043f87f5c86875118ab","venue_1":"ACM Multimedia","year":"2016","title":"Key Color Generation for Affective Multimedia Production: An Initial Method and Its Application","authors":"EunJin Kim, Hyeon-Jeong Suk","author_ids":"2927377, 1866213","abstract":"In this paper, we introduce a method that generates a key color to construct an aesthetic and affective harmony with visual content. Given an image and an affective term, our method creates a key color by combining a dominant hue of the image and a unique tone associated with the affective word. To match each affective term with a specific tone, we collected color themes from a crowd-sourced database and identified the most popular tone of color themes that are relevant to each affective term. The results of a user test showed that the method generates satisfactory key colors as much as designers do. Finally, as a prospective application, we employed our method to a promotional video editing prototype. Our method automatically generates a key color based on a frame of an input video and apply the color to a shape that delivers a promotional message. A second user study verifies that the video editing prototype with our method can effectively deliver the desired affective state with a satisfactory quality.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2d0b2aceafbdc97072bcc92249720169c8a98b4f","venue_1":"ACM Multimedia","year":"2011","title":"OpenMusic: visual programming environment for music composition, analysis and research","authors":"Jean Bresson, Carlos Agón, Gérard Assayag","author_ids":"2608740, 3122244, 2435162","abstract":"OpenMusic is an open source environment dedicated to music composition. The core of this environment is a full-featured visual programming language based on Common Lisp and CLOS (Common Lisp Object System) allowing to design processes for the generation or manipulation of musical material. This language can also be used for general purpose visual programming and other (possibly extra-musical) applications.","cites":"13","conferencePercentile":"87.46355685"},{"venue":"ACM Multimedia","id":"112a6d4d4b8cde926e1b68fc5a0797046d0818fa","venue_1":"ACM Multimedia","year":"2009","title":"Flickr hypergroups","authors":"Radu Andrei Negoescu, Brett Adams, Dinh Q. Phung, Svetha Venkatesh, Daniel Gatica-Perez","author_ids":"1937451, 1760106, 1749657, 1679520, 1698682","abstract":"The amount of multimedia content available online constantly increases, and this leads to problems for users who search for content or similar communities. Users in Flickr often self-organize in user communities through Flickr Groups. These groups are particularly interesting as they are a natural instantiation of the content~+~relations social media paradigm. We propose a novel approach to group searching through hypergroup discovery. Starting from roughly 11,000 Flickr groups' content and membership information, we create three different bag-of-word representations for groups, on which we learn probabilistic topic models. Finally, we cast the hypergroup discovery as a clustering problem that is solved via probabilistic affinity propagation. We show that hypergroups so found are generally consistent and can be described through topic-based and similarity-based measures. Our proposed solution could be relatively easily implemented as an application to enrich Flickr's traditional group search.","cites":"18","conferencePercentile":"82.85123967"},{"venue":"ACM Multimedia","id":"d3121709642d2e9b79f478a302142385194a2dd5","venue_1":"ACM Multimedia","year":"1999","title":"Position paper: Internet VoD cache server design","authors":"Carsten Griwodz, Michael Zink, Michael Liepert, Ralf Steinmetz","author_ids":"1745763, 1748142, 3309974, 1725298","abstract":"1. ABSTRACT We think that web caches will soon have to better support multimedia demands. In this paper we present a cache server design for internet video on demand (VoD) systems. Internet VoD today is dominated by systems like the Real G2 System [ 131 supporting various low bandwidth formats. The length and especially the quality of current video clips are very limited, and not applicable at all for commercial VoD. One of the major limitations is the necessity to stream the video clip directly from a central server to each client individually, because redistribution is not established yet. Intranet solutions have existed for a while, they use distributed systems but are typically managed from a central site. Current internet caching strategies can remain rather simple, since the assumption of a large cache and small data items is valid for the vast part of web traffic. Furthermore, the distributed content is typically free or not commercially relevant. This permits to mostly ignore security and copyright issues as well. However, we expect that the growth of the Internet and the integration of services will make the idea of wide-area distribution of commercial quality video over networks without central management feasible. Intelligent caching can be helpful in this for two reasons: l Movies have a considerable life cycle that can and should be taken into account [5] l The movement of full-sized high quality movies among caches is severely more expensive then that of video clips because of the required bandwidth and storage. We address these issues with the following efforts: Permission to make digital or hard copies of all or part of this work for personat or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. l Investigate more complex strategies and structures to position and access copies of expensive files. This is unlike the current approach of the web caching com-muties to efficiently handle cooperative caches: since the number of requests that reach these caches is large and requested files are mostly very small, these caches are mainly streamlined for simplicity and thus rapid request answers [ 11. l Support the delivery of large bulk …","cites":"1","conferencePercentile":"19.32773109"},{"venue":"ACM Multimedia","id":"5d082a70eab056a1c5e72e788db1e63091cce366","venue_1":"ACM Multimedia","year":"2006","title":"Extraction of social context and application to personal multimedia exploration","authors":"Brett Adams, Dinh Q. Phung, Svetha Venkatesh","author_ids":"1760106, 1749657, 1679520","abstract":"Personal media collections are often viewed and managed along the social dimension, the places we spend time at and the people we see, thus tools for extracting and using this information are required. We present novel algorithms for identifying socially significant places termed social spheres unobtrusively from GPS traces of daily life, and label them as one of Home, Work, or Other, with quantitative evaluation of 9 months taken from 5 users. We extract locational co-presence of these users and formulate a novel measure of social tie strength based on frequency of interaction, and the nature of spheres it occurs within. Comparative user studies of a multimedia browser designed to demonstrate the utility of social metadata indicate the usefulness of a simple interface allowing navigation and filtering in these terms. We note the application of social context is potentially much broader than personal media management, including context-aware device behaviour, life logs, social networks, and location-aware information services.","cites":"18","conferencePercentile":"82.38341969"},{"venue":"ACM Multimedia","id":"a8bdae5d174d982ddc70af95ff4c9e886e6058be","venue_1":"ACM Multimedia","year":"2006","title":"MAGICAL demonstration: system for automated metadata generation for instructional content","authors":"Chitra Dorai, Robert G. Farrell, Amy Katriel, Galina Kofman, Ying Li, Youngja Park","author_ids":"1719234, 1761756, 3316284, 2395835, 4744617, 8109042","abstract":"The \"Tools for Automatic Generation of Learning Object Metadata\" project addresses the requirement of developing advanced distributed learning delivery architecture and services for a large US government agency. We have developed a Webbased system called MAGIC (<i>Metadata Automated Generation for Instructional Content</i>) to assist content authors and course developers in generating metadata for learning objects and information assets to enable wider reuse of these objects across departments and organizations. Using the MAGIC system, content authors review and edit automatically-generated metadata sufficient to register and describe their assets for use and discovery in current and future distributed learning applications complying with the ADL SCORM standard. Course developers can use the system to assist in the conversion of existing courses to SCORM format or in developing new SCORM courses. The MAGIC system includes software tools to analyze and extract descriptive metadata from instructional videos, training documents, and other information assets. The tools generate some of the most critical SCORM metadata completely automatically. Benefits of MAGIC include easier reuse and repurposing, improved interoperability, and more timely registration of content for use by course developers. In this paper, we describe the system architecture, analysis tools developed, and services supported. A live demonstration of the system illustrating several use cases of the system will be presented at the conference, with a discussion of results from user studies and evaluation of the system.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"59ec5fc997a52bd77ee4eac008a305cb776d31a7","venue_1":"ACM Multimedia","year":"1994","title":"Automatic Presentation of Multimedia Documents Using Relational Grammars","authors":"Louis Weitzman, Kent Wittenburg","author_ids":"7511006, 3242147","abstract":"This paper describes an approach to the automatic presentation of multimedia documents based on parsing and syntax-directed translation using Relational Grammars. This translation is followed by a constraint solving mechanism to create the final layout. Grammatical rules provide the mechanism for mapping from a representation of the content of a presentation to forms that specify the media objects to be realized. These realization forms include sets of spatial and temporal constraints between elements of the presentation. Individual grammars encapsulate the &#8220;look and feel&#8221; of a presentation and can be used as generators of that style. By making the grammars sensitive to the requirements of the output medium, parsing can introduce flexibility into the information realization process.","cites":"84","conferencePercentile":"91.52542373"},{"venue":"ACM Multimedia","id":"a90073e457a9243c187e68ecf5c99753acd08a79","venue_1":"ACM Multimedia","year":"2005","title":"Creating MAGIC: system for generating learning object metadata for instructional content","authors":"Ying Li, Chitra Dorai, Robert G. Farrell","author_ids":"4744617, 1719234, 1761756","abstract":"This paper presents our latest work on building a system called MAGIC (Metadata Automated Generation for Instructional Content) that will automatically identify segments and generate critical metadata conforming with the SCORM (Sharable Content Object Reference Model) standard for instructional content. Various content analytics engines are utilized to automatically generate key metadata, which include audiovisual analysis modules that recognize semantic sound categories and identify narrators and informative text segments; text analysis modules that extract title, keywords and summary from text documents; and a text categorizer that classifies a document according to a pre-generated taxonomy. With MAGIC, instructional content developers can generate and edit SCORM metadata to richly describe their content asset for use in distributed learning applications. Experimental results obtained from collections of real data from targeted user communities will be presented.","cites":"11","conferencePercentile":"60.89108911"},{"venue":"ACM Multimedia","id":"3c906f727384f171875a983206cf65c4b16aecfa","venue_1":"ACM Multimedia","year":"2015","title":"Jointly Estimating Interactions and Head, Body Pose of Interactors from Distant Social Scenes","authors":"Subramanian Ramanathan, Jagannadan Varadarajan, Elisa Ricci, Oswald Lanz, Stefan Winkler","author_ids":"1742936, 2032648, 1878028, 1717522, 3421746","abstract":"We present joint estimation of <i>F-formations</i> and <i>head</i>, <i>body pose</i> of interactors in a social scene captured by surveillance cameras. Unlike prior works that have focused on (a) discovering F-formations based on head pose and position cues, or (b) jointly learned head and body pose of individuals based on anatomic constraints, we exploit positional and pose cues characterizing interactors and interactions to jointly infer both (a) and (b). We show how the joint inference framework benefits both F-formation and head, body pose estimation accuracy via experiments on two social datasets.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"d9a4f4252ba2836737a1bcb9f2d9f240a4302bf7","venue_1":"ACM Multimedia","year":"2015","title":"eRS: A System to Facilitate Emotion Recognition in Movies","authors":"Joël Dumoulin, Diana Affi, Elena Mugellini, Omar Abou Khaled","author_ids":"1792182, 1798023, 1802011, 1799647","abstract":"We present eRS, an open-source system whose purpose is to facilitate the workflow of emotion recognition in movies, released under the MIT license. The system consists of a Django project and an AngularJS web application. It allows to easily create emotional video datasets, process the videos, extract the features and model the emotion. All data is exposed by a REST API, making it available not only to the eRS web application, but also to other applications. All visualizations are interactive and linked to the playing video, allowing researchers to easily analyze the results of their algorithms. The system currently runs on Linux and OS X. eRS can be extended, to integrate new features and algorithms needed in the different steps of emotion recognition in movies.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"52ea5aa865a144d43bff883ddc8c68204893af6e","venue_1":"ACM Multimedia","year":"2015","title":"Movie's Affect Communication Using Multisensory Modalities","authors":"Joël Dumoulin, Diana Affi, Elena Mugellini, Omar Abou Khaled, Marco Bertini, Alberto Del Bimbo","author_ids":"1792182, 1798023, 1802011, 1799647, 1801509, 8196487","abstract":"The goal of the system presented in this demo is to make possible for the visually and hearing impaired audience to live empathetic viewing experiences using their home theatre. In this work we suggest the incorporation of new emotion communication modalities into the standard television, to provide the targeted audience with sensations that they do not have the opportunity to enjoy because of their disability.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"3d836081f4b03ee57ebfb2fd37218ebfb8b00699","venue_1":"ACM Multimedia","year":"2002","title":"MyLifeBits: fulfilling the Memex vision","authors":"Jim Gemmell, Gordon Bell, Roger Lueder, Steven M. Drucker, Curtis Wong","author_ids":"2348268, 6206605, 1870366, 2311676, 2654137","abstract":"MyLifeBits is a project to fulfill the Memex vision first posited by Vannevar Bush in 1945. It is a system for storing all of one's digital media, including documents, images, sounds, and videos. It is built on four principles: (1) collections and search must replace hierarchy for organization (2) many visualizations should be supported (3) annotations are critical to non-text media and must be made easy, and (4) authoring should be via transclusion.","cites":"283","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"19d752477a8d331cce20984efd284bfa0a374ad8","venue_1":"ACM Multimedia","year":"2009","title":"Automated localization of affective objects and actions in images via caption text-cum-eye gaze analysis","authors":"Subramanian Ramanathan, Harish Katti, Raymond Huang, Tat-Seng Chua, Mohan S. Kankanhalli","author_ids":"1742936, 2478739, 6713342, 1684968, 1744045","abstract":"We propose a novel framework to localize and label affective objects and actions in images through a combination of text, visual and gaze-based analysis. Human gaze provides useful cues to infer locations and interactions of affective objects. While concepts (labels) associated with an image can be determined from its caption, we demonstrate localization of these concepts upon learning from a statistical affect model for world concepts. The affect model is derived from non-invasively acquired fixation patterns on labeled images, and guides localization of affective objects (faces, reptiles) and actions (look, read) from fixations in unlabeled images. Experimental results obtained on a database of 500 images confirm the effectiveness and promise of the proposed approach.","cites":"14","conferencePercentile":"79.33884298"},{"venue":"ACM Multimedia","id":"c3cd2fc894d61df419fa60bd6cd29f6bb2083aed","venue_1":"ACM Multimedia","year":"2014","title":"Object Tracking using Reformative Transductive Learning with Sample Variational Correspondence","authors":"Tao Zhuo, Peng Zhang, Yanning Zhang, Wei Huang, Hichem Sahli","author_ids":"2628886, , 1801395, 1730584, 2750145","abstract":"Tracking-by-learning strategies have effectively solved many challenging problems for visual tracking. When labeled samples are limited, the learning performance can be improved by exploiting unlabeled ones. Thus, a key issue for semi-supervised learning is the label assignment of the unlabeled samples, which is the principal focus of transductive learning. Unfortunately, the optimization scheme employed by the transductive learning is hard to be applied to online tracking because of its large amount of computation for sample labeling. In this paper, a reformative transductive learning was proposed with the variational correspondence between the learning samples, which are utilized to build an effective matching cost function for more efficient label assignment during the learning of representative separators. By using a weighted accumulative average to update the coefficients via a fixed budget of support vectors, the proposed tracking has been demonstrated to outperform most of the state-of-art trackers.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"66c24fea251d803980bd71bb5c7c02f4c04e5cb2","venue_1":"ACM Multimedia","year":"2007","title":"One million heartbeats","authors":"Su-Chu Hsu, Jin-Yao Lin, Carven Chen, Ying-Chung Chen, Jiun-Shian Lin, Keng-Hau Chang","author_ids":"3327836, 2730227, 2994420, 3191746, 2362811, 2351298","abstract":"We explore possibilities for applying wireless sensor networks (WSN) in interactive art. One Million Heartbeats, our interactive artwork, uses a ZigBee wireless sensor network, bio-feedback sensors, video projection, sculpture, and a weblog. It depicts the struggle between twin fetuses in the \"world\" of a mother's womb, and many people interpret it as commentary on political or social events. We will collect one million \"heartbeats\" from participants - the behaviors of our participants determine the characteristics of the fetuses and the experience of the mother. Participant comments become part of the piece. This artwork explores collective social behavior. We believe that this piece shows a new medium for interactive art: ubiquitous wireless sensor networks.","cites":"1","conferencePercentile":"19.27083333"},{"venue":"ACM Multimedia","id":"1a9a8cb6568cd8bb4ca813f34247c76f0a72c61c","venue_1":"ACM Multimedia","year":"2001","title":"Fast client-server video summarization for continuous capture","authors":"John Dixon, Charles B. Owen","author_ids":"8727021, 1787354","abstract":"This paper describes a keyframe summarization method for client-server applications. This technique is designed for applications where a camera is collecting content on a continuous basis that must be transmitted in a summarized form to a remote database server over wireless network. The system combines three keyframe selection methods including a novel fast motion-based selection method, keyframe pooling and clustering for bandwidth control, and network bandwidth estimation.","cites":"0","conferencePercentile":"4.591836735"},{"venue":"ACM Multimedia","id":"2f6b270e222adb5799aa2405d7f65da85f439e64","venue_1":"ACM Multimedia","year":"2013","title":"Non-rigid target tracking based on 'flow-cut' in pair-wise frames with online hough forests","authors":"Tao Zhu, Yanning Zhang, Peng Zhang, Wei Huang, Hichem Sahli","author_ids":"1703737, 1801395, , 1730584, 2750145","abstract":"In conventional online learning based tracking studies, fixed-shape appearance modeling is often incorporated for training samples generation, as it is simple and convenient to be applied. However, for more general non-rigid and articulated object, this strategy may regard some background areas as foreground, which is likely to deteriorate the learning process. Recently published works utilize more than one patches to represent non-rigid object with foreground object segmentation, but most of these segmentation for target representation are performed only in single frame manner. Since the motion information between the consecutive frames was not considered by these approaches, when the backgrounds are similar to the target, accurate segmentation is hard to be achieved. In this work, we propose a novel model for non-rigid object segmentation by incorporating consecutive gradients flow between pair-wise frames into a Gibbs energy function. With help from motion information, the irregular target areas can be segmented more accurately during precise boundary convergence. The proposed segmentation model is incorporated into a semi-supervised online tracking framework for training samples generation. We test the proposed tracking on challenging videos involving heavy intrinsic variations and occlusions. As a result, the experiments demonstrate a significant improvement in tracking accuracy and robustness in comparison with other state-of-art tracking works.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"5a14c42c80d83c402eaab0746620e2189e37ba5c","venue_1":"ACM Multimedia","year":"2016","title":"Image2Text: A Multimodal Image Captioner","authors":"Chang Liu, Changhu Wang, Fuchun Sun, Yong Rui","author_ids":"1759234, 1697065, 1741199, 1728806","abstract":"In this work, we showcase the Image2Text system, which is a real-time captioning system that can generate human-level natural language description for any input image. We formulate the problem of image captioning as a multimodal translation task. Analogous to machine translation, we present a sequence-to-sequence recurrent neural networks (RNN) model for image caption generation. Different from most existing work where the whole image is represented by a convolutional neural networks (CNN) feature, we propose to represent the input image as a sequence of detected objects to serve as the source sequence of the RNN model. Based on the captioning framework, we develop a user-friendly system to automatically generated human-level captions for users. The system also enables users to detect salient objects in an image, and retrieve similar images and corresponding descriptions from a database.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"f0d5da00371904eb1e299ebc11b5b5b6aff5d421","venue_1":"ACM Multimedia","year":"2016","title":"Video ChatBot: Triggering Live Social Interactions by Automatic Video Commenting","authors":"Yehao Li, Ting Yao, Rui Hu, Tao Mei, Yong Rui","author_ids":"3431141, 8543685, 4234445, 1788123, 1728806","abstract":"We demonstrate a video chatbot, which can generate human-level emotional comments referring to the videos shared by users and trigger a conversation with users. Our video chatbot performs a large-scale similar video search to find visually similar videos w.r.t. a given video using approximate nearest-neighbor search. Then, the comments associated with the searched similar videos are ranked by learning a deep multi-view embedding space for modeling video content, visual sentiment and textual comments. The top ranked comments are selected as responses to the given video and trigger the succeeding text-based chat between users and the chatbot. The demonstration is conducted on a newly collected dataset with over 102K videos and 10.6M comments. Moreover, our video chatbot has great potential to increase live social interactions.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"446f572df97f0b852a1a5f91015faf17944c1234","venue_1":"ACM Multimedia","year":"2016","title":"Share-and-Chat: Achieving Human-Level Video Commenting by Search and Multi-View Embedding","authors":"Yehao Li, Ting Yao, Tao Mei, Hongyang Chao, Yong Rui","author_ids":"3431141, 8543685, 1788123, 2403535, 1728806","abstract":"Video has become a predominant social media for the booming live interactions. Automatic generation of emotional comments to a video has great potential to significantly increase user engagement in many socio-video applications (e.g., chat bot). Nevertheless, the problem of video commenting has been overlooked by the research community. The major challenges are that the generated comments are to be not only as natural as those from human beings, but also relevant to the video content. We present in this paper a novel two-stage deep learning-based approach to automatic video commenting. Our approach consists of two components. The first component, similar video search, efficiently finds the visually similar videos w.r.t. a given video using approximate nearest-neighbor search based on the learned deep video representations, while the second dynamic ranking effectively ranks the comments associated with the searched similar videos by learning a deep multi-view embedding space. For modeling the emotional view of videos, we incorporate visual sentiment, video content, and text comments into the learning of the embedding space. On a newly collected dataset with over 102K videos and 10.6M comments, we demonstrate that our approach outperforms several state-of-the-art methods and achieves human-level video commenting.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2f77e464414dae330df16a51971ec296ad76467e","venue_1":"ACM Multimedia","year":"2013","title":"Cross-media semantic representation via bi-directional learning to rank","authors":"Fei Wu, Xinyan Lu, Zhongfei Zhang, Shuicheng Yan, Yong Rui, Yueting Zhuang","author_ids":"1695826, 8649365, 1720488, 1698982, 1728806, 1755711","abstract":"In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. Existing approaches take either one-to-one paired data or uni-directional ranking examples (i.e., utilizing only text-query-image ranking examples or image-query-text ranking examples) as training examples, which do not make full use of bi-directional ranking examples (bi-directional ranking means that both text-query-image and image-query-text ranking examples are utilized in the training period) to achieve a better performance. In this paper, we consider learning a cross-media representation model from the perspective of optimizing a listwise ranking problem while taking advantage of bi-directional ranking examples. We propose a general cross-media ranking algorithm to optimize the bi-directional listwise ranking loss with a latent space embedding, which we call Bi-directional Cross-Media Semantic Representation Model (Bi-CMSRM). The latent space embedding is discriminatively learned by the structural large margin learning for optimization with certain ranking criteria (mean average precision in this paper) directly. We evaluate Bi-CMSRM on the Wikipedia and NUS-WIDE datasets and show that the utilization of the bi-directional ranking examples achieves a much better performance than only using the uni-directional ranking examples.","cites":"13","conferencePercentile":"90.66666667"},{"venue":"ACM Multimedia","id":"3c20bc3896276f87fa5f9ae6b4bab460bd5335e4","venue_1":"ACM Multimedia","year":"2012","title":"Annotating web images using NOVA: NOn-conVex group spArsity","authors":"Fei Wu, Ying Yuan, Yong Rui, Shuicheng Yan, Yueting Zhuang","author_ids":"1695826, 3112325, 1728806, 1698982, 1755711","abstract":"As image feature vector is large, selecting the right features plays a fundamental role in Web image annotation. Most existing approaches are either based on individual feature selection, which leads to local optima, or using a <i>convex</i> penalty, which leads to inconsistency. To address these difficulties, in this paper we propose a new sparsity-based approach NOVA (NOn-conVex group spArsity). To the best of our knowledge, NOVA is the first to introduce non-convex penalty for group selection in high-dimensional heterogeneous features space. Because it is a group-sparsity approach, it approximately reaches global optima. Because it uses non-convex penalty, it achieves the consistency. We demonstrate the superior performance of NOVA via three means. First, we present theoretical proof that NOVA is consistent, satisfying un-biasness, sparsity and continuity. Second, we show NOVA converges to the true underlying model by using a ground-truth-available generative-model simulation. Third, we report extensive experimental results on three diverse and widely-used data sets Kodak, MSRA-MM 2.0, and NUS-WIDE. We also compare NOVA against the state-of-the-art approaches, and report superior experimental results.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"78d1f388a39b601fcbe2b57a7a21510b745fd818","venue_1":"ACM Multimedia","year":"2016","title":"Discriminative Paired Dictionary Learning for Visual Recognition","authors":"Hui-Hung Wang, Yi-Ling Chen, Chen-Kuo Chiang","author_ids":"1836228, 5014321, 1713453","abstract":"A Paired Discriminative K-SVD (PD-KSVD) dictionary learning method is presented in this paper for visual recognition. To achieve high discrimination and low reconstruction errors simultaneously for sparse coding, we propose to learn class-specific sub-dictionaries from pairs of positive and negative classes to jointly reduce the reconstruction errors of positive classes while keeping the reconstruction errors of negative classes high. Then, multiple sub-dictionaries are concatenated with respect to the same negative class so that the non-zero sparse coefficients can be discriminatively distributed to improve classification accuracy. Compared to the current dictionary learning methods, the proposed PD-KSVD method achieves very competitive performance in a variety of visual recognition tasks on several publicly available datasets.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"ed96a661edba875065e7d65641cf00fff8a1dc87","venue_1":"ACM Multimedia","year":"2010","title":"Multi-layer stereo video matting: video matting","authors":"M. Jiang, Danny Crookes, Min Chen","author_ids":"8333311, 1691478, 1711628","abstract":"In this paper, an unsupervised scheme for stereo video matting is presented, where stereo motion analysis is combined to provide an automatic multi-layer clustering scheme of alpha components. With this multi-layer matting scheme, objects in both foreground and background can be extracted for background substitution. The experiment shows that the proposed scheme can have a better performance in term of automatic grouping of alpha components in comparison with min-cut matte grouping.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"62e296cd00372d8c88dcb288c3f3b22e0df1d44d","venue_1":"ACM Multimedia","year":"2006","title":"Human-centered design meets cognitive load theory: designing interfaces that help people think","authors":"Sharon L. Oviatt","author_ids":"2807460","abstract":"Historically, the development of computer systems has been primarily a technology-driven phenomenon, with technologists believing that \"users can adapt\" to whatever they build. <i>Human-centered design</i> advocates that a more promising and enduring approach is to model users' natural behavior to begin with so that interfaces can be designed that are more intuitive, easier to learn, and freer of performance errors. In this paper, we illustrate different user-centered design principles and specific strategies, as well as their advantages and the manner in which they enhance users' performance. We also summarize recent research findings from our lab comparing the performance characteristics of different educational interfaces that were based on user-centered design principles. One theme throughout our discussion is human-centered design that <i>minimizes users' cognitive load</i>, which effectively frees up mental resources for performing better while also remaining more attuned to the world around them.","cites":"41","conferencePercentile":"91.19170984"},{"venue":"ACM Multimedia","id":"141b0828df1e4ceb13ef3c77d7a25f845f16f4c7","venue_1":"ACM Multimedia","year":"1994","title":"Two-Dimensional Scaling Techniques for Adaptive, Rate-Based Transmission Control of Live Audio and Video Streams","authors":"Terry Talley, Kevin Jeffay","author_ids":"2108713, 1692620","abstract":"One of the major obstacles facing designers of video conferencing systems is the problem of ameliorating the effects of congestion on interconnected packet-switched networks that do not support real-time communication. We present a framework for transmission control that describes the current network environment as a set of sustainable bit and packet transmission-rate combinations and show that adaptively scaling both the bit and packet-rate of the audio and video streams can reduce the impact of congestion. We empirically demonstrate the validity of adapting <italic>both</italic> packet and bit-rate using a simple feedback mechanism and simple adaptation heuristics to deliver audio and video streams suitable for low-latency, high-fidelity playout.","cites":"29","conferencePercentile":"69.49152542"},{"venue":"ACM Multimedia","id":"f078eef611567d088f2ea3699537cfb2be9bb3d6","venue_1":"ACM Multimedia","year":"2015","title":"Giggler: An Intuitive, Real-Time Integrated Wireless In-Ear Monitoring and Personal Mixing System using Mobile Devices","authors":"Andries Valstar, Min-Chieh Hsiu, Te-Yen Wu, Mike Y. Chen","author_ids":"3326835, 2205744, 1983696, 2335746","abstract":"For live music performances, current Wireless In-Ear Monitoring and Personal Mixing setups require a lot of equipment and wiring. This paper introduces Giggler, a system that makes a Wireless In-Ear Monitoring and Personal Mixing experience, easier to setup, easier to use, faster to control and more accessible for musicians than conventional systems by integrating all equipment into one mobile device per musician. The results of the two user studies that we conducted show that Giggler's User Interface performs up to more than twice as fast as a traditional mixer and indicate that Giggler outperforms a Physical mixer because visual features are added to streamline the channel identification process.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"6506214d8ef732ff6069a0a37978de0dd9e133c6","venue_1":"ACM Multimedia","year":"2012","title":"Robust stroke-based video animation via layered motion and correspondence","authors":"Tao Lin, Liang Lin, Qing Wang","author_ids":"1719679, 1737218, 7135234","abstract":"This paper investigates a novel approach to reduce artifacts and visual flickering in generating painterly animations from real video clips. In the traditional painterly animation methods, the brush strokes are propagated over video frames by calculating optical flows, and the visual impression of animations are severely affected by incorrect correspondences. In our method, we combine motion segmentation and occlusion handing to establish accurate dense feature correspondences, which is shown to robust propagate brush strokes against complex motions and occlusions. Moreover, a beforehand rendering strategy is presented to alleviate stroke flickering. In the experiments, we generate a number of animations in cartoon and oil painting style. The quantitative evaluations of brush stabilization is presented as well.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"8a25a2f5de2c4bace455b058e36dc5200427d7ae","venue_1":"ACM Multimedia","year":"2012","title":"Interactive multimodal social robot for improving quality of care of elderly in Australian nursing homes","authors":"Rajiv Khosla, Mei-Tai Chu, Reza Kachouie, Keiji Yamada, Fujita Yoshihiro, Tomoharu Yamaguchi","author_ids":"1799561, 1790280, 3117368, 1808643, 2475818, 1802249","abstract":"This paper describes the design of multimodal robotic system, embodiment of multimodal interaction (voice, gestures, emotion, touch panel and dance) in assistive social robot (Matilda) for modeling group based and one-to-one interactions with technology adverse elderly in nursing homes. It describes the human-centered evaluation of Matilda based on quality, naturalness, user satisfaction and predictive accuracy based on first ever field trials in Australia. The multimodal social robots have facilitated breaking the technology barriers with the elderly leading to several aged care facilities and community centers showing interest in future trials.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"61b17f719bab899dd50bcc3be9d55673255fe102","venue_1":"ACM Multimedia","year":"2016","title":"Detecting Sarcasm in Multimodal Social Platforms","authors":"Rossano Schifanella, Paloma de Juan, Joel R. Tetreault, Liangliang Cao","author_ids":"2251027, 2616006, 1739099, 2464399","abstract":"Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the implied sentiment. The detection of sarcasm in social media platforms has been applied in the past mainly to textual utterances where lexical indicators (such as interjections and intensifiers), linguistic markers, and contextual information (such as user profiles, or past conversations) were used to detect the sarcastic tone. However, modern social media platforms allow to create multimodal messages where audiovisual content is integrated with the text, making the analysis of a mode in isolation partial. In our work, we first study the relationship between the textual and visual aspects in multimodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to quantify the extent to which images are perceived as necessary by human annotators. Moreover, we propose two different computational frameworks to detect sarcasm that integrate the textual and visual modalities. The first approach exploits visual semantics trained on an external dataset, and concatenates the semantics features with state-of-the-art textual features. The second method adapts a visual neural network initialized with parameters trained on ImageNet to multimodal sarcastic posts. Results show the positive effect of combining modalities for the detection of sarcasm across platforms and methods.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"4750b72b4735354429f544673fd50bd415cddf1b","venue_1":"ACM Multimedia","year":"2007","title":"Can we trust digital image forensics?","authors":"Thomas Gloe, Matthias Kirchner, Antje Winkler, Rainer Böhme","author_ids":"2476320, 2113490, 2083079, 1748910","abstract":"Compared to the prominent role digital images play in nowadays multimedia society, research in the field of image authenticity is still in its infancy. Only recently, research on digital image forensics has gained attention by addressing tamper detection and image source identification. However, most publications in this emerging field still lack rigorous discussions of robustness against strategic counterfeiters, who anticipate the existence of forensic techniques. As a result, the question of trustworthiness of digital image forensics arises. This work will take a closer look at two state-of-the-art forensic methods and proposes two counter-techniques; one to perform resampling operations undetectably and another one to forge traces of image origin. Implications for future image forensic systems will be discussed.","cites":"62","conferencePercentile":"95.83333333"},{"venue":"ACM Multimedia","id":"fcdebe6f4c216849e7273d343c7868a843f8c117","venue_1":"ACM Multimedia","year":"2008","title":"Randomized sub-vectors hashing for high-dimensional image feature matching","authors":"Heng Yang, Qing Wang, Zhoucan He","author_ids":"2389837, 7135234, 2061790","abstract":"High-dimensional image feature matching is an important part of many image matching based problems in computer vision which are solved by local invariant features. In this paper, we propose a new indexing/searching method based on Randomized Sub-Vectors Hashing (called RSVH) for high-dimensional image feature matching. The essential of the proposed idea is that the feature vectors are considered similar (measured by Euclidean distance) when the L2 norms of their corresponding randomized sub-vectors are approximately same respectively. Experimental results have demonstrated that our algorithm can perform much better than the famous BBF (Best-Bin-First) and LSH (Locality Sensitive Hashing) algorithms in extensive image matching and image retrieval applications.","cites":"4","conferencePercentile":"43.11926605"},{"venue":"ACM Multimedia","id":"bd41ce7362259dea758d84c219eeb0dd2f40337c","venue_1":"ACM Multimedia","year":"2016","title":"Multimedia COMMONS Workshop 2016 (MMCommons 2016): Datasets, Evaluation, and Reproducibility","authors":"Bart Thomee, Damian Borth, Julia Bernd","author_ids":"2463875, 1772549, 2923708","abstract":"Leveraged wisely, new datasets can inspire new multimedia methods and algorithms, as well as catalyze innovations in how their efficacy, efficiency, and generalizability can be evaluated. The availability of very large multimedia datasets like the Yahoo-Flickr Creative Commons 100 Million has offered unique opportunities for advancing the state of the art in multimedia processing, analysis, search, and visualization. The Multimedia Commons Initiative has been developing a community around the YFCC100M, including associated annotation and evaluation efforts. In addition to research in several multimedia subfields, including computer vision, image processing, and video content analysis, the YFCC100M and Multimedia Commons resources have been used in various competitions and benchmarks, such as the MediaEval Placing Task and the ACM Multimedia Grand Challenge competition. With additional annotation and curation, the data has the potential to enable major leaps forward in research.\n As use of the YFCC100M and the Multimedia Commons resources broadens across the multimedia community, the MMCommons'16 workshop offered an opportunity for researchers to share new research results, compare approaches, and coordinate efforts to maximize the scientific benefit of the initiative. In particular, the Multimedia Commons data has the potential to provide both inspiration and concrete resources to pursue some important \"meta-research\" questions, such as how to measure the scalability, generalizability, and reproducibility of methods across datasets, whether we need to rethink our evaluation paradigms as the field moves in new directions, and how annotation strategies affect the impact of benchmarks and data challenges using that data.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"647977dfba7ab6ea8bd006faaf22200c0e364e23","venue_1":"ACM Multimedia","year":"2001","title":"Bandwidth allocation in a self-managing multimedia file server","authors":"Vijay Sundaram, Prashant J. Shenoy","author_ids":"3001209, 1705052","abstract":"In this paper, we argue that manageability of file servers is just as important, if not more, as performance. We focus on the design of a self-managing file server and address the specific problem of automating bandwidth allocation to application classes in single-disk and multi-disk servers. The bandwidth allocation techniques that we propose consists of two key components: a workload monitoring module that efficiently monitors the load in each application class and a bandwidth manager that uses these workload statistics to dynamically determine the allocation of each class. We evaluate the efficacy of our techniques via a simulation study and demonstrate that our techniques (i) exploit the semantics of each application class while determining their allocations, (ii) provide control over the time-scale of monitoring and allocation, and (iii) provide stable behavior even during transient overloads. Our comparison with a static allocation technique shows that dynamic bandwidth allocation can yield queue lengths that are 59% smaller during overloads and admit a larger number of soft real-time clients into the system.","cites":"3","conferencePercentile":"33.67346939"},{"venue":"ACM Multimedia","id":"37866fea39deeff453802cde529dd9d32e0205a5","venue_1":"ACM Multimedia","year":"2012","title":"Sense beauty via face, dressing, and/or voice","authors":"Tam V. Nguyen, Si Liu, Bingbing Ni, Jun Tan, Yong Rui, Shuicheng Yan","author_ids":"1804177, 2777248, 2670268, 1712333, 1728806, 1698982","abstract":"Discovering the secret of beauty has been the pursuit of artists and philosophers for centuries. Nowadays, the computational model for beauty estimation has been actively explored in computer science community, yet with the focus mainly on facial features. In this work, we perform a comprehensive study of female attractiveness conveyed by single/multiple modalities of cues, i.e., face, dressing and/or voice, and aim to uncover how different modalities individually and collectively affect the human sense of beauty. To this end, we collect the first Multi-Modality Beauty (M<sup>2</sup>B) dataset in the world for female attractiveness study, which is thoroughly annotated with attractiveness levels converted from manual <i>k</i>-wise ratings and semantic attributes of different modalities. A novel Dual-supervised Feature-Attribute-Task (DFAT) network is proposed to jointly learn the beauty estimation models of single/multiple modalities as well as the attribute estimation models. The DFAT network differentiates itself by its supervision in both attribute and task layers. Several interesting beauty-sense observations over single/multiple modalities are reported, and the extensive experimental evaluations on the collected M2B dataset well demonstrate the effectiveness of the proposed DFAT network for female attractiveness estimation.","cites":"11","conferencePercentile":"87.02531646"},{"venue":"ACM Multimedia","id":"165084240a83fce623c63bea533441f5fdd80a75","venue_1":"ACM Multimedia","year":"2000","title":"Application performance in the QLinux multimedia operating system","authors":"Vijay Sundaram, Abhishek Chandra, Pawan Goyal, Prashant J. Shenoy, Jasleen Sahni, Harrick M. Vin","author_ids":"3001209, 4300423, 1804259, 1705052, 2406354, 8734926","abstract":"<i>In this paper, we argue that conventional operating systems need to be enhanced with predictable resource management mechanisms to meet the diverse performance requirements of emerging multimedia and web applications. We present QLinux&#8212;a multimedia operating system based on the Linux kernel that meets this requirement. QLinux employs hierarchical schedulers for fair, predictable allocation of processor, disk and network bandwidth, and accounting mechanisms for appropriate charging of resource usage. We experimentally evaluate the efficacy of these mechanisms using benchmarks and real-world applications. Our experimental results show that (i) emerging applications can indeed benefit from predictable allocation of resources, and (ii) the overheads imposed by the resource allocation mechanisms in QLinux are small. For instance, we show that the QLinux CPU scheduler can provide predictable performance guarantees to applications such as web servers and MPEG players, albeit at the expense of increasing the scheduling overhead. We conclude from our experiments that the benefits due to the resource management mechanisms in QLinux outweigh their increased overheads, making them a practical choice for conventional operating systems.</i>","cites":"56","conferencePercentile":"92.93478261"},{"venue":"ACM Multimedia","id":"361ddad9516449260b027a9891780aac1c136a6b","venue_1":"ACM Multimedia","year":"1999","title":"Architectural considerations for next generation file systems","authors":"Prashant J. Shenoy, Pawan Goyal, Harrick M. Vin","author_ids":"1705052, 1804259, 8734926","abstract":"We evaluate two architectural alternatives&#8212;partitioned and integrated&#8212;for designing next generation file systems. Whereas a partitioned server employs a separate file system for each application class, an integrated file server multiplexes its resources among all application classes; we evaluate the performance of the two architectures with respect to sharing of disk bandwidth among the application classes. We show that although the problem of sharing disk bandwidth in integrated file systems is conceptually similar to that of sharing network link bandwidth in integrated services networks, the arguments that demonstrate the superiority of integrated services networks over separate networks are not applicable to file systems. Furthermore, we show that: (i) an integrated server outperforms the partitioned server in a large operating region and has slightly worse performance in the remaining region, (ii) the capacity of an integrated server is larger than that of the partitioned server, and (iii) an integrated server outperforms the partitioned server by up to a factor of 6 in the presence of bursty workloads.","cites":"22","conferencePercentile":"72.26890756"},{"venue":"ACM Multimedia","id":"6dbf3584b5523ac4aab6846decb6260e9077e48d","venue_1":"ACM Multimedia","year":"1995","title":"Efficient Support for Scan Operations in Video Servers","authors":"Prashant J. Shenoy, Harrick M. Vin","author_ids":"1705052, 8734926","abstract":"In this paper, we present an algorithm that integrates scal-able compression techniques with placement algorithms for disk-arrays to provide efficient support for interactive scan operations (i.e., fast-forward and rewind) in video servers. We demonstrate that by suitably exploiting the characteristics of video streams and human perceptual tolerances, the overhead of such interactive operations can be substantially reduced. We present an analytical model for evaluating the impact of the fast-forward operation on the performance of the disk-array-based server. We validate the model through extensive simulations and analyze our results.","cites":"39","conferencePercentile":"65"},{"venue":"ACM Multimedia","id":"139dc8b6344e9b24dec5f13efba2a10899c2eaf4","venue_1":"ACM Multimedia","year":"2012","title":"Exploring and browsing photos through characteristic geographic tag regions","authors":"Bart Thomee, Adam Rae","author_ids":"2463875, 2441217","abstract":"We present a system that supports zoomable browsing and exploration of photos taken across the globe. Our system is based on a novel algorithm that automatically uncovers the colloquial boundaries of regions that are characteristic for individual tags used in a large collection of geo-referenced photos. We first model the data using scale-space theory, which allows us to represent it simultaneously across different scales as a family of increasingly smoothed density distributions, after which we derive the region boundaries by applying image analysis techniques to the scale-space representation of each tag. The interface visualizes the shape and size of the resulting boundaries for each tag along the dimensions of space and time across multiple scales, giving the user the ability to explore the world as patchwork of dynamic characterizing geographic tag regions and to browse through their associated photos.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"070e09bd6d30c1dd8e114c11f1cab14548e2e05e","venue_1":"ACM Multimedia","year":"2016","title":"A Fast Cattle Recognition System using Smart devices","authors":"Santosh Kumar, Sanjay Kumar Singh, Tanima Dutta, Hari Prabhat Gupta","author_ids":"1736293, 2172955, 1778257, 2414374","abstract":"A recognition system is very useful to recognize human, object, and animals. An animal recognition system plays an important role in livestock biometrics, that helps in recognition and verification of livestock in case of missed or swapped animals, false insurance claims, and reallocation of animals at slaughter houses. In this research, we propose a fast and cost-effective animal biometrics based cattle recognition system to quickly recognize and verify the false insurance claims of cattle using their primary muzzle point image pattern characteristics. To solve this major problem, users (owner, parentage, or other) have captured the images of cattle using their smart devices. The captured images are transferred to the server of the cattle recognition system using a wireless network or internet technology. The system performs pre-processing on the muzzle point image of cattle to remove and filter the noise, increases the quality, and enhance the contrast. The muzzle point features are extracted and supervised machine learning based multi-classifier pattern recognition techniques are applied for recognizing the cattle. The server has a database of cattle images which are provided by the owners. Finally, One-Shot-Similarity (OSS) matching and distance metric learning based techniques with ensemble of classifiers technique are used for matching the query muzzle image with the stored database.A prototype is also developed for evaluating the efficacy of the proposed system in term of recognition accuracy and end-to-end delay.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"3e409e456cee4cc718a669c117ba0144462c9b76","venue_1":"ACM Multimedia","year":"2012","title":"MOGAT: a cloud-based mobile game system with auditory training for children with cochlear implants","authors":"Yinsheng Zhou, Toni-Jan Keith Palma Monserrat, Ye Wang","author_ids":"3128975, 2375934, 1681196","abstract":"Musical auditory habilitation is an essential process in adapting cochlear implant recipients to the musical hearing context provided by cochlear implants. However, due to the cost and time limitation, it is impossible for hearing healthcare professionals to provide intensive and extensive musical auditory habilitation for every cochlear implant recipient. In order to provide an efficient and cost-effective musical auditory training for children with cochlear implants, we designed and developed MObile Games with Auditory Training (MOGAT) on off-the-shelf mobile devices. MOGAT includes three intuitive and interesting mobile games for training pitch perception and production, and a cloud-based web service for music therapists to support and evaluate individual habilitation. We demonstrate MOGAT for enhancing musical habilitation for children with cochlear implants.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"1a09bd922ee31d61d433d8a7c8bc37ad7628f730","venue_1":"ACM Multimedia","year":"2009","title":"Deep exploration for experiential image retrieval","authors":"Bart Thomee, Mark J. Huiskes, Erwin M. Bakker, Michael S. Lew","author_ids":"2463875, 1753110, 2111311, 1731570","abstract":"Experiential image retrieval systems aim to provide the user with a natural and intuitive search experience. The goal is to empower the user to navigate large collections based on his own needs and preferences, while simultaneously providing him with an accurate sense of what the database has to offer. In this paper we integrate a new browsing mechanism called deep exploration with the proven technique of retrieval by relevance feedback. In our approach, relevance feedback focuses the search on relevant regions, while deep exploration facilitates transparent navigation to promising regions of feature space that would normally remain unreachable. Optimal feature weights are determined automatically based on the evidential support for the relevance of each single feature. To achieve efficient refinement of the search space, images are ranked and presented to the user based on their likelihood of being useful for further exploration.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"59dcd0802886354c08da5cecaa3ef8a240f7c475","venue_1":"ACM Multimedia","year":"2014","title":"SmartVisio: Interactive Sketch Recognition with Natural Correction and Editing","authors":"Jie Wu, Changhu Wang, Liqing Zhang, Yong Rui","author_ids":"1717209, 1697065, 7137826, 1728806","abstract":"In this work, we introduce the SmartVisio system for interactive hand-drawn shape/diagram recognition. Different from existing work, SmartVisio is a real-time sketch recognition system based on Visio, to recognize hand-drawn flowchart/diagram with flexible interactions. This system enables a user to draw shapes or diagrams on the Visio interface, and then the hand-drawn shapes are automatically converted to formal shapes in real-time. To satisfy the interaction needs from common users, we propose an algorithm to detect a user's correction and editing during drawing, and then recognize in real time. We also propose a novel symbol recognition algorithm to better recognize or differentiate some visually similar shapes. By enabling users' natural correction/editing on various shapes, our system makes flowchart/diagram production much more natural and easier.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"15c7bc03276248312b78ba5e8c08c7516eb7fb41","venue_1":"ACM Multimedia","year":"2004","title":"Affinity relation discovery in image database clustering and content-based retrieval","authors":"Mei-Ling Shyu, Shu-Ching Chen, Min Chen, Chengcui Zhang","author_ids":"1693826, 1705664, 1711628, 1736783","abstract":"In this paper, we propose a unified framework, called &#60;i>Markov Model Mediator&#60;/i> (MMM), to facilitate image database clustering and to improve the query performance. The structure of the MMM framework consists of two hierarchical levels: local MMMs and integrated MMMs, which model the affinity relations among the images within a single image database and within a set of image databases, respectively, via an effective data mining process. The effectiveness and efficiency of the MMM framework for database clustering and image retrieval are demonstrated over a set of image databases which contain various numbers of images with different dimensions and concept categories.","cites":"7","conferencePercentile":"53.43137255"},{"venue":"ACM Multimedia","id":"cb367796f36868ee8fc94dd1842cdf17532d12f5","venue_1":"ACM Multimedia","year":"2010","title":"Multimedia content creation using societal-scale ubiquitous camera networks and human-centric wearable sensing","authors":"Mathew Laibowitz, Nan-Wei Gong, Joseph A. Paradiso","author_ids":"1740678, 1690147, 4798651","abstract":"We present a novel approach to the creation of user-generated, documentary video using a distributed network of sensor-enabled video cameras and wearable on-body sensor devices. The wearable sensors are used to identify the subjects in view of the camera system and label the captured video with real-time human-centric social and physical behavioral information. With these labels, massive amounts of continually recorded video can be browsed, searched, and automatically stitched into cohesive multimedia content. This system enables naturally occurring human behavior to drive and control a multimedia content creation system in order to create video output that is understandable, informative, and/or enjoyable to its human audience. The collected sensor data is further utilized to enhance the created multimedia content such as by using the data to edit and/or generate audio score, determine appropriate pacing of edits, and control the length and type of audio and video transitions directly from the content of the captured media. We present the design of the platform, the design of the multimedia content creation application, and the evaluated results from several live runs of the complete system.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"1154a28a2aa814656c21e8eb0d994ac525ce9c73","venue_1":"ACM Multimedia","year":"2001","title":"Designing annotation before it's needed","authors":"Frank Nack, Wolfgang Putz","author_ids":"1679840, 2319731","abstract":"This paper considers the automated and semi-automated annotation of audiovisual media in a new type of production framework, A4SM (Authoring System for Syntactic, Semantic and Semiotic Modelling). We present the architecture of the framework and outline the underlying XML-Schema based content description structures of A4SM. We then describe tools for a news and demonstrate how video material can be annotated in real time and how this information can not only be used for retrieval but also can be used during the different phases of the production process itself. Finally, we discuss the pros and cons of our approach of evolving semantic networks as the basis for audio- visual content description.","cites":"40","conferencePercentile":"89.79591837"},{"venue":"ACM Multimedia","id":"f56abb1c2004413bd65a2dcb2f8bb69bc4d4b6e5","venue_1":"ACM Multimedia","year":"2011","title":"A mobile interactive robot for gathering structured social video","authors":"Alexander Reben, Joseph A. Paradiso","author_ids":"2303668, 4798651","abstract":"Documentaries are typically captured in a very structured way, using teams to film and interview people. We developed an autonomous method for capturing structured cin&#233;ma v&#233;rit&#233; style documentaries through an interactive robotic camera, which was used as a mobile physical agent to facilitate interaction and story gathering within a ubiquitous media framework. We sent this robot out to autonomously gather human narrative about its environment. The robot had a specific story capture goal and leveraged humans to attain that goal. The robot collected a 1st person view of stories unfolding in real life, and as it engaged with its subjects via a preset dialog, these media clips were intrinsically structured. We evaluated this agent by way of determining \"complete\" vs. \"incomplete\" interactions. \"Complete\" interactions were those that generated viable and interesting videos, which could be edited together into a larger narrative. It was found that 30% of the interactions captured were \"complete\" interactions. Our results suggested that changes in the system would only produce incrementally more \"complete\" interactions, as external factors like natural bias or busyness of the user come into play. The types of users who encountered the robot were fairly polar; either they wanted to interact or did not - very few partial interactions went on for more than 1 minute. Users who partially interacted with the robot were found to treat it rougher than those who completed the full interaction. It was also determined that this type of limited-interaction system is best suited for short-term encounters. At the end of the study, a short cin&#233;ma v&#233;rit&#233; documentary showcasing the people and activity in our building was easily produced from the structured videos that were captured, indicating the utility of this approach.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"536a586f7b857e4e0845a17c4f4522861456a0ed","venue_1":"ACM Multimedia","year":"2011","title":"Towards low bit rate mobile visual search with multiple-channel coding","authors":"Rongrong Ji, Ling-Yu Duan, Jie Chen, Hongxun Yao, Yong Rui, Shih-Fu Chang, Wen Gao","author_ids":"1725599, 7667912, 1735702, 1720100, 1728806, 1735547, 3406319","abstract":"In this paper, we propose a multiple-channel coding scheme to extract compact visual descriptors for low bit rate mobile visual search. Different from previous visual search scenarios that send the query image, we make use of the ever growing mobile computational capability to directly extract compact visual descriptors at the mobile end. Meanwhile, stepping forward from the state-of-the-art compact descriptor extractions, we exploit the rich contextual cues at the mobile end (such as GPS tags for mobile visual search and 2D barcodes or RFID tags for mobile product search), together with the visual statistics at the reference database, to learn multiple coding channels. Therefore, we describe the query with one of many forms of high-dimensional visual signature, which is subsequently mapped to one or more channels and compressed. The compression function within each channel is learnt based on a novel robust PCA scheme, with specific consideration to preserve the retrieval ranking capability of the original signature. We have deployed our scheme on both iPhone4 and HTC DESIRE 7 to search ten million landmark images in a low bit rate setting. Quantitative comparisons to the state-of-the-arts demonstrate our significant advantages in descriptor compactness (with orders of magnitudes improvement) and retrieval mAP in mobile landmark, product, and CD/book cover search.","cites":"27","conferencePercentile":"96.64723032"},{"venue":"ACM Multimedia","id":"13fa06d6a5155a58164d552227d7f7602f3dbb8c","venue_1":"ACM Multimedia","year":"2010","title":"Modeling 3D facial expressions using geometry videos","authors":"Jiazhi Xia, Ying He, Dao Thi Phuong Quynh, XiaoMing Chen, Steven C. H. Hoi","author_ids":"1685764, 1734129, 2592951, 1772182, 1741126","abstract":"The significant advances in developing high-speed shape acquisition devices make it possible to capture the moving and deforming objects at video speeds. However, due to its complicated nature, it is technically challenging to effectively model and store the captured motion data. In this paper, we present a set of algorithms to construct geometry videos for 3D facial expressions, including hole filling, geodesic-based face segmentation, and expression-invariant parametrization. Our algorithms are efficient and robust, and can guarantee the exact correspondence of the salient features (eyes, mouth and nose). Geometry video naturally bridges the 3D motion data and 2D video, and provides a way to borrow the well-studied video processing techniques to motion data processing. With our proposed intra-frame prediction scheme based on H.264/AVC, we are able to compress the geometry videos into a very compact size while maintaining the video quality. Our experimental results on real-world datasets demonstrate that geometry video is effective for modeling the high-resolution 3D expression data.","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"87b7625fbfdf14c899eb061f5f733a4a384bc3d6","venue_1":"ACM Multimedia","year":"2007","title":"Correlative multi-label video annotation","authors":"Guo-Jun Qi, Xian-Sheng Hua, Yong Rui, Jinhui Tang, Tao Mei, HongJiang Zhang","author_ids":"7266598, 1746102, 1728806, 8053308, 1788123, 1718558","abstract":"Automatically annotating concepts for video is a key to semantic-level video browsing, search and navigation. The research on this topic evolved through two paradigms. The first paradigm used binary classification to detect each individual concept in a concept set. It achieved only limited success, as it did not model the inherent correlation between concepts, e.g., urban and building. The second paradigm added a second step on top of the individual concept detectors to fuse multiple concepts. However, its performance varies because the errors incurred in the first detection step can propagate to the second fusion step and therefore degrade the overall performance. To address the above issues, we propose a third paradigm which simultaneously classifies concepts and models correlations between them in a single step by using a novel <i>Correlative Multi-Label</i> (CML) framework. We compare the performance between our proposed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. We report superior performance from the proposed approach.","cites":"184","conferencePercentile":"98.4375"},{"venue":"ACM Multimedia","id":"6c0267370778d000ece98219ca8e2d5d949b37a0","venue_1":"ACM Multimedia","year":"2002","title":"Experiences in the design of the well, a group communication device for teleconviviality","authors":"Nicolas Roussel","author_ids":"1728921","abstract":"Over the last forty years, a number of audiovisual systems have been proposed to allow people to communicate over distance. However, although these systems have greatly improved in their ability to support formal meetings, they are still hardly usable for the informal discussions that take place before and after the meeting or during breaks. This paper presents the <i>well</i>, a group communication device that combines audio and video links with an original design to support <i>teleconviviality</i>, the emergence of a relaxed atmosphere well adapted to distributed informal communication. The <i>well</i> is not intended to replace existing video-conferencing systems, but rather to supplement them as a solution to the informal communication problem. After introducing some related work, we provide an overview of the design concept of the <i>well</i>. We then present some details about its hardware configuration and the video compositing software it uses. Finally, we discuss some lessons learned from this work and conclude with directions for future research.","cites":"15","conferencePercentile":"65.38461538"},{"venue":"ACM Multimedia","id":"0f7159320e20adcd2f31aa63c45da8d6143da42e","venue_1":"ACM Multimedia","year":"2007","title":"Trajectory based event tactics analysis in broadcast sports video","authors":"Guangyu Zhu, Qingming Huang, Changsheng Xu, Yong Rui, Shuqiang Jiang, Wen Gao, Hongxun Yao","author_ids":"4120574, 1689702, 1688633, 1728806, 1696610, 3406319, 1720100","abstract":"Most of existing approaches on event detection in sports video are general audience oriented. The extracted events are then presented to the audience without further analysis. However, professionals, such as soccer coaches, are more interested in the tactics used in the events. In this paper, we present a novel approach to extract tactic information from the goal event in broadcast soccer video and present the goal event in a tactic mode to the coaches and sports professionals. We first extract goal events with far-view shots based on analysis and alignment of web-casting text and broadcast video. For a detected goal event, we employ a multi-object detection and tracking algorithm to obtain the players and ball trajectories in the shot. Compared with existing work, we proposed an effective tactic representation called aggregate trajectory which is constructed based on multiple trajectories using a novel analysis of temporal-spatial interaction among the players and the ball. The interactive relationship with play region information and hypothesis testing for trajectory temporal-spatial distribution are exploited to analyze the tactic patterns in a hierarchical coarse-to-fine framework. The experimental results on the data of FIFA World Cup 2006 are promising and demonstrate our approach is effective.","cites":"41","conferencePercentile":"90.88541667"},{"venue":"ACM Multimedia","id":"1a5c76ecc1ac21b659f1e369c0d14454ab03c0e7","venue_1":"ACM Multimedia","year":"2005","title":"MobiLenin combining a multi-track music video, personal mobile phones and a public display into multi-user interactive entertainment","authors":"Jürgen Scheible, Timo Ojala","author_ids":"2150981, 7967584","abstract":"This paper introduces a novel and creative approach for coupling multimedia art with a non-conventional distributed human-computer interface for multi-user interactive entertainment. The proposed MobiLenin system allows a group of people to interact simultaneously with a multi-track music video shown on a large public display using their personal mobile phones, effectively empowering the group with the joint authorship of the video. The system is realized with a client-server architecture which includes server-driven real-time control of the client UI to guarantee ease of use and a lottery mechanism as an incentive for interaction. Our analysis of the findings of an empirical user evaluation conducted in a true environment of use shows that the MobiLenin system is successful, addressing many of the challenges identified in the literature. The proposed system offers a new form of interactive entertainment for pubs and other public places, and the underlying architecture provides a framework for realizing similar installations with different types of multimedia content.","cites":"37","conferencePercentile":"88.36633663"},{"venue":"ACM Multimedia","id":"bc9f877d226c7f9f2afcea38904777547ef9995c","venue_1":"ACM Multimedia","year":"1998","title":"On Caching and Prefetching of Virtual Objects in Distributed Virtual Environments","authors":"Jimmy H. P. Chim, Mark Green, Rynson W. H. Lau, Hong Va Leong, Antonio Si","author_ids":"1919676, 3081643, 1726262, 1714454, 1715919","abstract":"ld~~ces in networkg tetiology and the mtabkhtnent of the Morrnation Superhighway have rendered the virtnd Xbrary a concrete possibfi~. ?iTeze currently investigating user &\\Terience in _ through a large virtual environment in the contti% of bternet. This provid~ users with the abiity to view t+ous virtual objects from tirent & t anca and angles, using common web browsers. To dtiver a good petiorrnance for such applications, we need to addr~s Several issues in Merent resemch discipbes. F&t., we must be able to modd virtual objects tiec-tivdy. The recently devdoped techniques for mdti-~olution object modting in computer graphics are of great Aue here, since they are capable of sintp~g the object mod-& and therefore reducing the time to render them. Secon& tith the Eted bandwidth constraint of the btemet, we need to reduce the response time by reducing the amount of data requwted over the network One dtemative is to cache object mod& of high-w. Prefetching object mod~ by predicting those which ~e &dy to be used in the near fn-ture and dotioading them in advance wiU lead to a Mar improvement. Third, the bt.ernet often tiers from discon-nection. A caching mechanism hat dews objects to be cached n?th at least the-mminimum resolution m be we~ to provide at least a coarse x<ew of the objects to a disconnected >tiewerfor improved ~hd perception. k this paper, m propose a multi-T~*oZutionaching m~hrmtim wd invG tigate its tiectiveness in supporthg virtud w~u~ ap-FEcations in the ktemet. environment. The caching mechanism is further complemented with severs object prefetching m echanLms for predicting future acc~ed objects. The performance of our proposed me~ and their feasibfities are qnsmtXed via s \" mtdat.ed ~Terirnents. I introduti \" on Recmt &tab-at of the ~trorld \\Kde J5kb &truc-tme [2] has brought about a revolution change to the orghation and pr=entation of information. The technol-OD has ~owed mtiti-media information to be ac~ed on Perntkslon to maked~gi~al orhardcopi= of allorpanof Ifsii work for personalor cl=sroom use is granted wlthou! fee prov]ded that copies are not made or distributed for profitor commercial adwnxage, andthat cop]esbearlhis nol]ceandthefull citationon thefirst page.TOcopy othen';~%to republkh10postonse~e= or to redistribute10 lists, requires prior specific perrnrssion andor a fee. 171 be in a user-friendy manner across geographical boundaries. Of the numerous emerging mtdti-media applications, we en~lon a partictdar End of apphcation, the virtual wW-through apphcation [25], to be of interest. k a virtual *through application, a user, with access …","cites":"36","conferencePercentile":"74.03846154"},{"venue":"ACM Multimedia","id":"b327538a16a3095791439289fffb0250d2e6b506","venue_1":"ACM Multimedia","year":"2006","title":"ZooMICSS: a zoomable map image collection sensemaking system (the Katrina Rita context)","authors":"Ross Graeber, Andruid Kerne, M. Kathryn Henderson","author_ids":"2487369, 1694380, 1968036","abstract":"Access to devices that integrate Global Positioning data with image and sound acquisition becomes more common, enabling people to build large collections of locative multimedia. As the size and number of these locative media collections grow, so too does the importance of systems that support collection sensemaking. Media semantics, which include automatically acquired location data, as well as user-supplied annotations, play a key role in these user-centered processes of collection utilization. This demo presents a Zoomable Map Image Collection Sensemaking System that enables the collection, organization, browsing, and annotation of locative images. The Zoomable Map Perspective is supplemented by event-based clustering. Dynamic views are generated automatically from captured media. The system is currently being used to document the location and condition of homes and neighborhoods in the aftermath of Hurricane Katrina.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"ba56ed046ab1e367679d5874b630706fd051c70c","venue_1":"ACM Multimedia","year":"2002","title":"Tangible viewpoints: a physical approach to multimedia stories","authors":"Ali Mazalek, Glorianna Davenport, Hiroshi Ishii","author_ids":"2472678, 8295627, 1749649","abstract":"We present a multimedia storytelling system that couples a tangible interface with a multiple viewpoint approach to interactive narratives. Over the centuries, stories have moved from the physical environment (around campfires and on the stage), to the printed page, then to movie, television and computer screens. Today, using wireless and tag sensing technologies, storytellers are able to bring digital stories back into our physical environment. The Tangible Viewpoints system explores how physical objects and augmented surfaces can be used as tangible embodiments of different character perspectives in an interactive tale. These graspable surrogates provide a direct mode of navigation to the story world, a means of bridging the gap between cyberspace and our physical environment as we engage with digital stories. The system supports stories told in a range of media, including audio, video, still image and text.In this paper, we first provide a context for Tangible Viewpoints based on research in the areas of tangible interfaces and interactive narratives. We then offer an overview of the Tangible Viewpoints functionality, and explain the design and implementation of the system. The current system has been used in two storytelling projects. We discuss each one, and look at how user feedback has affected or will affect further development. We conclude by suggesting several future applications for the Tangible Viewpoints interface.","cites":"52","conferencePercentile":"91.02564103"},{"venue":"ACM Multimedia","id":"3c847217c18856f69b8af83b9db69e361f398fcc","venue_1":"ACM Multimedia","year":"2012","title":"PaperVideo: interacting with videos on multiple paper-like displays","authors":"Roman Lissermann, Simon Olberding, Benjamin Petry, Max Mühlhäuser, Jürgen Steimle","author_ids":"2432512, 3186111, 1979962, 1725964, 1790324","abstract":"Sifting and sense-making of video collections are important tasks in many professions. In contrast to sense-making of paper documents, where physical structuring of many documents has proven to be key to effective work, interaction with video is still restricted to the traditional \"one video at a time\" paradigm. This paper investigates how interaction with video can benefit from paper-like displays that allow for working with multiple videos simultaneously in physical space. We present a corresponding approach and system called PaperVideo, including novel interaction concepts for both video and audio. These include spatial techniques for temporal navigation, arranging, grouping and linking of videos, as well as for managing video contents and simultaneous audio playback on multiple displays. An evaluation with users provides insights into how paper-based navigation with videos improves active video work.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"3bb4e95c6bf71de5c2f2b7ddb156e9937d2c00ad","venue_1":"ACM Multimedia","year":"1993","title":"Open Architecture Multimedia Documents","authors":"Brian R. Gaines, Mildred L. G. Shaw","author_ids":"1723015, 1715862","abstract":"An open architecture multimedia document publication system is described which integrates a number of different representation technologies to provide a medium offering a wide spectrum of usage, from emulation of current paper publication, through electronic document delivery, multimedia inclusion of video and sound, structured hypermedia linkage, and formal knowledge representation supporting simulation and inference. The research is targeted on exploring new forms of scholarly communication, and the publication system supports collaborative document development, the authentication of disseminated material, and the citation, annotation and reuse of such material. The document publication system provides a rich word processing and page makeup environment with all the facilities normally expected, and adds multimedia, hypermedia and computational facilities incrementally and naturally, with careful attention to the usability of the human-computer interface. The result is an interactive document in which knowledge is represented in a variety of ways, some targeted on human interaction, some targeted on computational analysis, simulation and inference, and such that the document can be printed as a conventional paper or book losing the dynamic aspects of the material but retaining the visual representation.","cites":"15","conferencePercentile":"37.20930233"},{"venue":"ACM Multimedia","id":"42651049246b33fb84133df94d59d4552eea284d","venue_1":"ACM Multimedia","year":"2000","title":"Study of shot length and motion as contributing factors to movie tempo (poster session)","authors":"Brett Adams, Chitra Dorai, Svetha Venkatesh","author_ids":"1760106, 1719234, 1679520","abstract":"This work seeks to lay the framework of film grammar over the video to be analysed. We use the shot attributes of motion and shot length to produce a novel continuous measure of one of the aesthetic elements of films, namely the movie tempo. We refer to our previous work detailing the study of this construct and its automatic derivation, and also demonstrating its usefulness as an expressive element and as a sound basis for higher semantic descriptions such as dramatic events and story elements. Initial assessment of tempo was performed in our study on the basis that the relative importance of both shot length and motion in formulating the tempo function was the same. In this paper, we analyze their relative contributions to tempo, and demonstrate how these two factors can be manipulated to influence audience perception of movie time.","cites":"12","conferencePercentile":"67.39130435"},{"venue":"ACM Multimedia","id":"760a5eb257e91f121a819c8b468a806a967567bd","venue_1":"ACM Multimedia","year":"1998","title":"Protecting VoD the Easier Way","authors":"Carsten Griwodz, Oliver Merkel, Jana Dittmann, Ralf Steinmetz","author_ids":"1745763, 2309114, 1719310, 1725298","abstract":"1. ABSTRACT V&rious on-demand systems require that large numbers of customers are provided with the same mukimedia stream content or different but cIosely related content in short temporary sequence but not at exactly the same time. This includes video on demand and news on demand. A typical approach to increase the performance of such systems is caching. However in current commercial on-demand streaming applications in the Internet caches are used very rarely because a mechanism to protect the content from resale by the cache owners does not exist A typical solution is to transfer all content via protected unicast transmissions, which is an approach that does not scale. We want to present a trivial scheme that provides similar protection for the content but be used efficiently with mukicasting and caching. In this approach, the major part of the video is intentiontiy corrupted and can be distributed via mukicast connections, while the part for reconstruction of the original is deIivered to each receiver individuality. We propose also means to discourage resaIe of the multimedia content by customers. One proposal introduces receiver-sided introduction of watermarks into the video, the other uses infrequent corrupt bytes to achieve uniqueness of each copy. 1.1 Keywords Multirnedi&video-on-demand copyright corruption Permission 10make digl:al or hard copies of all or part of ~his work for persorml or classroom use is gramed wnhoul fee provided ~hat copies are nol made or diswibuled for profl or commercial advamage, and thal cop]es bear Ihls notice and ihe full eitalion on the firsI page. To copy o!herwse, 10 republis~ 10 posI on sewers or 10 red:s!ribule 10 llsts. requires prior specific permssion andor a fe~ AChl Md11media \" 98. BrrsIol. Many on-demand applications require that the same content is delivered to many different receivers in short sequence. In both VOD and NoD applications, the goal of the content provider is the frequent and rapid sale of contents in the most popular phase of their life cycle. The restriction of access to a small group of receivers is not intended for this type of content redistribution. In video-on-demand systems, for example, advertisements and current trends raise the popularity of certain video titles, and thus, the rate of requests to the specific title. In fact, videos exhibit certain long-term a=tig characteristics that are verified in [5]. This could be exploited in a distribution system by the introduction of caching and prefetching techniques. However, to …","cites":"26","conferencePercentile":"63.46153846"},{"venue":"ACM Multimedia","id":"741af697a7882e88da659eb2594f10f219462bb5","venue_1":"ACM Multimedia","year":"2003","title":"Weaving stories in digital media: when Spielberg makes home movies","authors":"Brett Adams, Svetha Venkatesh","author_ids":"1760106, 1679520","abstract":"In this paper we describe research aimed at enabling amateur video makers to improve both the technical quality and communicative capacity of their work. Motivated by the recognition that untold hours of home video are simply abandoned after capture, we have formulated the problem as one of defining the what and how of footage capture.We have implemented a framework that answers the first problem, the what, by means of the age-old communicative powers of Story; the second problem, the how, is addressed by means of well documented aesthetic principles that constitute the film profession, which impact both technical and cinematic considerations for a given project.We provide a brief overview of the process, beginning with the narrative template, embodying a chosen story, through the principal phases of generating a storyboard, directing, and editing, resulting in the finished product. We demonstrate the interplay of narrative, purpose for the production, and aesthetic agents, and their influence on the automatically generated storyboard with examples.","cites":"3","conferencePercentile":"21.62162162"},{"venue":"ACM Multimedia","id":"804be72777dd3484ee8ced759e75ff170cd0afc8","venue_1":"ACM Multimedia","year":"2004","title":"Director in your pocket: holistic help for the hapless home videographer","authors":"Brett Adams, Svetha Venkatesh","author_ids":"1760106, 1679520","abstract":"We present a new aspect of our ongoing research aimed at providing technology for the amateur home videographer. We aim to enable the production of quality video presentations that are well structured and use the expressive properties of the medium to full effect, regardless of the technical or artistic abilities of the user. This task requires that help be given to the user at or before capture time. We use a PDA platform to deliver 3d visualizations of &#60;i>shot directives&#60;/i>, instructions to the user about the type of footage to capture, and discuss issues connected with realizing high-level representations in concrete first person animations. Additionally, we discuss the mechanism for mating that metadata with captured footage and implementation issues.","cites":"2","conferencePercentile":"26.96078431"},{"venue":"ACM Multimedia","id":"2b7e6ccff8475fc68304198cc0d2f8a835e9b977","venue_1":"ACM Multimedia","year":"2005","title":"Situated event bootstrapping and capture guidance for automated home movie authoring","authors":"Brett Adams, Svetha Venkatesh","author_ids":"1760106, 1679520","abstract":"This paper describes a novel interactive media authoring framework, MediaTE, that enables amateurs to create videos of higher narrative or aesthetic quality with a completely mobile lifecycle. A novel event bootstrapping dialog is used to derive shot suggestions that yield both targetted footage and annotation enabling an automatic Computational Media Aesthetics-aware editing phase, the manual performance of which is typically a barrier to the amateur. This facilitates a move away from requiring a prior-conception of the events or locale being filmed, in the form of a template, to at-capture bootstrapping of this information. Metadata gathered as part of the critical path of media creation also has implications for the longevity and reuse of captured media assets. Results of an evaluation performed on both the usability and delivered media aspects of the system are discussed, which highlight the tenability of the proposed framework and the quality of the produced media.","cites":"8","conferencePercentile":"53.46534653"},{"venue":"ACM Multimedia","id":"4b6e3380a4f416e048838561161cbfea28adf838","venue_1":"ACM Multimedia","year":"2005","title":"Dynamic shot suggestion filtering for home video based on user performance","authors":"Brett Adams, Svetha Venkatesh","author_ids":"1760106, 1679520","abstract":"This paper presents novel additions to our existing amateur media creation framework. The framework provides at-capture guidance to enable the home movie maker to realize their aesthetic and narrative goals <i>and</i> automation of post-production editing. A common problem with the amateur filming context is its contingent nature, which often results in the failure to gain footage vital to the user's goals, even with at-capture software embedding. Accordingly, we have modelled minimizing the difference between target and captured footage at a given time during filming as a probability distribution divergence problem. We apply two policies of feedback to the user on thier performance, passive communication via a suggestion desirability measure, and active filtering of undesirable suggestions. We demonstrate the framework using each policy with a simulation of various user and filming situations with promising results.","cites":"1","conferencePercentile":"14.10891089"},{"venue":"ACM Multimedia","id":"f83f021c4bc1f41ff6d89efc6cf3488b349dc2f2","venue_1":"ACM Multimedia","year":"2006","title":"Browsing personal media archives with spatial context using panoramas","authors":"Brett Adams, Stewart Greenhill, Svetha Venkatesh","author_ids":"1760106, 2965462, 1679520","abstract":"This paper presents novel techniques for using panoramas as spatial context to enhance browsing of personal media archives. This context, scenes where frequent media capture takes place, is present in the disparate photos and videos, but not leveraged by traditional browsing techniques (e.g. thumbnails or zoomable interfaces). Coarse geo-position is often an insufficient index at such media capture hotspots. We experiment with panoramic video, which presents archive video organically blended with panoramas of media capture hotspots; Immersive browsing and filtering with media items projected onto spherical panoramas; and Detection and representation of links between panoramas to enable browsing of situated media in quasi-3D. We present proof-of-concept implementations and observations of their effectiveness, limitations, and open problems. Experiments confirm the intuition that each holds promise for augmenting traditional browsing environments.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"387d6de6d212a7c59f387b1c88a13043452307c5","venue_1":"ACM Multimedia","year":"2005","title":"My digital photos: where and when?","authors":"Neil O'Hare, Cathal Gurrin, Hyowon Lee, Noel Murphy, Alan F. Smeaton, Gareth J. F. Jones","author_ids":"8471052, 1737981, 1709368, 1701317, 1680223, 1750400","abstract":"In recent years digital cameras have seen an enormous rise in popularity, leading to a huge increase in the quantity of digital photos being taken. This brings with it the challenge of organising these large collections. We preset work which organises personal digital photo collections based on date/time and GPS location, which we believe will become a key organisational methodology over the next few years as consumer digital cameras evolve to incorporate GPS and as cameras in mobile phones spread further. The accompanying video illustrates the results of our research into digital photo management tools which contains a series of screen and user interactions highlighting how a user utilises the tools we are developing to manage a personal archive of digital photos.","cites":"6","conferencePercentile":"45.2970297"},{"venue":"ACM Multimedia","id":"51cc1629ff5a45e506109cbf53dcc685780447e5","venue_1":"ACM Multimedia","year":"2011","title":"Modeling 3D articulated motions with conformal geometry videos (CGVs)","authors":"Dao Thi Phuong Quynh, Ying He, XiaoMing Chen, Jiazhi Xia, Qian Sun, Steven C. H. Hoi","author_ids":"2592951, 1734129, 1772182, 1685764, 6503114, 1741126","abstract":"3D articulated motions are widely used in entertainment, sports, military, and medical applications. Among various techniques for modeling 3D motions, geometry videos (GVs) are a compact representation in that each frame is parameterized to a 2D domain, which captures the 3D geometry (x, y, z) to a pixel (r, g, b) in the image domain. As a result, the widely studied image/video processing techniques can be directly borrowed for 3D motion. This paper presents conformal geometry videos (CGVs), a novel extension of the traditional geometry videos by taking into the consideration of the isometric nature of 3D articulated motions. We prove that the 3D articulated motion can be uniquely (up to rigid motion) represented by (&#187;,H), where &#187; is the conformal factor characterizing the intrinsic property of the 3D motion, and H the mean curvature characterizing the extrinsic feature (i.e., embedding or appearance). Furthermore, the conformal factor &#187; is pose-invariant. Thus, in sharp contrast to the GVs which capture 3D motion by three channels, CGVs take only one channel of mean curvature H and the first frame of the conformal factor &#187;, i.e., approximately 1/3 the storage of the GVs. In addition, CGVs have strong spatial and temporal coherence, which favors various well studied video compression techniques. Thus, CGVs can be highly compressed by using the state-of the-art video compression techniques, such as H.264/AVC. Our experimental results on real-world 3D motions show that CGVs are a highly compact representation for 3D articulated motions, i.e., given CGVs and GVs of the same file size, CGVs show much better visual quality than GVs.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"91f619bbb5e800498db6cd735179527f5268b5cc","venue_1":"ACM Multimedia","year":"2005","title":"Affect-based indexing and retrieval of films","authors":"Ching Hau Chan, Gareth J. F. Jones","author_ids":"2665511, 1750400","abstract":"Digital multimedia systems are creating many new opportunities for rapid access to content archives. In order to explore these collections using search applications, the content must be annotated with significant features. An important and often overlooked aspect of human interpretation of multimedia data is the affective dimension. Affective labels of content can be extracted automatically from within multimedia data streams. These can then be used for content-based retrieval and browsing. In this study affective features extracted from multimedia audio content are mapped onto a set of keywords with predetermined emotional interpretations. These labels are then used to demonstrate affect-based retrieval on a range of feature films.","cites":"31","conferencePercentile":"84.65346535"},{"venue":"ACM Multimedia","id":"fa8bca02bdef43aab458b0f39facf38b03f067e4","venue_1":"ACM Multimedia","year":"1998","title":"Stimulus Tracking in Functional Magnetic Resonance Imaging (fMRI)","authors":"James Ford, Fillia Makedon, Charles B. Owen, Sterling C. Johnson, Andrew J. Saykin","author_ids":"2228347, 1728274, 1787354, 3326109, 7992909","abstract":"Functional hia~etic Resonance tiagery (~~) is a new' medical ima=tig technology pro~ti~mg tictional, as opposed to natomicd, mapptig of the human brain. The promise of this non-invasive technique has opened new' ch~enges for mdtimedia researchers. A the fieId of human functional neurotia=tig e~lodes and the technology of ~W advmces, computational techniques for expertient control, subject stimu~ generation, and joint stimti-activation tiaetig are Iachg. host computational work h= focused on impro~tig the analysis of the brab S=S. Tti paper describes eomputiationa~ mechanisms for the dehery md tiactig of multimedia stimti and a correlation framework for stimti and brain responses. NIediaStim, a new integrated data co~ection fraework for stimuIus tracking $hat k currently under development. NlediaSfi etiends the stimti paradi=m to composite multimedia presentations. To stiulate a real-tie sitiation as reafitica~y as possible, it is tiportant to stidy the brain Perrnksion lo ma}:edigi~alor hard copies of fl or part of thk ~!;ork for personal or classroom use k granted ;I:ithout fee pro~,ided ~ha~ copies are not made or distributed for profit or commertizl ada~antage, and fiat copies bear this no:i~ and ~heftil titation on the firs! page. To copy oihen!.k~ lo repub~i~ to post on senrers or 10 redistribute 10 lis~, requires prior specific perrnksion anifor a fee. response using CChnmersive \" stimu~, and have tooIs that record this interaction over tie in an environment that is totafly quantified and reproducible. 1.1 Keyords =, mdtimedia analysis tools 2. ~ODUC~ON The mati contribution of this paper is a new integrated approach for applying tests in Functional Magnetic Resonance haging (m. @ is a new approach to the study of the human bra~ tie processes of motor activity, cognitio~ memo~ retention, and learning, and how these relate to brain locations. ~ is also used to study a wide variety of neurologicrd conditions including diseases (e.g. ~S, epflepsy, and cancer), memory loss, behavioral disorders, and sentence processing in aphasics. W subjects observe or are asked to respond to stimuli, instructions, or questions (ofien by speaking or pushing a button) w~e fictional activity within their brain is recorded * an experimen~ researchers attempt to correlate various stimuli witi activations observed in spatial locations in the brain. The typical paradigm form experiment is to metiy control the stimuli, usuaUy by presenting words, pic~es, sounds, or motor activity at fm fixd intervti. A unique characteristic of ~ is that it is non-invasive (no tmcers, cathetetitio~ or surgery are …","cites":"2","conferencePercentile":"8.653846154"},{"venue":"ACM Multimedia","id":"100f03f29746f4a524b21371392d772502e98b8c","venue_1":"ACM Multimedia","year":"2011","title":"Up-fusion: an evolving multimedia decision fusion method","authors":"Xiangyu Wang, Yong Rui, Mohan S. Kankanhalli","author_ids":"1690702, 1728806, 1744045","abstract":"The amount of multimedia data available on the Internet has increased exponentially in the past few decades and is likely to keep on increasing. Given that a multimedia system has multiple information sources, fusion methods are critical for its analysis and understanding. However, most of the traditional fusion methods are static with respect to time. To address this, in recent years, several evolving fusion methods have been proposed. However, they can only be used in limited scenarios. For example, the context aware fusion methods need the context information to update the fusion model, but the context may not always be available in many applications. In this paper, a new evolving fusion method is proposed based on the online portfolio selection theory. The proposed method takes the correlation among different information sources into account, and evolves the fusion model when new multimedia data is added. It can deal with either crisp or soft decisions without requiring additional context information. Extensive experiments on concept detection task over TRECVID dataset have been conducted, and promising results have been obtained.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"05169f5562178234f7fa397d892c8ed1f0a3f681","venue_1":"ACM Multimedia","year":"2004","title":"Calculation of an aggregated level of interest function for recorded events","authors":"Rahul Nair","author_ids":"1799464","abstract":"As recording technology becomes pervasive there is a dramatic increase in the number of events being recorded in multimedia. The challenge now facing users is to quickly view the recorded content in the least amount of time. While there are several methods to analyze video based on ambient noise, scene changes, slide transitions, etc..., these techniques merely find features in the recording, they do not reveal which sections are important.\n This paper presents a method to calculate a Level Of Interest (LOI) function for an event by aggregating bookmarks made by the event attendees. The findings of a preliminary evaluation of the LOI function are also presented along with the design of an LOI based video browser.","cites":"5","conferencePercentile":"45.58823529"},{"venue":"ACM Multimedia","id":"003d5e38257cd640b1ff1c3dd951b7f88b47ec4d","venue_1":"ACM Multimedia","year":"2016","title":"Patterns of Free-form Curation: Visual Thinking with Web Content","authors":"Nic Lupfer, Andruid Kerne, Andrew M. Webb, Rhema Linder","author_ids":"3059034, 1694380, 1863950, 1992684","abstract":"Web curation involves choosing, organizing, and commenting on content. Popular web curation apps-- e.g., Facebook, Twitter, and Pinterest-- provide linear feeds that show people the latest content, but provide little support for articulating relationships among content elements. The new medium of free-form web curation enables multimedia elements to be spontaneously gathered from the web, written about, sketched amidst, manipulated, and visually assembled in a continuous space. Through free-form web curation, content is collected, interpreted, and arranged, creating context. We conducted a field study of 1581 students in 6 courses, spanning diverse fields. We derive patterns of free-form curation through a visual grounded theory analysis of the resulting dataset of 4426 curations. From the observed range of invocations of the patterns in the performance of ideation tasks, we conclude that free-form is valuable as a new medium of web curation in how it supports creative visual thinking.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"2022643030566689da29011c4dcfcb0abe3dbaf4","venue_1":"ACM Multimedia","year":"2011","title":"Towards multi-semantic image annotation with graph regularized exclusive group lasso","authors":"Xiangyu Chen, Xiao-Tong Yuan, Shuicheng Yan, Jinhui Tang, Yong Rui, Tat-Seng Chua","author_ids":"2572474, 8515907, 1698982, 8053308, 1728806, 1684968","abstract":"To bridge the semantic gap between low level feature and human perception, most of the existing algorithms aim mainly at annotating images with concepts coming from only one semantic space, e.g. cognitive or affective. The naive combination of the outputs from these spaces will implicitly force the conditional independence and ignore the correlations among the spaces. In this paper, to exploit the comprehensive semantic of images, we propose a general framework for harmoniously integrating the above multiple semantics, and investigating the problem of learning to annotate images with training images labeled in two or more correlated semantic spaces, such as fascinating nighttime, or exciting cat. This kind of semantic annotation is more oriented to real world search scenario. Our proposed approach outperforms the baseline algorithms by making the following contributions. 1) Unlike previous methods that annotate images within only one semantic space, our proposed multi-semantic annotation associates each image with labels from multiple semantic spaces. 2) We develop a multi-task linear discriminative model to learn a linear mapping from features to labels. The tasks are correlated by imposing the exclusive group lasso regularization for competitive feature selection, and the graph Laplacian regularization to deal with insufficient training sample issue. 3) A Nesterov-type smoothing approximation algorithm is presented for efficient optimization of our model. Extensive experiments on NUS-WIDEEmotive dataset (56k images) with 8&#215;81 emotive cognitive concepts and Object&#38;Scene datasets from NUS-WIDE well validate the effectiveness of the proposed approach.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"ec4a0d9909b19fa3a2e436dcfa7c3da13cd226ee","venue_1":"ACM Multimedia","year":"2010","title":"Interactive multimedia computing for creativity and expression","authors":"Andruid Kerne, Frank Nack, Luca Farulli","author_ids":"1694380, 1679840, 2776336","abstract":"In this paper we outline the aims and organization of the ACM MM 10 workshop on 'Interactive Multimedia Computing for Creativity and Expression'.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"7dda9ab2fcbc3ec2fd98cc110c293a7a91e822f1","venue_1":"ACM Multimedia","year":"2010","title":"Unified tag analysis with multi-edge graph","authors":"Dong Liu, Shuicheng Yan, Yong Rui, HongJiang Zhang","author_ids":"5688961, 1698982, 1728806, 1718558","abstract":"Image tags have become a key intermediate vehicle to organize, index and search the massive online image repositories. Extensive research has been conducted on different yet related tag analysis tasks, e.g., tag refinement, tag-to-region assignment, and automatic tagging. In this paper, we propose a new concept of multi-edge graph, through which a unified solution is derived for the different tag analysis tasks. Specifically, each vertex of the graph is first characterized by a unique image. Then each image is encoded as a region bag with multiple image segmentations, and the thresholding of the pairwise similarities between regions naturally constructs the multiple edges between each vertex pair. The unified tag analysis is then generally described as the tag propagation between a vertex and its edges, as well as between all edges cross the entire image repository. We develop a core vertex-vs-edge tag equation unique for multi-edge graph to unify the image/vertex tag(s) and region-pair/edge tag(s). Finally, unified tag analysis is formulated as a constrained optimization problem, where the objective function characterizing the cross-patch tag consistency is constrained by the core equations for all vertex pairs, and the cutting plane method is used for efficient optimization. Extensive experiments on various tag analysis tasks over three widely used benchmark datasets validate the effectiveness of our proposed unified solution.","cites":"28","conferencePercentile":"91.23287671"},{"venue":"ACM Multimedia","id":"2c253729d6170b31972ded6bfec1ea502f3ff86e","venue_1":"ACM Multimedia","year":"2003","title":"On image auto-annotation with latent space models","authors":"Florent Monay, Daniel Gatica-Perez","author_ids":"1824057, 1698682","abstract":"Image auto-annotation, i.e., the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset.","cites":"131","conferencePercentile":"96.3963964"},{"venue":"ACM Multimedia","id":"0ce700dda05a2d0348675ab28348312a9192aa55","venue_1":"ACM Multimedia","year":"2004","title":"PLSA-based image auto-annotation: constraining the latent space","authors":"Florent Monay, Daniel Gatica-Perez","author_ids":"1824057, 1698682","abstract":"We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.","cites":"118","conferencePercentile":"98.03921569"},{"venue":"ACM Multimedia","id":"20214575b8b69c1537211b39d4824072abde6993","venue_1":"ACM Multimedia","year":"1994","title":"The Walk-Through Approach to Authoring Multimedia Documents","authors":"Scott E. Hudson, Chen-Ning Hsi","author_ids":"1749296, 3145867","abstract":"This paper describes a novel approach to authoring multimedia documents based on the walk-through paradigm. Using this approach, multimedia authoring tasks can be performed in the context of the multimedia presentation under construction. It greatly simplifies the authoring process by hiding the use of composition constructs and eliminating the turn-around time from the editing and testing parts of the development cycle. End user multimedia authoring and fast prototyping can therefore be realized.","cites":"10","conferencePercentile":"46.61016949"},{"venue":"ACM Multimedia","id":"7780ec84ac006b50b077d9d3e77312b1d3b6a80a","venue_1":"ACM Multimedia","year":"2008","title":"Topickr: flickr groups and users reloaded","authors":"Radu Andrei Negoescu, Daniel Gatica-Perez","author_ids":"1937451, 1698682","abstract":"With the increased presence of digital imaging devices there also came an explosion in the amount of multimedia content available online. Users have transformed from passive consumers of media into content creators. Flickr.com is such an example of an online community, with over 2 billion photos (and more recently, videos as well), most of which are publicly available. The user interaction with the system also provides a plethora of metadata associated with this content, and in particular tags. One very important aspect in Flickr is the ability of users to organize in self-managed communities called groups. Although users and groups are conceptually different, in practice they can be represented in the same way: a bag-of-tags, which is amenable for probabilistic topic modeling. We present a topic-based approach to represent Flickr users and groups and demonstrate it with a web application, Topickr, that allows similarity based exploration of Flickr entities using their topic-based representation, learned in an unsupervised manner.","cites":"12","conferencePercentile":"71.78899083"},{"venue":"ACM Multimedia","id":"6ffec1f5f63689a4b59a53e212075f3f3d58e068","venue_1":"ACM Multimedia","year":"2010","title":"All things mobile: the present and future of mobile phone computing","authors":"Daniel Gatica-Perez","author_ids":"1698682","abstract":"This is the summary of the panel All Things Mobile: The Present and Future of Mobile Phone Computing.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"ad31309333c72fb5be4f9ff91191cdadaa22f955","venue_1":"ACM Multimedia","year":"2010","title":"Kodak moments and Flickr diamonds: how users shape large-scale media","authors":"Radu Andrei Negoescu, Alexander C. Loui, Daniel Gatica-Perez","author_ids":"1937451, 1707590, 1698682","abstract":"In today's age of digital multimedia deluge, a clear understanding of the dynamics of online communities is capital. Users have abandoned their role of passive consumers and are now the driving force behind large-scale media repositories, whose dynamics and shaping factors are not yet fully understood. In this paper we present a novel human-centered analysis of two major photo sharing websites, Flickr and Kodak Gallery. On a combined dataset of over 5 million tagged photos, we investigate fundamental differences and similarities at the level of tag usage and propose a joint probabilistic topic model to provide further insight into semantic differences between the two communities. Our results show that the effects of the users' motivations and needs can be strongly observed in this large-scale data, in the form of what we call Kodak Moments and Flickr Diamonds. They are an indication that system designers should carefully take into account the target audience and its needs.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"903b1d5e1c8191d84f2eaddf264546efb8c971ff","venue_1":"ACM Multimedia","year":"2007","title":"Generating views of the buzz: browsing popular media and authoring using mixed-initiative composition","authors":"Eunyee Koh, Andruid Kerne, Andrew M. Webb, Sashikanth Damaraju, David Sturdivant","author_ids":"2800305, 1694380, 1863950, 1703378, 2803360","abstract":"combinFormation's mixed-initiative composition space enables system agents and humans to engage in processes of finding relevant information, and forming and authoring collections. Previously, the system was developed and utilized to support information discovery. The efficacy of the system for supporting creativity has been established in some contexts.\n We present combinFormation as a tool for browsing popular media and authoring personal collections. Yahoo Buzz is an popular media collection of top search queries, categorized into genres such as actors, music and sports. Existing interfaces limit the human participant to only viewing results of a single search at any given time. This paper presents a new system structure which interleaves multiple searches concurrently in a round-robin manner, enabling participants to concurrently explore and connect diverse result sets, which, in aggregate, may consist of hundreds of documents. The mixed-initiative composition space serves as a media interface for combining search results and authoring personal collections. Evaluation using the Buzz demonstrated participants ability to browse more diverse information using combinFormation than with a typical browser. They experienced browsing and authoring as easier and more entertaining. Results have implications for a broad range of use contexts in which combined views of the results of multiple searches need to be authored, including research scenarios, as well as popular media.","cites":"5","conferencePercentile":"45.83333333"},{"venue":"ACM Multimedia","id":"1b98e813b3a3e19922d055dca1915921fe0c4868","venue_1":"ACM Multimedia","year":"2016","title":"Affective Contextual Mobile Recommender System","authors":"Chao Wu, Jia Jia, Wenwu Zhu, Xu Chen, Bowen Yang, Yaoxue Zhang","author_ids":"1780536, 1697300, 1735522, 1734498, 6936070, 1754576","abstract":"Exponential growth of media consumption in online social networks demands effective recommendation to improve the quality of experience especially for on-the-go mobile users. By means of large-scale trace-driven measurements over mobile Twitter traces from users, we reveal the significance of affective features in shaping users' social media behaviors. Existing recommender systems however, rarely support this psychological effect in real-life. To capture this effect, in this paper we propose Kaleido, a real mobile system to achieve an affect-aware learning-based social media recommendation.Specifically, we design a machine learning mechanism to infer the affective feature within media contents. Furthermore, a cluster-based latent bias model is provided for jointly training the affect, behavior and social contexts. Our comprehensive experiments on Android prototype expose a superior prediction accuracy of 82%, with more than 20% accuracy improvement over existing mobile recommender systems. Moreover, by enabling users to offload their machine learning procedures to the deployed edge-cloud testbed, our system achieves speed-up of a factor of 1,000 against the local data training execution on smartphones.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"5cfe70ccacd302938620662190c573cb6f19bdfb","venue_1":"ACM Multimedia","year":"2011","title":"Searching the past: an improved shape descriptor to retrieve maya hieroglyphs","authors":"Edgar Roman-Rangel, Carlos Pallan, Jean-Marc Odobez, Daniel Gatica-Perez","author_ids":"1718491, 2188107, 1719610, 1698682","abstract":"Archaeologists often spend significant time looking at traditional printed catalogs to identify and classify historical images. Our collaborative efforts between archaeologists and multimedia researchers seek to develop a tool to retrieve two specific types of ancient Maya visual information: hieroglyphs and iconographic elements. Towards that goal we present two contributions in this paper. The first one is the introduction and analysis of a new dataset of 3400+ Maya hieroglyphs, whose compilation involved manual search, annotation and segmentation by experts. This dataset presents several challenges for visual description and automatic retrieval as it is rich in complex visual details. The second and main contribution is the in-depth analysis of the Histogram Of Orientation Shape Context (HOOSC), and more precisely, the development of 4 improvements that were designed to handle the visual complexity of Maya hieroglyphs: open contours, mixture of thick and thin lines, hatches, large instance variability, and a variety of internal details. Experiments demonstrate that the adequate combination of our improvements to retrieve Maya hieroglyphs, provides results with roughly 20% more precision compared to the original HOOSC descriptor. Complementary results with the MPEG-7 shape dataset validate (or not) the proposed improvements, showing that the design of appropriate descriptors depends on the nature of the shapes one deals with.","cites":"11","conferencePercentile":"84.54810496"},{"venue":"ACM Multimedia","id":"21b4e3115e4a0987ab013174ee6371fe7eead948","venue_1":"ACM Multimedia","year":"2006","title":"Choreographic buttons: promoting social interaction through human movement and clear affordances","authors":"Andrew M. Webb, Andruid Kerne, Eunyee Koh, Pranesh Joshi, YoungJoo Park, Ross Graeber","author_ids":"1863950, 1694380, 2800305, 3285565, 2416662, 2487369","abstract":"We used human movement as the basis for designing a collaborative aesthetic design environment. Our intention was to promote social interaction and creative expression. We employed off-the-shelf computer vision technology. Movement became the basis for the choreography of gestures, the development of gesture recognition, and the development of imagery and visualization. We discovered that the design of clear affordances is no less important in movement-based than in mouse-based systems. Through an integrated and iterative design process, we developed a new type of affordance, the <i>choreographic button</i>, which integrates choreography, gesture recognition, and visual feedback. Jumping, a quick movement, and crouching, a sustained gesture, were choreographed to form a vocabulary that is personally expressive, and which also facilitates automatic recognition.How can we evaluate socially motivated interactive systems? To create a context for evaluation, we held an integrated exhibition, party, and user study event. This mixing of events produced an engaging environment in which participants could choose to interact with each other, as well as with the design environment. We prepared a mouse-based version of the design environment, and compared how people experienced it with the movementbased system. Our study demonstrates that movement-based affordances promote social interaction.","cites":"8","conferencePercentile":"62.95336788"},{"venue":"ACM Multimedia","id":"1e5a891afb5f63a8ca9f3eaed79c3a39e6706a87","venue_1":"ACM Multimedia","year":"2014","title":"Automatic Maya hieroglyph retrieval using shape and context information","authors":"Rui Hu, Carlos Pallan, Guido Krempel, Jean-Marc Odobez, Daniel Gatica-Perez","author_ids":"4234445, 2188107, 2953525, 1719610, 1698682","abstract":"We propose an automatic Maya hieroglyph retrieval method integrating shape and glyph context information. Two recent local shape descriptors, Gradient Field Histogram of Orientation Gradient (GF-HOG) and Histogram of Orientation Shape Context (HOOSC), are evaluated. To encode the context information, we propose to convert each Maya glyph block into a first-order Markov chain and apply the co-occurrence of neighbouring glyphs. The retrieval results obtained based on visual matching are therefore re-ranked. Experimental results show that our method can significantly improve the glyph retrieval accuracy even with a basic co-occurrence model. Furthermore, two unique glyph datasets are contributed which can be used as novel shape benchmarks in future research.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"46d02daba1371a54fc1540da996b701a996e9ab7","venue_1":"ACM Multimedia","year":"2001","title":"CPU/power-constrained mobile devices","authors":"Richard Han, Ching-Yung Lin, John R. Smith, Belle L. Tseng, Vida Ha","author_ids":"1719013, 1689953, 1788270, 8595363, 2201559","abstract":"Due to the limited processing capability, memory constraints, and the power budget of mobile clients, multimedia coders and/or decoders are often difficult to implement on wireless handheld PDAs. In this Universal Tuner project, we designed and implemented a wireless video streaming system that transcodes MPEG-1/2 videos or live TV broadcasting videos to the BW or indexed color Palm OS devices. In our system, the complexity of multimedia compression and decompression algorithms is adaptively partitioned between the encoder and decoder. A mobile client would selectively disable or reenable stages of the algorithm to adapt to the device's effective processing capability. Our variable-complexity strategy of selective disabling of modules supports graceful degradation of the complexity of multimedia coding and decoding into a mobile client's low-power mode, i.e. the clock frequency of its next-generation low power CPU has been scaled down to conserve power. We modified the structure of the standard motion-compensated DCT video codecs to implement a simplified the encoder on a PC server and the decoder on a complexity-constrained PDA viewing client.","cites":"1","conferencePercentile":"15.30612245"},{"venue":"ACM Multimedia","id":"a9fc84f46007976a50520ce28aa5d6cb94f0ec9a","venue_1":"ACM Multimedia","year":"2010","title":"Modeling human behavior with mobile phones","authors":"Daniel Gatica-Perez","author_ids":"1698682","abstract":"In just a few years, mobile phones have emerged as the ultimate multimedia device. This is the summary of a proposed tutorial on Modeling Human Behavior with Mobile Phones, which aims to present the scientific and technological state-of-the-art in mobile phone-based modeling of large-scale human behavior from a coherent perspective, and hopes to motivate further work in this domain by the multimedia research community.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"1e95ad6087a66ee4d9c87c4b958908dee3b9511b","venue_1":"ACM Multimedia","year":"2004","title":"Mobile media metadata: metadata creation system for mobile images","authors":"Marc Davis","author_ids":"1777964","abstract":"In the 2003, more camera phones were sold worldwide than digital cameras. With this new platform, we can leverage regularities in the spatio-temporal context and social community of media capture and use to infer media content. We created and deployed a \"Mobile Media Metadata\" (MMM) prototype on Nokia 3650 camera phones with 55 users that uses \"context-to-content\" inferencing, a shared metadata ontology, and user interaction at the point of capture to effectively infer media content annotations, specifically the semantic description of the location of the subject of users' photos.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"9af4288957441ee77f3c03145a1a46b1dceee89a","venue_1":"ACM Multimedia","year":"2009","title":"Streaming 3D meshes using spectral geometry images","authors":"Ying He, Boon-Seng Chew, Dayong Wang, Steven C. H. Hoi, Lap-Pui Chau","author_ids":"1734129, 1957696, 1980038, 1741126, 3311129","abstract":"The transmission of 3D models in the form of Geometry Images (GI) is an emerging and appealing concept due to the reduction in complexity from <b>R</b><sup>3</sup> to image space and wide availability of mature image processing tools and standards. However, geometry images often suffer from the artifacts and error during compression and transmission. Thus, there is a need to address the artifact reduction, error resilience and protection of such data information during the transmission across an error prone network. In this paper, we introduce a new concept, called <i>Spectral Geometry Images (SGI)</i>, which naturally combines the powerful spectral analysis with geometry images. We show that SGI is more effective than GI to generate visually pleasing shapes at high compression rates. Furthermore, by coupling SGI to the proposed error protection scheme, we are able to ensure the smooth delivery of 3D model across error networks for different packet loss rate simulated using the two-state Markov model.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"60ca436bf33a3174f3a5496690403285c6165ef6","venue_1":"ACM Multimedia","year":"2003","title":"Active capture: automatic direction for automatic movies","authors":"Marc Davis","author_ids":"1777964","abstract":"Current consumer media production is laborious, tedious, and produces unsatisfying results. To address this problem, Active Capture leverages media production knowledge, computer vision and audition algorithms, and user interaction techniques to automate direction and cinematography and thus enables the automatic production of annotated, high quality, reusable media assets. Active Capture is part of a new computational media production paradigm that transforms media production from a manual mechanical process into an automated computational one that can produce mass customized and personalized media integrating video of non-actors. The implemented system automates the process of capturing a non-actor performing two simple reusable actions (\"screaming\" and \"turning her head to look at the camera\") and automatically integrates those shots into various commercials and movie trailers.","cites":"11","conferencePercentile":"52.25225225"},{"venue":"ACM Multimedia","id":"4b87adccc5453f716b7444073303138d6ff7ebbb","venue_1":"ACM Multimedia","year":"2010","title":"ACM international workshop on social, adaptive and personalized multimedia interaction and access (SAPMIA 2010)","authors":"David Vallet, Naeem Ramzan, Martin Halvey, Charalampos Z. Patrikakis","author_ids":"1794304, 1728223, 1767534, 3096444","abstract":"In an effort to address and overcome some of the open issues that hinder effective access and interaction of multimedia content, this workshop will bring together individuals from a number of research communities, including but not limited to Multimedia Distribution and Access, Social Network Analysis, Multimedia Content Analysis, and User Modelling Adaptation and Personalization. It is our belief that a synergetic approach involving these areas of work can exceed their individual potentials, leading to improved access, understanding, and retrieval of multimedia content. The main objective of this workshop is to provide a forum to disseminate work that explicitly exploits the synergy between multimedia content analysis, personalisation, and next generation networking and community aspects of social networks. We believe that this integration could result on robust, personalized multimedia services, providing users with an improved multimedia experience.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"3cb1f4c1650f7e55b78abba5a00b56a90b8e0567","venue_1":"ACM Multimedia","year":"2014","title":"sTrack: Secure Tracking in Community Surveillance","authors":"Chun-Te Chu, Jaeyeon Jung, Zhicheng Liu, Ratul Mahajan","author_ids":"3328706, 1775827, 1995841, 3063054","abstract":"We present sTrack, a system that can track objects across multiple cameras without sharing any visual information between two cameras except whether an object was seen by both. To achieve this challenging privacy goal, we leverage recent advances in secure two-party computation and multi-camera tracking. We derive a new distance metric learning technique that is more suited for secure computation. Compared to the existing methods, our technique has lower complexity in secure computation without sacrificing the tracking accuracy. We implement it using a new Boolean circuit for secure tracking. Experiments using real datasets show that the performance overhead of secure tracking is low, adding only a few seconds over non-private tracking.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"77732f439f130cf80a8c9793123607a5ae8772c5","venue_1":"ACM Multimedia","year":"2003","title":"Application of a content-based percussive sound synthesizer to packet loss recovery in music streaming","authors":"Lonce L. Wyse, Ye Wang, Xinglei Zhu","author_ids":"2244569, 1681196, 1837323","abstract":"This paper presents a novel method to recover lost packets in music streaming using a synthesizer to generate percussive sounds. As an improvement of the state-of-the-art system that uses a content-based audio codebook, the new method can greatly reduce the redundant information needed to recover perceptually critical lost packets.","cites":"4","conferencePercentile":"27.02702703"},{"venue":"ACM Multimedia","id":"362be576361fbdc6fb35d4ef3d65f41545dd4a82","venue_1":"ACM Multimedia","year":"2003","title":"Content-based UEP: a new scheme for packet loss recovery in music streaming","authors":"Ye Wang, Ali Ahmaniemi, David Isherwood, Wendong Huang","author_ids":"1681196, 1688631, 2084460, 2401608","abstract":"Bandwidth efficiency and error robustness are two essential and conflicting requirements for streaming media content over error-prone channels, such as wireless channels. This paper describes a new scheme called content-based unequal error protection (C-UEP), which aims to improve the user-perceived QoS in the case of packet loss. We use music streaming as an example to show the effectiveness of the new concept. C-UEP requires only a small fraction of the redundancy used in existing forward error correction (FEC) methods. C-UEP classifies every audio segment (e.g. an encoding frame) into different classes to improve encoding efficiency. Salient transients such as drumbeats and note onsets are encoded with more redundancy in a secondary bitstream used to recover lost packets by the receiver. Formal perceptual evaluations show that our scheme improves audio quality significantly over simple muting and packet repetition baselines. This improvement is achieved with a negligible amount of redundancy, which is transmitted to the receiver ahead of playback.","cites":"11","conferencePercentile":"52.25225225"},{"venue":"ACM Multimedia","id":"47dfd72f7c6117d0c1adc2622e81d8ef744e325c","venue_1":"ACM Multimedia","year":"2004","title":"The creation of a music-driven digital violinist","authors":"Jun Yin, Ankur Dhanik, David Hsu, Ye Wang","author_ids":"1735997, 3345431, 6032664, 1681196","abstract":"This paper describes an initial attempt on a music-driven digital violinist (MDV) system, which automatically generates animation of a violinist based on violin music. MDV first analyzes the input audio signal and transcribes it into music notes. Next it uses the notes to synthesize the animated video of a violinist. Tests on the prototype system show that it achieves adequate visual realism and near real-time performance.","cites":"3","conferencePercentile":"33.33333333"},{"venue":"ACM Multimedia","id":"147faded89e86db0983fba2dabfe05da269e11b5","venue_1":"ACM Multimedia","year":"2004","title":"Singing voice detection in popular music","authors":"Tin Lay Nwe, Arun Shenoy, Ye Wang","author_ids":"2171327, 2056221, 1681196","abstract":"We propose a novel technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. Our technique uses a combination of harmonic content attenuation using higher level musical knowledge of key followed by sub-band energy processing to obtain features from the musical audio signal. We employ a Multi-Model Hidden Markov Model (MM-HMM) classifier for vocal and non-vocal classification that utilizes song structure information to create multiple models as opposed to conventional HMM training methods that employ only one model for each class. A statistical hypothesis testing approach followed by an automatic bootstrapping process is employed to further improve the accuracy of classification. An experimental evaluation on a database of 20 popular songs shows the validity of the proposed approach with an average classification accuracy of 86.7%.","cites":"28","conferencePercentile":"82.10784314"},{"venue":"ACM Multimedia","id":"6e7a77ea5deba1e3932618bacdf587546fa1ee5f","venue_1":"ACM Multimedia","year":"2004","title":"A framework for robust and scalable audio streaming","authors":"Ye Wang, Wendong Huang, Jari Korhonen","author_ids":"1681196, 2401608, 1719761","abstract":"We propose a framework to achieve bandwidth efficient, error robust and bitrate scalable audio streaming. Our approach is compatible with most audio compression format. The main contributions of this paper include: 1) the proposal of a Multi-Stage Interleaving (MSI) strategy which translates packet loss into loss of separate frequency components that are less perceptually significant; and 2) the design of a Layered Unequal-Sized Packetization (LUSP) scheme which enables bitrate scalability and prioritized packet transmission. The combination of the proposed MSI and LUSP allows the use of a set of simple yet effective methods of error concealment in the compressed domain. Our approach offers significant advantages over existing methods in terms of memory consumption (a savings of over 40 times in the sample MP3 implementation), and computational complexity, which are critical issues for battery-powered small devices.","cites":"8","conferencePercentile":"56.61764706"},{"venue":"ACM Multimedia","id":"31660bed2beb6210335c8e19060f813cb7a2cb34","venue_1":"ACM Multimedia","year":"2004","title":"LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics","authors":"Ye Wang, Min-Yen Kan, Tin Lay Nwe, Arun Shenoy, Jun Yin","author_ids":"1681196, 1807775, 2171327, 2056221, 1735997","abstract":"We present a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem using a multimodal approach, where the appropriate pairing of audio and text processing helps create a more accurate system. Our audio processing technique uses a combination of top-down and bottom-up approaches, combining the strength of low-level audio features and high-level musical knowledge to determine the hierarchical rhythm structure, singing voice and chorus sections in the musical audio. Text processing is also employed to approximate the length of the sung passages using the textual lyrics. Results show an average error of less than one bar for per-line alignment of the lyrics on a test bed of 20 songs (sampled from CD audio and carefully selected for variety). We perform holistic and per-component testing and analysis and outline steps for further development.","cites":"39","conferencePercentile":"87.74509804"},{"venue":"ACM Multimedia","id":"06d4bb79facaff9d08a55504a915c2bf2c906bdb","venue_1":"ACM Multimedia","year":"2005","title":"Power-aware bandwidth and stereo-image scalable audio decoding","authors":"Wendong Huang, Ye Wang, Samarjit Chakraborty","author_ids":"2401608, 1681196, 1722392","abstract":"We propose a new workload-scalable audio decoding scheme that would enable users to control the tradeoff between playback quality and power consumption in battery-powered portable audio players. Our objective is to give users a control at the decoder side, similar to the Long Play (LP) recording mode at the encoder side in many media recording devices. The main contribution of this paper is a proposal for a Bandwidth and Stereo-image Scalable (BSS) decoding scheme for single-layer audio formats such as MP3. The proposed scheme is based on an analysis of the perceptual relevance of different audio components in the compressed bitstream. The bandwidth and stereo-image scalability directly translates into scalability in terms of the computational workload generated by the decoder. This can be exploited by a voltage/frequency scalable processor to save energy and prolong the battery life.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"62a01fb036d498c955585f899cb9044c650cf879","venue_1":"ACM Multimedia","year":"2010","title":"Interactive panoramic video streaming system over restricted bandwidth network","authors":"Masayuki Inoue, Hideaki Kimata, Katsuhiko Fukazawa, Norihiko Matsuura","author_ids":"8605969, 2558752, 2526672, 2194027","abstract":"Many new applications are being created around the panoramic video service. The typical system divides the high resolution panoramic video into tiles and the sender transmits a set of tiles, the partial panoramic video. Coding each tile at a uniform bitrate yields poor video quality because each tile has different visual characteristics. This paper proposes a new data format and tile adaptive rate control to achieve high quality partial panoramic video transmission, even over restricted bandwidth networks. The proposed data format, based on the MVC standard, has two types of video stream and meta data. Each tile is encoded at multiple bitrates and multiplexed synchronously. The meta data has quality values of each tile at the multi-bitrates, and is used to determine the view_ids associated with the bitrates and user's desired view. Our tile-adaptive rate control proposal maximizes the partial panoramic video quality even in restricted bandwidth networks. An experiment shows that the proposed method can achieve higher video quality. The method ensures that the facial elements in the user's view, which often exhibit the greatest motion and to which we are most sensitive, have high quality.","cites":"9","conferencePercentile":"77.39726027"},{"venue":"ACM Multimedia","id":"09e62f2f74242ade670ee46966dbf504fe499013","venue_1":"ACM Multimedia","year":"2005","title":"Using offline bitstream analysis for power-aware video decoding in portable devices","authors":"Yicheng Huang, Samarjit Chakraborty, Ye Wang","author_ids":"3092967, 1722392, 1681196","abstract":"Dynamic voltage/frequency scheduling algorithms for multimedia applications have recently been a subject of intensive research. Many of these algorithms use control-theoretic feedback techniques to predict the future execution demand of an application based on the demand in the recent past. Such techniques suffer from two major disadvantages: (i) they are computationally expensive, and (ii) it is difficult to give performance or quality-of-service guarantees based on these techniques (since the predictions can occasionally turn out to be incorrect). To address these shortcomings, in this paper we propose a completely new approach for dynamic voltage and frequency scaling. Our technique is based on an offline bitstream analysis of multimedia files. Based on this analysis, we insert metadata information describing the computational demand that will be generated when decoding the file. Such bitstream analysis and metadata insertion can be done when the multimedia file is being downloaded into a portable device from a desktop computer. In this paper we illustrate this technique using the MPEG-2 decoder application. We show that the amount of metadata that needs to be inserted is a very small fraction of the total size of the video clip and it can lead to significant energy savings. The metadata inserted will typically consist of the frequency value at which the processor needs to be run at different points in time during the decoding process. Lastly, in contrast to <i>runtime prediction</i>-based techniques, our scheme can be used to provide performance and quality-of-service guarantees and at the same time avoids any runtime computation overhead.","cites":"18","conferencePercentile":"74.5049505"},{"venue":"ACM Multimedia","id":"34d4f0c6967ce127b74bf0dacb5af81ca4acc61d","venue_1":"ACM Multimedia","year":"2005","title":"Digital violin tutor: an integrated system for beginning violin learners","authors":"Jun Yin, Ye Wang, David Hsu","author_ids":"1735997, 1681196, 6032664","abstract":"Prompt feedback is essential for beginning violin learners; however, most amateur learners can only meet with teachers and receive feedback once or twice a week. To help such learners, we have attempted an initial design of Digital Violin Tutor (DVT), an integrated system that provides the much-needed feedback when human teachers are not available. DVT combines violin audio transcription with visualization. Our transcription method is fast, accurate, and robust again noise for violin audio recorded in home environments. The visualization is designed to be intuitive and easily understandable by people with little music knowledge. The different visualization modalities--video, 2D fingerboard animation, 3D avatar animation--help learners to practice and learn more effectively. The entire system has been implemented with off-the-shelf hardware and shown to be practical in home environments. In our user study, the system has received very positive evaluation.","cites":"27","conferencePercentile":"81.93069307"},{"venue":"ACM Multimedia","id":"5ab8e8e0e67f180e8d440b8cbd299f6d2b2c16bd","venue_1":"ACM Multimedia","year":"2003","title":"Proscenium: a framework for spatio-temporal video editing","authors":"Eric P. Bennett, Leonard McMillan","author_ids":"3037279, 1748115","abstract":"We present an approach to video editing where movie sequences are treated as spatio-temporal volumes that can be sheered and warped under user control. This simple capability enables new video editing operations that support complex postproduction modifications, such as object removal and/or changes in camera motion. Our methods do not rely on complicated and error-prone image analysis or computer vision methods. Moreover, they facilitate an editing approach to video that is similar to standard image-editing tasks. Central to our system is a movie representation framework called Proscenium that supports efficient queries and operations on spatio-temporal volumes while maintaining the original source content. We have adopted a graph-based lazy-evaluation model in order to support interactive visualizations, complex data modifications, and efficient processing of large spatio-temporal volumes.","cites":"25","conferencePercentile":"72.07207207"},{"venue":"ACM Multimedia","id":"1bc8c057fc5b3e6e041c940a6245d2b62c020831","venue_1":"ACM Multimedia","year":"2006","title":"Syllabic level automatic synchronization of music signals and text lyrics","authors":"Denny Iskandar, Ye Wang, Min-Yen Kan, Haizhou Li","author_ids":"2151695, 1681196, 1807775, 1711271","abstract":"We present a framework to synchronize pop music to corresponding text lyric. We refine line level alignment achievable by existing work to syllabic level by using a dynamic programming process. Our main contribution is using music knowledge to constrain the dynamic programming search. This is done by modeling (1) non-uniform note length distribution and (2) a note length distribution for each section type (for example intro, chorus, and bridge). These reduce alignment error by 6.4% and improve time efficiency by a factor of 2.2.","cites":"17","conferencePercentile":"80.31088083"},{"venue":"ACM Multimedia","id":"0342f1da801d60baf38ac6b57ba0faa2558fbb86","venue_1":"ACM Multimedia","year":"2007","title":"Visual analysis of fingering for pedagogical violin transcription","authors":"Bingjun Zhang, Jia Zhu, Ye Wang, Wee Kheng Leow","author_ids":"2522345, 1712835, 1681196, 1787377","abstract":"Automatic music transcription, in spite of decades of research, remains a challenging research problem. The traditional audio-only approach has yet to achieve a satisfactory performance for any computer-aided pedagogical system. Inspired by the high correlation between violin playing techniques (fingering, bowing) and the played acoustic notes, this paper presents a first attempt in visual analysis of violin fingering to compensate for the difficulties in audio-only music transcription. This is achieved by a robust multiple finger tracking algorithm and a string detection method that extract press, release, and fingertip position from the fingering video and automatically translate the fingering information into the played acoustic note, i.e., onset, offset, and pitches. Experimental results reveal high correctness in multiple finger tracking and string detection, thus paving the way for an improved audio-visual violin transcription system.","cites":"9","conferencePercentile":"63.54166667"},{"venue":"ACM Multimedia","id":"098d00dae0a26edac83ce8c390abea1177358854","venue_1":"ACM Multimedia","year":"2008","title":"Search trails using user feedback to improve video search","authors":"Frank Hopfgartner, David Vallet, Martin Halvey, Joemon M. Jose","author_ids":"1759761, 1794304, 1767534, 1685211","abstract":"In this paper we present an innovative approach for aiding users in the difficult task of video search. We use community based feedback mined from the interactions of previous users of our video search system to aid users in their search tasks. This feedback is the basis for providing recommendations to users of our video retrieval system. The ultimate goal of this system is to improve the quality of the results that users find, and in doing so, help users to explore a large and difficult information space and help them consider search options that they may not have considered otherwise. In particular we wish to make the difficult task of search for video much easier for users. The results of a user evaluation indicate that we achieved our goals, the performance of the users in retrieving relevant videos improved, and users were able to explore the collection to a greater extent.","cites":"17","conferencePercentile":"77.98165138"},{"venue":"ACM Multimedia","id":"ab87c137ae3bdff14642f3000e2b7d359daf6dd6","venue_1":"ACM Multimedia","year":"2010","title":"iPhotobook: creating photo books on mobile devices","authors":"Jun Xiao, Nic Lyons, Clayton Brian Atkins, Yuli Gao, Hui Chao, Xuemei Zhang","author_ids":"2781973, 2460517, 1870182, 2026151, 2972266, 5113938","abstract":"The amount of photo that is captured and stored with mobile devices is growing rapidly. We regularly see traditional desktop multimedia applications being ported to mobile devices. However, less often do we see novel interaction mechanism being developed to effectively deal with the physical limitations of the display and input of such devices. We built a mobile application called iPhotobook that enables users to create and edit photo books on iPhones. The application leveraged image analysis algorithms to automate user tasks of photo selection, grouping, editing and layout that would be difficult to accomplish without a large screen input. In this paper, we present these technologies and more importantly illustrate how they work together seamlessly with a gesture based user interface that creates a fun photo book authoring experience. Although the iPhone is our platform of choice and photo book creation is our target application, our innovations on UI design and automation algorithms may well be generalized to other small screen devices and applied to other mobile media applications.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"7ec6238873aa39aea631a647a4d17ca0c26667e9","venue_1":"ACM Multimedia","year":"2006","title":"Collaborative dancing in tele-immersive environment","authors":"Zhenyu Yang, Bin Yu, Wanmin Wu, Ross Diankov, Ruzena Bajcsy","author_ids":"1771274, 1774174, 2524314, 2054955, 1784213","abstract":"We present a study of collaborative dancing between remote dancers in a tele-immersive environment which features 3D full and real body capturing, wide field of view, multi-display 3D rendering, and attachment free participant. We invite two professional dancers to perform collaborative dancing in the environment. The coordination requires one dancer to take the lead while the other follows by appropriate movement. Throughout the experiment, the dancers are dancing at various motion rates to evaluate how well the collaborative dancing is supported with the current technical boundary. Our important findings indicate that 1) tele-immersive environments have strong potential impact on the concept of choreography and communication of live dance performance, 2) the presence of multi-view display, real body 3D rendering, audio channel, and less intrusiveness greatly enhances the immersive and dancing experience, and 3) the level of synchronization achieved by the dancers is higher than that expected from the video rate.","cites":"21","conferencePercentile":"84.71502591"},{"venue":"ACM Multimedia","id":"ca27050a5fd25016657ff4a1046f38043b4b7a56","venue_1":"ACM Multimedia","year":"2003","title":"MobiPicture: browsing pictures on mobile devices","authors":"Ming-Yu Wang, Xing Xie, Wei-Ying Ma, HongJiang Zhang","author_ids":"3141227, 1687677, 1705244, 1718558","abstract":"Pictures have become increasingly common and popular in mobile communication. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of pictures on the small screen. MobiPicture is a prototype system which includes a set of novel features to aid or automate a set of common image browsing tasks such as the thumbnail view, set-as-background, zooming and scrolling.","cites":"9","conferencePercentile":"45.4954955"},{"venue":"ACM Multimedia","id":"760aecfe9a711e714b3e6540ee32f45b192c393c","venue_1":"ACM Multimedia","year":"2002","title":"Enabling personalization services on the edge","authors":"Xing Xie, Hua-Jun Zeng, Wei-Ying Ma","author_ids":"1687677, 1741348, 1705244","abstract":"In this paper, we describe our work on enabling personalization services on the edge of the Internet. In contrast to traditional approaches, this method offers many advantages. First, content providers can make their content people-aware while the content is delivered to the user. Second, since the personalization work is distributed to an overlay network of edge servers, it greatly improves the scalability and availability of the services. Third, the sufficiency of user data collected in edge servers enables us to build more accurate user models which further enhance the performance of personalization services. We describe the implementation of a prototype system named Avatar and present some experimental result in this paper.","cites":"7","conferencePercentile":"42.30769231"},{"venue":"ACM Multimedia","id":"76d94b5ed0a73a347bdb983c1ff20ef4259fe7ef","venue_1":"ACM Multimedia","year":"2010","title":"Precise indoor localization using smart phones","authors":"Eladio Martin, Oriol Vinyals, Gerald Friedland, Ruzena Bajcsy","author_ids":"2452653, 1689108, 1797144, 1784213","abstract":"We present an indoor localization application leveraging the sensing capabilities of current state of the art smart phones. To the best of our knowledge, our application is the first one to be implemented in smart phones and integrating both offline and online phases of fingerprinting, delivering an accuracy of up to 1.5 meters. In particular, we have studied the possibilities offered by WiFi radio, cellular communications radio, accelerometer and magnetometer, already embedded in smart phones, with the intention to build a multimodal solution for localization. We have also implemented a new approach for the statistical processing of radio signal strengths, showing that it can outperform existing deterministic techniques.","cites":"46","conferencePercentile":"96.71232877"},{"venue":"ACM Multimedia","id":"0714b6164021a14afba1dbcbc21c28620557c79d","venue_1":"ACM Multimedia","year":"1999","title":"Visualizing music and audio using self-similarity","authors":"Jonathan Foote","author_ids":"1797460","abstract":"This paper presents a novel approach to visualizing the time structure of music and audio. The acoustic similarity between any two instants of an audio recording is displayed in a 2D representation, allowing identification of structural and rhythmic characteristics. Examples are presented for classical and popular music. Applications include content-based analysis and segmentation, as well as tempo and structure extraction.","cites":"182","conferencePercentile":"99.15966387"},{"venue":"ACM Multimedia","id":"9fc46bff148a2ffdf027f17fb2384c3a7a4749f2","venue_1":"ACM Multimedia","year":"1996","title":"Open-Vocabulary Speech Indexing for Voice and Video Mail Retrieval","authors":"Martin G. Brown, Jonathan Foote, Gareth J. F. Jones, Karen Spärck Jones, Steve J. Young","author_ids":"1878348, 1797460, 1750400, 3539548, 1736708","abstract":"This paper presents recent work on a multimedia retrieval project at Cambridge University and Olivetti Research Limited (ORL). We present novel techniques that allow extremely rapid audio indexing, at rates approaching several thousand times real time. Unlike other methods, these techniques do not depend on a fixed vocabulary recognition system or on keywords that must be known well in advance. Using statistical methods developed for text, these indexing techniques allow rapid and efficient retrieval and browsing of audio and video documents. Thk paper presents the project background, the indexing and retrieval techniques, and a video mail retrieval application incorporating content-based audio indexing, retrieval, and browsing.","cites":"56","conferencePercentile":"77.77777778"},{"venue":"ACM Multimedia","id":"2d411826cd7865638b65e1b5f92043c245f009f9","venue_1":"ACM Multimedia","year":"2015","title":"Collaborative Fashion Recommendation: A Functional Tensor Factorization Approach","authors":"Yang Hu, Xi Yi, Larry S. Davis","author_ids":"4371098, 7929852, 1693428","abstract":"With the rapid expansion of online shopping for fashion products, effective fashion recommendation has become an increasingly important problem. In this work, we study the problem of personalized outfit recommendation, i.e. automatically suggesting outfits to users that fit their personal fashion preferences. Unlike existing recommendation systems that usually recommend individual items, we suggest sets of items, which interact with each other, to users. We propose a functional tensor factorization method to model the interactions between user and fashion items. To effectively utilize the multi-modal features of the fashion items, we use a gradient boosting based method to learn nonlinear functions to map the feature vectors from the feature space into some low dimensional latent space. The effectiveness of the proposed algorithm is validated through extensive experiments on real world user data from a popular fashion-focused social network.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"86a200647f89ed81db8031ccfbcb5368a32bed6c","venue_1":"ACM Multimedia","year":"2015","title":"SHOE: Sibling Hashing with Output Embeddings","authors":"Sravanthi Bondugula, Varun Manjunatha, Larry S. Davis, David S. Doermann","author_ids":"1828838, 1977256, 1693428, 1679490","abstract":"We present a supervised binary encoding scheme for image retrieval that learns projections by taking into account similarity between classes obtained from output embeddings. Our motivation is that binary hash codes learned in this way improve the visual quality of retrieval results by ranking related (or ``sibling'') class images before unrelated class images. We employ a sequential greedy optimization that learns relationship aware projections by minimizing the difference between inner products of binary codes and output embedding vectors. We develop a joint optimization framework to learn projections which improve the accuracy of supervised hashing over the current state of the art with respect to standard and sibling evaluation metrics. We further obtain discriminative features learned from correlations of kernelized input CNN features and output embeddings, which significantly boosts performance. Experiments are performed on three datasets: CUB-2011, SUN-Attribute and ImageNet ILSVRC 2010, where we show significant improvement in sibling performance metrics over state-of-the-art supervised hashing techniques, while maintaining performance with respect to standard metrics.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"e69ca8b0bdf0fa9089707254964025b8812b7b58","venue_1":"ACM Multimedia","year":"2011","title":"Socially relevant simulation games: a design study","authors":"Ramin Tadayon, Ashish Amresh, Winslow Burleson","author_ids":"1698472, 2145037, 1765229","abstract":"Socially Relevant Simulation Games (SRSG), a new medium for social interaction, based on real-world skills and skill development, creates a single gaming framework that connects both serious and casual players. Through a detailed case study this paper presents a design process and framework for SRSG, in the context of mixed-reality golf swing simulations. The SRSG, entitled \"World of Golf\", utilizes a real-time expert system to capture, analyze, and evaluate golf swing metrics. The game combines swing data with players' backgrounds, e.g., handicaps, to form individual profiles. These profiles are then used to implement a golf simulation game using artificially controlled agents who inherit the skill levels of their corresponding human users. The simulation and assessment modules provide the serious player with tools to build golf skills while allowing casual players to engage within a simulated social world. A framework that incorporates simulated golf competitions among these social agents is presented and validated by comparing the usage statistics of 10 PGA Golf Management (PGM) students with 10 non-professional students.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"fcfac3a7ffbc5259c2443113a23541ef87d66721","venue_1":"ACM Multimedia","year":"2013","title":"Cross-media topic mining on wikipedia","authors":"Xikui Wang, Yang Liu, Donghui Wang, Fei Wu","author_ids":"1853550, 1750084, 1721899, 1695826","abstract":"As a collaborative wiki-based encyclopedia, Wikipedia provides a huge amount of articles of various categories. In addition to their text corpus, Wikipedia also contains plenty of images which makes the articles more intuitive for readers to understand. To better organize these visual and textual data, one promising area of research is to jointly model the embedding topics across multi-modal data (i.e, <i>cross-media</i>) from Wikipedia. In this work, we propose to learn the projection matrices that map the data from heterogeneous feature spaces into a unified latent topic space. Different from previous approaches, by imposing the l<sub>1</sub> regularizers to the projection matrices, only a small number of relevant visual/textual words are associated with each topic, which makes our model more interpretable and robust. Furthermore, the correlations of Wikipedia data in different modalities are explicitly considered in our model. The effectiveness of the proposed topic extraction algorithm is verified by several experiments conducted on real Wikipedia datasets.","cites":"3","conferencePercentile":"60"},{"venue":"ACM Multimedia","id":"f88678136d2e10e5f388b4129ecab84998c04675","venue_1":"ACM Multimedia","year":"2016","title":"Video eCommerce: Towards Online Video Advertising","authors":"Zhi-Qi Cheng, Yang Liu, Xiao Wu, Xian-Sheng Hua","author_ids":"3493929, 1750084, 4824827, 1746102","abstract":"The prevalence of online videos provides an opportunity for e-commerce companies to exhibit their product ads in videos by recommendation. In this paper, we propose an advertising system named Video eCommerce to exhibit appropriate product ads to particular users at proper time stamps of videos, which takes into account video semantics, user shopping preference and viewing behavior feedback by a two-level strategy. At the first level, Co-Relation Regression (CRR) model is novelly proposed to construct the semantic association between keyframes and products. Heterogeneous information network (HIN) is adopted to build the user shopping preference from two different e-commerce platforms, Tmall and MagicBox, which alleviates the problems of data sparsity and cold start. In addition, Video Scene Importance Model (VSIM) utilizes the viewing behavior of users to embed ads at the most attractive position within the video stream. At the second level, taking the results of CRR, HIN and VSIM as the input, Heterogeneous Relation Matrix Factorization (HRMF) is applied for product advertising. Extensive evaluation on a variety of online videos from Tmall MagicBox demonstrates that Video eCommerce achieves promising performance, which significantly outperforms the state-of-the-art advertising methods.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"dd52841191e65a276d13384b65ffef0b7540bb9b","venue_1":"ACM Multimedia","year":"1999","title":"Taming the wolf in sheep's clothing: privacy in multimedia communications","authors":"Anne Adams, M. Angela Sasse","author_ids":"1736601, 1752376","abstract":"When ubiquitous multimedia technology is introduced in an organization, the privacy implications of that technology are rarely addressed. Users usually extend the trust they have in an organization to the technology it employs. This paper reports results from interviews with 24 Internet Engineering Task Force (IETF) attendees whose presentations or contributions to IETF sessions were transmitted on the multicast backbone (Mbone). Due to a high level of trust in the organization, these users had few initial concerns about the privacy implications of this technology. However, interviewees' trust relied on inaccurate assumptions, since the interviews revealed a number of potential and actual invasions of privacy in transmission, recording and editing of multicast data. Previous research found that users who experience an unexpected invasion of their privacy are not only likely to reject the technology that afforded the invasion, but lose trust in the organization that introduced it [2,3]. We discuss a number of mechanisms and policies for protecting users' privacy in this particular application, and propose a strategy for introducing networked multimedia technology in general.","cites":"24","conferencePercentile":"73.52941176"},{"venue":"ACM Multimedia","id":"d7a817cfa0c6a38049aedcb79c2abfb0db66d3e3","venue_1":"ACM Multimedia","year":"1996","title":"A Multimedia System for Authoring Motion Pictures","authors":"Ronald Baecker, Alan J. Rosenthal, Naomi Friedlander, Eric Smith, Andrew Cohen","author_ids":"1715312, 2204333, 2454110, 8302957, 5457915","abstract":"MAD (Movie Authoring and Design) is a novel design and authoring system that facilitates the process of creating dynamic visual presentations such as motion pictures and lecture-demonstrations. MAD supports the process by enhancing the author's ability to structure and modify a presentation and to visualize the ultimate result. It does this by allowing both top-down design and bottom-up creation with a hierarchical multimedia document representation; by supporting the flexible inclusion and combination of words, images, sounds, and video sequences; and by providing real-time playback of the best approximation to the ultimate presentation that can be produced at any stage of the design process. MAD represents a paradigm shift from traditional methods of authoring and producing motion pictures. Its development therefore requires in-depth observation of a variety of users working on a variety of filmmaking projects. After describing the key concepts underlying MAD and the current, second-generation prototype software, we describe a number of interesting applications of MAD. In doing so, we review how we have worked with users in an iterative design process and how studies of the work of these users have informed key design issues. BACKGROUND Computer technology has been used increasingly in motion picture production over the past decade. One of the most significant uses has been in post-production, including the very successful digital video storage, editing, and assembly systems (e. g., Avid, [4]). Pre-production application systems have included word processors and specialized script writing systems, systems for the design of storyboards, project management tools and spreadsheets used for","cites":"49","conferencePercentile":"66.66666667"},{"venue":"ACM Multimedia","id":"18f77fe185e41ca5ee2688ee7cc206c9bf45253b","venue_1":"ACM Multimedia","year":"2004","title":"Using web frequency within multi-media exhibitions","authors":"David A. Shamma, Sara Owsley, Shannon Bradshaw, Kristian J. Hammond","author_ids":"1760364, 3344151, 2467419, 2796629","abstract":"In this article, we explore the structure of the web as an indicator of popular culture and its use in multi-media exhibits. In a series of art and technology installations, the software agency needs to keep 'grounded' to what people can readily understand. We administered a survey to understand how people perceived word and phrase obscurity related with frequency information gathered from a popular Web search engine. We found the frequency data gathered from the engine closely matched judgments gathered from people. The results of this study point to the new applications of the WWW in art and multi-media exhibits as an indicator of popular culture.","cites":"6","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"0780c7340929bc4c5aa24b252c8efd4069ec99a6","venue_1":"ACM Multimedia","year":"2004","title":"The association engine: a free associative digital improviser","authors":"Sara Owsley, David A. Shamma, Kristian J. Hammond, Shannon Bradshaw, Sanjay Sood","author_ids":"3344151, 1760364, 2796629, 2467419, 2150824","abstract":"In this article, we present the Association Engine, a multimedia installation that explores the space of language and exposes connections between words by performing a series of improvisational games. It stands out from previous efforts in the realm of computers and theater as the agency has been empowered with a cultural understanding, more specifically, it uses the Web as a source of cultural reality.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"1c5f56429300be266bf47d4cec6ee3ecaf5a8667","venue_1":"ACM Multimedia","year":"2005","title":"MusicStory: a personalized music video creator","authors":"David A. Shamma, Bryan Pardo, Kristian J. Hammond","author_ids":"1760364, 1744936, 2796629","abstract":"In this paper, we describe MusicStory, a system that automatically creates videos to accompany music with lyrics. MusicStory uses common search engines, photo-sharing websites, and simple analysis of the dynamics and tempo of the music to create personalized photo-narratives. Video pacing and content is based on the content of the song and structure of the image repositories selected. The image associations MusicStory presents amplify the emotional experience by externalizing the imagery in song lyrics with the content found within a social network. The resulting work juxtaposes the meanings inherent in the social network with those in the song.","cites":"12","conferencePercentile":"63.11881188"},{"venue":"ACM Multimedia","id":"8b57278fc6df038615883754e4f5e2bbcb0a6cd1","venue_1":"ACM Multimedia","year":"1999","title":"An excitation level based psychoacoustic model for audio compression","authors":"Ye Wang, Miikka Vilermo","author_ids":"1681196, 3227495","abstract":"This paper describes an excitation level based psychoacoustic model to estimate the simultaneous masking threshold for audio coding. The system has the following stages: 1) a windowing function; 2) a time-to-frequency transformation; 3) an excitation level calculation block similar to that in Moore and Glasberg's loudness model; 4) a correction factor for estimating masking threshold; 5) the inclusion of the absolute masking threshold; 6) the output Signal-to-Masking ratio. We have evaluated the performance by integrating the proposed psychoacoustic model into an audio coder similar to MPEG-2 AAC, which contains only the basic coding tools. Our model performs better than or as well as the psychoacoustic model suggested in the MPEG-2 AAC audio coding standard for all the test signals. We can achieve almost transparent quality with bitrate below 64 kbps for most of the critical test signals. Significant improvements have been achieved with speech signals, which are always difficult for transform audio coders.","cites":"2","conferencePercentile":"26.47058824"},{"venue":"ACM Multimedia","id":"3c5b16553f16e9c5228ca5a3b69fdffed25e38b2","venue_1":"ACM Multimedia","year":"2001","title":"A compressed domain beat detector using MP3 audio bitstreams","authors":"Ye Wang, Miikka Vilermo","author_ids":"1681196, 3227495","abstract":"This paper presents a novel beat detector that processes MPEG-1 Layer III (known as MP3) encoded audio bitstreams directly in the compressed domain. Most previous beat detection or tracking systems dealing with MIDI or PCM signals are not directly applicable to compressed audio bitstreams, such as MP3 bitstreams. We have developed the beat detector as a part of a beat-pattern based error concealment scheme for streaming music over error prone channels. Special effort was used to obtain a tailored trade-off between performance, complexity and memory consumption for this specific application. A comparison between the machine-detected results to the human annotation has shown that the proposed method correctly tracked beats in 4 out of 6 popular music test signals. The results were analyzed.","cites":"21","conferencePercentile":"81.63265306"},{"venue":"ACM Multimedia","id":"1c63b201a27442462cb7f5abc87b28f3b8eb5dcc","venue_1":"ACM Multimedia","year":"2006","title":"Buzz: telling compelling stories","authors":"Sara Owsley, Kristian J. Hammond, David A. Shamma, Sanjay Sood","author_ids":"3344151, 2796629, 1760364, 2150824","abstract":"This paper describes a digital theater installation called <i>Buzz</i>. <i>Buzz</i> consists of virtual actors who express the collective voice generated by weblogs (blogs). These actors find compelling stories from blogs and perform them. In this paper, we explore what it means for a story to be compelling and describe a set of techniques for retrieving compelling stories. We also outline an architecture for high level direction of a performance using Adaptive Retrieval Charts (ARCs), allowing a director-level of interaction with the performance system. Our overall goal in this work is to build a model of human behavior on a new foundation of query formation, information retrieval and filtering.","cites":"7","conferencePercentile":"58.29015544"},{"venue":"ACM Multimedia","id":"9d59e0f75be3add1f5bf9d20930f9b3da8f647cf","venue_1":"ACM Multimedia","year":"2014","title":"Who's Time Is It Anyway?: Investigating the Accuracy of Camera Timestamps","authors":"Bart Thomee, Jose G. Moreno, David A. Shamma","author_ids":"2463875, 1679521, 1760364","abstract":"People take photos all over the world at all times of day; each photo depicting a place and a moment worth capturing. In the context of multimedia analysis and social computing, accurate location and time information about where and when these photos were taken is of importance for understanding event semantics, image content and many other purposes. While location information associated with photos is known to be relatively accurate, time is not. From a sample of 10 million public Flickr photos, we observe that 37% of the photos differ more than an hour between their camera timestamps and GPS timestamps with respect to local time at the locations where the photos were taken. Erroneous time information may adversely influence the correctness of any kind of temporal analysis that relies on camera timestamps, as well as research and real-world applications that require accurate knowledge of when and where photos were captured. In light of our observations we propose a simple yet effective metadata-only technique for improving the accuracy of camera timestamps.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"c9a6017a7753bb6592200dae28ba2edb7b81438f","venue_1":"ACM Multimedia","year":"2014","title":"Points of Interest Detection from Multiple Sensor-Rich Videos in Geo-Space","authors":"Ying Zhang, Roger Zimmermann, Luming Zhang, David A. Shamma","author_ids":"1752812, 1790974, 1763785, 1760364","abstract":"Recently, the popularity of user generated videos has highlighted efficient video indexing and browsing as an urgent problem. Points of interest (POI) detection is a technique to address this issue by establishing the implicit relationship among different media resources. The majority of existing studies detect POI by visual similarity, leveraging computer vision techniques. However, these methods suffer from high computational complexity when processing large-scale video sets and are challenging due to the sparse visual correlation among different consumer videos. The advent of geo-referenced videos provides an opportunity to detect POIs in an efficient manner. In this work, we first propose a probability model to formulate the capture intention distribution for video frames. Second, we detect the POIs by considering the contributions from multiple videos. Evaluations demonstrate that our algorithm successfully detects POIs with a reduced error distance from several dozens to a few meters.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"8a3d969732951e4d22c608d465b091b9272f03bb","venue_1":"ACM Multimedia","year":"2015","title":"Multimedia COMMONS - Community-Organized Multimodal Mining: Opportunities for Novel Solutions (MMCommons Workshop 2015)","authors":"Gerald Friedland, Chong-Wah Ngo, David A. Shamma","author_ids":"1797144, 1751681, 1760364","abstract":"The Multimedia COMMONS workshop laid the groundwork for developing a research community around the Multimedia Genome Project (MMGP), an initiative initially focused on annotation of---and research using---the 99.2 million images and nearly 800,000 videos in the Yahoo Flickr Creative Commons 100 Million dataset (YFCC100M). Current and potential users of the YFCC100M presented new research and systems that used this unprecedentedly large, unprecedentedly open-source dataset; discussed ideas for future data challenges and new benchmarking tasks that would not previously have been possible; and suggested priorities and plans for annotation and distribution based on community needs and interests.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"4c9dc6797f086cfcefa9b527807384248bd7803e","venue_1":"ACM Multimedia","year":"2004","title":"Sumi-nagashi: creation of new style media art with haptic digital colors","authors":"Shunsuke Yoshida, Jun Kurumisawa, Haruo Noma, Nobuji Tetsutani, Kenichi Hosaka","author_ids":"1787112, 2303137, 2743783, 1767833, 2608946","abstract":"This installation provides painters with a method for feeling attributes of digital colors and a fluid canvas. When a user of this installation moves the stylus paintbrush over the digital canvas, he/she senses the \"weight of the colors\" through the brush. For example, the user experiences dark colors as heavy in weight and light colors as light in weight. Complex painting is expressed as a mixed tactile sensation using a new desk-style force feedback system called the \"Proactive Desk.\" Other existing digital painting systems that use haptic cues usually aim to be physically and visually correct. In this approach, however, we invested effort in enhancing the relationship between digital colors as a virtual material and the sense of touch. Additionally, we took the importance of co-located drawing work space for creative tasks into account. We believe that this experience will arouse new inspiration and give digital painters the opportunity to experience again how important touch is for creativity in art.","cites":"4","conferencePercentile":"39.95098039"},{"venue":"ACM Multimedia","id":"70fa13b31906c59c0b79d8c18a0614c5aaf77235","venue_1":"ACM Multimedia","year":"2010","title":"KPB-SIFT: a compact local feature descriptor","authors":"Gangqiang Zhao, Ling Chen, Gencai Chen, Junsong Yuan","author_ids":"2581147, 1717165, 3025640, 1746449","abstract":"Invariant feature descriptors such as SIFT and GLOH have been demonstrated to be very robust for image matching and object recognition. However, such descriptors are typically of high dimensionality, e.g. 128-dimension in the case of SIFT. This limits the performance of feature matching techniques in terms of speed and scalability. A new compact feature descriptor, called Kernel Projection Based SIFT (KPB-SIFT), is presented in this paper. Like SIFT, our descriptor encodes the salient aspects of image information in the feature point's neighborhood. However, instead of using SIFT's smoothed weighted histograms, we apply kernel projection techniques to orientation gradient patches. The produced KPB-SIFT descriptor is more compact as compared to the state-of-the-art, does not require pre-training step needed by PCA based descriptors, and shows superior advantages in terms of distinctiveness, invariance to scale, and tolerance of geometric distortions. We extensively evaluated the effectiveness of KPB-SIFT with datasets acquired under varying circumstances.","cites":"7","conferencePercentile":"70.82191781"},{"venue":"ACM Multimedia","id":"6e6b68fdf5e32386c215736ff47ab89fbad66e53","venue_1":"ACM Multimedia","year":"2009","title":"MagicPhotobook: designer inspired, user perfected photo albums","authors":"Yuli Gao, Clayton Brian Atkins, Phil Cheatle, Jun Xiao, Xuemei Zhang, Hui Chao, Peng Wu, Daniel Tretter, David Slatter, Andrew Carter, Roland Penny, Chris Willis","author_ids":"2026151, 1870182, 2631305, 2781973, 5113938, 2972266, 1754821, 2096613, 3315505, 5353182, 2888657, 2873810","abstract":"Computer-assisted photo album creation continues to be a challenging application as it requires integrated technical solutions to many difficult problems. Effective solutions must leverage both design knowledge and image understanding algorithms to automate time-consuming tasks like image selection, grouping, cropping, layout and background selection. At the same time, they should allow the user to cater to personal tastes by fine-tuning aspects of album appearance. MagicPhotobook is a photobook authoring system that takes steps in these directions by providing advances over prior solutions in the following areas: automatic image selection and theme-based image grouping; dynamic page layout; automatic cropping; automatic background selection; design-preserving background artwork transformation; and a simple yet powerful user interface for personalization.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"0901b1638217aa464f980df65165fb54f7f17741","venue_1":"ACM Multimedia","year":"2009","title":"Face based image navigation and search","authors":"Tong Zhang, Jun Xiao, Di Wen, Xiaoqing Ding","author_ids":"2012471, 2781973, 2069385, 1683321","abstract":"People are often the most important subjects in photos, and the ability of finding photos of a particular person easily and quickly in an image collection is highly desired. In this paper, we present a face clustering system which automatically groups photos into clusters, with each cluster containing photos of the same person. This is done based on an advanced face recognition engine and a semi-supervised clustering approach. The system achieved good clustering accuracy when tested on different image sets and by different users. Moreover, features such as adding new images, face cluster navigation and face based image retrieval are added that greatly improve the usability of the system. It also facilitates efficient manual manipulations of clustering results. On top of this technology, image navigation systems have been built, including the \"face bubble\" visualization which provides one-glance view of a photo collection, and shows the relations among people.","cites":"9","conferencePercentile":"64.04958678"},{"venue":"ACM Multimedia","id":"2c8c3776f0f4ab04514353033011df1ffedbe2ee","venue_1":"ACM Multimedia","year":"2010","title":"A 3d data intensive tele-immersive grid","authors":"Benjamin Petit, Thomas Dupeux, Benoit Bossavit, Joeffrey Legaux, Bruno Raffin, Emmanuel Melin, Jean-Sébastien Franco, Ingo Assenmacher, Edmond Boyer","author_ids":"2978195, 2317922, 2922994, 2647219, 2583571, 1805751, 1754664, 2178505, 1719388","abstract":"Networked virtual environments like Second Life enable distant people to meet for leisure as well as work. But users are represented through avatars controlled by keyboards and mouses, leading to a low sense of presence especially regarding body language. Multi-camera real-time 3D modeling offers a way to ensure a significantly higher sense of presence. But producing quality geometries, well textured, and to enable distant user tele-presence in non trivial virtual environments is still a challenge today.\n In this paper we present a tele-immersive system based on multi-camera 3D modeling. Users from distant sites are immersed in a rich virtual environment served by a parallel terrain rendering engine. Distant users, present through their 3D model, can perform some local interactions while having a strong visual presence. We experimented our system between three large cities a few hundreds kilometers apart from each other. This work demonstrate the feasibility of a rich 3D multimedia environment ensuring users a strong sense of presence.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"a6e6500e0112c3f82ecbe50c99263f08a9342912","venue_1":"ACM Multimedia","year":"2009","title":"Location sensitive indexing for image-based advertising","authors":"Dechao Liu, Matthew R. Scott, Rongrong Ji, Wei Jiang, Hongxun Yao, Xing Xie","author_ids":"3074015, 1915350, 1725599, 4344679, 1720100, 1687677","abstract":"This paper introduces the architecture of our location sensitive indexing model which is used in a platform designed to deliver advertisements to users who primarily utilize images as queries instead of textual keywords. The indexing model facilitates an advertiser's ability to bid on images, such as billboards or logos, in order to obtain user feedback in judging image attractiveness. Additionally, the model enables automatic evaluation of advertisement popularity by mining users' query logs, which is critical for generating advertisement recommendations. The location sensitive architecture of this model enables effective and efficient functionality in large-scale scenarios. In the model's structure, our Location Sensitive Visual Indexing model (LSVI) incorporates location information that subdivides geographical regions for precise and localized image matching. By collecting feedback from mobile users, location-based mining can also help discover popular advertisements as well as their representative images. We have deployed our platform into a real-world advertising system in Beijing, China, which demonstrates effective results in comparative studies with both alternative and state-of-the-art approaches.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"1a5072f3ba1f030b4c15f170b5ea85ea43f4921b","venue_1":"ACM Multimedia","year":"2010","title":"Personalized photograph ranking and selection system","authors":"Che-Hua Yeh, Yuan-Chen Ho, Brian A. Barsky, Ming Ouhyoung","author_ids":"2560391, 2101810, 1700557, 1744863","abstract":"In this paper, we propose a novel personalized ranking system for amateur photographs. Although some of the features used in our system are similar to previous work, new features, such as texture, RGB color, portrait (through face detection), and black-and-white, are included for individual preferences. Our goal of automatically ranking photographs is not intended for award-wining professional photographs but for photographs taken by amateurs, especially when individual preference is taken into account.\n The performance of our system in terms of precision-recall diagram and binary classification accuracy (93%) is close to the best results to date for both overall system and individual features. Two personalized ranking user interfaces are provided: one is feature-based and the other is example-based. Although both interfaces are effective in providing personalized preferences, our user study showed that example-based was preferred by twice as many people as feature-based.","cites":"35","conferencePercentile":"94.38356164"},{"venue":"ACM Multimedia","id":"d656386578e764bbd73d53e988d23a46227e90d6","venue_1":"ACM Multimedia","year":"2003","title":"Looking into video frames on small displays","authors":"Xin Fan, Xing Xie, He-Qin Zhou, Wei-Ying Ma","author_ids":"1710408, 1687677, 2457832, 1705244","abstract":"With the growing popularity of personal digital assistants and smart phones, people have become enthusiastic to watch videos through these mobile devices. However, a crucial challenge is to provide a better user experience for browsing videos on the limited and heterogeneous screen sizes. In this paper, we present a novel approach which allows users to overcome the display constraints by zooming into video frames while browsing. An automatic approach for detecting the focus regions is introduced to minimize the amount of user interaction. In order to improve the quality of output stream, virtual camera control is employed in the system. Preliminary evaluation shows that this approach is an effective way for video browsing on small displays.","cites":"55","conferencePercentile":"89.18918919"},{"venue":"ACM Multimedia","id":"003469391ad902ef42928fc7726f397af7b665c6","venue_1":"ACM Multimedia","year":"2014","title":"Validating an iOS-based Rhythmic Auditory Cueing Evaluation (iRACE) for Parkinson's Disease","authors":"Shenggao Zhu, Robert J. Ellis, Gottfried Schlaug, Yee Sien Ng, Ye Wang","author_ids":"2901597, 2773584, 2159369, 2716033, 1681196","abstract":"Movement disorders such as Parkinson's disease (PD) will affect a rapidly growing segment of the population as society continues to age. Rhythmic Auditory Cueing (RAC) is a well-supported evidence-based intervention for the treatment of gait impairments in PD. RAC interventions have not been widely adopted, however, due to limitations in access to personnel, technological, and financial resources. To help \"scale up\" RAC for wider distribution, we have developed an iOS-based Rhythmic Auditory Cueing Evaluation (iRACE) mobile application to deliver RAC and assess motor performance in PD patients. The touchscreen of the mobile device is used to assess motor timing during index finger tapping, and the device's built-in tri-axial accelerometer and gyroscope to assess step time and step length during walking. Novel machine learning-based gait analysis algorithms have been developed for iRACE, including heel strike detection, step length quantification, and left-versus-right foot identification. The concurrent validity of iRACE was assessed using a clinic-standard instrumented walking mat and a pair of force-sensing resistor sensors. Results from 10 PD patients reveal that iRACE has low error rates (<&#177;1.0%) across a set of four clinically relevant outcome measures, indicating a potentially useful clinical tool.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"a67b29a4d7b15a1380bf24f4c172381d97c839cc","venue_1":"ACM Multimedia","year":"2008","title":"Multimedia power management on a platter: from audio to video & games","authors":"Samarjit Chakraborty, Ye Wang","author_ids":"1722392, 1681196","abstract":"Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia applications running on such battery-operated portable devices. In particular, we will discuss a host of techniques, some of which are applicable to audio processing applications, some to video processing, and the others to interactive 3D game applications. The tutorial will be helpful to students, researchers, application developers and engineers who have a background in traditional real-time multimedia applications and would like to get an overview of the important issues and solutions pertaining to using and developing power management techniques for the multimedia domain.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"00a6ff44da7b64ac866c5c8f7feb350c06637815","venue_1":"ACM Multimedia","year":"1994","title":"Content-Based Retrieval of Segmented Images","authors":"Tat-Seng Chua, S.-K. Lim, Hung Keng Pung","author_ids":"1684968, 1821087, 1766776","abstract":"Most general content-based image retrieval techniques use colour and texture as main retrieval indices. A recent technique uses colour pairs to model distinct object boundaries for retrieval. These techniques have been applied to overall image contents without taking into account the characteristics of individual objects. While the techniques work well for the retrieval of images with similar overall contents (including backgrounds), their accuracies are limited because they are unable to take advantage of individual object's visual characteristics, and to perform object-level retrieval. This paper looks specifically at the use of colour-pair technique for fuzzy object-level image retrieval. Three extensions are applied to the basic colour-pair technique: (a) the development of a similarity-based ranking formula for colour-pairs matching; (b) the use of segmented objects for object-level retrieval; and (c) the inclusion of perceptually similar colours for fuzzy retrieval. A computer-aided segmentation technique is developed to segment the images' contents. Experimental results indicate that the extensions have led to substantial improvements in the retrieval performance. These extensions are sufficiently general and can be applied to other content-based image retrieval techniques.","cites":"28","conferencePercentile":"67.79661017"},{"venue":"ACM Multimedia","id":"599a3b08877292027cc5f4f82707d041ff7da316","venue_1":"ACM Multimedia","year":"2009","title":"CompositeMap: a novel music similarity measure for personalized multimodal music search","authors":"Bingjun Zhang, Qiaoliang Xiang, Ye Wang, Jialie Shen","author_ids":"2522345, 3235642, 1681196, 1723020","abstract":"How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems in music information retrieval. This paper demonstrates a novel multimodal and adaptive music similarity measure (CompositeMap) with its application in a personalized multimodal music search system. CompositeMap can effectively combine music properties from different aspects into compact signatures via supervised learning, which lays the foundation for effective and efficient music search. In addition, an incremental Locality Sensitive Hashing algorithm is developed to support more efficient search processes. Experimental results based on two large music collections reveal various advantages in effectiveness, efficiency, adaptiveness, and scalability of the proposed music similarity measure and the music search system.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"6f21978f435cf751fd7966814c69ae9ac4321472","venue_1":"ACM Multimedia","year":"2009","title":"Comprehensive query-dependent fusion using regression-on-folksonomies: a case study of multimodal music search","authors":"Bingjun Zhang, Qiaoliang Xiang, Huanhuan Lu, Jialie Shen, Ye Wang","author_ids":"2522345, 3235642, 6364142, 1723020, 1681196","abstract":"The combination of heterogeneous knowledge sources has been widely regarded as an effective approach to boost retrieval accuracy in many information retrieval domains. While various technologies have been recently developed for information retrieval, multimodal music search has not kept pace with the enormous growth of data on the Internet. In this paper, we study the problem of integrating multiple online information sources to conduct effective query dependent fusion (QDF) of multiple search experts for music retrieval. We have developed a novel framework to construct a knowledge space of users' information need from online folksonomy data. With this innovation, a large number of comprehensive queries can be automatically constructed to train a better generalized QDF system against unseen user queries. In addition, our framework models QDF problem by regression of the optimal combination strategy on a query. Distinguished from the previous approaches, the regression model of QDF (RQDF) offers superior modeling capability with less constraints and more efficient computation. To validate our approach, a large scale test collection has been collected from different online sources, such as Last.fm, Wikipedia, and YouTube. All test data will be released to the public for better research synergy in multimodal music search. Our performance study indicates that the accuracy, efficiency, and robustness of the multimodal music search can be improved significantly by the proposed folksonomy-RQDF approach. In addition, since no human involvement is required to collect training examples, our approach offers great feasibility and practicality in system development.","cites":"8","conferencePercentile":"60.12396694"},{"venue":"ACM Multimedia","id":"b733ef09330f1042ed886e2b53e47e5b7a8dd9d2","venue_1":"ACM Multimedia","year":"2013","title":"Non-reference audio quality assessment for online live music recordings","authors":"Zhonghua Li, Ju-Chiang Wang, Jingli Cai, Zhiyan Duan, Hsin-Min Wang, Ye Wang","author_ids":"2746295, 1727820, 5370107, 2591822, 1710199, 1681196","abstract":"Immensely popular video sharing websites such as YouTube have become the most important sources of music information for Internet users and the most prominent platform for sharing live music. The audio quality of this huge amount of live music recordings, however, varies significantly due to factors such as environmental noise, location, and recording device. However, most video search engines do not take audio quality into consideration when retrieving and ranking results. Given the fact that most users prefer live music videos with better audio quality, we propose the first automatic, non-reference audio quality assessment framework for live music video search online. We first construct two annotated datasets of live music recordings. The first dataset contains 500 human-annotated pieces, and the second contains 2,400 synthetic pieces systematically generated by adding noise effects to clean recordings. Then, we formulate the assessment task as a ranking problem and try to solve it using a learning-based scheme. To validate the effectiveness of our framework, we perform both objective and subjective evaluations. Results show that our framework significantly improves the ranking performance of live music recording retrieval and can prove useful for various real-world music applications.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"16e4e55f9f8e5d7bf9da50c705dbc2a92e835b4f","venue_1":"ACM Multimedia","year":"2012","title":"MOGAT: mobile games with auditory training for children with cochlear implants","authors":"Yinsheng Zhou, Khe Chai Sim, Patsy Tan, Ye Wang","author_ids":"3128975, 1693612, 2451550, 1681196","abstract":"Cochlear implants have improved the lives of tens of thousands of the hearing impaired by providing sufficient auditory perception for speech, but these devices are far from satisfactory for music perception. Many cochlear implant recipients, especially pre-lingually deafened children, have difficulty recognizing and producing specific pitches. To improve musical auditory habilitation for children post cochlear implantation, we developed MOGAT: MObile Games with Auditory Training. The system includes three musical games built with off-the-shelf mobile devices to train their pitch perception and intonation skills respectively, and a cloud-based web service which allows music therapists to monitor and design individual training for children. The design of the games and web service was informed by a pilot survey (N=60 children). To ensure widespread use with low-cost mobile devices, we minimized the computation load while retaining highly accurate audio analysis. A 6-week user study (N=15 children) showed that the music habilitation with MOGAT was intuitive, enjoyable and motivating. It has improved most children's pitch discrimination and production, and several children's improvement was statistically significant (<i>p</i><0.05).","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"0779f013e485945cb4d1642134c7ec0b30bb865b","venue_1":"ACM Multimedia","year":"2009","title":"MOGFUN: musical mObile group for FUN","authors":"Yinsheng Zhou, Zhonghua Li, Dillion Tan, Graham Percival, Ye Wang","author_ids":"3128975, 2746295, 2625696, 2307208, 1681196","abstract":"The computational power and sensory capabilities of mobile devices are increasing dramatically these days, rendering them suitable for real-time sound synthesis and various musical expressions. In this paper, we demonstrate a novel mobile music making system which leverages the ubiquity, ultra-mobility, and multi-modality of mobile devices (iPod touch) for people to create and compose music collaboratively. Unlike the conventional music making applications which generate the music on a single mobile device with a preset sound and interface, our system allows several players in a group to be connected together through wireless LAN network, creating music with different sounds and interfaces. Finally, the performance can be recorded as a single music file and played back in the future. The paper also shows some application scenarios for this collaborative music making system in future research.","cites":"3","conferencePercentile":"36.98347107"},{"venue":"ACM Multimedia","id":"1ce72f929a457eb65697f757197a078e9899fba9","venue_1":"ACM Multimedia","year":"2003","title":"Panoptes: scalable low-power video sensor networking technologies","authors":"Wu-chi Feng, Brian Code, Edward C. Kaiser, Mike Shea, Wu-chang Feng, Louis Bavoil","author_ids":"1789317, 2817391, 2302123, 2918452, 1737674, 2058872","abstract":"Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This article describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance. We describe two video sensor platforms that can deliver high-quality video over 802.11 networks with a power requirement less than 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system.","cites":"108","conferencePercentile":"95.4954955"},{"venue":"ACM Multimedia","id":"20d6c9a0e28ce4a27ac68de86e0c0b265d625622","venue_1":"ACM Multimedia","year":"2009","title":"SaVE: sensor-assisted motion estimation for efficient h.264/AVC video encoding","authors":"XiaoMing Chen, Zhendong Zhao, Ahmad Rahmati, Ye Wang, Lin Zhong","author_ids":"1772182, 2435606, 3132962, 1681196, 5581056","abstract":"Motion estimation is a key component of modern video encoding and is very compute-intensive. We present a novel Sensor-assisted Video Encoding (SaVE) method to reduce the computational complexity of motion estimation in H.264/AVC encoders, leveraging accelerometers and digital compasses that are increasingly available on mobile devices. Using these sensors, SaVE calculates the rotational movement of a camera and then infers the global motion in the camera image sensor; it subsequently employs the estimated global motion to simplify the state-of-the-art motion estimation algorithms, UMHS and EPZS used in H.264/AVC encoders. We have constructed a prototype of SaVE and report extensive evaluation of it. Our experimental results show that SaVE can reduce the computations of UMHS and EPZS algorithms by up to 27% and 18%, respectively, while achieving the same or better video quality.","cites":"10","conferencePercentile":"67.97520661"},{"venue":"ACM Multimedia","id":"c9915bb620a5eb040469a4b58071ba5e933a283e","venue_1":"ACM Multimedia","year":"2009","title":"Mining city landmarks from blogs by graph modeling","authors":"Rongrong Ji, Xing Xie, Hongxun Yao, Wei-Ying Ma","author_ids":"1725599, 1687677, 1720100, 1705244","abstract":"Recent years have witnessed great prosperity in community-contributed multimedia. Discovering, extracting, and summarizing knowledge from these data enables us to make better sense of the world. In this paper, we report our work on mining famous city landmarks from blogs for personalized tourist suggestions. Our main contribution is a graph modeling framework to discover city landmarks by mining blog photo correlations with community supervision. This modeling fuses context, content, and community information in a style that simulates both static (PageRank) and dynamic (HITS) ranking models to highlight representative data from the consensus of blog users.\n Preliminary, we identify geographical locations of page contents to harvest city sight photos from Web blogs, based on which we structure these photos into a Scene-View hierarchy* within each city. Our graph modeling consists of two phases: First, within a given scene, we present a PhotoRank algorithm to discover its representative views, which analogizes PageRank to model context and content photo correlations for graph-based popularity propagation. Second, among scenes within each city, we present a Landmark-HITS model to discover city landmarks, which integrates author correlations to infer scene popularity in a semi-supervised reinforcement manner. Based on graph modeling, we further achieve personalized tourist suggestions by the collaborative filtering of tourism logs and author correlations. Based on a real-world dataset from Windows Live Spaces blogs containing nearly 400,000 sight photos, we have deployed our framework in a VisualTourism system, with comparisons to state-of-the-arts. We also investigate how the city popularities, user locations (e.g. Asian or Euro. blog users), and sequential events (e.g. Olympic Games) influence our Landmark discovery results and the tourist suggestion tendencies.","cites":"33","conferencePercentile":"92.76859504"},{"venue":"ACM Multimedia","id":"aa4e42588dd7433955f8d8ab2666f2d8a0b8c96f","venue_1":"ACM Multimedia","year":"2010","title":"A music search engine for therapeutic gait training","authors":"Zhonghua Li, Qiaoliang Xiang, Jason Hockman, Jianqing Yang, Yu Yi, Ichiro Fujinaga, Ye Wang","author_ids":"2746295, 3235642, 2505173, 3181596, 2842026, 2190426, 1681196","abstract":"A music retrieval system is introduced that incorporate tempo, cultural, and beat strength features to help music therapists provide appropriate music for gait training for Parkinson's patients. Unlike current methods available to music therapists (e.g., personal CD/MP3 library search) we propose a domain-specific search engine that utilizes database of music found on YouTube. We independently evaluate the efficacy of our tempo, cultural, and beat strength features on a music database extracted from YouTube. Results from our user study demonstrate the effectiveness and usefulness of our search engine for this application.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"1f4b6293b76b1e21339d76fa6ec1f3e3db18c46c","venue_1":"ACM Multimedia","year":"2011","title":"Document dependent fusion in multimodal music retrieval","authors":"Zhonghua Li, Bingjun Zhang, Ye Wang","author_ids":"2746295, 2522345, 1681196","abstract":"In this paper, we propose a novel multimodal fusion framework, document dependent fusion (DDF), which derives the optimal combination strategy for each individual document in the fusion process. For each document, we derive a document weight vector by estimating the descriptive abilities of its different modalities. The document weight vector also enables our framework to be easily integrated with existing multimodal fusion schemes, and achieve a better combination strategy for each document given a query. Experiments are conducted on a 17174-song music database to compare the retrieval accuracy of traditional query independent fusion and query dependent fusion approaches, and that obtained after integrating DDF with them. Experimental results indicate that DDF can significantly improve the retrieval performance of current fusion approaches.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"26e9776d51794b1540d5fceff2c026ceab5a2c23","venue_1":"ACM Multimedia","year":"2008","title":"Active post-refined multimodality video semantic concept detection with tensor representation","authors":"Yanan Liu, Fei Wu, Yueting Zhuang, Jun Xiao","author_ids":"5317322, 1695826, 1755711, 2781973","abstract":"In this paper, we resolve the problem of multi-modality video representation and semantic concept detection. Interaction and integration of multi-modality media types such as visual, audio and textual data in video are essential to video semantic analysis. Traditionally, videos are represented as vectors in the Euclidean space. Many learning algorithms are then taken to these vectors in a high dimensional space for dimension reduction, classification, clustering and so on. However, the multiple modalities in video not only have their own properties, but also have correlations among them; whereas the simple vector representation weakens the power of these relatively independent modalities and even ignores their relations to some extent. In this paper, we introduce a higher-order tensor framework for video analysis, in which we represent image, video and text three modalities in video shots as data points by the 3rd-order tensor called tensorshots. We propose a novel dimension reduction method that explicitly considers the manifold structure of the tensor space from multimodal media data which is temporal associated co-occurrence and then detect video semantic concepts through powerful classifiers which take tensor as input. Our algorithm preserves the intrinsic structure of the submanifold where tensorshots are sampled, and is also able to map out-of-sample data points directly. Moreover we apply an active learning based contextual and temporal post-refining strategy to enhance detection accuracy. Experiment results show that our method improves the performance of video semantic concept detection.","cites":"7","conferencePercentile":"58.48623853"},{"venue":"ACM Multimedia","id":"196e8a4693f1f4677f538c01f6b7db00ffa6d2fa","venue_1":"ACM Multimedia","year":"2008","title":"Mixed-initiative photo collage authoring","authors":"Jun Xiao, Xuemei Zhang, Phil Cheatle, Yuli Gao, Clayton Brian Atkins","author_ids":"2781973, 5113938, 2631305, 2026151, 1870182","abstract":"Creating an artifact that captures the story or memory from a large photo collection is a difficult task, because the tools available are either too difficult to learn, or oversimplified to the point that they lack flexibility. Individual techniques have been developed to automate parts of the selection-editing-composition cycle, but relatively little has been done to strike the right overall balance between the fully automatic and the fully manual. In this paper, we present miCollage, which attempts to piece together individual technologies to create a compelling collage authoring experience. The system consists of three main components. In the selection component, the system can make proactive suggestions about which photos to add to the collage as well as help the user to find similar or related photos. In the editing component, the system applies automatic cropping and enhancement to the images. In the layout component, the system suggests alternative layouts but is still able to accommodate manual changes, while satisfying various spatial constraints. The user interface connects the components seamlessly, allowing a best of both worlds between fully manual and fully automatic collage authoring.","cites":"22","conferencePercentile":"85.32110092"},{"venue":"ACM Multimedia","id":"bacf8fe916e5de2f446e33d16796f9870a118493","venue_1":"ACM Multimedia","year":"2014","title":"Taste+: Digitally Enhancing Taste Sensations of Food and Beverages","authors":"Nimesha Ranasinghe, Kuan-Yi Lee, Gajan Suthokumar, Ellen Yi-Luen Do","author_ids":"1722792, 2403479, 2304561, 1689168","abstract":"In recent years, digital media technologies expand horizons into non-traditional sensory modalities, for instance, the sense of taste. This demonstration presents `taste+,' which digitally improves the taste sensations of food and beverages without additional flavoring ingredients. It primarily utilizes weak and controlled electrical pulses on the human tongue to enhance sourness, saltiness, and bitterness of food or beverages. Taste+ consists of two prototype utensils, a bottle and a spoon. Both utensils have embedded electronic control modules to achieve enhanced taste sensations. These modules apply controlled electrical pulses on the tongue through silver electrodes attached to the mouth pieces. Furthermore, different superimposed colors symbolize distinct taste sensations, lemon green represents sour, ocean blue is salty, red for red wine bitter. The initial experimental results suggested that sourness and saltiness are the main sensations that could be evoked while bitterness has comparatively mild responses. Furthermore, we also observed several mixed sensations such as salty-sour together.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"7252391c4e271e006d881e340e3862994c08706d","venue_1":"ACM Multimedia","year":"2008","title":"SenseCoding: accelerometer-assisted motion estimation for efficient video encoding","authors":"Guangming Hong, Ahmad Rahmati, Ye Wang, Lin Zhong","author_ids":"2677746, 3132962, 1681196, 5581056","abstract":"Accelerometers have appeared on many camcorders, cameras and mobile phones. We present algorithms that estimate camera movement from accelerometer readings and apply the estimation to significantly improve the compute-intense motion estimation in video encoding. We have implemented a working prototype that simultaneously captures video and three-axis acceleration data. The video is then compressed with a reference MPEG-2 encoder, modified to incorporate the accelerometer readings to assist motion estimation. Our experimental data shows a two to three times speed improvement for the entire encoding process, in comparison with full search.","cites":"10","conferencePercentile":"68.34862385"},{"venue":"ACM Multimedia","id":"2cb87a92ddb80f831a2c73b4ec77adc83232c347","venue_1":"ACM Multimedia","year":"2004","title":"Context data in geo-referenced digital photo collections","authors":"Mor Naaman, Susumu Harada, QianYing Wang, Hector Garcia-Molina, Andreas Paepcke","author_ids":"1687465, 2975043, 3029618, 1695250, 1750481","abstract":"Given time and location information about digital photographs we can automatically generate an abundance of related contextual metadata, using off-the-shelf and Web-based data sources. Among these are the local daylight status and weather conditions at the time and place a photo was taken. This metadata has the potential of serving as memory cues and filters when browsing photo collections, especially as these collections grow into the tens of thousands and span dozens of years.\n We describe the contextual metadata that we automatically assemble for a photograph, given time and location, as well as a browser interface that utilizes that metadata. We then present the results of a user study and a survey that together expose which categories of contextual metadata are most useful for recalling and finding photographs. We identify among still unavailable metadata categories those that are most promising to develop next.","cites":"102","conferencePercentile":"97.05882353"},{"venue":"ACM Multimedia","id":"089cde716af0bff5765eb4504426a79b3a555668","venue_1":"ACM Multimedia","year":"2007","title":"How flickr helps us make sense of the world: context and content in community-contributed media collections","authors":"Lyndon S. Kennedy, Mor Naaman, Shane Ahern, Rahul Nair, Tye Rattenbury","author_ids":"1765581, 1687465, 2796251, 1799464, 3332698","abstract":"The advent of media-sharing sites like Flickr and YouTube has drastically increased the volume of community-contributed multimedia resources available on the web. These collections have a previously unimagined depth and breadth, and have generated new opportunities - and new challenges - to multimedia research. How do we analyze, understand and extract patterns from these new collections? How can we use these unstructured, unrestricted community contributions of media (and annotation) to generate \"knowledge\".\n As a test case, we study Flickr - a popular photo sharing website. Flickr supports photo, time and location metadata, as well as a light-weight annotation model. We extract information from this dataset using two different approaches. First, we employ a location-driven approach to generate aggregate knowledge in the form of \"representative tags\" for arbitrary areas in the world. Second, we use a tag-driven approach to automatically extract place and event semantics for Flickr tags, based on each tag's metadata patterns.\n With the patterns we extract from tags and metadata, vision algorithms can be employed with greater precision. In particular, we demonstrate a location-tag-vision-based approach to retrieving images of geography-related landmarks and features from the Flickr dataset. The results suggest that community-contributed media and annotation can enhance and improve our access to multimedia resources - and our understanding of the world.","cites":"194","conferencePercentile":"98.95833333"},{"venue":"ACM Multimedia","id":"6c8c4dd219cf012d205ae59f5572dddd8c6fd94c","venue_1":"ACM Multimedia","year":"2007","title":"Zurfer: mobile multimedia access in spatial, social and topical context","authors":"Amy Hwang, Shane Ahern, Simon King, Mor Naaman, Rahul Nair, Jeannie Hui-I Yang","author_ids":"8445516, 2796251, 1721016, 1687465, 1799464, 2198547","abstract":"What happens when you can access all the world's media, but the access is constrained by screen size, bandwidth, attention, and battery life? We present a novel mobile context-aware software prototype that enables access to images on the go. Our prototype utilizes the channel metaphor to give users contextual access to media of interest according to key dimensions: spatial, social, and topical.\n Our experimental prototype attempts to be playful and simple to use, yet provide powerful and comprehensive media access. A temporally-driven sorting scheme for media items allows quick and easy access to items of interest in any dimension. For ad-hoc tasks, we extend the application with keyword search to deliver the long tail of media and images.\n Elements of social interaction and communication around the photographs are built into the mobile application, to increase user engagement. The application utilizes Flickr.com as an image and social-network data source, but could easily be extended to support other websites and media formats.","cites":"8","conferencePercentile":"59.375"},{"venue":"ACM Multimedia","id":"d9e2fd23661693623a3720c7edfa57ccb1a61dc4","venue_1":"ACM Multimedia","year":"2007","title":"A workload prediction model for decoding mpeg video and its application to workload-scalable transcoding","authors":"Yicheng Huang, An Vu Tran, Ye Wang","author_ids":"3092967, 2416698, 1681196","abstract":"Multimedia playback is restricted by the processing power of mobile devices, and in particular, the playback quality can be degraded due to insufficient processing power. To address this problem, we propose a new workload-scalable transcoding scheme which converts a pre-recorded video bitstream into a new video bitstream that satisfies the device's workload constraint, while keeping the transcoding distortion minimal. The key of this proposed transcoding scheme lies on a new workload prediction model, which is fast, accurate and is generic enough to apply to different video formats, decoder implementations and target platforms. The main contributions of this paper include 1) a workload prediction model for decoding MPEG video based on an offline bitstream analysis method; 2) a transcoding scheme that uses the proposed model to control the decoding workload on the target device. To facilitate our transcoding scheme, we have proposed a compressed domain distortion measure (CDDM) that takes effects from both frames per second (fps) and bits per frame (bpf) into consideration. CDDM ensures the transcoded video bitstream to have the best playback quality given the device's workload constraint. Both the workload prediction model and the transcoding scheme are evaluated experimentally.","cites":"7","conferencePercentile":"53.90625"},{"venue":"ACM Multimedia","id":"2e1de7b5f3a52434dc93d50050007262eb76a260","venue_1":"ACM Multimedia","year":"2007","title":"A compressed domain distortion measure for fast video transcoding","authors":"Yicheng Huang, An Vu Tran, Ye Wang","author_ids":"3092967, 2416698, 1681196","abstract":"Video applications on different mobile devices are becoming increasingly popular. It is an attractive alternative to transcode a high quality non-scalable video bitstream to match constraints (such as bandwidth or processing power) of different platforms with a similar functionality as a scalable video format. In principle, such a transcoder can reduce either the bit per frame (bpf) or the frame per second (fps) of the original bitstream to meet a particular constraint. In the case that multiple candidates with different combinations of bpf and fps satisfy the constraint, an objective video quality measure is needed for the transcoder to choose the candidate with the overall best quality considering both the spatial quality (reflected by bpf) and the temporal quality (reflected by fps). Conventional measures, such as PSNR and MSE operate in the pixel-domain, require full decoding of both the original and candidate video bitstreams and are computationally very expensive. This drawback renders them unsuitable for real-time transcoding applications. To solve this problem, we propose a Mean Compressed Domain Error (MCDE) to predict the quality of the transcoded video. Experimental results show that the proposed MCDE can predict video quality accurately with a negligible computational complexity in comparison with the conventional MSE/PSNR.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"766c8c30aa5eb34b9d30fa78b1f1141f8f1c27c4","venue_1":"ACM Multimedia","year":"1998","title":"Measuring Perceived Quality of Speech and Video in Multimedia Conferencing Applications","authors":"Anna Watson, M. Angela Sasse","author_ids":"4841479, 1752376","abstract":"1. ABSTRACT 'I&m is currenffymuch discussion of Quality of service (Qos) measurements at the network IeveIof real-time mdimedia serviq but it is theszd.jectie @typerwivedbytie-r that will determine whether these applications are adopteil This paperangues that ITU-recommended metiodsfor subjective quality msessment of _ and video arenotsuitalie for asssing the qua?ityof many newer services and applications. We present an outline of what we beiieve to be a more suitabIe testingmethodoIogy, vddchacknowkdges tie muklimensional mture of perceived audio and video qwdity.","cites":"101","conferencePercentile":"93.26923077"},{"venue":"ACM Multimedia","id":"54f6deddc4005963e034bea1bc09a514973c512a","venue_1":"ACM Multimedia","year":"2000","title":"The good, the bad, and the muffled: the impact of different degradations on Internet speech","authors":"Anna Watson, M. Angela Sasse","author_ids":"4841479, 1752376","abstract":"This paper presents an experiment comparing the relative impact of different types of degradation on subjective quality ratings of interactive speech transmitted over packet-switched networks. The experiment was inspired by observations made during a large-scale, long-term field trial of multicast conferencing. We observed that user reports of unsatisfactory speech quality were rarely due to network effects such as packet loss and jitter. A subsequent analysis of conference recordings found that in most cases, the impairment was caused by end-system hardware, equipment setup or user behavior. The results from the experiment confirm that the effects of volume differences, echo and bad microphones are rated worse than the level of packet loss most users are likely to experience on the Internet today, provided that a simple repair mechanism is used. Consequently, anyone designing or deploying network speech applications and services ought to consider the addition of diagnostics and tutorials to ensure acceptable speech quality.","cites":"18","conferencePercentile":"76.08695652"},{"venue":"ACM Multimedia","id":"10fdfe4ea18f5550d461a952dc63f86b728b5dc3","venue_1":"ACM Multimedia","year":"2007","title":"The kindest cut: enhancing the user experience of mobile tv through adequate zooming","authors":"Hendrik Knoche, Marco Papaleo, M. Angela Sasse, Alessandro Vanelli-Coralli","author_ids":"7659993, 1932848, 1752376, 3239285","abstract":"The growing market of Mobile TV requires automated adaptation of standard TV footage to small size displays. Especially extreme long shots (XLS) depicting distant objects can spoil the user experience, e.g. in soccer content. Automated zooming schemes can improve the visual experience if the resulting footage meets user expectations in terms of the visual detail and quality but does not omit valuable context information. Current zooming schemes are ignorant of beneficial zoom ranges for a given target size when applied to standard definition TV footage. In two experiments 84 participants were able to switch between original and zoom enhanced soccer footage at three sizes - from 320x240 (QVGA) down to 176x144 (QCIF). Eye tracking and subjective ratings showed that zoom factors between 1.14 and 1.33 were preferred for all sizes. Interviews revealed that a zoom factor of 1.6 was too high for QVGA content due to low perceived video quality, but beneficial for QCIF size. The optimal zoom depended on the target display size. We include a function to compute the optimal zoom for XLS depending on the target device size. It can be applied in automatic content adaptation schemes and should stimulate further research on the requirements of different shot types in video coding.","cites":"17","conferencePercentile":"74.21875"},{"venue":"ACM Multimedia","id":"45ca474cf28f37fbef008be88248d264e26d2bdf","venue_1":"ACM Multimedia","year":"1999","title":"NAIVE - network aware Internet video encoding","authors":"Héctor M. Briceño, Steven J. Gortler, Leonard McMillan","author_ids":"2007034, 2415843, 1748115","abstract":"<italic>The distribution of digital video content over computer networks has become commonplace. Unfortunately, most digital video encoding standards do not degrade gracefully in the face of packet losses, which often occur in a bursty fashion. We propose an new video encoding system that scales well with respect to the network's performance and degrades gracefully under packet loss. Our encoder sends packets that consist of a small random subset of pixels distributed throughout a video frame. The receiver places samples in their proper location (through a previously agreed ordering), and applies a reconstruction algorithm on the received samples to produce an image. Each of the packets is independent, and does not depend on the successful transmission of any other packets. Additionally, each packet contains information that is distributed over the entire image. We also apply spatial and temporal optimization to achieve better compression.</italic>","cites":"16","conferencePercentile":"64.70588235"},{"venue":"ACM Multimedia","id":"543d63a83d2ea8eed037f219695dcf2e6c75b9eb","venue_1":"ACM Multimedia","year":"2012","title":"One shot learning gesture recognition with Kinect sensor","authors":"Di Wu, Fan Zhu, Ling Shao, Hui Zhang","author_ids":"7464209, 3224570, 1764031, 1688592","abstract":"Gestures are both natural and intuitive for Human-Computer-Interaction (HCI) and the one-shot learning scenario is one of the real world situations in terms of gesture recognition problems. In this demo, we present a hand gesture recognition system using the Kinect sensor, which addresses the problem of one-shot learning gesture recognition with a user-defined training and testing system. Such a system can behave like a remote control where the user can allocate a specific function using a prefered gesture by performing it only once. To adopt the gesture recognition framework, the system first automatically segments an action sequence into atomic tokens, and then adopts the Extended-Motion-History-Image (Extended-MHI) for motion feature representation. We evaluate the performance of our system quantitatively in Chalearn Gesture Challenge, and apply it to a virtual one shot learning gesture recognition system.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"01153e2d0cfd1ebb9f53d9e3cd4ba654fa396aee","venue_1":"ACM Multimedia","year":"2008","title":"To catch a thief - you need at least 8 frames per second: the impact of frame rates on user performance in a CCTV detection task","authors":"Hina Uttam Keval, M. Angela Sasse","author_ids":"3448072, 1752376","abstract":"The new generation of digital CCTV systems can be tailored to serve a wide range of security requirements. However, many digital CCTV systems produce video which is insufficient in video quality to support specific security tasks, such as crime detection. We report a study investigating the impact of lowering frame rates on an observer's ability to distinguish between crime and no crime events from post-event recorded video. 80 participants viewed 32 video scenes at 1, 5, 8, and 12 frames per second (fps). The task required observers to determine if one of three possible events had occurred. Results showed that the number of correct detections, task confidence decreased significantly at 8 fps and lower. Our results provide CCTV practitioners with a minimum frame rate level (8 fps) for event detection, a task performed by CCTV users of varying skill and experience.","cites":"6","conferencePercentile":"53.89908257"},{"venue":"ACM Multimedia","id":"4c3d2c7948879f4e97452372bffd077ac6ff8734","venue_1":"ACM Multimedia","year":"2006","title":"ViCo: an adaptive distributed video correlation system","authors":"Xiaohui Gu, Zhen Wen, Ching-Yung Lin, Philip S. Yu","author_ids":"1692583, 1735018, 1689953, 1703117","abstract":"Many emerging applications such as video sensor monitoring can benefit from an on-line video correlation system, which can be used to discover linkages between different video streams in realtime. However, on-line video correlations are often resource-intensive where a single host can be easily overloaded. We present a novel adaptive distributed on-line video correlation system called ViCo. Unlike single stream processing, correlations between different video streams require a distributed execution system to observe a new correlation constraint that any two correlated data must be distributed to the same host. ViCo achieves three unique features: (1) <i>correlation-awareness</i> that ViCo can guarantee the correlation accuracy while spreading excessive workload on multiple hosts; (2) <i>adaptability</i> that the system can adjust algorithm behaviors and switch between different algorithms to adapt to dynamic stream environments; and (3) <i>fine-granularity</i> that the workload of one resource-intensive correlation request can be divided and distributed among multiple hosts. We have implemented and deployed a prototype of ViCo on a commercial cluster system. Our experiment results using both real videos and synthetic workloads show that ViCo outperforms existing techniques for scaling-up the performance of video correlations.","cites":"11","conferencePercentile":"72.27979275"},{"venue":"ACM Multimedia","id":"4131e377d74407f493fbfa5f44edb873455f67fc","venue_1":"ACM Multimedia","year":"2005","title":"Supporting multi-party voice-over-IP services with peer-to-peer stream processing","authors":"Xiaohui Gu, Zhen Wen, Philip S. Yu, Zon-Yin Shae","author_ids":"1692583, 1735018, 1703117, 1903303","abstract":"Multi-party voice-over-IP (MVoIP) services provide economical and natural group communication mechanisms for many emerging applications such as on-line gaming, distance collaboration, and tele-immersion. In this paper, we present a novel peer-to-peer (P2P) stream processing system called peerTalk to provide resource-efficient and failure-resilient MVoIP services. Different from previous work, our solution is fully distributed and self-organizing without requiring specialized servers or IP multicast support. Particularly, we decouple the stream processing in MVoIP services into two phases: (1) <i>aggregation phase</i> that mixes audio streams from active speakers into a single stream; and (2) <i>distribution phase</i> that distributes the mixed audio stream to all listeners. The decoupled model allows us to optimize and adapt the P2P stream mixing and distribution processes separately. Specifically, we can adaptively spread stream mixing workload among resource-constrained peer hosts according to current speaking activities. We have implemented a prototype of the peerTalk system and conducted experiments in real-world wide-area networks. The results show that peerTalk can achieve lower resource contention and better service quality than previous common solution.","cites":"6","conferencePercentile":"45.2970297"},{"venue":"ACM Multimedia","id":"77aa6c6328a0a84954e0212cc7d3e6baab0f9d64","venue_1":"ACM Multimedia","year":"2008","title":"The sweet spot: how people trade off size and definition on mobile devices","authors":"Hendrik Knoche, M. Angela Sasse","author_ids":"7659993, 1752376","abstract":"Mobile TV can deliver up-to-date content to users on the move. But it is currently unclear how to best adapt higher resolution TV content. In this paper, we describe a laboratory study with 35 participants who watched short clips of different content and shot types on a 200ppi PDA display at a resolution of either 120x90 or 168x128. Participants selected their preferred size and rated the acceptability of the visual experience. The preferred viewing ratio depended on the resolution and had to be at least 9.8H. The minimal angular resolution people required and which limited the up-scaling factor was 14 pixels per degree. Extreme long shots were best when depicted actors were at least 0.7&#176; high. A second study researched the ecological validity of previous lab results by comparing them to results from the field. Image size yielded more value for users in the field than was apparent from lab results. In conclusion, current prediction models based on preferred viewing distances for TV and large displays do not predict viewing preferences on mobile devices. Our results will help to further the understanding of multimedia perception and service designers to deliver both economically viable and enjoyable experiences.","cites":"9","conferencePercentile":"65.36697248"},{"venue":"ACM Multimedia","id":"33eb2ad8b6040c350f669fef2b84e6d3e6004069","venue_1":"ACM Multimedia","year":"1994","title":"Getting the Model Right for Video-Mediated Communication (Panel Abstract)","authors":"Sylvia Wilbur, Garry Beirne, Jon Crowcroft, J. Robert Ensor, John C. Tang","author_ids":"2426217, 1945133, 1726850, 2210075, 1808267","abstract":"rooms — these are just some of the abstractions that the research community have evolved for computer-mediated video communications. Each abstraction implies a particular context for communication, supported by an appropriate User Interface design and set of multimedia services. The success of public telephone services owes much to the simple but powerful call abstraction that unifies the services provided by the PSTNS. If video really is to succeed, a uniform abstraction seems necessary both from the users point of view. and for systems developers. Which of the current models, if any, is most appropriate? Perhaps the abstraction that will succeed has yet to emerge or will evolve slowly from a merger of the various approaches. Meanwhile, the engineers forge ahead in the construction of national and international communications infras-tructures, so these issues must be raised now. Users will need models of VMC that help them make effective use of the wide variety of multimedia services that will soon be available. Sylvia Wilbur has been involved in CSCW research since 1986, her current interests focusing on support for real-time mul-timedia distributed collaboration. A system embodying the concepts of media space has been built at QMW, and is in daily use within the department. Dr. Finn has developed user interfaces for a variety of systems, including informational and educational multimedia displays, expert systems. a reading assistant, and a voice-driven application. In particular, she has spent the past two years associated with a project to develop a collaborative teleconferencing and multimedia authoring system, during which time she has become increasingly aware of the limitations such a system imposes on communication between users. She is currently conducting a research project to more fully examine the characteristics of such communication. The Ontario Telepresence Project is a three-year research project based at the University of Toronto, We are interested in technologies that support a sense of social proximity despite geographical and/or temporal distance. Our interest in this is centered on our desire to support collaboration at a distance, and to create environments that foster a strong sense of community, despite the distance separating those active in it. The rapid convergence of telecommunications, computational and audio/visual ttxhnologies provide new opportunities to support this kind of telepresence. The Ontario Telepresence Project is a joint government, university and industry project, set up to undertake research to exploit these opportunities. The research is human centered. Our objectives are to develop …","cites":"0","conferencePercentile":"12.71186441"},{"venue":"ACM Multimedia","id":"2b454d34b3317855f81bbba2314b911dfdfd8fb3","venue_1":"ACM Multimedia","year":"2001","title":"An integrated framework for face modeling, facial motion analysis and synthesis","authors":"Pengyu Hong, Zhen Wen, Thomas S. Huang","author_ids":"3278420, 1735018, 1739208","abstract":"This paper presents an integrated framework for face modeling, facial motion analysis and synthesis. This framework systematically addresses three closely related research issues: (1) selecting a quantitative visual representation for face modeling and face animation; (2) automatic facial motion analysis based on the same visual representation; and (3) speech to facial coarticulation modeling. The framework provides a guideline for methodically building a face modeling and animation system. The systematicness of the framework is reflected by the links among its components, whose details are presented. Based on this framework, we improved a face modeling and animation system, called the iFACE system [4]. The final system provides functionalities for customizing a generic face model for an individual, text driven face animation, off-line speech driven face animation, and real-time speech driven face animation.","cites":"8","conferencePercentile":"52.55102041"},{"venue":"ACM Multimedia","id":"4ce905c729a86955db6ac9b7b4615179f03ef9da","venue_1":"ACM Multimedia","year":"2008","title":"iDVT: an interactive digital violin tutoring system based on audio-visual fusion","authors":"Huanhuan Lu, Bingjun Zhang, Ye Wang, Wee Kheng Leow","author_ids":"6364142, 2522345, 1681196, 1787377","abstract":"iDVT (interactive Digital Violin Tutor) is a violin learning system exploiting physical and virtual resources and interactivity. It aims at providing the user with new effective learning experience. This demonstration paper briefly describes the structure of the system and the underlying audio-visual processing techniques employed in the system.","cites":"2","conferencePercentile":"29.81651376"},{"venue":"ACM Multimedia","id":"2a452acb4818daf6451fa0e68c86c12aaaaaba50","venue_1":"ACM Multimedia","year":"1994","title":"Adaptive Color Map Selection Algorithm for Motion Sequences","authors":"John L. Furlani, Leonard McMillan, Lee Westover","author_ids":"2204122, 1748115, 3033057","abstract":"We present a simple and intuitive algorithm for the quantization of full-color images which has been designed to apply to static images and motion sequences equally well. Our technique eliminates the perils of hardware colormap flashing which is inherent in other well known algorithms for selecting colormap representatives. We compare our technique with existing static image colormap generation techniques to show the quality of the resultant quantization.","cites":"3","conferencePercentile":"33.05084746"},{"venue":"ACM Multimedia","id":"530e04facc8a3bce4c13474d40cd81238f6545bf","venue_1":"ACM Multimedia","year":"2009","title":"Manipulating lossless video in the compressed domain","authors":"William Thies, Steven Hall, Saman P. Amarasinghe","author_ids":"1718457, 6089641, 1709150","abstract":"A compressed-domain transformation is one that operates directly on the compressed format, rather than requiring conversion to an uncompressed format prior to processing. Performing operations in the compressed domain offers large speedups, as it reduces the volume of data processed and avoids the overhead of re-compression.\n While previous researchers have focused on compressed-domain techniques for lossy data formats, there are few techniques that apply to lossless formats. In this paper, we present a general technique for transforming lossless data as compressed with the sliding-window Lempel Ziv algorithm (LZ77). We focus on applications in video editing, where our technique supports color adjustment, video compositing, and other operations directly on the Apple Animation format (a variant of LZ77).\n We implemented a subset of our technique as an automatic program transformation. Using the StreamIt language, users write a program to operate on uncompressed data, and our compiler transforms the program to operate on compressed data. Experiments show that the technique offers speedups roughly proportional to the compression factor. For our benchmark suite of 12 videos in Apple Animation format, speedups range from 1.1x to 471x, with a median of 15x.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"290487e0ab1775cc29c39082879b7abeac6bf834","venue_1":"ACM Multimedia","year":"2000","title":"Topic segmentation of news speech using word similarity","authors":"Seiichi Takao, Jun Ogata, Yasuo Ariki","author_ids":"2737826, 6619470, 1678564","abstract":"Conventional topic segmentation utilizes <itailic>cosine</italic> measure as the similarity between consecutive passages. However, the <italic>cosine</italic> measure has a problem that it can not reflect the similarity unless exactly the same words are included in the passages. To solve this problem, in this paper, we propose a method to acquire the word similarity between different words from the input data directly and automatically by managing to collect the same topic sections. Further more, we propose a method to compute the passage similarity based on the word similarity. Finally we propose a method of topic segmentation based on the passage similarity in an unsupervised mode.","cites":"9","conferencePercentile":"59.23913043"},{"venue":"ACM Multimedia","id":"7b2dabfd79440e55d515591969398457a8d8c84e","venue_1":"ACM Multimedia","year":"2007","title":"Large head movement tracking using sift-based registration","authors":"Gangqiang Zhao, Ling Chen, Jie Song, Gencai Chen","author_ids":"2581147, 1717165, 1719329, 3025640","abstract":"Although there exists dozens of vision based 3D head tracking methods, none of them considers the problem of large motion, especially the movement along the Z axis. In this paper we propose a novel tracking method to handle this problem by using Scale Invariant Feature Transform (SIFT) based registration algorithm. Salient SIFT features are first detected and tracked between two images, and then the 3D points corresponding to these features are obtained from a stereo camera. With these 3D points, a registration algorithm in a RANSAC framework is employed to detect the outliers and estimate the head pose. Performance evaluation shows an accurate pose recovery (3&#176; RMS) when the head has large motion, even with movement along the Z axis was about 150 cm.","cites":"20","conferencePercentile":"79.42708333"},{"venue":"ACM Multimedia","id":"cdf036b8ef4320e2f5c475430907886f3b0d2383","venue_1":"ACM Multimedia","year":"2000","title":"FlyCam: practical panoramic video","authors":"Jonathan Foote, Don Kimber","author_ids":"1797460, 2178004","abstract":"This demonstration presents a computationally and materially inexpensive system for panoramic video imaging. Digitally combining images from an array of inexpensive video cameras results in a wide-field panoramic camera, from inexpensive off-the-shelf- hardware. Digital processing both corrects lens distortion and seamlessly merges images into a panoramic video image. Electronically selecting a region of this results in a rapidly steerable &#8220;virtual camera.&#8221;","cites":"25","conferencePercentile":"80.97826087"},{"venue":"ACM Multimedia","id":"72e9843608b716dbb1eeba258482f6c5b0b37ca2","venue_1":"ACM Multimedia","year":"1999","title":"Inquiry with imagery: historical archive retrieval with digital cameras","authors":"Brian K. Smith, Erik Blankinship, Alfred Ashford, Michael Baker, Timothy Hirzel","author_ids":"3598181, 2563349, 2337850, 4087574, 1814928","abstract":"This paper describes an integration of geographic information systems (GIS) and multimedia technologies to transform the ways K-12 students learn about their local communities. We have augmented a digital camera with a global positioning system (GPS) and a digital compass to record its position and orientation when pictures are taken. The metadata are used to retrieve and present historical images of the photographed locations to students. Another set of tools allows them to annotate and compare these historical images to develop explanations of how and why their communities have changed over time. We describe the camera architecture and learning outcomes that we expect to see in classroom use.","cites":"8","conferencePercentile":"52.94117647"},{"venue":"ACM Multimedia","id":"be740d327307d12dd6a9d50baf3afb5b74e05cd3","venue_1":"ACM Multimedia","year":"2004","title":"Video transport over wireless networks","authors":"Harinath Garudadri, Phoom Sagetong, Sanjiv Nanda","author_ids":"2911156, 3072005, 1966109","abstract":"In this paper, we propose an efficient scheme to transport video over wireless networks, specifically cdma2000&#174; 1x. Speech transmission over cdma2000&#174; uses a variable rate voice coder (vocoder) over a channel with multiple fixed rates. We apply these ideas to compressed video transmission over wireless IP networks. Explicit Bit Rate (EBR) video compression is designed to match the video encoder output to a set of fixed channel rates. We show that in comparison with VBR video transmission over a fixed rate wireless channel, EBR video transmission provides improved error resilience, reduced latency and improved efficiency.","cites":"1","conferencePercentile":"19.11764706"},{"venue":"ACM Multimedia","id":"2ca345b925d7e936b8193a841e0e44f9f7d1b1f2","venue_1":"ACM Multimedia","year":"2005","title":"Coevolutionary feature synthesized EM algorithm for image retrieval","authors":"Rui Li, Bir Bhanu, Anlei Dong","author_ids":"1704992, 1707159, 3300216","abstract":"As a commonly used unsupervised learning algorithm in <i>Content-Based Image Retrieval</i> (CBIR), <i>Expectation-Maximization</i> (EM) algorithm has several limitations, especially in high dimensional feature spaces where the data are limited and the computational cost varies exponentially with the number of feature dimensions. Moreover, the convergence is guaranteed only at a local maximum. In this paper, we propose a unified framework of a novel learning approach, namely <i>Coevolutionary Feature Synthesized Expectation-Maximization</i> (CFS-EM), to achieve satisfactory learning in spite of these difficulties. The CFS-EM is a hybrid of <i>coevolutionary genetic programming</i> (CGP) and EM algorithm. The advantages of CFS-EM are: 1) it synthesizes low-dimensional features based on CGP algorithm, which yields near optimal nonlinear transformation and classification precision comparable to kernel methods such as the <i>support vector machine</i> (SVM); 2) the explicitness of feature transformation is especially suitable for image retrieval because the images can be searched in the synthesized low-dimensional space, while kernel-based methods have to make classification computation in the original high-dimensional space; 3) the unlabeled data can be boosted with the help of the class distribution learning using CGP feature synthesis approach. Experimental results show that CFS-EM outperforms pure EM and CGP alone, and is comparable to SVM in the sense of classification. It is computationally more efficient than SVM in query phase. Moreover, it has a high likelihood that it will jump out of a local maximum to provide near optimal results and a better estimation of parameters.","cites":"3","conferencePercentile":"28.96039604"},{"venue":"ACM Multimedia","id":"08fb43bddf5d29207cde1efe79f136050e1a709e","venue_1":"ACM Multimedia","year":"2014","title":"Gibber: Abstractions for Creative Multimedia Programming","authors":"Charles Roberts, Matthew Wright, JoAnn Kuchera-Morin, Tobias Höllerer","author_ids":"6114171, 1678887, 1727605, 1743721","abstract":"We describe design decisions informing the development of <i>Gibber</i>, an audiovisual programming environment for the browser. Our design comprises a consistent notation across modalities in addition to high-level abstractions affording intuitive declarations of multimodal mappings, unified timing constructs, and rapid, iterative reinvocations of constructors while preserving the state of audio and visual graphs. We discuss the features of our environment and the abstractions that enable them. We close by describing use cases, including live audiovisual performances and computer science education.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"dd0aef0d44e740580212d6efb5286446494729ba","venue_1":"ACM Multimedia","year":"2013","title":"Multi-feature canonical correlation analysis for face photo-sketch image retrieval","authors":"Dihong Gong, Zhifeng Li, Jianzhuang Liu, Yu Qiao","author_ids":"2856494, 2451576, 7137861, 1690077","abstract":"Automatic face photo-sketch image retrieval has attracted great attention in recent years due to its important applications in real life. The major difficulty in automatic face photo-sketch image retrieval lies in the fact that there exists great discrepancy between the different image modalities (photo and sketch). In order to reduce such discrepancy and improve the performance of automatic face photo-sketch image retrieval, we propose a new framework called multi-feature canonical correlation analysis (MCCA) to effectively address this problem. The MCCA is an extension and improvement of the canonical correlation analysis (CCA) algorithmusing multiple features combined with two different random sampling methods in feature space and sample space. In this framework, we first represent each photo or sketch using a patch-based local feature representation scheme, in which histograms of oriented gradients (HOG) and multi-scale local binary pattern (MLBP) serve as the local descriptors. Canonical correlation analysis (CCA) is then performed on a collection of random subspaces to construct an ensemble of classifiers for photo-sketch image retrieval. Extensive experiments on two public-domain face photo-sketch datasets (CUFS and CUFSF) clearly show that the proposed approach obtains a substantial improvement over the state-of-the-art.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"466633cb5c1e861df134c553392fb01b2eb6fd8d","venue_1":"ACM Multimedia","year":"2007","title":"Multi-scale video cropping","authors":"Hazem El-Alfy, David W. Jacobs, Larry S. Davis","author_ids":"1837090, 1771485, 1693428","abstract":"We consider the problem of <i>cropping</i> surveillance videos. This process chooses a trajectory that a small sub-window can take through the video, selecting the most important parts of the video for display on a smaller monitor. We model the information content of the video simply, by whether the image changes at each pixel. Then we show that we can find the globally optimal trajectory for a cropping window by using a shortest path algorithm. In practice, we can speed up this process without affecting the results, by stitching together trajectories computed over short intervals. This also reduces system latency. We then show that we can use a second shortest path formulation to find good cuts from one trajectory to another, improving coverage of interesting events in the video. We describe additional techniques to improve the quality and efficiency of the algorithm, and show results on surveillance videos.","cites":"19","conferencePercentile":"78.125"},{"venue":"ACM Multimedia","id":"1b47a96ad7ca3742db6b3e50d5b9da1018c08a4e","venue_1":"ACM Multimedia","year":"2012","title":"Optimal semi-supervised metric learning for image retrieval","authors":"Kun Zhao, Wei Liu, Jianzhuang Liu","author_ids":"1681247, 3406279, 7137861","abstract":"In a typical content-based image retrieval (CBIR) system, images are represented as vectors and similarities between images are measured by a specified distance metric. However, the traditional Euclidean distance cannot always deliver satisfactory performance, so an effective metric sensible to the input data is desired. Tremendous recent works on metric learning have exhibited promising performance, but most of them suffer from limited label information and expensive training costs. In this paper, we propose two novel metric learning approaches, Optimal Semi-Supervised Metric Learning and its kernelized version. In the proposed approaches, we incorporate information from both labeled and unlabeled data to design a convex and computationally tractable learning framework which results in a globally optimal solution to the target metric of much lower rank than the original data dimension. Experiments on several image benchmarks demonstrate that our approaches lead to consistently better distance metrics than the state-of-the-arts in terms of accuracy for image retrieval.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"fb0b2a8a7061d06cb00eda2060114caea85c8b2d","venue_1":"ACM Multimedia","year":"2012","title":"Online non-feedback image re-ranking via dominant data selection","authors":"Chen Cao, Shifeng Chen, Yuhong Li, Jianzhuang Liu","author_ids":"2863559, 2869725, 1999221, 7137861","abstract":"Image re-ranking aims at improving the precision of keyword-based image retrieval, mainly by introducing visual features to re-rank. Many existing approaches require offline training for every keyword, which are unsuitable for online image search. Other real-time approaches demand user interaction, which are inappropriate for large-scale image collection. To improve the accuracy of web image retrieval in a practicable manner, we propose a novel re-ranking algorithm to explore the cluster information of the image set. First, we build spectral graph on images that retrieved bysearch engine, and remove isolated nodes as noisy images. Then, we select positive samples from the most dominant cluster in initial top-ranked images, and the samples are used for semi-supervised learning and ranking. Our algorithm is online and non-feedback. Experiments on two public databases demonstrate that our algorithm outperforms the state-of-the-art approaches.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"0966d249559bc866748da19d1a758bff4f76a72c","venue_1":"ACM Multimedia","year":"2011","title":"3D object retrieval with semantic attributes","authors":"Boqing Gong, Jianzhuang Liu, Xiaogang Wang, Xiaoou Tang","author_ids":"1760838, 7137861, 2868636, 1741901","abstract":"Humans are capable of describing objects using attributes, such as \"the object looks circular and is man-made\". Motivated by these high-level descriptions, we build a user-friendly 3D object retrieval system, where the user can browse the database and search for targeted objects using semantic attributes. The main advantage of our system is that it does not require the user to find or sketch a 3D object as the query for 3D object retrieval. Besides, to the best of our knowledge, our system has obtained the best retrieval performance on three popular benchmarks.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"6174fa92a95e0a6a5ff2cd6d7cff06497c706b6d","venue_1":"ACM Multimedia","year":"2011","title":"Automatic object segmentation from large scale 3D urban point clouds through manifold embedded mode seeking","authors":"Zhiding Yu, Chunjing Xu, Jianzhuang Liu, Oscar C. Au, Xiaoou Tang","author_ids":"1751019, 1691522, 7137861, 1741229, 1741901","abstract":"This paper presents a system that can automatically segment objects in large scale 3D point clouds obtained from urban ranging images. The system consists of three steps: The first one involves a ground detection process that can detect relatively complex terrain and separate it from other objects. The second step superpixelizes the remaining objects to speed up the segmentation process. In the final step, a manifold embedded mode seeking method is adopted to segment the point clouds. Even though the segmentation of urban objects is a challenging problem in terms of accuracy and problem scale, our system can efficiently generate very good segmentation results. The proposed manifold learning effectively improves the segmentation performance due to the fact that continuous artificial objects often have manifold-like structures.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"0fffdd497f6ec9dc2c184e1fab716d82e867d37e","venue_1":"ACM Multimedia","year":"2014","title":"Sound-Light Giblet","authors":"Charles Roberts","author_ids":"6114171","abstract":"We describe an audiovisual live coding performance created using <i>Gibber</i>, a creative coding environment that runs in the browser. The performance takes advantage of novel affordances for rapidly creating music, shaders, and mappings that tie together audio and visual modalities.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"53b3ba6e12d9bfceb8891469009fbf54b8cc7650","venue_1":"ACM Multimedia","year":"2011","title":"Edge-preserving single image super-resolution","authors":"Qiang Zhou, Shifeng Chen, Jianzhuang Liu, Xiaoou Tang","author_ids":"1792796, 2869725, 7137861, 1741901","abstract":"This paper proposes a novel approach to single image super-resolution. First, an image up-sampling scheme is proposed which takes the advantages of both bilateral filtering and mean shift image segmentation. Then we use a shock filter to enhance strong edges in the initial up-sampling result and obtain an intermediate high-resolution image. Finally, we enforce a reconstruction constraint on the high-resolution image so that fine details can be inferred by back projection. Since strong edges in the intermediate result are enhanced, ringing artifacts can be suppressed in the back projection step. We compare our algorithm with several state-of-the-art image super-resolution algorithms. Qualitative and quantitative experimental results demonstrate that our approach performs the best.","cites":"4","conferencePercentile":"61.37026239"},{"venue":"ACM Multimedia","id":"7e95e32d578ba14e9f4c4ad7471c1562df9fc068","venue_1":"ACM Multimedia","year":"2014","title":"HuEvent'14: 2014 workshop on human-centered event understanding from multimedia","authors":"Ansgar Scherp, Vasileios Mezaris, Bogdan Ionescu, Francesco G. B. De Natale","author_ids":"1753135, 1737436, 1796198, 2680112","abstract":"This workshop focuses on the human-centered aspects of understanding events from multimedia content. This includes the notion of objects and their relation to events. The workshop brings together researchers from the different areas in multimedia and beyond that are interested in understanding the concept of events.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"c492b1eeb2ba578ff790ab351aa49417f0ee6fba","venue_1":"ACM Multimedia","year":"2015","title":"About Events, Objects, and their Relationships: Human-centered Event Understanding from Multimedia","authors":"Ansgar Scherp, Vasileios Mezaris, Bogdan Ionescu, Francesco G. B. De Natale","author_ids":"1753135, 1737436, 1796198, 2680112","abstract":"HuEvent'15 is a continuation of previous year's successful workshop on events in multimedia. It focuses on the human-centered aspects of understanding events from multimedia content. This includes the notion of objects and their relation to events. The workshop brings together researchers from the different areas in multimedia and beyond that are interested in understanding the concept of events.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"efc9c23df2883c71c0146246d370ad76a1a6ea4b","venue_1":"ACM Multimedia","year":"2010","title":"Fast image rearrangement via multi-scale patch copying","authors":"Jiayao Hu, Shifeng Chen, Jianzhuang Liu, Xiaoou Tang","author_ids":"1826871, 2869725, 7137861, 1741901","abstract":"In this paper, we propose a simple interactive way for a novel type of image synthesis called image rearrangement whose goal is to construct a new image based on some objects cropped from source images. The synthesis results are obtained by copying patches from the source images in a globally consistent way. The patch copying problem is formulated with the Markov random field model, and belief propagation is used as the optimization tool. To speed up our algorithm, a two-step belief propagation and a multi-scale patch copying scheme are taken. Experimental results indicate that our algorithm obtains satisfactory results in both performance and efficiency.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"04b4df7ebcad43ce4a7a5eeae605a2709bb08883","venue_1":"ACM Multimedia","year":"2011","title":"A new analysis method for paired comparison and its application to 3D quality assessment","authors":"Jong-Seok Lee, Lutz Goldmann, Touradj Ebrahimi","author_ids":"5722542, 2379709, 1681498","abstract":"Among various subjective quality evaluation methodologies, paired comparison has the advantage of improved simplicity of the subjects' evaluation task due to simplified rating scales and direct comparison of two stimuli. Thus, it may lead to more reliable results when individual quality levels are difficult to define, quality differences between stimuli are small or multiple quality factors are involved. This paper proposes a new method to analyze results of paired comparison-based subjective tests. By assuming that ties convey information about significant differences between two stimuli being compared, the confidence intervals for the quality scores are estimated using a maximum likelihood criterion, which enables us to intuitively examine the significance of quality score differences. We describe the complete test methodology including the test procedure, outlier detection and score analysis applied to quality assessment of 3D images acquired using varying camera distances. Experimental results demonstrate the usefulness of the proposed analysis method, as well as the enhanced quality discriminability of the paired comparison methodology in comparison to the conventional single stimulus methodology.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"3319d946b2821701dabc322e05e483f2d0f2327f","venue_1":"ACM Multimedia","year":"2006","title":"Presence and portrayal: video for casual home dialogues","authors":"David J. Chatting, Josie S. Galpin, Judith S. Donath","author_ids":"3032079, 2465569, 1779398","abstract":"In this paper we present experimental results rating the experience of users conversing in a casual video/audio dialogue, in a simulated home environment. Here video-realistic images are problematic and prone to numerous \"medium effects\", such as unaligned eye-gaze, which can be misattributed as personal flaws. We tested three levels of manipulated video to see if they improved user's sense of: (a) presence, (b) portrayal and (c) preference. By blurring the background we found a manipulation that is both preferred and more efficiently coded than the original.","cites":"11","conferencePercentile":"72.27979275"},{"venue":"ACM Multimedia","id":"fcd3d557863e71dd5ce8bcf918adbe22ec59e62f","venue_1":"ACM Multimedia","year":"2013","title":"Facial landmark localization based on hierarchical pose regression with cascaded random ferns","authors":"Zhanpeng Zhang, Wei Zhang, Jianzhuang Liu, Xiaoou Tang","author_ids":"3152448, 1739541, 7137861, 1741901","abstract":"The main challenge of facial landmark localization in real-world application is that the large changes of head pose and facial expressions cause substantial image appearance variations. To avoid high dimensional regression in the 3D and 2D facial pose spaces simultaneously, we propose a hierarchical pose regression approach, estimating the head rotation, facial components and landmarks hierarchically. The regression process works in a unified cascaded fern framework. We present generalized gradient boosted ferns (GBFs) for the regression framework, which give better performance than traditional ferns. The framework also achieves real time performance. We verify our method on the latest benchmark datasets. The results show that it outperforms state-of-the-art methods in both accuracy and speed.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"a63c5e52fb700df4835d50b566cd4621c76fe4a4","venue_1":"ACM Multimedia","year":"2010","title":"Subjective evaluation of scalable video coding for content distribution","authors":"Jong-Seok Lee, Francesca De Simone, Naeem Ramzan, Zhijie Zhao, Engin Kurutepe, Thomas Sikora, Jörn Ostermann, Ebroul Izquierdo, Touradj Ebrahimi","author_ids":"5722542, 2315642, 1728223, 2897375, 3328264, 1733384, 1800370, 1732655, 1681498","abstract":"This paper investigates the influence of the combination of the scalability parameters in scalable video coding (SVC) schemes on the subjective visual quality. We aim at providing guidelines for an adaptation strategy of SVC that can select the optimal scalability options for resource-constrained networks. Extensive subjective tests are conducted by using two different scalable video codecs and high definition contents. The results are analyzed with respect to five dimensions, namely, codec, content, spatial resolution, temporal resolution, and frame quality.","cites":"21","conferencePercentile":"88.76712329"},{"venue":"ACM Multimedia","id":"496b8d24bf8da6753477eeddc45da5d39c2d723e","venue_1":"ACM Multimedia","year":"2005","title":"IrisNet: an internet-scale architecture for multimedia sensors","authors":"Jason Campbell, Phillip B. Gibbons, Suman Nath, Padmanabhan Pillai, Srinivasan Seshan, Rahul Sukthankar","author_ids":"3563052, 3309259, 1716368, 1802347, 1730191, 1694199","abstract":"Most current sensor network research explores the use of extremely simple sensors on small devices called motes and focuses on over-coming the resource constraints of these devices. In contrast, our research explores the challenges of multimedia sensors and is motivated by the fact that multimedia devices, such as cameras, are rapidly becoming inexpensive, yet their use in a sensor network presents a number of unique challenges. For example, the data rates involved with multimedia sensors are orders of magnitude greater than those for sensor motes and this data cannot easily be processed by traditional sensor network techniques that focus on scalar data. In addition, the richness of the data generated by multimedia sensors makes them useful for a wide variety of applications. This paper presents an overview of I<sc>RIS</sc>N<sc>ET</sc>, a sensor network architecture that enables the creation of a planetary-scale infrastructure of multimedia sensors that can be shared by a large number of applications. To ensure the efficient collection of sensor readings, I<sc>RIS</sc>N<sc>ET</sc> enables the application-specific processing of sensor feeds on the significant computation resources that are typically attached to multimedia sensors. I<sc>RIS</sc>N<sc>ET</sc> enables the storage of sensor readings close to their source by providing a convenient and extensible distributed XML database infrastructure. Finally, I<sc>RIS</sc>N<sc>ET</sc> provides a number of multimedia processing primitives that enable the effective processing of sensor feeds in-network and at-sensor.","cites":"47","conferencePercentile":"91.58415842"},{"venue":"ACM Multimedia","id":"2d07fed36f2f708c5d313151068bdb6e562a6f60","venue_1":"ACM Multimedia","year":"2009","title":"Towards characterizing user interaction with progressively transmitted 3D meshes","authors":"Ransi Nilaksha De Silva, Wei Cheng, Dan Liu, Wei Tsang Ooi, Shengdong Zhao","author_ids":"1887420, 1707763, 1745635, 1678873, 2645457","abstract":"We collected traces of how 37 users interacted with 9 progressively streamed and rendered 3D meshes. We analyze the traces and discuss the insights that we learned in relation to design of efficient and scalable progressive mesh streaming systems. Our traces indicate that user actions are predictable and exhibit skewed access pattern. This finding could lead to design of efficient pre-fetching and caching techniques for progressive mesh streaming.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"1f98e155a5848887780d387419c95f249e0e8501","venue_1":"ACM Multimedia","year":"2013","title":"Analysis and forecasting of trending topics in online media streams","authors":"Tim Althoff, Damian Borth, Jörn Hees, Andreas Dengel","author_ids":"1745524, 1772549, 1694314, 1703343","abstract":"Among the vast information available on the web, social media streams capture what people currently pay attention to and how they feel about certain topics. Awareness of such trending topics plays a crucial role in multimedia systems such as trend aware recommendation and automatic vocabulary selection for video concept detection systems. Correctly utilizing trending topics requires a better understanding of their various characteristics in different social media streams. To this end, we present the first comprehensive study across three major online and social media streams, Twitter, Google, and Wikipedia, covering thousands of trending topics during an observation period of an entire year. Our results indicate that depending on one's requirements one does not necessarily have to turn to Twitter for information about current events and that some media streams strongly emphasize content of specific categories. As our second key contribution, we further present a novel approach for the challenging task of forecasting the life cycle of trending topics in the very moment they emerge. Our fully automated approach is based on a nearest neighbor forecasting technique exploiting our assumption that semantically similar topics exhibit similar behavior.\n We demonstrate on a large-scale dataset of Wikipedia page view statistics that forecasts by the proposed approach are about 9-48k views closer to the actual viewing statistics compared to baseline methods and achieve a mean average percentage error of 45-19% for time periods of up to 14 days.","cites":"9","conferencePercentile":"84.66666667"},{"venue":"ACM Multimedia","id":"232a965081f4a4b1f82a11dfcb4a6b08c0d41901","venue_1":"ACM Multimedia","year":"2016","title":"A Pragmatically Designed Adaptive and Web-compliant Object-based Video Streaming Methodology: Implementation and Subjective Evaluation","authors":"Maarten Wijnants, Gustavo Rovelo, Peter Quax, Wim Lamotte","author_ids":"2078593, 2438351, 1794986, 1775641","abstract":"The bulk of contemporary online video traffic is encoded in a traditional manner, hereby neglecting most, if not all, of the semantics of the underlying visual scene. One essential piece of semantic information in the context of video streaming is awareness of the objects that jointly constitute the scene. A canonical example of a benefit associated with such object awareness is the ability to subdivide a video fragment in respectively a background and one or more foreground objects. This paper reports on a pragmatically designed video streaming approach that exploits object-related knowledge in order to improve the real-time adaptability of video streaming sessions (manifested in the form of increased granularity in terms of streaming quality control). The proposed approach is completely compliant with present-day video codecs and HTTP Adaptive Streaming schemes, most notably H.264 and MPEG-DASH. Findings from subjecting the proposed video streaming technique to a comparative subjective evaluation suggest that scenarios exist where the presented approach holds the capacity to improve on traditional streaming in terms of user-perceived video quality.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"d05e383a14ddf7697fa0fd31b3ba1ce4c4609adb","venue_1":"ACM Multimedia","year":"2008","title":"Eavesdropping: audience interaction in networked audio performance","authors":"Jack Stockholm, Philippe Pasquier","author_ids":"3354322, 1717343","abstract":"Eavesdropping is an internet-based, interactive audio system that explores network mediated, musical performance in shared public spaces. The project aims to develop an environment which increases audience interaction and connectedness in a localized, computer-controlled performance. The system is a client-server architecture made of three components: (1) an audio preparation interface, (2) an interactive performance interface, and (3) a machine learning-based conductor. An artificial conductor mixes an acoustic ecology based on mood data entered by participants while learning from their feedback. Technicalities and early evaluation are presented.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"2c5c9a6cd63424fba87744b0ab10f9076bd21586","venue_1":"ACM Multimedia","year":"2012","title":"On shape and the computability of emotions","authors":"Xin Lu, Poonam Suryanarayan, Reginald B. Adams, Jia Li, Michelle G. Newman, James Zijun Wang","author_ids":"1706990, 2682595, 2164408, 1687366, 2965295, 5657771","abstract":"We investigated how shape features in natural images influence emotions aroused in human beings. Shapes and their characteristics such as roundness, angularity, simplicity, and complexity have been postulated to affect the emotional responses of human beings in the field of visual arts and psychology. However, no prior research has modeled the dimensionality of emotions aroused by roundness and angularity. Our contributions include an in depth statistical analysis to understand the relationship between shapes and emotions. Through experimental results on the International Affective Picture System (IAPS) dataset we provide evidence for the significance of roundness-angularity and simplicity-complexity on predicting emotional content in images. We combine our shape features with other state-of-the-art features to show a gain in prediction and classification accuracy. We model emotions from a dimensional perspective in order to predict valence and arousal ratings which have advantages over modeling the traditional discrete emotional categories. Finally, we distinguish images with strong emotional content from emotionally neutral images with high accuracy.","cites":"20","conferencePercentile":"94.14556962"},{"venue":"ACM Multimedia","id":"a2422eca3b5aa59e22c9525d51069429f3e36515","venue_1":"ACM Multimedia","year":"2009","title":"Visual summaries of popular landmarks from community photo collections","authors":"Wei-Chao Chen, Agathe Battestini, Natasha Gelfand, Vidya Setlur","author_ids":"1715853, 1700609, 1683095, 1689165","abstract":"We present a novel data-driven algorithm that leverages online image repositories such as Flickr for automatically generating tourist maps. Our hypothesis is that, given a large enough dataset of images with geo-based metadata, clusters of matching images from that dataset tend to provide reliable cues as to what the popular tourist spots may be. Our algorithm takes the geographical area of interest as input and retrieves geotagged photos from online photo collections. By clustering the photos based on their locations and identifying the popular tags for each cluster, our algorithm generates a set of points of interest (POIs) for the area. After retrieving additional photos based on these discovered POI tags, we use image matching to find the most representative landmark view for each POI. Finally, we remove clutter from the representative image and apply tooning to generate a map icon for each landmark.","cites":"30","conferencePercentile":"91.52892562"},{"venue":"ACM Multimedia","id":"da048c059f94dd099e42345754e0639501a9852a","venue_1":"ACM Multimedia","year":"2001","title":"Consistency control for distributed interactive media","authors":"Jürgen Vogel, Martin Mauve","author_ids":"1688261, 1751862","abstract":"In this paper we present a generic consistency control service for distributed interactive media, i.e. media which allow a distributed group of users to interact with the medium itself. Consistency control is vital to these media since they typically require that a local copy of the medium's state be maintained by each user's application. Our service helps the applications to keep the local state copies consistent. The main characteristics of this service are as follows: a significant number of inconsistencies are prevented by using a mechanism called local lag. Inconsistencies that cannot be prevented are repaired by an improved timewarp algorithm that can be executed locally without burdening the network or the applications of other users. Exceptional situations and consistency during late-join situations are supported by a consistent state request mechanism. Moreover, the service also supports the application in detecting intention conflicts between the actions of distinct users. The major part of this functionality is based on a media model and the application level protocol for distributed interactive media (RTP/I) and can thus be reused by arbitrary RTP/I-based applications. In order to demonstrate the feasibility of our approach and to evaluate its performance we have integrated the generic consistency service into a shared whiteboard system.","cites":"20","conferencePercentile":"80.10204082"},{"venue":"ACM Multimedia","id":"3590cf639041a14ee204a338e8cd1b47e56c0d55","venue_1":"ACM Multimedia","year":"2007","title":"Tracking multiple speakers using CPHD filter","authors":"Nam Trung Pham, Weimin Huang, Sim Heng Ong","author_ids":"1750135, 1742173, 1685644","abstract":"In this paper, we present an efficient method for tracking multiple speakers in a reverberant environment. The proposed method is based on the cardinalized probability hypothesis density (CPHD) filter. Because the CPHD filter can handle a large amount of clutter measurements, our method has a high reliability when tracking multiple speakers. Simulation experiments are presented to demonstrate the performance of the proposed method.","cites":"2","conferencePercentile":"28.125"},{"venue":"ACM Multimedia","id":"12fdd8a468b0c36659e97925efecd6577fbc1ef6","venue_1":"ACM Multimedia","year":"1997","title":"What Should a Wildebeest Say? Interactive Nature Films for High School Classrooms","authors":"Brian K. Smith, Brian J. Reiser","author_ids":"3598181, 1692035","abstract":"Nature documentaries play an important role in high school biology classrooms, yet they deliver a passive and biased account of the behavior of organisms. To engage students in more active problem solving around behavioral topics, we created an interactive video system called Animal Landlord. Part of a week-long curriculum designed to introduce concepts in behavioral ecology, Animal Landlord presents film clips of the Serengeti lion hunting its prey. Students select and annotate video frames with explanations of their significance to the hunt, compare annotations across films, and ultimately genera!-ize a qualitative model of predation behaviors. This paper discusses the motivations for changing the nature of documentary use in the classroom, the ways in which we change the form of traditional narration for pedagogical purposes, and the interac-tivity that emerges in the social context of the classroom. INTRODUCTION New reform efforts in education (e.g., [17]) attempt to move students away from passive textbook and lecture activities by advocating more student-directed activities. Providing students with rich problem settings in which they can engage in iterative hypothesis generation and testing and explanation of causal relationships may result in more productive learning that shallow exposure to a broad base of content [4, 231. Many computer-based learning environments have been developed to provide interactive settings for students to engage in realistic activities.","cites":"13","conferencePercentile":"40.47619048"},{"venue":"ACM Multimedia","id":"fee90baedc3fdd0b7c151fa996f476121d0da1c7","venue_1":"ACM Multimedia","year":"2010","title":"Multi-display map touring with tangible widget","authors":"Marco Piovesana, Ying-Jui Chen, Neng-Hao Yu, Hsiang-Tao Wu, Li-Wei Chan, Yi-Ping Hung","author_ids":"1967217, 2656151, 2436503, 1775992, 1682665, 7312257","abstract":"Many map systems are created to help the user finding a place or define a route to follow. Google Map extends the concept of \"surfing the map\" by adding a street view that allows the user to explore a place from real pictures, creating the same feeling of walking through the streets. The horizontal 2D map and vertical panoramic street view, however, cause usability problems, while operating with traditional computer mouse and keyboards and presenting by single vertical or horizontal display. This paper presents a new table system composed of a horizontal tabletop screen and a vertical screen. The map view and the street view are displayed on the horizontal and vertical displays of our system respectively. Users can place the tangible pawn on the 2D map to have direct access of the street view from the pawn's point of view. In the user study, we compare our system with a standard computer system in the navigation task. The results reported that our system improves the intuitiveness of use, efficiency of city exploring and ease of remembrance on spaces that are not familiar beforehand. We also discuss limitations of using tangible objects for map navigation.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"4f0996573f05b634c2d3cf67124b9fb9f19960e8","venue_1":"ACM Multimedia","year":"2005","title":"Echology: an interactive spatial sound and video artwork","authors":"Meghan Deutscher, Reynald Hoskinson, Sachiyo Takahashi, Sidney Fels","author_ids":"2637706, 2924938, 2872623, 1749457","abstract":"We present a novel way of manipulating a spatial soundscape, one that encourages collaboration and exploration. Through a table-top display surrounded by speakers and lights, participants are invited to engage in peaceful play with Beluga whales shown through a live web camera feed from the Vancouver Aquarium in Canada. Eight softly glowing buttons and a simple interface encourage collaboration with others who are also enjoying the swirling Beluga sounds overhead.","cites":"5","conferencePercentile":"41.83168317"},{"venue":"ACM Multimedia","id":"f9b199786027136a0599cc47829081841d6d065f","venue_1":"ACM Multimedia","year":"2016","title":"A Deeply-Supervised Deconvolutional Network for Horizon Line Detection","authors":"Lorenzo Porzi, Samuel Rota Bulò, Elisa Ricci","author_ids":"3202308, 2145174, 1878028","abstract":"Automatic skyline detection from mountain pictures is an important task in many applications, such as web image retrieval, augmented reality and autonomous robot navigation. Recent works addressing the problem of Horizon Line Detection (HLD) demonstrated that learning-based boundary detection techniques are more accurate than traditional filtering methods. In this paper we introduce a novel approach for skyline detection, which adheres to a learning-based paradigm and exploits the representation power of deep architectures to improve the horizon line detection accuracy. Differently from previous works, we explore a novel deconvolutional architecture, which introduces intermediate levels of supervision to support the learning process. Our experiments, conducted on a publicly available dataset, confirm that the proposed method outperforms previous learning-based HLD techniques by reducing the number of spurious edge pixels.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"0a11ffd59acd961f9972ceb9a31809fa271a4bbe","venue_1":"ACM Multimedia","year":"2010","title":"AudioFeeds: a mobile auditory application for monitoring online activities","authors":"Tilman Dingler, Stephen A. Brewster","author_ids":"1726630, 1721608","abstract":"User participation has transformed the way news travel the globe. With the rise of the 'Web 2.0' phenomenon users have been empowered with the means of creating and distributing informational items, which we call social feeds. Platforms like Twitter and Facebook provide a variety of tools to facilitate real-time communication among people. But social sites are not limited to personal chat; they also provide an effective means for organizing large groups of people in response to catastrophic disasters. Monitoring these feeds can provide time-critical information, but can easily lead to information overload due to the large amount of data being shared.\n In this paper we introduce a mobile auditory display application called AudioFeeds that allows users to maintain an overview of activities in different social feeds. AudioFeeds runs on a mobile device and enables users to get an overview of their social networks and spot peaks in activity by sonifying social feeds and creating a spatialised soundscape around the user's head. We conducted a user study looking into different aspects of activity monitoring. Results show that our application provides an effective way for monitoring overall activity levels and allows users to identify activity peaks with 86.1% accuracy even when mobile.","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"114836a70e39922898def7f623985bba53ee0b05","venue_1":"ACM Multimedia","year":"2015","title":"Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology","authors":"Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara, Shih-Fu Chang","author_ids":"2447185, 4725884, 1680075, 2109913, 7179616, 1735547","abstract":"Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of &gt;15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, &gt;7.36M images and their metadata is also released.","cites":"18","conferencePercentile":"98.88888889"},{"venue":"ACM Multimedia","id":"402e22f4a2a9855c59374db8113d104ee309532a","venue_1":"ACM Multimedia","year":"1996","title":"Meme Media and a World-Wide Meme Pool","authors":"Yuzuru Tanaka","author_ids":"1720508","abstract":"Computers are expanding their target of augmentation from individuals to groups, and furthermore from groups to societies. While people in a group share a definite common task goal, people in a society share their knowledge resources and reuse them to produce new ones. The augmentation of societies requires a new type of media that can carry varieties of knowledge resources, replicate themselves, recombine themselves, and be naturally selected by their environment. They may be called meme media since they carry what R. Dawkins called \" memed'. The accumulation of memes in a society will form a meme pool, which will work as a gene pool to bring a rapid evolution of knowledge resources shared by this society. We need a worldwide repository of memes, and a good browser to access this repository. This repository works as a marketplace where people can publish memes, browse through them, and reuse some of them. This paper reviews IntelligentPad as a meme media architecture, and proposes two new system architectures that work as marketplace systems for meme media.","cites":"13","conferencePercentile":"27.77777778"},{"venue":"ACM Multimedia","id":"aa54ace3d525fbac13b690e3562e4406b2c37fcf","venue_1":"ACM Multimedia","year":"2014","title":"Interactive Line Drawing Recognition and Vectorization with Commodity Camera","authors":"Pradeep Kumar Jayaraman, Chi-Wing Fu","author_ids":"2514027, 1699457","abstract":"This paper presents a novel method that interactively recognizes and vectorizes hand-drawn strokes in front of a commodity webcam. Compared to existing methods, which recognize strokes on a completed drawing, our method captures both spatial and temporal information of the strokes, and faithfully vectorizes them with timestamps. By this, we can avoid various stroke recognition ambiguities, enhance the vectorization quality, and recover the stroke drawing order.\n This is a challenging problem, requiring robust tracking of pencil tip, accurate modeling of pen-paper contact, handling pen-paper and hand-paper occlusion, while achieving interactive performance. To address these issues, we develop the following novel techniques. First, we perform robust spatio-temporal tracking of pencil tip by extracting discriminable features, which can be classified with a fast cascade of classifiers. Second, we model the pen-paper contact by analyzing the edge-profile of the acquired trajectory and extracting the portions related to individual strokes. Lastly, we propose a spatio-temporal method to reconstruct meaningful strokes, which are coherent to the stroke drawing continuity and drawing order. By integrating these techniques, our method can support interactive recognition and vectorization of drawn strokes that are faithful to the actual strokes drawn by the user, and facilitate the development of various multimedia applications such as video scribing, cartoon production, and pen input interface.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"a9534771b4cc198523d13f56a7468417717bf215","venue_1":"ACM Multimedia","year":"2007","title":"On the minimum delay peer-to-peer video streaming: how realtime can it be?","authors":"Yong Liu","author_ids":"7135550","abstract":"P2P systems exploit the uploading bandwidth of individual peers to distribute content at low server cost. While the P2P bandwidth sharing design is very efficient for bandwidth sensitive applications, it imposes a fundamental performance constraint for delay sensitive applications: <i>the uploading bandwidth of a peer cannot be utilized to upload a piece of content until it completes the download of that content</i>. This constraint sets up a limit on how fast a piece of content can be disseminated to all peers in a P2P system. In this paper, we theoretically study the impact of this inherent delay constraint and derive the minimum delay bounds for realtime P2P streaming systems. We show that the bandwidth heterogeneity among peers can be exploited to significantly improve the delay performance of all peers. We further propose a simple <i>snow-ball streaming</i> algorithm to approach the minimum delay bound in realtime P2P video streaming. Our analysis suggests that the proposed algorithm has better delay performance and more robust than existing tree-based streaming solutions. Insights brought forth by our study can be used to guide the design of new P2P systems with shorter startup delays.","cites":"52","conferencePercentile":"95.3125"},{"venue":"ACM Multimedia","id":"26d60bd745dab2672b3d8fdee356f3a56db95aea","venue_1":"ACM Multimedia","year":"2012","title":"A user study on image browsing on touchscreens","authors":"David Ahlström, Marco A. Hudelist, Klaus Schöffmann, Gerald Schaefer","author_ids":"3017010, 3005976, 3273381, 1731558","abstract":"Default image browsing interfaces on touch-based mobile devices provide limited support for image search tasks. To facilitate fast and convenient searches we propose an alternative interface that takes advantage of 3D graphics and arranges images on a rotatable globe according to color similarity. In a user study we compare the new design to the iPad's image browser. Results collected from 24 participants show that for color-sorted image collections the globe can reduce search time by 23% without causing more errors and that it is perceived as being fun to use and preferred over the standard browsing interface by 70% of the participants.","cites":"10","conferencePercentile":"85.75949367"},{"venue":"ACM Multimedia","id":"20ba49ea6b91a12dca4ef28c73028e82cfdbcf35","venue_1":"ACM Multimedia","year":"2013","title":"Semantic technologies for multimedia content: foundations and applications","authors":"Ansgar Scherp","author_ids":"1753135","abstract":"Higher-level semantics for multimedia content is essential to answer questions like ``Give me all presentations of German Physicists of the 20th century''. The tutorial provides an introduction and overview to such semantics and the developments in multimedia metadata. It introduces current advancements for describing media on the web using Linked Open Data and other more expressive semantic technologies. The application of such technologies will be shown at concrete examples.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"cd75b71b47ba65775cd0f4f8ce27e734d0ef8604","venue_1":"ACM Multimedia","year":"2004","title":"From transmission to multiplicity: interactive art installations as a site for research","authors":"Keir Smith","author_ids":"2876189","abstract":"Central Hypothesis: &#60;i>The methods with which many contemporary interactive art installations are being designed, built and experienced is a model of multiplicity. Further, that the study of the multitudinous nature of this experience can inform our understanding of how people interact, in a computer interface mediated group situation, with both each other and the interface. The outcomes of which can, in turn, help improve how we design and build interfaces for collaborative interaction.&#60;/i> .\n This research, through a literature review, interviews with practitioners in the field and analysis of techniques and technologies the field employs, intends to show that this model of multiplicity, which I will call MMM (Multiple Modalities [for the Multitudes1]), is a common interactive installation art methodology. In which the single author is replaced by a group of collaborators, the single object is replaced by a multitude of objects and that, most importantly in this research, the single reader can be replaced by multitudinous collaborative viusers.\n The technology to build computer mediated collaborative interfaces is in its infancy. The human-computer interaction (HCI) community has vast experience with the single-user, single-interface situation, but precious little with multi-user interfaces, when those users are in same local space, using the same interface. It is the intention of the &#60;i>in situ&#60;/i> study component of this thesis can be used, in conjunction with an appropriate literature review, to remedy this deficiency in our understanding.\n This research, employing a case study methodology, will investigate two works. &#60;i>Conversations&#60;/i> a multimedia project currently being developed at the iCinema Centre and the &#60;i>configurable&#60;/i>, experimental, interactive, multi-user same-site, installation &#60;i>Socialising with Strangers&#60;/i> will be Then to help inform our understanding of group collaborative interaction. The technical infrastructure of &#60;i>SwS&#60;/i> will include logging and recording software. A review of relevant research, as well as analysis of the data collected from &#60;i>SwS&#60;/i>, &#60;i>in situ&#60;/i>, will be combined to aid the generation of a set of guidelines for computer-mediated interactive systems for multiple locally situated viusers.","cites":"1","conferencePercentile":"19.11764706"},{"venue":"ACM Multimedia","id":"f31dcc810885bf20d474c46a84befb7762d592d2","venue_1":"ACM Multimedia","year":"1999","title":"Multimedia access and retrieval: the state of the art and future directions (panel session)","authors":"Shih-Fu Chang, Gwendal Auffret, Jonathan Foote, Chung-Sheng Li, Behzad Shahraray, Tanveer F. Syeda-Mahmood, HongJiang Zhang","author_ids":"1735547, 3178710, 1797460, 1755389, 2413848, 2193427, 1718558","abstract":"Theme Several years have passed since the research topic of content based multimedia retrieval emerged. We have witnessed the burgeoning research activities into a plenitude of new indexing, retrieval, and filtering tools for images, video, audio, music, graphics, and their combinations with text-based information. Exciting research opportunities arise when integrating knowledge from multiple disciplines, such as media content processing, database, information retrieval, and machine user interface. In the commercial domain, we have also witnessed several impressive efforts moving technologies into practical arenas. This panel includes experts from industry, research labs, and academia. The panel will assess the state of the art and articulate the important future directions in the general field of multimedia access and retrieval. The discussion will include, but not be limited to the following issues:. What major technical breakthroughs have been achieved in industry and academia in the last few years? What are the lessons we learn from these efforts?. What successful applications have been realized? What new potential applications are emerging?. What new research challenges are arising in this cross-disciplinary area? Can knowledge from the field of Information Retrieval be extended to solve the multimedia retrieval problems? Are there new content analysis challenges beyond those being tackled in audiovisual signal analysis, computer vision, and speech recognition? Does multimedia retrieval require new modalities for human-computer interactions?. What should international standards like MPEG-7 cover? Will these types of standards have significant impact on the market place? While different standards for metadata and resource description are being developed by various standardization groups, what kinds of cooperative relationships are necessary? Permission to make digital or hard copies of all or part of this work for personal or ClaSSrOOm use is granted without fee provided that copies are not made 0r distributed for profit or commercial advantage and that Copies bear this notice and the full citation on the first page. To copy otherwise. to republish. to post on servers or to redistribute to liStS, requires prior specific permission and/or a fee. As more and more multimedia libraries are being built, accurate content indexing strategies are needed in order to allow efficient retrieval of images and sounds. As the cost of manual annotation remains prohibitive, librarians turn to automatic indexing technology and to emerging metadata standards such as MPEG-7. However the gap between signal-based descriptions provided by extraction tools and conceptual descriptions required to answer library users' queries remains huge. In …","cites":"24","conferencePercentile":"73.52941176"},{"venue":"ACM Multimedia","id":"489a7b2b1a8c22e761e92aa9e9d56874afaa40eb","venue_1":"ACM Multimedia","year":"2015","title":"Automated Video Editing for Aesthetic Quality Improvement","authors":"Jun-Ho Choi, Jong-Seok Lee","author_ids":"6076894, 5722542","abstract":"In these days, a large number of videos is taken by various kinds of handheld devices, but many of them have poor aesthetic quality. In this paper, we present an automated video editing system that uses the shot length, camera motion, and color distribution as key aesthetic features. Given an amateur video, our system computes the original unrefined camera motion as homography and tries to remove some unreliable frames, which consequently splits the video into several shots. It then applies enhancement processes, including reconstruction of the overall camera motions and harmonization of color distributions. We apply our method to some amateur videos and evaluate the results through a subjective test. It is demonstrated that reducing the shot length in our method is a key point of editing that can lead enhanced satisfaction by viewers for the edited videos.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"3ee379e2503d3dde8beefae255150166447e89df","venue_1":"ACM Multimedia","year":"2010","title":"Yongzheng emperor's interactive tabletop: seamless multimedia system in a museum context","authors":"Chun-Ko Hsieh, I-Ling Liu, Neng-Hao Yu, Yueh-Hsuan Chiang, Hsiang-Tao Wu, Ying-Jui Chen, Yi-Ping Hung","author_ids":"1722666, 1683717, 2436503, 2024179, 1775992, 2656151, 7312257","abstract":"In this paper, we propose the seamless multimedia system <i>Yongzheng Emperor's interactive tabletop</i>, which has been incorporated into the special exhibition \"Harmony and Integrity: The Yongzheng Emperor and His Times\" at the National Palace Museum in Taiwan. The multimedia system features the innovative use of physical artifacts - Yongzheng figurines and a model of Yongzheng-era calendar clock as tangible user interfaces, which have been used to activate on the Surface the emperor's life at court and chronological events of his times. Museum audiences can naturally and intuitively explore the Emperor's stories by the use of hand gestures. The system vividly connects the modern world of the audiences with the emperor's virtual world, engaging museum audiences in the most interactive and compelling way to learn about the emperor. The paper aims to present the development of the seamless tabletop system in a historical museum context, including design principles, implementation, applications, and effectiveness of the system. Our contribution in this project is to demonstrate a new exhibition display model for the museum sector.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"153d8fcf4077a1e8c7fbd2ebaa9842d140aa27de","venue_1":"ACM Multimedia","year":"2006","title":"Detecting irregularity in videos using kernel estimation and KD trees","authors":"Yun Li, Chunjing Xu, Jianzhuang Liu, Xiaoou Tang","author_ids":"1724845, 1691522, 7137861, 1741901","abstract":"Automatic event understanding is the ultimate goal for many visual surveillance systems. In this paper, we propose a novel approach for on-line detecting unusual human activities in videos without the need to explicitly define all valid configurations. Within the framework of Bayesian inference, the detection process is formulated as an MAP estimation where we attempt to find whether activities in new video segments have similar activities in a video database. Our approach has three contributions: firstly, we build the statistical representation of normal behaviors in the database using nonparametric kernel density estimation; secondly, local feature descriptors are highly compressed using PCA and stored in a K-D tree structure, making the search for behavior-based similarity fast and effective; thirdly, the K-D trees are used to generate multiple hypotheses which compete for the optimal classification. The approach requires no tracking, no explicit motion estimation, and no predefined class-based templates. Experimental results have validated our approach in many real-world video sequences.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"bcc85dd63c04f4d5771a498ca17e73427c0e3f55","venue_1":"ACM Multimedia","year":"2006","title":"3D object retrieval using 2D line drawing and graph based relevance reedback","authors":"Liangliang Cao, Jianzhuang Liu, Xiaoou Tang","author_ids":"2464399, 7137861, 1741901","abstract":"This paper aims to provide a user-friendly interface for 3D object retrieval. In previous 3D retrieval systems, the user mainly uses two methods to input a query: providing an existing 3D objects, or providing partial shape information of desired objects such as text and 2D shapes. The first method fails when the user does not have a similar 3D object in hand, and the second method cannot sufficiently describe 3D shapes of objects. We believe that the best way is to have a good interface that can convert a 2D sketch drawn by the user into a 3D object as the query. A 2D line drawing is easy to be drawn and is the simplest and most direct way of illustrating a 3D object. In this paper, we develop an interface of 3D object reconstruction from line drawings, which allows the user to draw line drawings of objects with both planar and curved surfaces. In addition, in order to refine the retrieved results, we develop a relevance feedback algorithm based on a novel graph discriminant analysis. Compared with recently published relevance feedback algorithms, our algorithm achieves better retrieval performance.","cites":"16","conferencePercentile":"79.2746114"},{"venue":"ACM Multimedia","id":"c7b589181fad005e0d37eee5f95c2864d6fee114","venue_1":"ACM Multimedia","year":"1997","title":"An Improved Auditory Interface for the Exploration of Lists","authors":"Ian J. Pitt, Alistair D. N. Edwards","author_ids":"2374300, 1684360","abstract":"Synthetic speech is widely used to enable blind people to receive output from computer systems. However, speech is slow to use compared with vision and places far higher demands on short-term memow. These problems are particularly apparent when exploring farge data structures such as lists and tables. An experiment was conducted in which subjects were asked to memorise and recite lists of filenames. Analysis showed that subjects organised the material to aid recall and used a range of prosodic devices to convey this organisation to the listener. A practical list-reading program was developed which replicates-as far as possible the methods of organisation and spoken presentation used by the subjects. This program was evaluated in a practical task, in comparison with an existing speech-based system for blind users. The results showed that subjects performed the task significantly faster using the demonstration system and also reported lower levels of effort and mental demand.","cites":"5","conferencePercentile":"19.04761905"},{"venue":"ACM Multimedia","id":"adf28c6ffde4737a7734edf35aceed5473ddf131","venue_1":"ACM Multimedia","year":"2012","title":"Low bitrate source-filter model based compression of vibrotactile texture signals in haptic teleoperation","authors":"Rahul Gopal Chaudhari, Burak Cizmeci, Katherine J. Kuchenbecker, Seungmoon Choi, Eckehard G. Steinbach","author_ids":"2612650, 2669779, 1716879, 1718126, 7252930","abstract":"Vibrotactile signals convey the touch-based characteristics of object surfaces felt through a tool. They particularly enhance the quality of human-machine interactions by providing realistic haptic perception of textures. In this paper, inspired by the similarities observed between vibrotactile texture signals and speech signals, we present a novel vibrotactile texture codec for bilateral teleoperation, based on well-known speech coding techniques. The proposed low bitrate, high quality codec preserves not only the spectral signature vital to the general feel of the texture, but also important temporal features of the texture signal. We report a compression ratio of 8:1 (12.5 %) with a <i>constant</i> output bitrate of 4 kbps, and we validate the perceptual transparency of the codec via rigorous subjective tests and analyses.","cites":"5","conferencePercentile":"70.88607595"},{"venue":"ACM Multimedia","id":"61c6e9958529461c1772ea17838ce584a0d7cdca","venue_1":"ACM Multimedia","year":"2011","title":"Hypergraph spectral hashing for similarity search of social image","authors":"Yueting Zhuang, Yang Liu, Fei Wu, Yin Zhang, Jian Shao","author_ids":"1755711, 1750084, 1695826, 1685889, 8187421","abstract":"The development of social media brings great challenges to image retrieval on both efficiency and accuracy. In addition to achieving fast similarity search over large scale data, it is very crucial to represent the complex and high-order relationships among the social contents to improve the semantic understanding of social images.In this paper, unified hypergraph is implemented to model the various relationships among images and other contexts in social media. Moreover, we extend traditional spectral hashing to hypergraph to accelerate similarity search of social images by mapping semantically related vertices into similar binary codes within a short Hamming distance. Furthermore, the proposed HSH approach is extended to out-of-sample data in a supervised manner. We evaluated our approach on the dataset crawled from <i>Flickr</i> and the experiment results indicate that our proposed HSH approach is both efficient and effective.","cites":"8","conferencePercentile":"77.8425656"},{"venue":"ACM Multimedia","id":"7c66ec70e670cd24371cf94620276ecf769b38ca","venue_1":"ACM Multimedia","year":"2006","title":"Shape from regularities for interactive 3D reconstruction of piecewise planar objects from single images","authors":"Zhenguo Li, Jianzhuang Liu, Xiaoou Tang","author_ids":"7718952, 7137861, 1741901","abstract":"3D object reconstruction from single 2D images has many applications in multimedia. This paper proposes an approach based on image regularities such as connectivity, parallelism, and orthogonality possessed by the objects with simple user interactions. It is assumed that the objects are piecewise planar. By representing the 3D objects as a shape vector consisting of the normals of the faces of the objects, we impose geometric constraints on this shape vector using the regularities of the objects. We derive a system of equations in terms of the shape vector and the focal length, which we can solve for the shape vector optimally. Experimental results on real images are shown to demonstrate the effectiveness of this method.","cites":"2","conferencePercentile":"29.79274611"},{"venue":"ACM Multimedia","id":"706d695ccdbc2c11f8140fed9fd5f9c2d2c758d4","venue_1":"ACM Multimedia","year":"2015","title":"EEG Connectivity Analysis in Perception of Tone-mapped High Dynamic Range Videos","authors":"Seong-Eun Moon, Jong-Seok Lee","author_ids":"2914869, 5722542","abstract":"High dynamic range (HDR) imaging has attracted attention as a new technology for immersive multimedia experience. In comparison to conventional low dynamic range (LDR) contents, HDR contents are expected to provide better quality of experience (QoE). In this paper, we investigate implicit QoE measurement of tone-mapped HDR videos by using connectivity-based EEG features that convey information about simultaneous activations of different brain regions and thus can explain better the cognitive process than the conventional features using single channel powers. Through the experiment classifying EEG signals into tone-mapped HDR and LDR, it is shown that the connectivity features, particularly those representing directed information flows between brain regions, are effective in both subject-dependent and subject-independent scenarios.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"3a0600d4edf55cba384e4e60e7655d98317c1184","venue_1":"ACM Multimedia","year":"2003","title":"Interleaving media data for MPEG-4 presentations","authors":"Jeff Boston, Michelle Y. Kim, William L. Luken, Edward So, Steve Wood","author_ids":"2651972, 2003203, 2575390, 2619534, 7853531","abstract":"A composite multimedia presentation may be represented by a sequence of virtual media data packets. An algorithm is presented for ordering these virtual media data packets so as to minimize the initial delay required to transfer the composite stream from a media server to a client. This algorithm has been implemented as part of the IBM Toolkit for MPEG-4.","cites":"0","conferencePercentile":"4.054054054"},{"venue":"ACM Multimedia","id":"7e1dd29578bf6c007cd768769b6af1cb62e13eea","venue_1":"ACM Multimedia","year":"2015","title":"Subjectivity in Aesthetic Quality Assessment of Digital Photographs: Analysis of User Comments","authors":"Won-Hee Kim, Jun-Ho Choi, Jong-Seok Lee","author_ids":"7891267, 6076894, 5722542","abstract":"While most of the existing work in aesthetic image quality assessment focuses on the overall (or average) opinion of users, this paper raises the issue of subjectivity (or taste) of aesthetic quality. We argue that subjectivity differs among different images, and investigate what causes such difference. We first analyze statistics of the user ratings of photos in a photo contest website, DPChallenge, in the viewpoint of average and standard deviation values of the ratings. Then, more importantly, we analyze the users' comments in order to identify sources contributing to subjectivity. When considering the importance of personalization in photo applications, we believe that our findings will be a valuable first step in the relevant future research.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"0dfef1e280845d4beb410f5cf6dcea9b1b9c4d1f","venue_1":"ACM Multimedia","year":"2000","title":"Techniques for interactive video cubism (poster session)","authors":"Sidney Fels, Eric Lee, Kenji Mase","author_ids":"1749457, 4394702, 1722602","abstract":"This paper presents an interactive video visualization technique called video cubism. With this technique, video data is considered to be a block of three dimensional data where frames of video data comprise the third dimension. The user can observe and manipulate a cut plane or cut sphere through the video data. An external real-time video source may also be attached to the video cube. The visualization leads to images that are aesthetically interesting as well as being useful for image analysis.","cites":"1","conferencePercentile":"16.84782609"},{"venue":"ACM Multimedia","id":"6435c6c4ed66ebaf75025d0208c40410248f5d72","venue_1":"ACM Multimedia","year":"2005","title":"Designing time-based interactions with multimedia","authors":"Eric Lee","author_ids":"4394702","abstract":"The current model of time in multimedia frameworks poses particular problems when designing multimedia systems with time-based interaction. We propose to expand and extend an existing distinction between semantic time and real time from music and film theory to multimedia systems design. The semantic time concept also forms the foundation of a new software framework for multimedia systems that we are building, that, unlike most existing frameworks, includes mechanisms for time-based effects such as time-stretching.","cites":"0","conferencePercentile":"4.950495049"},{"venue":"ACM Multimedia","id":"923b957c5e9777d888c7c9e780518b6bda4aeaf4","venue_1":"ACM Multimedia","year":"2004","title":"Remote interactive graffiti","authors":"Jonathan Foote, Don Kimber","author_ids":"1797460, 2178004","abstract":"We present an installation that allows distributed internet participants to \"draw\" on a public scene using light. The iLight system is a camera/projector system designed for remote collaboration. Using a familiar digital drawing interface, remote users \"draw\" on a live video image of a real-life object or scene. Graics drawn by the user are then projected onto the scene, where they are visible in the camera image. Because camera distortions are corrected and the video is aligned with the image canvas, drawn graics appear exactly where desired. Thus the remote users may harmlessly mark a ysical object to serve their own their artistic and/or expressive needs. We also describe how local participants may interact with remote users through the projected images. Besides the intrinsic \"neat factor\" of action at a distance, this installation serves as an experiment in how multiple users from different locales and cultures can create a social space that interacts with a ysical one, as well as raising issues of free expression in a non-destructive context.","cites":"2","conferencePercentile":"26.96078431"},{"venue":"ACM Multimedia","id":"20a1a844d033f2bdbe06c0983c58c4afee592ad2","venue_1":"ACM Multimedia","year":"2001","title":"Panoramic video capturing and compressed domain virtual camera control","authors":"Xinding Sun, Jonathan Foote, Don Kimber, B. S. Manjunath","author_ids":"3248811, 1797460, 2178004, 8671823","abstract":"A system for capturing panoramic video and a novel method for corresponding compressed domain virtual camera control is presented. It targets applications such as classroom lectures and video conferencing. The proposed method is based on the FlyCam panoramic video system that is designed to produce high resolution and wide-angle video sequences by stitching the video pictures from multiple stationary cameras. The panoramic video sequence is compressed into an MPEG-2 stream for delivery. The proposed method integrates region of Interest (ROI) detection, tracking, and virtual camera control, and works on compressed domain information only. It first detects the ROI in the P (predictive coded) picture using only the macroblock type information, It then up-samples this detection result to obtain the ROI of the whole video stream. The ROI is tracked using a Kalman filter. The Kalman filter estimation results are used for virtual camera control that simulates human controlled video recording. The system has no physical camera motion and the virtual camera parameters are readily available for video indexing. The proposed system has been implemented for real time processing.","cites":"19","conferencePercentile":"78.06122449"},{"venue":"ACM Multimedia","id":"612bf2b805db5308173d878ac39eec7f814fec9e","venue_1":"ACM Multimedia","year":"2001","title":"FlyAbout: spatially indexed panoramic video","authors":"Don Kimber, Jonathan Foote, Surapong Lertsithichai","author_ids":"2178004, 1797460, 2543736","abstract":"We describe a system called FlyAbout which uses spatially indexed panoramic video for virtual reality applications. Panoramic video is captured by moving a 360@deg camera along continuous paths. Users can interactively replay the video with the ability to view any interesting object or choose a particular direction. Spatially indexed video gives the ability to travel along paths or roads with a map-like interface. At junctions, or intersection points, users can chose which path to follow as well as which direction to look, allowing interaction not available with conventional video. Combining the spatial index with a spatial databsdde of maps or objects allows users to navigate to specific locations or interactively inspect particular objects.","cites":"26","conferencePercentile":"83.67346939"},{"venue":"ACM Multimedia","id":"422d49444f21c2d99d748405fe00f56c0732d15b","venue_1":"ACM Multimedia","year":"2005","title":"The SINE WAVE ORCHESTRA stay","authors":"Kazuhiro Jo, Ken Furudate, Daisuke Ishida, Mizuki Noguchi","author_ids":"3093077, 7861273, 2369936, 2699045","abstract":"This is a report of creative and technical considerations in building a participatory sound performance The SINE WAVE ORCHESTRA stay. In this performance, the participants one by one leave their own sine wave in the performance space. These sine waves form a mutually interfering sound space while the sound field of a room changes during the performance.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"bafe244a5115eaed7eb366ff6d9417c3929f48f3","venue_1":"ACM Multimedia","year":"1994","title":"Placeholder: Landscape and Narrative in Virtual Environments","authors":"Brenda Laurel, Rachel M. Strickland","author_ids":"2313966, 2750203","abstract":"A note on authorship Sections of this paper are authored by different individuals. The people who worked on Placeholder shared a core set of goals, but each individual contributor had distinct interests, values and ways of working. Each of us reflects on it differently, seeing different shortcomings and strengths in the work, and learning different lessons fi'om the process. For these reasons it seems important to preserve the singularity of each author's voice. The idea of using virtual reality for entertainment purposes is actually quite recent in the history of the technology. Early VR entertainment applications, appearing in the late 1980s, were extensions of the existing \"serious\" application of simulation-based training. Flight-sire technology-motion platforms used in sync with motion video or animation-were much more readily adapted to theme park applications than immersive VR with head-mounted displays. Motion platform rides, of which Star Tours is probably the best known example, trade off individual viewpoint control and the sense of agency for thrilling, finely calibrated effects and the optimization of \"throughput\"-that is, getting the most people through the ride in the least time. Networked pods, as used in Virtual World Entertainment systems (previously Battletech), are also derived from flight-sim technology. \"Classic\" virtual reality, employing head-mounted displays and various forms of body tracking, presents serious throughput problems in theme park settings. It takes time to get the gear onto the participants. Only a handful of people can experience the attraction simultaneously (although a nmch larger audience might watch what the people \"inside\" the VR are doing). A hard-driving plot with distinct beginning, middle attd end is a great way to control how long an experience takes, but \"classic\" VR is inimical to this kind of attthorial control-it works best when people can move about and do things in virtual environments in a relatively unconstrained way. In fact, it may be that the nature of VR makes it inappropriate to think of it as an entertainment medium. Mass entertaimnent implies the consumption of some performance by a large audience. Generally speaking, the size of the audience is inversely proportional to the degree of influence over the course of events that can be afforded any one person'. Not\" can we simply turn to human-to-human interaction as the source of engagement attd still support a large number of simultaneous participants; virtual spaces seem not to differ fl'om actual ones in terms of social and attentional constraints …","cites":"35","conferencePercentile":"71.18644068"},{"venue":"ACM Multimedia","id":"2efcdd8c08c7a522f09f824bdca6c92569ac41d0","venue_1":"ACM Multimedia","year":"2005","title":"MMM2: mobile media metadata for media sharing","authors":"Marc Davis, John F. Canny, Nancy A. Van House, Nathaniel Good, Simon King, Rahul Nair, Carrie Burgener, Bruce Rinehart, Rachel M. Strickland, Guy Campbell, Scott Fisher, Nick Reid","author_ids":"1777964, 1729041, 1791418, 6201430, 1721016, 1799464, 2252812, 3374918, 2750203, 7998170, 2195831, 1833448","abstract":"As cameraphones become the dominant platform for consumer multimedia capture worldwide, multimedia researchers are faced both with the challenge of how to help users manage the billions of photographs they are collectively producing and the opportunity to leverage cameraphones' ability to automatically capture temporal, spatial, and social contextual metadata to help manage consumer multimedia content. In our Mobile Media Metadata 2 (MMM2) prototype, we apply collaborative filtering techniques to automatically gathered contextual metadata to infer the likely sharing recipients for photos captured on cameraphones. We show that while current cameraphone sharing interfaces are fraught with difficulty, it is possible to use a context-aware approach to make the sharing of cameraphone photos simpler and more satisfying for users. Based on our analysis of the relative contributions of different cameraphone sensors to predicting the likely recipients for photos, we discover for our user population that the temporal context of photo capture proved highly predictive of photo sharing behavior.","cites":"59","conferencePercentile":"95.04950495"},{"venue":"ACM Multimedia","id":"710d03825340dea8b902015a58b272aedbb4c7d8","venue_1":"ACM Multimedia","year":"2006","title":"Multimedia content protection","authors":"Dulce B. Ponceleon, Julian A. Cerruti","author_ids":"1802641, 2802890","abstract":"Multimedia content protection is a controversial topic. Content owners want to protect their rights while consumers want flexible usage, privacy and seamless content flow. In this tutorial we cover from cryptography fundamentals, to history, emerging standards, state-of-the-art approaches and live demos.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"5af6f0d2095ce5af899a20dae0aec86d9d354385","venue_1":"ACM Multimedia","year":"2007","title":"Image inpainting by global structure and texture propagation","authors":"Huang Ting, Shifeng Chen, Jianzhuang Liu, Xiaoou Tang","author_ids":"1812303, 2869725, 7137861, 1741901","abstract":"Image inpainting is a technique to repair damaged images or modify images in a non-detectable form. In this paper, a novel global algorithm for region filling is proposed for image inpainting. After removing objects from an image, our approach fills the regions using patches taken from the image. The filling process is formulated as an energy minimization problem by Markov random fields (MRFs) and the belief propagation (BP) is utilized to solve the problem. Our energy function includes structure and texture information obtained from the image. One challenge in using BP is that its computational complexity is the square of the number of label candidates. To reduce the large number of label candidates, we present a coarse-to-fine scheme where two BPs run with much smaller numbers of label candidates instead of one BP running with a large number of label candidates. Experimental results demonstrate the excellent performance of our algorithm over other related algorithms.","cites":"19","conferencePercentile":"78.125"},{"venue":"ACM Multimedia","id":"3a2dfc193a510427f1ab79889fbf7a0396fe209a","venue_1":"ACM Multimedia","year":"2007","title":"Image matting using linear optimization","authors":"Shifeng Chen, Zhenguo Li, Jianzhuang Liu, Xiaoou Tang","author_ids":"2869725, 7718952, 7137861, 1741901","abstract":"An image can be assumed to be a composite of the foreground and the background. The foreground and the background of each pixel are linearly combined in terms of this pixel's foreground opacity (called alpha). Image matting is the process of estimating the foreground, the background and the alpha for each pixel. In this paper, we transform the ill-posed image matting problem into two over-determined linear optimization problems by introducing two medium variables and imposing smoothness constraints. Closed form solutions can be obtained from the two problems. Extensive experimental results indicate that our algorithm can generate high-quality matting results.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"1bf4e85ebbf8e7790c6ab20d88a4d3c164ea4299","venue_1":"ACM Multimedia","year":"2007","title":"Learning to gesture: applying appropriate animations to spoken text","authors":"Nathan D. Nichols, Jiahui Liu, Bryan Pardo, Kristian J. Hammond, Lawrence Birnbaum","author_ids":"2825790, 4136635, 1744936, 2796629, 1681932","abstract":"We propose a machine learning system that learns to choose human gestures to accompany novel text. The system is trained on scripts comprised of speech and animations that were hand-coded by professional animators and shipped in video games. We treat this as a text-classification problem, classifying speech as corresponding with specific classes of gestures. We have built and tested two separate classifiers. The first is trained simply on the frequencies of different animations in the corpus. The second extracts text features from each script, and maps these features to the gestures that accompany the script. We have experimented with using a number of features of the text, including n-grams, emotional valence of the text, and parts-of-speech. Using a na&#239;ve Bayes classifier, the system learns to associate these features with appropriate classes of gestures. Once trained, the system can be given novel text for which it will attempt to assign appropriate gestures. We examine the performance of the two classifiers by using n-fold cross-validation over our training data, as well as two user studies of subjective evaluation of the results. Although there are many possible applications of automated gesture assignment, we hope to apply this technique to a system that produces an automated news show.","cites":"2","conferencePercentile":"28.125"},{"venue":"ACM Multimedia","id":"5222abc4e9794ff4f554a480a5141ad51b7b3e51","venue_1":"ACM Multimedia","year":"2009","title":"Video completion via motion guided spatial-temporal global optimization","authors":"Ming Liu, Shifeng Chen, Jianzhuang Liu, Xiaoou Tang","author_ids":"5367622, 2869725, 7137861, 1741901","abstract":"In this paper, a novel global optimization based approach is proposed for video completion whose target is to restore the spatial-temporal missing regions of a video in a visually plausible way. Our algorithm consists of two stages: motion field completion and color completion via global optimization. First, local motions within the missing parts are completed patch-by-patch greedily using pre-computed available motions in the video. Then the missing regions are filled by sampling patches from available parts of the video. We formulate the video completion as a global energy minimization problem by Markov random fields (MRFs). Based on the completed motion field of the video, a well-defined energy function involving both spatial and temporal coherence relationship is constructed. A coarse-to-fine Belief Propagation (BP) is proposed to solve the optimization problem. Experimental results have demonstrated the good performance of our algorithm.","cites":"9","conferencePercentile":"64.04958678"},{"venue":"ACM Multimedia","id":"6bd2b461a48b8793aeed307c16b03e852763a97a","venue_1":"ACM Multimedia","year":"2009","title":"Boosting 3D object retrieval by object flexibility","authors":"Boqing Gong, Chunjing Xu, Jianzhuang Liu, Xiaoou Tang","author_ids":"1760838, 1691522, 7137861, 1741901","abstract":"In this paper, we propose a novel feature, called object flexibility, at a point of a 3D object to describe how the neighborhood of this point is massively connected to the object. We show that this feature is stable to the deformation of objects' articulations, in addition to commonly concerned linear transforms, i.e., translation, scale, and rotation. A shape descriptor is obtained based on this feature using the bag-of-words model. As an application, the descriptor is used to perform 3D object retrieval. Extensive experiments demonstrate its superiority over a variety of existing 3D shape descriptors in the retrieval of articulated objects, as well as its enhancement of other shape descriptors to retrieve generic 3D objects.","cites":"7","conferencePercentile":"55.78512397"},{"venue":"ACM Multimedia","id":"b6952268be84d9a37a8d48517f4e16e8ac01a6ec","venue_1":"ACM Multimedia","year":"2013","title":"Object co-segmentation via discriminative low rank matrix recovery","authors":"Yong Li, Jing Liu, Zechao Li, Yang Liu, Hanqing Lu","author_ids":"1689181, 5661757, 3233021, 1745356, 1694235","abstract":"The goal of this paper is to simultaneously segment the object regions appearing in a set of images of the same object class, known as object co-segmentation. Different from typical methods, simply assuming that the regions common among images are the object regions, we additionally consider the disturbance from consistent backgrounds, and indicate not only common regions but salient ones among images to be the object regions. To this end, we propose a Discriminative Low Rank matrix Recovery (DLRR) algorithm to divide the over-completely segmented regions (i.e.,superpixels) of a given image set into object and non-object ones. In DLRR, a low-rank matrix recovery term is adopted to detect salient regions in an image, while a discriminative learning term is used to distinguish the object regions from all the super-pixels. An additional regularized term is imported to jointly measure the disagreement between the predicted saliency and the objectiveness probability corresponding to each super-pixel of the image set. For the unified learning problem by connecting the above three terms, we design an efficient optimization procedure based on block-coordinate descent. Extensive experiments are conducted on two public datasets, i.e., MSRC and iCoseg, and the comparisons with some state-of-the-arts demonstrate the effectiveness of our work.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"692a8ed54c4336d181c6c15046407b7ee3183e77","venue_1":"ACM Multimedia","year":"2009","title":"Automatic facial expression recognition on a single 3D face by exploring shape deformation","authors":"Boqing Gong, Yueming Wang, Jianzhuang Liu, Xiaoou Tang","author_ids":"1760838, 7135663, 7137861, 1741901","abstract":"Facial expression recognition has many applications in multimedia processing and the development of 3D data acquisition techniques makes it possible to identify expressions using 3D shape information. In this paper, we propose an automatic facial expression recognition approach based on a single 3D face. The shape of an expressional 3D face is approximated as the sum of two parts, a basic facial shape component (BFSC) and an expressional shape component (ESC). The BFSC represents the basic face structure and neutral-style shape and the ESC contains shape changes caused by facial expressions. To separate the BFSC and ESC, our method firstly builds a reference face for each input 3D non-neutral face by a learning method, which well represents the basic facial shape. Then, based on the BFSC and the original expressional face, a facial expression descriptor is designed. The surface depth changes are considered in the descriptor. Finally, the descriptor is input into an SVM to recognize the expression. Unlike previous methods which recognize a facial expression with the help of manually labeled key points and/or a neutral face, our method works on a single 3D face without any manual assistance. Extensive experiments are carried out on the BU-3DFE database and comparisons with existing methods are conducted. The experimental results show the effectiveness of our method.","cites":"23","conferencePercentile":"87.80991736"},{"venue":"ACM Multimedia","id":"efd47484542e74277ea6f8d8e64c3833f702dbdc","venue_1":"ACM Multimedia","year":"2015","title":"Semi- and Weakly- Supervised Semantic Segmentation with Deep Convolutional Neural Networks","authors":"Yuhang Wang, Jing Liu, Yong Li, Hanqing Lu","author_ids":"7708083, 5661757, 1689181, 1694235","abstract":"Successful semantic segmentation methods typically rely on the training datasets containing a large number of pixel-wise labeled images. To alleviate the dependence on such a fully annotated training dataset, in this paper, we propose a semi- and weakly-supervised learning framework by exploring images most only with image-level labels and very few with pixel-level labels, in which two stages of Convolutional Neural Network (CNN) training are included. First, a pixel-level supervised CNN is trained on very few fully annotated images. Second, given a large number of images with only image-level labels available, a collaborative-supervised CNN is designed to jointly perform the pixel-level and image-level classification tasks, while the pixel-level labels are predicted by the fully-supervised network in the first stage. The collaborative-supervised network can remain the discriminative ability of the fully-supervised model learned with fully labeled images, and further enhance the performance by importing more weakly labeled data. Our experiments on two challenging datasets, i.e, PASCAL VOC 2007 and LabelMe LMO, demonstrate the satisfactory performance of our approach, nearly matching the results achieved when all training images have pixel-level labels.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"8f19bc8c48f0691971ec3fbc1cbe842439d7e1b9","venue_1":"ACM Multimedia","year":"2008","title":"Precise object cutout from images","authors":"Ming Liu, Shifeng Chen, Jianzhuang Liu","author_ids":"5367622, 2869725, 7137861","abstract":"In this paper we propose a novel approach to the problem of interactive foreground/background segmentation in images. With user provided strokes which indicate foreground and background seeds, we estimate two Gaussian mixture models, one for foreground and the other for background, and define two quantities to measure the initial probabilities of each pixel belonging to the foreground and the background respectively. An optimization function constructed based on the quantities and the boundary and coherent region information is proposed to solve the segmentation problem. By relaxing the hard binary segmentation to a soft labelling problem in the continuous domain, a closed form global optimal solution can be achieved, which directly results in the final binary segmentation output. Experimental results demonstrate the excellent performance of our algorithm.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"a5ad965821e4d922b0b41b078be69daf8e533e87","venue_1":"ACM Multimedia","year":"2016","title":"Objectness-aware Semantic Segmentation","authors":"Yuhang Wang, Jing Liu, Yong Li, Junjie Yan, Hanqing Lu","author_ids":"7708083, 5661757, 1689181, 1721677, 1694235","abstract":"Recent advances in semantic segmentation are driven by the success of fully convolutional neural network (FCN). However, the coarse label map from the network and the object discrimination ability for semantic segmentation weaken the performance of those FCN-based models. To address these issues, we propose an objectness-aware semantic segmentation framework (OA-Seg) by jointly learning an object proposal network (OPN) and a lightweight deconvolutional neural network (Light-DCNN). First, OPN is learned based on a fully convolutional architecture to simultaneously predict object bounding boxes and their objectness scores. Second, we design a Light-DCNN to provide a finer upsampling way than FCN. The Light-DCNN is constructed with convolutional layers in VGG-net and their mirrored deconvolutional structure, where all fully-connected layers are removed. And hierarchical classification layers are added to multi-scale deconvolutional features to introduce more contextual information for pixel-wise label prediction. Compared with previous works, our approach performs an obvious decrease on model size and convergence time. Thorough evaluations are performed on the PASCAL VOC 2012 benchmark, and our model yields impressive results on its validation data (70.3% mean IoU) and test data (74.1% mean IoU).","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"00ecab50c59fd77fbb9d740a0981b81778ce0b77","venue_1":"ACM Multimedia","year":"2016","title":"SocialFX: Studying a Crowdsourced Folksonomy of Audio Effects Terms","authors":"Taylor Zheng, Prem Seetharaman, Bryan Pardo","author_ids":"3493719, 2855322, 1744936","abstract":"We present the analysis of crowdsourced studies into how a population of Amazon Mechanical Turk Workers describe three commonly used audio effects: equalization, reverberation, and dynamic range compression. We find three categories of words used to describe audio: ones that are generally used across effects, ones that tend towards a single effect, and ones that are exclusive to a single effect. We present select examples from these categories. We visualize and present an analysis of the shared descriptor space between audio effects. Data on the strength of association between words and effects is made available online for a set of 4297 words drawn from 1233 unique users for three effects (equalization, reverberation, compression). This dataset is an important step towards implementing of an end-to-end language-based audio production system, in which a user describes a creative goal, as they would to a professional audio engineer, and the system picks which audio effect to apply, as well as the setting of the audio effect.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"065c4bc61375960aa76fb2dab66015fa7af5036f","venue_1":"ACM Multimedia","year":"2010","title":"3D object search through semantic component","authors":"Chunjing Xu, Zhengwu Zhang, Jianzhuang Liu, Xiaoou Tang","author_ids":"1691522, 2047671, 7137861, 1741901","abstract":"In this paper, we present a novel concept named semantic component for 3D object search which describes a key component that semantically defines a 3D object. In most cases, the semantic component is intra-category stable and therefore can be used to construct an efficient 3D object retrieval scheme. By segmenting an object into segments and learning the similar segments shared by all the objects in the same category, we can summarise what human uses for object recognition, from the analysis of which we develop a method to find the semantic component of an object. In our experiments, the proposed method is justified and the effectiveness of our algorithm is also demonstrated.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"70f851b752afee3eeb7166578e8b054b17425570","venue_1":"ACM Multimedia","year":"2013","title":"πLDA: document clustering with selective structural constraints","authors":"Siliang Tang, Hanqi Wang, Jian Shao, Fei Wu, Ming Chen, Yueting Zhuang","author_ids":"1774936, 7644453, 8187421, 1695826, 1711264, 1755711","abstract":"Segments, such as sentence boundaries in texts or annotated regions in images, can be considered as useful structural constraints (i.e., <i>priors</i>) for unsupervised topic modeling. However, some <i>segment units</i> (e.g., words in texts or visual words in images) inside a given segment may be irrelevant to the topic of this segment due to their characteristics. This paper proposes a model called &#960;LDA, which introduces a latent variable &#960; into LDA, a traditional topic model, to capture the characteristic of each segment unit. That is to say, the &#960;LDA model is conducted to determine whether a segment unit is assigned (or <i>selected</i>) to the topic embedded in its corresponding segment. Compared with other approaches that assume all the segment units in one segment to share a common topic, our proposed &#960;LDA has the <i>selective</i> ability to discover the discriminative segment units (e.g., informative words or visual words). Experimental results and interpretations of them are presented for demonstrating the promising performance of our method.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"1bb0e404bfa2324ca7cb47e2a1f8348bc95ff6d9","venue_1":"ACM Multimedia","year":"2014","title":"SynthAssist: an audio synthesizer programmed with vocal imitation","authors":"Mark Brozier Cartwright, Bryan Pardo","author_ids":"4388320, 1744936","abstract":"While programming an audio synthesizer can be difficult, if a user has a general idea of the sound they are trying to program, they may be able to imitate it with their voice. In this technical demonstration, we demonstrate SynthAssist, a system that allows the user to program an audio synthesizer using vocal imitation and interactive feedback. This system treats synthesizer programming as an audio information retrieval task. To account for the limitations of the human voice, it compares vocal imitations to synthesizer sounds by using both absolute and relative temporal shapes of relevant audio features, and it refines the query and feature weights using relevance feedback.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"0cd6c70ac57b796c12bd59229ea901a77ce8f066","venue_1":"ACM Multimedia","year":"2010","title":"Music recommendation by unified hypergraph: combining social media information and music content","authors":"Jiajun Bu, Shulong Tan, Chun Chen, Can Wang, Hao Wu, Lijun Zhang, Xiaofei He","author_ids":"8475311, 2054844, 5371645, 1804563, 1739602, 1707675, 3945955","abstract":"Acoustic-based music recommender systems have received increasing interest in recent years. Due to the semantic gap between low level acoustic features and high level music concepts, many researchers have explored collaborative filtering techniques in music recommender systems. Traditional collaborative filtering music recommendation methods only focus on user rating information. However, there are various kinds of social media information, including different types of objects and relations among these objects, in music social communities such as Last.fm and Pandora. This information is valuable for music recommendation. However, there are two challenges to exploit this rich social media information: (a) There are many different types of objects and relations in music social communities, which makes it difficult to develop a unified framework taking into account all objects and relations. (b) In these communities, some relations are much more sophisticated than pairwise relation, and thus cannot be simply modeled by a graph. In this paper, we propose a novel music recommendation algorithm by using both multiple kinds of social media information and music acoustic-based content. Instead of graph, we use hypergraph to model the various objects and relations, and consider music recommendation as a ranking problem on this hypergraph. While an edge of an ordinary graph connects only two objects, a hyperedge represents a set of objects. In this way, hypergraph can be naturally used to model high-order relations. Experiments on a data set collected from the music social community Last.fm have demonstrated the effectiveness of our proposed algorithm.","cites":"61","conferencePercentile":"97.53424658"},{"venue":"ACM Multimedia","id":"84e86a78d0b91a3c0f638947e3adb8826476ac54","venue_1":"ACM Multimedia","year":"2008","title":"Personal location based services on place-enhanced blog","authors":"Hideki Kaji, Masatoshi Arikawa","author_ids":"2656938, 1713381","abstract":"This paper proposes personal location based services on blog to make users recall past memories and things to do by displaying their personal records like diary, schedule and to-do list depending on the positions of the users. A lot of Internet users are recording their personal experiences and knowledge as texts and other digital media on the network. These users' created content is called \"User Generated Content\" (UGC). One of the reasons that users record their experiences and knowledge is not only informing other users about author's opinions but also retrieving them as needed. Although users can retrieve these records easily through their personal computer, it is difficult to retrieve them at the right place because most UGC systems do not have features to deal with some place attributes, on the other hand current commercial LBS do not support to deal with personal records. Our proposed tool provides users with an environment to store personal records with related place attributes, and to retrieve these personal records at the right place. There are two applications on this tool, a place enhanced blog and a place reminder on a mobile phone. A place enhanced blog provide users with blog interfaces for inputting place information. The place reminder is a browser for spatial data on the place enhanced blog. Users can generate place information by writing personal records on their blog. Furthermore, using the place reminder, other users can retrieve all users' permitted personal records on the spots. Also, the place reminder can serve as a simple navigation system using both GPS and non-GPS navigation methods","cites":"1","conferencePercentile":"23.62385321"},{"venue":"ACM Multimedia","id":"9104cd460448ad1e07d95f67ed3888ef218b7924","venue_1":"ACM Multimedia","year":"2012","title":"Virtual reference view generation for CBIR-based visual pose estimation","authors":"Robert Huitl, Georg Schroth, Sebastian Hilsenbeck, Florian Schweiger, Eckehard G. Steinbach","author_ids":"1746858, 1701294, 1983634, 2119331, 7252930","abstract":"Determining the pose of a mobile device based on visual information is a promising approach to solve the indoor localization problem. We present an approach that transforms localized images along a mapping trajectory into virtual viewpoints that cover a set of densely sampled camera positions and orientations in a confined environment. The viewpoints are represented by their respective bag-of-features vectors and image retrieval techniques are applied to determine the most likely pose of query images at very low computational complexity. As virtual image locations and orientations are decoupled from actual image locations, the system is able to work with sparse reference imagery and copes well with perspective distortion. Experiments confirm that pose retrieval performance is significantly improved.","cites":"7","conferencePercentile":"78.32278481"},{"venue":"ACM Multimedia","id":"9596da2438e2d82c1e4785bccd58abacd561dc41","venue_1":"ACM Multimedia","year":"1999","title":"Live paper: video augmentation to simulate interactive paper","authors":"Charles Robertson, John Robinson","author_ids":"8632757, 7527123","abstract":"We report progress in the development of an augmented space to simulate paper with embedded computational facilities. We use a video camera and a data projector to turn sheets of paper into I/O devices. To the user, the illusion of paper as computer is strong. Our system provides audio and video augmentation, including collaboration with remote users through videowriting (real-time extraction of writing on a page) and projected annotations.","cites":"5","conferencePercentile":"41.17647059"},{"venue":"ACM Multimedia","id":"b3266e31542e9b6e77c1fd438f7d34c836d9d07e","venue_1":"ACM Multimedia","year":"2010","title":"An interactive multimedia framework for digital heritage narratives","authors":"Neeharika Adabala, Naren Datha, Joseph Joy, Chinmay Kulkarni, Ajay Manchepalli, Aditya Sankar, Rebecca Walton","author_ids":"2442159, 2539233, 7014224, 4924730, 2098058, 3246173, 3185584","abstract":"The cultural heritage of a region is conveyed by both tangible physical artifacts and intangible aspects in the form of stories, dance styles, rituals, etc. Hitherto, the task of creating digital representations for each of these aspects has been addressed in isolation, i.e. using specific media most suited to the artifact such as video, audio, three-dimensional (3D) models, scanning, etc. The challenge of bringing together these separate elements to create a coherent story, however, has remained unaddressed until recently. In this paper we present a unified digital framework that enables the integration of disparate representations of heritage elements into a holistic entity. Our approach results in a compelling and engaging narration that affords a unified user experience. Our solution supports both active (user-controlled explorations) and passive (watching pre-orchestrated narrations) user interactions.\n We demonstrate the capabilities of our framework through a qualitative user study based on two rich interactive narratives built using our framework: (1) history and folklore surrounding a temple in South India, and (2) a historical account of an educational institution also in South India.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"c11fe728cdb6ad000ff188b9f3b8e7cd132fd776","venue_1":"ACM Multimedia","year":"2014","title":"Crowdsourcing a Reverberation Descriptor Map","authors":"Prem Seetharaman, Bryan Pardo","author_ids":"2855322, 1744936","abstract":"Audio production is central to every kind of media that involves sound, such as film, television, and music and involves transforming audio into a state ready for consumption by the public. One of the most commonly-used audio production tools is the reverberator. Current interfaces are often complex and hard-to-understand. We seek to simplify these interfaces by letting users communicate their audio production objective with descriptive language (e.g. \"Make the drums sound bigger.\"). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin warmer using a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions lead to the goal. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialReverb, a project to crowdsource a vocabulary of audio descriptors that can be mapped onto concrete actions using a parametric reverberator. We deployed SocialReverb, on Mechanical Turk, where 513 unique users described 256 instances of reverberation using 2861 unique words. We used this data to build a concept map showing which words are popular descriptors, which ones map consistently to specific reverberation types, and which ones are synonyms. This promises to enable future interfaces that let the user communicate their production needs using natural language.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"0ac8913fa19887044b26d79b1440de3b4a3639db","venue_1":"ACM Multimedia","year":"2009","title":"A method for rapid personalization of audio equalization parameters","authors":"Andrew T. Sabin, Bryan Pardo","author_ids":"2194931, 1744936","abstract":"Potential users of audio production software, such as audio equalizers, may be discouraged by the complexity of the interface. We describe a system that simplifies the interface by quickly mapping an individual's preferred sound manipulation onto parameters for audio equalization. This system learns mappings by presenting a sequence of equalizer settings to the user and correlating the gain in each frequency band with the user's preference rating. Learning typically converges in 25 user ratings (under two minutes). The system then creates a simple on-screen slider that lets the user manipulate the audio in terms of the descriptive term, without need to learn or use the parameters of an equalizer. Results are reported on the speed and effectiveness of the system for a set of 19 users and a set of five descriptive terms.","cites":"8","conferencePercentile":"60.12396694"},{"venue":"ACM Multimedia","id":"1788b1b52a056b4d3f0174ac4d0d781ab354f86d","venue_1":"ACM Multimedia","year":"2014","title":"Reverbalize: A Crowdsourced Reverberation Controller","authors":"Prem Seetharaman, Bryan Pardo","author_ids":"2855322, 1744936","abstract":"One of the most commonly-used audio production tools is the reverberator. Reverberators apply subtle or large echo effects to sound and are typically used in commercial audio recordings. Current reverberator interfaces are often complex and hard-to-understand. In this work, we describe Reverbalize, a novel and easy-to-use interface for a reverberator. Reverbalize uses crowdsourced data to create a 2-dimensional map of adjectives used to describe reverberation (e.g. \"underwater'). Adjacent words describe similar reverberation effects. Word size correlates with agreement for the definition of a word. To use Reverbalize, the user simply clicks on the descriptive adjective that best describes the desired effect. The tool modifies the sound accordingly. A text search box also lets the user type in the desired word.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"c9394b8e8ef0e5341a63c00f9d6fd55a4da56393","venue_1":"ACM Multimedia","year":"2012","title":"Interactive music video application for smartphones based on free-viewpoint video and audio rendering","authors":"Toshiharu Horiuchi, Hiroshi Sankoh, Tsuneo Kato, Sei Naito","author_ids":"2850375, 2087437, 3202572, 2261344","abstract":"This paper presents a novel interactive music video application for smartphones based on free-viewpoint video technology in conjunction with three-dimensional positional audio technology. A user can enjoy a music video from a moving viewpoint that the user can manipulate by the touch screen, with the positional audio through the headphone. A user can even manipulate the positions of the performers on the stage as well as the viewpoint. The application, consisting of our audio rendering engine for multiple AAC ADTS files and our video rendering engine for multiple H.264 ES files, runs on a smartphone in stand-alone mode. The application has been released as official content from a music label for Android and iOS.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"1a70454957c46644a4a055a0199d73583c1d10b0","venue_1":"ACM Multimedia","year":"2010","title":"TOP-SURF: a visual words toolkit","authors":"Bart Thomee, Erwin M. Bakker, Michael S. Lew","author_ids":"2463875, 2111311, 1731570","abstract":"TOP-SURF is an image descriptor that combines interest points with visual words, resulting in a high performance yet compact descriptor that is designed with a wide range of content-based image retrieval applications in mind. TOP-SURF offers the flexibility to vary descriptor size and supports very fast image matching. In addition to the source code for the visual word extraction and comparisons, we also provide a high level API and very large pre-computed codebooks targeting web image content for both research and teaching purposes.","cites":"32","conferencePercentile":"93.15068493"},{"venue":"ACM Multimedia","id":"9a80e4c04697a7702a46a1738c71f8e848105705","venue_1":"ACM Multimedia","year":"2011","title":"Ensemble approach based on conditional random field for multi-label image and video annotation","authors":"Xin-Shun Xu, Yuan Jiang, Liang Peng, Xiangyang Xue, Zhi-Hua Zhou","author_ids":"2800124, 5465260, 1908475, 5507458, 1692625","abstract":"Multi-label image/video annotation is a challenging task that allows to correlate more than one high-level semantic keyword with an image/video-clip. Previously, a single model is usually used for the annotation task, with relatively large variance in performance. The correlation among the annotation keywords should also be considered. In this paper, to reduce the performance variance and exploit the correlation between keywords, we propose the En-CRF (Ensemble based on Conditional Random Field) method. In this method, multiple models are first trained for each keyword, then the predictions of these models and the correlations between keywords are incorporated into a conditional random field. Experimental results on benchmark data set, including Corel5k and TRECVID 2005, show that the En-CRF method is superior or highly competitive to several state-of-the-art methods.","cites":"6","conferencePercentile":"71.86588921"},{"venue":"ACM Multimedia","id":"b83d0e4cbf02db4dac75b1bc3ce3c4ff909f0ae2","venue_1":"ACM Multimedia","year":"2012","title":"International workshop on socially-aware multimedia (SAM'12)","authors":"Pablo César, David A. Shamma, Doug Williams, Cees Snoek","author_ids":"1743507, 1760364, 1683394, 1789850","abstract":"Multimedia social communication is filtering into everyday use. Videoconferencing is appearing in the living room and beyond, television is becoming smart and social, and media sharing applications are transforming the way we converse and recall events. The confluence of computer-mediated interaction, social networking, and multimedia content are radically reshaping social communications, bringing new challenges and opportunities. This workshop provides an opportunity to explore socially-aware multimedia, in which the social dimension of mediated interactions between people are considered as important as the characteristics of the media content. Even though this social dimension is implicitly addressed in some current solutions, further research is needed to better understand what makes multimedia socially-aware. In other words, social interactivity needs to become a first class citizen of multimedia research.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"7d0f129ec01c05fbac75cb7c7bb0ab895620c582","venue_1":"ACM Multimedia","year":"2013","title":"2nd international workshop on socially-aware multimedia (SAM'13)","authors":"Pablo César, Matthew Cooper, David A. Shamma, Doug Williams","author_ids":"1743507, 4268667, 1760364, 1683394","abstract":"Multimedia social communication is becoming commonplace. Television is becoming smart and social; media sharing applications are transforming the way we converse and recall events and videoconferencing is a common application on our computers, phones, tablets and even televisions. The confluence of computer-mediated interaction, social networking, and multimedia content are radically reshaping social communications, bringing new challenges and opportunities. This workshop, in its second edition, provides an opportunity to explore socially-aware multimedia, in which the social dimension of mediated interactions between people are considered to be as important as the characteristics of the media content. Even though this social dimension is implicitly addressed in some current solutions, further research is needed to better understand what makes multimedia socially-aware.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"ceda42010ea494759f831919592adaf15f39e37f","venue_1":"ACM Multimedia","year":"2014","title":"3rd International Workshop on Socially-Aware Multimedia (SAM'14)","authors":"Pablo César, David A. Shamma, Matthew Cooper, Aisling Kelliher","author_ids":"1743507, 1760364, 4268667, 1820838","abstract":"Multimedia social communication is becoming commonplace. Television is becoming smart and social; media sharing applications are transforming the way we converse and recall events and videoconferencing is a common application on our computers, phones, tablets and even televisions. The confluence of computer-mediated interaction, social networking, and multimedia content are radically reshaping social communications, bringing new challenges and opportunities. This workshop, in its third edition, provides an opportunity to explore socially-aware multimedia, in which the social dimension of mediated interactions between people are considered to be as important as the characteristics of the media content. Even though this social dimension is implicitly addressed in some current solutions, further research is needed to better understand what makes multimedia socially-aware.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"dcaad60a242746b6e3b65a6544ebe3f5187573b4","venue_1":"ACM Multimedia","year":"2008","title":"A scheduling algorithm for time bounded delivery of packets on the internet","authors":"Ishan Vaishnavi","author_ids":"1796581","abstract":"This thesis aims to provide a better scheduling algorithm for Real-Time delivery of packets. A number of emerging applications such as VoIP, Tele-immersive environments, distributed media viewing and distributed gaming require real-time delivery of packets. Currently the scheduling algorithms used to decide the priority ordering over packets only consider the deadline as the parameter. This may not be optimal, since it ignores the network traffic over the routes that these packets may take. The idea of this thesis is to propose a method of calculating a probability measure of each packet meeting it's deadline at every intermediate node. This probability measure is based on the time left for deadline to expire, the number of nodes further in the packets route to reach it's destination and the traffic on this route. The algorithm assumes a certain level on time synchronisation over the network.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"92f98353c9918caf89ccfe0b40aa291a646b2582","venue_1":"ACM Multimedia","year":"2005","title":"Complementing your TV-viewing by web content automatically-transformed into TV-program-type content","authors":"Akiyo Nadamoto, Katsumi Tanaka","author_ids":"2229815, 1750132","abstract":"Despite much talk about the fusion of broadcasting and the Internet, no technology has been established for fusing web and TV program content. In this paper, we propose ways to transform web content into TV-program-type content as a first step towards the fusion of these media. Our transformation method is based on two criteria - the transmitted information and the dialogue among character agents. The method deals with both an audio component and a visual component. By combining these techniques, we can transform web content into various forms of TV-program-type content depending on the user's aims. We present three different prototype systems, u-Pav which reads out the entire text of web content and presents image animation, Web2TV which reads out the entire text of web content and presents character agent animation, and Web2Talkshow which presents keyword-based dialogue and character agent animation. These prototype systems enable users to watch web content in the same way, they watch a TV program.","cites":"10","conferencePercentile":"59.15841584"},{"venue":"ACM Multimedia","id":"140a95d869db40b1ecf9800acc61d92c7a3f2f41","venue_1":"ACM Multimedia","year":"2005","title":"Generation of views of TV content using TV viewers' perspectives expressed in live chats on the web","authors":"Hisashi Miyamori, Satoshi Nakamura, Katsumi Tanaka","author_ids":"1714279, 4807127, 1750132","abstract":"We propose a method of generating views of TV programs based on viewer's perspectives expressed in live chats on the Web. Important scenes in a program and responses by particular viewers can be extracted efficiently by statistically computing and/or recognizing live chat data obtained in sync with the broadcast content. We show that by using the computed results, views can be generated that indicate the momentum of reactions by viewers and scenes of interest to particular viewers whose preferences are similar to those of the viewer, etc. This is a new way of viewing TV content from various perspectives.","cites":"13","conferencePercentile":"65.59405941"},{"venue":"ACM Multimedia","id":"78afe2243ddea84bee1dc8090c62137ba923fd4c","venue_1":"ACM Multimedia","year":"2009","title":"GBED: group based event detection method for audio sensor networks","authors":"Qi Li, Huadong Ma","author_ids":"1682467, 1720462","abstract":"Audio event detection is one of the most important applications in the multimedia sensor networks. In this paper, we present a Group Based Event Detection method, called GBED, to detect the audio events in the audio sensor networks. In the proposed method, we train the basic audio events separately based on statistical learning, and combine them together by some prior knowledge in specific domains. By using GBED we can combine the statistical learning and human knowledge together and reduce the training complexity for complex real-world situations. We deploy this approach on an audio sensor network to evaluate its performance, and the experiment evaluations demonstrate that GBED can achieve satisfied results.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"f61ab58a90e754ebc2e229149b1d0b2f22a14506","venue_1":"ACM Multimedia","year":"2015","title":"A Distributed Theatre Experiment with Shakespeare","authors":"Doug Williams, Ian Kegel, Marian Florin Ursu, Pablo César, Jack Jansen, Erik Geelhoed, Andras Horti, Michael Frantzis, Bill Scott","author_ids":"1683394, 2883994, 1733725, 1743507, 1724349, 1735527, 1689024, 1682189, 1706463","abstract":"This paper reports on an experimental production of The Tempest that was developed in collaboration with Miracle Theatre Company realised as a distributed performance from two separate stages through a dynamically configured telepresence system. The production allowed an exploration of the way a range of technologies, including consumer grade broadband, cameras and projection technologies could affect the development and delivery of live theatre by regional touring company. The architecture of the communication platform used to deliver the performance is introduced as are two novel software tools that are used to describe and control the way the play should be captured and represented.\n The experimental production was thoroughly evaluated and the feedback from audience and theatre professionals is presented in some detail.\n A considered observation of the process and the way it differs from film, TV and theatre suggest that distributed theatre can be treated as a new genre of storytelling.","cites":"5","conferencePercentile":"90.55555556"},{"venue":"ACM Multimedia","id":"46eddf032bdc418c4347e0346dc7f324e7a93f3b","venue_1":"ACM Multimedia","year":"2001","title":"Speech-driven cartoon animation with emotions","authors":"Yan Li, Feng Yu, Ying-Qing Xu, Eric Chang, Harry Shum","author_ids":"1723002, 2375993, 1742571, 6586107, 1698102","abstract":"In this paper, we present a cartoon face animation system for multimedia HCI applications. We animate face cartoons not only from input speech, but also based on emotions derived from speech signal. Using a corpus of over 700 utterances from different speakers, we have trained SVMs (support vector machines) to recognize four categories of emotions: neutral, happiness, anger and sadness. Given each input speech phrase, we identify its emotion content as a mixture of all four emotions, rather than classifying it into a single emotion. Then, facial expressions are= generated from the recovered emotion for each phrase, by morphing different cartoon templates that correspond to various emotions. To ensure smooth transitions in the animation, we apply low-pass filtering to the recovered (and possibly jumpy) emotion sequence. Moreover, lip-syncing is applied to produce the lip movement from speech, by recovering a statistical audio-visual mapping. Experimental results demonstrate that cartoon animation sequences generated by our system are of good and convincing quality.","cites":"12","conferencePercentile":"62.75510204"},{"venue":"ACM Multimedia","id":"304da8bfc680e8ed8bd546b03b1f698c6a1ca9c7","venue_1":"ACM Multimedia","year":"2002","title":"PicToon: a personalized image-based cartoon system","authors":"Hong Chen, Nanning Zheng, Lin Liang, Yan Li, Ying-Qing Xu, Harry Shum","author_ids":"5596871, 1747704, 1680293, 1723002, 1742571, 1698102","abstract":"In this paper, we present <b>PicToon</b>, a cartoon system which can generate a personalized car<b>toon</b> face from an input <b>Pic</b>ture. PicToon is easy to use and requires little user interaction. Our system consists of three major components: an image-based Cartoon Generator, an interactive Cartoon Editor for exaggeration, and a speech-driven Cartoon Animator. First, to capture an artistic style, the cartoon generation is decoupled into two processes: sketch generation and stroke rendering. An example-based approach is taken to automatically generate sketch lines which depict the facial structure. An inhomogeneous non-parametric sampling plus a flexible facial template is employed to extract the vector-based facial sketch. Various styles of strokes can then be applied. Second, with the pre-designed templates in Cartoon Editor, the user can easily make the cartoon exaggerated or more expressive. Third, a real-time lip-syncing algorithm is also developed that recovers a statistical audio-visual mapping between the character's voice and the corresponding lip configuration. Experimental results demonstrate the effectiveness of our system.","cites":"20","conferencePercentile":"74.78632479"},{"venue":"ACM Multimedia","id":"5e460ddf1857fbbb4cb35275a3f91db7bbe8abc0","venue_1":"ACM Multimedia","year":"2015","title":"Analysing Audience Response to Performing Events: A Web Platform for Interactive Exploration of Physiological Sensor Data","authors":"Thomas Röggla, Chen Wang, Pablo César","author_ids":"1976550, 1710899, 1743507","abstract":"This paper presents a web interface for the exploration of audience response to a performing arts event. The platform temporally synchronises data obtained from physiological sensors with video recordings. More concretely, it is geared towards people from the creative industry, e.g. theatre directors, who want to gain deeper insights into how audiences perceive their performances. The platform takes the raw data from sensors and corresponding video recordings and visualises both synchronised in a more digestible manner. This document presents the major features of the application and explains the reasoning behind its various visualisations and interactive capabilities and how it can benefit performing artists.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"3e3c6c5008bb7ab8fd590759c1822efff80b8719","venue_1":"ACM Multimedia","year":"2000","title":"Enabling next generation streaming media networks (keynote session)","authors":"Eric A. Brewer","author_ids":"1759010","abstract":"The proliferation of broadband access, enhancements in production and encoding technologies and increasing consumer demand for rich, high bandwidth content are fueling the need for reliable and cost-effective streaming media delivery networks that ensure a high quality end-user experience. These networks provide a scalable infrastructure for both live and on-demand streaming media. In his keynote, Dr. Brewer will address specific IP infrastructure needs for streaming media delivery; the importance of caching, multicast and bandwidth provisioning; enhancing media delivery with network edge services; and putting it all together to create a high-performance, reliable streaming media delivery network.","cites":"0","conferencePercentile":"7.065217391"},{"venue":"ACM Multimedia","id":"1fdf59b18b084043dafec0e7164b0d612dfd1545","venue_1":"ACM Multimedia","year":"1998","title":"A Proxy Architecture for Reliable Multicast in Heterogeneous Environments","authors":"Yatin Chawathe, Steve A. Fink, Steven McCanne, Eric A. Brewer","author_ids":"2729365, 3247651, 2887819, 1759010","abstract":"IP Multicast has proven to be an eeective communication primitive for best eeort, large-scale, multi-point audio/video confer-encing applications. While the best-eeort transport of real-time digital audio/video is a relatively straightforward and well understood problem, many other applications like multicast-based shared whiteboards and shared text editors are more challenging to design because their underlying media require reliable transport , i.e., a \\reliable multicast\" protocol. The design of scalable end-to-end reliable multicast protocols has unfortunately proven to be an especially hard problem, exacerbated by the enormous degree of network and system heterogeneity present in the In-ternet. In this paper, we propose to tackle the heterogeneity problem with a hybrid model for reliable multicast that relies in part on end-to-end loss recovery mechanisms and in part on intelligent and application-aware adaptation carried out within the network. In our framework, application-aware agents | or proxies | use detailed knowledge of application semantics to hide the eeects of heterogeneity from the rest of the system. We present a general architecture for proxy-based reliable multicast called the Reliable Multicast proXy (RMX) model and describe a prototype implementation of an RMX for a shared whiteboard application for hand-held PDAs. 1 Introduction The Internet multicast backbone, or MBone 14, 13], forms the conduit for the \\IP multicast forwarding service,\" an extension of the traditional, best-eeort Internet datagram model for eecient group-oriented communication. In IP Multicast, each source's data ow is delivered eeciently to all interested receivers according to a multicast routing tree. For large-scale group communication, the band-width savings aaorded by multicast are enormous, and consequently , a large and growing number of multimedia con-ferencing tools 24, 31, 21, 29, 20] have been developed that exploit multicast and the MBone. Though multicast applications reap enormous performance beneets from the underlying multicast service, they are fundamentally challenged by the heterogeneity that is inher","cites":"48","conferencePercentile":"78.84615385"},{"venue":"ACM Multimedia","id":"da9fd18f5920b9361a5c2e77c4005fba4f14072d","venue_1":"ACM Multimedia","year":"2008","title":"Intuitive page-turning interface of e-books on flexible e-paper based on user studies","authors":"Taichi Tajika, Tomoko Yonezawa, Noriaki Mitsunaga","author_ids":"1693356, 1696632, 1683341","abstract":"In this paper we propose an intuitive page-turning and browsing interface of e-books on a flexible e-paper based on user studies. Our user studies showed various types of page-turning actions such as flipping, grasping, and sliding by different situations or users. We categorized these actions into three categories: turn, flip through, and leaf through the page(s). Based on this categorized model, we have developed a conceptual design and prototype of an interface for an e-book reader, which enables intuitive page-turning interactions using a simple architecture in both hardware and software design. The prototype has a flexible plastic sheet with bend sensors, which is attached to a small LCD monitor to physically unite the visual display with a tangible control interface based on the natural page-turning actions as used in reading a real book. The prototype handles all three page-turning actions observed in the user studies by interpreting the bend degree of the sheet.","cites":"8","conferencePercentile":"61.69724771"},{"venue":"ACM Multimedia","id":"df015e2789fdaae1e8564f5cbf8a7c16854a782f","venue_1":"ACM Multimedia","year":"2005","title":"Perceptual media compression for multiple viewers with feedback delay","authors":"Oleg V. Komogortsev, Javed I. Khan","author_ids":"1778350, 1724118","abstract":"Human eyes have limited perception capabilities; for example, only 2 degrees of our 140 degree vision field provide the highest quality of perception. Due to this fact the idea of perceptual focus emerged to allow a visual content to be changed in a way that only part of the visual field where a human gaze is directed is encoded with a high quality. The image quality in the periphery can be reduced without a viewer noticing it. This compression approach allows a significant decrease in the number of bits required for image encoding, and in the case of the 3D image rendering, it decreases the computational burden. A number of previous researchers have investigated the topic of perceptual focus but only for a single viewer. In our research we investigate a dynamically changing multi-viewer scenario. In this type of scenario a number of people are watching the same visual content at the same time. Each person has his/her own perceptual focus area which changes over time. The visual content is sent through a network with a fixed delay/lag which provides an additional challenge to the whole scheme. The goal of our work was to investigate and develop a method of multi-viewer perceptual focus zones adaptation for real-time media perceptual compression and transmission. In our research we also look into the impact that such a method can have on transmission bandwidth and computational burden reduction.","cites":"1","conferencePercentile":"14.10891089"},{"venue":"ACM Multimedia","id":"3aa0c5c5cb5faf3c9f3c4694bf5f486d1ea0e351","venue_1":"ACM Multimedia","year":"2008","title":"Easytoon: an easy and quick tool to personalize a cartoon storyboard using family photo album","authors":"Shifeng Chen, Yuandong Tian, Fang Wen, Ying-Qing Xu, Xiaoou Tang","author_ids":"2869725, 3213303, 1716835, 1742571, 1741901","abstract":"A family photo album based cartoon personalization system, EasyToon, is proposed in this paper. Using state of the art computer vision and graphics technologies and effective UI design, the interactive tool can quickly generate a personalized cartoon storyboard, which naturally blends a real face chosen from the family photo album into a cartoon picture. The personalized cartoon image is easily and quickly obtained in two main steps. First, the best face candidate is selected from the album interactively. Then a personalized cartoon image is automatically synthesized by blending the selected face into the interesting cartoon image. Experiments show that most users express great interest in our system. Without any art background, they can make a personalized cartoon of high quality using the EasyToon within minutes.","cites":"5","conferencePercentile":"48.85321101"},{"venue":"ACM Multimedia","id":"0b18d0b7fe7cd97af54e45fb59f23a06ca3752b3","venue_1":"ACM Multimedia","year":"2004","title":"Predictive perceptual compression for real time video communication","authors":"Oleg V. Komogortsev, Javed I. Khan","author_ids":"1778350, 1724118","abstract":"Approximately 2 degrees in our 140 degree vision span has sharp vision. Many researchers have been fascinated by the idea of eye-tracking integrated perceptual compression of an image or video, yet any practical system has yet to emerge. The unique challenge presented by real time perceptual video streaming is how to handle the fast nature of the human eye and provide its integration with computationally intensive video transcoding scheme. The delay introduced by video transmission in the network presents a difficulty. This delay creates a problem when we try to use information about eye movements for perceptual encoding. In this paper we discuss a new approach to the eye-tracker based video compression. Rather than relying on the point of gaze, this novel scheme tracks a vicinity of interest and offers a prediction mechanism for eye movements. The described system compensates the interim eye movements between the sampling and actual coding. The proposed scheme can be applied to a large variety of today's video compression standards. We have developed an eye gaze-aware MPEG-2 transcoder that can perceptually re-encode a live video stream in real time. The experiments we have conducted illustrate the substantial impact this integrated prediction method has on perceptual video compression and bit-rate reduction.","cites":"13","conferencePercentile":"68.62745098"},{"venue":"ACM Multimedia","id":"3b53282f5e2d6d321fb0dd0cf0c2500a4553f4ca","venue_1":"ACM Multimedia","year":"2002","title":"Distributed meetings: a meeting capture and broadcasting system","authors":"Ross Cutler, Yong Rui, Anoop Gupta, Jonathan J. Cadiz, Ivan Tashev, Li-wei He, Alex Colburn, Zhengyou Zhang, Zicheng Liu, Steve Silverberg","author_ids":"2792294, 1728806, 1725517, 2745040, 2417067, 1689581, 5945803, 1732465, 1691128, 3332644","abstract":"The common meeting is an integral part of everyday life for most workgroups. However, due to travel, time, or other constraints, people are often not able to attend all the meetings they need to. Teleconferencing and recording of meetings can address this problem. In this paper we describe a system that provides these features, as well as a user study evaluation of the system. The system uses a variety of capture devices (a novel 360&#176; camera, a whiteboard camera, an overview camera, and a microphone array) to provide a rich experience for people who want to participate in a meeting from a distance. The system is also combined with speaker clustering, spatial indexing, and time compression to provide a rich experience for people who miss a meeting and want to watch it afterward.","cites":"163","conferencePercentile":"97.43589744"},{"venue":"ACM Multimedia","id":"03b870d44ba3c0e85148ba0c94f09c548f4ebc83","venue_1":"ACM Multimedia","year":"2001","title":"Resource adaptive netcentric systems: a case study with SONET - a self-organizing network embedded transcoder","authors":"Javed I. Khan, Seung Su Yang, Qiong Gu, Darsan Patel, Patrick Mail, Oleg V. Komogortsev, Wansik Oh, Zhong Guo","author_ids":"1724118, 1744307, 2924259, 5256106, 2844252, 1778350, 1890956, 1864750","abstract":"In this paper we discuss architecture for network aware adaptive systems for next generation networks. We present in the context of a novel cognizant video transcoding system, which is capable of negotiating local network state based rate and let the video propagate over extreme network with highly asymmetric link and node capacities utilizing knowlege about the network, content protocol and the content itself.","cites":"2","conferencePercentile":"26.02040816"},{"venue":"ACM Multimedia","id":"d6fac39d9f3d16252a7d3be9fb2eed73992ec83f","venue_1":"ACM Multimedia","year":"2004","title":"Where are the brave new mobile multimedia applications?","authors":"Susanne Boll, Sudhir R. Ahuja, Dirk Friebel, Bradley Horowitz, Neerja Raman, Sai Shankar Nandagopalan","author_ids":"1714281, 2899244, 2677428, 2460543, 2072553, 3026466","abstract":"With the availability of new and powerful mobile devices evolving network infrastructures in point-to-point networks towards 3G networks, wireless networks and also broadcasting of digital TV to set top boxes as well as the availability of a tremendous amount of media such as pictures from camera phones, and digital music, multimedia started its triumphal procession to inform, entertain, and educate users everywhere. On the road to a mobile and multimedia society, substantial effort is needed to bring easy to use, easy to access and helpful applications to the everyday user. Means for easy and imaginative creation of multimedia content, efficient management of multimedia content, transmission and streaming of stored and life content, sharing and exchanging multimedia content, multimedia support for collaboration and virtual meetings, and multimodal interaction support for multimedia systems are some of the issues that need to be addressed here. The industry panel invites leading companies in the field to bring forward the real challenges and driving applications in the field of next generation's mobile and home multimedia applications and vividly discuss the panelists' different views on the following questions: • What are the brave new applications and services for the next decade? • How to make these applications and services useful and commercially successful? • What are the key technological challenges of the brave new multimedia applications? • Will mobile and home multimedia be the driving force for next decade's technology developments? • Is research paying sufficient attention to the right questions and technology needed to develop and run the new and brave multimedia applications? • Where are the technological bottlenecks and obstacles that could thwart the new applications to come? With the industrial panel we would like to stimulate discussions on challenges and future research trends in mobile multimedia.","cites":"0","conferencePercentile":"7.107843137"},{"venue":"ACM Multimedia","id":"2a6f25c631198a86dc87201bb138e165e0182933","venue_1":"ACM Multimedia","year":"2001","title":"Building an intelligent camera management system","authors":"Yong Rui, Li-wei He, Anoop Gupta, Qiong Liu","author_ids":"1728806, 1689581, 1725517, 1794500","abstract":"Given rapid improvements in storage devices, network infrastructure and streaming-media technologies, a large number of corporations and universities are recording lectures and making them available online for anytime, anywhere access. However, producing high-quality lecture videos is still labor intensive and expensive. Fortunately, recent technology advances are making it feasible to build automated camera management systems to capture lectures. In this paper we report our design of such a system, including system configuration, audio-visual tracking techniques, software architecture, and user study. Motivated by different roles in a professional video production team, we have developed a multi-cinematographer single-director camera management system. The system performs lecturer tracking, audience tracking, and video editing all fully automatically, and offers quality close to that of human-operated systems.","cites":"41","conferencePercentile":"90.81632653"},{"venue":"ACM Multimedia","id":"6183e5077b071c3723ce28a733f1657442846913","venue_1":"ACM Multimedia","year":"2009","title":"Mobile media search: has media search finally found its perfect platform? part II","authors":"Berna Erol, Jiebo Luo, Shih-Fu Chang, Minoru Etoh, Hsiao-Wuen Hon, Qian Lin, Vidya Setlur","author_ids":"1685739, 1717319, 1735547, 8268643, 2089692, 5683660, 1689165","abstract":"Recently, many exciting media search applications have been introduced to take advantage of smart phones' audiovisual capture capabilities and their being always on and connected. These applications address a real pain point for most mobile users and allow them to search with minimal text entry, if any. Is the mobile platform an ideal fit for media search? Are audio and visual signal processing technologies sufficiently accurate to support most mobile search applications? What are the killer applications of mobile media search? Earlier in 2009 at ICASSP, a panel on this topic stirred up great interest and enthusiasm while leaving many questions untouched due to the limited time.\n In this panel, we continue and expand the discussions on the challenges and potentials of mobile media search, with a different mix in the panel.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"c03a574c2e3af7d312c8392fdac4fb3b0a226fb0","venue_1":"ACM Multimedia","year":"1997","title":"Scheduling Video Programs in Near Video-on-Demand Systems","authors":"Emmanuel L. Abram-Profeta, Kang G. Shin","author_ids":"2371663, 1730051","abstract":"This paper presents an analytical (in contrast to commonly-used simulations) approach to program scheduling in near video-on-demand (NVoD) systems. NVoD servers batch cus-tomers' requests by sourcing the same mr$erial at certain intervals called phase offsets. The proposed approach to analytical modeling integrates both customers' and service-provider's views to account for the tradeoff between system throughput and customers' partial patience. We first determine the optimal scheduling of movies of different populari-ties for maximum throughput and the lowest average phase offset. Next, we deal with quasi video-on-demand (QVoD) systems, in which programs are scheduled based on a threshold on the number of pending requests. The throughput is found to be usually greater in QVoD than in NVoD, except for the extreme case of nonstationary request arrivals. This observation is then used to improve throughput without compromising customers' QoS in terms of average phase offset and the corresponding dispersion. 1 Introduction The need to batch requests for the same movie title together in a video-on-demand (VoD) system has long been recognized for scalability and immediate deployment [lo]. Reduction of per-customer system cost and improvement of system scalability can both be achieved by delaying the VoD server's response to customers' requests made during the same batching interval and hence enabling the server to multicast the requested video. In this paper we investigate a batching strategy which sources the same material at equally-spaced intervals, called phase ofiets. This kind of VoD service, in which subscribers who order a particular movie to start within a specific time window are grouped together, is termed \" Near-VoD \" (NVOD) [2, 111. As mentioned in [2], the main advantage of NVoD systems over other batching policies is that, by keeping the batching interval nearly constant per movie title, it is possible to provide customers with limited and scalable VCR capaMity. It is usually recognized that full support for continuous interactive functions in a multicast VoD system can only be achieved by dedicating a channel per customer, thereby seriously limiting system scalability. In NVoD systems , on the other hand, limited continuity in VCR actions can be provided by caching a small amount of video data (e.g., 5 minutes' worth of video) in a buffer located close to the client, for instance, in the customers' premise equipment (CPE). This buffer can then be accessed without removing the customer from the multicast group. Moreover, staggered phase offsets support for discontinuous VCR …","cites":"23","conferencePercentile":"58.33333333"},{"venue":"ACM Multimedia","id":"08e8afd846111ebf7111e01b4f539788b7bbecb5","venue_1":"ACM Multimedia","year":"2008","title":"Interactive content presentation based on expressed emotion and physiological feedback","authors":"Tien-Lin Wu, Hsuan-Kai Wang, Chien-Chang Ho, Yuan-Pin Lin, Ting-Ting Hu, Ming-Fang Weng, Li-Wei Chan, Changhua Yang, Yi-Hsuan Yang, Yi-Ping Hung, Yung-Yu Chuang, Hsin-Hsi Chen, Homer H. Chen, Jyh-Horng Chen, Shyh-Kang Jeng","author_ids":"3351833, 2319237, 3024007, 2127206, 1807867, 2854960, 1682665, 1782222, 1784893, 7312257, 3032320, 1748891, 1769857, 2056203, 1798258","abstract":"In this technical demonstration, we showcase an interactive content presentation (ICP) system that integrates media-expressed-emotion-based composition, user-perceived preference feedback, and interactive digital art creation. ICP harmonizes the browsing of multimedia contents by presenting them in the form of music videos (photos, blog articles with accompanied music) based on their expressed emotion similarity. ICP facilitates content browsing by automatically and dynamically selecting the media to be played next in real time, responding to user's preference feedback measured from physiological signals. In addition, ICP enhances the enjoyments of content browsing by incorporating interactive digital art creation. ICP achieves these goals by properly integrating recent researches on media-expressed emotion classification,cross-media composition, and physiological signal processing.","cites":"6","conferencePercentile":"53.89908257"},{"venue":"ACM Multimedia","id":"29616db9103f18e595726ed7d4629fc8de9e790d","venue_1":"ACM Multimedia","year":"2004","title":"Supporting continuous consistency in multiplayer online games","authors":"Frederick W. B. Li, Lewis W. F. Li, Rynson W. H. Lau","author_ids":"7279850, 2624547, 1726262","abstract":"Multiplayer online games have become very popular in recent years. However, they generally suffer from network latency problem. If a player changes its states, it will take some time before the changes are reflected to other concurrent players. This significantly affects the interactivity of the game. Sometimes, it may even cause disputes among the players. In this paper, we present a continuous consistency control mechanism to support collaborative game applications. Specifically, we propose a relaxed consistency control model for continuous events. Based on this model, we have developed a method to provide a global-wise continuous synchronization on the states of dynamic game objects presented among concurrent game players. We show the performance of the proposed method through some experiments.","cites":"28","conferencePercentile":"82.10784314"},{"venue":"ACM Multimedia","id":"4738ba7506b14fbddd3394f6a912d539ba2be4d7","venue_1":"ACM Multimedia","year":"2002","title":"Adaptive partitioning for multi-server distributed virtual environments","authors":"Rynson W. H. Lau, Beatrice Ng, Antonio Si, Frederick W. B. Li","author_ids":"1726262, 1955144, 1715919, 7279850","abstract":"A distributed virtual environment (DVE) allows users at different geographical locations to share information and interact within a common virtual environment (VE) via a local network or through the Internet. However, when the number of users exploring the VE increases, the server will quickly become the bottleneck. To enable good performance, we are currently developing a multi-server DVE prototype. In this paper, we describe an adaptive data partitioning technique to dynamically partition the whole VE into regions. All objects within each region will be managed by a single server. As the loading of the servers changes, we show how it can be redistributed while minimizing the communication cost. Our initial results show that the proposed adaptive partitioning technique significantly improves the performance of the overall system.","cites":"3","conferencePercentile":"25.64102564"},{"venue":"ACM Multimedia","id":"2fb633194cbda55451d885a41ccbc2fe44752537","venue_1":"ACM Multimedia","year":"2010","title":"Visual search applications for connecting published works to digital material","authors":"Jamey Graham, Jorge Moraleda, Jonathan J. Hull, Timothée Bailloeul, Xu Liu, Andrea Mariotta","author_ids":"1975815, 2176024, 1694191, 3355773, 1785951, 3310261","abstract":"Visual search connects physical (<i>offline</i>) objects with (<i>online</i>) digital media. Using objects from the environment, like newspapers, magazines, books and posters, we can retrieve supplemental information from the online world. In this demonstration, we show a framework for delivering visual search services to users of mobile devices. We show how users can point a mobile device at any location in a document, magazine or book to view related, online material on the device. We describe client applications now being deployed for the iPhone and the server architecture used for recognition of scanned images.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"caf4311bc8368894c7f605aa6b521fe0e92fed30","venue_1":"ACM Multimedia","year":"2008","title":"HOTPAPER demonstration: multimedia interaction with paper using mobile phones","authors":"Berna Erol, Jamey Graham, Emilio R. Antúnez, Jonathan J. Hull","author_ids":"1685739, 1975815, 2483141, 1694191","abstract":"HotPaper enables users to electronically interact with paper documents by pointing their camera phones at a document. Electronic interaction is in the form of reading and/or writing images, audio and video clips, urls, notes, etc to the paper. HotPaper does not require any machine readable code, such as a barcode or watermark, to be present on the document. It utilizes our Brick Wall Coding document recognition algorithm [1] that uses a small document patch image to retrieve electronic versions of documents without performing OCR. The HotPaper demonstration runs on a Treo 700w 312 MHz mobile phone and performs document recognition at 4 frames per second using 176x144 video input.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"8a5420526771bcad390172668a071829df93f8ce","venue_1":"ACM Multimedia","year":"1999","title":"Video Manga: generating semantically meaningful video summaries","authors":"Shingo Uchihashi, Jonathan Foote, Andreas Girgensohn, John S. Boreczky","author_ids":"1713254, 1797460, 2195286, 2719487","abstract":"This paper presents methods for automatically creating pictorial video summaries that resemble comic books. The relative importance of video segments is computed from their length and novelty. Image and audio analysis is used to automatically detect and emphasize meaningful events. Based on this importance measure, we choose relevant keyframes. Selected keyframes are sized by importance, and then efficiently packed into a pictorial summary. We present a quantitative measure of how well a summary captures the salient events in a video, and show how it can be used to improve our summaries. The result is a compact and visually pleasing summary that captures semantically important events, and is suitable for printing or Web access. Such a summary can be further enhanced by including text captions derived from OCR or other methods. We describe how the automatically generated summaries are used to simplify access to a large collection of videos.","cites":"176","conferencePercentile":"98.31932773"},{"venue":"ACM Multimedia","id":"5bf8e797fb75072c072cc721ed415320daaa8978","venue_1":"ACM Multimedia","year":"2000","title":"A generic late-join service for distributed interactive media","authors":"Jürgen Vogel, Martin Mauve, Werner Geyer, Volker Hilt, Christoph Kuhmünch","author_ids":"1688261, 1751862, 1750692, 2809994, 1784075","abstract":"In this paper we present a generic late-join service for distributed interactive media, i.e, networked media which involve user interactions. Examples for distributed interactive media are shared whiteboards, networked computer games and distributed virtual environments. The generic late-join service allows a latecomer to join an ongoing session. This requires that the shared state of the medium is transmitted from the old participants of the session to the latecomer in an efficient and scalable way. In order to be generic and useful for a broad range of distributed interactive media, we have implemented the late-join service based on the Real Time Application Level Protocol for Distributed Interactive Media (RTP/I). All applications which employ this protocol can also use the generic late-join service. Furthermore the late-join service can be adapted to the specific needs of a given application by specifying policies for the late-join process. Applications which do use a different application level protocol than RTP/I may still use the concepts presented in this work. However, they will not be able to profit from our RTP/I based implementation.","cites":"17","conferencePercentile":"75"},{"venue":"ACM Multimedia","id":"39852f098581f881c15e68892e94b7c72421099d","venue_1":"ACM Multimedia","year":"2007","title":"A modern day video flip-book: creating a printable representation from time-based media","authors":"Berna Erol, Jamey Graham, Jonathan J. Hull, Peter E. Hart","author_ids":"1685739, 1975815, 1694191, 3108177","abstract":"In this paper, we describe a method for storing an entire video, animation sequence, or any other media type, on paper. The method is based on printing a key frame from a video on paper along with a barcode that encodes the motion information and other auxiliary information in MPEG-4 format. Unlike other video barcode systems in the prior art, a barcode in our system does not contain a link to the video clip; instead it contains motion information. A client device applies the motion information to an image of the video key frame to obtain full motion video. Therefore, the paper document is a self contained representation of a video clip and access to a server is not required. We modified an MPEG-4 [1] encoder and decoder to implement the video flip-book encoder and decoder. Experiments show that it is possible to encode several seconds of video on paper using our method. This is sufficient to create small animations for some printed materials such as video greeting cards and children's books.","cites":"1","conferencePercentile":"19.27083333"},{"venue":"ACM Multimedia","id":"16ddb47eb263d3678161148ff710979f30e3e7ac","venue_1":"ACM Multimedia","year":"2007","title":"Audio-visual multi-person tracking and identification for smart environments","authors":"Keni Bernardin, Rainer Stiefelhagen","author_ids":"1701229, 1742325","abstract":"This paper presents a novel system for the automatic and unobtrusive tracking and identification of multiple persons in an indoor environment. Information from several fixed cameras is fused in a particle filter framework to simultaneously track multiple occupants. A set of steerable fuzzy-controlled pan-tilt-zoom cameras serves to smoothly track persons of interest and opportunistically capture facial close-ups for face identification. In parallel, speech segmentation, sound source localization and speaker identification are performed using several far-field microphones and arrays. The information coming asynchronously and sporadically from several sources, such as track updates and spatio-temporally localized visual and acoustic identification cues, is fused at higher level to gradually refine the global scene model and increase the system's confidence in the set of recognized identities. The system has been trained on a small set of users' faces and/or voices and showed good performance in natural meeting scenarios at quickly acquiring their identities and complementing the ID information missing in single modalities.","cites":"24","conferencePercentile":"84.375"},{"venue":"ACM Multimedia","id":"5df535ac054ddfa25a710598d2fc2ce6ff2bb0c1","venue_1":"ACM Multimedia","year":"2001","title":"Exploring benefits of non-linear time compression","authors":"Li-wei He, Anoop Gupta","author_ids":"1689581, 1725517","abstract":"In comparison to text, audio-video content is much more challenging to browse. <i>Time-compression</i> has been suggested as a key technology that can support browsing-time compression speeds up the playback of audio-video content without causing the pitch to change. Simple forms of time-compression are starting to appear in commercial streaming-media products from Microsoft and Real Networks.In this paper we explore the potential benefits of more recent and advanced types of time compression, called <i>non-linear time compression</i>. The most advanced of these algorithms exploit fine-grain structure of human speech (e.g., phonemes) to differentially speedup segments of speech, so that the overall speedup can be higher. In this paper we explore what are the actual gains achieved by end-users from these advanced algorithms. Our results indicate that the gains are actually quite small in common cases and come with significant system complexity and some audio/video synchronization issues.","cites":"23","conferencePercentile":"82.65306122"},{"venue":"ACM Multimedia","id":"0044119524c08164650250163120bcb89adf6345","venue_1":"ACM Multimedia","year":"2008","title":"A context-aware virtual secretary in a smart office environment","authors":"Maria Danninger, Rainer Stiefelhagen","author_ids":"2138043, 1742325","abstract":"A lot of the communication at the workplace - via the phone as well as face-to-face - occurs in inappropriate contexts, disturbing meetings and conversations, invading personal and corporate privacy, and more broadly breaking social norms. This is because both, callers and visitors in front of closed office doors, face the same problem: they can only guess the other person's current availability for a conversation.\n We present a context-aware Virtual Secretary designed to facilitate more socially appropriate communication at the workplace. This service aims towards understanding a person's activity in smart offices, and passes on important contextual information to callers and visitors in order to facilitate more informed human decisions about how and when to initiate contact.\n We have deployed this Virtual Secretary in the office of a senior researcher, mediating all his actual phone calls and in-person meetings for several weeks. With the Virtual Secretary active, the number of inappropriate workplace interruptions could be significantly reduced.","cites":"11","conferencePercentile":"70.18348624"},{"venue":"ACM Multimedia","id":"a1cbcc7d474a93e631b62b4f7ef0a92311629d18","venue_1":"ACM Multimedia","year":"2010","title":"Interactive person-retrieval in TV series and distributed surveillance video","authors":"Martin Bäuml, Mika Fischer, Keni Bernardin, Hazim Kemal Ekenel, Rainer Stiefelhagen","author_ids":"2642997, 2284204, 1701229, 3025777, 1742325","abstract":"Tracking and identifying persons in videos are important building blocks in many applications. For browsing of multimedia data or interactive investigation of surveillance footage it is not even necessary to uniquely identify a person. Rather it often suffices to find occurrences of a person indicated by the user with an exemplary image sequence. We present two systems in which the search for a specific person can be initiated by a sample image sequence and then be further refined by interactive feedback by the operator. In the first system, episodes of TV series have been processed offline and can be searched for occurrences of the different characters. The second system tracks people online in multiple cameras and makes the sequences immediately searchable from a central station","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"d35d51ccf075940c22ad8ba827b2c00e2a46ed24","venue_1":"ACM Multimedia","year":"2012","title":"Improving dense image correspondence estimation with interactive user guidance","authors":"Kai Ruhl, Benjamin Hell, Felix Klose, Christian Lipski, Sören Petersen, Marcus A. Magnor","author_ids":"2283927, 2765149, 2547248, 1775396, 2726147, 1686739","abstract":"High quality dense image correspondence estimation between two images is an essential pre-requisite for view interpolation in visual media production. Due to the ill-posed nature of the problem, automated estimation approaches are prone to erroneous correspondences and subsequent quality degradation, e.g. in the presence of ambiguous movements that require human scene understanding to resolve. Where visually convincing results are essential, artifacts resulting from estimation errors must be repaired by hand with image editing tools. In this paper, we propose a new workflow alternative by fixing the correspondences instead of fixing the interpolated images. We combine realtime interactive correspondence display, multi-level user guidance and algorithmic subpixel precision to counteract failure cases of automated estimation algorithms. Our results show that already few interactions improve the visual quality considerably.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"1ffb27aac80b703ed8f7188984db7e6b1653b8be","venue_1":"ACM Multimedia","year":"2003","title":"The video paper multimedia playback system","authors":"Jamey Graham, Berna Erol, Jonathan J. Hull, Dar-Shyang Lee","author_ids":"1975815, 1685739, 1694191, 1711411","abstract":"Video Paper is a prototype system for multimedia browsing, analysis, and replay. Key frames extracted from a video recording are printed on paper together with bar codes that allow for random access and replay. A transcript for the audio track can also be shown so that users can read what was said, thus making the document a stand-alone representation for the contents of the multimedia recording. The Video Paper system has been used for several applications, including the analysis of recorded meetings, broadcast news, oral histories and personal recordings. This demonstration will show how the Video Paper system was applied to these domains and the various replay systems that were developed, including a self-contained portable implementation on a PDA and a fixed implementation on desktop PC.","cites":"19","conferencePercentile":"65.31531532"},{"venue":"ACM Multimedia","id":"9ae7a4803ff42c37e1c9b165f7a29b89463519e7","venue_1":"ACM Multimedia","year":"2014","title":"States of Diffusion for n+1 devices","authors":"Lonce L. Wyse","author_ids":"2244569","abstract":"States of Diffusion is a participative audio installation artwork that explores patterns of sound transformation that move through space as they evolve spectrally. Six one-minute segments exploring themes of intonation and timing of events across possibly many different audio sources are coordinated via a web server. The spatial diffusion flows through mobile devices held by visitors to the gallery. Visitors also exert a subtle influence over sound through the movement of their device and their movement through the gallery.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"88dda97e704ecb54c151f3eba401bc318197912d","venue_1":"ACM Multimedia","year":"2002","title":"Portable meeting recorder","authors":"Dar-Shyang Lee, Berna Erol, Jamey Graham, Jonathan J. Hull, Norihiko Murata","author_ids":"1711411, 1685739, 1975815, 1694191, 3249047","abstract":"The design and implementation of a portable meeting recorder is presented. Composed of an omni-directional video camera with four-channel audio capture, the system saves a view of all the activity in a meeting and the directions from which people spoke. Subsequent analysis computes metadata that includes video activity analysis of the compressed data stream and audio processing that helps locate events that occurred during the meeting. Automatic calculation of the room in which the meeting occurred allows for efficient navigation of a collection of recorded meetings. A user interface is populated from the metadata description to allow for simple browsing and location of significant events.","cites":"53","conferencePercentile":"92.30769231"},{"venue":"ACM Multimedia","id":"313a72f0c514bceb046343fe7bb115e7474ef87a","venue_1":"ACM Multimedia","year":"1999","title":"Auto-summarization of audio-video presentations","authors":"Li-wei He, Elizabeth Sanocki, Anoop Gupta, Jonathan Grudin","author_ids":"1689581, 1731644, 1725517, 1715607","abstract":"As streaming audio-video technology becomes widespread, there is a dramatic increase in the amount of multimedia content available on the net. Users face a new challenge: How to examine large amounts of multimedia content quickly. One technique that can enable quick overview of multimedia is video summaries; that is, a shorter version assembled by picking important segments from the original.\nWe evaluate three techniques for automatic creation of summaries for online audio-video presentations. These techniques exploit information in the audio signal (e.g., pitch and pause information), knowledge of slide transition points in the presentation, and information about access patterns of previous users. We report a user study that compares automatically generated summaries that are 20%-25% the length of full presentations to author generated summaries. Users learn from the computer-generated summaries, although less than from authors' summaries. They initially find computer-generated summaries less coherent, but quickly grow accustomed to them.","cites":"140","conferencePercentile":"97.4789916"},{"venue":"ACM Multimedia","id":"33ebaa82d1d9eb509c92fa11818efad941508794","venue_1":"ACM Multimedia","year":"2014","title":"Face-Based Automatic Personality Perception","authors":"Noura Al Moubayed, Yolanda Vazquez-Alvarez, Alex McKay, Alessandro Vinciarelli","author_ids":"1711819, 3011937, 2699821, 1719436","abstract":"Automatic Personality Perception is the task of automatically predicting the personality traits people attribute to others. This work presents experiments where such a task is performed by mapping facial appearance into the Big-Five personality traits, namely Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. The experiments are performed over the pictures of the FERET corpus, originally collected for biometrics purposes, for a total of 829 individuals. The results show that it is possible to automatically predict whether a person is perceived to be above or below median with an accuracy close to 70 percent (depending on the trait).","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"fc5f2ec63c7f0b77ed3138b667ab7046ba15486b","venue_1":"ACM Multimedia","year":"2014","title":"Interactive Audio Web Development Workflow","authors":"Lonce L. Wyse","author_ids":"2244569","abstract":"New low-level sound synthesis capabilities have recently become available in Web browsers. However, there is a considerable gap between the enabling technology for interactive audio and its wide-spread adoption in Web media content. We identify several areas where technologies are necessary to support the various stages of development and deployment, describe systems we have developed to address those needs, and show how they work together within a specific Web content development scenario.","cites":"2","conferencePercentile":"54.81927711"},{"venue":"ACM Multimedia","id":"e0779e17380622d7848f2afd7d99af768ad11cd3","venue_1":"ACM Multimedia","year":"2013","title":"Latent feature learning in social media network","authors":"Zhaoquan Yuan, Jitao Sang, Yan Liu, Changsheng Xu","author_ids":"2052119, 2254380, 1681842, 1688633","abstract":"The current trend in social media analysis and application is to use the pre-defined features and devoted to the later model development modules to meet the end tasks. In this work, we claim that representation is critical to the end tasks and contributes much to the model development module. We provide evidence that specially learned feature well addresses the diverse, heterogeneous and collective characteristics of social media data. Therefore, we propose to transfer the focus from the model development to latent feature learning, and present a general feature learning framework based on the popular deep architecture. In particular, following the proposed framework, we design a novel relational generative deep learning model to test the idea on link analysis tasks in the social media networks. We show that the derived latent features well embed both the media content and their observed links, leading to improvement in social media tasks of user recommendation and social image annotation.","cites":"6","conferencePercentile":"76.44444444"},{"venue":"ACM Multimedia","id":"b8d1feba6812ce75ea661a0b1f7bfba6f8a665c5","venue_1":"ACM Multimedia","year":"2010","title":"ACM workshop on advanced video streaming techniques for peer-to-peer networks and social networking","authors":"Gabriella Olmo, Christian Timmerer, Pascal Frossard, Keith Mitchell","author_ids":"1727145, 1757605, 1703189, 8496756","abstract":"This paper provides a summary and overview of the ACM workshop on advanced video streaming techniques for peer-to-peer networks and social networking.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"aadd8d4006b586e9b767426c9a5b56cc8fc0563a","venue_1":"ACM Multimedia","year":"2011","title":"Multimodal QoE evaluation in P2P-based IPTV systems","authors":"Mu Mu, Johnathan Ishmael, Keith Mitchell, Nicholas J. P. Race, Andreas Mauthe","author_ids":"1774994, 2215545, 8496756, 1780344, 1749658","abstract":"Peer-to-Peer (P2P) technologies provide efficient and low-cost delivery for commercial and user-generated content. Although a number of audio-visual content distribution services have been developed using P2P-based mechanisms, it is still a challenge to guarantee satisfactory user experience on high quality video streaming services due to the dynamic nature of P2P distribution. An objective assessment service is the key enabler of service quality monitoring and management. However, this complex task can not be achieved by any individual objective model that only captures a certain aspect of service quality. This paper introduces a multimodal quality evaluation framework that is specifically designed and implemented for the assessment of video streaming services in P2P-based IPTV systems. Results of initial experiments show the effectiveness of this framework.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"430431e8302378ab41f7b7f652871d415af0a047","venue_1":"ACM Multimedia","year":"2012","title":"Semi-supervised multi-instance multi-label learning for video annotation task","authors":"Xin-Shun Xu, Yuan Jiang, Xiangyang Xue, Zhi-Hua Zhou","author_ids":"2800124, 5465260, 5507458, 1692625","abstract":"Traditional approaches for automatic video annotation usually represent one video clip with a flat feature vector, neglecting the fact that video data contain natural structures. It is also noteworthy that a video clip is often relevant to multiple concepts. Indeed, the video annotation task is inherently a Multi-Instance Multi-Label learning (MIML) problem. Considering that manually annotating videos is labor-intensive and time-consuming, this paper proposes a semi-supervised MIML approach, SSMIML, which is able to exploit abundant unannotated videos to help improve the annotation performance. This approach takes label correlations into account, and enforces similar instances to share similar multi-labels. Evaluation on TREVID 2005 show that the proposed approach outperforms several state-of-the-art methods.","cites":"7","conferencePercentile":"78.32278481"},{"venue":"ACM Multimedia","id":"2e6486a86ebfe4d41c610ed715f97b21b10b2c55","venue_1":"ACM Multimedia","year":"2014","title":"FreeViewer: An Intelligent Director for 3D Tele-Immersion System","authors":"Zhenhuan Gao, Shannon Chen, Klara Nahrstedt","author_ids":"3308955, 1805705, 1688353","abstract":"This paper proposes FreeViewer, a 3D Tele-Immersion view-control system that allows viewers to see arbitrary side of the performer by intelligently choosing the streams of a subset of cameras and changing the point of view in a 3D virtual space. The view changing is actuated by the change of the sensor data from wearable devices (eg. Google Glass, smartphone) on the performer able to monitor the current orientation.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"d9f2ccadc40e0d01239a95ea3b732ba6cfe6db67","venue_1":"ACM Multimedia","year":"2014","title":"What Strikes the Strings of Your Heart?: Multi-Label Dimensionality Reduction for Music Emotion Analysis","authors":"Yang Liu, Yan Liu, Yu Zhao, Kien A. Hua","author_ids":"1750084, 1681842, 1729901, 1805087","abstract":"Music can convey and evoke powerful emotions. This amazing ability has fascinated the general public and also attracted the researchers from different fields to discover the relationship between music and emotion. Psychologists have indicated that some specific characters of rhythm, harmony, melody, and also their combinations can evoke certain kinds of emotions. Their hypotheses are based on real life experience and proved by psychological paradigms on human beings. Aiming at the same target, this paper intends to design a systematic and quantitative framework, and answer three widely interested questions: 1) what are the intrinsic features embedded in music signal that essentially evoke human emotions; 2) to what extent these features influence human emotions; and 3) whether the findings from computational models are consistent with the existing research results from psychological experiments. We formulate the problem as a multi-label dimensionality reduction problem and provide the optimal solution. The proposed multi-emotion similarity preserving embedding technique not only shows better performance in two standard music emotion datasets but also demonstrates some interesting observations for further research in this interdisciplinary topic.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"37e638a976fb030f6b447f2fa11c5f67593755ee","venue_1":"ACM Multimedia","year":"2014","title":"Stevens' Power Law in 3D Tele-immersion: Towards Subjective Modeling of Multimodal Cyber Interaction","authors":"Sabrina Schulte, Shannon Chen, Klara Nahrstedt","author_ids":"2923508, 1805705, 1688353","abstract":"In this paper we verify the insufficiency of Stevens' power law to describe the relationship between QoS and QoE factors. User studies that target different types of application scenarios of 3D Tele-immersion (3DTI) are conducted and the results show no significant power trend in the relationship between packet loss and perceptual quality metrics. We further verify that activity characteristics, activity objectives, and users' roles in the 3DTI session also have profound effects on the service quality aside to the QoS level. Thus, simple one-factor psychophysical laws are inadequate of serving as a QoS-QoE mapping model.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"7311ff5daad35e0c005c1e1db0abd77690bb7055","venue_1":"ACM Multimedia","year":"2001","title":"Supporting audiovisual query using dynamic programming","authors":"Milind R. Naphade, Roy Wang, Thomas S. Huang","author_ids":"1780080, 3126591, 1739208","abstract":"A necessary capability for content-based retrieval is to support the paradigm of query by example. Most systems for video retrieval support queries using image sequences only. We present an algorithm for matching multimodal (audio-visual) patterns for the purpose of content-based video retrieval. The novel ability of our approach to use the information content in multiple media coupled with a strong emphasis on temporal similarity differentiates it from the state-of-the-art in content-based retrieval. At the core of the pattern matching scheme is a dynamic programming algorithm, which leads to a significant improvement in performance. Coupling the use of audio with video this algorithm can be applied to grouping of shots based on audio-visual similarity. We also support relevance feedback. The user can provide feedback to the system, by choosing clips, which are closer to the user's desired target. The system then automatically adjusts the relative weights or relevance of the media and fetches different sets of target clips accordingly. It is our observation that a few iterations of such feedback are generally sufficient, for retrieving the desired video clips.","cites":"8","conferencePercentile":"52.55102041"},{"venue":"ACM Multimedia","id":"253d6983eaee3e17dee67bdc2f0866b00dcf8b42","venue_1":"ACM Multimedia","year":"2013","title":"Physical modelling and supervised training of a virtual string quartet","authors":"Graham Percival, Nicholas Bailey, George Tzanetakis","author_ids":"2307208, 2135919, 1693065","abstract":"This work improves the realism of synthesis and performance of string quartet music by generating audio through physical modelling of the violins, viola, and cello. To perform music with the physical models, virtual musicians interpret the musical score and generate actions which control the physical models. The resulting audio and haptic signals are examined with support vector machines, which adjust the bowing parameters in order to establish and maintain a desirable timbre. This intelligent feedback control is trained with human input, but after the initial training is completed, the virtual musicians perform autonomously. The system can synthesize and control different instruments of the same type (e.g., multiple distinct violins) and has been tested on two distinct string quartets (total of 8 violins, 2 violas, 2 cellos). In addition to audio, the system creates a video animation of the instruments performing the sheet music.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"c89519f5ffb643825adc55aaa7920c901672c85a","venue_1":"ACM Multimedia","year":"2016","title":"Learning Music Emotion Primitives via Supervised Dynamic Clustering","authors":"Yang Liu, Yan Liu, Xiang Zhang, Gong Chen, Kejun Zhang","author_ids":"1750084, 1681842, 1686870, 1740096, 7151256","abstract":"This paper explores a fundamental problem in music emotion analysis, i.e., how to segment the music sequence into a set of basic emotive units, which are named as emotion primitives. Current works on music emotion analysis are mainly based on the fixed-length music segments, which often leads to the difficulty of accurate emotion recognition. Short music segment, such as an individual music frame, may fail to evoke emotion response. Long music segment, such as an entire song, may convey various emotions over time. Moreover, the minimum length of music segment varies depending on the types of the emotions. To address these problems, we propose a novel method dubbed supervised dynamic clustering (SDC) to automatically decompose the music sequence into meaningful segments with various lengths. First, the music sequence is represented by a set of music frames. Then, the music frames are clustered according to the valence-arousal values in the emotion space. The clustering results are used to initialize the music segmentation. After that, a dynamic programming scheme is employed to jointly optimize the subsequent segmentation and grouping in the music feature space. Experimental results on standard dataset show both the effectiveness and the rationality of the proposed method.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"ba044d1b5a6ad4eb7c50ad27037a342492c5c851","venue_1":"ACM Multimedia","year":"2014","title":"Back and to the future: quality provisioning for multimedia content delivery","authors":"Klara Nahrstedt","author_ids":"1688353","abstract":"Quality Provisioning concept has been with us for at least 25 years and it started with the claim to be one of the necessary building blocks for multimedia content distribution and delivery. A lot of research has been done on Quality of Service and Quality of Experience by the ACM Special Interest Group on Multimedia (SIGMM) community and other research communities. So where are we with respect to broader impact and deployment of Quality Provisioning in multimedia networks, systems and applications? Did it become a necessary building block for multimedia content delivery or not? During the talk I will go back and to the future, discussing my journey regarding quality topics ranging from Quality of Service (QoS) in multimedia networks and end systems, to Experiential Quality (QoE) for current and future multimedia applications. I will reflect on successes and failures of Quality Provisioning mechanisms, policies, algorithms, protocols and management frameworks in multimedia networks, systems and applications as they evolved from up to the point of an almost ubiquitous presence of multimedia services to the on-going discussions about network neutrality and multimedia service provisioning. I will also argue that the future for Quality Provisioning is bright with numerous exciting research problems since users expect at this point nothing but high quality multimedia content delivery anytime, anywhere, any content, and on any device.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"d355a338de560b3d5861d80c0c9a6010258a30cb","venue_1":"ACM Multimedia","year":"2012","title":"Understanding screen contents for building a high performance, real time screen sharing system","authors":"Surendar Chandra, Jacob T. Biehl, John S. Boreczky, Scott Carter, Lawrence A. Rowe","author_ids":"1717644, 7457467, 2719487, 1804229, 1723155","abstract":"Faithful sharing of screen contents is an important collaboration feature. Prior systems were designed to operate over constrained networks. They performed poorly even without such bottlenecks. To build a high performance screen sharing system, we empirically analyzed screen contents for a variety of scenarios. We showed that screen updates were sporadic with long periods of inactivity. When active, screens were updated at far higher rates than was supported by earlier systems. The mismatch was pronounced for interactive scenarios. Even during active screen updates, the number of updated pixels were frequently small. We showed that crucial information can be lost if individual updates were merged. When the available system resources could not support high capture rates, we showed ways in which updates can be <i>effectively</i> collapsed. We showed that Zlib lossless compression performed poorly for screen updates. By analyzing the screen pixels, we developed a practical transformation that significantly improved compression rates. Our system captured 240 updates per second while only using 4.6 Mbps for interactive scenarios. Still, while playing movies in fullscreen mode, our approach could not achieve higher capture rates than prior systems; the CPU remains the bottleneck. A system that incorporates our findings is deployed within the lab.","cites":"6","conferencePercentile":"75"},{"venue":"ACM Multimedia","id":"685ae3f026919a9cc3083a9d7d77d430c6ffdee2","venue_1":"ACM Multimedia","year":"2010","title":"NudgeCam: toward targeted, higher quality media capture","authors":"Scott Carter, John Adcock, John Doherty, Stacy M. Branham","author_ids":"1804229, 5529541, 3881379, 2932126","abstract":"NudgeCam is a mobile application that can help users capture more relevant, higher quality media. To guide users to capture media more relevant to a particular project, third-party template creators can show users media that demonstrates relevant content and can tell users what content should be present in each captured media using tags and other meta-data such as location and camera orientation. To encourage higher quality media capture, NudgeCam provides real time feedback based on standard media capture heuristics, including face positioning, pan speed, audio quality, and many others. We describe an implementation of NudgeCam on the Android platform as well as field deployments of the application.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"24f9e744f1e814906066aa6aa805c66dff3c73be","venue_1":"ACM Multimedia","year":"2013","title":"3D teleimmersive activity classification based on application-system metadata","authors":"Aadhar Jain, Ahsan Arefin, Raoul Rivas, Chien-nan Chen, Klara Nahrstedt","author_ids":"2073425, 3356724, 2385623, 2715134, 1688353","abstract":"Being able to detect and recognize human activities is essential for 3D collaborative applications for efficient quality of service provisioning and device management. A broad range of research has been devoted to analyze media data to identify human activity, which requires the knowledge of data format, application-specific coding technique and computationally expensive image analysis. In this paper, we propose a human activity detection technique based on application generated metadata and related system metadata. Our approach does not depend on specific data format or coding technique. We evaluate our algorithm with different cyber-physical setups, and show that we can achieve very high accuracy (above 97%) by using a good learning model.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"5516624d2cc50ac7d00ff5742fb9b61b92f05dc3","venue_1":"ACM Multimedia","year":"2013","title":"TEEVE endpoint: towards the ease of 3D tele-immersive application development","authors":"Pengye Xia, Klara Nahrstedt","author_ids":"2079184, 1688353","abstract":"We present TEEVE Endpoint, which is a runtime engine to handle the creation, transmission and rendering of 3D Tele-immersive (3DTI) data and provides application programming interfaces (APIs) to developers to easily create 3DTI applications.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"0011cc15dcd204f4aae320a3e9595d35ebb77e8d","venue_1":"ACM Multimedia","year":"2008","title":"PicNTell: a camcorder metaphor for screen recording","authors":"Scott Carter, Laurent Denoue","author_ids":"1804229, 1788661","abstract":"PicNTell is a new technique for generating compelling screencasts where users can quickly record desktop activities and generate videos that are embeddable on popular video sharing distributions such as YouTube&#174;. While standard video editing and screen capture tools are useful for some editing tasks, they have two main drawbacks: (1) they require users to import and organize media in a separate interface, and (2) they do not support natural (or camcorder-like) screen recording, and instead usually require the user to define a specific region or window to record. In this paper we review current screen recording use, and present the PicNTell system, pilot studies, and a new six degree-of-freedom tracker we are developing in response to our findings.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"9711a6388689d31b347b9a1bb6fb81a25ef52c1b","venue_1":"ACM Multimedia","year":"2016","title":"StressClick: Sensing Stress from Gaze-Click Patterns","authors":"Michael Xuelin Huang, Jiajia Li, Grace Ngai, Hong Va Leong","author_ids":"3328025, 5018858, 1706729, 1714454","abstract":"Stress sensing is valuable in many applications, including online learning crowdsourcing and other daily human-computer interactions. Traditional affective computing techniques investigate affect inference based on different individual modalities, such as facial expression, vocal tones, and physiological signals or the aggregation of signals of these independent modalities, without explicitly exploiting their inter-connections. In contrast, this paper focuses on exploring the impact of mental stress on the coordination between two human nervous systems, the somatic and autonomic nervous systems. Specifically, we present the analysis of the subtle but indicative pattern of human gaze behaviors surrounding a mouse-click event, i.e. the gaze-click pattern. Our evaluation shows that mental stress affects the gaze-click pattern, and this influence has largely been ignored in previous work. This paper, therefore, further proposes a non-intrusive approach to inferring human stress level based on the gaze-click pattern, using only data collected from the common computer webcam and mouse. We conducted a human study on solving math questions under different stress levels to explore the validity of stress recognition based on this coordination pattern. Experimental results show the effectiveness of our technique and the generalizability of the proposed features for user-independent modeling. Our results suggest that it may be possible to detect stress non-intrusively in the wild, without the need for specialized equipment.","cites":"1","conferencePercentile":"90"},{"venue":"ACM Multimedia","id":"a6857f081ca65ab5f3abad964e03cfc331dbf8ca","venue_1":"ACM Multimedia","year":"2003","title":"Enhancing web accessibility","authors":"Alison Lee, Vicki L. Hanson","author_ids":"2369432, 1730685","abstract":"This demonstration will illustrate the key technical and user interface aspects of the Web Adaptation Technology. Various transformations underlying the system will be shown that illustrate how this approach enables a wide range of users with reduced visual, cognitive, and motor abilities to access a large proportion of Web pages using a standard browser.","cites":"4","conferencePercentile":"27.02702703"},{"venue":"ACM Multimedia","id":"72082a2c670be7f2d85a3c5229df97d98e6d1f3e","venue_1":"ACM Multimedia","year":"2006","title":"Taking sides: dynamic text and hip-hop performance","authors":"Jason Lewis, Yannick Assogba","author_ids":"2180900, 2867282","abstract":"In this paper we describe <i>Taking Sides</i>, a performance using a real-time speech visualization software system called TextEngine. <i>Taking Sides</i> is a collaboration between our research studio and Montreal hip-hop artist Dwayne Hanley. Our primary goal was to create a strong conceptual link between the text visualization, the content of the artist's lyrics, and his performance style. Additionally we wanted to test the flexibility of TextEngine in developing customized performance applications. Pursuing these goals led us through a three month development effort that cycled tightly between design, performance and programmatic iterations.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"0d263ddb562e17a3d3d96d540d77d747a0518bc1","venue_1":"ACM Multimedia","year":"2011","title":"Semi-supervised manifold ordinal regression for image ranking","authors":"Yang Liu, Yan Liu, Sheng-hua Zhong, Keith C. C. Chan","author_ids":"1739842, 1681842, 1700861, 1757062","abstract":"In this paper, we present a novel algorithm called manifold ordinal regression (MOR) for image ranking. By modeling the manifold information in the objective function, MOR is capable of uncovering the intrinsically nonlinear structure held by the image data sets. By optimizing the ranking information of the training data sets, the proposed algorithm provides faithful rating to the new coming images. To offer more general solution for the real-word tasks, we further provide the semi-supervised manifold ordinal regression (SS-MOR). Experiments on various data sets validate the effectiveness of the proposed algorithms.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"62033df4368508287f71ddaf25c148c6cde4dce2","venue_1":"ACM Multimedia","year":"2010","title":"Unsupervised summarization of rushes videos","authors":"Yang Liu, Feng Zhou, Wei Liu, Fernando De la Torre, Yan Liu","author_ids":"1739842, 5845482, 1722649, 7821918, 1681842","abstract":"This paper proposes a new framework to formulate summarization of rushes video as an unsupervised learning problem. We pose the problem of video summarization as one of time-series clustering, and proposed Constrained Aligned Cluster Analysis (CACA). CACA combines kernel k-means, Dynamic Time Alignment Kernel (DTAK), and unlike previous work, CACA jointly optimizes video segmentation and shot clustering. CACA is effciently solved via dynamic programming. Experimental results on the TRECVID 2007 and 2008 BBC rushes video summarization databases validate the accuracy and effectiveness of CACA.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"2cb459817a3cfc4293f99bcb5edecb07d7c70941","venue_1":"ACM Multimedia","year":"2005","title":"An automated end-to-end lecture capturing and broadcasting system","authors":"Cha Zhang, Jim Crawford, Yong Rui, Li-wei He","author_ids":"1706673, 3023843, 1728806, 1689581","abstract":"We present a complete end-to-end system that is fully automated and supports capturing, broadcasting, viewing, archiving and search. Specifically, we describe a system architecture that minimizes the pre- and post-production time, and a fully automated lecture capturing system called <i>iCam2</i>, which synchronously captures all the contents of the lecture, including audio, video and visual aids. As no staff is needed during the capturing and broadcasting process, the operation cost of our system is negligible. The system has been used on a daily basis for more than 4 years, during which 467 lectures were captured with 17,000+ online viewers.","cites":"37","conferencePercentile":"88.36633663"},{"venue":"ACM Multimedia","id":"810157a630f16f3bb2bf0b5ba0f2fe755a468551","venue_1":"ACM Multimedia","year":"2009","title":"W2ANE: when words are not enough: online multimedia language assistant for people with aphasia","authors":"Xiaojuan Ma, Sonya S. Nikolova, Perry R. Cook","author_ids":"7832481, 2146081, 1716507","abstract":"In this paper, we introduce W2ANE, an Online Multimedia Language Assistant for individuals with aphasia, a language disorder that affects millions of people. W2ANE offers a rich online multimedia library (OMLA) supported by an adaptable and adaptive vocabulary scaffold (ViVA). The system, accessible over the Internet, provides a platform for applications such as looking up unknown words, constructing phrases for communication, practicing pronunciations, and accessing content. W2ANE also enables resource sharing and remote collaboration.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"876da286b7fe2970e146e44d105abbe1d7018421","venue_1":"ACM Multimedia","year":"2016","title":"WorkCache: Salvaging siloed knowledge","authors":"Scott Carter, Laurent Denoue, Matthew Cooper","author_ids":"1804229, 1788661, 4268667","abstract":"The proliferation of workplace multimedia collaboration applications has meant on one hand more opportunities for group work but on the other more data locked away in proprietary interfaces. We are developing new tools to capture and access multimedia content from any source. In this demo, we focus primarily on new methods that allow users to rapidly reconstitute, enhance, and share document-based information.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"15ad1172c85adb4a067b156d0624474b4bf8bc94","venue_1":"ACM Multimedia","year":"2005","title":"Learning the semantics of multimedia queries and concepts from a small number of examples","authors":"Apostol Natsev, Milind R. Naphade, Jelena Tesic","author_ids":"1820908, 1780080, 1789352","abstract":"In this paper we unify two supposedly distinct tasks in multimedia retrieval. One task involves answering queries with a few examples. The other involves learning models for semantic concepts, also with a few examples. In our view these two tasks are identical with the only differentiation being the number of examples that are available for training. Once we adopt this unified view, we then apply identical techniques for solving both problems and evaluate the performance using the NIST TRECVID benchmark evaluation data [15]. We propose a combination hypothesis of two complementary classes of techniques, a nearest neighbor model using only positive examples and a discriminative support vector machine model using both positive and negative examples. In case of queries, where negative examples are rarely provided to seed the search, we create pseudo-negative samples. We then combine the ranked lists generated by evaluating the test database using both methods, to create a final ranked list of retrieved multimedia items. We evaluate this approach for rare concept and query topic modeling using the NIST TRECVID video corpus.In both tasks we find that applying the combination hypothesis across both modeling techniques and a variety of features results in enhanced performance over any of the baseline models, as well as in improved robustness with respect to training examples and visual features. In particular, we observe an improvement of 6% for rare concept detection and 17% for the search task.","cites":"89","conferencePercentile":"97.02970297"},{"venue":"ACM Multimedia","id":"1f92ae17014b1fc9dfad02512b4d2af6b75b8805","venue_1":"ACM Multimedia","year":"2004","title":"On the detection of semantic concepts at TRECVID","authors":"Milind R. Naphade, John R. Smith","author_ids":"1780080, 1788270","abstract":"Semantic multimedia management is necessary for the effective and widespread utilization of multimedia repositories and realizing the potential that lies untapped in the rich multimodal information content. This challenge has driven researchers to devise new algorithms and systems that enable automatic or semi-automatic tagging of large scale multimedia content with rich semantics. An emerging research area is the detection of a predetermined set of semantic concepts that can act as semantic filters and aid in search, and manipulation. The NIST TRECVID benchmark has responded by creating a task that has evaluated the performance of concept detection. Within the scope of this benchmark task, this paper studies trends in the emerging concept detection systems, architectures and algorithms. It also analyzes strategies that have yielded reasonable success, and challenges and gaps that lie ahead.","cites":"89","conferencePercentile":"95.09803922"},{"venue":"ACM Multimedia","id":"cdff2aadd9640efe3604758435825f99ea37e3b6","venue_1":"ACM Multimedia","year":"2016","title":"Intelli-Wrench: Smart Navigation Tool for Mechanical Assembly and Maintenance","authors":"Toru Takahashi, Yuta Kudo, Rui Ishiyama","author_ids":"2288937, 3493514, 1709089","abstract":"We present Intelli-Wrench, a smart tool for providing just-in-time information for mechanical assembly and maintenance workers, which releases the workers from miss-operation and annoying manual documents. The Intelli-Wrench captures the image of a bolt head as its \"fingerprint\" by using FIBAR (Fingerprint Imaging by Binary Angular Reflection) imaging method. The fingerprint image is then uploaded to a cloud server and matched with parts databases in order to provide the specific information associated with the bolt. Receiving the information, the Intelli-Wrench informs the workers about the designated location and torque requirements of the bolt. This direct integration of information retrieval into a tangible tool provides immediate access to relevant information otherwise found in manual documents. Furthermore, the Intelli-Wrench automatically logs the interaction and eliminates the annoying pointing-and-calling procedure which is the traditional way of secure servicing. We demonstrate a working prototype and interaction scenario.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"489f4012e09087d7377b9d746f77fb041f16f7d4","venue_1":"ACM Multimedia","year":"2003","title":"MPEG-7 video automatic labeling system","authors":"Ching-Yung Lin, Belle L. Tseng, Milind R. Naphade, Apostol Natsev, John R. Smith","author_ids":"1689953, 8595363, 1780080, 1820908, 1788270","abstract":"In this demo, we show a novel end-to-end video automatic labeling system, which accepts MPEG-1 sequence inputs and generates MPEG-7 XML metadata files. Detections are based on the prior established anchor models. This system has two parts: model training process and labeling process. They are comprised of seven modules: Shot Segmentation, Region Segmentation, Annotation, Feature Extraction, Model Learning, Classification, and XML Rendering.","cites":"8","conferencePercentile":"42.79279279"},{"venue":"ACM Multimedia","id":"0b410eb5e3bec593c52ad0bf82b8aa630358c0f5","venue_1":"ACM Multimedia","year":"2005","title":"Can small be beautiful?: assessing image resolution requirements for mobile TV","authors":"Hendrik Knoche, John D. McCarthy, M. Angela Sasse","author_ids":"7659993, 2633090, 1752376","abstract":"Mobile TV services are now being offered in several countries, but for cost reasons, most of these services offer material directly recoded for mobile consumption (i.e. without additional editing). The experiment reported in this paper, aims to assess the image resolution and bitrate requirements for displaying this type of material on mobile devices. The study, with 128 participants, examined responses to four different image resolutions, seven video encoding bitrates, two audio bitrates and four content types. The results show that acceptability is significantly lower for images smaller than 168x126, regardless of content type. The effect is more pronounced when bandwidth is abundant, and is due to important detail being lost in the smaller screens. In contrast to previous studies, participants are more likely to rate image quality as unacceptable when the audio quality is high.","cites":"58","conferencePercentile":"94.55445545"},{"venue":"ACM Multimedia","id":"03eb66e369635b84b1aa31c49cbd9672ccf76c69","venue_1":"ACM Multimedia","year":"2005","title":"Designing a large-scale video chat application","authors":"Jeremiah Scholl, Peter Parnes, John D. McCarthy, M. Angela Sasse","author_ids":"2196908, 2027288, 2633090, 1752376","abstract":"Studies of video conferencing systems generally focus on scenarios where users communicate using an audio channel. However, text chat serves users in a wide variety of contexts, and is commonly included in multimedia conferencing systems as a complement to the audio channel. This paper introduces a prototype application which integrates video and text communication, and describes a formative evaluation of the prototype with 53 users in a social setting. We focus the evaluation on bandwidth and view navigation requirements in order to determine how to better serve users with <i>video chat</i>, and discuss how the findings from this evaluation can inform the design of future video chat applications. Bandwidth requirements are evaluated through user perceptions of video delivered using three different bandwidth schemes. For view navigation, we examine a system that automatically switches the video focus to the current \"chatter\", instead of requiring users to navigate manually to find the video steam they are interested in viewing.","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"5b03d735a7ec09a4c573137357493eb4c3169311","venue_1":"ACM Multimedia","year":"2006","title":"Reading the fine print: the effect of text legibility on perceived video quality in mobile tv","authors":"Hendrik Knoche, John D. McCarthy, M. Angela Sasse","author_ids":"7659993, 2633090, 1752376","abstract":"Mobile TV services are available in an increasing number of countries. For cost reasons, most of these services offer material directly recoded for mobile consumption (i.e. without additional editing). This paper reports the findings of a study on the influence of text legibility and quality on the perceived video quality of mobile TV content. The study, with 64 participants, examined responses to news footage presented at four image resolutions and seven video encoding bitrates. The results showed that a simulated separate delivery of a news ticker and other textual information significantly increased the perceived video quality of the entire screen for native speakers. In addition, some automatable changes to the layout of news content resulted in substantial increases in perceived video quality. The results can be used to quantify the perceived quality gains when considering text delivery separately from the video stream and in the development of more accurate multimedia quality models.","cites":"6","conferencePercentile":"54.40414508"},{"venue":"ACM Multimedia","id":"6c91dd54ed070f8cc0bbfcdb5f8c7e244d31bfb2","venue_1":"ACM Multimedia","year":"2016","title":"MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression","authors":"Rufael Mekuria, Pablo César","author_ids":"2086342, 1743507","abstract":"We present MP3DG-PCC, an open source framework for design, implementation and evaluation of point cloud compression algorithms. The framework includes objective quality metrics, lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation. The framework and proposed methodology is in use for the development of an international point cloud compression standard in MPEG. In addition, the library is integrated with the popular point cloud library, making a large number of point cloud processing available and aligning the work with the broader open source community.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"2016b60a42d5734287ccd39013ae9e6ee1ac2bac","venue_1":"ACM Multimedia","year":"1994","title":"A Beat Tracking System for Acoustic Signals of Music","authors":"Masataka Goto, Yoichi Muraoka","author_ids":"1720652, 1726805","abstract":"This paper presents a beat tracking system that processes acoustic signals of music and recognizes temporal positions of beats in time. Musical beat tracking is needed by various multimedia applications such as video editing, audio editing, and stage lighting control. Previous systems were not able to deal with acoustic signals that contained sounds of various instruments, especially drums. They dealt with either MIDI signals or acoustic signals played on a few instruments, and in the latter case, did not work in real time. Our system deals with popular music in which drums maintain the beat. Because our system examines multiple hypotheses in parallel, it can follow beats without losing track of them, even if some hypotheses become wrong. Our system has been implemented on a parallel computer, the Fujitsu AP1000. In our experiment, the system correctly tracked beats in 27 out of 30 commercially distributed popular songs.","cites":"59","conferencePercentile":"83.05084746"},{"venue":"ACM Multimedia","id":"6fb1ef2e95c0d9846e322291e1e02ccd4029d7c6","venue_1":"ACM Multimedia","year":"2015","title":"DeepFont: Identify Your Font from An Image","authors":"Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang","author_ids":"2969311, 1706007, 1722322, 2177801, 1696487, 2407639, 1739208","abstract":"As font is one of the core design concepts, automatic font identification and similar font suggestion from an image or photo has been on the wish list of many designers. We study the Visual Font Recognition (VFR) problem [4] LFE, and advance the state-of-the-art remarkably by developing the <i>DeepFont</i> system. First of all, we build up the first available large-scale VFR dataset, named <i>AdobeVFR</i>, consisting of both labeled synthetic data and partially labeled real-world data. Next, to combat the domain mismatch between available training and testing data, we introduce a Convolutional Neural Network (CNN) decomposition approach, using a domain adaptation technique based on a Stacked Convolutional Auto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. Moreover, we study a novel learning-based model compression approach, in order to reduce the DeepFont model size without sacrificing its performance. The DeepFont system achieves an accuracy of higher than 80% (top-5) on our collected dataset, and also produces a good font similarity measure for font selection and suggestion. We also achieve around 6 times compression of the model without any visible loss of recognition accuracy.","cites":"5","conferencePercentile":"90.55555556"},{"venue":"ACM Multimedia","id":"1c305efcc497eb04897fe84f6414ed150f38be22","venue_1":"ACM Multimedia","year":"2015","title":"DeepFont: A System for Font Recognition and Similarity","authors":"Zhangyang Wang, Jianchao Yang, Hailin Jin, Jonathan Brandt, Eli Shechtman, Aseem Agarwala, Zhaowen Wang, Yuyan Song, Joseph Hsieh, Sarah Kong, Thomas S. Huang","author_ids":"2969311, 1706007, 1722322, 2407639, 2177801, 1696487, 8491483, 2514602, 2028421, 2961185, 1739208","abstract":"We develop the <i>DeepFont</i> system, a large-scale learning-based solution for automatic font identification, organization and selection. In this proposed technical demonstration, we will give our audience a tour to the DeepFont system, with the focus on its impacts on real consumer products, including but not limited to: 1) a cloud-based iOS App for font recognition; 2) a web-based tool for font similarity evaluation and discovery.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"f0cb0adeddf9767ee9bac9d4c0a7e5a99fd1eb1d","venue_1":"ACM Multimedia","year":"2007","title":"Image stablization for 2D barcode in handheld devices","authors":"Chung-Hua Chu, De-Nian Yang, Ming-Syan Chen","author_ids":"1699931, 1731140, 1691171","abstract":"With the ubiquitous of cellular phones, mobile applications with 2D barcodes have drawn a lot of attentions in recent years. However, the previous works for extracting 2D barcodes from an image do not consider the distortion resulted from camera shake. Moreover, the previous works for extracting 2D barcodes from an image do not take a complex background into account. In this paper, therefore, we propose an efficient and effective algorithm to extract 2D barcode from a complex background in a camera-shaken image. Compared with previous approaches, our algorithm outperforms in not only smaller running time but also higher accuracy of the barcode recognition.","cites":"10","conferencePercentile":"66.92708333"},{"venue":"ACM Multimedia","id":"98e7535da039fd95b678f73dba390e3286f0a0d7","venue_1":"ACM Multimedia","year":"2015","title":"Searching and Browsing Live, Web-based Meetings","authors":"Scott Carter, Laurent Denoue, Matthew Cooper","author_ids":"1804229, 1788661, 4268667","abstract":"Establishing common ground is one of the key problems for any form of communication. The problem is particularly pronounced in remote meetings, in which participants can easily lose track of the details of dialogue for any number of reasons. In this demo we present a web-based tool, MixMeet, that allows teleconferencing participants to search the contents of live meetings so they can rapidly retrieve previously shared content to get on the same page, correct a misunderstanding, or discuss a new idea.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"6e8c710af7ff91edd9324e24d70d93032d4f73b9","venue_1":"ACM Multimedia","year":"2010","title":"Supervised manifold learning for image and video classification","authors":"Yang Liu, Yan Liu, Keith C. C. Chan","author_ids":"1739842, 1681842, 1757062","abstract":"This paper presents a supervised manifold learning model for dimensionality reduction in image and video classification tasks. Unlike most manifold learning models that emphasize the distance preserving, we propose a novel algorithm called maximum distance embedding (MDE), which aims to maximize the distances between some particular pairs of data points, with the intention of flattening the local nonlinearity and keeping the discriminant information simultaneously in the embedded feature space. Moreover, MDE measures the dissimilarity between data points using L1-norm distance, which is more robust to outliers than widely used Frobenius norm distance. To adapt the nature tensor structure of image and video data, we further propose the multilinear MDE (M2DE). Experiments on various datasets demonstrate that both MDE and M2DE achieve impressive embedding results of image and video data for classification tasks.","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"04926009741f2a62fc746fd01b1f8cc338f24336","venue_1":"ACM Multimedia","year":"2011","title":"Bilinear deep learning for image classification","authors":"Sheng-hua Zhong, Yan Liu, Yang Liu","author_ids":"1700861, 1681842, 1739842","abstract":"Image classification is a well-known classical problem in multimedia content analysis. This paper proposes a novel deep learning model called bilinear deep belief network (BDBN) for image classification. Unlike previous image classification models, BDBN aims to provide human-like judgment by referencing the architecture of the human visual system and the procedure of intelligent perception. Therefore, the multi-layer structure of the cortex and the propagation of information in the visual areas of the brain are realized faithfully. Unlike most existing deep models, BDBN utilizes a bilinear discriminant strategy to simulate the \"initial guess\" in human object recognition, and at the same time to avoid falling into a bad local optimum. To preserve the natural tensor structure of the image data, a novel deep architecture with greedy layer-wise reconstruction and global fine-tuning is proposed. To adapt real-world image classification tasks, we develop BDBN under a semi-supervised learning framework, which makes the deep model work well when labeled images are insufficient. Comparative experiments on three standard datasets show that the proposed algorithm outperforms both representative classification models and existing deep learning techniques. More interestingly, our demonstrations show that the proposed BDBN works consistently with the visual perception of humans.","cites":"14","conferencePercentile":"88.7755102"},{"venue":"ACM Multimedia","id":"93e5e61f4e257af37ad53ccb039bff37b93d65d0","venue_1":"ACM Multimedia","year":"2013","title":"Activity-aware adaptive compression: a morphing-based frame synthesis application in 3DTI","authors":"Shannon Chen, Pengye Xia, Klara Nahrstedt","author_ids":"1805705, 2079184, 1688353","abstract":"In view of the different demands on quality of service of different user activities in the 3D Tele-immersive (3DTI) environment, we combine activity recognition and real-time morphing-based compression and present the Activity-Aware Adaptive Compression. We implement this scheme on our 3DTI platform: the TEEVE Endpoint, which is a runtime engine to handle the creation, transmission and rendering of 3DTI data. User study as well as objective evaluation of the scheme show that it can achieve 25% more bandwidth saving compared to conventional 3D data compression as zlib without perceptible degradation in the user experience.","cites":"7","conferencePercentile":"80.22222222"},{"venue":"ACM Multimedia","id":"ae6fd773625a77f673463dac4902a0a8517a3c54","venue_1":"ACM Multimedia","year":"1994","title":"Collaborative Multimedia: Getting Beyond the Obvious (Panel)","authors":"Bonnie A. Nardi, Sara A. Bly, Ellen Isaacs, Sha Xin Wei, Steve Whittaker","author_ids":"1694741, 1761570, 1731031, 1791821, 1691048","abstract":"Collaborative multimedia applications have been somewhat lacking in imagination, focusing on \" talking heads \" video as a way to create telepresence, or on data retrieval for simple information \" foraging \" and sharing tasks. In this panel we would like to look beyond these obvious and uninspired applications to some of the more interesting, useful, and intellectually stimulating applications that are under development in corporate and university research labs, We argue that collaborative mul-timedia technologies should be used not just to retrieve and/or present \" passive \" information but as a means of providing richer, deeper ways to collaboratively compose shared artifacts such as documents, movies, data visualizations, simulations, designs such as architectural drawings, bulletin boards, libraries, and animations. We should also create ways to collaboratively analyze data within these artifacts. The panel will discuss ways to promote uses of collaborative multimedia technologies that go beyond inert depiction to more active, thoughtful activities. Our media space experience has shown us that multiple media entrronments offer rich support for the process surrounding work activity. All too often, designers and implementors of col-laborative tools are concerned with the users' task and ignore the activity in which that task is situated. Our current prototypes are particularly aimed at the process rather than the artifacts of work activity. Thus, we focus on the value of colleague \" awareness \" of one another, on informal interactions, and on \" near synchronous \" uses of recorded media. Digital audio and video with high bandwidth connections will be critical in providing the necessary flow of information as well as providing new ways of building multimedia applications, Primary issues concern how users will manage the access and control of new forms of technology-mediated communication. Ellen lsaacs The Forum system being prototype at Sun Microsystems allows a speaker to broadcast a talk, presentation, or class over the network. Audience members sit at their desktops and watch video (and hear audio) of the speaker, view the slides, and participate by asking questions (verbally or textually) or responding to \" poll questions. \" This sys[em turns out to be especially powerful for audiences, who can easily tune into a broadcast, use other desktop tools to take notes, and even attend to other tasks during the slower parts of a presentation. However, we have found that, although most speakers find it worthwhile to give a talk, they also find it awkward …","cites":"0","conferencePercentile":"12.71186441"},{"venue":"ACM Multimedia","id":"1359f4bf167fc742b5bef59e88044976caf77e35","venue_1":"ACM Multimedia","year":"2012","title":"Detecting rule of simplicity from photos","authors":"Long Mai, Hoang Le, Yuzhen Niu, Yu-Chi Lai, Feng Liu","author_ids":"2712573, 7177031, 8368489, 3312303, 1734409","abstract":"Simplicity refers to one of the most important photography composition rules. Simplicity states that simplifying the image background can draw viewers' attention to the subject of interest in a photograph and help them better comprehend and appreciate it. Understanding whether a photo respects photography rules or not facilitates photo quality assessment. In this paper, we present a method to automatically detect whether a photo is composed according to the rule of simplicity. We design features according to the definition, implementation and effect of the rule. First, we make use of saliency analysis to infer the subject of interest in a photo and measure its compactness. Second, we segment an image into background and foreground and measure the homogeneity within the background as another feature. Third, when looking at an image created with the rule of simplicity, different viewers tend to agree on what the subject of interest is in this photo. We accordingly measure the consistency among various saliency detection results as a feature. We experiment with these features in a range of machine learning methods. Our experiments show that our methods, together with these features, provide an encouraging result in detecting the rule of simplicity in a photo.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"5e8c4276e22b808ca1aa4947e2f5c2817ef0208c","venue_1":"ACM Multimedia","year":"2008","title":"Confidence based multimodal fusion for person identification","authors":"Philipp Große, Hartwig Holzapfel, Alexander H. Waibel","author_ids":"3035108, 2934024, 4500589","abstract":"Person identification is of great interest for various kinds of applications and interactive systems. In our system we use face recognition and voice recognition from data recorded in an interactive dialogue system. In such a system, sequential images and sequential utterances can be used to improve recognition accuracy over single hypotheses. The presented approach uses confidence-based fusion for sequence hypotheses, for multimodal fusion, and to provide a reliability measure of the classification quality that can be used to decide when to trust and when to ignore classification results.","cites":"3","conferencePercentile":"36.00917431"},{"venue":"ACM Multimedia","id":"4b518c1a3a344e9458075b4e05f24c5818163a3a","venue_1":"ACM Multimedia","year":"1999","title":"Multimodal people ID for a multimedia meeting browser","authors":"Jie Yang, Xiaojin Zhu, Ralph Gross, John Kominek, Yue Pan, Alexander H. Waibel","author_ids":"1688428, 3428912, 1741233, 2182809, 1794348, 4500589","abstract":"A meeting browser is a system that allows users to review a multimedia meeting record from a variety of indexing methods. Identification of meeting participants is essential for creating such a multimedia meeting record. Moreover, knowing who is speaking can enhance the performance of speech recognition and indexing meeting transcription. In this paper, we present an approach that identifies meeting participants by fusing multimodal inputs. We use face ID, speaker ID, color appearance ID, and sound source directional ID to identify and track meeting. After describing the different modules in detail, we will discuss a framework for combining the information sources. Integration of the multimodal people ID into the multimedia meeting browser is in its preliminary stage.","cites":"46","conferencePercentile":"89.49579832"},{"venue":"ACM Multimedia","id":"7c88ed637d61b52153f62c3832860e5dd1fcb119","venue_1":"ACM Multimedia","year":"2007","title":"Madame bovary on the holodeck: immersive interactive storytelling","authors":"Marc Cavazza, Jean-Luc Lugrin, David Pizzi, Fred Charles","author_ids":"1696638, 3120007, 1691265, 1776150","abstract":"In this paper, we describe a small-scale, yet complete, integration of a real-time immersive interactive storytelling system. While significant progress has been achieved in recent years on the individual component technologies of interactive storytelling, the main objective of this work is to investigate the concept of interactive storytelling in a fully immersive context. We describe each individual component of immersive interactive storytelling from a technical perspective. We have used a commercial game engine as a development environment, supporting real-time visualisation as well as the inclusion of Artificial Intelligence components controlling virtual actors. This visualisation engine has been ported to an immersive setting using dedicated software and hardware supporting real-time stereoscopic visualisation. The hardware platform is built around a 4-sided CAVE-like immersive display operated by a PC-cluster. The interactive storytelling engine is constituted by a planning system based on characters motivations and emotional states. The user can interact with the virtual world using multimodal interaction. We illustrate the system's behaviour on the implementation of excerpts from <i>Madame Bovary</i>, a classic XIX<sup>th</sup> century novel, and demonstrate the ability for the user to play the role of one of the characters and influence the unfolding of the story by his actions.","cites":"34","conferencePercentile":"89.0625"},{"venue":"ACM Multimedia","id":"1e139bc11ee748cd9ba60b0e47569dd6891dd707","venue_1":"ACM Multimedia","year":"2008","title":"Enhancing social sharing of videos: fragment, annotate, enrich, and share","authors":"Pablo César, Dick C. A. Bulterman, David Geerts, Jack Jansen, Hendrik Knoche, William Seager","author_ids":"1743507, 1726923, 1802499, 1724349, 7659993, 8515109","abstract":"Media consumption is an inherently social activity, serving to communicate ideas and emotions across both small- and large-scale communities. The migration of the media experience to personal computers retains social viewing, but typically only via a non-social, strictly personal interface. This paper presents an architecture and implementation for media content selection, content (re)organization, and content sharing within a user community that is heterogeneous in terms of both participants and devices. In addition, our application allows the user to enrich the content as a differentiated personalization activity targeted to his/her peer-group. We describe the goals, architecture and implementation of our system in this paper. In order to validate our results, we also present results from two user studies involving disjoint sets of test participants.","cites":"38","conferencePercentile":"91.97247706"},{"venue":"ACM Multimedia","id":"833bddc3eab690b11bbbcfbaa5bcc546b060de91","venue_1":"ACM Multimedia","year":"2011","title":"Towards synergy between the open source and the research multimedia communities","authors":"Pablo César, Wei Tsang Ooi, Ben Moskowitz, Zohar Babin, Dick C. A. Bulterman, Rainer Lienhart, Robert Richter","author_ids":"1743507, 1678873, 2292706, 2021938, 1726923, 1803227, 2506919","abstract":"This panel extends current efforts from the ACM Multimedia 2011 Organization Committee in taking an important step towards open source projects. The panelists include speakers who are among the leading figures from the open source community. The goal is to provide a shared space for discussion and interaction among consolidated and new open source projects and multimedia researchers.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"520915791b0d4aa383be354ebb79fa84b9b7e01f","venue_1":"ACM Multimedia","year":"1999","title":"Modeling focus of attention for meeting indexing","authors":"Rainer Stiefelhagen, Jie Yang, Alexander H. Waibel","author_ids":"1742325, 1688428, 4500589","abstract":"Visual cues, such as gesturing, looking at each other or monitoring each others facial expressions, play an important role in meetings. Such information can be used for indexing of multimedia meeting recordings. In this paper, we present an approach to detect who is looking at whom during a meeting. Our proposal is to employ Hidden Markov Models to characterize participants' focus of attention by using gaze information as well as knowledge about the number and positions of people present in a meeting. The number and positions of the participants faces are detected in the field of view of a panoramic camera. We use neural networks to estimate the directions of participants' gaze from camera images. We discuss the implementation of the approach in detail including system architecture, data collection, and evaluation. The system has achieved an accuracy rate of up to 93 % in detecting focus of attention on test sequences taken from meetings. We have used focus of attention as an index in a multimedia meeting browser.","cites":"46","conferencePercentile":"89.49579832"},{"venue":"ACM Multimedia","id":"a8594c09396089976f749b33abbca4d0c62d026b","venue_1":"ACM Multimedia","year":"2016","title":"Research Challenges in Developing Multimedia Systems for Managing Emergency Situations","authors":"Mengfan Tang, Siripen Pongpaichet, Ramesh Jain","author_ids":"3057568, 1998655, 4521564","abstract":"With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"7fdbd6f8a451f98076426192ac426b6538ba95b7","venue_1":"ACM Multimedia","year":"2010","title":"Changing characters' point of view in interactive storytelling","authors":"Fred Charles, Julie Porteous, Marc Cavazza","author_ids":"1776150, 1768788, 1696638","abstract":"Virtual characters are at the epicentre of Interactive Storytelling systems and in recent years multiple AI planning approaches have been described to specify their autonomous behaviour. This demonstrator provides an overview of our novel approach to the definition of virtual characters aimed at achieving a balance between character autonomy and global plot structure which proposes the notion of a character's Point of View. Additionally, the demonstrator offers the active spectator the ability to discover the story described from the perspective of a number of different characters. We present our fully-implemented Interactive Narrative based on Shakespeare's Merchant of Venice. The system, which features a novel AI planning approach to story generation, can generate very different stories depending on the Point of View adopted and support dynamic modification of the story world which results in different story consequences.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"0d7c592c38c5ad856d5bb62b6ebf168cd975e776","venue_1":"ACM Multimedia","year":"2013","title":"Consistent stereo image editing","authors":"Tao Yan, Shengfeng He, Rynson W. H. Lau, Yun Xu","author_ids":"3189560, 2548483, 1726262, 6476924","abstract":"Stereo images and videos are very popular in recent years, and techniques for processing this media are attracting a lot of attention. In this paper, we extend the shift-map method for stereo image editing. Our method simultaneously processes the left and right images on pixel level using a global optimization algorithm. It enforces photo consistence between the two images and preserves 3D scene structures. It also addresses the occlusion and disocclusion problem, which may enable many stereo image editing functions, such as depth mapping, object depth adjustment and non-homogeneous image resizing. Our experiments show that the proposed method produces high quality results in various editing functions.","cites":"2","conferencePercentile":"47.55555556"},{"venue":"ACM Multimedia","id":"1c41d04f9dfbab42d1b4dd8838178897265b9089","venue_1":"ACM Multimedia","year":"2010","title":"AIR conferencing: accelerated instant replay for in-meeting multimodal review","authors":"Kori Inkpen Quinn, Rajesh Hegde, Sasa Junuzovic, Christopher Brooks, John C. Tang, Zhengyou Zhang","author_ids":"2735374, 6466598, 2999736, 7246690, 1808267, 1732465","abstract":"When people attend meetings they may miss parts of the discussion if they, for example, step out to take a phone call, go to the bathroom, or have a momentary lapse in concentration. As a result, they may need to catch up on what they missed upon returning to the meeting. Asking other attendees for a recap is often disruptive. To avoid such disruptions, we have developed an Accelerated Instant Replay (AIR) Conferencing system for videoconferencing that enables participants to privately catch up to an ongoing meeting. We explored several mechanisms where the meeting content is replayed at an accelerated rate so that the participants can catch up to the live discussion reasonably quickly.","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"1ffdec500d847dc702cdf84e8fe826a71c3c4340","venue_1":"ACM Multimedia","year":"2014","title":"A Robust Panel Extraction Method for Manga","authors":"Xufang Pang, Ying Cao, Rynson W. H. Lau, Antoni B. Chan","author_ids":"2293733, 4525102, 1726262, 7751933","abstract":"Automatically extracting frames/panels from digital comic pages is crucial for techniques that facilitate comic reading on mobile devices with limited display areas. However, automatic panel extraction for manga, i.e., Japanese comics, can be especially challenging, largely because of its complex panel layout design mixed with various visual symbols throughout the page. In this paper, we propose a robust method for automatically extracting panels from digital manga pages. Our method first extracts the panel block by closing open panels and identifying a page background mask. It then performs a recursive binary splitting to partition the panel block into a set of sub-blocks, where an optimal splitting line at each recursive level is determined adaptively.","cites":"4","conferencePercentile":"75.90361446"},{"venue":"ACM Multimedia","id":"10efbea1b767a2eae629c1ac66b6479c8bb5f7dc","venue_1":"ACM Multimedia","year":"2016","title":"InnerView: Learning Place Ambiance from Social Media Images","authors":"Darshan Santani, Rui Hu, Daniel Gatica-Perez","author_ids":"1765362, 4234445, 1698682","abstract":"In the recent past, there has been interest in characterizing the physical and social ambiance of urban spaces to understand how people perceive and form impressions of these environments based on physical and psychological constructs. Building on our earlier work on characterizing ambiance of indoor places, we present a methodology to automatically infer impressions of place ambiance, using generic deep learning features extracted from images publicly shared on Foursquare. We base our methodology on a corpus of 45,000 images from 300 popular places in six cities on Foursquare. Our results indicate the feasibility to automatically infer place ambiance with a maximum R<sup>2</sup> of 0.53 using features extracted from a pre-trained convolutional neural network. We found that features extracted from deep learning with convolutional nets consistently outperformed individual and combinations of several low-level image features (including Color, GIST, HOG and LBP) to infer all the studied 13 ambiance dimensions. Our work constitutes a first study to automatically infer ambiance impressions of indoor places from deep features learned from images shared on social media.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"04717b9ebd4d6c10a19fbd371b094a3af17dfbef","venue_1":"ACM Multimedia","year":"2015","title":"Loud and Trendy: Crowdsourcing Impressions of Social Ambiance in Popular Indoor Urban Places","authors":"Darshan Santani, Daniel Gatica-Perez","author_ids":"1765362, 1698682","abstract":"New research cutting across architecture, urban studies, and psychology is contextualizing the understanding of urban spaces according to the perceptions of their inhabitants. One fundamental construct that relates place and experience is ambiance, which is defined as \"the mood or feeling associated with a particular place\". We posit that the systematic study of ambiance dimensions in cities is a new domain for which multimedia research can make pivotal contributions. We present a study to examine how images collected from social media can be used for the crowdsourced characterization of indoor ambiance impressions in popular urban places. We design a crowdsourcing framework to understand suitability of social images as data source to convey place ambiance, to examine what type of images are most suitable to describe ambiance, and to assess how people perceive places socially from the perspective of ambiance along 13 dimensions. Our study is based on 50,000 Foursquare images collected from 300 popular places across six cities worldwide. The results show that reliable estimates of ambiance can be obtained for several of the dimensions. Furthermore, we found that most aggregate impressions of ambiance are similar across popular places in all studied cities. We conclude by presenting a multidisciplinary research agenda for future research in this domain.","cites":"6","conferencePercentile":"92.03703704"},{"venue":"ACM Multimedia","id":"50d350a1d9d713454ba84b4d507820f28cc83718","venue_1":"ACM Multimedia","year":"2013","title":"Speaking swiss: languages and venues in foursquare","authors":"Darshan Santani, Daniel Gatica-Perez","author_ids":"1765362, 1698682","abstract":"Due to increasing globalization, urban societies are becoming more multicultural. The availability of large-scale digital mobility traces e.g. from tweets or checkins provides an opportunity to explore multiculturalism that until recently could only be addressed using survey-based methods. In this paper we examine a basic facet of multiculturalism through the lens of language use across multiple cities in Switzerland. Using data obtained from Foursquare over 330 days, we present a descriptive analysis of linguistic differences and similarities across five urban agglomerations in a multicultural, western European country.","cites":"4","conferencePercentile":"67.11111111"},{"venue":"ACM Multimedia","id":"6834b359ec0fcd8a2144c13f4ec869038f5612b3","venue_1":"ACM Multimedia","year":"2010","title":"Interactive storytelling via video content recombination","authors":"Julie Porteous, Sergio Benini, Luca Canini, Fred Charles, Marc Cavazza, Riccardo Leonardi","author_ids":"1768788, 2949630, 2271653, 1776150, 1696638, 1741369","abstract":"In the paper we present a prototype of video-based storytelling that is able to generate multiple story variants from a baseline video. The video content for the system is generated by an adaptation of forefront video summarisation techniques that decompose the video into a number of Logical Story Units (LSU) representing sequences of contiguous and interconnected shots sharing a common semantic thread. Alternative storylines are generated using AI Planning techniques and these are used to direct the combination of elementary LSU for output. We report early results from experiments with the prototype in which the reordering of video shots on the basis of their high-level semantics produces trailers giving the illusion of different storylines.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"6c735a32eb980439f61ec5cb49cb7840034e8b86","venue_1":"ACM Multimedia","year":"2011","title":"Generating story variants with constrained video recombination","authors":"Alberto Piacenza, Fabrizio Guerrini, Nicola Adami, Riccardo Leonardi, Julie Porteous, Jonathan Teutenberg, Marc Cavazza","author_ids":"3134795, 1806359, 1686930, 1741369, 1768788, 3158212, 1696638","abstract":"We present a novel approach to the automatic generation of filmic variants within an implemented Video-Based Storytelling (VBS) system that successfully integrates video segmentation with stochastically controlled re-ordering techniques and narrative generation via AI planning. We have introduced flexibility into the video recombination process by sequencing video shots in a way that maintains local video consistency and this is combined with exploitation of shot polysemy to enable shot reuse in a range of valid semantic contexts. Results of evaluations on output narratives using a shared set of video data show consistency in terms of local video sequences and global causality with no loss of generative power.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"113b861e0b56107426aa17e0e6819c01f6a46ad9","venue_1":"ACM Multimedia","year":"2011","title":"Changing video arrangement for constructing alternative stories","authors":"Alberto Piacenza, Fabrizio Guerrini, Nicola Adami, Riccardo Leonardi, Jonathan Teutenberg, Julie Porteous, Marc Cavazza","author_ids":"3134795, 1806359, 1686930, 1741369, 3158212, 1768788, 1696638","abstract":"Currently, automatic generation of filmic variants faces a number of key technical issues and thus it usually resorts to the shooting of multiple versions of alternative scenes. However, recent advancements in video analysis has made this objective feasible, though semantic consistency must be somehow preserved. This demo presents a video-based storytelling (VBS) system that successfully integrates video processing with narrative generation by means of a shared semantic description. The novel filmic variants are constructed through a flexible video recombination process that takes advantage of the polysemy of baseline video segments. The short output video clips shown in this demo prove how the generated narratives are semantically consistent while keeping generative power intact.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"a7bac182afa7d07adac647990fe832ed1f8ade00","venue_1":"ACM Multimedia","year":"2009","title":"Concept, content and the convict","authors":"Mika Luma Tuomola, Teemu Korpilahti, Jaakko Pesonen, Abhigyan Singh, Robert Villa, P. Punitha, Yue Feng, Joemon M. Jose","author_ids":"2462606, 3245833, 1957624, 2358447, 3328238, 2700683, 2398200, 1685211","abstract":"This paper describes the concepts behind and implementation of the multimedia art work <i>Alan01 / AlanOnline</i>, which wakes up the 1952 criminally convicted Alan Turing as a piece of code within the art work - thus fulfilling Turing's own vision of preserving human consciousness in a computer. The work's context is described within the development of associative storytelling structures built up by interactive user feedback via an image and video retrieval system. The input to the retrieval system is generated by <i>Alan01 / AlanOnline</i> via their respective sketch interfaces, the output of the retrieval system being fed back to <i>Alan01 / AlanOnline</i> for further processing and presentation to the user within the context of the overall artistic experience. This paper, in addition to presenting the productions and image retrieval system, also presents the installation and online production user reception and some of the issues and observations made during the development of the systems.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"e8086b3bc1504b6a28232ddd1f037d975dd66986","venue_1":"ACM Multimedia","year":"2013","title":"The ACM multimedia 2013 art exhibition","authors":"Marc Cavazza, Antonio Camurri","author_ids":"1696638, 3141704","abstract":"The Art Exhibition of ACM Multimedia 2013 has attracted significant work from a variety of digital artists collaborating with research institutions. We have endeavored to select exhibits that achieved an interesting balance between technology and artistic intent. The techniques underpinning these artworks are relevant to several technical tracks of the conference, in particular those dealing with human-centered and interactive media. We briefly review how the various installations revisit current topics in Multimedia research, focusing more specifically on their approach to dynamic content generation, user experience, multimodality, and affective interfaces. Once again, the unique blend of technology and user experience does not limit itself to showcasing recent advances in interactive media, and should be of interest to all conference participants as well as the general public the exhibition space will be open to.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"b80e216634cf1b5f4cee63a212b08ae92683d889","venue_1":"ACM Multimedia","year":"2009","title":"ACM international workshop on multimedia technologies for distance learning (MTDL 2009)","authors":"Timothy K. Shih, Rynson W. H. Lau, Neil Y. Yen","author_ids":"1723732, 1726262, 3448267","abstract":"The MTDL 2009 workshop aims to discuss the impact of multimedia technologies to e-Learning. This workshop is held in conjunction with the ACM Multimedia 2009 Conference in Beijing. As a cover paper of this workshop, we briefly summarize important technologies used in e-learning in the first section, followed by a discussion of important issues proposed in the 11 papers accepted to the workshop (among the 24 submissions).","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"79e92ee03f7eb95feffe3fa7b19b04c885ba744b","venue_1":"ACM Multimedia","year":"2002","title":"Multi-party distributed audio service with TCP fairness","authors":"Milena Radenkovic, Chris Greenhalgh","author_ids":"1688358, 1714469","abstract":"Distributed Partial Mixing is an approach to creating a distributed audio service that supports optimisation of bandwidth utilization across multiple related audio streams (e.g. from concurrently active audio sources) while maintaining fairness to TCP traffic in best effort networks. Rate adaptation of streamed audio is difficult because of its rate sensitivity, the relatively limited range of encoding bandwidths available and the potential impact on the end user of rate-adaptation artefacts (such as changes of encoding). This paper describes and demonstrates how our design combines TCP-fairness with the stability that is desirable for streaming audio and other rate sensitive media. In particular, our design combines: a distributed multi-stream management/mixing architecture, loss event and round-trip time monitoring, rate limiting based on a TCP rate equation, tuned increase and decrease strategies and a loss-driven network probing mode. Experimental validation is performed over a wide range of network conditions including against various congesting levels, TCP and independent DPM traffic.","cites":"8","conferencePercentile":"46.58119658"},{"venue":"ACM Multimedia","id":"e90d3926af5f965ccfa7320c00bf66ea77a4ceee","venue_1":"ACM Multimedia","year":"2011","title":"The third ACM international workshop on multimedia technologies for distance learning (MTDL 2011)","authors":"Rynson W. H. Lau, Timothy K. Shih, Frederick W. B. Li, Neil Y. Yen","author_ids":"1726262, 1723732, 7279850, 3448267","abstract":"The MTDL 2011 workshop in its third edition aims to continue in the contribution and evaluation of the impact of multimedia technologies to e-Learning. This workshop is held in conjunction with the ACM Multimedia 2011 Conference in Scottsdale, Arizona, U.S.A. As a cover paper of this workshop, we briefly summarize important issues to be addressed in e-learning in the first section, followed by a discussion of important issues proposed in the 5 papers accepted to the workshop (among the 9 submissions), plus 3 invited papers.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"233d738dcc69468454912ec526c509d06d6978fc","venue_1":"ACM Multimedia","year":"2011","title":"A real-time database architecture for motion capture data","authors":"Pengjie Wang, Rynson W. H. Lau, Mingmin Zhang, Jiang Wang, Haiyu Song, Zhigeng Pan","author_ids":"3195829, 1726262, 1727682, 1727526, 3316036, 1716395","abstract":"Due to the popularity of motion capture data in many applications, such as games, movies and virtual environments, huge collections of motion capture data are now available. It is becoming important to store these data in compressed form while being able to retrieve them without much overhead. However, there is little work that addresses both issues together. In this paper, we address these two issues by proposing a novel database architecture. First, we propose a lossless compression algorithm to compress the motion clips, which is based on a novel Alpha Parallelogram Predictor (APP) to estimate the degree of freedom (DOF) of each child joint from its immediate neighbors and parents that have already been processed. Second, we propose to store selected eigenvalues and eigenvectors of each motion clip, which only require a very small amount of memory overheads, for faster filtering of irrelevant motions. Based on this architecture, real-time queries become a three-step process. In the first two steps, we perform a quick filtering to identify relevant motion clips in the database through a two-level indexing structure. In the third step, only a small number of candidate clips are uncompressed and accurately matched with a Dynamic Time Warping algorithm. Our results show that users can efficiently search clips from this losslessly compressed motion database.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"f24d3a8a5c3489dac3502afbcc6d702f8043c469","venue_1":"ACM Multimedia","year":"2010","title":"Content without context is meaningless","authors":"Ramesh Jain, Pinaki Sinha","author_ids":"4521564, 2174583","abstract":"We revisit one of the most fundamental problems in multimedia that is receiving enormous attention from researchers without making much progress in solving it: the problem of bridging the semantic gap. Research in this area has focused on developing increasingly rigorous techniques using the content. Researchers consider that <i>Content is King</i> and ignore everything else. In this paper, first we will discuss how this infatuation with content continues to be the biggest hurdle in the success of, ironically, content based approaches for multimedia search. Lately, many commercial systems have ignored content in favor of context and demonstrated better success. Given that the mobile phones are the major platform for the next generation of computing, context becomes easily available and more relevant. We show that it is not Content Versus Context; rather it is Content and Context that is required to bridge the semantic gap. In this paper, first we will discuss reasons for our approach and then present approaches that appropriately combine context with content to help bridge the semantic gap and solve important problems in multimedia computing.","cites":"22","conferencePercentile":"89.5890411"},{"venue":"ACM Multimedia","id":"85b37bded0fc77674c0a53e0c8cfc87b9512fb09","venue_1":"ACM Multimedia","year":"2010","title":"The second ACM international workshop on multimedia technologies for distance learning (MTDL 2010)","authors":"Timothy K. Shih, Rynson W. H. Lau, Nadia Magnenat-Thalmann, Marc Spaniol, Baltasar Fernández-Manjón","author_ids":"1723732, 1726262, 1695679, 3115338, 1717664","abstract":"The MTDL 2010 workshop in its second edition aims to continue in the contribution and evaluation of the impact of multimedia technologies to e-Learning. This workshop is held in conjunction with the ACM Multimedia 2010 Conference in Firenze (Italy). As a cover paper of this workshop, we briefly summarize important issues to be addressed in e-learning in the first section, followed by a discussion of important issues proposed in the 6 papers accepted to the workshop (among the 14 submissions).","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"30ed2aba4eed3d41ff30fcd5a923aa6faa757f8c","venue_1":"ACM Multimedia","year":"2010","title":"Hybrid load balancing for online games","authors":"Rynson W. H. Lau","author_ids":"1726262","abstract":"As massively multiplayer online games are becoming very popular, how to support a large number of concurrent users while maintaining the game performance has become an important research topic. There are two main research directions based on the multi-server architecture, global load balancing, which is optimal but computationally expensive, or local load balancing, which is not optimal but efficient. In this paper, we propose a hybrid load balancing approach to support massively multiplayer online gaming. Our idea is to augment a local load balancing algorithm with some global load information, which may be obtained less frequently. We propose two methods to implement the hybrid approach. Our results show that the proposed methods reduce the frequency of server overloading and improve the overall game performance significantly.","cites":"4","conferencePercentile":"55.61643836"},{"venue":"ACM Multimedia","id":"892d3daf4e22c6c314fe5341c579bab26e490c94","venue_1":"ACM Multimedia","year":"2011","title":"Modeling and representing events in multimedia","authors":"Vasileios Mezaris, Ansgar Scherp, Ramesh Jain, Mohan S. Kankanhalli, Huiyu Zhou, Jianguo Zhang, Liang Wang, Zhengyou Zhang","author_ids":"1737436, 1753135, 4521564, 1744045, 4163527, 3663414, 1745316, 1732465","abstract":"This paper presents an overview of the Joint Workshop on Modeling and Representing Events (JMRE), which is held as part of ACM Multimedia 2011. JMRE is concerned with the understanding of events from multimedia, and with using events in order to better organize and consume multimedia.","cites":"9","conferencePercentile":"80.4664723"},{"venue":"ACM Multimedia","id":"ac0bee9da89835d4f54fbf7accba193bc439b326","venue_1":"ACM Multimedia","year":"2012","title":"Situation recognition: an evolving problem for heterogeneous dynamic big multimedia data","authors":"Vivek K. Singh, Mingyan Gao, Ramesh Jain","author_ids":"4685302, 1700498, 4521564","abstract":"With the growth in social media, internet of things, and planetary-scale sensing there is an unprecedented need to assimilate spatio-temporally distributed multimedia streams into actionable information. Consequently the concepts like objects, scenes, and events, need to be extended to recognize <i>situations</i> (e.g. epidemics, traffic jams, seasons, flash mobs). This paper motivates and computationally grounds the problem of situation recognition. It describes a systematic approach for combining multimodal real-time big data into actionable situations. Specifically it presents a generic approach for <i>modeling</i> and <i>recognizing situations</i>. A set of generic building blocks and guidelines help the domain experts model their situations of interest. The created models can be tested, refined, and deployed into practice using a developed system (EventShop). Results of applying this approach to create multiple situation-aware applications by combining heterogeneous streams (e.g. Twitter, Google Insights, Satellite imagery, Census) are presented.","cites":"8","conferencePercentile":"81.48734177"},{"venue":"ACM Multimedia","id":"3e535325ab1b5e7cc534bc5abe7ecc624582e5ae","venue_1":"ACM Multimedia","year":"2009","title":"ACM 2009 workshop on ambient media computing (AMC'09) overview","authors":"Howard Leung, Cha Zhang, Qing Li, Rynson W. H. Lau, Benjamin W. Wah, Abdulmotaleb El-Saddik, K. Selçuk Candan, L. Irene Cheng","author_ids":"1771986, 1706673, 6440058, 1726262, 1730303, 1695990, 1720972, 8267051","abstract":"With the widespread deployment of sensors-visual, haptic, or otherwise-all around us, it is increasingly possible to gather and act over information about human activities without intrusive means. This \"ambient media intelligence\" allows us to develop human-centric multimedia applications that can strongly impact our daily lives; for example, human gestures and movements in the physical space can be used to interact with digital, multimedia content or conversely situational information, monitor the human-centric interface based on user response can be used to allocate available resources effectively to meet the user's current preference.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"03275a5e7595cd6508f14007a1514a9683e776a1","venue_1":"ACM Multimedia","year":"2015","title":"Bringing Deep Causality to Multimedia Data Streams","authors":"Laleh Jalali, Ramesh Jain","author_ids":"1841383, 4521564","abstract":"We live in a data abundance era. Availability of large volume of diverse multimedia data streams (ranging from video, to tweets, to activity, and to PM2.5) can now be used to solve many critical societal problems. Causal modeling across multimedia data streams is essential to reap the potential of this data. However, effective frameworks combining formal abstract approaches with practical computational algorithms for causal inference from such data are needed to utilize available data from diverse sensors. We propose a causal modeling framework that builds on data-driven techniques while emphasizing and including the appropriate human knowledge in causal inference. We show that this formal framework can help in designing a causal model with a systematic approach that facilitates framing sharper scientific questions, incorporating expert's knowledge as causal assumptions, and evaluating the plausibility of these assumptions. We show the applicability of the framework in a an important Asthma management application using meteorological and pollution data streams.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"0f0bcce95b3b0edfe81c961b22487319746b2242","venue_1":"ACM Multimedia","year":"2016","title":"Exploration of Large Image Corpuses in Virtual Reality","authors":"Sanket Khanwalkar, Shonali Balakrishna, Ramesh Jain","author_ids":"3493407, 3493721, 4521564","abstract":"With the increasing capture of photos and their proliferation on social media, there is a pressing need for a more intuitive and versatile image search and exploration system. Image search systems have long been confined to the binds of the 2D legacy screens and the keyword text-box. With the recent advances in Virtual Reality (VR) technology, a move towards an immersive VR environment will redefine the image navigation experience. To this end, we propose a VR platform that gathers images from various sources, and addresses the 5 Ws of image search - what, where, when, who and why. We achieve this by providing the user with two modes of interactive exploration - (i) A mode that allows for a graph based navigation of an image dataset, using a steering wheel visualization, along multiple dimensions of time, location, visual concept, people, etc. and (ii) Another mode that provides an intuitive exploration of the image dataset using a logical hierarchy of visual concepts. Our contributions include creating a VR image exploration experience that is intuitive and allows image navigation along multiple dimensions.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"a71b36bcbc88f7dcfca92b6c30cb74ed16ff3325","venue_1":"ACM Multimedia","year":"2004","title":"New ways of worldmaking: the Alterne platform for VR art","authors":"Marc Cavazza, Jean-Luc Lugrin, Simon Hartley, Paolo Libardi, Matthew J. Barnes, Mikael Le Bras, Marc Le Renard, Louis Bec, Alok Nandi","author_ids":"1696638, 3120007, 2978666, 1991107, 1895336, 7841736, 8171664, 3289538, 3186442","abstract":"We introduce a novel approach to the creation of Virtual Reality Art installations, which supports the design of alternative worlds, in which laws of Physics can be redefined to induce new user experiences. To implement this concept of \"Alternative Reality\", we have used Artificial Intelligence techniques to support the definition of the virtual environment behaviour, an approach inspired by Qualitative Reasoning systems. Besides the redefinition of physical laws, we have developed mechanisms for eliciting causal relations between events, as causality plays an important part in users' perception of virtual worlds. Our pilot installation is a CAVETM-like system incorporating a state-of-the-art computer game engine as visualisation software, which has-been ported to this immersive display. The event-based system underlying the game engine is used to bypass the native Physics engine and replace it with our Alternative Reality software. A first prototype has been fully implemented, the Alternative Reality modules totaling over 100,000 lines of C++ code. We present early results obtained with this approach, illustrated with examples taken from two artistic briefs, developed by digital artists associated to this research.","cites":"12","conferencePercentile":"65.93137255"},{"venue":"ACM Multimedia","id":"4328933890f5a89ad0af69990926d8484f403e4b","venue_1":"ACM Multimedia","year":"2011","title":"Personalized portraits ranking","authors":"Lei Huang, Tian Xia, Ji Wan, Yongdong Zhang, Shouxun Lin","author_ids":"1750444, 1737585, 2622533, 1699819, 1745406","abstract":"Portraits, also known as images of people, constitute an important part of consumer photos. Existing methods manage portraits based on either explicit objectives, e.g., a specified person or event, or aesthetics, i.e., the aesthetic quality of portraits. This paper presents a novel system for personalized portraits ranking. First, four kinds of personalized features, i.e., composition, clothing style, affection and social relationship are proposed to quantify users' intent. Then, example-based and sketch-based user interfaces (UI) are developed, which are capable of capturing users' personal intent hardly described by queries or aesthetics. Finally, portraits ranking is implemented by combing these features together with the developed user interfaces. Experimental results show that the system performs well in providing personalized preferences and the proposed features are effective for portraits ranking. From the user study, our system gets promising results.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"6ee5fe4a75eaa93745b9553a219c3097598563d1","venue_1":"ACM Multimedia","year":"2009","title":"Locally non-negative linear structure learning for interactive image retrieval","authors":"Lei Bao, Juan Cao, Tian Xia, Yongdong Zhang, Jintao Li","author_ids":"4050343, 7468114, 1737585, 1699819, 8722263","abstract":"A successful interactive image retrieval system is expected to quickly return as many relevant results as possible while costing less users' effort. Considering these system demands, firstly we propose a novel semi-supervised learning algorithm called Locally Non-negative Linear Structure Learning (LNLS), which is based on the assumption that the labels of each data should be sufficiently smooth with respect to the locally non-negative linear structure of dataset. It has two main merits: first, it is robust to the small sample learning problem since it learns structure from both labeled and unlabeled data; second, by emphasizing the non-negativity of locally linear structure, this algorithm preserves the non-negative inherent characteristic of image data and can truly reveal the intrinsic structure of the images corpus, especially the asymmetric relationship between images. Meanwhile, we explore an online updating algorithm for LNLS to tackle the large computation cost. Thus the model can be generalized to the new queries or the newly-labeled samples without retraining. Furthermore, an active learning method for LNLS is proposed to make the most of users' effort to improve the learner. The encouraging experimental results demonstrate the effectiveness and efficiency of our proposed methods.","cites":"1","conferencePercentile":"19.83471074"},{"venue":"ACM Multimedia","id":"664ccdcc614a8ecfbfedadc7b42b9537fe43d3f1","venue_1":"ACM Multimedia","year":"2008","title":"Probabilistic integration of sparse audio-visual cues for identity tracking","authors":"Keni Bernardin, Rainer Stiefelhagen, Alexander H. Waibel","author_ids":"1701229, 1742325, 4500589","abstract":"In the context of smart environments, the ability to track and identify persons is a key factor, determining the scope and flexibility of analytical components or intelligent services that can be provided. While some amount of work has been done concerning the camera-based tracking of multiple users in a variety of scenarios, technologies for acoustic and visual identification, such as face or voice ID, are unfortunately still subjected to severe limitations when distantly placed sensors have to be used. Because of this, reliable cues for identification can be hard to obtain without user cooperation, especially when multiple users are involved.\n In this paper, we present a novel technique for the tracking and identification of multiple persons in a smart environment using distantly placed audio-visual sensors. The technique builds on the opportunistic integration of tracking as well as face and voice identification cues, gained from several cameras and microphones, whenever these cues can be captured with a sufficient degree of confidence. A probabilistic model is used to keep track of identified persons and update the belief in their identities whenever new observations can be made. The technique has been systematically evaluated on the CLEAR Interactive Seminar database, a large audio-visual corpus of realistic meeting scenarios captured in a variety of smart rooms.","cites":"6","conferencePercentile":"53.89908257"},{"venue":"ACM Multimedia","id":"95a94b247052788f0649808749586789b4ffe0b6","venue_1":"ACM Multimedia","year":"2004","title":"Disruption-tolerant content-aware video streaming","authors":"Tiecheng Liu, Srihari Nelakuditi","author_ids":"5107433, 1686965","abstract":"Communication between a pair of nodes in the network may get disrupted due to failures of links/nodes resulting in zero effective bandwidth between them during the recovery period. It has been observed that such disruptions are not too uncommon and may last from tens of seconds to minutes. Even an occasional such disruption can drastically degrade the viewing experience of a participant in a video streaming session particularly when a sequence of frames central to the story are lost during the disruption. The conventional approach of prefetching video frames and patching lost ones with retransmissions is not always viable when disruptions are localized and experienced only by a few among many receivers. Error spreading approaches that distribute the losses across the video work well only when the disruptions are quite short. As a better alternative, we propose a disruption-tolerant content-aware video streaming approach that combines the techniques of content summarization and error spreading to enhance viewers experience even when the disruptions are long. We introduce the notion of \"substitutable content summary frames\" and provide a method to select these frames and also their transmission order to mitigate the impact of a disruption. In the event of a disruption, the already received summary frames are played by the client during disruption and near normal playback is resumed after the disruption. We evaluate our approach and demonstrate that it provides acceptable viewing experience with minimal startup latency and client buffer.","cites":"6","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"a3d86052a5f2558b08f63b9a2d1949074f861e2a","venue_1":"ACM Multimedia","year":"1997","title":"Constraints for the Web","authors":"Alan Borning, Richard Kuang-Hsu Lin, Kim Marriott","author_ids":"1725422, 2348344, 1768695","abstract":"Constraints can be used to specify the desired layout of a web document, and also the behaviour of embedded applets. We present a system architecture in which both the author and the viewer can impose page layout constraints, some required and some preferential. The final appearance of the web page is thus the result of negotiation between author and viewer, where this negotiation is carried out by solving the set of required and preferential constraints imposed by both parties. We identify two plausible system architec-tures, based on different ways of dividing the work of constraint solving between web server and web client. Finally, we describe an implementation of a prototype constraint-based web authoring system and viewer, which also provides constraint-based embedded applets. 1 Introduction The explosive growth of the web has demonstrated the power of this new medium. However, current web authoring tools allow the author to create documents using the fixed set of available HTML codes, but not to specify complex relationships among parts of the document in any convenient way. On the viewing side, web documents are often less flexible than one might like. Typically the user of a web browser has only small control over the~appearance of the presented information the viewer can resize the browser or set default font information, but not much more. More sophisticated interactions are available, but only as a special case when provided by the author, by lilling in a form, or interacting with an applet embedded in the document. A constraint is simply a statement of a relation (in the mathematical sense) that we would like to have hold. Constraints have been used for many years in interactive graph-ical applications for such things as specifying window and page layout, specifying relationships among parts of a drawing , specifying animations, maintaining consistency between Permission lo wake digiWhrd copies ofnll or pa11 ofthis mnterinl for personnl or chssroom use is granted without 1Pe provided thnt he copies nre not made or distributed li)r pro13 or commercial ndvrmtnp, the copyright notice, the title ofthe publica:llion and its date appear, and nolice is given that copyright is by petmission oflhe ACM. Inc. To copy othcnvise, to republish. IO post on servers or to redistribucl: IO lists, requires specitic permission nndlor fee. ACM Mullimedin 97 .%r~tl/t! bl'nshington (LsA application data and a view of that data, maintaining consistency between multiple views, and representing physical laws in …","cites":"30","conferencePercentile":"70.23809524"},{"venue":"ACM Multimedia","id":"52c12c00cda57d32e871d7ba65e2f9bdd2a00b06","venue_1":"ACM Multimedia","year":"1999","title":"A QoS architecture for collaborative virtual environments","authors":"Chris Greenhalgh, Steve Benford, Gail Reynard","author_ids":"1714469, 1738239, 3128241","abstract":"We present a QoS architecture for collaborative virtual environments (CVEs), focusing on the management of streamed video within shared virtual worlds. Users express QoS requirements by negotiating levels of mutual awareness using our previously defined spatial model of interaction. The architecture uses these awareness values as part of dynamic QoS management. A key aspect of the architecture is that it maintains a balance between the needs of a group of users as a whole (e.g., which streams are admitted onto a shared network) versus those of individual users within the group (e.g., which streams are subscribed to by a local host). We walk through a demonstration scenario, a virtual shopping mall, to show the architecture at work.","cites":"18","conferencePercentile":"66.80672269"},{"venue":"ACM Multimedia","id":"e005ee666f446de49a17832d808e691b3cff7eee","venue_1":"ACM Multimedia","year":"2016","title":"GeoTracks: Adaptive Music for Everyday Journeys","authors":"Chris Greenhalgh, Adrian Hazzard, Sean McGrath, Steve Benford","author_ids":"1714469, 3237858, 2097730, 1738239","abstract":"Listening to music on the move is an everyday activity for many people. This paper proposes geotracks and geolists, music tracks and playlists of existing music that are aligned and adapted to specific journeys. We describe how everyday walking journeys such as commutes to work and existing popular music tracks can each be analysed, decomposed and then brought together, using musical adaptations including skipping and repeating parts of tracks, dynamically remixing tracks and cross-fades. Using a naturalistic experiment we compared walking while listening to geotracks (dynamically adapted using GPS location information) to walking while listening to a fixed playlist. Overall, participants enjoyed the walk more when listening to the adaptive geotracks. However adapting the lengths of tracks appeared to detract from the experience of the music in some situations and for some participants, revealing trade-offs in achieving fine-grained alignment of music and walking journeys.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"328f6c6e1679998bb33b531806757609c82eeaac","venue_1":"ACM Multimedia","year":"2012","title":"Human-computer dance interaction with realtime accelerometer control","authors":"Takuya Yasunaga, Atsushi Nakazawa, Haruo Takemura","author_ids":"8738160, 3287897, 1748743","abstract":"Motion-capture-based character animations are widely used in computer graphics and interactive games.In this paper, we show a novel approach to create dancing character animations that react to input music and an accelerometer manipulated by a user. Since the sensor reads express intensities of users' body movements, the system can synthesize character motions whose intensities are synchronized to those of users. Our system consists of analysis phase and synthesis phase. In the analysis phase, the musical beat and segments are detected from input sound, and motion rhythm and intensities are found from motion capture data. With the results of this analysis, we generate a motion graph that can generate character motions matched to the musical rhythm. In synthesis phase, the system receives the output data from an accelerometer and traverses the motion graph according to the matching result between the sensor data and the motion intensity. As the result, our system adds an interactive component to live dancing performed by virtual characters.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"1f3911701d56c8c2d8f0c19ef1f3fdee3dd00094","venue_1":"ACM Multimedia","year":"2011","title":"Efficient multi-modal retrieval in conceptual space","authors":"Jun Imura, Teppei Fujisawa, Tatsuya Harada, Yasuo Kuniyoshi","author_ids":"2685485, 2401197, 1790553, 1744602","abstract":"In this paper, we propose a new, efficient retrieval system for large-scale multi-modal data including video tracks. With large-scale multi-modal data, the huge data size and various contents cause degradation of efficiency and precision of retrieval results. Recent research on image annotation and retrieval shows that image features based on the Bag-of-Visual Words approach with local descriptors such as SIFT perform surprisingly well with large-scale image datasets. Those powerful descriptors tend to be high-dimensional, imposing a high computational cost for approximate nearest neighbor searching in raw feature space. Our video retrieval method is focused on the correlation between image, sound, and location information recorded simultaneously, and to learn conceptual space describing the contents of the data to realize efficient searching. Experiments show good performance of our retrieval system with low memory usage and temporal complexity.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"455f0f8e642955da1e554c0558e6039c90efc990","venue_1":"ACM Multimedia","year":"1999","title":"Passive capture and structuring of lectures","authors":"Sugata Mukhopadhyay, Brian Christopher Smith","author_ids":"2944910, 1937274","abstract":"Despite recent advances in authoring systems and tools, creating multimedia presentations remains a labor-intensive process. This paper describes a system for automatically constructing structured multimedia documents from live presentations. The automatically produced documents contain synchronized and edited audio, video, images, and text. Two essential problems, synchronization of captured data and automatic editing, are identified and solved.","cites":"187","conferencePercentile":"100"},{"venue":"ACM Multimedia","id":"9bfa80bbcdd376fdd9ccd5a0cdb105de2ce4d9ea","venue_1":"ACM Multimedia","year":"2004","title":"BiReality: mutually-immersive telepresence","authors":"Norman P. Jouppi, Subu Iyer, Stan Thomas, April Slayden Mitchell","author_ids":"1715454, 3065747, 7138560, 1886807","abstract":"BiReality (a.k.a. Mutually-Immersive Telepresence) uses a teleoperated robotic surrogate to provide an immersive telepresence system for face-to-face interactions. Our goal is to recreate to the greatest extent practical, both for the user and the people at the remote location, the sensory experience relevant for face-to-face interactions of the user actually being in the remote location. Our system provides a 360-degree surround immersive audio and visual experience for both the user and remote participants, and streams eight 704x480 MPEG-2 coded videos totaling 20Mb/s. The system preserves gaze and eye contact, presents local and remote participants to each other at life size, and preserves the head height of the user at the remote location. Initial user experiences are presented.","cites":"13","conferencePercentile":"68.62745098"},{"venue":"ACM Multimedia","id":"d75826ee01d7337e0cf92120fd8fa1c9cefb9b4e","venue_1":"ACM Multimedia","year":"1999","title":"Automatic modeling of a 3D city map from real-world video","authors":"Hiroshi Kawasaki, Tomoyuki Yatabe, Katsushi Ikeuchi, Masao Sakauchi","author_ids":"1710962, 2123639, 1739452, 1780232","abstract":"Mixed reality (MR) systems which integrate the virtual world and the real world have become a major topic in the research area of multimedia. As a practical application of these MR systems, we propose an efficient method for making a 3D map from real-world video data. The proposed method is an automatic organization method focusing on video objects to describe video data in an efficient way, i.e., by collating the real-world video data with map information using DP matching. To demonstrate the reliability of this method, we describe successful experiments that we performed using 3D information obtained from the real-world video data.","cites":"14","conferencePercentile":"62.18487395"},{"venue":"ACM Multimedia","id":"3f15c4fcef59798814b9a429a78af2672408b184","venue_1":"ACM Multimedia","year":"2000","title":"Stochastic resource prediction and admission for interactive sessions on multimedia servers","authors":"Matthias Friedrich, Silvia von Stackelberg, Karl Aberer","author_ids":"2862761, 2470995, 1751802","abstract":"In highly interactive multimedia applications startup latency is significant, and may negatively impact performance and Quality of Service (QoS). To avoid this, our approach is to admit whole multimedia sessions instead of single media streams. For the prediction of the varying resource demands within a session, which are mainly correlated to user behavior, we model user behavior as Continuous Time Markov Chains (CTMCs). In this paper, we propose a mathematical analysis of the CTMC model. This allows to anticipate possible overload and in turn to plan an admission control policy. As a result, our approach provides better control on the tradeoff between server utilization and QoS. Simulation studies confirm this capability.","cites":"7","conferencePercentile":"52.7173913"},{"venue":"ACM Multimedia","id":"871f43a70f3171768734e0c2add1f263b8f47a70","venue_1":"ACM Multimedia","year":"2004","title":"Manifold-ranking based image retrieval","authors":"Jingrui He, Mingjing Li, HongJiang Zhang, Hanghang Tong, Changshui Zhang","author_ids":"2770331, 8392859, 1718558, 8163721, 1700883","abstract":"In this paper, we propose a novel transductive learning framework named manifold-ranking based image retrieval (MRBIR). Given a query image, MRBIR first makes use of a manifold ranking algorithm to explore the relationship among all the data points in the feature space, and then measures relevance between the query and all the images in the database accordingly, which is different from traditional similarity metrics based on pair-wise distance. In relevance feedback, if only positive examples are available, they are added to the query set to improve the retrieval result; if examples of both labels can be obtained, MRBIR discriminately spreads the ranking scores of positive and negative examples, considering the asymmetry between these two types of images. Furthermore, three active learning methods are incorporated into MRBIR, which select images in each round of relevance feedback according to different principles, aiming to maximally improve the ranking result. Experimental results on a general-purpose image database show that MRBIR attains a significant improvement over existing systems from all aspects.","cites":"169","conferencePercentile":"99.50980392"},{"venue":"ACM Multimedia","id":"c982a546e0ba11ba38968fc76fab58b2b535e22f","venue_1":"ACM Multimedia","year":"2016","title":"Personal Multi-view Viewpoint Recommendation based on Trajectory Distribution of the Viewing Target","authors":"Xueting Wang, Kensho Hara, Yu Enokibori, Takatsugu Hirayama, Kenji Mase","author_ids":"8417187, 3301241, 1730188, 2639141, 1722602","abstract":"Multi-camera videos with abundant information and high flexibility are expected to be useful in a wide range of applications, such as surveillance systems, web lecture broadcasting, concerts and sports viewing, etc. Viewers can enjoy a high-presence viewing experience of their own choosing by means of virtual camera switching and controlling viewing interfaces. However, some viewers may feel annoyed by continual manual viewpoint selection, especially when the number of selectable viewpoints is relatively large. In order to solve this issue, we propose an automatic viewpoint-recommending method designed especially for soccer games. This method focuses on a viewer's personal preference for viewpoint-selection, instead of common and professional editing rules. We assume that the different trajectory distributions cause a difference in the viewpoint selection according to personal preference. We therefore analyze the relationship between the viewer's personal viewpoint selecting tendency and the spatio-temporal game context. We compare methods based on a Gaussian mixture model, a general histogram+SVM and bag-of-words+SVM to seek the best representation for this relationship. The performance of the proposed methods are verified by assessing the degree of similarity between the recommended viewpoints and the viewers' edited records.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"0e3e781e58790082837b5de59758dbd587850df9","venue_1":"ACM Multimedia","year":"2012","title":"Modeling the qoe of rate changes in SKYPE/SILK VoIP calls","authors":"Chien-Nan Chen, Cing-yu Chu, Su-Ling Yeh, Hao-Hua Chu, Polly Huang","author_ids":"2963347, 1795820, 1793663, 1698140, 1700573","abstract":"The effective end-to-end transport of delay-sensitive voice data has long been a problem in multimedia networking. One of the major issues is determining the sending rate of real-time VoIP streams such that the user experience is maximized per unit network resource consumed. A particularly interesting complication that remains to be addressed is that the available bandwidth is often dynamic. Thus, it is unclear whether a marginal increase warrants better user experience. If a user naively tunes the sending rate to the optimum at any given opportunity, the user experience could fluctuate.\n To investigate the effects of magnitude and frequency of rate changes on user experience, we recruited 127 human participants to systematically score emulated Skype calls with different combinations of rate changes, including varying magnitude and frequency of rate changes. Results show that 1) the rate change frequency affects the user experience on a logarithmic scale, echoing Weber-Fechner's Law [1], 2) the effect of rate change magnitude depends on how users perceive the quality difference, and 3) this study derives a closed-form model of user perception for rate changes for Skype calls.","cites":"3","conferencePercentile":"56.64556962"},{"venue":"ACM Multimedia","id":"441a50ab05ea2e6301160cfd5172cddf9574d63e","venue_1":"ACM Multimedia","year":"2008","title":"Interactive spatial multimedia for communication of art in the physical museum space","authors":"Karen Johanne Kortbek, Kaj Grønbæk","author_ids":"2643631, 1687315","abstract":"This paper discusses the application of three spatial multimedia techniques for communication of art in the physical museum space. In contrast to the widespread use of computers in cultural heritage and natural science museums, it is generally a challenge to introduce technology in art museums without disturbing the art works. This has usually been limited to individual audio guides. In our case we strive to achieve holistic and social experiences with seamless transitions between art experience and communication related to the artworks.\n To reach a holistic experience with minimal disturbance of the artworks we apply three spatial multimedia techniques where the only interaction device needed is the human body. The three techniques are: 1) spatially bounded audio; 2) floor-based multimedia; 3) multimedia interior. The paper describes the application of these techniques for communication of information in a Mariko Mori exhibition. The multimedia installations and their implementation are described. It is argued that the utilization of the spatial multimedia techniques support holistic and social art experience. The multimedia installations were in function for a three and a half month exhibition period and they were approved on beforehand by the artist to be in concordance with the artworks.","cites":"18","conferencePercentile":"80.73394495"},{"venue":"ACM Multimedia","id":"ab935dfcee008391231f3f875d292f98a462898f","venue_1":"ACM Multimedia","year":"2011","title":"Refining local descriptors by embedding semantic information for visual categorization","authors":"Yingbin Zheng, Renzhong Wei, Hong Lu, Xiangyang Xue","author_ids":"3015119, 2978227, 3655082, 5507458","abstract":"Local descriptor extraction and vector quantization are the important components of widely-used Bag-of-Features (BoF) model for visual categorization. This paper proposes a simple and efficient approach to refine the local descriptors for vector quantization by embedding semantic information. The original local descriptors are integrated by a sequence of category-independent and category-dependent basis. Particularly, the category-dependent basis is learned by minimizing the joint loss minimization over local descriptors from different categories with a shared regularization penalty, which can be formulated as a linear programming problem. The transferred descriptors are further quantized and aggregated to the visual vocabulary. Experiments are performed on PASCAL VOC 2007 benchmark and the quantitative comparisons with several state-of-the-art approaches demonstrate the effectiveness of our proposed approach.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"7b62d43a8d7c790f558f2ecfbe5a7b26213d7ed0","venue_1":"ACM Multimedia","year":"2014","title":"Matrix Completion for Cross-view Pairwise Constraint Propagation","authors":"Zheng Yang, Yao Hu, Haifeng Liu, Huajun Chen, Zhaohui Wu","author_ids":"1785641, 3598350, 1678964, 1729778, 1687635","abstract":"As pairwise constraints are usually easier to access than label information, pairwise constraint propagation attracts more and more attention in semi-supervised learning. Most existing pairwise constraint propagation methods are based on canonical graph propagation model, which heavily depends on the edge weights in the graph and cannot preserve local and global consistency simultaneously. In order to address this drawback, we cast cross-view pairwise constraint propagation into a problem of low rank matrix completion and propose a Matrix Completion method for cross-view Pairwise Constraint Propagation(MCPCP). With low rank requirement and graph regularization, our MCPCP can preserve local and global consistency simultaneously. We develop an algorithm based on alternating direction method of multipliers(ADMM) to solve the optimization problem. Finally, the effectiveness of MCPCP is demonstrated in cross-view multimedia retrieval.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"701563ed248cfe5b8dba05228aac5049dee0d975","venue_1":"ACM Multimedia","year":"2002","title":"Post-hoc worknotes: a concept demo of video content management","authors":"Ola Andersson, Elenor Cacciatore, Jonas Löwgren, Thomas Lundin","author_ids":"2630053, 1806784, 2547367, 2865756","abstract":"<i>Post-hoc worknotes</i> is a concept demonstration, an envisionment showing how workgroup communication could be supported using a combination of existing technologies in the field of nontextual information management. We have identified a number of use-driven requirements for video content management, including the tailoring of metadata structures to project and process models.","cites":"3","conferencePercentile":"25.64102564"},{"venue":"ACM Multimedia","id":"4a56cd5f9d0959438c4873f4969b99c68f30a840","venue_1":"ACM Multimedia","year":"2004","title":"Image-based modeling and rendering with geometric proxy","authors":"Angus M. K. Siu, Rynson W. H. Lau","author_ids":"2356246, 1726262","abstract":"In this paper, we present an image-based method to recover a geometric proxy and generate novel views. We use an integrated modeling and rendering approach to deal with the difficulty of modeling, and reduce the sampling rate. Our system is based on two novel techniques. First, we propose the &#60;i>Adaptive Mesh Segmentation&#60;/i> (AMS) technique for recovering geometric proxy of a scene environment. Second, we propose the &#60;i>Trifocal Morphing&#60;/i> technique for efficient rendering with the geometric proxy, which can handle non-matched regions of the scene. Our method allows images to be sparsely captured and thus highly reduces the manual image acquisition effort as well as the data size.","cites":"0","conferencePercentile":"7.107843137"},{"venue":"ACM Multimedia","id":"11461006cc595687869a1e7f3f25dddad86f0cc9","venue_1":"ACM Multimedia","year":"2011","title":"Level influence of spatial pyramid matching in object classification","authors":"Hong Lu, Renzhong Wei, Yanran Shen, Xiangyang Xue","author_ids":"3655082, 2978227, 2812270, 5507458","abstract":"In this paper we propose to effectively consider the shape and size variations for object classification. Specifically, a novel image matching method is proposed to incorporate the image segmentation with Spatial Pyramid Matching (SPM), and test our method on flower classification. A Level Influence Factor (LIF) is introduced to represent weights of different pyramid levels based on the statistical information of each segmented image. Then the images are classified based on the LIF weighted spatial pyramid bag-of-visual-words feature, and some levels with weight values zeros are not needed to be compared further. Also, in SPM matching stage, the block in one image is compared with not only its corresponding block in another image, but also the spatially neighboring blocks of the corresponding blocks to find the best match. This fuzzy matching method can incorporate some translation of objects. Experiments are performed on a flower dataset containing 1360 images from 17 different categories. And experimental results demonstrate that our proposed method has better time efficiency than traditional SPM and outperforms the state-of-art flower classification methods.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"7b6409a59fec42af4eacb3d5c5d032769c175137","venue_1":"ACM Multimedia","year":"2011","title":"Creating personalized memories from social events: community-based support for multi-camera recordings of school concerts","authors":"Rodrigo Laiola Guimarães, Pablo César, Dick C. A. Bulterman, Vilmos Zsombori, Ian Kegel","author_ids":"1693206, 1743507, 1726923, 3179371, 2883994","abstract":"The wide availability of relatively high-quality cameras makes it easy for many users to capture video fragments of social events such as concerts, sports events or community gatherings. The wide availability of simple sharing tools makes it nearly as easy to upload individual fragments to on-line video sites. Current work on video mashups focuses on the creation of a video summary based on the characteristics of individual media fragments, but it fails to address the interpersonal relationships and time-variant social context within a community of diverse (but related) users. The aim of this paper is to reformulate the research problem of video authoring, by investigating the social relationships of the media 'authors' relative to the performers. Based on a 10-month evaluation process, we specify a set of guidelines for the design and implementation of socially-aware video editing and sharing tools. Our contributions have been realized and evaluated in a prototype software that enables community-based users to navigate through a large common content space and to generate highly personalized video compilations of targeted interest within a social circle. According to the results, a system like ours is a valid alternative for social interactions when apart. We hope that our insights can stimulate future research on socially-aware multimedia tools and applications.","cites":"8","conferencePercentile":"77.8425656"},{"venue":"ACM Multimedia","id":"05ac6584924102e948cde41ab01d44088b1381ce","venue_1":"ACM Multimedia","year":"1993","title":"Phoneshell: The Telephone as Computer Terminal","authors":"Chris Schmandt","author_ids":"1729321","abstract":"This paper describes Phoneshell, a telephone based application providing remote voice access to personal desktop databases such as voice mail, email, calendar, and rolodex. Several forms of information can also be faxed on demand. Phoneshell offers its users numerous opportunities to record voice entries into its underlying databases; this new utility for stored voice as a data type, necessitates multimedia support for the traditional graphical user interfaces to these same databases. The experiences of a small Phoneshell user community are discussed, with emphasis on key features which are most important to its success. The underlying software architecture used by Phoneshell includes a toolkit for building interactive telephone-based services. At the airport on the way home your flight is posted with a 20 minute delay, hardly time to hook up a modem; instead, you just call your office computer on the nearest pay phone. After hearing a new voice mail message, you turn your attention to your email; it has been filtered and you have time for only the most important messages, which are read to you by a speech synthesizer. Darn, that new sponsor wants to come visit next Tuesday. You check your calendar, and see that you are free except for lunch with a friend from across town. You record an entry for the sponsor visit into the calendar so that your secretary will keep the day free. Then you switch to your rolodex, and look up your friend. Since you don't have time for a conversation, you don't call her to reschedule lunch, but instead record a voice message which will be sent to her as email. You have no additional important messages, but without hanging up the phone you call home through your own rolodex, and leave a message on the machine there saying that you'll miss dinner. Elapsed time: 4 minutes; still time to find some inedible airport food.","cites":"36","conferencePercentile":"69.76744186"},{"venue":"ACM Multimedia","id":"0fe0fc90d12c0107b60e357d5f921f1febb6c9e3","venue_1":"ACM Multimedia","year":"2010","title":"Semantic video indexing by fusing explicit and implicit context spaces","authors":"Yingbin Zheng, Renzhong Wei, Hong Lu, Xiangyang Xue","author_ids":"3015119, 2978227, 3655082, 5507458","abstract":"This paper addresses the problem of context-based concept fusion (CBCF) for concept detection and semantic video indexing. We introduce a novel framework based on constructing context spaces of concepts, such that the contextual correlations are used to improve the performance of concept detectors. Different from traditional CBCF approach, we present two kinds of such context spaces: explicit context space for modeling the correlation of pairwise concepts, and implicit context space for representing latent themes trained from a set of concepts. The final concept detection scores are then directly fused from explicit and implicit context spaces. Experiments are presented on TRECVid 2006 benchmark and the comparisons with several state-of-the-art approaches demonstrate the effectiveness of proposed framework.","cites":"2","conferencePercentile":"40.4109589"},{"venue":"ACM Multimedia","id":"9d32b9bf5f7ec86bc876216c4271f18fe68e7dcd","venue_1":"ACM Multimedia","year":"2015","title":"Online Object Tracking Based on CNN with Metropolis-Hasting Re-Sampling","authors":"Xiangzeng Zhou, Lei Xie, Peng Zhang, Yanning Zhang","author_ids":"1950522, 4043707, , 1801395","abstract":"Tracking-by-learning strategies have been effective in solving many challenging problems in visual tracking, in which the learning sample generation and labeling play important roles for final performance. Since the concern of deep learning based approaches has shown an impressive performance in different vision tasks, how to properly apply the learning model, such as CNN, to an online tracking framework is still challenging. In this paper, to overcome the overfitting problem caused by straight-forward incorporation, we propose an online tracking framework by constructing a CNN based adaptive appearance model to generate more reliable training data over time. With a reformative Metropolis-Hastings re-sampling scheme to reshape particles for a better state posterior representation during online learning, the proposed tracking outperforms most of the state-of-art trackers on challenging benchmark video sequences.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"66040846f0313bab2986da1b699344e368acaed5","venue_1":"ACM Multimedia","year":"2010","title":"Supporting personal photo storytelling for social albums","authors":"Pere Obrador, Rodrigo de Oliveira, Nuria Oliver","author_ids":"2953301, 8641731, 1692808","abstract":"Information overload is one of today's major concerns. As high-resolution digital cameras become increasingly pervasive, unprecedented amounts of social media are being uploaded to online social networks on a daily basis. In order to support users on selecting the best photos to create an online photo album, attention has been devoted to the development of automatic approaches for photo storytelling. In this paper, we present a novel photo collection summarization system that learns some of the users' social context by analyzing their online photo albums, and includes storytelling principles and face and image aesthetic ranking in order to assist users in creating new photo albums to be shared online. In an in-depth user study conducted with 12 subjects, the proposed system was validated as a first step in the photo album creation process, helping users reduce workload to accomplish such a task. Our findings suggest that a human audio/video professional with cinematographic skills does not perform better than our proposed system.","cites":"19","conferencePercentile":"87.80821918"},{"venue":"ACM Multimedia","id":"34f1e09bc88b32d307aad43fa08f106e32b259a2","venue_1":"ACM Multimedia","year":"2014","title":"Dynamic Resource Management in Cloud-based Distributed Virtual Environments","authors":"Yunhua Deng, Siqi Shen, Zhe Huang, Alexandru Iosup, Rynson W. H. Lau","author_ids":"8165062, 2959135, 4169506, 1760940, 1726262","abstract":"As an elastic hosting platform, cloud computing has been attracting many attentions for transferring compute-intensive applications from static self-hosting to flexible cloud-based hosting. Distributed virtual environments (DVEs) which typically involve massive users interacting at the same time and feature significant workload dynamics either in spatial due to in-game user mobility or in temporal due to the fluctuating user population, potentially are suitable applications with cloud-based hosting because of the need of resource elasticity. We explore the dynamic resource management for cloud-based DVEs by taking into account their multi-level workload dynamics which differ them from other applications. Simulation results demonstrates the advantages of our developed methods over existing ones.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"9b3f8bba48b5bdaac58bf26e0a03a3b0af167c83","venue_1":"ACM Multimedia","year":"2012","title":"DVS: a dynamic multi-video summarization system of sensor-rich videos in geo-space","authors":"Ying Zhang, Roger Zimmermann","author_ids":"1752812, 1790974","abstract":"It is now very easy to produce user generated videos (UGV) due to progress in camera and recording technologies on mobile devices, such as smartphones. Additionally, the ubiquitous, built-in sensors in digital devices can greatly enrich these videos with sensor descriptions, for example geo-spatial properties. A repository of such sensor-rich videos is a valuable source of information for prospective tourists when they plan to visit a city and would like to get a preview of its main attractions. Inspired by this, we have built an interactive geo-video search system. On a given map, a user specifies a start point and a destination and the system dynamically retrieves a video summarization along the path between the two points. Moreover, the user can interactively update the query during the video playback by dragging the route on the map. The main features of our technique are, first, that it is fully automatic and leverages sensor meta-data information which is acquired in conjunction with videos. Second, the system dynamically adapts to query updates in real-time. Third, a concise but comprehensive summarization from multiple user generated videos is presented for any queried route.","cites":"2","conferencePercentile":"46.99367089"},{"venue":"ACM Multimedia","id":"917ab5384aefc7e81fb18553de473f91cb3396c1","venue_1":"ACM Multimedia","year":"2010","title":"Exploring touch and breath in networked wearable installation design","authors":"Thecla Schiphorst, Jinsil Seo, Norman Jaffe","author_ids":"2689742, 3181464, 2909113","abstract":"This paper describes the artistic design concepts for the interactive wearable artworks <i>tendrils</i> and <i>exhale</i> exhibited at ACM Multimedia 2010 Interactive Art Program in Firenze Italy at the at the Palazzo Medici-Riccardi from 25 October through 6 November, 2010. These wearable art works are based in artistic exploration influenced by the <i>somatic turn</i>: an approach to designing for experience using body based practices that highlight the concept of <i>somaesthetics</i> as an approach to the design of expressive interaction. The interactive wearable art installations <i>tendrils</i> and <i>exhale</i> emphasize the experience of self-observation, poetics, materiality, and computational semantics that support sensory input such as touch and breath. In the context of interaction, somaesthetics offers a bridging strategy between embodied practices based in somatics, and the design for an aesthetics of interaction in wearable technology. These artworks illustrate the value of exploring artistic design strategies that employ a somaesthetic approach, and exemplify this approach in the design of a networked, wearable interactive art.","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"838be799c848915bd1e87ffd7609abf46c21154a","venue_1":"ACM Multimedia","year":"2013","title":"euTV: a system for media monitoring and publishing","authors":"Marco Bertini, Alberto Del Bimbo, George Ioannidis, Emile Bijk, Isabel Trancoso, Hugo Meinedo","author_ids":"1801509, 8196487, 3498173, 2211142, 1691021, 1748419","abstract":"In this paper, we describe the euTV system, which provides a flexible approach to collect, manage, annotate and publish collections of images, videos and textual documents. The system is based on a Service Oriented Architecture that allows to combine and orchestrate a large set of web services for automatic and manual annotation, retrieval, browsing, ingestion and authoring of multimedia sources. euTV tools have been used to create several publicly available vertical applications, addressing different use cases. Positive results of user evaluations have shown that the system can be effectively used to create different types of applications.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"860fa468dadc13ae699072d51224d6ccfbccabe1","venue_1":"ACM Multimedia","year":"2010","title":"Requirements and design space for interactive public displays","authors":"Jörg Müller, Florian Alt, Daniel Michelis, Albrecht Schmidt","author_ids":"1798163, 1693046, 1973526, 1678329","abstract":"Digital immersion is moving into public space. Interactive screens and public displays are deployed in urban environments, malls, and shop windows. Inner city areas, airports, train stations and stadiums are experiencing a transformation from traditional to digital displays enabling new forms of multimedia presentation and new user experiences. Imagine a walkway with digital displays that allows a user to immerse herself in her favorite content while moving through public space. In this paper we discuss the fundamentals for creating exciting public displays and multimedia experiences enabling new forms of engagement with digital content. Interaction in public space and with public displays can be categorized in phases, each having specific requirements. Attracting, engaging and motivating the user are central design issues that are addressed in this paper. We provide a comprehensive analysis of the design space explaining mental models and interaction modalities and we conclude a taxonomy for interactive public display from this analysis. Our analysis and the taxonomy are grounded in a large number of research projects, art installations and experience. With our contribution we aim at providing a comprehensive guide for designers and developers of interactive multimedia on public displays.","cites":"145","conferencePercentile":"99.45205479"},{"venue":"ACM Multimedia","id":"31ee3ce5bbff0be0fd6d209d20dabc374df96bda","venue_1":"ACM Multimedia","year":"2011","title":"myUnity: a new platform to support communication in the modern workplace","authors":"Jacob T. Biehl, Thea Turner, William van Melle, Andreas Girgensohn","author_ids":"7457467, 2729644, 7870081, 2195286","abstract":"Modern office work practices increasingly breach traditional boundaries of time and place, making it difficult to interact with colleagues. To address these problems, we developed myUnity, a software and sensor platform that enables rich workplace awareness and coordination. myUnity is an integrated platform that collects information from a set of independent sensors and external data aggregators to report user location, availability, tasks, and communication channels. myUnity's sensing architecture is component-based, allowing channels of awareness information to be added, updated, or removed at any time. Our current system includes a variety of sensor and data input, including camera-based activity classification, wireless location trilateration, and network activity monitoring. These and other input channels are combined and composited into a single, highlevel presence state. Early studies of a myUnity deployment have demonstrated that use of the platform allows quick access to core awareness information and show its utility in supporting communication and collaboration in the modern workplace.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"bcda01b4c6c6f6692b027bd8d89caa005f4e2677","venue_1":"ACM Multimedia","year":"2014","title":"Interactive 3D Animation Creation and Viewing System based on Motion Graph and Pose Estimation Method","authors":"Masayuki Furukawa, Yasuhiro Akagi, Yukiko Kawai, Hiroshi Kawasaki","author_ids":"8263864, 3176012, 1700611, 1710962","abstract":"This paper proposes an interactive 3D animation system specifically aiming efficient control of human motion. However there are various commercial products for creating movies and game contents, those are still difficult to deal with for non-professional users. To ease the creation process and encourage to utilize 3D animation for the general users, e.g., in the field of such as education, medicine and so on, we propose a system using Kinect. The data of skeleton models of human motion estimated by Kinect is processed to generate Motion Graph and finally restructure the data automatically for 3D character models. We also propose an efficient 3D animation viewing system based on touch interface for tablet device, which enables intuitive control of multiple motions of the human activity. To evaluate the effectiveness of the method, we implemented a prototype system and created several 3D animations.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"82f848144447911d300990d747aafcc70bf1f4d7","venue_1":"ACM Multimedia","year":"1998","title":"Performance Analysis of the RIO Multimedia Storage System with Heterogeneous Disk Configurations","authors":"Jose Renato Santos, Richard R. Muntz","author_ids":"1787738, 1704462","abstract":"RIO is a multimedia object server which manages a set of parallel disks and supports real-time data delivery with statistical delay guarantees. RIO uses random data allocation on disks combined with partial replication to achieve load balance and high performance. In this paper we analyze the performance of RIO when the set of disks used to store data blocks is not homogeneous, having both diierent band-widths and diierent storage capacities. The basic problem to be addressed for heterogeneous conngurations is that, on average, the fraction of the load directed to each disk is proportional to the amount of data stored on it, which may not be proportional to the disk bandwidth. This may cause some disks to be overloaded, with long queues and delays, even though bandwidth is available on other disks. This reduces the system throughput or increases the delay bound that can be guaranteed. This problem arises whenever the bandwidth to space ratio (BSR) is not uniform across all disks. In RIO this problem is addressed by replicating a fraction of data blocks. Replication allows some of the load of the disks with smaller BSR to be redirected to the disks with higher BSR, reducing or eliminating the long range load imbalance, depending on the amount of replication. 1 Introduction A prototype real-time storage system that can store and deliver both real-time and non real-time multimedia data to a large number of simultaneous users has been designed and implemented. The system, named RIO (Randomized I/O), supports high data rates and many simultaneous data streams by using the aggregate bandwidth of a collection of parallel disks 12]. RIO is part of the VWDS (Virtual World Data Server) Multimedia System that can simultaneously support many types of multimedia applications such as video on demand, 3D interactive virtual world navigation,","cites":"54","conferencePercentile":"82.69230769"},{"venue":"ACM Multimedia","id":"2d1f6c442f4e69d5d29ef9b87f8a10c075227133","venue_1":"ACM Multimedia","year":"1999","title":"Multimedia research: the grand challenges for the next decade (panel session)","authors":"Wendy Hall, Philippe Aigrain, Dick C. A. Bulterman, Lawrence A. Rowe, Brian Christopher Smith","author_ids":"1685385, 2759584, 1726923, 1723155, 1937274","abstract":"Theme of panel Suppose you have been asked by your research funding council to write a case to support a significant amount of funding being put into a pot called \" multimedia \" over the next 5-10 years, How would you make your case? What would you give as the major success stories in multimedia research over the last 10 years-what has made the most impact on the research and the business community? What are the \" must fund \" areas for the next S-10 years i.e where do we really need breakthroughs for \" multimedia research \" to have a major impact on future systems? What research topics are being worked on now that you think do not warrant the investment? What are the safe-bets that should be funded because it is clear that they are going to deliver results in the not too distant future? What are the risky areas, where breakthroughs don't seem likely at the moment, but if research was successful then the impact would be enormous? Permission to make digital or hard copies of all or parl Of this work for personal or classroom use is granted without fee Provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first Page. To copy otherwise, to republish, tp post on servers Or 10 redistribute to lists, requires prior specific permission and/or a fee. Finally, what is the justification for funding multimedia in the future-can it deliver on its promises, are there real systems beyond the hype? Panelists Philippe Agrain is Head of Sector \" Software Technologies \" in the European Commission Information Society Technologies R&D Programme. He was trained as a mathematician and theoretical scientis, and holds a Doctorat and the Habilitation a Diriger les Recherches from University of Paris 7. From 1972 to 1981, he worked in the software engineering research labs of various software companies. He was a research fellow at U.C.Berkeley in 1982. Since then, and before joining the European Commission in 1996, he headed research teams in the field of computer processing, indexing, retrieval and user interfaces for audiovisual media. He is the author of more than 60 technical and technology assessment papers.","cites":"0","conferencePercentile":"7.983193277"},{"venue":"ACM Multimedia","id":"11d4b65d164db319f4fdcdadaafac764f102ef8e","venue_1":"ACM Multimedia","year":"1997","title":"Motion and Feature-Based Video Metamorphosis","authors":"Robert Szewczyk, Andras Ferencz, Henry Andrews, Brian Christopher Smith","author_ids":"1735911, 3236352, 7689144, 1937274","abstract":"We present a new technique for morphing two video sequences. Our approach extends still image metamorphosis techniques to video by performing motion tracking on the objects. Besides reducing the amount of user input required to morph two sequences by an order of magnitude, the additional motion information helps us to segment the image into foreground and background parts. By morphing these parts independently and overlaying the results, output quality is improved. We compare our approach to conventional motion image morphing techniques in terms of the quality of the output image and the human input required.","cites":"4","conferencePercentile":"16.66666667"},{"venue":"ACM Multimedia","id":"0df25efb9d98b174ebb0128240a15954f5642703","venue_1":"ACM Multimedia","year":"1996","title":"CU-SeeMe VR Immersive Desktop Teleconferencing","authors":"Jefferson Han, Brian Christopher Smith","author_ids":"2244015, 1937274","abstract":"Current video-conferencing systems provide a \" video-in-a-window \" user interface. This paper presents a distributed video-conferencing system called CU-SeeMe VR that embeds live video and audio conferencing in a virtual space. This paper describes a prototype implementation of CU-SeeMe VR, including the user interface, system architecture, and a detailed look at the enabling technologies. Future directions and the implications of the virtual reality metaphor are discussed.","cites":"13","conferencePercentile":"27.77777778"},{"venue":"ACM Multimedia","id":"88b98d7b20ede342fe471b2889ace70d082e4db7","venue_1":"ACM Multimedia","year":"1995","title":"Query by Humming: Musical Information Retrieval in an Audio Database","authors":"Asif Ghias, Jonathan Logan, David Chamberlin, Brian Christopher Smith","author_ids":"2262970, 3132262, 2367791, 1937274","abstract":"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.","cites":"338","conferencePercentile":"95"},{"venue":"ACM Multimedia","id":"70a09ac030ba65e66d7c83789054e545e689ef53","venue_1":"ACM Multimedia","year":"1994","title":"Fast Software Processing of Motion JPEG Video","authors":"Brian Christopher Smith","author_ids":"1937274","abstract":"This paper introduces a set of techniques for processing video data compressed using JPEG compression at near real-time rates on current generation workstations. Performance is improved over traditional methods by processing video data in compressed form, avoiding compression and decompression and reducing the amount of data processed. An approximation technique called <italic>condensation</italic> is developed that further reduces the complexity of the operation. The class of operations that are computable using the techniques developed in this paper are called linear, global digital specials effects (LGDSEs), and represent those effects where a pixel in the output image is a linear combination of pixels in the input image. Many important video processing problems, including convolution, scaling, rotation, translation, and transcoding can be expressed as LGDSEs.","cites":"12","conferencePercentile":"51.69491525"},{"venue":"ACM Multimedia","id":"8d181608e54c630aa5ea7b8c69f276c3727a891b","venue_1":"ACM Multimedia","year":"2015","title":"Click-through-based Deep Visual-Semantic Embedding for Image Search","authors":"Yuan Liu, Zhongchao Shi, Xue Li, Gang Wang","author_ids":"1826185, 2558130, 6990205, 4148672","abstract":"The problem of image search is mostly considered from the perspectives of feature-based vector model and image ranker learning. A fundamental issue that underlies the success of these approaches is the similarity learning between query and image. The need of image surrounding texts in feature-based vector model, however, makes the similarity sensitive to the quality of text descriptions. On the other, the image ranker learning can suffer from robustness problem, originating from the fact that human labeled query-image pairs do not always predict user search intention precisely. We demonstrate in this paper that the above two issues can be well mitigated by jointly exploring visual-semantic embedding and the use of click-through data. Specifically, we propose a novel click-through-based deep visual-semantic embedding (C-DVSE) model for learning query and image similarity. The proposed model consists of two components: a deep convolutional neural networks followed by an image embedding layer for learning visual embedding, and a deep neural networks for generating query semantic embedding. The objective of our model is to maximize the correlation between semantic (query) and visual (clicked image) embedding. When the visual-semantic embedding is learnt, query-image similarity can be directly computed by cosine similarity on this embedding space. On a large-scale click-based image dataset with 11.7 million queries and one million images, our model is shown to be powerful for keyword-based image search with superior performance over several state-of-the-art methods.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"e30eaf2362189df763b8a5c8fc82f85fa5fa9f27","venue_1":"ACM Multimedia","year":"2014","title":"Dynamic Background Learning through Deep Auto-encoder Networks","authors":"Pei Xu, Mao Ye, Xue Li, Qihe Liu, Yi Yang, Jian Ding","author_ids":"1755959, 5792401, 6990205, 2209867, 1698559, 1749386","abstract":"Background learning is a pre-processing of motion detection which is a basis step of video analysis. For the static background, many previous works have already achieved good performance. However, the results on learning dynamic background are still much to be improved. To address this challenge, in this paper, a novel and practical method is proposed based on deep auto-encoder networks. Firstly, dynamic background images are extracted through a deep auto-encoder network (called Background Extraction Network) from video frames containing motion objects. Then, a dynamic background model is learned by another deep auto-encoder network (called Background Learning Network) using the extracted background images as the input. To be more flexible, our background model can be updated on-line to absorb more training samples. Our main contributions are 1) a cascade of two deep auto-encoder networks which can deal with the separation of dynamic background and foregrounds very efficiently; 2) a method of online learning is adopted to accelerate the training of Background Extraction Network. Compared with previous algorithms, our approach obtains the best performance over six benchmark data sets. Especially, the experiments show that our algorithm can handle large variation background very well.","cites":"3","conferencePercentile":"66.26506024"},{"venue":"ACM Multimedia","id":"9d04a1f74fc168e5fc391ff2135c896ade611a52","venue_1":"ACM Multimedia","year":"2003","title":"Video cut editing rule based on participants' gaze in multiparty conversation","authors":"Yoshinao Takemae, Kazuhiro Otsuka, Naoki Mukawa","author_ids":"3003956, 1713258, 1713260","abstract":"This paper proposes a video cut editing rule based on participants' gaze for extracting and conveying the flow of conversation in multiparty conversation. Systems that record meetings and those that support teleconferences are attracting considerable interest. Conventional systems use a fixed-viewpoint camera and simple camera selection based on participants' utterances. However, conventional systems fail to convey a sufficient amount of nonverbal information about the participants and the flow of conversation. We focus on participants' gaze since it is a good indicator of the participants' intent and emotion, conversational attention etc. We propose a video cut editing rule based on the convergence of participants' gaze direction. We conduct an experiment to evaluate the effectiveness of the proposed method. The results indicate that the proposed method can successfully convey who is taking to whom, which is a key indicator of the flow of conversation.","cites":"10","conferencePercentile":"48.1981982"},{"venue":"ACM Multimedia","id":"0ed10b173271dfd068d8804b676c8d013a060d17","venue_1":"ACM Multimedia","year":"2013","title":"Online human gesture recognition from motion data streams","authors":"Xin Zhao, Xue Li, Chaoyi Pang, Xiaofeng Zhu, Quan Z. Sheng","author_ids":"5245201, 6990205, 2445015, 1708731, 1713128","abstract":"Online human gesture recognition has a wide range of applications in computer vision, especially in human-computer interaction applications. Recent introduction of cost-effective depth cameras brings on a new trend of research on body-movement gesture recognition. However, there are two major challenges: i) how to continuously recognize gestures from unsegmented streams, and ii) how to differentiate different styles of a same gesture from other types of gestures. In this paper, we solve these two problems with a new effective and efficient feature extraction method that uses a dynamic matching approach to construct a feature vector for each frame and improves sensitivity to the features of different gestures and decreases sensitivity to the features of gestures within the same class. Our comprehensive experiments on MSRC-12 Kinect Gesture and MSR-Action3D datasets have demonstrated a superior performance than the stat-of-the-art approaches.","cites":"22","conferencePercentile":"95.55555556"},{"venue":"ACM Multimedia","id":"8f4acef3ffda97503cec6d421d3c3a3a467e65d0","venue_1":"ACM Multimedia","year":"2004","title":"Enhancing security of frequency domain video encryption","authors":"Zheng Liu, Xue Li, Zhao Yang Dong","author_ids":"3722707, 6990205, 3171612","abstract":"A potential security problem in frequency domain video encryption is that some trivial information such as the distribution of DCT coefficients may leak out secret. To illuminate this problem, we performed a successful attack on video using the distribution information of DCT coefficients. Then, according to the weak points discovered, a novel video encryption algorithm, working on run-length coded data, is proposed. It has amended identified security problems, while preserving high efficiency and the adaptability to cooperate with compression schemes.","cites":"6","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"c0f9173da7a5746418b424511dafd0dc30c8b720","venue_1":"ACM Multimedia","year":"1993","title":"Performance of a Software MPEG Video Decoder","authors":"Ketan Mayer-Patel, Brian Christopher Smith, Lawrence A. Rowe","author_ids":"3715598, 1937274, 1723155","abstract":"The design and implementation of a software decoder for MPEG video bitstreams is described. The software has been ported to numerous platforms including PC's, workstations, and mainframe computers. Performance comparisons are given for different bitstreams and platforms including a unique metric devised to compare price/performance across different platforms (percentage of required bit rate per dollar). We also show that memory bandwidth is the primary limitation in performance of the decoder, not the computational complexity of the inverse discrete cosine transform as is commonly thought.","cites":"125","conferencePercentile":"93.02325581"},{"venue":"ACM Multimedia","id":"1795ccf31d8bde6aa7bc21707928c37d84e3161f","venue_1":"ACM Multimedia","year":"2001","title":"A robust audio classification and segmentation method","authors":"Lie Lu, Hao Jiang, HongJiang Zhang","author_ids":"1813401, 4795925, 1718558","abstract":"In this paper, we present a robust algorithm for audio classification that is capable of segmenting and classifying an audio stream into speech, music, environment sound and silence. Audio classification is processed in two steps, which makes it suitable for different applications. The first step of the classification is speech and non-speech discrimination. In this step, a novel algorithm based on KNN and LSP VQ is presented. The second step further divides non-speech class into music, environment sounds and silence with a rule based classification scheme. Some new features such as the noise frame ratio and band periodicity are introduced and discussed in detail. Our experiments in the context of video structure parsing have shown the algorithms produce very satisfactory results.","cites":"100","conferencePercentile":"98.97959184"},{"venue":"ACM Multimedia","id":"654d007f2eaa99fba6e11b320d7a2cb9618a3f86","venue_1":"ACM Multimedia","year":"2009","title":"Multi-modal scene segmentation using scene transition graphs","authors":"Panagiotis Sidiropoulos, Vasileios Mezaris, Yiannis Kompatsiaris, Hugo Meinedo, Isabel Trancoso","author_ids":"1726550, 1737436, 1906503, 1748419, 1691021","abstract":"In this work the problem of automatic decomposition of video into elementary semantic units, known in the literature as scenes, is addressed. Two multi-modal automatic scene segmentation techniques are proposed, both building upon the Scene Transition Graph (STG). In the first of the proposed approaches, speaker diarization results are used for introducing a post-processing step to the STG construction algorithm, with the objective of discarding scene boundaries erroneously identified according to visual-only dissimilarity. In the second approach, speaker diarization and additional audio analysis results are employed and a separate audio-based STG is constructed, in parallel to the original STG based on visual information. The two STGs are subsequently combined. Preliminary results from the application of the proposed techniques to broadcast videos reveal their improved performance over previous approaches.","cites":"11","conferencePercentile":"72.10743802"},{"venue":"ACM Multimedia","id":"aff9e0247613971469ed00f9f0dacc901c8c17fb","venue_1":"ACM Multimedia","year":"2011","title":"Prediction of the inter-observer visual congruency (IOVC) and application to image ranking","authors":"Olivier Le Meur, Thierry Baccino, Aline Roumy","author_ids":"1789744, 1731790, 1985157","abstract":"This paper proposes an automatic method for predicting the inter-observer visual congruency (IOVC). The IOVC reflects the congruence or the variability among different subjects looking at the same image. Predicting this congruence is of interest for image processing applications where the visual perception of a picture matters such as website design, advertisement, etc. This paper makes several new contributions. First, a computational model of the IOVC is proposed. This new model is a mixture of low-level visual features extracted from the input picture where model's parameters are learned by using a large eye-tracking database. Once the parameters have been learned, it can be used for any new picture. Second, regarding low-level visual feature extraction, we propose a new scheme to compute the depth of field of a picture. Finally, once the training and the feature extraction have been carried out, a score ranging from 0 (minimal congruency) to 1 (maximal congruency) is computed. A value of 1 indicates that observers would focus on the same locations and suggests that the picture presents strong locations of interest. A second database of eye movements is used to assess the performance of the proposed model. Results show that our IOVC criterion outperforms the Feature Congestion measure \\cite{Rosenholtz2007}. To illustrate the interest of the proposed model, we have used it to automatically rank personalized photograph.","cites":"12","conferencePercentile":"85.86005831"},{"venue":"ACM Multimedia","id":"89a8133644decd006606f7af41a597c015ed2914","venue_1":"ACM Multimedia","year":"2015","title":"AR in Hand: Egocentric Palm Pose Tracking and Gesture Recognition for Augmented Reality Applications","authors":"Hui Liang, Junsong Yuan, Daniel Thalmann, Nadia Magnenat-Thalmann","author_ids":"3036305, 1746449, 1689760, 1695679","abstract":"Wearable devices such as Microsoft Hololens and Google glass are highly popular in recent years. As traditional input hardware is difficult to use on such platforms, vision-based hand pose tracking and gesture control techniques are more suitable alternatives. This demo shows the possibility to interact with 3D contents with bare hands on wearable devices by two Augmented Reality applications, including virtual teapot manipulation and fountain animation in hand. Technically, we use a head-mounted depth camera to capture the RGB-D images from egocentric view, and adopt the random forest to regress for the palm pose and classify the hand gesture simultaneously via a spatial-voting framework. The predicted pose and gesture are used to render the 3D virtual objects, which are overlaid onto the hand region in input RGB images with camera calibration parameters for seamless virtual and real scene synthesis.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"55b06ce04a9449e64604e45782cb13894b224703","venue_1":"ACM Multimedia","year":"2015","title":"OmniViewer: Enabling Multi-modal 3D DASH","authors":"Zhenhuan Gao, Shannon Chen, Klara Nahrstedt","author_ids":"3308955, 1805705, 1688353","abstract":"This paper presents OmniViewer, a multi-modal 3D video streaming system based on Dynamic Adaptive Streaming over HTTP (DASH) standard. OmniViewer allows users to view arbitrary side of a performer by choosing the view angle from 0&#176; to 360&#176;. Besides, according to the current available bandwidth, it can also adaptively change the bitrate of rendered 3D video for both smooth and high-quality view rendering. Finally, OmniViewer extends traditional DASH implementation to support multi-modal data streaming besides video and audio.","cites":"4","conferencePercentile":"87.77777778"},{"venue":"ACM Multimedia","id":"885a3c707c840d2da4e67de5c3fe3699572f2e40","venue_1":"ACM Multimedia","year":"2012","title":"Immersive multiplayer tennis with microsoft kinect and body sensor networks","authors":"Suraj Raghuraman, Karthik Venkatraman, Zhanyu Wang, Jian Wu, Jacob Clements, Reza Lotfian, B. Prabhakaran, Xiaohu Guo, Roozbeh Jafari, Klara Nahrstedt","author_ids":"1773627, 1717738, 1898692, 1734550, 3040086, 1864661, 1748167, 4253073, 1719008, 1688353","abstract":"We present an immersive gaming demonstration using the minimum amount of wearable sensors. The game demonstrated is two-player tennis. We combine a virtual environment with real 3D representations of physical objects like the players and the tennis racquet (if available). The main objective of the game is to provide as real an experience of tennis as possible, while also being as less intrusive as possible. The game is played across a network, and this opens the possibility of two remote players playing a game together on a single virtual tennis pitch. The Microsoft Kinect sensors are used to obtain a 3D point cloud and a skeletal map representation of the player. This 3D point cloud is mapped on to the virtual tennis pitch. We also use a wireless wearable Attitude and Heading Reference System (AHRS) mote, which is strapped onto the wrist of the players. This mote gives us precise information about the movement (swing, rotation etc.) of the playing arm. This information along with the skeletal map is used to implement the physics of the game. Using this game we demonstrate our solutions for simultaneous data acquisition, 3D point-cloud mapping in a virtual space, use of the Kinect and AHRS sensors to calibrate real and virtual objects and for interaction of virtual objects with a 3D point cloud.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"8dd4c782526b2fa8128ec13fe20f1c9e5cbea00f","venue_1":"ACM Multimedia","year":"2008","title":"Le salon de récurrence","authors":"Yuri Choi, Soonil Kwon, Yong Ho Kim","author_ids":"2145731, 2040905, 2692092","abstract":"Hair salon is an ordinary space we casually visit when we want to change our hairstyles. Although it requires active interaction with hairdresser to get the style as desired, possible motions and gestures of customers are usually limited. In this work, we present a multimedia work that attempts to show the visualization of hairstyles in fantasy based on the customer's simple gestures including eye movement or foot tapping, and to create spatialized sound according to the eye-tracking information to offer multi-modal feedback. We present this as an approach to our daily lives using ordinary media.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"d3008b4122e50a28f6cc1fa98ac6af28b42271ea","venue_1":"ACM Multimedia","year":"2015","title":"Searching Persuasively: Joint Event Detection and Evidence Recounting with Limited Supervision","authors":"Xiaojun Chang, Yaoliang Yu, Yi Yang, Alexander G. Hauptmann","author_ids":"1729163, 2443546, 1698559, 7661726","abstract":"Multimedia event detection (MED) and multimedia event recounting (MER) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results. In order to leverage the mutual benefits of the two tasks, we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events. Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place. Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences. To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level. The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which, after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora. We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets, and obtain very promising results for both MED and MER.","cites":"16","conferencePercentile":"97.96296296"},{"venue":"ACM Multimedia","id":"c284934bd5192cc6257f688782c4db6807db3b11","venue_1":"ACM Multimedia","year":"1993","title":"A Multimedia Component Kit: Experiences with Visual Composition of Applications","authors":"Vicki de Mey, Simon J. Gibbs","author_ids":"1749600, 1714934","abstract":"In this paper we present an object-oriented perspective to multime-dia and discuss an approach for prototyping distributed multimedia applications. We describe the implementation of a component kit, an example application, called a virtual museum, and a visual composition tool. The tool allows interactive construction of multime-dia applications from generic software components by direct manipulation and graphical editing.","cites":"23","conferencePercentile":"48.8372093"},{"venue":"ACM Multimedia","id":"3b9593a7ba7138673e3d34c5fb68d299c832ae9c","venue_1":"ACM Multimedia","year":"2015","title":"Image Profiling for History Events on the Fly","authors":"Jia Chen, Qin Jin, Yong Yu, Alexander G. Hauptmann","author_ids":"5562274, 8362559, 3578922, 7661726","abstract":"History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as \"wild and dirty\" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.","cites":"2","conferencePercentile":"74.07407407"},{"venue":"ACM Multimedia","id":"634bfb81c9998efaa5b67e82beb3c382aeda1053","venue_1":"ACM Multimedia","year":"2011","title":"A smart assistant for shooting virtual cinematography with motion-tracked cameras","authors":"Christophe Lino, Marc Christie, Roberto Ranon, William H. Bares","author_ids":"2869929, 1701717, 1727883, 2872735","abstract":"This demonstration shows how an automated assistant encoded with knowledge of cinematography practice can offer suggested viewpoints to a filmmaker operating a hand-held motion-tracked virtual camera device. Our system, called Director's Lens, uses an intelligent cinematography engine to compute, at the request of the filmmaker, a set of suitable camera placements for starting a shot that represent semantically and cinematically distinct choices for visualizing the current narrative. Editing decisions and hand-held camera compositions made by the user in turn influence the system's suggestions for subsequent shots. The result is a novel virtual cinematography workflow that enhances the filmmaker's creative potential by enabling efficient exploration of a wide range of computer-suggested cinematographic possibilities.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"3191b4c225c19144da5912221c4c8137fd6d5672","venue_1":"ACM Multimedia","year":"2011","title":"The director's lens: an intelligent assistant for virtual cinematography","authors":"Christophe Lino, Marc Christie, Roberto Ranon, William H. Bares","author_ids":"2869929, 1701717, 1727883, 2872735","abstract":"We present the Director's Lens, an intelligent interactive assistant for crafting virtual cinematography using a motion-tracked hand-held device that can be aimed like a real camera. The system employs an intelligent cinematography engine that can compute, at the request of the filmmaker, a set of suitable camera placements for starting a shot. These suggestions represent semantically and cinematically distinct choices for visualizing the current narrative. In computing suggestions, the system considers established cinema conventions of continuity and composition along with the filmmaker's previous selected suggestions, and also his or her manually crafted camera compositions, by a machine learning component that adapts shot editing preferences from user-created camera edits. The result is a novel workflow based on interactive collaboration of human creativity with automated intelligence that enables efficient exploration of a wide range of cinematographic possibilities, and rapid production of computer-generated animated movies.","cites":"11","conferencePercentile":"84.54810496"},{"venue":"ACM Multimedia","id":"55ae61ef4cb13573aac2f8099a0c29b5303f0fd9","venue_1":"ACM Multimedia","year":"2008","title":"Fobs: an open source object-oriented library for accessing multimedia content","authors":"Jose San Pedro","author_ids":"2871028","abstract":"The exceptionally large nature of multimedia content has motivated the creation of many different compression algorithms and encapsulation formats to make its transportation and storage feasible. Developers of multimedia applications have to deal repeatedly with the massive number of forms in which content is present, turning the single task of media access into an unnecessary challenge. The open source project FOBS provides a way to abstract developers from these difficulties, by offering an intuitive and powerful object oriented multimedia access API. FOBS has been conceived to be inherently platform independent and to be easily adaptable to multiple programming languages, making the addition of multimedia support possible in almost any application.","cites":"2","conferencePercentile":"29.81651376"},{"venue":"ACM Multimedia","id":"92ee1816733c09da195ef7ffe2addaf5a895ff08","venue_1":"ACM Multimedia","year":"2015","title":"Theia: A Fast and Scalable Structure-from-Motion Library","authors":"Chris Sweeney, Tobias Höllerer, Matthew Turk","author_ids":"2224763, 1743721, 1752714","abstract":"In this paper, we have presented a comprehensive multi-view geometry library, Theia, that focuses on large-scale SfM. In addition to state-of-the-art scalable SfM pipelines, the library provides numerous tools that are useful for students, researchers, and industry experts in the field of multi-view geometry. Theia contains clean code that is well documented (with code comments and the website) and easy to extend. The modular design allows for users to easily implement and experiment with new algorithms within our current pipeline without having to implement a full end-to-end SfM pipeline themselves. Theia has already gathered a large number of diverse users from universities, startups, and industry and we hope to continue to gather users and active contributors from the open-source community.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"a4db1ee9ec8127a5c21a4c79261a1f298f651421","venue_1":"ACM Multimedia","year":"2014","title":"Instructional Videos for Unsupervised Harvesting and Learning of Action Examples","authors":"Shoou-I Yu, Lu Jiang, Alexander G. Hauptmann","author_ids":"2927024, 1697318, 7661726","abstract":"Online instructional videos have become a popular way for people to learn new skills encompassing art, cooking and sports. As watching instructional videos is a natural way for humans to learn, analogously, machines can also gain knowledge from these videos. We propose to utilize the large amount of instructional videos available online to harvest examples of various actions in an unsupervised fashion. The key observation is that in instructional videos, the instructor's action is highly correlated with the instructor's narration. By leveraging this correlation, we can exploit the timing of action corresponding terms in the speech transcript to temporally localize actions in the video and harvest action examples. The proposed method is scalable as it requires no human intervention. Experiments show that the examples harvested are of reasonably good quality, and action detectors trained on data collected by our unsupervised method yields comparable performance with detectors trained with manually collected data on the TRECVID Multimedia Event Detection task.","cites":"16","conferencePercentile":"94.17670683"},{"venue":"ACM Multimedia","id":"169682b2e0780a184069c2c649815db13b308c80","venue_1":"ACM Multimedia","year":"2009","title":"Swan boat: pervasive social game to enhance treadmill running","authors":"Miru Ahn, Sungwon Peter Choe, Sungjun Kwon, Byunglim Park, Taiwoo Park, Sooho Cho, Jaesang Park, Yunseok Rhee, Junehwa Song","author_ids":"2195447, 2840947, 2066756, 3205457, 1797370, 2687467, 1976762, 2288877, 1789470","abstract":"We designed and implemented a pervasive game called Swan Boat that targets the bland and tedious nature of running on a treadmill, making it fun through social interaction and immersive game play. We developed Swan Boat on top of PSD, a platform for pervasive games, and using the Interactive Treadmill hardware. We conducted a user study to evaluate our game.","cites":"7","conferencePercentile":"55.78512397"},{"venue":"ACM Multimedia","id":"3cd04037f855f9a61a98c4c7b9b4fa3a7069c74e","venue_1":"ACM Multimedia","year":"2000","title":"Temporal links: recording and replaying virtual environments","authors":"Chris Greenhalgh, Jim Purbrick, Steve Benford, Michael P. Craven, Adam Drozd, Ian Taylor","author_ids":"1714469, 1805978, 1738239, 7928121, 2111372, 4416638","abstract":"Virtual reality (VR) currently lacks the kinds of sophisticated production technologies that are commonly available for established media such as video and audio. This paper introduces the idea of temporal links, which provide a flexible mechanism for replaying past or recent recordings of virtual environments within other real-time virtual environments. Their flexibility arises from a combination of temporal, spatial and presentational properties. Temporal properties determine the relationship between time in a live environment and time in a recording, including the apparent speed and direction of replay. Spatial properties determine the spatial relationship between the environment and the recording. Presentational properties determine the appearance of the recording within the environment. These properties may be fixed, dynamically varied by an application, or directly controlled in real-time by users. Consequently, temporal links have a wide variety of potential uses, including supporting post-production tools for virtual environments, post-exercise debriefing in training simulators, and asynchranous communication such as VR email, as well as providing new forms of content for virtual worlds that refer to past activity. We define temporal links and their properties and describe their implementation in the MASSIVE-3 Collaborative Virtual Environment (CVE) system, focusing on the underlying record and replay mechanisms. We also demonstrate applications for adding new content to an existing virtual world, and a VR post-production editor.","cites":"25","conferencePercentile":"80.97826087"},{"venue":"ACM Multimedia","id":"af25635520d332be09ebaedaf81978f32e225d83","venue_1":"ACM Multimedia","year":"2011","title":"\"U are happy life\": telling the future's stories","authors":"Genevieve Bell","author_ids":"3032479","abstract":"Bio Genevieve Bell joined Intel in 1998 as a researcher in Corporate Technology Group's People and Practices Research team — Intel's first social science oriented research team. She helped drive the company's first non-U.S. field studies to inform business group strategy and products and conducted groundbreaking work in urban Asia in the early 2000s. Bell has been the driving force behind Intel's emerging user-experience focus: over the last decade, she has fundamentally changed how Intel envisions, plans, and develops its platforms. Bell currently leads an R&D team of social scientists, interaction designers, human factors engineers, and a range of technology researchers to create the next generation of compelling user experiences across a range of internet-connected devices, platforms, and services. She will drive user-centered experience and design across the compute continuum. She has written more than 25 journal articles and book chapters on a range of subjects focused on the intersection of technology and society. Her book, \" Divining the Digital Future, \" co-authored with Prof. Paul Dourish, will be released by MIT Press in spring 2011. Raised in Australia, Bell received her bachelor's degree in anthropology from Bryn Mawr College in 1990. She received her master's and doctorate degrees in anthropology from Stanford University","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"ff506af24c9fb0feeb7f4a3eb80ceffe87b9e81b","venue_1":"ACM Multimedia","year":"2010","title":"e-Fovea: a multi-resolution approach with steerable focus to large-scale and high-resolution monitoring","authors":"Kuan-Wen Chen, Chih-Wei Lin, Mike Y. Chen, Yi-Ping Hung","author_ids":"1798879, 3241078, 2335746, 7312257","abstract":"This paper presents e-Fovea, a system that combines both multi-resolution camera input and multi-resolution steerable projector output to support large-scale and high-resolution visual monitoring. e-Fovea utilizes a design similar to the human eyes, which provides peripheral vision with a steerable fovea that is in higher resolution. e-Fovea is implemented using a steerable telephoto camera and a wide-angle camera. The telephoto image is displayed using a projector with a steerable mirror, and overlaid on the wide-angle image that is displayed using a second projector.\n We have deployed e-Fovea in two installations to demonstrate its feasibility. We have also conducted two user studies, with a total of 36 participants, to compare e-Fovea to two existing multi-resolution visual monitoring designs. The user study results show that for visual monitoring tasks, our e-Fovea design with steerable focus is significantly faster than existing approaches and preferred by users","cites":"1","conferencePercentile":"26.71232877"},{"venue":"ACM Multimedia","id":"64c80b4255a06e00193f294766148ce8c55de84d","venue_1":"ACM Multimedia","year":"1998","title":"Nsync - A Toolkit for Building Interactive Multimedia Presentations","authors":"Brian P. Bailey, Joseph A. Konstan, Robert Cooley, Moses Dejong","author_ids":"1681836, 2478310, 8681165, 2122259","abstract":"1. Abstiact Creating innovative interactive mukirnedia presentations requires a Seat dd of time, SW and effoti We have developed a mdtimedia synchrotition tooKL cded Nsync @renounced 'in-sync'), to address the complicated issues inherent in designing flexi%le, interactive rntitimedia presenbtions. me tooKt consists of ti'o primary components, a declarative synchrotiation definition language and a run-time presentation management system The synchronimtion detition lan~ge supports the specification of synchronous interaction% asynchronous interaction fie-grained relationships, and combwtions of each through the use of conjunctive and disjunctive operators. Re-computed playout schedties are too Wexiile to ded with synchronous interactio~ and a more adaptive presentation management system is required Nsync's run-time system uses a novel predictive logic to predict tie fiture behavior of a presentation As the viewer makes decisions, tie presentation is updated and new predictions are made in order to maintain consistency with the ~tiewer's w%hes and the integri~ of the presentation's message. 2. Introduction hteractive mdtirnedia presentations represent an asynchronous co~aboration be~een the presentation author and the presentation viewer. The role of the presentation author is to design an interactive presentation at the praent time thatd restdt in an individurd e~erience for a variety of potentitiy *own viewers at a fitire time. To aid in designing this individd ex~eriertce, the autior must be able to request viewer participation at key points tithin the presentatio~ and *O must be able to spec~ the action to take based on &e resdt of the viewer's response. Here viewer partie$ation is defined as the abfity to re~ond to reques~ such as a nmki-choice questioq or as the ab~ty 10 navigate within the presentatio~ choosrng one of Perrrrtsslonto make dlgilal or hard copies of dl or part of Ibis vtork for personal or classroom use is granted wzthoul fee provided iha! copies are not made or dtstnbuted for profit or commercial adt,antage, and that copies bear lhm notice and the fulI citation on the first page. To copy o~henvke.IOrepublish to post on sem,ers or IO redistribu~e to lists, requires prior specific perrmss]onand)or a fee. seved different possible playout paths. The role of the praentation viewer is to cognitively absorb the message inherent within the presentation and to provide responses when viewer participation is requested. The viewer should also be dewed to exercise control over the playback of the presentation. Here control is defined as the abtiity to skip ahea~ skip behin~ or adjust the playback rate at any …","cites":"25","conferencePercentile":"60.57692308"},{"venue":"ACM Multimedia","id":"cab7b095e8d5936a93df64d75e6d0408c090832f","venue_1":"ACM Multimedia","year":"2013","title":"ACM MM MIIRH 2013: workshop on multimedia indexing and information retrieval for healthcare","authors":"Jenny Benois-Pineau, Alexia Briassouli, Alexander G. Hauptmann","author_ids":"1700068, 1713991, 7661726","abstract":"Healthcare systems are depending on increasingly sophisticated and ubiquitous technology, while telehealth is rapidly gaining importance with the advent of low-cost and effective technological solutions in medicine. The increase in the worldwide elderly population and the burden this is inflicting upon the workforce, societies and economies are making remote care and independent living at home a necessity. MIIRH is the first workshop on multimedia analysis for remote care of and assisted living solutions which enable people that are incapacitated in some regard to continue living independently at home and remain active members of society. The topics addressed in MIIRH are extremely timely, as multitudes of cost-effective and high quality care solutions are already being developed and used, rendering the examination of new medical, healthcare paradigms an absolute necessity.","cites":"0","conferencePercentile":"10.88888889"},{"venue":"ACM Multimedia","id":"9b2265fb7cc522d917a7087da4565c1daf56632a","venue_1":"ACM Multimedia","year":"2006","title":"On the significance of cluster-temporal browsing for generic video retrieval: a statistical analysis","authors":"Mika Rautiainen, Tapio Seppänen, Timo Ojala","author_ids":"1744921, 1769925, 7967584","abstract":"In this paper, we test statistically the effect of content-based browsing in generic video retrieval. Using TRECVID 2004 and 2005 experiments, we demonstrate that content-based browsing improves retrieval over sequential queries and relevance feedback. Two user groups, novices and system developers have been used in the experiments on large and multilingual video collections. Novice users were found to achieve improvement in search effectiveness with cluster-temporal browsing by statistically significant amount. System developers did not have statistically significant difference between the different system configurations.","cites":"4","conferencePercentile":"46.11398964"},{"venue":"ACM Multimedia","id":"4ceca5eb894aee5c49b6ab8300970f88c8dabf9e","venue_1":"ACM Multimedia","year":"2014","title":"Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search","authors":"Lu Jiang, Deyu Meng, Teruko Mitamura, Alexander G. Hauptmann","author_ids":"1697318, 7480838, 1706595, 7661726","abstract":"Reranking has been a focal technique in multimedia retrieval due to its efficacy in improving initial retrieval results. Current reranking methods, however, mainly rely on the heuristic weighting. In this paper, we propose a novel reranking approach called Self-Paced Reranking (SPaR) for multimodal data. As its name suggests, SPaR utilizes samples from easy to more complex ones in a self-paced fashion. SPaR is special in that it has a concise mathematical objective to optimize and useful properties that can be theoretically verified. It on one hand offers a unified framework providing theoretical justifications for current reranking methods, and on the other hand generates a spectrum of new reranking schemes. This paper also advances the state-of-the-art self-paced learning research which potentially benefits applications in other fields. Experimental results validate the efficacy and the efficiency of the proposed method on both image and video search tasks. Notably, SPaR achieves by far the best result on the challenging TRECVID multimedia event search task.","cites":"30","conferencePercentile":"97.99196787"},{"venue":"ACM Multimedia","id":"a0f31a2f954dbb6359f6ca93e69c5a471726850d","venue_1":"ACM Multimedia","year":"2013","title":"Spatio-temporal fisher vector coding for surveillance event detection","authors":"Qiang Chen, Yang Cai, Lisa M. Brown, Ankur Datta, Quanfu Fan, Rogério Schmidt Feris, Shuicheng Yan, Alexander G. Hauptmann, Sharath Pankanti","author_ids":"1737025, 1801727, 5730081, 7207324, 3024917, 1723233, 1698982, 7661726, 1767897","abstract":"We present a generic event detection system evaluated in the Surveillance Event Detection (SED) task of TRECVID 2012. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task. This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames. A Gaussian Mixture Model(GMM) is learned to model the distribution of the low level features. Then for each sliding window, the Fisher vector encoding [improvedFV] is used to generate the sample representation. The model is learnt using a Linear SVM for each event. The main novelty of our system is the introduction of Fisher vector encoding into video event detection. Fisher vector encoding has demonstrated great success in image classification. The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features. FV encoding uses higher order statistics in place of histograms in the standard BoW. FV has several good properties: (a) it can naturally separate the video specific information from the noisy local features and (b) we can use a linear model for this representation. We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time. We also take advantage of non-trivial object localization techniques to feed into the video event detection, e.g. multi-scale detection and non-maximum suppression. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types.","cites":"3","conferencePercentile":"60"},{"venue":"ACM Multimedia","id":"75223101283ef8abea55174f334516ddbaa769b9","venue_1":"ACM Multimedia","year":"2006","title":"Progressive cut","authors":"Chao Wang, Qiong Yang, Mo Chen, Xiaoou Tang, Zhongfu Ye","author_ids":"6250419, 1759537, 1733987, 1741901, 2066033","abstract":"Recently, interactive image cutout technique becomes prevalent for image segmentation problem due to its easy-to-use nature. However, most existing stroke-based interactive object cutout system did not consider the user intention inherent in the user interaction process. Strokes in sequential steps are treated as a collection rather than a process, and only the color information of the additional stroke is used to update the color model in the graph cut framework. Accordingly, unexpected fluctuation effect may occur during the process of interactive object cutout. In fact, each step of user interaction reflects the user's evaluation of previous result and his/her intention. By analyzing the user's intention behind the interaction, we propose a progressive cut algorithm, which explicitly models the user's intention into a graph cut framework for the object cutout task. Three aspects of user intention are utilized: 1) the color of the stroke indicates the kind of change s/he expects, 2) the location of the stroke indicates the region of interest, 3) the relative position between the stroke and the previous result indicates the segmentation error. By incorporating such information into the cutout system, the new algorithm removes the unexpected fluctuation effect of existing stroke-based graph-cut methods, and thus provides the user a more controllable result with fewer strokes and faster visual feedback. Experiments and user study show the strength of progressive cut in accuracy, speed, controllability, and user experience.","cites":"1","conferencePercentile":"21.76165803"},{"venue":"ACM Multimedia","id":"1cf3a79f8340f08c194e7d142363b5b298f4b3b1","venue_1":"ACM Multimedia","year":"2000","title":"Implementation of aural attributes for simulation of room effects in virtual environments","authors":"Kenji Suzuki, Yuji Nishoji, Jens Herder","author_ids":"4471601, 2232555, 8646686","abstract":"The audio design for vitural environments includes simulation of acoustical room properties besides specifying sound sources and sinks and their behavior. Virtual environments supporting room reverberation not only gain realism but also provide additional information to the user about surrounding space. Catching the different sound properties by the different spaces requires partitioning the space by the properties of aural spaces. We define soundscape and aural attributes as an application and multimedia content interface. Calculated data on an abstract level is sent to spatialization backends. Part of this research was the implementation of a device driver for the Roland Sound Space Processor. This device not only directionalizes sound sources, but also controls room effects  like reverberation.","cites":"0","conferencePercentile":"7.065217391"},{"venue":"ACM Multimedia","id":"304cec2a334bc099cc4f597bba86a0492b454932","venue_1":"ACM Multimedia","year":"2004","title":"Practical voltage scaling for mobile multimedia devices","authors":"Wanghong Yuan, Klara Nahrstedt","author_ids":"2072183, 1688353","abstract":"This paper presents the design, implementation, and evaluation of a &#60;i>practical&#60;/i> voltage scaling (PDVS) algorithm for mobile devices primarily running multimedia applications. PDVS seeks to minimize the total energy of the whole device while meeting multimedia timing requirements. To do this, PDVS extends traditional real-time scheduling by deciding &#60;i>what execution speed&#60;/i> in addition to when to execute what applications. PDVS makes these decisions based on the discrete speed levels of the CPU, the total power of the device at different speeds, and the probability distribution of CPU demand of multimedia applications. We have implemented PDVS in the Linux kernel and evaluated it on an HP laptop. Our experimental results show that PDVS saves energy substantially without affecting multimedia performance. It saves energy by 14.4% to 37.2% compared to scheduling algorithms without voltage scaling and by up to 10.4% compared to previous voltage scaling algorithms that assume an ideal CPU with continuous speeds and cubic power-speed relationship.","cites":"31","conferencePercentile":"84.31372549"},{"venue":"ACM Multimedia","id":"a4e0352b15d4e54aee56954fc961abf8876bff6b","venue_1":"ACM Multimedia","year":"2004","title":"A taxonomy for multimedia service composition","authors":"Klara Nahrstedt, Wolf-Tilo Balke","author_ids":"1688353, 1720266","abstract":"The realization of multimedia systems still heavily relies on building monolithic systems that need to be reengineered for every change in the application and little of which can be reused in subsequent developments even for similar applications. Hence, building complex large scale multimedia systems is still a difficult and challenging problem. Service-based architectures, like researched in the Web community, form a possible solution to this problem: The service-based paradigm decomposes complex tasks into smaller independent entities (e.g. Web services), and then supports a flexible service composition in a variety of ways. However, due to the characteristics of multimedia applications and rich semantic structure of multimedia data and workflows, a direct application of Web-based research results is still difficult. The reason is that Web service frameworks cannot yet cope with the complexity of multimedia applications and their metadata. In this paper, we describe a basic taxonomy for the composition of services to support complex multimedia workflows. We will investigate in detail the necessary steps and methodology for multimedia service compositions and apply our taxonomy to different service composition instances. We will illustrate all composition instances within our taxonomy with case studies and point to possible techniques for the composition problem.","cites":"17","conferencePercentile":"74.75490196"},{"venue":"ACM Multimedia","id":"3de5736c0271582b555c31a615d0bd05dcab74c9","venue_1":"ACM Multimedia","year":"2003","title":"Video summarization based on user log enhanced link analysis","authors":"Bin Yu, Wei-Ying Ma, Klara Nahrstedt, HongJiang Zhang","author_ids":"1774174, 1705244, 1688353, 1718558","abstract":"Efficient video data management calls for intelligent video summarization tools that automatically generate concise video summaries for fast skimming and browsing. Traditional video summarization techniques are based on low-level feature analysis, which generally fails to capture the semantics of video content. Our vision is that users unintentionally embed their understanding of the video content in their interaction with computers. This valuable knowledge, which is difficult for computers to learn autonomously, can be utilized for video summarization process. In this paper, we present an intelligent video browsing and summarization system that utilizes previous viewers' browsing log to facilitate future viewers. Specifically, a novel ShotRank notion is proposed as a measure of the subjective interestingness and importance of each video shot. A ShotRank computation framework is constructed to seamlessly unify low-level video analysis and user browsing log mining. The resulting ShotRank is used to organize the presentation of video shots and generate video skims. Experimental results from user studies have strongly confirmed that ShotRank indeed represents the subjective notion of interestingness and importance of each video shot, and it significantly improves future viewers' browsing experience.","cites":"45","conferencePercentile":"85.58558559"},{"venue":"ACM Multimedia","id":"64226f83592540a45a1b61d8540689f01304abaa","venue_1":"ACM Multimedia","year":"2003","title":"A scalable overlay video mixing service model","authors":"Bin Yu, Klara Nahrstedt","author_ids":"1774174, 1688353","abstract":"Time 2 2 2 1 3 1 3 1 Information 1 2 3 Figure 1. Information disparity over time and between streams","cites":"1","conferencePercentile":"11.26126126"},{"venue":"ACM Multimedia","id":"63ebcf6ad4232bbe92bad370fab16348ee1bb99b","venue_1":"ACM Multimedia","year":"2002","title":"A programming framework for quality-aware ubiquitous multimedia applications","authors":"Duangdao Wichadakul, Xiaohui Gu, Klara Nahrstedt","author_ids":"1754953, 1692583, 1688353","abstract":"Ubiquitous computing promises a computing environment that seamlessly and pervasively delivers applications to the user, despite changes of resources, devices, and locations. However, few ubiquitous multimedia applications (UMAs) exist up-to-date. One of the main reasons lies in the fact that it is difficult and error-prone to build a UMA which is mobile and deployable in different ubiquitous environments, and still provides acceptable application-specific Quality-of-Service (QoS) guarantees. In this paper, we present the design and implementation of a novel programming framework, called 'QCompiler\" to address the challenges. The framework includes (1) a high-level application specification for the application developer to easily write a UMA with specific quality, mobility, and ubiquity supports, (2) a meta-data compilation, which provides automated consistency checks, translations, and substitutions, to relieve the application developer from dealing with complex programming related to quality, mobility, and ubiquity, (3) a binding, which prepares a quality-aware specification to be executable, in a specific deployment environment, and (4)a run-time meta-data execution, utilizing the meta-data compilation's results, to manage and control a quality-aware multimedia application. As a case study, we apply the programming framework to build a mobile Video-on-Demand (VoD) application. The experimental results show tradeoffs between easiness and flexibility to develop and deploy UMA, and overheads during UMA instantiation and adaptation.","cites":"20","conferencePercentile":"74.78632479"},{"venue":"ACM Multimedia","id":"b26e999798993257dbc69f984638bff56731f599","venue_1":"ACM Multimedia","year":"1997","title":"An Integrated Metric for Video QoS","authors":"Nalini Venkatasubramanian, Klara Nahrstedt","author_ids":"1732742, 1688353","abstract":"to obtain cost-effective QoS. In this paper, we address the issues in designing metrics that are important in evaluating the Quality of Service(QoS) In this paper, we address the issues in designing metrics that are important in evaluating the QoS of video transmission. There has been little work in det ermining effective metrics of QoS for video transmission that characterize both cost (revenue generated or service demand) and guaranteed service. The metrics of analysis and comparison for video transmission must be determined as an end-to-end measure of QoS from video server to end-user(s). By developing these metrics, we hope to enhance the client, server and networking components of a system with monitoring capabilities to measure and evaluate video characterizations. This paper is organized as follows. In Section 2, we discuss a workload model for developing and understanding QoS metrics. Section 3 presents empirical studies and experimental justification for the metric selection based on the three systems-VOSAIC, hierarchical VOD and the remote VCR systems. Section 4 proposes a new integrated metric for measuring video QoS and the analytical framework to express the tradeoffs. We also propose a metric-based QoS architecture along with negotiation and reward protocols. In Section 5 we discuss related work and conclude with future research directions in Section 6. of video transmission. We propose a new metric for video QoS called the weighted cost-satisfaction ratio based on requirements from two perspectives: the user and the service provider. To understand real video workload environments and user behavioral patterns, we obtained and analyzed empirical results from the VOSAIC (video-over-the-Web) system , a hierarchical video-on-demand (VOD) system and a remote VCR system. Based on these results, we define parameters of resource consumption (storage and network bandwidth etc.) and user satisfaction (jitter, syncbroniza-tion skew) and derive analytical interrelationships among the metric parameters. We also draw an economic relationship between the user-satisfaction and resource consumption factors to solve metric optimization relations.","cites":"29","conferencePercentile":"66.66666667"},{"venue":"ACM Multimedia","id":"16168a186c1df2ba96344d5b3dd0a54828130b8b","venue_1":"ACM Multimedia","year":"2005","title":"Chameleon: application level power management with performance isolation","authors":"Xiaotao Liu, Prashant J. Shenoy, Mark D. Corner","author_ids":"3164700, 1705052, 2664272","abstract":"In this paper, we present Chameleon---an application-level power management approach for reducing energy consumption in mobile processors. Our approach exports the entire responsibility of power management decisions to the application level. We propose an operating system interface that can be used by applications to achieve energy savings. We consider three classes of applications---soft real-time, interactive and batch---and design user-level power management strategies for representative applications such as a movie player, a word processor, a web browser, and a batch compiler. We also design a user level power manager based on <i>GraceOS</i> using Chameleon. We implement our approach in the Linux kernel running on a Sony Transmeta laptop. Our experiments show that, compared to the traditional system-wide CPU voltage scaling approaches, Chameleon can achieve up to 32-50% energy savings while delivering comparable or better performance to applications. Further, Chameleon imposes small overheads and is very effective at scheduling concurrent applications with diverse energy needs.","cites":"9","conferencePercentile":"56.68316832"},{"venue":"ACM Multimedia","id":"fd0a462934359b52051ae95b970c9e01f4f32539","venue_1":"ACM Multimedia","year":"2015","title":"Fast and Accurate Content-based Semantic Search in 100M Internet Videos","authors":"Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mitamura, Alexander G. Hauptmann","author_ids":"1697318, 2927024, 7480838, 1698559, 1706595, 7661726","abstract":"Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus cannot scale to \"big data\" that are readily available on the Internet. This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index. The proposed adjustment model relies on a concise optimization framework with interpretations. The proposed index leverages the text-based inverted index for video retrieval. Experimental results validate the efficacy and the efficiency of the proposed method. The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance. Specifically, the proposed method (with reranking) achieves the best result on the challenging TRECVID Multimedia Event Detection (MED) zero-example task. It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos.","cites":"16","conferencePercentile":"97.96296296"},{"venue":"ACM Multimedia","id":"9e468c84af25cf85df0b383f3c8d5b587f8eb65a","venue_1":"ACM Multimedia","year":"2011","title":"Content quality based image retrieval with multiple instance boost ranking","authors":"Peng Yang, Hui Li, Qingshan Liu, Lin Zhong, Dimitris N. Metaxas","author_ids":"1683829, 1750828, 1806361, 5581056, 1711560","abstract":"Most previous works treated image retrieval as a classification problem or a similarity measurement problem. In this paper, we propose a new idea for image retrieval, in which we regard image retrieval as a ranking issue by evaluating image content quality. Based on the content preference between the images, the image pairs are organized to build the data set for rank learning. Because image content generally is disclosed by image patches with meaningful objects, each image is looked as one bag, and the regions inside are the corresponding instances. In order to save the computation cost, the instances in the image are the rectangle regions and the integral histogram is applied to speed up histogram feature extraction. Due to the feature dimension is high, we propose a boost-based multiple instance learning for image retrieval. Based on different assumptions in multiple instance setting, Mean, Max and TopK ranking models are developed with Boost learning. Experiments on the real-world images from Flickr, Pisca, and Google shows that the power of the proposed method.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"e321546b5bdb0c15e768a635df60aa230a571dfd","venue_1":"ACM Multimedia","year":"2008","title":"Facial age estimation by nonlinear aging pattern subspace","authors":"Xin Geng, Kate Smith-Miles, Zhi-Hua Zhou","author_ids":"1735299, 2848275, 1692625","abstract":"Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are defined as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identified.","cites":"16","conferencePercentile":"75.2293578"},{"venue":"ACM Multimedia","id":"2f53115da6ec40a01d3ccc2a8b0043ae08235a4e","venue_1":"ACM Multimedia","year":"2012","title":"Coulda, woulda, shoulda: 20 years of multimedia opportunities","authors":"Klara Nahrstedt, Malcolm Slaney","author_ids":"1688353, 1725788","abstract":"The ACM Special Interest Group on Multimedia (SIGMM) is celebrating the 20th anniversary of establishing its premier conference, the ACM International Conference on Multimedia (ACM Multimedia). The panel \"Coulda, Woulda, Shoulda\" is part of the celebration at the ACM Multimedia 2012. The panelists and the audience will discuss the 20 years of multimedia opportunities that our community has seen, took upon and pushed forward to advance the state of the art.","cites":"0","conferencePercentile":"12.1835443"},{"venue":"ACM Multimedia","id":"31f2f7336e30de9170453f8480d06401fe2a797e","venue_1":"ACM Multimedia","year":"2009","title":"Face image modeling by multilinear subspace analysis with missing values","authors":"Xin Geng, Kate Smith-Miles, Zhi-Hua Zhou, Liang Wang","author_ids":"1735299, 2848275, 1692625, 1765136","abstract":"The main difficulty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-$n$ product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difficult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M$^2$SA, which can work on the training tensor with massive missing values. Thus M$^2$SA can be used to model face images even when there are only a small number of face images with limited variations which will cause missing values in the training tensor). Experiments on face recognition show that M$^2$SA can work reasonably well with up to $70\\%$ missing values in the training tensor.","cites":"19","conferencePercentile":"84.09090909"},{"venue":"ACM Multimedia","id":"67a2f2d28714e8fef44ca026b0ddab603b4aece4","venue_1":"ACM Multimedia","year":"2010","title":"One person labels one million images","authors":"Jinhui Tang, Qiang Chen, Shuicheng Yan, Tat-Seng Chua, Ramesh Jain","author_ids":"8053308, 1737025, 1698982, 1684968, 4521564","abstract":"Targeting the same objective of alleviating the manual work as automatic annotation, in this paper, we propose a novel framework with minimal human effort to manually annotate a large-scale image corpus. In this framework, a dynamic multi-scale cluster labeling strategy is proposed to manually label the clusters of similar image regions. The users label the multi-scale clusters of regions instead of individual images, thus each labeling operation can annotate hundreds or even thousands of images simultaneously with much reduced manual work. Meanwhile the manual labeling guarantees the accuracy of the labels. Compared to automatic annotation, the proposed framework is more flexible, general and effective, especially for annotating those labels with large semantic gaps. Experiments on NUS-WIDE dataset demonstrate that the proposed fast manual annotation framework is much more effective than automatic annotation and comparatively efficient.","cites":"5","conferencePercentile":"61.50684932"},{"venue":"ACM Multimedia","id":"c3381cd34dfc17a172ebfaeb73f08c880384ed3a","venue_1":"ACM Multimedia","year":"2003","title":"eXtensible content protection","authors":"Florian Pestoni, Clemens Drews","author_ids":"1772677, 2763565","abstract":"This paper describes a proof of concept implementation of xCP, a content protection scheme for home networks.","cites":"0","conferencePercentile":"4.054054054"},{"venue":"ACM Multimedia","id":"b5b04871ecec53a9b71c7056315d5f8e160d24c3","venue_1":"ACM Multimedia","year":"2016","title":"Describing Videos using Multi-modal Fusion","authors":"Qin Jin, Jia Chen, Shizhe Chen, Yifan Xiong, Alexander G. Hauptmann","author_ids":"8362559, 5562274, 3009919, 3493516, 7661726","abstract":"Describing videos with natural language is one of the ultimate goals of video understanding. Video records multi-modal information including image, motion, aural, speech and so on. MSR Video to Language Challenge provides a good chance to study multi-modality fusion in caption task. In this paper, we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework. Features from visual, aural, speech and meta modalities are fused together to represent the video contents. Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) are then used as the decoder to generate natural language sentences. Experimental results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework, which achieved top performance in both common metrics evaluation and human evaluation.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"37228bbbbc7212e2d7888259cb1c4caa038eb09b","venue_1":"ACM Multimedia","year":"2014","title":"CeleLabel: an interactive system for annotating celebrities in web videos","authors":"Zhineng Chen, Jinfeng Bai, Chong-Wah Ngo, Bailan Feng, Bo Xu","author_ids":"8157255, 7238512, 1751681, 2819930, 1749224","abstract":"Manual annotation of celebrities in Web videos is an essential task in many people-related Web services. The task, however, poses a significant challenge even to skillful annotators, mainly due to the large quantity of unfamiliar and greatly varied celebrities, and the lack of a customized system for it. This work develops CeleLabel, an interactive system for manually annotating celebrities in the Web video domain. The peculiarity of CeleLabel is to exploit and display multiple types of information that could assist the annotation, including video content, context surrounding and within a video, celebrity images on the Web, and human factors. Using the system, annotators can interactively switch between two views, i.e., merging similar faces and labeling faces with names, to approach the annotation. User studies show that the CeleLabel leads to a much better labeling efficiency and satisfaction.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"1ecc892db011a30ed3dd8a7465441fd54d560374","venue_1":"ACM Multimedia","year":"2005","title":"SEVA: sensor-enhanced video annotation","authors":"Xiaotao Liu, Mark D. Corner, Prashant J. Shenoy","author_ids":"3164700, 2664272, 1705052","abstract":"In this article, we study how a sensor-rich world can be exploited by digital recording devices such as cameras and camcorders to improve a user's ability to search through a large repository of image and video files. We design and implement a digital recording system that records identities and locations of objects (as advertised by their sensors) along with visual images (as recorded by a camera). The process, which we refer to as <i>Sensor-Enhanced Video Annotation (SEVA)</i>, combines a series of correlation, interpolation, and extrapolation techniques. It produces a tagged stream that later can be used to efficiently search for videos or frames containing particular objects or people. We present detailed experiments with a prototype of our system using both stationary and mobile objects as well as GPS and ultrasound. Our experiments show that: (i) SEVA has zero error rates for static objects, except very close to the boundary of the viewable area; (ii) for moving objects or a moving camera, SEVA only misses objects leaving or entering the viewable area by 1--2 frames; (iii) SEVA can scale to 10 fast-moving objects using current sensor technology; and (iv) SEVA runs online using relatively inexpensive hardware.","cites":"21","conferencePercentile":"77.72277228"},{"venue":"ACM Multimedia","id":"b5aaa746d532a93eec2f4ad53451972486010efb","venue_1":"ACM Multimedia","year":"2010","title":"Explicit and implicit concept-based video retrieval with bipartite graph propagation model","authors":"Lei Bao, Juan Cao, Yongdong Zhang, Jintao Li, Ming-yu Chen, Alexander G. Hauptmann","author_ids":"4050343, 7468114, 1699819, 8722263, 2668362, 7661726","abstract":"The major scientific problem for content-based video retrieval is the semantic gap. Generally speaking, there are two appropriate ways to bridge the semantic gap: the first one is from human perspective (top-down) and the other one is from computer perspective (bottom-up). The top-down method defines a concept lexicon from human perspective, trains the detector for each concept based on supervised learning, and then indexes the corpus with concept detectors. Since each concept has an explicit semantic meaning, we call this concept as an explicit concept. The bottom-up approach directly discovers the underlying latent topics from video corpus by machine perspective using an unsupervised learning. The video corpus is indexed subsequently by these latent topics. As opposite to explicit concepts, we name latent topics as implicit concepts. Given the explicit concept set is pre-defined and independent of the corpus, it is impossible to completely describe corpus and users' queries. On the other hand, the implicit concepts are dynamic and dependent on the corpus, which is able to fully describe corpus and users' queries. Therefore, combining explicit and implicit concepts could be a promising way to bridge the semantic gap effectively. In this paper, a Bipartite Graph Propagation Model (BGPM) is applied to automatically balance influences from explicit and implicit concepts. Concept nodes with strong connections to queries are reinforced no matter explicit or implicit. Demonstrated by the experiments on TREVID 2008 video dataset, BGPM successfully fuses explicit and implicit concepts to achieve a significant improvement on 48 search tasks.","cites":"8","conferencePercentile":"74.24657534"},{"venue":"ACM Multimedia","id":"c3b99026d3c9a5439af9afc2f458c0b65fa80909","venue_1":"ACM Multimedia","year":"2010","title":"ACM international workshop on very-large-scale multimedia corpus, mining and retrieval (VLS-MCMR'10)","authors":"Benoit Huet, Tat-Seng Chua, Alexander G. Hauptmann","author_ids":"2086066, 1684968, 7661726","abstract":"The purpose of this workshop is to bring together researchers interested in the construction and analysis of Very Large Scale Multimedia Corpus, as well as the methodologies to Mine and Retrieve information from them. The Workshop will provide a forum to consolidate key issues related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"4149d596183a99e074a4ffa10056511707219210","venue_1":"ACM Multimedia","year":"2010","title":"Hybrid active learning for cross-domain video concept detection","authors":"Huan Li, Yuan Shi, Ming-yu Chen, Alexander G. Hauptmann, Zhang Xiong","author_ids":"1775194, 1750529, 2668362, 7661726, 1751034","abstract":"Cross-domain video concept detection is a challenging task due to the distribution difference between the source domain and target domain. In order to avoid expensive labeling the target-domain data, Active Learning can be used to incrementally learn a target classifier by reusing the one in the source domain. It uses a discriminative query strategy and picks the most ambiguous samples to label, which could fail if the distribution difference is too large. In this paper, to deal with large difference in data distributions, we propose a generative query strategy which is then combined with the existing discriminative one to yield a hybrid method. This method adaptively fits the distribution differences and gives a mixture strategy that performs more robustly compared to both single strategies. Experimental results on TRECVID semantic concept detection task demonstrate superior performance of our hybrid method.","cites":"6","conferencePercentile":"66.71232877"},{"venue":"ACM Multimedia","id":"a76cbb2731f0c0b7ad0c6e4dd3dedd123c59b771","venue_1":"ACM Multimedia","year":"2000","title":"Video scouting demonstration: smart content selection and recording","authors":"Nevenka Dimitrova, Lalitha Agnihotri, Radu S. Jasinschi, John Zimmerman, George Marmaropoulos, Thomas McGee, Serhan Dagtas","author_ids":"6240744, 2242803, 1683237, 7137890, 2879656, 1915278, 2631288","abstract":"Smart video content selection and recording is the best selling feature of the current personal TV receivers like TiVo. These devices operate at the TV program level in that they use electronic program guides and user's program personal preferences to help consumers record and watch programs that match their interests. In this Video Scouting demonstration, we present a system that allows for the filtering and retrieving of TV sub-programs based on user's content preferences. The filtering process is realized via real-time video, audio, and transcript analysis. The demonstrator personalizes the TV experience in the areas of celebrity and financial information. The technology can translate into differentiating storage and set-top box product features for finding your favorite actors, most interesting personalized financial news of the day, commercial compaction and enhancement, and content augmentation with other sources of information such as Web pages and encyclopedia. The demonstrator also reflects our active involvement in the MPEG-7 standard (Content Description Interface).","cites":"1","conferencePercentile":"16.84782609"},{"venue":"ACM Multimedia","id":"21d71dd15c9da25040a73d6abe421d8cf7c54585","venue_1":"ACM Multimedia","year":"2003","title":"The combination limit in multimedia retrieval","authors":"Rong Yan, Alexander G. Hauptmann","author_ids":"1767526, 7661726","abstract":"Combining search results from multimedia sources is crucial for dealing with heterogeneous multimedia data, particularly in multimedia retrieval where a final ranked list of items of interest is returned sorted by confidence or relevance. However, relatively little attention has been given to combination functions, especially their upper bound performance limits. This paper presents a theoretical framework for studying upper bounds for two types of combination functions. A general upper bound and two approximations are proposed for monotonic combination functions. We also studied the upper bounds for linear combination functions using a global optimization technique. Our experimental results show that the choice of combination functions has a considerable influence to retrieval performance.","cites":"37","conferencePercentile":"78.82882883"},{"venue":"ACM Multimedia","id":"ff332d337331c4953f189493e0e4e4b81de7034e","venue_1":"ACM Multimedia","year":"2001","title":"Personalizing video recorders using multimedia processing and integration","authors":"Nevenka Dimitrova, Radu S. Jasinschi, Lalitha Agnihotri, John Zimmerman, Thomas McGee, Dongge Li","author_ids":"6240744, 1683237, 2242803, 7137890, 1915278, 7379526","abstract":"Current personal Vido recorders make it very easy for consumers to record whole TV programs. Our research however, focuses on personalizing TV at a sub-program level. We use a traditional Content-Based Information Retrieval system architecture consisting of archiving and retrieval modules. The archiving module employs a three-layered, multimodal integration framework to segment, analyze, characterize, and classify segments. The retrieval module relies on users personal preferences to deliver both full programs and video segments of interest. We tested retrieval concepts with real users and discovered that they see more value in segmenting non-narrative programs (e.g. news) than narrative programs (e.g. movies). We benchmarked individual algorithms and segment classification for celebrity and financial segments as instances of non-narrative content. For celebrity segments we obtained a total precision of 94.1% and recall of 85.7%, and for financial segments a total precision of 81.1% and a recall of 86.9%.","cites":"1","conferencePercentile":"15.30612245"},{"venue":"ACM Multimedia","id":"927af11ffe25c2c1178c81c88a986769e979ce31","venue_1":"ACM Multimedia","year":"2016","title":"LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning","authors":"Chengxi Ye, Chen Zhao, Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos","author_ids":"3300969, 1963231, 7607499, 1759899, 1697493","abstract":"LightNet is a lightweight, versatile, purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"39de5850485955600e293e2afb7aab8b241a7cb3","venue_1":"ACM Multimedia","year":"2003","title":"Negative pseudo-relevance feedback in content-based video retrieval","authors":"Rong Yan, Alexander G. Hauptmann, Rong Jin","author_ids":"1767526, 7661726, 1718400","abstract":"Video information retrieval requires a system to find information relevant to a query which may be represented simultaneously in different ways through a text description, audio, still images and/or video sequences. We present a novel approach that uses pseudo-relevance feedback from retrieved items that are NOT similar to the query items without further inquiring user feedback. We provide insight into this approach using a statistical model and suggest a score combination scheme via posterior probability estimation. An evaluation on the 2002 TREC Video Track queries shows that this technique can improve video retrieval performance on a real collection. We believe that negative pseudo-relevance feedback shows great promise for very difficult multimedia retrieval tasks, especially when combined with other different retrieval algorithms.","cites":"53","conferencePercentile":"88.28828829"},{"venue":"ACM Multimedia","id":"9c0c2cb81091000fbc62858f770d258e4538529a","venue_1":"ACM Multimedia","year":"2013","title":"RealSense: directional interaction for proximate mobile sharing using built-in orientation sensors","authors":"Chien-Pang Lin, Cheng-Yao Wang, Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen","author_ids":"1988596, 2834025, 3085443, 1905963, 2335746","abstract":"We present RealSense, a technology that enables users to easily share media files with proximate users by performing directional gestures on mobile devices. RealSense leverages the natural human group behavior of forming a circle and facing the center of the group. By continuously monitoring the directional heading of each device using only built-in orientation sensors, RealSense can compute the relative direction between all the devices. It simplifies media sharing because users do not need to lookup and specify the user IDs and device IDs of the intended recipients. We first evaluated the feasibility and design of RealSense, including the orientation sensor error and the minimal arc degree for selecting recipients. We then compared RealSense with three other common sharing interactions: 1) linear menu, 2) pie menu, and 3) NFC. Our results show that participants preferred RealSense over other sharing interactions, especially for groups of participants who were unacquainted with each other.","cites":"1","conferencePercentile":"30.22222222"},{"venue":"ACM Multimedia","id":"10b6eea3d647aa28d20094024b233c1edfc84629","venue_1":"ACM Multimedia","year":"2004","title":"A personal projected display","authors":"Mark Ashdown, Peter Robinson","author_ids":"2564873, 1780152","abstract":"User interfaces using windows, keyboard and mouse have been in use for over 30 years, but only offer limited facilities to the user. Conventional displays are small, at least compared with a physical desk; conventional input devices restrict both manual expression and cognitive flexibility; remote collaboration is a poor shadow of sitting in the same room. We show how recent technological advances in large display devices and input devices can address these problems. The &#60;i>Escritoire&#60;/i> is a desk-based interface using overlapping projectors to create a large display with a high resolution region in the centre for detailed work. Two pens provide bimanual input over the entire area, and an interface like physical paper addresses some of the affordances not provided by the conventional user interface. Multiple desks can be connected to allow remote collaboration. The system has been tested with single users and collaborating pairs.","cites":"8","conferencePercentile":"56.61764706"},{"venue":"ACM Multimedia","id":"5e80e2ffb264b89d1e2c468fbc1b9174f0e27f43","venue_1":"ACM Multimedia","year":"2004","title":"Naming every individual in news video monologues","authors":"Jun Yang, Alexander G. Hauptmann","author_ids":"2812015, 7661726","abstract":"Naming every individual person appearing in broadcast news videos with names detected from the video transcript leads to better access of the news video content. In this paper, we approach this challenging problem with a statistical learning method. Two categories of information extracted from multiple video modalities have been explored, namely &#60;i>features&#60;/i>, which help distinguish the true name of every person, as well as &#60;i>constraints&#60;/i>, which reveal the relationships among the names of different persons. The person-naming problem is formulated into a learning framework which predicts the most likely name for each person based on the features, and refines the predictions using the constraints. Experiments conducted on ABC World New Tonight and CNN Headline News videos demonstrate that this approach outperforms a non-learning alternative by a large amount.","cites":"37","conferencePercentile":"87.25490196"},{"venue":"ACM Multimedia","id":"15a4cd8715420d068b96ad752265c8018ca85def","venue_1":"ACM Multimedia","year":"2002","title":"Collages as dynamic summaries for news video","authors":"Michael G. Christel, Alexander G. Hauptmann, Howard D. Wactlar, Tobun Dorbin Ng","author_ids":"7307726, 7661726, 1679880, 2418260","abstract":"This paper introduces the <i>video collage</i>, a novel effective interface for browsing and interpreting video collections. The paper discusses how collages are automatically produced, illustrates their use, and evaluates their effectiveness as summaries across news stories. Collages are presentations of text and images derived from multiple video sources, which provide an interactive visualization for a set of video documents, summarizing their contents and providing a navigation aid for further exploration. The dynamic creation of collages is based on user context, e.g., an originating query, coupled with automatic processing to refine the candidate imagery. Named entity identification and common phrase extraction provides descriptive text. The dynamic manipulation of collages allows user-directed browsing and reveals additional detail. The utility of collages as summaries is examined with respect to other published news summaries.","cites":"60","conferencePercentile":"93.58974359"},{"venue":"ACM Multimedia","id":"148bbc544444ab0f63cddcca2141f5ead7bd6777","venue_1":"ACM Multimedia","year":"2015","title":"Accelerating Large-scale Image Retrieval on Heterogeneous Architectures with Spark","authors":"Hanli Wang, Bo Xiao, Lei Wang, Jun Wu","author_ids":"2774427, 1719582, 1743559, 1714535","abstract":"Apache Spark is a general-purpose cluster computing system for big data processing and has drawn much attention recently from several fields, such as pattern recognition, machine learning and so on. Unlike MapReduce, Spark is especially suitable for iterative and interactive computations. With the computing power of Spark, a utility library, referred to as IRlib, is proposed in this work to accelerate large-scale image retrieval applications by jointly harnessing the power of GPU. Similar to the built-in machine learning library of Spark, namely MLlib, IRlib fits into the Spark APIs and benefits from the powerful functionalities of Spark. The main contributions of IRlib lie in two-folds. First, IRlib provides a uniform set of APIs for the programming of image retrieval applications. Second, the computational performance of Spark equipped with multiple GPUs is dramatically boosted by developing high performance modules for common image retrieval related algorithms. Comparative experiments concerning large-scale image retrieval are carried out to demonstrate the significant performance improvement achieved by IRlib as compared with single CPU thread implementation as well as Spark without GPUs employed.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"2af7ea5c03509510228ca55ac66c8b34c4d0c7a0","venue_1":"ACM Multimedia","year":"2014","title":"From Writing to Painting: A Kinect-Based Cross-Modal Chinese Painting Generation System","authors":"Jiajia Li, Grace Ngai, Stephen Chi-fai Chan, Kien A. Hua, Hong Va Leong, Alvin T. S. Chan","author_ids":"5018858, 1706729, 2508623, 1805087, 1714454, 1733004","abstract":"As computer and interaction technologies mature, a much broader range of media is now used for input and output, each of which has its own rich repertoire of techniques, instruments, and cultural heritage. The combination of multiple media can produce novel multimedia human-computer interaction approaches which are more efficient and interesting than traditional single media methods. This paper presents CalliPaint, a system for cross-modal art generation that links together Chinese ink brush calligraphy writing and Chinese landscape painting. We investigate the mapping between the two modalities based on concepts of metaphoric congruence, and implement our findings into a prototype system. A multi-step evaluation experiment with real users suggests that CalliPaint provides a realistic and intuitive experience that allows even novice users to create attractive landscape paintings from writing. Comparison with a general-purpose digital painting software suggests that CalliPaint provides users with a more enjoyable experience. Finally, exhibiting CalliPaint in an open-access location for use by casual users without any training shows that the system is easy to learn.","cites":"1","conferencePercentile":"41.56626506"},{"venue":"ACM Multimedia","id":"783ec326d4031f13930efc129015e7018cc8a263","venue_1":"ACM Multimedia","year":"2002","title":"News video classification using SVM-based multimodal classifiers and combination strategies","authors":"Wei-Hao Lin, Alexander G. Hauptmann","author_ids":"8131758, 7661726","abstract":"Video classification is the first step toward multimedia content understanding. When video is classified into conceptual categories, it is usually desirable to combine evidence from multiple modalities. However, combination strategies in previous studies were usually ad hoc. We investigate a meta-classification combination strategy using Support Vector Machine, and compare it with probability-based strategies. Text features from closed-captions and visual features from images are combined to classify broadcast news video. The experimental results show that combining multimodal classifiers can significantly improve recall and precision, and our meta-classification strategy gives better precision than the approach of taking the product of the posterior probabilities.","cites":"52","conferencePercentile":"91.02564103"},{"venue":"ACM Multimedia","id":"2ea8e5b71ecc193f88747e8b85791459bcf0fd89","venue_1":"ACM Multimedia","year":"2014","title":"Building a Self-Learning Eye Gaze Model from User Interaction Data","authors":"Michael Xuelin Huang, Tiffany C. K. Kwok, Grace Ngai, Hong Va Leong, Stephen Chi-fai Chan","author_ids":"3328025, 2170310, 1706729, 1714454, 2508623","abstract":"Most eye gaze estimation systems rely on explicit calibration, which is inconvenient to the user, limits the amount of possible training data and consequently the performance. Since there is likely a strong correlation between gaze and interaction cues, such as cursor and caret locations, a supervised learning algorithm can learn the complex mapping between gaze features and the gaze point by training on incremental data collected implicitly from normal computer interactions. We develop a set of robust geometric gaze features and a corresponding data validation mechanism that identifies good training data from noisy interaction-informed data collected in real-use scenarios. Based on a study of gaze movement patterns, we apply behavior-informed validation to extract gaze features that correspond with the interaction cue, and data-driven validation provides another level of crosschecking using previous good data. Experimental evaluation shows that the proposed method achieves an average error of 4.06&#186;, and demonstrates the effectiveness of the proposed gaze estimation method and corresponding validation mechanism.","cites":"5","conferencePercentile":"80.72289157"},{"venue":"ACM Multimedia","id":"a7b71462326b5073d51af12b45d14e1bf1e39369","venue_1":"ACM Multimedia","year":"2011","title":"Ensemble multi-instance multi-label learning approach for video annotation task","authors":"Xin-Shun Xu, Xiangyang Xue, Zhi-Hua Zhou","author_ids":"2800124, 5507458, 1692625","abstract":"Automatic video annotation is an important ingredient for video indexing, browsing, and retrieval. Traditional studies represent one video clip with a flat feature vector; however, video data usually has natural structure. Moreover, a video clip is generally relevant to multiple concepts. Indeed, the video annotation task is inherently a Multi-Instance Multi-Label (MIML) learning problem. In this paper, we propose the En-MIMLSVM approach for the video annotation task. It considers the class imbalance and long time training problems of most video annotation tasks. In addition, a temporally consistent weighted multi-instance kernel is developed to take into account both the temporal consistency in video data and the significance of instances of different levels in pyramid representation. The En-MIMLSVM is evaluated on TRECVID 2005 data set, and the results show that it outperforms several state-of-the-art methods.","cites":"12","conferencePercentile":"85.86005831"},{"venue":"ACM Multimedia","id":"db53752bc1a9f49ad22ce0e95d66d9e86b5412c7","venue_1":"ACM Multimedia","year":"2011","title":"MUSIZ: a generic framework for music resizing with stretching and cropping","authors":"Zhang Liu, Chaokun Wang, Yiyuan Bai, Hao Wang, Jianmin Wang","author_ids":"2624431, 8205851, 2129063, 1728073, 1751179","abstract":"Content-aware music adaption, i.e. music resizing, in temporal constraints starts drawing attention from multimedia communities because of the need of real-world scenarios, e.g. animation production and radio advertisement production. The goal of music resizing is to change the length of a music track to a user preferred length using a series of basic operations, e.g. compression, prolonging, cropping and repeating. The only existing music resizing approach so far, called LyDAR, suffers from some limitations. For example, it cannot support prolonging a music track and cannot compress music pieces with very small stretch rates. In this paper, we propose MUSIZ, a generic framework for MUsic reSIZing. Observing the diversity of quality degradation for different segments, we propose the concept of stretch-resistance to measure the degree of quality degradation after a segment is stretched. MUSIZ stretches high stretch-resistance segments intensively and relieves low stretch-resistance segments to reduce the negative impact on the stretched music piece. For short length resizing requests, we develop a contiguity-preservative cropping algorithm to remove segments before stretching, while smoothing the abrupt change at the joint between two segments. Comprehensive experimental results show that MUSIZ is superior to the existing approaches.","cites":"3","conferencePercentile":"52.7696793"},{"venue":"ACM Multimedia","id":"47f02c696e2803cc9cfd68ab0a2d214ad8ca29bd","venue_1":"ACM Multimedia","year":"2008","title":"Automatic text discovering through stroke-based segmentation and text string combination","authors":"Lei Xu, Yingfei Liu, Kongqiao Wang, Hao Wang","author_ids":"1867241, 2851786, 1767888, 1728073","abstract":"In this paper we present a novel framework of automatic text discovering for content-based multimedia application. For single image, the stroke-based binarization and the coarse-to-fine text extraction will collaborate to generate a clean text image for recognition. For image sequence, multi-frame text enhancement is adopted to increase the text/background contrast, and the recognition results are finally refined by the text string combination algorithm to get more precise semantic information. Two prototype demos have been successfully developed on mobile phones. The experimental results on different platforms show the superior performance of the proposed method.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"30394ca26b373bef62ebcd4ada5a166dc92cebb7","venue_1":"ACM Multimedia","year":"1998","title":"MidiSpace: A Temporal Constraint-Based Music Spatializer","authors":"François Pachet, Olivier Delerue","author_ids":"1986454, 3027035","abstract":"1. ~S~CT We develop mdtimedia technology for enriching the music fitening eqerience. We propose a system-MdiSpace-in which mers may kten to music \\vMe controtig in rd tie the lo-ation of sound sourcw, through a simple interface. We introduce the problem of mtiiizg co~zsfite~zcy, and propose a solution based on a constraint propagation mechbm. The proposed system contains both m authoring mode, in which sound engineers may specify spati*ation constraints to be satisfied, and a fitening mode in which kteners m modify the Io&tion of sources under the Supetilon of a constraint solver that ensures the spati~ation always satisfies the constraints. We describe the architecture of the system =d report on e~eriments done so far. 2. AC-LIS~~G T?re befieve that fistening environments of the future can be greatiy enhanced by inte=mting models of musical perception into musicrd 5stening devices, provided we develop appropriate software technolo=~ to exploit them ~s is the basis of the research conductti on \" Active Estening \" at Sony Computer Science bboratory, Paris. Permissionto make dIgilal or hard copies of afl or part of this work for pemonal or classroom use is granted wlxhout fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this noti~ and the full citation on the first page. To copy othem~e. 10republish to post on servers or to redistribute to lists, requires prior specific perrrtrsslonand'ora fee. Active Listening refers to the idea that listeners can be given some degree of control on the music they listen to, that give the possibility of proposing different musical perceptions on a piece of music, by opposition to traditiotud tistening, in which the musical media is playti I passively by some neutral device. me objective is both to increase the musicrd comfort of listeners, and, when , possible, to provide fisteners with smoother paths to new music (music they do not bow, or do not like). ~ese , control parameters create impticifly control spaces in I which musical pieces can be listened to in various ways. , Active fistening is thus reIated to the notion of Open Form in composition [10] but differs by two aspects: 1) we seek to create tistening environments for em.sting music repertoires, rather than creating environments for composition or fi= musicrd exploration (such as PatchWork [16], OpenMusic [2], or CommonMusic [25]), and 2) we aim at creating environments in which the variations …","cites":"12","conferencePercentile":"32.69230769"},{"venue":"ACM Multimedia","id":"9db77739a1b84c7a07c13a446684c9b63966f39e","venue_1":"ACM Multimedia","year":"2016","title":"The Lifecycle of Geotagged Multimedia Data","authors":"Rossano Schifanella, Bart Thomee","author_ids":"2251027, 2463875","abstract":"The world is a big place. At any given instant something is happening somewhere, but even when nothing is going on people still find ways to generate multimedia data, ranging from social media posts, to photos and videos. A substantial number of these media objects is associated with a location, and in an increasingly mobile and connected world (both in terms of people and devices), this number is only bound to get larger. Yet, in the multimedia literature we observe that many researchers often unwittingly treat the geospatial dimension as if it were a regular feature dimension, despite it requiring special attention. In order to avoid pitfalls and to steer clear of erroneous conclusions, this tutorial aims to teach researchers and students how geotagged multimedia data differs from regular data and to educate them on best practices when dealing with such data. We will cover the lifecycle of geotagged data in multimedia research, where the topics range from how this kind of data is represented, processed, analyzed, and visualized. The tutorial requires both passive and active involvement, where we not only present the material, but the attendees also get the opportunity to interact with it using a variety of open source data and tools that we have prepared using a virtual machine.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"11a2592244e2d737fc87572db55dc4bbfe95897b","venue_1":"ACM Multimedia","year":"2004","title":"Successful approaches in the TREC video retrieval evaluations","authors":"Alexander G. Hauptmann, Michael G. Christel","author_ids":"7661726, 7307726","abstract":"This paper reviews successful approaches in evaluations of video retrieval over the last three years. The task involves the search and retrieval of shots from MPEG digitized video recordings using a combination of automatic speech, image and video analysis and information retrieval technologies. The search evaluations are grouped into interactive (with a human in the loop) and non-interactive (where the human merely enters the query into the system) submissions. Most non-interactive search approaches have relied extensively on text retrieval, and only recently have image-based features contributed reliably to improved search performance. Interactive approaches have substantially outperformed all non-interactive approaches, with most systems relying heavily on the user's ability to refine queries and reject spurious answers. We will examine both the successful automatic search approaches and the user interface techniques that have enabled high performance video retrieval.","cites":"68","conferencePercentile":"93.38235294"},{"venue":"ACM Multimedia","id":"04288fe31c2e3839202761e72cd1b87d4f52672f","venue_1":"ACM Multimedia","year":"2004","title":"Learning query-class dependent weights in automatic video retrieval","authors":"Rong Yan, Jun Yang, Alexander G. Hauptmann","author_ids":"1767526, 2812015, 7661726","abstract":"Combining retrieval results from multiple modalities plays a crucial role for video retrieval systems, especially for automatic video retrieval systems without any user feedback and query expansion. However, most of current systems only utilize query independent combination or rely on explicit user weighting. In this work, we propose using query-class dependent weights within a hierarchial mixture-of-expert framework to combine multiple retrieval results. We first classify each user query into one of the four predefined categories and then aggregate the retrieval results with query-class associated weights, which can be learned from the development data efficiently and generalized to the unseen queries easily. Our experimental results demonstrate that the performance with query-class dependent weights can considerably surpass that with the query independent weights.","cites":"92","conferencePercentile":"96.07843137"},{"venue":"ACM Multimedia","id":"11471030c4b659bb8264afb3c692a604baf1d3d6","venue_1":"ACM Multimedia","year":"2004","title":"Towards auto-documentary: tracking the evolution of news stories","authors":"Pinar Duygulu Sahin, Jia-Yu Pan, David A. Forsyth","author_ids":"6785979, 1943594, 1744452","abstract":"News videos constitute an important source of information for tracking and documenting important events. In these videos, news stories are often accompanied by short video shots that tend to be repeated during the course of the event. Automatic detection of such repetitions is essential for creating auto-documentaries, for alleviating the limitation of traditional textual topic detection methods. In this paper, we propose novel methods for detecting and tracking the evolution of news over time. The proposed method exploits both visual cues and textual information to summarize evolving news stories. Experiments are carried on the TREC-VID data set consisting of 120 hours of news videos from two different channels.","cites":"31","conferencePercentile":"84.31372549"},{"venue":"ACM Multimedia","id":"00d0a4af3c46986f9f419a0b2cb578c86a951817","venue_1":"ACM Multimedia","year":"1994","title":"A Continuous Media Application Supporting Dynamic QOS Control on Real-Time Mach","authors":"Tatsuo Nakajima, Hiroshi Tezuka","author_ids":"2846273, 1745933","abstract":"A <italic>QOS control</italic> is one of the most important factors in continuous media applications. The QOS levels of the applications should be maintained even if several continuous media applications are running concurrently. Also, the QOS levels should be changed according to a number of applications and their respective importances when a system is overloaded. This paper presents our experience with a video-on-demand system that supports <italic>a dynamic QOS control</italic> scheme on Real-Time Mach, and shows the necessity of a real-time resource management for building such applications.","cites":"26","conferencePercentile":"63.55932203"},{"venue":"ACM Multimedia","id":"a9e69dd982d9b28c7d1666da999eac59a914dcd4","venue_1":"ACM Multimedia","year":"2009","title":"ACM SIGMM the first workshop on web-scale multimedia corpus (WSMC09)","authors":"Benoit Huet, Jinhui Tang, Alexander G. Hauptmann","author_ids":"2086066, 8053308, 7661726","abstract":"The purpose of this workshop is to bring together researchers interested in the construction and analysis of Web Scale multimedia datasets and resources. The Workshop will provide a forum to consolidate key factors related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.","cites":"0","conferencePercentile":"7.231404959"},{"venue":"ACM Multimedia","id":"192fd13a58da7f945375827227dc14f563ae4e65","venue_1":"ACM Multimedia","year":"2009","title":"Identifying news videos' ideological perspectives using emphatic patterns of visual concepts","authors":"Wei-Hao Lin, Alexander G. Hauptmann","author_ids":"8131758, 7661726","abstract":"Television news has become the predominant way of understanding the world around us, but individual news broadcasters can frame or mislead an audience's understanding of political and social issues. We are developing a computer system that can automatically identify highly biased television news and encourage audiences to seek news stories from contrasting viewpoints. But can computers identify the ideological perspective from which a news video was produced? We propose a method based on an empathic pattern of visual concepts: news broadcasters holding contrasting ideological beliefs appear to emphasize different subsets of visual concepts. We formalize the emphatic patterns and propose a statistical model. We evaluate the proposed model on a large broadcast news video archive with promising experimental results.","cites":"2","conferencePercentile":"29.33884298"},{"venue":"ACM Multimedia","id":"1b708cc971b2c16ae59b031fc57180042986424d","venue_1":"ACM Multimedia","year":"2007","title":"Novelty detection for cross-lingual news stories with visual duplicates and speech transcripts","authors":"Xiao Wu, Alexander G. Hauptmann, Chong-Wah Ngo","author_ids":"4824827, 7661726, 1751681","abstract":"An overwhelming volume of news videos from different channels and languages is available today, which demands automatic management of this abundant information. To effectively search, retrieve, browse and track cross-lingual news stories, a news story similarity measure plays a critical role in assessing the novelty and redundancy among them. In this paper, we explore the novelty and redundancy detection with visual duplicates and speech transcripts for cross-lingual news stories. News stories are represented by a sequence of keyframes in the visual track and a set of words extracted from speech transcript in the audio track. A major difference to pure text documents is that the number of keyframes in one story is relatively small compared to the number of words and there exist a large number of non-near duplicate keyframes. These features make the behavior of similarity measures different compared to traditional textual collections. Furthermore, the textual features and visual features complement each other for news stories. They can be further combined to boost the performance. Experiments on the TRECVID-2005 cross-lingual news video corpus show that approaches on textual features and visual features demonstrate different performance, and measures on visual features are quite effective. Overall, the cosine distance on keyframes is still a robust measure. Language models built on visual features demonstrate promising performance. The fusion of textual and visual features improves overall performance.","cites":"22","conferencePercentile":"81.51041667"},{"venue":"ACM Multimedia","id":"882995d1c14fd5cb8008e872badd80616d788c86","venue_1":"ACM Multimedia","year":"2007","title":"Practical elimination of near-duplicates from web video search","authors":"Xiao Wu, Alexander G. Hauptmann, Chong-Wah Ngo","author_ids":"4824827, 7661726, 1751681","abstract":"Current web video search results rely exclusively on text keywords or user-supplied tags. A search on typical popular video often returns many duplicate and near-duplicate videos in the top results. This paper outlines ways to cluster and filter out the near-duplicate video using a hierarchical approach. Initial triage is performed using fast signatures derived from color histograms. Only when a video cannot be clearly classified as novel or near-duplicate using global signatures, we apply a more expensive local feature based near-duplicate detection which provides very accurate duplicate analysis through more costly computation. The results of 24 queries in a data set of 12,790 videos retrieved from Google, Yahoo! and YouTube show that this hierarchical approach can dramatically reduce redundant video displayed to the user in the top result set, at relatively small computational cost.","cites":"127","conferencePercentile":"97.65625"},{"venue":"ACM Multimedia","id":"b151c04d9fe0574d202c61a874bf0c7600569965","venue_1":"ACM Multimedia","year":"2014","title":"MiSCon: a hot plugging tool for real-time motion-based system control","authors":"Jun Chen, Chaokun Wang, Lei Yang, Qingfu Wen, Xu Wang","author_ids":"3543702, 8205851, 1704442, 3297797, 1712593","abstract":"In this demonstration, we proposed a hot plugging tool for the real-time motion-based system control, which is more portable and application-independent than the existing commercial motion-based sensing devices such as Kinect, Wii and PlayStation Move. This tool captures and recognizes people's real-time motions through the built-in camera of PCs, mobile phones or tablets, and automatically executes the system events which have been mapped with people's customized body motion, e.g., the head and the fist. The tool relieves people from the conventional ways to play games and use applications, and enables them to customize their preferred ways to control the systems.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"0b15b4fec6e98aa94bebe37d001cd006c4138c47","venue_1":"ACM Multimedia","year":"2007","title":"Cross-domain video concept detection using adaptive svms","authors":"Jun Yang, Rong Yan, Alexander G. Hauptmann","author_ids":"2812015, 1767526, 7661726","abstract":"Many multimedia applications can benefit from techniques for adapting existing classifiers to data with different distributions. One example is cross-domain video concept detection which aims to adapt concept classifiers across various video domains. In this paper, we explore two key problems for classifier adaptation: (1) how to transform existing classifier(s) into an effective classifier for a new dataset that only has a limited number of labeled examples, and (2) how to select the best existing classifier(s) for adaptation. For the first problem, we propose <i>Adaptive Support Vector Machines</i> (A-SVMs) as a general method to adapt one or more existing classifiers of any type to the new dataset. It aims to learn the \"delta function\" between the original and adapted classifier using an objective function similar to SVMs. For the second problem, we estimate the performance of each existing classifier on the sparsely-labeled new dataset by analyzing its score distribution and other meta features, and select the classifiers with the best estimated performance. The proposed method outperforms several baseline and competing methods in terms of classification accuracy and efficiency in cross-domain concept detection in the TRECVID corpus.","cites":"229","conferencePercentile":"99.47916667"},{"venue":"ACM Multimedia","id":"1c9912035fd9ccbff05a315624889c410edc79c6","venue_1":"ACM Multimedia","year":"2006","title":"Extreme video retrieval: joint maximization of human and computer performance","authors":"Alexander G. Hauptmann, Wei-Hao Lin, Rong Yan, Jun Yang, Ming-yu Chen","author_ids":"7661726, 8131758, 1767526, 2812015, 2668362","abstract":"We present an efficient system for video search that maximizes the use of human bandwidth, while at the same time exploiting the machine's ability to learn in real-time from user selected relevant video clips. The system exploits the human capability for rapidly scanning imagery augmenting it with an active learning loop, which attempts to always present the most relevant material based on the current information. Two versions of the human interface were evaluated, one with variable page sizes and manual paging, the other with a fixed page size and automatic paging. Both require absolute attention and focus of the user for optimal performance. In either case, as users search and find relevant results, the system can invisibly re-rank its previous best guesses using a number of knowledge sources, such as image similarity, text similarity, and temporal proximity. Experimental evidence shows a significant improvement using the combined extremes of human and machine power over either approach alone.","cites":"63","conferencePercentile":"95.59585492"},{"venue":"ACM Multimedia","id":"f302da1a1d4090c4359fab373e2f61a127ee1436","venue_1":"ACM Multimedia","year":"2002","title":"Experiences with building middleware for audio and visual networked home appliances on commodity software","authors":"Tatsuo Nakajima","author_ids":"2846273","abstract":"In this paper, we describe our currently ongoing work to build distributed middleware for networked audio and visual home appliances, which is executed on commodity software. The current prototype has adopted HAVi(Home Audio/Video Interoperability) as distributed middleware for controlling home appliances, which makes it possible to integrate a variety of home appliances and services. In our system, we have implemented HAVi in Java. Programs processing continuous media that emulate digital consumer devices are directly implemented on the Linux operating systems. Since Java and Linux are running on a variety of embedded platforms, our software can be ported to a target embedded system without the modification. Therefore, new home applications can be developed on a standard PC platform, and this makes the development cost of future advanced home appliances dramatically cheap.","cites":"5","conferencePercentile":"32.90598291"},{"venue":"ACM Multimedia","id":"b279303c2546b899946203e03d553f3557df46bc","venue_1":"ACM Multimedia","year":"2006","title":"3WNews: who, where, and when in news video","authors":"Jun Yang, Alexander G. Hauptmann","author_ids":"2812015, 7661726","abstract":"We describe 3WNews as a novel system for browsing news video by the people (who) and locations (where) appearing in the footage as well as the time (when) of news events. The people names, locations, and time expressions are recognized from video transcript and their ambiguous references are resolved. As a key advantage, 3WNews distinguishes the people and locations that actually appear in the video from those merely mentioned in the transcript, and uses them as (better) indexes for browsing. It also supports browsing of news video by event time instead of broadcasting time.","cites":"0","conferencePercentile":"9.067357513"},{"venue":"ACM Multimedia","id":"206867c2e9610bb8fd173fd8d1bc589bf5e92fd2","venue_1":"ACM Multimedia","year":"2007","title":"Reliability-based 3D reconstruction in real environment","authors":"Hansung Kim, Ryuuki Sakamoto, Itaru Kitahara, Tomoji Toriyama, Kiyoshi Kogure","author_ids":"2057244, 1767579, 7251192, 2032579, 1791839","abstract":"We present a practical 3D reconstruction method that guarantees robust visual hull construction in real environments where segmentation errors and occlusion exist. The proposed method consists of foreground extraction and reliability-based shape-from-silhouette, and they are connected by the intra-/inter-silhouette reliabilities. In foreground extraction, all regions are classified into four categories based on their intra-reliabilities. Then the reliability-based shape-from-silhouette technique reconstructs a visual hull by carving a 3D space based on the intra-/inter-silhouette reliabilities. The proposed method provides a reliable visual hull in real environments without much increment of the system complexity compared with conventional systems.","cites":"4","conferencePercentile":"40.36458333"},{"venue":"ACM Multimedia","id":"1c9fc401f7249b9dbb4a623605b621d724250fd7","venue_1":"ACM Multimedia","year":"2005","title":"Putting active learning into multimedia applications: dynamic definition and refinement of concept classifiers","authors":"Ming-yu Chen, Michael G. Christel, Alexander G. Hauptmann, Howard D. Wactlar","author_ids":"2668362, 7307726, 7661726, 1679880","abstract":"The authors developed an extensible system for video exploitation that puts the user in control to better accommodate novel situations and source material. Visually dense displays of thumbnail imagery in storyboard views are used for shot-based video exploration and retrieval. The user can identify a need for a class of audiovisual detection, adeptly and fluently supply training material for that class, and iteratively evaluate and improve the resulting automatic classification produced via multiple modality active learning and SVM. By iteratively reviewing the output of the classifier and updating the positive and negative training samples with less effort than typical for relevance feedback systems, the user can play an active role in directing the classification process while still needing to truth only a very small percentage of the multimedia data set. Examples are given illustrating the iterative creation of a classifier for a concept of interest to be included in subsequent investigations, and for a concept typically deemed irrelevant to be weeded out in follow-up queries. Filtering and browsing tools making use of existing and iteratively added concepts put the user further in control of the multimedia browsing and retrieval process.","cites":"26","conferencePercentile":"81.18811881"},{"venue":"ACM Multimedia","id":"29479bb4fe8c04695e6f5ae59901d15f8da6124b","venue_1":"ACM Multimedia","year":"2005","title":"Multiple instance learning for labeling faces in broadcasting news video","authors":"Jun Yang, Rong Yan, Alexander G. Hauptmann","author_ids":"2812015, 1767526, 7661726","abstract":"Labeling faces in news video with their names is an interesting research problem which was previously solved using supervised methods that demand significant user efforts on labeling training data. In this paper, we investigate a more challenging setting of the problem where there is no complete information on data labels. Specifically, by exploiting the uniqueness of a face's name, we formulate the problem as a special multi-instance learning (MIL) problem, namely exclusive MIL or eMIL problem, so that it can be tackled by a model trained with <i>partial</i> labeling information as the anonymity judgment of faces, which requires less user effort to collect. We propose two discriminative probabilistic learning methods named Exclusive Density (ED) and Iterative ED for eMIL problems. Experiments on the face labeling problem shows that the performance of the proposed approaches are superior to the traditional MIL algorithms and close to the performance achieved by supervised methods trained with complete data labels.","cites":"33","conferencePercentile":"85.14851485"},{"venue":"ACM Multimedia","id":"ffd376fd5b259cd41a5355a1a86455764d6a3156","venue_1":"ACM Multimedia","year":"2010","title":"i-m-Space: interactive multimedia-enhanced space for rehabilitation of breast cancer patients","authors":"Ju-Chun Ko, Wei-Han Chen, Meng-Chieh Yu, Han-Hung Lin, Jin-Yao Lin, Szu-Wei Wu, Yi-Yu Chung, I-Ling Hu, Wei-Ting Peng, Shih-Yao Lin, Chia-Han Chang, Pei-Hsuan Chou, King-Jen Chang, Mei-Lan Chang, Sue-huei Chen, Jin-Shing Chen, Ming-Sui Lee, Mike Y. Chen, Yi-Ping Hung","author_ids":"3243124, 2824756, 1740396, 2027846, 2730227, 2250469, 1878878, 3334037, 2954425, 1794068, 1925968, 6000166, 2961173, 2465673, 2405540, 2244990, 3288949, 2335746, 7312257","abstract":"This paper presents i-m-Space, an interactive multimedia rehabilitation space that helps the post-surgery recovery of breast cancer patients. Our goal is to improve patients' physical therapy and psychological relaxation experience through careful applications of multimedia technology. i-m-Space consists of three types of breathing-based relaxation and three types for interactive exercise-based rehabilitation.\n Our inter-disciplinary team includes medical professionals, multimedia engineers, designers, and artists. We have implemented i-m-Space in an experimental space in collaboration a local breast cancer foundation. To evaluation i-m-Space, we have recruited several patients who recently recovered from breast cancer to use i-m-Space and to share their first-hand experiences. Our contributions includes the following: 1) injecting a sense of fun and playfulness into traditional therapy to attract patients; 2) providing therapists with sufficient flexibility so they can personalize therapy sessions for each patient; 3) maintaining safety of patients.","cites":"0","conferencePercentile":"9.452054795"},{"venue":"ACM Multimedia","id":"2391debd04773199dc00f5d463603e244bb7adba","venue_1":"ACM Multimedia","year":"2004","title":"Scenographies of the past and museums of the future: from the wunderkammer to body-driven interactive narrative spaces","authors":"Flavia Sparacino","author_ids":"2951448","abstract":"This paper offers an overview and discussion of the numerous innovative technological solutions adopted for the exhibit: \"Puccini Set Designer\" (\"La Scena di Puccini\"), organized with the support and collaboration of Milan's renown La Scala opera theater. The exhibition used a wide range of state-of-the-art technologies to convey most effectively to the audience Puccini's work as set designer. For the co-presence and coordinated use of several technologies that transform the visitor from passive spectator to orchestrator of the museum experience, it marks a step towards the \"museum of the future\". A true innovator in opera set design, Giacomo Puccini broke new ground through the use of both modern technologies - such as electric stage lighting - and a narrative structure closer to the audience of his day. Similarly, drawing inspiration from the Puccinian set, this exhibition reinterprets the museum space as an exquisitely scenic place where lighting, choreography, narrative rhythm, costumes and colors are produced with the aid of state-of-the-art technologies. The museum space enhanced by these new narrative tools based on innovative technologies resembles a stage set where the main characters are the objects themselves, a set complete with special effects and stage tricks expressly designed to delight the spectator, and keep his interest alive.","cites":"29","conferencePercentile":"82.84313725"},{"venue":"ACM Multimedia","id":"a33387912f977f3eff0548e006efe930504450b5","venue_1":"ACM Multimedia","year":"2000","title":"SURFing the home with your TV","authors":"Dimitrios Voutsas, Christine Halverson","author_ids":"2545887, 2875633","abstract":"SURF (Simple UI to Retrieve Favorites) is a system with which a user can interact, through a TV, both with electronic devices inside the house and with the World Wide Web (WWW). The user communicates with the TV  by voice in two modes: reactive and proactive. We illustrate how, by speaking naturally, the user can check on locations in the home, or get information from appliances and specific information from the Internet, in a manner that does not interrupt the TV viewing experience.","cites":"3","conferencePercentile":"32.60869565"},{"venue":"ACM Multimedia","id":"3e3e45476380ee034928caaad677db31c51f8aa3","venue_1":"ACM Multimedia","year":"2005","title":"What is the state of our community?","authors":"Yong Rui, Ramesh Jain, Nicolas D. Georganas, HongJiang Zhang, Klara Nahrstedt, John R. Smith, Mohan S. Kankanhalli","author_ids":"1728806, 4521564, 1723057, 1718558, 1688353, 1788270, 1744045","abstract":"Panel Topic Compared with other research areas, multimedia is still a relative young community. While we made significant progress in the past decade on multimedia data capture, transmission, analysis and authoring, we still have a long way to meet our grand challenges [1]. For our healthy growth in the past decade, we owe thanks to the pioneers and leaders. We are all now responsible to elevate the community to the next level. A question that we multimedia researchers want to ask ourselves is why multimedia is still not a first-tier research area. In this panel, we bring together SIGMM Chair, EiCs of major multimedia journals, and past ACM multimedia program chairs together to explore potential solutions. Curricular • Observation: While multimedia related courses are taught here and there in schools, it is not part of the core CS/EE curricular. • Question: Can we have a healthy and strong multimedia community without having it in the core curricular? What makes multimedia a required course? What should be the main chapters of a multimedia textbook? What are the foundational sciences of multimedia? Industry support • Observation: SIGGRAPH and SIGCHI are quite influential partly because they both have strong industries stand behind them. • Question: What is our industry? Can it be distributed multimedia communication and collaboration systems? Can it be multimedia authoring? Who are the influential companies in those domains? What are our potential killer applications? Can this community be first tier without owning killer applications? Conferences • Observation: We have one good conference, and then many third tier ones. • Question: Can the community be healthy without having a strong 2 nd tier conference? How can we work toward establishing at least one more good multimedia conference? Review process • Observation: Though we have the retreat report [1], not all the conference PC members or journal reviewers follow the suggestions from the report. We do not have a uniform guide line. A paper can be rejected just because the reviewer has a different definition of multimedia. • Question: How should we select PC members? What is a good review process? Is combining two media together enough to be a multimedia paper? If we want to encourage a particular research direction, e.g., audio retrieval, or hepatic sensors, can single media be good enough to be a multimedia paper? Should we emphasize that human (e.g., as an interacting component, or as an end …","cites":"4","conferencePercentile":"36.63366337"},{"venue":"ACM Multimedia","id":"fe6f3dd621c7154c13f3b8910bdfb6b18a1ccae2","venue_1":"ACM Multimedia","year":"2003","title":"Approximate matching algorithms for music information retrieval using vocal input","authors":"Richard L. Kline, Ephraim P. Glinert","author_ids":"1832207, 3072281","abstract":"Effective use of multimedia collections requires efficient and intuitive methods of searching and browsing. This work considers databases which store music and explores how these may best be searched by providing input queries in some musical form. For the average person, humming several notes of the desired melody is the most straightforward method for providing this input, but such input is very likely to contain several errors. Previously proposed implementations of so-called <i>query-by-humming</i> systems are effective only when the number of input errors is small. We conducted experiments which revealed that the expected error rate for user queries is much higher than existing algorithms can tolerate. We then developed algorithms based on approximate matching techniques which deliver much improved results when comparing error-filled vocal user queries against a music collection.","cites":"11","conferencePercentile":"52.25225225"},{"venue":"ACM Multimedia","id":"148eddfd371fc29af8bdf54b3c7380926060ffc8","venue_1":"ACM Multimedia","year":"2012","title":"A domain-specific music search engine for gait training","authors":"Zhonghua Li, Ye Wang","author_ids":"2746295, 1681196","abstract":"This paper demonstrates a domain-specific music retrieval system to help music therapists find appropriate music for Parkinson's disease patients in their gait training. Different from existing music search engines, this system incorporates multiple music dimensions (i.e., tempo, cultural style, and beat strength) required in gait training, and facilitates the searching process by allowing music retrieval directly on these dimensions. To support music search by tempo, a user-perception based method is also proposed to improve state-of-the-art tempo estimation algorithms. We conducted a user study and evaluated the system efficacy in searching music using different types of queries based on these music dimensions. Experimental results demonstrate the effectiveness and usability of our system in therapeutic gait training.","cites":"1","conferencePercentile":"32.75316456"},{"venue":"ACM Multimedia","id":"1e9473df815cf8079a6a10e4fefc2d65ed376e27","venue_1":"ACM Multimedia","year":"2004","title":"Multimedia streaming via TCP: an analytic performance study","authors":"Bing Wang, James F. Kurose, Prashant J. Shenoy, Donald F. Towsley","author_ids":"1718232, 1775839, 1705052, 1705427","abstract":"TCP is widely used in commercial media streaming systems, with recent measurement studies indicating that a significant fraction of Internet streaming media is currently delivered over HTTP/TCP. These observations motivate us to develop analytic performance models to systematically investigate the performance of TCP for both live and stored media streaming. We validate our models via &#60;i>ns&#60;/i> simulations and experiments conducted over the Internet. Our models provide guidelines indicating the circumstances under which TCP streaming leads to satisfactory performance, showing, for example, that TCP generally provides good streaming performance when the achievable TCP throughput is roughly twice the media bitrate, with only a few seconds of startup delay.","cites":"138","conferencePercentile":"99.01960784"},{"venue":"ACM Multimedia","id":"57b1d397d5573fd914034101ae38f33880d61897","venue_1":"ACM Multimedia","year":"2009","title":"Real-time remote rendering of 3D video for mobile devices","authors":"Shu Shi, Won Jong Jeon, Klara Nahrstedt, Roy H. Campbell","author_ids":"1737787, 2590470, 1688353, 1687256","abstract":"At the convergence of computer vision, graphics, and multimedia, the emerging 3D video technology promises immersive experiences in a truly seamless environment. However, the requirements of huge network bandwidth and computing resources make it still a big challenge to render 3D video on mobile devices at real-time. In this paper, we present how remote rendering framework can be used to solve the problem. The differences between dynamic 3D video and static graphic models are analyzed. A general proxy-based framework is presented to render 3D video streams on the proxy and transmit the rendered scene to mobile devices over a wireless network. An image-based approach is proposed to enhance 3D interactivity and reduce the interaction delay. Experiments prove that the remote rendering framework can be effectively used for quality 3D video rendering on mobile devices in real time.","cites":"18","conferencePercentile":"82.85123967"},{"venue":"ACM Multimedia","id":"94fa19f5560b50dc2b20f5f324342792bf2b6523","venue_1":"ACM Multimedia","year":"2009","title":"MobileTI: a portable tele-immersive system","authors":"Wanmin Wu, Raoul Rivas, Ahsan Arefin, Shu Shi, Renata M. Sheppard, Bach D. Bui, Klara Nahrstedt","author_ids":"2524314, 2385623, 3356724, 1737787, 3090516, 2052485, 1688353","abstract":"We present MobileTI, a portable tele-immersive system that merges 3D video representations of users in real time to enable remote collaboration across geographical distances. With portability as a main goal, we address the challenges in the camera setup, time synchronization, video acquisition, and networking in the design and implementation of the system. Having been deployed in public performances, MobileTI proves to be effective, efficient, and user-friendly. Our experimental findings in terms of technical performance and user feedback are presented.","cites":"8","conferencePercentile":"60.12396694"},{"venue":"ACM Multimedia","id":"37217f11b576b2b43f245295553f1aca413d59ae","venue_1":"ACM Multimedia","year":"2000","title":"IRM: integrated region matching for image retrieval","authors":"Jia Li, James Ze Wang, Gio Wiederhold","author_ids":"1687366, 1699550, 1683556","abstract":"Content-based image retrieval using region segmentation has been an active research area. We present IRM (Integrated Region Matching), a novel similarity measure for region-based image similarity comparison. The targeted image retrieval systems represent an image by a set of regions, roughly corresponding to objects, which are characterized by features reflecting color, texture, shape, and location properties. The IRM measure for evaluating overall similarity between images incorporates properties of all the regions in the images by a region-matching scheme. Compared with retrieval based on individual regions, the overall similarity approach reduces the influence of inaccurate segmentation, helps to clarify the semantics of a particular region, and enables a simple querying interface for region-based image retrieval systems. The IRM has been implemented as a part of our experimental SIMPLIcity image retrieval system. The application to a database of about 200,000 general-purpose images shows exceptional robustness to image alterations such as intensity variation, sharpness variation, color distortions, shape distortions, cropping, shifting, and rotation. Compared with several existing systems, our system in general achieves more accurate retrieval at higher speed.","cites":"152","conferencePercentile":"98.91304348"},{"venue":"ACM Multimedia","id":"1648424be391939a81d00cb30a7b649209e0ee2a","venue_1":"ACM Multimedia","year":"2014","title":"Fast Instance Search Based on Approximate Bichromatic Reverse Nearest Neighbor Search","authors":"Masakazu Iwamura, Nobuaki Matozaki, Koichi Kise","author_ids":"1788920, 2510296, 8716080","abstract":"In the TRECVID Instance Search (INS) task, it is known that use of BM25, which is an improvement of the TFIDF,greatly improves retrieval performance. Its calculation, however, requires tremendous amount of computational cost and this fact makes its use intractable. In this paper, we present its efficient computational method. Since the BM25 is obtained by solving the bichromatic reverse nearest neighbor (BRNN)search problem,we propose an approximate method for the problem based on the state-of-the-art approximate nearest neighbor search method, bucket distance hashing (BDH). An experiment using the TRECVID INS 2012 dataset showed that the proposed method reduced computational cost to less than 1/3500 of the brute-force search with keeping the accuracy.","cites":"0","conferencePercentile":"16.86746988"},{"venue":"ACM Multimedia","id":"3055b4df91e92c32637f3a5146c13875c963dc72","venue_1":"ACM Multimedia","year":"2008","title":"Advancing interactive collaborative mediums through tele-immersive dance (TED): a symbiotic creativity and design environment for art and computer science","authors":"Renata M. Sheppard, Mahsa Kamali, Raoul Rivas, Morihiko Tamai, Zhenyu Yang, Wanmin Wu, Klara Nahrstedt","author_ids":"3090516, 2069498, 2385623, 1773079, 1771274, 2524314, 1688353","abstract":"The Tele-immersive Dance Environment (TED) is a geographically distributed, real-time 3-D virtual room where multiple participants interact independent of physical distance. TED, a highly interactive collaborative environment, offers digital options with multiple viewpoints, enhancing the creative movement composition involved with dance choreography. A symbiotic relationship for creativity and design exists between dance artists and computer scientists as the tele-immersive environment is analyzed as a creativity and learning tool. We introduce the advancements of the interactive digital options, new interface developments, user study results, and the possibility of a computational model for human creativity through Laban Movement Analysis.","cites":"27","conferencePercentile":"88.99082569"},{"venue":"ACM Multimedia","id":"1ac8ccd720e08a2e9c67400b48d73b9a746553b7","venue_1":"ACM Multimedia","year":"2005","title":"Media processing workflow design and execution with ARIA","authors":"Lina Peng, Gisik Kwon, K. Selçuk Candan, Kyung Dong Ryu, Karam S. Chatha, Hari Sundaram, Yinpeng Chen","author_ids":"6467891, 2416487, 1720972, 2591044, 2427496, 8607462, 2249952","abstract":"Recently, we introduced a novel ARchitecture for Interactive Arts (ARIA) middleware that processes, filters, and fuses sensory inputs and actuates responses in real-time while providing various Quality of Service (QoS) guarantees. The objective of ARIA is to incorporate realtime, sensed, and archived media and audience responses into live performances, on demand. An ARIA media workflow graph describes <i>how</i> the data sensed through media capture devices will be processed and <i>what</i> audio-visual responses will be actuated. Thus, each data object streamed between ARIA processing components is subject to transformations, as described by a media workflow graph. The media capture and processing components, such as media filters and fusion operators, are programmable and adaptable; i.e, the delay, size, frequency, and quality/precision characteristics of individual operators can be controlled via a number of parameters. In [1, 4, 5], we developed static and dynamic optimization algorithms which maximize the quality of the actuated responses, minimize the corresponding delay and the resource usage. In this demonstration, we present the ARIA GUI and the underlying kernel. More specifically, we describe how to design a media processing workflow, with adaptive operators, using the ARIA GUI and how to use the various optimization and adaptation alternatives provided by the ARIA kernel to execute media processing workflows.","cites":"2","conferencePercentile":"22.02970297"},{"venue":"ACM Multimedia","id":"4b56f50ce5e9e73d54f973609436490f9f60c957","venue_1":"ACM Multimedia","year":"2006","title":"The design of a real-time, multimodal biofeedback system for stroke patient rehabilitation","authors":"Yinpeng Chen, He Huang, Weiwei Xu, Richard Isaac Wallis, Hari Sundaram, Thanassis Rikakis, Todd Ingalls, Loren Olson, Jiping He","author_ids":"2249952, 1724708, 6953977, 8043317, 8607462, 7543848, 3327062, 2827994, 1806373","abstract":"This paper presents a novel real-time, multi-modal biofeedback system for stroke patient therapy. The problem is important as traditional mechanisms of rehabilitation are monotonous, and do not incorporate detailed quantitative assessment of recovery in addition to traditional clinical schemes. We have been working on developing an experiential media system that integrates task dependent physical therapy and cognitive stimuli within an interactive, multimodal environment. The environment provides a purposeful, engaging, visual and auditory scene in which patients can practice functional therapeutic reaching tasks, while receiving different types of simultaneous feedback indicating measures of both performance and results. There are three contributions of this paper - (a) identification of features and goals for the functional task (b) The development of sophisticated feedback (auditory and visual) mechanisms that match the semantics of action of the task. We additionally develop novel action-feedback coupling mechanisms. (c) New metrics to validate the ability of the system to promote learnability, stylization and engagement. We have validated the system for nine subjects with excellent results.","cites":"19","conferencePercentile":"83.93782383"},{"venue":"ACM Multimedia","id":"40a4ed38493170fdcd071ce32c1e493743dbc455","venue_1":"ACM Multimedia","year":"2015","title":"Modeling Perspective Effects in Photographic Composition","authors":"Zihan Zhou, Siqiong He, Jia Li, James Ze Wang","author_ids":"2519795, 3241942, 1687366, 1699550","abstract":"Automatic understanding of photo composition is a valuable technology in multiple areas including digital photography, multimedia advertising, entertainment, and image retrieval. In this paper, we propose a method to model geometrically the compositional effects of linear perspective. Comparing with existing methods which have focused on basic rules of design such as simplicity, visual balance, golden ratio, and the rule of thirds, our new quantitative model is more comprehensive whenever perspective is relevant. We first develop a new hierarchical segmentation algorithm that integrates classic photometric cues with a new geometric cue inspired by perspective geometry. We then show how these cues can be used directly to detect the dominant vanishing point in an image without extracting any line segments, a technique with implications for multimedia applications beyond this work. Finally, we demonstrate an interesting application of the proposed method for providing on-site composition feedback through an image retrieval system.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"b54677ec96feef3092ebba4f8365e49785fa945d","venue_1":"ACM Multimedia","year":"2009","title":"Visual localization of non-stationary sound sources","authors":"Yuyu Liu, Yoichi Sato","author_ids":"3329496, 1711166","abstract":"Sound source can be visually localized by analyzing the correlation between audio and visual data. To correctly analyze this correlation, the sound source is required to be stationary in a scene to date. We introduce a technique that localizes the non-stationary sound sources to overcome this limitation. The problem is formulated as finding the optimal visual trajectories that best represent the movement of the sound source over the pixels in a spatio-temporal volume. Using a beam search, we search these optimal visual trajectories by maximizing the correlation between the newly introduced audiovisual features of inconsistency. An incremental correlation evaluation with mutual information is developed here, which significantly reduces the computational cost. The correlations computed along the optimal trajectories are finally incorporated into a segmentation technique to localize a sound source region in the first visual frame of the current time window. Experimental results demonstrate the effectiveness of our method.","cites":"5","conferencePercentile":"48.14049587"},{"venue":"ACM Multimedia","id":"76c8968193b3ffcbefca31268cce17bb83a27fca","venue_1":"ACM Multimedia","year":"2009","title":"Sensation-based photo cropping","authors":"Masashi Nishiyama, Takahiro Okabe, Yoichi Sato, Imari Sato","author_ids":"2817435, 1706742, 1711166, 1746794","abstract":"This paper proposes a novel method for automatically cropping a photo using a quality classifier that assesses whether the cropped region is agreeable to users. We statistically build this quality classifier using large photo collections available on websites where people manually insert quality scores to photos. We first trim the original image and then decide on the candidates for cropping. We find the cropped region with the highest quality score by applying the quality classifier to the candidates. Current automatic photo cropping techniques search for attention grabbing regions that consist of salient pixels from the original photo. They are not always pleasant to users because they do not take into account the quality of the cropped region. Our method with the quality classifier outperforms a state-of-the-art method that takes into consideration only the user's attention for automatic photo cropping.","cites":"41","conferencePercentile":"95.45454545"},{"venue":"ACM Multimedia","id":"30cf364221ecd5fb2860e200716b276e7ab87728","venue_1":"ACM Multimedia","year":"2010","title":"Saliency detection based on 2D log-gabor wavelets and center bias","authors":"Min Wang, Jia Li, Tiejun Huang, YongHong Tian, Ling-Yu Duan, Guochen Jia","author_ids":"4480979, 1687366, 1682042, 2529814, 7667912, 3330751","abstract":"Visual saliency can be a useful tool for image content analysis such as automatic image cropping and image compression. In existing methods on visual saliency detection, most of them are related to the model of receptive field. In this paper, we propose a bottom-up model which introduces 2D Log-Gabor wavelets for saliency detection. Compared with the traditional model of receptive field, the 2D Log-Gabor wavelets can better simulate the biological characteristics of the simple cortical cell in the receptive filed. Moreover, we also incorporate the influence of center bias into our model, which is a common phenomenon that directs visual attention to the center of images in natural scenes. Experimental results show that our approach outperforms three state-of-the-art approaches remarkably.","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"2d66077f8740110e2dcd83380435268fa924138b","venue_1":"ACM Multimedia","year":"2008","title":"View-dependent real-time 3d video compression for mobile devices","authors":"Shu Shi, Klara Nahrstedt, Roy H. Campbell","author_ids":"1737787, 1688353, 1687256","abstract":"3D video is an emerging technology that promises immersive experiences in a truly seamless environment. Currently, 3D video systems still require excessive bandwidth and computation power provided by gigabit switches and multi-core workstations machines. In order to extend the experience to mobile devices, we present a view-dependent compression methodology that shows great promise in making 3D video a reality on resource-constrained mobile devices. Using our technology, we are able to achieve a software-only rendering on a Nokia N800 PDA with only wireless network transmission. We believe that with the use of newer handhelds and improvements to our compression techniques, we will be able to deliver full-motion 3D video soon.","cites":"7","conferencePercentile":"58.48623853"},{"venue":"ACM Multimedia","id":"6951a24609451048922ae2ee1c8fd87a259f1f4b","venue_1":"ACM Multimedia","year":"2007","title":"Towards multi-site collaboration in tele-immersive environments","authors":"Wanmin Wu, Zhenyu Yang, Klara Nahrstedt, Gregorij Kurillo, Ruzena Bajcsy","author_ids":"2524314, 1771274, 1688353, 2862376, 1784213","abstract":"Tele-immersion is emerging as a new medium that creates 3D photorealistic, immersive, and interactive experience between geographically dispersed users. However, most existing tele-immersive systems can only support two-way collaboration. In this paper we propose a multi-layer framework and a new data dissemination protocol to support <i>multi-site</i> collaboration. The problem context is unique as multiple remote sites participate in an interactive tele-immersive session, where each site has multiple correlated 3D video streams to send (later referred as <i>multi-stream/multi-site</i> environments). The key challenge is to disseminate such large number of 3D live video streams among these sites subject to the bandwidth and latency constraints while satisfying QoS guarantees in visual quality. Among our findings is that the simple randomized algorithm outperforms many other static algorithms in the unique context. Moreover, the streams generated from one site have high semantic correlation, because often cameras at one site are shooting the same scene, only from different angles. We exploit the stream correlation in the multicast protocol to minimize the <i>level of loss</i>.","cites":"7","conferencePercentile":"53.90625"},{"venue":"ACM Multimedia","id":"66edc1de8b1cc9a968270b9fb87de75ee710de57","venue_1":"ACM Multimedia","year":"2006","title":"Learning from facial aging patterns for automatic age estimation","authors":"Xin Geng, Zhi-Hua Zhou, Yu Zhang, Gang Li, Honghua Dai","author_ids":"1735299, 1692625, 1727577, 1727716, 3173158","abstract":"Age Specific Human-Computer Interaction (ASHCI) has vast potential applications in daily life. However, automatic age estimation technique is still underdeveloped. One of the main reasons is that the aging effects on human faces present several unique characteristics which make age estimation a challenging task that requires non-standard classification approaches. According to the speciality of the facial aging effects, this paper proposes the AGES (AGing pattErn Subspace) method for automatic age estimation. The basic idea is to model the aging pattern, which is defined as a sequence of personal aging face images, by learning a representative subspace. The proper aging pattern for an unseen face image is then determined by the projection in the subspace that can best reconstruct the face image, while the position of the face image in that aging pattern will indicate its age. The AGES method has shown encouraging performance in the comparative experiments either as an age estimator or as an age range estimator.","cites":"67","conferencePercentile":"97.40932642"},{"venue":"ACM Multimedia","id":"1908e4565bd1384fe2826f2f3c2b9500b8070f7f","venue_1":"ACM Multimedia","year":"2010","title":"Automatic interesting object extraction from images using complementary saliency maps","authors":"Haonan Yu, Jia Li, YongHong Tian, Tiejun Huang","author_ids":"2910174, 1687366, 2529814, 1682042","abstract":"Automatic interesting object extraction is widely used in many image applications. Among various extraction approaches, saliency-based ones usually have a better performance since they well accord with human visual perception. However, nearly all existing saliency-based approaches suffer the integrity problem, namely, the extracted result is either a small part of the object (referred to as sketch-like) or a large region that contains some redundant part of the background (referred to as envelope-like). In this paper, we propose a novel object extraction approach by integrating two kinds of \"complementary\" saliency maps (i.e., sketch-like and envelope-like maps). In our approach, the extraction process is decomposed into two sub-processes, one used to extract a high-precision result based on the sketch-like map, and the other used to extract a high-recall result based on the envelope-like map. Then a classification step is used to extract an exact object based on the two results. By transferring the complex extraction task to an easier classification problem, our approach can effectively break down the integrity problem. Experimental results show that the proposed approach outperforms six state-of-art saliency-based methods remarkably in automatic object extraction, and is even comparable to some interactive approaches.","cites":"11","conferencePercentile":"80.1369863"},{"venue":"ACM Multimedia","id":"2b5de4fd3fda1856f955b42f06be565c1a78d240","venue_1":"ACM Multimedia","year":"2006","title":"A real-time, multimodal biofeedback system for stroke patient rehabilitation","authors":"Yinpeng Chen, Weiwei Xu, Richard Isaac Wallis, Hari Sundaram, Thanassis Rikakis, Todd Ingalls, Loren Olson, Jiping He","author_ids":"2249952, 6953977, 8043317, 8607462, 7543848, 3327062, 2827994, 1806373","abstract":"This paper presents a novel real-time, multi-modal biofeedback system for stoke patient therapy. The problem is important as traditional mechanisms of rehabilitation are monotonous, and do not incorporate detailed quantitative assessment of recovery in addition to traditional clinical schemes. We have been working on developing an experiential media system that integrates task dependent physical therapy and cognitive stimuli within an interactive, multimodal environment. The environment provides a purposeful, engaging, visual and auditory scene in which patients can practice functional therapeutic reaching tasks, while receiving different types of simultaneous feedback indicating measures of both performance and results. There are two contributions of this paper - (a) identification of features and goals for the functional task, (b) the development of sophisticated feedback (auditory and visual) mechanisms that match the semantics of action of the task.","cites":"3","conferencePercentile":"38.34196891"},{"venue":"ACM Multimedia","id":"50f9c565fc2cbd67185a2dfa39b8af05caf8b15c","venue_1":"ACM Multimedia","year":"2009","title":"Beyond flat surface computing: challenges of depth-aware and curved interfaces","authors":"Hrvoje Benko","author_ids":"2704133","abstract":"In the past decade, multi-touch-sensitive interactive surfaces have transitioned from pure research prototypes in the lab, to commercial products with wide-spread adoption. One of the longer term visions of this research follows the idea of ubiquitous computing, where everyday surfaces in our environment are made interactive. However, most of current interfaces remain firmly tied to the traditional flat rectangular displays of the today's computers and while they benefit from the directness and the ease of use, they are often not much more than <i>touch-enabled</i> standard desktop interfaces.\n In this paper, we argue for explorations that transcend the traditional notion of the flat display, and envision interfaces that are curved, three-dimensional, or that cross the boundary between the digital and physical world. In particular, we present two research directions that explore this idea: (a) exploring the three-dimensional interaction space above the display and (b) enabling gestural and touch interactions on curved devices for novel interaction possibilities. To illustrate both of these, we draw examples from our own work and the work of others, and guide the reader through several case studies that highlight the challenges and benefits of such novel interfaces. The implications on media requirements and collaboration aspects are discussed in detail, and, whenever possible, we highlight promising directions of future research. We believe that the compelling application design for future non-flat user interfaces will greatly depend on exploiting the unique characteristics of the given form factor.","cites":"34","conferencePercentile":"93.59504132"},{"venue":"ACM Multimedia","id":"3ecceb62f2437fe6276ce8f10bb8faeb4e8678f3","venue_1":"ACM Multimedia","year":"2007","title":"Media adaptation framework in biofeedback system for stroke patient rehabilitation","authors":"Yinpeng Chen, Weiwei Xu, Hari Sundaram, Thanassis Rikakis, Sheng-Min Liu","author_ids":"2249952, 6953977, 8607462, 7543848, 2318640","abstract":"In this paper, we present a media adaptation framework for an immersive biofeedback system for stroke patient rehabilitation. In our biofeedback system, media adaptation refers to changes in audio/visual feedback as well as changes in physical environment. Effective media adaptation frameworks help patients recover generative plans for arm movement with potential for significantly shortened therapeutic time. The media adaptation problem has significant challenges - (a) high dimensionality of adaptation parameter space (b) variability in the patient performance across and within sessions(c) the actual rehabilitation plan is typically a non first-order Markov process, making the learning task hard.\n Our key insight is to understand media adaptation as a real-time feedback control problem. We use a mixture-of-experts based Dynamic Decision Network (DDN) for online media adaptation. We train DDN mixtures per patient, per session. The mixture models address two basic questions - (a) given a specific adaptation suggested by the domain expert, predict patient performance and (b) given an expected performance, determine optimal adaptation decision. The questions are answered through an optimality criterion based search on DDN models trained in previous sessions. We have also developed new validation metrics and have very good results for both questions on actual stroke rehabilitation data.","cites":"7","conferencePercentile":"53.90625"},{"venue":"ACM Multimedia","id":"33b46897b5129cf83aed8e23eefbe1d2752871a5","venue_1":"ACM Multimedia","year":"2011","title":"Human-centric control of video functions and underlying resources in 3D tele-immersive systems","authors":"Wanmin Wu, Klara Nahrstedt","author_ids":"2524314, 1688353","abstract":"3D tele-immersion (3DTI) has the potential of enabling virtual-reality-like interaction among remote people with real-time 3D video. However, today's 3DTI systems still suffer from various performance issues, limiting their broader deployment, due to the enormous demand on temporal (computing) and spatial (networking) resources. Past research focused on system-centric approaches for technical optimization, without taking human users into the loop. We argue that human factors (including user preferences, semantics, limitations, etc.) are an important and integral part of the cyber-physical 3DTI systems, and should not be neglected.","cites":"1","conferencePercentile":"30.75801749"},{"venue":"ACM Multimedia","id":"0abe0d0d6ecfce98dc954270f6c7c26bc5195840","venue_1":"ACM Multimedia","year":"2011","title":"Using graphics rendering contexts to enhance the real-time video coding for mobile cloud gaming","authors":"Shu Shi, Cheng-Hsin Hsu, Klara Nahrstedt, Roy H. Campbell","author_ids":"1737787, 1806563, 1688353, 1687256","abstract":"The emerging cloud gaming service has been growing rapidly, but not yet able to reach mobile customers due to many limitations, such as bandwidth and latency. We introduce a 3D image warping assisted real-time video coding method that can potentially meet all the requirements of mobile cloud gaming. The proposed video encoder selects a set of key frames in the video sequence, uses the 3D image warping algorithm to interpolate other non-key frames, and encodes the key frames and the residues frames with an H.264/AVC encoder. Our approach is novel in taking advantage of the run-time graphics rendering contexts (rendering viewpoint, pixel depth, camera motion, etc.) from the 3D game engine to enhance the performance of video encoding for the cloud gaming service. The experiments indicate that our proposed video encoder has the potential to beat the state-of-art x264 encoder in the scenario of real-time cloud gaming. For example, by implementing the proposed method in a 3D tank battle game, we experimentally show that more than 2 dB quality improvement is possible.","cites":"17","conferencePercentile":"91.83673469"},{"venue":"ACM Multimedia","id":"9673b63c36c35dee105a4fba12af0d4c8b895603","venue_1":"ACM Multimedia","year":"2012","title":"A genetic algorithm for audio retargeting","authors":"Stephan Wenger, Marcus A. Magnor","author_ids":"1833738, 1686739","abstract":"We present an <i>audio retargeting</i> technique to create custom soundtracks for movies and games from existing audio material (typically music) by automatically rearranging audio segments. Constraints can be specified to make the length of the audio fit the length of a movie scene, or to align parts of a piece of music with particular events. Existing approaches typically create soundtracks with many unnecessary and often disruptive transitions. We extend a recent analysis and resynthesis method with a novel genetic algorithm for finding an optimal succession of audio segments that minimizes the number of audible transitions and repetitions as well as the deviation from user-specified constraints. Compared to prior work, our experiments with audio examples from different musical genres show a significant improvement with respect to the optimization criteria, and the resulting soundtracks contain few, if any, noticeable transitions.","cites":"4","conferencePercentile":"64.39873418"},{"venue":"ACM Multimedia","id":"8ae36348d77abd7e8e8471d9355222a88776d6c6","venue_1":"ACM Multimedia","year":"2011","title":"Networking of multimedia women event beyond epsilon science: where to look and how to realize new opportunities","authors":"Klara Nahrstedt, Svetha Venkatesh, Nalini Venkatasubramanian, Dulce B. Ponceleon, Maria Zemankova, Susanne Boll","author_ids":"1688353, 1679520, 1732742, 1802641, 2345443, 1714281","abstract":"\"Networking of Multimedia Women\" event is a continuation of an on-going conversation in the multimedia research community and efforts by the ACM SIGMM to engage and promote female researchers in multimedia community, enable networking of junior and senior female researchers, and give insights towards successful professional careers based on examples.\n This year, the event will have a theme, called \"Beyond Epsilon Science\", where preeminent senior female researchers from academia, industry and government, Svetha Venkatesh, Nalini Venkatasubramanian, Dulce Ponceleon, Susanne Boll, and Maria Zemankova will present and discuss how to go beyond epsilon science, where to look for big ideas with high social impact, as well as how to obtain funding to realize these ideas, innovations and opportunities. Their current research projects and funding efforts, and their personal experiences will drive the event's discussions, awareness of major research and funding initiatives, answers to open questions and insights into successful professional careers.","cites":"0","conferencePercentile":"10.64139942"},{"venue":"ACM Multimedia","id":"5a82a640989a85c5b9cdfa3e35abf0ea95e82fbd","venue_1":"ACM Multimedia","year":"2005","title":"Accurate repeat finding and object skipping using fingerprints","authors":"Cormac Herley","author_ids":"1679146","abstract":"This paper introduces a novel and very accurate segmentation algorithm. It is very efficient and consumes less than 10% of CPU on a simple desktop PC to segment a stream in real-time. It operates on an audio stream, or on the audio portion of a audio-visual stream. It is very accurate: it accurately detects the positions and durations of objects on an over-the-air broadcast television signal, and songs on both FM and internet radio stations (as checked against labeled ground truth streams). The algorithm does not require any prior information or training. We detail the system design and present results of segmenting broadcast streams.","cites":"7","conferencePercentile":"50"},{"venue":"ACM Multimedia","id":"96b98c27b0f8eafb50ee88ac88286d8f0637845f","venue_1":"ACM Multimedia","year":"2010","title":"TwitterSigns: microblogging on the walls","authors":"Markus Buzeck, Jörg Müller","author_ids":"2298670, 1798163","abstract":"In this paper we present TwitterSigns, an approach to display microblogs on public displays. Two different kinds of microblog entries (tweets) are selected for display: Tweets that were posted in the immediate environment of the display, and tweets that were posted by people associated with the location where the displays are installed (locals). The prototype was tested in a university setting on 4 displays for 4 weeks and compared to the information system that is usually running on the displays (iDisplays). Using face detection we show that people look significantly longer at TwitterSigns than at iDisplays. Interviews show that the relationship of viewer and poster as well as the tweet content are much more important than time and location of the tweet. Viewers recall and recognize mostly tweets from people they know, and of apparent importance for themselves (like a apparent bomb found in the city center). Furthermore, TwitterSigns change the way people use twitter (e.g. they feel more responsible for what they tweet). Passers-by seem only to look for keywords and only stop and read the whole tweet if they found some interesting keyword.","cites":"3","conferencePercentile":"49.17808219"},{"venue":"ACM Multimedia","id":"1f79baa7d0f65996b7a571b232e487e552efbc2d","venue_1":"ACM Multimedia","year":"2012","title":"3D fingertip and palm tracking in depth image sequences","authors":"Hui Liang, Junsong Yuan, Daniel Thalmann","author_ids":"3036305, 1746449, 1689760","abstract":"We present a vision-based approach for robust 3D fingertip and palm tracking on depth images using a single Kinect sensor. First the hand is segmented in the depth images by applying depth and morphological constraints. The palm is located by performing distance transform to the hand contour and tracked with a Kalman filter. The fingertips are detected by combining three depth-based features and tracked with a particle filter over successive frames. Quantitative results on synthetic depth sequences show the proposed scheme can track the fingertips quite accurately. Besides, its capabilities are further demonstrated through a real-life human-computer interaction application.","cites":"19","conferencePercentile":"93.67088608"},{"venue":"ACM Multimedia","id":"69ee40c5cb7625d7d737c4b514498b2813ed5a11","venue_1":"ACM Multimedia","year":"2011","title":"Color-plus-depth level-of-detail in 3D tele-immersive video: a psychophysical approach","authors":"Wanmin Wu, Ahsan Arefin, Gregorij Kurillo, Pooja Agarwal, Klara Nahrstedt, Ruzena Bajcsy","author_ids":"2524314, 3356724, 2862376, 8552114, 1688353, 1784213","abstract":"This paper presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Detail peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just Noticeable Degradation and Just Unacceptable Degradation thresholds on the factor. In light of the results, we describe the design and implementation of a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage while considerably enhancing the overall perceived visual quality.","cites":"18","conferencePercentile":"92.71137026"},{"venue":"ACM Multimedia","id":"f2c137261784bb942f4f51c5e6eb2c58c9e74d98","venue_1":"ACM Multimedia","year":"2009","title":"i-m-Tube: an interactive multi-resolution tubular display","authors":"Jin-Yao Lin, Yen-Yu Chen, Ju-Chun Ko, HuiShan Kao, Wei-Han Chen, Tsun-Hung Tsai, Su-Chu Hsu, Yi-Ping Hung","author_ids":"2730227, 3002155, 3243124, 2289658, 2824756, 3321401, 3327836, 7312257","abstract":"In this paper we introduce a tubular interface, the i-m-Tube, which provides convenient user interaction with multimedia content by multi-touch input and multi-resolution display. With its tubular surface, the i-m-Tube is suitable for displaying panoramic image content like the Chinese scroll painting \"Along the River During the Ch'ingming Festival\" which has been regarded as a national treasure and widely known by its extraordinary width and its various details. The strength of our system is that it is not just suitable for displaying panoramic content, but also possible to create an intuitive and natural context for user interaction with multimedia content displayed on the i-m-Tube by multi-finger touch. It also provides a high resolution area on demand, which is projected by a steerable projector in a Full-HD resolution. We have developed our system as an interface which can be applied adaptively to different applications, having the features of multi-touch and multi-resolution interactivity on a tubular display. In this work, we have implemented several prototypes designed for different applications, like \"Tetris360\" aiming on co-op arcade gaming, and \"Panoramic Cover Flow\" for interactive digital signage, to show the capabilities and opportunities of the i-m-Tube display system. With our preliminary experiments on the i-m-Tube, it is exciting to see the potential of the i-m-Tube as a new interface for people to meet the interactive multimedia touch in this generation.","cites":"7","conferencePercentile":"55.78512397"},{"venue":"ACM Multimedia","id":"1a4d09d08510dc128ba8017770e6f3b270494d37","venue_1":"ACM Multimedia","year":"2011","title":"A psychophysical approach for real-time 3D video processing","authors":"Wanmin Wu, Ahsan Arefin, Gregorij Kurillo, Pooja Agarwal, Klara Nahrstedt, Ruzena Bajcsy","author_ids":"2524314, 3356724, 2862376, 8552114, 1688353, 1784213","abstract":"This paper presents a psychophysical approach to control a new factor called Color-plus-Depth Level-of-Detail in polygon-based 3D tele-immersive video. Based on our psychophysical study that demonstrates the existence of perceptual thresholds on the factor, we present a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage while considerably enhancing the overall perceived visual quality.","cites":"2","conferencePercentile":"44.3148688"},{"venue":"ACM Multimedia","id":"4616f08347a89bfe13f99fd6e90f10a36313f685","venue_1":"ACM Multimedia","year":"2011","title":"Tele-immersive gaming for everybody","authors":"Ahsan Arefin, Zixia Huang, Raoul Rivas, Shu Shi, Wanmin Wu, Klara Nahrstedt","author_ids":"3356724, 5519938, 2385623, 1737787, 2524314, 1688353","abstract":"In this demonstration, we present two 3D tele-immersive games: light-saber dual and block fencing that merge 3D video representations of participants in real-time to enable remote interactions in a virtual world. The light-saber dual arranges participants in a symmetric setup where both participants interact with each other in a virtual world with similar goals. On the other hand, the block fencing creates an asymmetric setup where participants interact with virtual objects having different goals. Using these two setups, we address the challenges and novelty of our solutions in portable environment setup, data acquisition, multi-stream synchronization, multi-stream session management, mobile device rendering, and overlay communication in the design and implementation of advanced 3D tele-immersive systems.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"63b21f10dfef4e3fdf1efb526caafaccfe917a39","venue_1":"ACM Multimedia","year":"2010","title":"A high-quality low-delay remote rendering system for 3D video","authors":"Shu Shi, Mahsa Kamali, Klara Nahrstedt, John C. Hart, Roy H. Campbell","author_ids":"1737787, 2069498, 1688353, 1696485, 1687256","abstract":"As an emerging technology, 3D video has shown a great potential to become the next generation media for tele-immersion. However, streaming and rendering this dynamic 3D data in real-time requires tremendous network bandwidth and computing resources. In this paper, we build a remote rendering model to better study different remote rendering designs and define 3D video rendering as an optimization problem. Moreover, we design a 3D video remote rendering system that significantly reduces the delay while maintaining high rendering quality. We also propose a reference viewpoint prediction algorithm with super sampling support that requires much less computation resources but provides better performance than the search-based algorithms proposed in the related work.","cites":"15","conferencePercentile":"84.10958904"},{"venue":"ACM Multimedia","id":"69e87badd0313ba73e0eb20733270e697ed601f7","venue_1":"ACM Multimedia","year":"2009","title":"Quality of experience in distributed interactive multimedia environments: toward a theoretical framework","authors":"Wanmin Wu, Ahsan Arefin, Raoul Rivas, Klara Nahrstedt, Renata M. Sheppard, Zhenyu Yang","author_ids":"2524314, 3356724, 2385623, 1688353, 3090516, 1771274","abstract":"The past decades have witnessed a rapid growth of Distributed Interactive Multimedia Environments (DIMEs). Despite their intensity of user-involved interaction, the existing evaluation frameworks remain very much system-centric. As a step toward the human-centric paradigm, we present a conceptual framework of Quality of Experience (QoE) in DIMEs, to model, measure, and understand user experience and its relationship with the traditional Quality of Service (QoS) metrics. A multi-displinary approach is taken to build up the framework based on the theoretical results from various fields including psychology, cognitive sciences, sociology, and information technology. We introduce a mapping methodology to quantify the correlations between QoS and QoE, and describe our controlled and uncontrolled studies as illustrating examples. The results present the first deep study to model the multi-facet QoE construct, map the QoS-QoE relationship, and capture the human-centric quality modalities in the context of DIMEs.","cites":"70","conferencePercentile":"98.14049587"},{"venue":"ACM Multimedia","id":"59a29dd675001d8d69b475cbbe3bfbf478add399","venue_1":"ACM Multimedia","year":"2011","title":"A home-based adaptive mixed reality rehabilitation system","authors":"Diana Siwiak, Nicole Lehrer, Michael Baran, Yinpeng Chen, Margaret Duff, Todd Ingalls, Thanassis Rikakis","author_ids":"3304227, 3265651, 7696496, 2249952, 2756969, 3327062, 7543848","abstract":"This paper presents an interactive home-based adaptive mixed reality system (HAMRR) for upper extremity stroke rehabilitation. This home-based system is an extension of a previously designed and currently implemented clinical system. The goal of HAMRR is to restore motor function to chronic stroke survivors by providing an engaging long-term reaching task therapy at home. The HAMMR system tracks movement of the wrist and torso, and provides real-time, post-trial, and post-set multimodal feedback to encourage the stroke survivor to self-assess his or her movement and engage in active learning of new movement strategies. This experiential media system uses a computational adaptation scheme to create a continuously challenging and unique multi-year therapy experience through the use of multiple, integrated audio and visual feedback streams. Novel design features include creating an over-arching story for the participant, the ability of the system to adapt the feedback over multiple time scales, and the ability for this system to integrate into any home.","cites":"5","conferencePercentile":"67.93002915"},{"venue":"ACM Multimedia","id":"51d06c42b344ccdd4116b739d8fc7c9ab9853a97","venue_1":"ACM Multimedia","year":"2009","title":"VideoMule: a consensus learning approach to multi-label classification from noisy user-generated videos","authors":"Chandrasekar Ramachandran, Rahul Malik, Xin Jin, Jing Gao, Klara Nahrstedt, Jiawei Han","author_ids":"2816149, 3255046, 1785303, 1698083, 1688353, 1722175","abstract":"With the growing proliferation of conversational media and devices for generating multimedia content, the Internet has seen an expansion in websites catering to user-generated media. Most of the user-generated content is multimodal in nature as it has videos, audio, text (in the form of tags), comments and so on. Content analysis is a challenging problem on this type of media since it is noisy, unstructured and unreliable. In this paper we propose VideoMule, a consensus learning approach for multi-label video classification from noisy user-generated videos. In our scheme, we train classification and clustering algorithms on individual modes of information such as user comments, tags, video features and so on. We then combine the results of trained classifiers and clustering algorithms using a novel heuristic consensus learning algorithm which as a whole performs better than each individual learning model.","cites":"12","conferencePercentile":"75.41322314"},{"venue":"ACM Multimedia","id":"bef9e21f9f46f41ccdfbbb495180f5ece7feb675","venue_1":"ACM Multimedia","year":"2007","title":"Learning the consensus on visual quality for next-generation image management","authors":"Ritendra Datta, Jia Li, James Ze Wang","author_ids":"1778639, 1687366, 1699550","abstract":"While personal and community-based image collections grow by the day, the demand for novel photo management capabilities grows with it. Recent research has shown that it is possible to learn the consensus on visual quality measures such as <i>aesthetics</i> with a moderate degree of success. Here, we seek to push this performance to more realistic levels and use it to (a) help select high-quality pictures from collections, and (b) eliminate low-quality ones, introducing appropriate performance metrics in each case. To achieve this, we propose a sequential arrangement of a weighted linear least squares regressor and a naive Bayes' classifier, applied to a set of visual features previously found useful for quality prediction. Experiments on real-world data for these tasks show promising performance, with significant improvements over a previously proposed SVM-based method.","cites":"26","conferencePercentile":"85.67708333"},{"venue":"ACM Multimedia","id":"2500c19f3b9ca73f6627c5c323d40acef574c298","venue_1":"ACM Multimedia","year":"2015","title":"Interactive Scene Flow Editing for Improved Image-based Rendering and Virtual Spacetime Navigation","authors":"Kai Ruhl, Martin Eisemann, Anna Hilsmann, Peter Eisert, Marcus A. Magnor","author_ids":"2283927, 1701306, 3353355, 2516942, 1686739","abstract":"High-quality stereo and optical flow maps are essential for a multitude of tasks in visual media production, e.g. virtual camera navigation, disparity adaptation or scene editing. Rather than estimating stereo and optical flow separately, scene flow is a valid alternative since it combines both spatial and temporal information and recently surpassed the former two in terms of accuracy. However, since automated scene flow estimation is non-accurate in a number of situations, resulting rendering artifacts have to be corrected manually in each output frame, an elaborate and time-consuming task. We propose a novel workflow to edit the scene flow itself, catching the problem at its source and yielding a more flexible instrument for further processing. By integrating user edits in early stages of the optimization, we allow the use of approximate scribbles instead of accurate editing, thereby reducing interaction times. Our results show that editing the scene flow improves the quality of visual results considerably while requiring vastly less editing effort.","cites":"1","conferencePercentile":"56.48148148"},{"venue":"ACM Multimedia","id":"eb092263bfa779f6438d3d247d4f81e7b2dd20ac","venue_1":"ACM Multimedia","year":"2008","title":"Tele-immersive dance (TED): evolution in progress","authors":"Renata M. Sheppard, Mahsa Kamali, Morihiko Tamai, Raoul Rivas, Zhenyu Yang, Wanmin Wu, Klara Nahrstedt","author_ids":"3090516, 2069498, 1773079, 2385623, 1771274, 2524314, 1688353","abstract":"We demonstrate the Tele-immersive Dance Environment (TED), a geographically distributed, real-time 3-D virtual room where multiple participants interact independent of physical distance. TED, a highly interactive collaborative environment, offers digital options with multiple viewpoints, enhancing the creative movement composition involved with dance choreography. We present the advancements of the interactive digital options, new interface developments, and user study results.","cites":"0","conferencePercentile":"10.09174312"},{"venue":"ACM Multimedia","id":"31c194cf26ab638a4b3c331234294f857f25912d","venue_1":"ACM Multimedia","year":"2015","title":"Web-based Interactive Free-Viewpoint Streaming: A framework for high quality interactive free viewpoint navigation","authors":"Matthias Ueberheide, Felix Klose, Tilak Varisetty, Markus Fidler, Marcus A. Magnor","author_ids":"2568161, 2547248, 2751822, 1713236, 1686739","abstract":"Recent advances in free-viewpoint rendering techniques as well as the continued improvements of the internet network infrastructure open the door for challenging new applications. In this paper, we present a framework for interactive free-viewpoint streaming with open standards and software. Network bandwidth, encoding strategy as well as codec support for open source browsers are key constraints to be considered for our interactive streaming applications. Our framework is capable of real-time server-side rendering and interactively streaming the output by means of open source streaming. To enable viewer interaction with the free-viewpoint video rendering back-end in a standard browser, user events are captured with Javascript and transmitted using WebSockets. The rendered video is streamed to the browser using the FFmpeg free software project. This paper discusses the applicability of open source streaming and presents timing measurements for video-frame transmission over network.","cites":"0","conferencePercentile":"21.85185185"},{"venue":"ACM Multimedia","id":"57dd6a8b3e462550af7b743094df6f37c99864c6","venue_1":"ACM Multimedia","year":"2001","title":"FIRM: fuzzily integrated region matching for content-based image retrieval","authors":"Yixin Chen, James Ze Wang, Jia Li","author_ids":"1750884, 1699550, 1687366","abstract":"We propose FIRM (Fuzzily Integrated Region Matching), an efficient and robust similarity measure for region-based image retrieval. Each image in our retrieval system is represented by a set of regions that are characterized by fuzzy sets. The FIRM measure, representing the overall similarity between two images, is defined as the similarity between two families of fuzzy sets. Compared with similarity measures based on individual regions and on all regions with crisp feature representations, our approach greatly reduces the influence of inaccurate segmentation. Experimental results based on a database of about 200,000 general-purpose images demonstrate improved accuracy, robustness, and high speed.","cites":"3","conferencePercentile":"33.67346939"},{"venue":"ACM Multimedia","id":"54645594792b5e9d1af7041f8af923a2d1bd886b","venue_1":"ACM Multimedia","year":"2007","title":"New digital options in geographically distributed dance collaborations with TEEVE: tele-immersive environments for everybody","authors":"Renata M. Sheppard, Wanmin Wu, Zhenyu Yang, Klara Nahrstedt, Lisa Wymore, Gregorij Kurillo, Ruzena Bajcsy, Katherine Mezur","author_ids":"3090516, 2524314, 1771274, 1688353, 3135516, 2862376, 1784213, 2509305","abstract":"The study of 3D Tele-immersion impact on remote collaborative work represents a very interesting and challenging research topic. In this paper, we introduce the latest accomplishments of TEEVE research which merges computer science with dance choreography. This collaborative research model is ideal for creative, interdisciplinary problem solving. TEEVE offers an entirely new interface for dance choreography as a creative tool and alternative performance venue.","cites":"5","conferencePercentile":"45.83333333"},{"venue":"ACM Multimedia","id":"92baf7aad93d92b2f7c9e8b1f2e826e308c94bf9","venue_1":"ACM Multimedia","year":"2002","title":"Learning-based linguistic indexing of pictures with 2--d MHMMs","authors":"James Ze Wang, Jia Li","author_ids":"1699550, 1687366","abstract":"Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of concepts automatically based on statistical modeling. Images of any given concept category are regarded as instances of a stochastic process that characterizes the category. To measure the extent of association between an image and the textual description of a category of images, the likelihood of the occurrence of the image based on the stochastic process derived from the category is computed. A high likelihood indicates a strong association. In our experimental implementation, the ALIP (Automatic Linguistic Indexing of Pictures) system, we focus on a particular group of stochastic processes for describing images, that is, the two-dimensional multiresolution hidden Markov models (2-D MHMMs). We implemented and tested the system on a photographic image database of 600 different semantic cat- egories, each with about 40 training images. Tested using 3,000 images outside the training database, the system has demonstrated good accuracy and high potential in linguistic indexing of these test images.","cites":"39","conferencePercentile":"86.32478632"},{"venue":"ACM Multimedia","id":"bc4bc803efa73958608ff4cec9d094672ed39965","venue_1":"ACM Multimedia","year":"2005","title":"IMAGINATION: a robust image-based CAPTCHA generation system","authors":"Ritendra Datta, Jia Li, James Ze Wang","author_ids":"1778639, 1687366, 1699550","abstract":"We propose IMAGINATION (IMAge Generation for INternet AuthenticaTION), a system for the generation of attack-resistant, user-friendly, image-based CAPTCHAs. In our system, we produce controlled distortions on randomly chosen images and present them to the user for annotation from a given list of words. The distortions are performed in a way that satisfies the incongruous requirements of low perceptual degradation and high resistance to attack by content-based image retrieval systems. Word choices are carefully generated to avoid ambiguity as well as to avoid attacks based on the choices themselves. Preliminary results demonstrate the attack-resistance and user-friendliness of our system compared to text-based CAPTCHAs.","cites":"47","conferencePercentile":"91.58415842"},{"venue":"ACM Multimedia","id":"dca7321c31fbef3aba5778974df0698dce1621d3","venue_1":"ACM Multimedia","year":"2007","title":"Empirical study of 3D video source coding for autostereoscopic displays","authors":"Roger Cheng, Klara Nahrstedt","author_ids":"3823381, 1688353","abstract":"The recent commercial availability of autostereoscopic displays has led to a rise in interest in 3D video. The need to store and transmit 3D video has created some interesting challenges. 3D video contains both color and depth information, and should be treated differently from 2D video for optimal results. This paper explores ideas for efficient 3D video source coding, tests these ideas in a human subject test with 27 subjects, and analyzes and discusses the results. It is concluded that for the specific display and codec used, 3D video file sizes can be reduced to about one-quarter of their original sizes without significant degradation in quality.","cites":"3","conferencePercentile":"34.63541667"},{"venue":"ACM Multimedia","id":"5bdd0753190024c0338a03d108ca82976b603ba8","venue_1":"ACM Multimedia","year":"2005","title":"Two-scale image retrieval with significant meta-information feedback","authors":"Jia Li","author_ids":"1687366","abstract":"A two-scale image retrieval system is developed to provide efficient search in large-scale databases as well as flexibility for users to incorporate ubjective preferences during retrieval. A new clustering method is developed for images each characterized by a varying number of weighted feature vectors. Furthermore, significant meta-information is mined within every cluster. A scanning mode of retrieval is created using cluster centers, which serve as a low scale version of a database in contrast to original images. In particular, users are presented with representative images of highly ranked clusters along with prominent meta-information. This retrieval approach enables users to quickly examine a large and diverse portion of a database surrounding a query and to learn about hidden connections between visual patterns and non-imagery types of data. The clusters formed also facilitate fast search in the case of individual image-based retrieval by filtering out images whose cluster centers are far from the query. The two-scale retrieval system has been implemented on a fine art painting database. Advantages of the system have been demonstrated by quantitative evaluation of the retrieval performance.","cites":"8","conferencePercentile":"53.46534653"},{"venue":"ACM Multimedia","id":"4d81c9503f35ea60ca6f011086d41d4f12a1912a","venue_1":"ACM Multimedia","year":"2006","title":"Toward bridging the annotation-retrieval gap in image search by a generative modeling approach","authors":"Ritendra Datta, Weina Ge, Jia Li, James Ze Wang","author_ids":"1778639, 1713529, 1687366, 1699550","abstract":"While automatic image annotation remains an actively pursued research topic, enhancement of image search through its use has not been extensively explored. We propose an annotation-driven image retrieval approach and argue that under a number of different scenarios, this is very effective for semantically meaningful image search. In particular, our system is demonstrated to effectively handle cases of partially tagged and completely untagged image databases, multiple keyword queries, and example based queries with or without tags, all in near-realtime. Because our approach utilizes extra knowledge from a training dataset, it outperforms state-of-the-art visual similarity based retrieval techniques. For this purpose, a novel structure-composition model constructed from Beta distributions is developed to capture the spatial relationship among segmented regions of images. This model combined with the Gaussian mixture model produces scalable categorization of generic images. The categorization results are found to surpass previously reported results in speed and accuracy. Our novel annotation framework utilizes the categorization results to select tags based on term frequency, term saliency, and a WordNet-based measure of congruity, to boost salient tags while penalizing potentially unrelated ones. A bag of words distance measure based on WordNet is used to compute semantic similarity. The effectiveness of our approach is shown through extensive experiments.","cites":"25","conferencePercentile":"87.30569948"},{"venue":"ACM Multimedia","id":"12ea92a54a8c67934aa8740863ef0d3e48cb681d","venue_1":"ACM Multimedia","year":"2007","title":"Tagging over time: real-world image annotation by lightweight meta-learning","authors":"Ritendra Datta, Dhiraj Joshi, Jia Li, James Ze Wang","author_ids":"1778639, 6163828, 1687366, 1699550","abstract":"Automatic image annotation has been a hot-pursuit among multimedia researchers of late. Modest performance guarantees and limited adaptability often restrict its applicability to real-world settings. We propose <i>tagging over time</i> (T/T) to push the technology toward real-world applicability. Of particular interest are online systems that receive user-provided images and feedback over time, with user focus possibly changing and evolving. The T/T framework consists of a principled probabilistic approach to meta-learning, which acts as a go-between for a 'black-box' annotation system and the users. Inspired by <i>inductive transfer</i>, the approach attempts to harness available information, including the black-box model's performance, the image representations, and the WordNet ontology. Being computationally 'lightweight', this meta-learner efficiently re-trains over time, to improve and/or adapt to changes. The black-box annotation model is not required to be re-trained, allowing computationally intensive algorithms to be used. We experiment with standard image datasets and real-world data streams, using two existing annotation systems as black-boxes. Both batch and online annotation settings are experimented with. It is observed that the addition of this meta-learning layer produces much improved results that outperform best-known results. For the online setting, the T/T approach produces progressively better annotation with time, significantly outperforming the black-box as well as the static form of the meta-learner, on real-world data.","cites":"13","conferencePercentile":"72.13541667"},{"venue":"ACM Multimedia","id":"772b78ffe1e793d96b7b679d0d34e59c4668f1fe","venue_1":"ACM Multimedia","year":"1994","title":"The VuSystem: A Programming System for Visual Processing of Digital Video","authors":"Christopher Lindblad, David Wetherall, David L. Tennenhouse","author_ids":"1778053, 1780215, 1755120","abstract":"In computer-participative multimedia applications, the computer not only manipulates media, but also digests it and performs independent actions based on media content. We present a design approach that applies the programming techniques of visualization systems to the development of computer-participative multimedia applications. We describe an implementation based on this approach, and report performance measurements that demonstrate it is practical. We conclude by describing three applications written with the system, and suggest future directions for research in the area.","cites":"46","conferencePercentile":"78.81355932"},{"venue":"ACM Multimedia","id":"e58d794c65795a0e21b179b548b0fdc98756b1cd","venue_1":"ACM Multimedia","year":"2007","title":"Atomique: a photo repository for decentralized and distributed photo sharing on the web","authors":"Marian Dörk, Andreas Nürnberger, Javier Velasco-Martin","author_ids":"3271672, 1759689, 6973070","abstract":"Photo sharing sites such as Flickr [3] and Fotolog [4] allow users to publish, organize, and explore photos in a community context combining sharing, annotation, and conversation. However, most hosted communities have architectural and institutional shortcomings due to their usually centralized nature. Users have little or no control over the community's policies, functionality, or appearance. The social data, such as contacts, comments, and group activity, are locked within a community, while it is usually not possible to interact with users from other communities.\n In this work we introduce <i>Atomique</i> a Web-based software enabling decentralized photo sharing by using open data formats and protocols from the blogosphere, i.e., RSS and Trackback, allowing exchange and discussion of photos in decentralized groups.","cites":"0","conferencePercentile":"7.552083333"},{"venue":"ACM Multimedia","id":"9ddb90a629bc5e88c6f4f354cce7513ccec6f989","venue_1":"ACM Multimedia","year":"2015","title":"An Affordable Solution for Binocular Eye Tracking and Calibration in Head-mounted Displays","authors":"Michael Stengel, Steve Grogorick, Martin Eisemann, Elmar Eisemann, Marcus A. Magnor","author_ids":"2267017, 2860857, 1701306, 1737690, 1686739","abstract":"Immersion is the ultimate goal of head-mounted displays (HMD) for Virtual Reality (VR) in order to produce a convincing user experience. Two important aspects in this context are motion sickness, often due to imprecise calibration, and the integration of a reliable eye tracking. We propose an affordable hard- and software solution for drift-free eye-tracking and user-friendly lens calibration within an HMD. The use of dichroic mirrors leads to a lean design that provides the full field-of-view (FOV) while using commodity cameras for eye tracking. Our prototype supports personalizable lens positioning to accommodate for different interocular distances. On the software side, a model-based calibration procedure adjusts the eye tracking system and gaze estimation to varying lens positions. Challenges such as partial occlusions due to the lens holders and eye lids are handled by a novel robust monocular pupil-tracking approach. We present four applications of our work: Gaze map estimation, foveated rendering for depth of field, gaze-contingent level-of-detail, and gaze control of virtual avatars.","cites":"3","conferencePercentile":"82.40740741"},{"venue":"ACM Multimedia","id":"369e71d3eeec20752b698c967ca7d2ea3af6d1bd","venue_1":"ACM Multimedia","year":"2016","title":"Visual Analytics for Multimedia: Challenges and Opportunities","authors":"Jarke J. van Wijk","author_ids":"1709060","abstract":"Understanding huge multimedia collections is a huge challenge. Given a set of hundreds of thousands or millions of images, how to to understand its contents and how to find the images that are relevant for the task at hand? Using a combination of automated methods, visualization, and interaction, known as visual analytics, is probably the only way to go, combining the strengths of man and machine. An overview is given of trends in data visualization and visual analytics is given, and examples of recent work in multimedia analytics are presented. Exploiting meta-data, using interaction with relatively simple visual representations, and alignment with the work flow of users are promising routes, but scalability and evaluation are still challenging.","cites":"0","conferencePercentile":"42.22222222"},{"venue":"ACM Multimedia","id":"6b7b7caa2ebabdabe2a7f9fafbedd09243a8df0f","venue_1":"ACM Multimedia","year":"2007","title":"ViewCast: view dissemination and management for multi-party 3d tele-immersive environments","authors":"Zhenyu Yang, Wanmin Wu, Klara Nahrstedt, Gregorij Kurillo, Ruzena Bajcsy","author_ids":"1771274, 2524314, 1688353, 2862376, 1784213","abstract":"Real-time distributed multi-party/multi-stream systems are becoming more popular in many areas such as 3D tele-immersion, multi-camera conferencing and security surveillance. However, the construction of such systems in large scale is impeded by the huge demand of computing and networking resources and the lack of a simple yet powerful networking model to handle interconnection, scalability and quality of service (QoS) guarantees. We make two main contributions in the paper: (1) we propose a novel generalized ViewCast model for multi-party/multi-stream video-mediated systems that fills the gap between high-level user interest and low level per-stream management, and (2) we demonstrate the ViewCast model by applying it to the multi-party 3D Tele-Immersive (3DTI) collaboration among geographically dispersed users. More specifically, we show how the ViewCast model is used in supporting stream data dissemination, coordination and QoS management among multiple 3D tele-immersive environments. We present our experimental results in both real implementation and simulation to show that our ViewCast-based solution achieves high efficiency, scalability, and quality in supporting multi-party 3DTI collaboration.","cites":"31","conferencePercentile":"88.54166667"}]}