{"ACM_Trans._Graph..csv":[{"venue":"ACM Trans. Graph.","id":"0475debf6815983e1d4901aaf563749b552467ae","venue_1":"ACM Trans. Graph.","year":"2009","title":"Interpolatory point set surfaces - convexity and Hermite data","authors":"Marc Alexa, Anders Adamson","author_ids":"1751554, 2032408","abstract":"Point set surfaces define a (typically) manifold surface from a set of scattered points. The definition involves weighted centroids and a gradient field. The data points are interpolated if singular weight functions are used to define the centroids. While this way of deriving an interpolatory scheme appears natural, we show that it has two deficiencies: Convexity of the input is not preserved and the extension to Hermite data is numerically unstable. We present a generalization of the standard scheme that we call <i>Hermite point set surface</i>. It allows interpolating, given normal constraints in a stable way. It also yields an intuitive parameter for shape control and preserves convexity in most situations. The analysis of derivatives also leads to a more natural way to define normals, in case they are not supplied with the point data. We conclude by comparing to similar surface definitions.","cites":"22","conferencePercentile":"31.7679558"},{"venue":"ACM Trans. Graph.","id":"51cba12767f5a89533117e3651f4d32cd2782d4a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Parameterization-free projection for geometry reconstruction","authors":"Yaron Lipman, Daniel Cohen-Or, David Levin, Hillel Tal-Ezer","author_ids":"3232072, 1701009, 1689997, 2261165","abstract":"We introduce a Locally Optimal Projection operator (LOP) for surface approximation from point-set data. The operator is parameterization free, in the sense that it does not rely on estimating a local normal, fitting a local plane, or using any other local parametric representation. Therefore, it can deal with noisy data which clutters the orientation of the points. The method performs well in cases of ambiguous orientation, e.g., if two folds of a surface lie near each other, and other cases of complex geometry in which methods based upon local plane fitting may fail. Although defined by a global minimization problem, the method is effectively local, and it provides a second order approximation to smooth surfaces. Hence allowing good surface approximation without using any explicit or implicit approximation space. Furthermore, we show that LOP is highly robust to noise and outliers and demonstrate its effectiveness by applying it to raw scanned data of complex shapes.","cites":"55","conferencePercentile":"56.4"},{"venue":"ACM Trans. Graph.","id":"248a4f9862e5455010c1a7c653bf2691b8eb53f7","venue_1":"ACM Trans. Graph.","year":"2004","title":"Mesh editing with poisson-based gradient field manipulation","authors":"Yizhou Yu, Kun Zhou, Dong Xu, Xiaohan Shi, Hujun Bao, Baining Guo, Harry Shum","author_ids":"7877246, 6671887, 1714390, 3132594, 1679542, 2738456, 1698102","abstract":"In this paper, we introduce a novel approach to mesh editing with the Poisson equation as the theoretical foundation. The most distinctive feature of this approach is that it modifies the original mesh geometry implicitly through gradient field manipulation. Our approach can produce desirable and pleasing results for both global and local editing operations, such as deformation, object merging, and smoothing. With the help from a few novel interactive tools, these operations can be performed conveniently with a small amount of user interaction. Our technique has three key components, a basic mesh solver based on the Poisson equation, a gradient field manipulation scheme using local transforms, and a generalized boundary condition representation based on local frames. Experimental results indicate that our framework can outperform previous related mesh editing techniques.","cites":"297","conferencePercentile":"86.95652174"},{"venue":"ACM Trans. Graph.","id":"5d0d4e2c3d04731211b43dd658d4e7eed901a7f4","venue_1":"ACM Trans. Graph.","year":"2007","title":"Handle-aware isolines for scalable shape editing","authors":"Oscar Kin-Chung Au, Hongbo Fu, Chiew-Lan Tai, Daniel Cohen-Or","author_ids":"1752997, 1691065, 1702674, 1701009","abstract":"Handle-based mesh deformation is essentially a nonlinear problem. To allow scalability, the original deformation problem can be approximately represented by a compact set of control variables. We show the direct relation between the locations of handles on the mesh and the local rigidity under deformation, and introduce the notion of <i>handle-aware rigidity</i>. Then, we present a reduced model whose control variables are intelligently distributed across the surface, respecting the rigidity information and the geometry. Specifically, for each handle, the control variables are the transformations of the isolines of a harmonic scalar field representing the deformation propagation from that handle. The isolines constitute a virtual skeletal structure similar to the bones in skinning deformation, thus correctly capturing the low-frequency shape deformation. To interpolate the transformations from the isolines to the original mesh, we design a method which is local, linear and geometry-dependent. This novel interpolation scheme and the transformation-based reduced domain allow each iteration of the nonlinear solver to be fully computed over the reduced domain. This makes the per-iteration cost dependent on only the number of isolines and enables compelling deformation of highly detailed shapes at interactive rates. In addition, we show how the handle-driven isolines provide an efficient means for deformation transfer without full shape correspondence.","cites":"27","conferencePercentile":"22.4"},{"venue":"ACM Trans. Graph.","id":"00e6ff3f9672761dfffcf2e107bd8f7fc614e8c5","venue_1":"ACM Trans. Graph.","year":"2012","title":"Recursive interlocking puzzles","authors":"Peng Song, Chi-Wing Fu, Daniel Cohen-Or","author_ids":"5268700, 1699457, 1701009","abstract":"Interlocking puzzles are very challenging geometric problems with the fascinating property that once we solve one by putting together the puzzle pieces, the puzzle pieces interlock with one another, preventing the assembly from falling apart. Though interlocking puzzles have been known for hundreds of years, very little is known about the governing mechanics. Thus, designing new interlocking geometries is basically accomplished with extensive manual effort or expensive exhaustive search with computers.\n In this paper, we revisit the notion of interlocking in greater depth, and devise a formal method of the interlocking mechanics. From this, we can develop a constructive approach for devising new interlocking geometries that directly guarantees the validity of the interlocking instead of exhaustively testing it. In particular, we focus on an interesting subclass of interlocking puzzles that are recursive in the sense that the assembly of puzzle pieces can remain an interlocking puzzle also after sequential removal of pieces; there is only one specific sequence of assembling, or disassembling, such a puzzle. Our proposed method can allow efficient generation of recursive interlocking geometries of various complexities, and by further realizing it with LEGO bricks, we can enable the hand-built creation of custom puzzle games.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"33ab5884d8427cbb024d917703f38ba8f9d39c8b","venue_1":"ACM Trans. Graph.","year":"2006","title":"Salient geometric features for partial shape matching and similarity","authors":"Ran Gal, Daniel Cohen-Or","author_ids":"2835543, 1701009","abstract":"This article introduces a method for partial matching of surfaces represented by triangular meshes. Our method matches surface regions that are numerically and topologically dissimilar, but approximately similar regions. We introduce novel local surface descriptors which efficiently represent the geometry of local regions of the surface. The descriptors are defined independently of the underlying triangulation, and form a compatible representation that allows matching of surfaces with different triangulations. To cope with the combinatorial complexity of partial matching of large meshes, we introduce the abstraction of salient geometric features and present a method to construct them. A salient geometric feature is a compound high-level feature of nontrivial local shapes. We show that a relatively small number of such salient geometric features characterizes the surface well for various similarity applications. Matching salient geometric features is based on indexing rotation-invariant features and a voting scheme accelerated by geometric hashing. We demonstrate the effectiveness of our method with a number of applications, such as computing self-similarity, alignments, and subparts similarity.","cites":"202","conferencePercentile":"94.44444444"},{"venue":"ACM Trans. Graph.","id":"551fe057a4d1395b87d56d5b56e7f98807fe535f","venue_1":"ACM Trans. Graph.","year":"2011","title":"Converting 3D furniture models to fabricatable parts and connectors","authors":"Manfred Lau, Akira Ohgawara, Jun Mitani, Takeo Igarashi","author_ids":"2007260, 2118725, 2618827, 1717356","abstract":"Although there is an abundance of 3D models available, most of them exist only in virtual simulation and are not immediately usable as physical objects in the real world. We solve the problem of taking as input a 3D model of a man-made object, and automatically generating the parts and connectors needed to build the corresponding physical object. We focus on furniture models, and we define formal grammars for IKEA cabinets and tables. We perform lexical analysis to identify the primitive parts of the 3D model. Structural analysis then gives structural information to these parts, and generates the connectors (i.e. nails, screws) needed to attach the parts together. We demonstrate our approach with arbitrary 3D models of cabinets and tables available online.","cites":"44","conferencePercentile":"81.05263158"},{"venue":"ACM Trans. Graph.","id":"403d58446986a0f0496d7122d9b842a47b17e8f2","venue_1":"ACM Trans. Graph.","year":"2007","title":"Volume and shape preservation via moving frame manipulation","authors":"Yaron Lipman, Daniel Cohen-Or, Ran Gal, David Levin","author_ids":"3232072, 1701009, 2835543, 1689997","abstract":"This article introduces a method for mesh editing that is aimed at preserving shape and volume. We present two new developments: The first is a minimization of a functional expressing a geometric distance measure between two isometric surfaces. The second is a local volume analysis linking the volume of an object to its surface curvature. Our method is based upon the moving frames representation of meshes. Applying a rotation field to the moving frames defines an isometry. Given rotational constraints, the mesh is deformed by an optimal isometry defined by minimizing the distance measure between original and deformed meshes. The resulting isometry nicely preserves the surface details, but when large rotations are applied, the volumetric behavior of the model may be unsatisfactory. Using the local volume analysis, we define a scalar field by which we scale the moving frames. Scaled and rotated moving frames restore volumetric properties of the original mesh, while properly maintaining the surface details. Our results show that even extreme deformations can be applied to meshes, with only minimal distortion of surface details and object volume.","cites":"38","conferencePercentile":"36.8"},{"venue":"ACM Trans. Graph.","id":"14ce768b1296680f4443d9c201027f6b3576f468","venue_1":"ACM Trans. Graph.","year":"2011","title":"Interactive hybrid simulation of large-scale traffic","authors":"Jason Sewall, David Wilkie, Ming C. Lin","author_ids":"2308989, 2293889, 1709625","abstract":"We present a novel, real-time algorithm for modeling large-scale, realistic traffic using a hybrid model of both continuum and agent-based methods for traffic simulation. We simulate individual vehicles in regions of interest using state-of-the-art agent-based models of driver behavior, and use a faster continuum model of traffic flow in the remainder of the road network. Our key contributions are efficient techniques for the dynamic coupling of discrete vehicle simulation with the aggregated behavior of continuum techniques for traffic simulation. We demonstrate the flexibility and scalability of our interactive visual simulation technique on extensive road networks using both real-world traffic data and synthetic scenarios. These techniques demonstrate the applicability of hybrid techniques to the efficient simulation of large-scale flows with complex dynamics.","cites":"23","conferencePercentile":"50.52631579"},{"venue":"ACM Trans. Graph.","id":"0b4e1498185ddbee1715b61037ebb58450c48687","venue_1":"ACM Trans. Graph.","year":"2009","title":"Modeling spatial and temporal variation in motion data","authors":"Manfred Lau, Ziv Bar-Joseph, James J. Kuffner","author_ids":"2007260, 1733101, 1743428","abstract":"We present a novel method to model and synthesize variation in motion data. Given a few examples of a particular type of motion as input, we learn a generative model that is able to synthesize a family of spatial and temporal variants that are statistically similar to the input examples. The new variants retain the features of the original examples, but are <i>not exact copies</i> of them. We learn a Dynamic Bayesian Network model from the input examples that enables us to capture properties of conditional independence in the data, and model it using a multivariate probability distribution. We present results for a variety of human motion, and 2D handwritten characters. We perform a user study to show that our new variants are less repetitive than typical game and crowd simulation approaches of re-playing a small number of existing motion clips. Our technique can synthesize new variants efficiently and has a small memory requirement.","cites":"27","conferencePercentile":"41.16022099"},{"venue":"ACM Trans. Graph.","id":"4cc49698c511d7a0aaa57ea428bc7f5925bba81f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Efficient preconditioning of laplacian matrices for computer graphics","authors":"Dilip Krishnan, Raanan Fattal, Richard Szeliski","author_ids":"1979918, 3230440, 1717841","abstract":"We present a new multi-level preconditioning scheme for discrete Poisson equations that arise in various computer graphics applications such as colorization, edge-preserving decomposition for two-dimensional images, and geodesic distances and diffusion on three-dimensional meshes. Our approach interleaves the selection of fine-and coarse-level variables with the removal of weak connections between potential fine-level variables (<i>sparsification</i>) and the <i>compensation</i> for these changes by strengthening nearby connections. By applying these operations before each elimination step and repeating the procedure recursively on the resulting smaller systems, we obtain a highly efficient multi-level preconditioning scheme with linear time and memory requirements. Our experiments demonstrate that our new scheme outperforms or is comparable with other state-of-the-art methods, both in terms of operation count and wall-clock time. This speedup is achieved by the new method's ability to reduce the condition number of irregular Laplacian matrices as well as homogeneous systems. It can therefore be used for a wide variety of computational photography problems, as well as several 3D mesh processing tasks, without the need to carefully match the algorithm to the problem characteristics.","cites":"28","conferencePercentile":"83.25791855"},{"venue":"ACM Trans. Graph.","id":"2f1af7459d4c21aab69f54ac54af60ea92af17b9","venue_1":"ACM Trans. Graph.","year":"2009","title":"Face poser: Interactive modeling of 3D facial expressions using facial priors","authors":"Manfred Lau, Jinxiang Chai, Ying-Qing Xu, Harry Shum","author_ids":"2007260, 1759700, 1742571, 1698102","abstract":"This article presents an intuitive and easy-to-use system for interactively posing 3D facial expressions. The user can model and edit facial expressions by drawing freeform strokes, by specifying distances between facial points, by incrementally editing curves on the face, or by directly dragging facial points in 2D screen space. Designing such an interface for 3D facial modeling and editing is challenging because many unnatural facial expressions might be consistent with the user's input. We formulate the problem in a maximum a posteriori framework by combining the user's input with priors embedded in a large set of facial expression data. Maximizing the posteriori allows us to generate an optimal and natural facial expression that achieves the goal specified by the user. We evaluate the performance of our system by conducting a thorough comparison of our method with alternative facial modeling techniques. To demonstrate the usability of our system, we also perform a user study of our system and compare with state-of-the-art facial expression modeling software (Poser 7).","cites":"19","conferencePercentile":"25.69060773"},{"venue":"ACM Trans. Graph.","id":"6d2f9cbd7e7495c672d50acf5bf042687dac85c5","venue_1":"ACM Trans. Graph.","year":"2010","title":"Spectral sampling of manifolds","authors":"A. Cengiz Öztireli, Marc Alexa, Markus H. Gross","author_ids":"1787433, 1751554, 1743207","abstract":"A central problem in computer graphics is finding optimal sampling conditions for a given surface representation. We propose a new method to solve this problem based on spectral analysis of manifolds which results in faithful reconstructions and high quality isotropic samplings, is efficient, out-of-core, feature sensitive, intuitive to control and simple to implement. We approach the problem in a novel way by utilizing results from spectral analysis, kernel methods, and matrix perturbation theory. Change in a manifold due to a single point is quantified by a local measure that limits the change in the Laplace-Beltrami spectrum of the manifold. Hence, we do not need to explicitly compute the spectrum or any global quantity, which makes our algorithms very efficient. Although our main focus is on sampling surfaces, the analysis and algorithms are general and can be applied for simplifying and resampling point clouds lying near a manifold of arbitrary dimension.","cites":"17","conferencePercentile":"26.02339181"},{"venue":"ACM Trans. Graph.","id":"0ba44b04178fb7c3081afb93df9b92bea8c877ae","venue_1":"ACM Trans. Graph.","year":"2010","title":"Data-driven image color theme enhancement","authors":"Baoyuan Wang, Yizhou Yu, Tien-Tsin Wong, Chun Chen, Ying-Qing Xu","author_ids":"2450889, 7877246, 1720633, 5371645, 1742571","abstract":"It is often important for designers and photographers to convey or enhance desired color themes in their work. A color theme is typically defined as a template of colors and an associated verbal description. This paper presents a data-driven method for enhancing a desired color theme in an image. We formulate our goal as a unified optimization that simultaneously considers a desired color theme, texture-color relationships as well as automatic or user-specified color constraints. Quantifying the difference between an image and a color theme is made possible by color mood spaces and a generalization of an additivity relationship for two-color combinations. We incorporate prior knowledge, such as texture-color relationships, extracted from a database of photographs to maintain a natural look of the edited images. Experiments and a user study have confirmed the effectiveness of our method.","cites":"42","conferencePercentile":"73.68421053"},{"venue":"ACM Trans. Graph.","id":"bdc7e0b6f2ca240f16418ecca32bb5e5ad07400a","venue_1":"ACM Trans. Graph.","year":"2012","title":"All-hex meshing using singularity-restricted field","authors":"Yufei Li, Yang Liu, Weiwei Xu, Wenping Wang, Baining Guo","author_ids":"2832591, 1742731, 6953977, 1698520, 2738456","abstract":"Decomposing a volume into high-quality hexahedral cells is a challenging task in geometric modeling and computational geometry. Inspired by the use of cross field in quad meshing and the CubeCover approach in hex meshing, we present a complete all-hex meshing framework based on <i>singularity-restricted field</i> that is essential to induce a valid all-hex structure. Given a volume represented by a tetrahedral mesh, we first compute a boundary-aligned 3D frame field inside it, then convert the frame field to be singularity-restricted by our effective topological operations. In our all-hex meshing framework, we apply the CubeCover method to achieve the volume parametrization. For reducing degenerate elements appearing in the volume parametrization, we also propose novel tetrahedral split operations to preprocess singularity-restricted frame fields. Experimental results show that our algorithm generates high-quality all-hex meshes from a variety of 3D volumes robustly and efficiently.","cites":"36","conferencePercentile":"80.55555556"},{"venue":"ACM Trans. Graph.","id":"bd7ca62000d1b36e0d7f7edd2e0f9cd629327086","venue_1":"ACM Trans. Graph.","year":"2016","title":"Tactile mesh saliency","authors":"Manfred Lau, Kapil Dev, Weiqi Shi, Julie Dorsey, Holly E. Rushmeier","author_ids":"2007260, 2590311, 1888244, 1775220, 1690595","abstract":"While the concept of visual saliency has been previously explored in the areas of mesh and image processing, saliency detection also applies to other sensory stimuli. In this paper, we explore the problem of tactile mesh saliency, where we define salient points on a virtual mesh as those that a human is more likely to grasp, press, or touch if the mesh were a real-world object. We solve the problem of taking as input a 3D mesh and computing the relative tactile saliency of every mesh vertex. Since it is difficult to manually define a tactile saliency measure, we introduce a crowdsourcing and learning framework. It is typically easy for humans to provide relative rankings of saliency between vertices rather than absolute values. We thereby collect crowdsourced data of such relative rankings and take a learning-to-rank approach. We develop a new formulation to combine deep learning and learning-to-rank methods to compute a tactile saliency measure. We demonstrate our framework with a variety of 3D meshes and various applications including material suggestion for rendering and fabrication.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"1cc16369a79159ea1d54eff3f300025099ce0603","venue_1":"ACM Trans. Graph.","year":"2013","title":"Computing self-supporting surfaces by regular triangulation","authors":"Yang Liu, Hao Pan, John Snyder, Wenping Wang, Baining Guo","author_ids":"1742731, 1757387, 6314473, 1698520, 2738456","abstract":"Masonry structures must be compressively self-supporting; designing such surfaces forms an important topic in architecture as well as a challenging problem in geometric modeling. Under certain conditions, a surjective mapping exists between a <i>power diagram</i>, defined by a set of 2D vertices and associated weights, and the reciprocal diagram that characterizes the force diagram of a discrete self-supporting network. This observation lets us define a new and convenient parameterization for the space of self-supporting networks. Based on it and the discrete geometry of this design space, we present novel geometry processing methods including surface smoothing and remeshing which significantly reduce the magnitude of force densities and homogenize their distribution.","cites":"24","conferencePercentile":"78.73303167"},{"venue":"ACM Trans. Graph.","id":"5acf35c97f9ce094ead01c780b6ed56cf6e72309","venue_1":"ACM Trans. Graph.","year":"2012","title":"Motion-guided mechanical toy modeling","authors":"Lifeng Zhu, Weiwei Xu, John Snyder, Yang Liu, Guoping Wang, Baining Guo","author_ids":"8301547, 6953977, 6314473, 1742731, 2896119, 2738456","abstract":"We introduce a new method to synthesize mechanical toys solely from the motion of their features. The designer specifies the geometry and a time-varying rotation and translation of each rigid feature component. Our algorithm automatically generates a mechanism assembly located in a box below the feature base that produces the specified motion. Parts in the assembly are selected from a parameterized set including belt-pulleys, gears, crank-sliders, quick-returns, and various cams (snail, ellipse, and double-ellipse). Positions and parameters for these parts are optimized to generate the specified motion, minimize a simple measure of complexity, and yield a well-distributed layout of parts over the driving axes. Our solution uses a special initialization procedure followed by simulated annealing to efficiently search the complex configuration space for an optimal assembly.","cites":"41","conferencePercentile":"86.11111111"},{"venue":"ACM Trans. Graph.","id":"bc07e730d8d6e7b6f9e5b1d5e547772cb069634b","venue_1":"ACM Trans. Graph.","year":"2012","title":"Sketch-based shape retrieval","authors":"Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand, Marc Alexa","author_ids":"2165937, 2581563, 1747280, 2881445, 1751554","abstract":"We develop a system for 3D object retrieval based on sketched feature lines as input. For objective evaluation, we collect a large number of query sketches from human users that are related to an existing data base of objects. The sketches turn out to be generally quite abstract with large local and global deviations from the original shape. Based on this observation, we decide to use a bag-of-features approach over computer generated line drawings of the objects. We develop a targeted feature transform based on Gabor filters for this system. We can show objectively that this transform is better suited than other approaches from the literature developed for similar tasks. Moreover, we demonstrate how to optimize the parameters of our, as well as other approaches, based on the gathered sketches. In the resulting comparison, our approach is significantly better than any other system described so far.","cites":"62","conferencePercentile":"96.46464646"},{"venue":"ACM Trans. Graph.","id":"980229631e6c0b7444dfe1e48f28f64a54de819c","venue_1":"ACM Trans. Graph.","year":"2015","title":"Computational interlocking furniture assembly","authors":"Chi-Wing Fu, Peng Song, Xiaoqi Yan, Lee Wei Yang, Pradeep Kumar Jayaraman, Daniel Cohen-Or","author_ids":"1699457, 5268700, 2426331, 2667684, 2514027, 1701009","abstract":"Furniture typically consists of assemblies of elongated and planar parts that are connected together by glue, nails, hinges, screws, or other means that do not encourage disassembly and re-assembly. An alternative approach is to use an interlocking mechanism, where the component parts tightly interlock with one another. The challenge in designing such a network of interlocking joints is that local analysis is insufficient to guarantee global interlocking, and there is a huge number of joint combinations that require an enormous exploration effort to ensure global interlocking. In this paper, we present a computational solution to support the design of a network of interlocking joints that form a globally-interlocking furniture assembly. The key idea is to break the furniture complex into an overlapping set of small groups, where the parts in each group are immobilized by a local key, and adjacent groups are further locked with dependencies. The dependency among the groups saves the effort of exploring the immobilization of every subset of parts in the assembly, thus allowing the intensive interlocking computation to be localized within each small group. We demonstrate the effectiveness of our technique on many globally-interlocking furniture assemblies of various shapes and complexity.","cites":"8","conferencePercentile":"85.30612245"},{"venue":"ACM Trans. Graph.","id":"32b318d5de51d0f614df1faf72fda218ca28fec4","venue_1":"ACM Trans. Graph.","year":"2016","title":"CofiFab: coarse-to-fine fabrication of large 3D objects","authors":"Peng Song, Bailin Deng, Ziqi Wang, Zhichao Dong, Wei Li, Chi-Wing Fu, Ligang Liu","author_ids":"5268700, 2964129, 2486512, 3430968, 1688012, 1699457, 1724542","abstract":"This paper presents CofiFab, a coarse-to-fine 3D fabrication solution, combining 3D printing and 2D laser cutting for cost-effective fabrication of large objects at lower cost and higher speed. Our key approach is to first build coarse internal base structures within the given 3D object using laser cutting, and then attach thin 3D-printed parts, as an external shell, onto the base to recover the fine surface details. CofiFab achieves this with three novel algorithmic components. First, we formulate an optimization model to compute fabricatable polyhedrons of maximized volume, as the geometry of the internal base. Second, we devise a new interlocking scheme to tightly connect the laser-cut parts into a strong internal base, by iteratively building a network of nonorthogonal joints and interlocking parts around polyhedral corners. Lastly, we optimize the partitioning of the external object shell into 3D-printable parts, while saving support material and avoiding overhangs. Besides cost saving, these components also consider aesthetics, stability and balancing. Hence, CofiFab can efficiently produce large objects by assembly. To evaluate CofiFab, we fabricate objects of varying shapes and sizes, and show that CofiFab can significantly outperform previous methods.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"50612493253f5cea7ecf406e9a8d75382b96a83e","venue_1":"ACM Trans. Graph.","year":"2012","title":"How do humans sketch objects?","authors":"Mathias Eitz, James Hays, Marc Alexa","author_ids":"2165937, 2151506, 1751554","abstract":"Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56% accuracy (chance is 0.4%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.","cites":"109","conferencePercentile":"99.49494949"},{"venue":"ACM Trans. Graph.","id":"2f5fb214c1686e87a5c2acdc799f4bfab75d6d33","venue_1":"ACM Trans. Graph.","year":"2007","title":"Solid texture synthesis from 2D exemplars","authors":"Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, Tien-Tsin Wong","author_ids":"2891193, 1699457, 1701009, 1850438, 1684384, 1720633","abstract":"We present a novel method for synthesizing solid textures from 2D texture exemplars. First, we extend 2D texture optimization techniques to synthesize 3D texture solids. Next, the non-parametric texture optimization approach is integrated with histogram matching, which forces the global statistics of the synthesized solid to match those of the exemplar. This improves the convergence of the synthesis process and enables using smaller neighborhoods. In addition to producing compelling texture mapped surfaces, our method also effectively models the material in the interior of solid objects. We also demonstrate that our method is well-suited for synthesizing textures with a large number of channels per texel.","cites":"110","conferencePercentile":"82.8"},{"venue":"ACM Trans. Graph.","id":"6976323e5c8cd904ab8bbf7a74e4754aefc05e95","venue_1":"ACM Trans. Graph.","year":"2016","title":"DisCo: Display-Camera Communication Using Rolling Shutter Sensors","authors":"Kensei Jo, Mohit Gupta, Shree K. Nayar","author_ids":"2682173, 2122821, 1750470","abstract":"We present DisCo, a novel display-camera communication system. DisCo enables displays and cameras to communicate with each other while also displaying and capturing images for human consumption. Messages are transmitted by temporally modulating the display brightness at high frequencies so that they are imperceptible to humans. Messages are received by a rolling shutter camera that converts the temporally modulated incident light into a spatial flicker pattern. In the captured image, the flicker pattern is superimposed on the pattern shown on the display. The flicker and the display pattern are separated by capturing two images with different exposures. The proposed system performs robustly in challenging real-world situations such as occlusion, variable display size, defocus blur, perspective distortion, and camera rotation. Unlike several existing visible light communication methods, DisCo works with off-the-shelf image sensors. It is compatible with a variety of sources (including displays, single LEDs), as well as reflective surfaces illuminated with light sources. We have built hardware prototypes that demonstrate DisCo&#8217;s performance in several scenarios. Because of its robustness, speed, ease of use, and generality, DisCo can be widely deployed in several applications, such as advertising, pairing of displays with cell phones, tagging objects in stores and museums, and indoor navigation.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"4de6929036d029598f13e059ead8c5b1e0a1b985","venue_1":"ACM Trans. Graph.","year":"2013","title":"Image-based rendering in the gradient domain","authors":"Johannes Kopf, Fabian Langguth, Daniel Scharstein, Richard Szeliski, Michael Goesele","author_ids":"2891193, 1877166, 1709053, 1717841, 1689293","abstract":"We propose a novel image-based rendering algorithm for handling complex scenes that may include reflective surfaces. Our key contribution lies in treating the problem in the gradient domain. We use a standard technique to estimate scene depth, but assign depths to image gradients rather than pixels. A novel view is obtained by rendering the horizontal and vertical gradients, from which the final result is reconstructed through Poisson integration using an approximate solution as a data term. Our algorithm is able to handle general scenes including reflections and similar effects <i>without</i> explicitly separating the scene into reflective and transmissive parts, as required by previous work. Our prototype renderer is fully implemented on the GPU and runs in real time on commodity hardware.","cites":"7","conferencePercentile":"21.26696833"},{"venue":"ACM Trans. Graph.","id":"021f0e3f470e642237845e7729af3707d699cf28","venue_1":"ACM Trans. Graph.","year":"2007","title":"Interactive topology-aware surface reconstruction","authors":"Andrei Sharf, Thomas Lewiner, Gil Shklarski, Sivan Toledo, Daniel Cohen-Or","author_ids":"2120270, 3212265, 3026890, 1731568, 1701009","abstract":"The reconstruction of a complete watertight model from scan data is still a difficult process. In particular, since scanned data is often incomplete, the reconstruction of the expected shape is an ill-posed problem. Techniques that reconstruct poorly-sampled areas without any user intervention fail in many cases to faithfully reconstruct the topology of the model. The method that we introduce in this paper is topology-aware: it uses minimal user input to make correct decisions at regions where the topology of the model cannot be automatically induced with a reasonable degree of confidence. We first construct a continuous function over a three-dimensional domain. This function is constructed by minimizing a penalty function combining the data points, user constraints, and a regularization term. The optimization problem is formulated in a mesh-independent manner, and mapped onto a specific mesh using the finite-element method. The zero level-set of this function is a first approximation of the reconstructed surface. At complex under-sampled regions, the constraints might be insufficient. Hence, we analyze the local topological stability of the zero level-set to detect weak regions of the surface. These regions are suggested to the user for adding local inside/outside constraints by merely scribbling over a 2D tablet. Each new user constraint modifies the minimization problem, which is solved incrementally. The process is repeated, converging to a topology-stable reconstruction. Reconstructions of models acquired by a structured-light scanner with a small number of scribbles demonstrate the effectiveness of the method.","cites":"40","conferencePercentile":"39.2"},{"venue":"ACM Trans. Graph.","id":"5010134d701ba63dc6a27ffcc4fd83b4d78876ad","venue_1":"ACM Trans. Graph.","year":"2012","title":"Sparse PDF maps for non-linear multi-resolution image operations","authors":"Markus Hadwiger, Ronell Sicat, Johanna Beyer, Jens H. Krüger, Torsten Möller","author_ids":"1761518, 2036789, 2505591, 1729186, 1743737","abstract":"We introduce a new type of multi-resolution image pyramid for high-resolution images called <i>sparse pdf maps</i> (sPDF-maps). Each pyramid level consists of a sparse encoding of continuous probability density functions (pdfs) of pixel neighborhoods in the original image. The encoded pdfs enable the accurate computation of non-linear image operations directly in any pyramid level with proper pre-filtering for anti-aliasing, without accessing higher or lower resolutions. The sparsity of sPDF-maps makes them feasible for gigapixel images, while enabling direct evaluation of a variety of non-linear operators from the same representation. We illustrate this versatility for antialiased color mapping, <i>O</i>(<i>n</i>) local Laplacian filters, smoothed local histogram filters (e.g., median or mode filters), and bilateral filters.","cites":"4","conferencePercentile":"3.535353535"},{"venue":"ACM Trans. Graph.","id":"2c08ceb16f46fff5e416f9943acc42f6fabe2ba8","venue_1":"ACM Trans. Graph.","year":"2006","title":"Recursive Wang tiles for real-time blue noise","authors":"Johannes Kopf, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski","author_ids":"2891193, 1701009, 1850438, 1684384","abstract":"Well distributed point sets play an important role in a variety of computer graphics contexts, such as anti-aliasing, global illumination, halftoning, non-photorealistic rendering, point-based modeling and rendering, and geometry processing. In this paper, we introduce a novel technique for rapidly generating large point sets possessing a blue noise Fourier spectrum and high visual quality. Our technique generates non-periodic point sets, distributed over arbitrarily large areas. The local density of a point set may be prescribed by an arbitrary target density function, without any preset bound on the maximum density. Our technique is deterministic and tile-based; thus, any local portion of a potentially infinite point set may be consistently regenerated as needed. The memory footprint of the technique is constant, and the cost to generate any local portion of the point set is proportional to the integral over the target density in that area. These properties make our technique highly suitable for a variety of real-time interactive applications, some of which are demonstrated in the paper.Our technique utilizes a set of carefully constructed <i>progressive</i> and <i>recursive</i> blue noise Wang tiles. The use of Wang tiles enables the generation of infinite non-periodic tilings. The progressive point sets inside each tile are able to produce spatially varying point densities. Recursion allows our technique to adaptively subdivide tiles only where high density is required, and makes it possible to zoom into point sets by an arbitrary amount, while maintaining a constant apparent density.","cites":"92","conferencePercentile":"69.90740741"},{"venue":"ACM Trans. Graph.","id":"3d19c6e06806a3ea71286702233a578eccaf04a2","venue_1":"ACM Trans. Graph.","year":"2009","title":"A tool to create illuminant and reflectance spectra for light-driven graphics and visualization","authors":"Steven Bergner, Mark S. Drew, Torsten Möller","author_ids":"1767733, 1680726, 1743737","abstract":"Full spectra allow the generation of a physically correct rendering of a scene under different lighting conditions. In this article we devise a tool to augment a palette of given lights and material reflectances with constructed spectra, yielding specified colors or spectral properties such as metamerism or objective color constancy. We utilize this to emphasize or hide parts of a scene by matching or differentiating colors under different illuminations. These color criteria are expressed as a quadratic programming problem, which may be solved with positivity constraints. Further, we characterize full spectra of lights, surfaces, and transmissive materials in an efficient linear subspace model by forming eigenvectors of sets of spectra and transform them to an intermediate space in which spectral interactions reduce to simple component-wise multiplications during rendering. The proposed method enhances the user's freedom in designing photo-realistic scenes and helps in creating expressive visualizations. A key application of our technique is to use specific spectral lighting to scale the visual complexity of a scene by controlling visibility of texture details in surface graphics or material details in volume rendering.","cites":"5","conferencePercentile":"7.458563536"},{"venue":"ACM Trans. Graph.","id":"79cff18e28c479ff27dd42695942bd0a2f32a672","venue_1":"ACM Trans. Graph.","year":"2011","title":"Example-based image color and tone style enhancement","authors":"Baoyuan Wang, Yizhou Yu, Ying-Qing Xu","author_ids":"2450889, 7877246, 1742571","abstract":"Color and tone adjustments are among the most frequent image enhancement operations. We define a color and tone style as a set of explicit or implicit rules governing color and tone adjustments. Our goal in this paper is to learn implicit color and tone adjustment rules from examples. That is, given a set of examples, each of which is a pair of corresponding images before and after adjustments, we would like to discover the underlying mathematical relationships optimally connecting the color and tone of corresponding pixels in all image pairs. We formally define tone and color adjustment rules as mappings, and propose to approximate complicated spatially varying nonlinear mappings in a piecewise manner. The reason behind this is that a very complicated mapping can still be locally approximated with a low-order polynomial model. Parameters within such low-order models are trained using data extracted from example image pairs. We successfully apply our framework in two scenarios, low-quality photo enhancement by transferring the style of a high-end camera, and photo enhancement using styles learned from photographers and designers.","cites":"24","conferencePercentile":"53.42105263"},{"venue":"ACM Trans. Graph.","id":"14516f32a828ea9429cfe84ad9c7338a702c3e7c","venue_1":"ACM Trans. Graph.","year":"2013","title":"Reciprocal frame structures made easy","authors":"Peng Song, Chi-Wing Fu, Prashant Goswami, Jianmin Zheng, Niloy J. Mitra, Daniel Cohen-Or","author_ids":"5268700, 1699457, 1685364, 1719885, 1710455, 1701009","abstract":"A reciprocal frame (RF) is a self-supported three-dimensional structure made up of three or more sloping rods, which form a closed circuit, namely an RF-unit. Large RF-structures built as complex grillages of one or a few similar RF-units have an intrinsic beauty derived from their inherent self-similar and highly symmetric patterns. Designing RF-structures that span over large domains is an intricate and complex task. In this paper, we present an interactive computational tool for designing RF-structures over a 3D guiding surface, focusing on the aesthetic aspect of the design.\n There are three key contributions in this work. First, we draw an analogy between RF-structures and plane tiling with regular polygons, and develop a computational scheme to generate coherent RF-tessellations from simple grammar rules. Second, we employ a conformal mapping to lift the 2D tessellation over a 3D guiding surface, allowing a real-time preview and efficient exploration of wide ranges of RF design parameters. Third, we devise an optimization method to guarantee the collinearity of contact joints along each rod, while preserving the geometric properties of the RF-structure. Our tool not only supports the design of wide variety of RF pattern classes and their variations, but also allows preview and refinement through interactive controls.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"23f66c3340eabcd4952b7169538b60b762bd70b9","venue_1":"ACM Trans. Graph.","year":"2011","title":"Discrete Laplacians on general polygonal meshes","authors":"Marc Alexa, Max Wardetzky","author_ids":"1751554, 1763180","abstract":"While the theory and applications of discrete Laplacians on <i>triangulated</i> surfaces are well developed, far less is known about the general <i>polygonal</i> case. We present here a principled approach for constructing geometric discrete Laplacians on surfaces with arbitrary polygonal faces, encompassing non-planar and non-convex polygons. Our construction is guided by closely mimicking structural properties of the smooth Laplace--Beltrami operator. Among other features, our construction leads to an extension of the widely employed cotan formula from triangles to polygons. Besides carefully laying out theoretical aspects, we demonstrate the versatility of our approach for a variety of geometry processing applications, embarking on situations that would have been more difficult to achieve based on geometric Laplacians for simplicial meshes or purely combinatorial Laplacians for general meshes.","cites":"20","conferencePercentile":"44.21052632"},{"venue":"ACM Trans. Graph.","id":"05d1c3ecfed582a9097cf527870f379946b1eb68","venue_1":"ACM Trans. Graph.","year":"2015","title":"LazyFluids: appearance transfer for fluid animations","authors":"Ondrej Jamriska, Jakub Fiser, Paul Asente, Jingwan Lu, Eli Shechtman, Daniel Sýkora","author_ids":"3193874, 2798088, 2934421, 2054975, 2177801, 7997286","abstract":"In this paper we present a novel approach to appearance transfer for fluid animations based on flow-guided texture synthesis. In contrast to common practice where pre-captured sets of fluid elements are combined in order to achieve desired motion and look, we bring the possibility of fine-tuning motion properties in advance using CG techniques, and then transferring the desired look from a selected appearance exemplar. We demonstrate that such a practical work-flow cannot be simply implemented using current state-of-the-art techniques, analyze what the main obstacles are, and propose a solution to resolve them. In addition, we extend the algorithm to allow for synthesis with rich boundary effects and video exemplars. Finally, we present numerous results that demonstrate the versatility of the proposed approach.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"7751aef501c1e37522052f254cd3bff4231a523a","venue_1":"ACM Trans. Graph.","year":"2015","title":"AutoConnect: computational design of 3D-printable connectors","authors":"Yuki Koyama, Shinjiro Sueda, Emma Steinhardt, Takeo Igarashi, Ariel Shamir, Wojciech Matusik","author_ids":"2196816, 7229044, 2996386, 1717356, 2947946, 1752521","abstract":"We present AutoConnect, an automatic method that creates customized, 3D-printable connectors attaching two physical objects together. Users simply position and orient virtual models of the two objects that they want to connect and indicate some auxiliary information such as weight and dimensions. Then, AutoConnect creates several alternative designs that users can choose from for 3D printing. The design of the connector is created by combining two holders, one for each object. We categorize the holders into two types. The first type holds standard objects such as pipes and planes. We utilize a database of parameterized mechanical holders and optimize the holder shape based on the grip strength and material consumption. The second type holds free-form objects. These are procedurally generated shell-gripper designs created based on geometric analysis of the object. We illustrate the use of our method by demonstrating many examples of connectors and practical use cases.","cites":"7","conferencePercentile":"81.2244898"},{"venue":"ACM Trans. Graph.","id":"909669c279b2327f804e790421a2bdbf477c2b2b","venue_1":"ACM Trans. Graph.","year":"2013","title":"Joint view expansion and filtering for automultiscopic 3D displays","authors":"Piotr Didyk, Pitchaya Sitthi-amorn, William T. Freeman, Frédo Durand, Wojciech Matusik","author_ids":"3307078, 1936229, 1768236, 1728125, 1752521","abstract":"Multi-view autostereoscopic displays provide an immersive, glasses-free 3D viewing experience, but they require correctly filtered content from multiple viewpoints. This, however, cannot be easily obtained with current stereoscopic production pipelines. We provide a practical solution that takes a stereoscopic video as an input and converts it to multi-view and filtered video streams that can be used to drive multi-view autostereoscopic displays. The method combines a phase-based video magnification and an interperspective antialiasing into a single filtering process. The whole algorithm is simple and can be efficiently implemented on current GPUs to yield a near real-time performance. Furthermore, the ability to retarget disparity is naturally supported. Our method is robust and works well for challenging video scenes with defocus blur, motion blur, transparent materials, and specularities. We show that our results are superior when compared to the state-of-the-art depth-based rendering methods. Finally, we showcase the method in the context of a real-time 3D videoconferencing system that requires only two cameras.","cites":"8","conferencePercentile":"25.56561086"},{"venue":"ACM Trans. Graph.","id":"25a427697567018e1acde58090ac10b77e3d3834","venue_1":"ACM Trans. Graph.","year":"1997","title":"An Optimal Algorithm for Expanding the Composition of Polynomials","authors":"Wayne Liu, Stephen Mann","author_ids":"1779598, 8011582","abstract":"A runtime analysis is made of a previously published algorithm for polynomial composition. The relationship between this composition algorithm and Sablonnie&grave;re's algorithm is explored. This composition algorithm is then made optimal aby first performing a change of basis.","cites":"12","conferencePercentile":"28.57142857"},{"venue":"ACM Trans. Graph.","id":"245d9988e91023f7464ec83cfd5b103254edb879","venue_1":"ACM Trans. Graph.","year":"2011","title":"Global parametrization of range image sets","authors":"Nico Pietroni, Marco Tarini, Olga Sorkine-Hornung, Denis Zorin","author_ids":"3283805, 3153457, 2250001, 1798055","abstract":"We present a method to globally parameterize a surface represented by height maps over a set of planes (range images). In contrast to other parametrization techniques, we do not start with a manifold mesh. The parametrization we compute defines a manifold structure, it is seamless and globally smooth, can be aligned to geometric features and shows good quality in terms of angle and area preservation, comparable to current parametrization techniques for meshes. Computing such global seamless parametrization makes it possible to perform quad remeshing, texture mapping and texture synthesis and many other types of geometry processing operations. Our approach is based on a formulation of the Poisson equation on a manifold structure defined for the surface by the range images. Construction of such global parametrization requires only a way to project surface data onto a set of planes, and can be applied directly to implicit surfaces, nonmanifold surfaces, very large meshes, and collections of range scans. We demonstrate application of our technique to all these geometry types.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"db426bd0a261fb84b5764c19ed727eb3d3e6cf00","venue_1":"ACM Trans. Graph.","year":"2007","title":"Volume illustration using wang cubes","authors":"Aidong Lu, David S. Ebert, Wei Qiao, Martin Kraus, Benjamin Mora","author_ids":"1750464, 1701512, 1957119, 7803724, 8327348","abstract":"To create a new, flexible system for volume illustration, we have explored the use of Wang Cubes, the 3D extension of 2D Wang Tiles. We use small sets of Wang Cubes to generate a large variety of nonperiodic illustrative 3D patterns and texture, which otherwise would be too large to use in real applications. We also develop a direct volume rendering framework with the generated patterns and textures. Our framework can be used to render volume datasets effectively and a variety of rendering styles can be achieved with less storage.\n Specifically, we extend the nonperiodic tiling process of Wang Tiles to Wang Cubes and modify it for multipurpose tiling. We automatically generate isotropic Wang Cubes consisting of 3D patterns or textures to simulate various illustrative effects. Anisotropic Wang Cubes are generated to yield patterns by using the volume data, curvature, and gradient information. We also extend the definition of Wang Cubes into a set of different sized cubes to provide multiresolution volume rendering. Finally, we provide both coherent 3D geometry-based and texture-based rendering frameworks that can be integrated with arbitrary feature exploration methods.","cites":"8","conferencePercentile":"2.4"},{"venue":"ACM Trans. Graph.","id":"405eb02d94697949b58f9ed4ae2f41b8fa4a7801","venue_1":"ACM Trans. Graph.","year":"2005","title":"Low-complexity maximum intensity projection","authors":"Benjamin Mora, David S. Ebert","author_ids":"8327348, 1701512","abstract":"Many techniques have already been proposed to improve the efficiency of maximum intensity projection (MIP) volume rendering, but none of them considered the possible hypothesis of a better complexity than either O(<i>n</i>) for finding the maximum value of <i>n</i> samples along a ray or O(<i>n</i><sup>3</sup>) for an object-order algorithm. Here, we fully model and analyze the use of octrees for MIP, and we mathematically show that the average MIP complexity can be reduced to O(<i>n</i><sup>2</sup>) for an object-order algorithm, or to O(log(<i>n</i>)) per ray when using the image-order variant of our algorithm. Therefore, this improvement establishes a major advance for interactive MIP visualization of large-volume data.In parallel, we also present an object-order implementation of our algorithm, satisfying the theoretical O(<i>n</i><sup>2</sup>) result. It is based on hierarchical occlusion maps that perform on-the-fly visibility of the data, and our results show that it is the most efficient solution for MIP available to date.","cites":"9","conferencePercentile":"3.629032258"},{"venue":"ACM Trans. Graph.","id":"69202c6877330dc0d00ab6f9d2fc8d301ee6eddd","venue_1":"ACM Trans. Graph.","year":"1993","title":"Functional Composition Algorithms via Blossoming","authors":"Tony DeRose, Ron Goldman, Hans Hagen, Stephen Mann","author_ids":"1792251, 1705063, 1749682, 8011582","abstract":"In view of the fundamental role that functional composition plays in mathematics, it is not surprising that a variety of problems in geometric modeling can be viewed as instances of the following composition problem: given representations for two functions <italic>F</italic> and <italic>G</italic>, compute a representation of the function <italic>H</italic> = <italic>F o G</italic>. We examine this problem in detail for the case when <italic>F</italic> and <italic>G</italic> are given in either Be&#180;zier or B-spline form. Blossoming techniques are used to gain theoretical insight into the structure of the solution which is then used to develop efficient, tightly codable algorithms. From a practical point of view, if the composition algorithms are implemented as library routines, a number of geometric-modeling problems can be solved with a small amount of additional software.","cites":"38","conferencePercentile":"57.14285714"},{"venue":"ACM Trans. Graph.","id":"396709d6c1f655895f0f7b1c2efd2082d7a70e9c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Superimposing dynamic range","authors":"Oliver Bimber, Daisuke Iwai","author_ids":"1683711, 1752105","abstract":"We present a simple and cost-efficient way of extending contrast, perceived tonal resolution, and color space of reflective media, such as paper prints, hardcopy photographs, or electronic paper displays. A calibrated projector-camera system is applied for automatic registration, radiometric scanning and superimposition. A second modulation of the projected light on the surface of such media results in a high dynamic range visualization. This holds application potential for a variety of domains, such as radiology, astronomy, optical microscopy, conservation and restoration of historic art, modern art and entertainment installations.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"332cbc7179b1b6614d7f69a506bd7865ede6a7ee","venue_1":"ACM Trans. Graph.","year":"2011","title":"Realistic perspective projections for virtual objects and environments","authors":"Frank Steinicke, Gerd Bruder, Scott Kuhl","author_ids":"1740244, 1867700, 2213575","abstract":"Computer graphics systems provide sophisticated means to render virtual 3D space to 2D display surfaces by applying planar geometric projections. In a realistic viewing condition the perspective applied for rendering should appropriately account for the viewer's location relative to the image. As a result, an observer would not be able to distinguish between a rendering of a virtual environment on a computer screen and a view &#8220;through&#8221; the screen at an identical real-world scene. Until now, little effort has been made to identify perspective projections which cause human observers to judge them to be realistic.\n In this article we analyze observers' awareness of perspective distortions of virtual scenes displayed on a computer screen. These distortions warp the virtual scene and make it differ significantly from how the scene would look in reality. We describe psychophysical experiments that explore the subject's ability to discriminate between different perspective projections and identify projections that most closely match an equivalent real scene. We found that the field of view used for perspective rendering should match the actual visual angle of the display to provide users with a realistic view. However, we found that slight changes of the field of view in the range of 10-20&percnt; for two classes of test environments did not cause a distorted mental image of the observed models.","cites":"10","conferencePercentile":"19.47368421"},{"venue":"ACM Trans. Graph.","id":"3c0927dc97515b2ae9d97744513440f0a06f2142","venue_1":"ACM Trans. Graph.","year":"2003","title":"Flows on surfaces of arbitrary topology","authors":"Jos Stam","author_ids":"1720764","abstract":"In this paper we introduce a method to simulate fluid flows on smooth surfaces of arbitrary topology: an effect never seen before. We achieve this by combining a two-dimensional stable fluid solver with an atlas of parametrizations of a Catmull-Clark surface. The contributions of this paper are: (i) an extension of the Stable Fluids solver to arbitrary curvilinear coordinates, (ii) an elegant method to handle cross-patch boundary conditions and (iii) a set of new external forces custom tailored for surface flows. Our techniques can also be generalized to handle other types of processes on surfaces modeled by partial differential equations, such as reaction-diffusion. Some of our simulations allow a user to interactively place densities and apply forces to the surface, then watch their effects in real-time. We have also computed higher resolution animations of surface flows off-line.","cites":"124","conferencePercentile":"68.8172043"},{"venue":"ACM Trans. Graph.","id":"43e48b702fbe1feba53afbf82ec322cc9a61ae6c","venue_1":"ACM Trans. Graph.","year":"2013","title":"OpenSurfaces: a richly annotated catalog of surface appearance","authors":"Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala","author_ids":"3119803, 3222840, 1830653, 8261370","abstract":"The appearance of surfaces in real-world scenes is determined by the materials, textures, and context in which the surfaces appear. However, the datasets we have for visualizing and modeling rich surface appearance in context, in applications such as home remodeling, are quite limited. To help address this need, we present OpenSurfaces, a rich, labeled database consisting of thousands of examples of surfaces segmented from consumer photographs of interiors, and annotated with material parameters (reflectance, material names), texture information (surface normals, rectified textures), and contextual information (scene category, and object names).\n Retrieving usable surface information from uncalibrated Internet photo collections is challenging. We use human annotations and present a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk). Because of the noise and variability inherent in Internet photos and novice annotators, designing this annotation engine was a key challenge; we present a multi-stage set of annotation tasks with quality checks and validation. We demonstrate the use of this database in proof-of-concept applications including surface retexturing and material and image browsing, and discuss future uses. OpenSurfaces is a public resource available at http://opensurfaces.cs.cornell.edu/.","cites":"43","conferencePercentile":"94.11764706"},{"venue":"ACM Trans. Graph.","id":"fb528dcfa355779aff996cd1f0ef9d96f918080f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Intrinsic images in the wild","authors":"Sean Bell, Kavita Bala, Noah Snavely","author_ids":"3119803, 8261370, 1830653","abstract":"Intrinsic image decomposition separates an image into a reflectance layer and a shading layer. Automatic intrinsic image decomposition remains a significant challenge, particularly for real-world scenes. Advances on this longstanding problem have been spurred by public datasets of ground truth data, such as the MIT Intrinsic Images dataset. However, the difficulty of acquiring ground truth data has meant that such datasets cover a small range of materials and objects. In contrast, real-world scenes contain a rich range of shapes and materials, lit by complex illumination.\n In this paper we introduce <i>Intrinsic Images in the Wild</i>, a large-scale, public dataset for evaluating intrinsic image decompositions of indoor scenes. We create this benchmark through millions of crowdsourced annotations of relative comparisons of material properties at pairs of points in each scene. Crowdsourcing enables a scalable approach to acquiring a large database, and uses the ability of humans to judge material comparisons, despite variations in illumination. Given our database, we develop a dense CRF-based intrinsic image algorithm for images in the wild that outperforms a range of state-of-the-art intrinsic image algorithms. Intrinsic image decomposition remains a challenging problem; we release our code and database publicly to support future research on this problem, available online at http://intrinsic.cs.cornell.edu/.","cites":"46","conferencePercentile":"98.97119342"},{"venue":"ACM Trans. Graph.","id":"41b3458c4f3288321c329f5f56989520b0d7e587","venue_1":"ACM Trans. Graph.","year":"2011","title":"Interference-aware geometric modeling","authors":"David Harmon, Daniele Panozzo, Olga Sorkine-Hornung, Denis Zorin","author_ids":"1786660, 3241132, 2250001, 1798055","abstract":"While often a requirement for geometric models, there has been little research in resolving the interaction of deforming surfaces during real-time modeling sessions. To address this important topic, we introduce an interference algorithm specifically designed for the domain of geometric modeling. This algorithm is general, easily working within existing modeling paradigms to maintain their important properties. Our algorithm is fast, and is able to maintain interactive rates on complex deforming meshes of over 75K faces, while robustly removing intersections. Lastly, our method is controllable, allowing fine-tuning to meet the specific needs of the user. This includes support for minimum separation between surfaces and control over the relative rigidity of interacting objects.","cites":"19","conferencePercentile":"42.89473684"},{"venue":"ACM Trans. Graph.","id":"6b578ed86b91ca3dd2cb1978a6945f5bf32b1888","venue_1":"ACM Trans. Graph.","year":"2013","title":"Augmenting physical avatars using projector-based illumination","authors":"Amit Bermano, Philipp Brüschweiler, Anselm Grundhöfer, Daisuke Iwai, Bernd Bickel, Markus H. Gross","author_ids":"1755628, 2423373, 1697405, 1752105, 3083909, 1743207","abstract":"Animated animatronic figures are a unique way to give physical presence to a character. However, their movement and expressions are often limited due to mechanical constraints. In this paper, we propose a complete process for augmenting physical avatars using projector-based illumination, significantly increasing their expressiveness. Given an input animation, the system decomposes the motion into low-frequency motion that can be physically reproduced by the animatronic head and high-frequency details that are added using projected shading. At the core is a spatio-temporal optimization process that compresses the motion in gradient space, ensuring faithful motion replay while respecting the physical limitations of the system. We also propose a complete multi-camera and projection system, including a novel defocused projection and subsurface scattering compensation scheme. The result of our system is a highly expressive physical avatar that features facial details and motion otherwise unattainable due to physical constraints.","cites":"9","conferencePercentile":"29.63800905"},{"venue":"ACM Trans. Graph.","id":"29ee07cfd19bed5c00ca6dcc39386a79fe0064e8","venue_1":"ACM Trans. Graph.","year":"2011","title":"On the velocity of an implicit surface","authors":"Jos Stam, Ryan Schmidt","author_ids":"1720764, 2291899","abstract":"In this article we derive an equation for the velocity of an arbitrary time-evolving implicit surface. Strictly speaking, only the normal component of the velocity is unambiguously defined. This is because an implicit surface does not have a unique parametrization. However, by enforcing a constraint on the evolution of the normal field we obtain a unique tangential component. We apply our formulas to surface tracking and to the problem of computing velocity vectors of a motion blurred blobby surface. Other possible applications are mentioned at the end of the article.","cites":"7","conferencePercentile":"10.26315789"},{"venue":"ACM Trans. Graph.","id":"4a5d8b68cd0f00438e8e418016bd62870d859591","venue_1":"ACM Trans. Graph.","year":"2012","title":"Fields on symmetric surfaces","authors":"Daniele Panozzo, Yaron Lipman, Enrico Puppo, Denis Zorin","author_ids":"3241132, 3232072, 1698966, 1798055","abstract":"Direction fields, line fields and cross fields are used in a variety of computer graphics applications ranging from non-photorealistic rendering to remeshing. In many cases, it is desirable that fields adhere to symmetry, which is predominant in natural as well as man-made shapes. We present an algorithm for designing smooth N-symmetry fields on surfaces respecting <i>generalized symmetries</i> of the shape, while maintaining alignment with local features. Our formulation for constructing symmetry fields is based on global symmetries, which are given as input to the algorithm, with no isometry assumptions. We explore in detail the properties of generalized symmetries (reflections in particular), and we also develop an algorithm for the robust computation of such symmetry maps, based on a small number of correspondences, for surfaces of genus zero.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"9c3d78f3481ec323dd6ee63b8adb6fd8522f6771","venue_1":"ACM Trans. Graph.","year":"2012","title":"Global parametrization by incremental flattening","authors":"Ashish Myles, Denis Zorin","author_ids":"2103823, 1798055","abstract":"Global parametrization of surfaces requires singularities (cones) to keep distortion minimal. We describe a method for finding cone locations and angles and an algorithm for global parametrization which aim to produce seamless parametrizations with low metric distortion. The idea of the method is to evolve the metric of the surface, starting with the original metric so that a growing fraction of the area of the surface is constrained to have zero Gaussian curvature; the curvature becomes gradually concentrated at a small set of vertices which become cones. We demonstrate that the resulting parametrizations have significantly lower metric distortion compared to previously proposed methods.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"913f86ed7403bef5ad4e9a56935759ab58729b9a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Fabricating BRDFs at high spatial resolution using wave optics","authors":"Anat Levin, Daniel Glasner, Ying Xiong, Frédo Durand, William T. Freeman, Wojciech Matusik, Todd E. Zickler","author_ids":"1801055, 1753860, 4124572, 1728125, 1768236, 1752521, 1713451","abstract":"Recent attempts to fabricate surfaces with custom reflectance functions boast impressive angular resolution, yet their spatial resolution is limited. In this paper we present a method to construct spatially varying reflectance at a high resolution of up to 220dpi, orders of magnitude greater than previous attempts, albeit with a lower angular resolution. The resolution of previous approaches is limited by the machining, but more fundamentally, by the geometric optics model on which they are built. Beyond a certain scale geometric optics models break down and wave effects must be taken into account. We present an analysis of incoherent reflectance based on wave optics and gain important insights into reflectance design. We further suggest and demonstrate a practical method, which takes into account the limitations of existing micro-fabrication techniques such as photolithography to design and fabricate a range of reflection effects, based on wave interference.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"5d65492d88fb74ff4687a4792b831a8618ceeffb","venue_1":"ACM Trans. Graph.","year":"2009","title":"Patch-based image vectorization with automatic curvilinear feature alignment","authors":"Tian Xia, Binbin Liao, Yizhou Yu","author_ids":"1737585, 2080663, 7877246","abstract":"Raster image vectorization is increasingly important since vector-based graphical contents have been adopted in personal computers and on the Internet. In this paper, we introduce an effective vector-based representation and its associated vectorization algorithm for full-color raster images. There are two important characteristics of our representation. First, the image plane is decomposed into nonoverlapping parametric triangular patches with curved boundaries. Such a simplicial layout supports a flexible topology and facilitates adaptive patch distribution. Second, a subset of the curved patch boundaries are dedicated to faithfully representing curvilinear features. They are automatically aligned with the features. Because of this, patches are expected to have moderate internal variations that can be well approximated using smooth functions. We have developed effective techniques for patch boundary optimization and patch color fitting to accurately and compactly approximate raster images with both smooth variations and curvilinear features. A real-time GPU-accelerated parallel algorithm based on recursive patch subdivision has also been developed for rasterizing a vectorized image. Experiments and comparisons indicate our image vectorization algorithm achieves a more accurate and compact vector-based representation than existing ones do.","cites":"23","conferencePercentile":"33.70165746"},{"venue":"ACM Trans. Graph.","id":"5b5edad5d810b9e0a23a18a67b6b7f7824122b86","venue_1":"ACM Trans. Graph.","year":"2013","title":"Worst-case structural analysis","authors":"Qingnan Zhou, Julian Panetta, Denis Zorin","author_ids":"2760811, 1964417, 1798055","abstract":"Direct digital manufacturing is a set of rapidly evolving technologies that provide easy ways to manufacture highly customized and unique products. The development pipeline for such products is radically different from the conventional manufacturing pipeline: 3D geometric models are designed by users often with little or no manufacturing experience, and sent directly to the printer. Structural analysis on the user side with conventional tools is often unfeasible as it requires specialized training and software. Trial-and-error, the most common approach, is time-consuming and expensive.\n We present a method that would identify structural problems in objects designed for 3D printing <i>based on geometry and material properties only</i>, without specific assumptions on loads and manual load setup. We solve a constrained optimization problem to determine the \"worst\" load distribution for a shape that will cause high local stress or large deformations. While in its general form this optimization has a prohibitively high computational cost, we demonstrate that an approximate method makes it possible to solve the problem rapidly for a broad range of printed models. We validate our method both computationally and experimentally and demonstrate that it has good predictive power for a number of diverse 3D printed shapes.","cites":"45","conferencePercentile":"95.02262443"},{"venue":"ACM Trans. Graph.","id":"a317631cee33212244b30faa7bb62be804324815","venue_1":"ACM Trans. Graph.","year":"2013","title":"Subspace integration with local deformations","authors":"David Harmon, Denis Zorin","author_ids":"1786660, 1798055","abstract":"Subspace techniques greatly reduce the cost of nonlinear simulation by approximating deformations with a small custom basis. In order to represent the deformations well (in terms of a global metric), the basis functions usually have global support, and cannot capture localized deformations. While reduced-space basis functions can be localized to some extent, capturing truly local deformations would still require a very large number of precomputed basis functions, significantly degrading both precomputation and online performance. We present an efficient approach to handling local deformations that cannot be predicted, most commonly arising from contact and collisions, by augmenting the subspace basis with custom functions derived from <i>analytic</i> solutions to static loading problems. We also present a new cubature scheme designed to facilitate fast computation of the necessary runtime quantities while undergoing a changing basis. Our examples yield a two order of magnitude speedup over full-coordinate simulations, striking a desirable balance between runtime speeds and expressive ability.","cites":"18","conferencePercentile":"70.13574661"},{"venue":"ACM Trans. Graph.","id":"f52088a61b63e79c769e920e6889ef02bd6cf976","venue_1":"ACM Trans. Graph.","year":"2016","title":"Jump: virtual reality video","authors":"Noah Snavely, Sameer Agarwal, Steven M. Seitz","author_ids":"1830653, 3109650, 1679223","abstract":"We present Jump, a practical system for capturing high resolution, omnidirectional stereo (ODS) video suitable for wide scale consumption in currently available virtual reality (VR) headsets. Our system consists of a video camera built using off-the-shelf components and a fully automatic stitching pipeline capable of capturing video content in the ODS format. We have discovered and analyzed the distortions inherent to ODS when used for VR display as well as those introduced by our capture method and show that they are small enough to make this approach suitable for capturing a wide variety of scenes. Our stitching algorithm produces robust results by reducing the problem to one of pairwise image interpolation followed by compositing. We introduce novel optical flow and compositing methods designed specifically for this task. Our algorithm is temporally coherent and efficient, is currently running at scale on a distributed computing platform, and is capable of processing hours of footage each day.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"02538f22ed29232967e772e3b9adaf405b7452b1","venue_1":"ACM Trans. Graph.","year":"2013","title":"Controlled-distortion constrained global parametrization","authors":"Ashish Myles, Denis Zorin","author_ids":"2103823, 1798055","abstract":"The quality of a global parametrization is determined by a number of factors, including amount of distortion, number of singularities (cones), and alignment with features and boundaries. Placement of cones plays a decisive role in determining the overall distortion of the parametrization; at the same time, feature and boundary alignment also affect the cone placement. A number of methods were proposed for automatic choice of cone positions, either based on singularities of cross-fields and emphasizing alignment, or based on distortion optimization.\n In this paper we describe a method for placing cones for seamless global parametrizations with alignment constraints. We use a close relation between variation-minimizing cross-fields and related 1-forms and conformal maps, and demonstrate how it leads to a constrained optimization problem formulation. We show for boundary-aligned parametrizations metric distortion may be reduced by <i>cone chains</i>, sometimes to an arbitrarily small value, and the trade-off between the distortion and the number of cones can be controlled by a regularization term. Constrained parametrizations computed using our method have significantly lower distortion compared to the state-of-the art field-based method, yet maintain feature and boundary alignment. In the most extreme cases, parametrization collapse due to alignment constraints is eliminated.","cites":"17","conferencePercentile":"67.19457014"},{"venue":"ACM Trans. Graph.","id":"027cb0a56f4845f7f7fbf7c1121cdeb9f665ebd1","venue_1":"ACM Trans. Graph.","year":"2013","title":"Content-adaptive lenticular prints","authors":"James Tompkin, Simon Heinzle, Jan Kautz, Wojciech Matusik","author_ids":"1854493, 1782613, 1690538, 1752521","abstract":"Lenticular prints are a popular medium for producing automultiscopic glasses-free 3D images. The light field emitted by such prints has a fixed spatial and angular resolution. We increase both perceived angular and spatial resolution by modifying the lenslet array to better match the content of a given light field. Our optimization algorithm analyzes the input light field and computes an optimal lenslet size, shape, and arrangement that best matches the input light field given a set of output parameters. The resulting emitted light field shows higher detail and smoother motion parallax compared to fixed-size lens arrays. We demonstrate our technique using rendered simulations and by 3D printing lens arrays, and we validate our approach in simulation with a user study.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"25b7b97fa8f158c449a65923fbb21a59e625f769","venue_1":"ACM Trans. Graph.","year":"2006","title":"Appearance manifolds for modeling time-variant appearance of materials","authors":"Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun Bao, Baining Guo, Harry Shum","author_ids":"4912907, 1743927, 1686911, 1726696, 6250419, 1679542, 2738456, 1698102","abstract":"We present a visual simulation technique called <i>appearance manifolds</i> for modeling the time-variant surface appearance of a material from data captured at a single instant in time. In modeling time-variant appearance, our method takes advantage of the key observation that concurrent variations in appearance over a surface represent different degrees of weathering. By reorganizing these various appearances in a manner that reveals their relative order with respect to weathering degree, our method infers spatial and temporal appearance properties of the material's weathering process that can be used to convincingly generate its weathered appearance at different points in time. Results with natural non-linear reflectance variations are demonstrated in applications such as visual simulation of weathering on 3D models, increasing and decreasing the weathering of real objects, and material transfer with weathering effects.","cites":"43","conferencePercentile":"31.01851852"},{"venue":"ACM Trans. Graph.","id":"15332de1b0d045be1f0d75108a85422b75032242","venue_1":"ACM Trans. Graph.","year":"2004","title":"Fluid control using the adjoint method","authors":"Antoine McNamara, Adrien Treuille, Zoran Popovic, Jos Stam","author_ids":"3150791, 3064395, 1696595, 1720764","abstract":"We describe a novel method for controlling physics-based fluid simulations through gradient-based nonlinear optimization. Using a technique known as the <i>adjoint method</i>, derivatives can be computed efficiently, even for large 3D simulations with millions of control parameters. In addition, we introduce the first method for the full control of free-surface liquids. We show how to compute adjoint derivatives through each step of the simulation, including the fast marching algorithm, and describe a new set of control parameters specifically designed for liquids.","cites":"137","conferencePercentile":"68.47826087"},{"venue":"ACM Trans. Graph.","id":"a0c36501e4d80e769c35d54795df3636feebaac8","venue_1":"ACM Trans. Graph.","year":"2003","title":"Keyframe control of smoke simulations","authors":"Adrien Treuille, Antoine McNamara, Zoran Popovic, Jos Stam","author_ids":"3064395, 3150791, 1696595, 1720764","abstract":"We describe a method for controlling smoke simulations through user-specified keyframes. To achieve the desired behavior, a continuous quasi-Newton optimization solves for appropriate \"wind\" forces to be applied to the underlying velocity field throughout the simulation. The cornerstone of our approach is a method to efficiently compute exact derivatives through the steps of a fluid simulation. We formulate an objective function corresponding to how well a simulation matches the user's keyframes, and use the derivatives to solve for force parameters that minimize this function. For animations with several keyframes, we present a novel multiple-shooting approach. By splitting large problems into smaller overlapping subproblems, we greatly speed up the optimization process while avoiding certain local minima.","cites":"138","conferencePercentile":"72.04301075"},{"venue":"ACM Trans. Graph.","id":"675e31918cb11473c8b9773ac41e10a4a68e43ef","venue_1":"ACM Trans. Graph.","year":"2010","title":"Feature-aligned T-meshes","authors":"Ashish Myles, Nico Pietroni, Denis Kovacs, Denis Zorin","author_ids":"2103823, 3283805, 2075157, 1798055","abstract":"High-order and regularly sampled surface representations are more efficient and compact than general meshes and considerably simplify many geometric modeling and processing algorithms. A number of recent algorithms for conversion of arbitrary meshes to regularly sampled form (typically quadrangulation) aim to align the resulting mesh with feature lines of the geometry. While resulting in a substantial improvement in mesh quality, feature alignment makes it difficult to obtain coarse regular patch partitions of the mesh.\n In this paper, we propose an approach to constructing patch layouts consisting of small numbers of quadrilateral patches while maintaining good feature alignment. To achieve this, we use quadrilateral T-meshes, for which the intersection of two faces may not be the whole edge or vertex, but a part of an edge. T-meshes offer more flexibility for reduction of the number of patches and vertices in a base domain while maintaining alignment with geometric features. At the same time, T-meshes retain many desirable features of quadrangulations, allowing construction of high-order representations, easy packing of regularly sampled geometric data into textures, as well as supporting different types of discretizations for physical simulation.","cites":"29","conferencePercentile":"50.87719298"},{"venue":"ACM Trans. Graph.","id":"5b72fc7760d5f64f2d590b5a3cd4f9df3f8df7b3","venue_1":"ACM Trans. Graph.","year":"2011","title":"Estimating dual-scale properties of glossy surfaces from step-edge lighting","authors":"Chun-Po Wang, Noah Snavely, Steve Marschner","author_ids":"3213514, 1830653, 2593798","abstract":"This paper introduces a rapid appearance capture method suited for a variety of common indoor surfaces, in which a single photograph of the reflection of a step edge is used to estimate both a BRDF and a statistical model for visible surface geometry, or mesostructure. It is applicable to surfaces with statistically stationary variation in surface height, even when these variations are large enough to produce visible texture in the image. Results are shown from a prototype system using a separate camera and LCD, demonstrating good visual matches for a range of man-made indoor materials.","cites":"11","conferencePercentile":"22.10526316"},{"venue":"ACM Trans. Graph.","id":"0928f50bdf185fba53f38190dd9fa4402685fded","venue_1":"ACM Trans. Graph.","year":"2008","title":"Structure-aware halftoning","authors":"Wai-Man Pang, Yingge Qu, Tien-Tsin Wong, Daniel Cohen-Or, Pheng-Ann Heng","author_ids":"1729624, 2549934, 1720633, 1701009, 1714602","abstract":"This paper presents an optimization-based halftoning technique that preserves the structure and tone similarities between the original and the halftone images. By optimizing an objective function consisting of both the structure and the tone metrics, the generated halftone images preserve visually sensitive texture details as well as the local tone. It possesses the blue-noise property and does not introduce annoying patterns. Unlike the existing edge-enhancement halftoning, the proposed method does not suffer from the deficiencies of edge detector. Our method is tested on various types of images. In multiple experiments and the user study, our method consistently obtains the best scores among all tested methods.","cites":"38","conferencePercentile":"44.75308642"},{"venue":"ACM Trans. Graph.","id":"34ecda046ac5d8ae4e42333e6d775516f696296e","venue_1":"ACM Trans. Graph.","year":"2008","title":"4-points Congruent Sets for Robust Pairwise Surface Registration","authors":"Dror Aiger, Niloy J. Mitra, Daniel Cohen-Or","author_ids":"1708690, 1710455, 1701009","abstract":"We introduce 4PCS, a fast and robust alignment scheme for 3D point sets that uses wide bases, which are known to be resilient to noise and outliers. The algorithm allows registering raw noisy data, possibly contaminated with outliers, without pre-filtering or denoising the data. Further, the method significantly reduces the number of trials required to establish a reliable registration between the underlying surfaces in the presence of noise, without any assumptions about starting alignment. Our method is based on a novel technique to extract all coplanar 4-points sets from a 3D point set that are approximately congruent, under rigid transformation, to a given set of coplanar 4-points. This extraction procedure runs in roughly <i>O(n<sup>2</sup> + k)</i> time, where <i>n</i> is the number of candidate points and <i>k</i> is the number of reported 4-points sets. In practice, when noise level is low and there is sufficient overlap, using local descriptors the time complexity reduces to <i>O(n + k)</i>. We also propose an extension to handle similarity and affine transforms. Our technique achieves an order of magnitude asymptotic acceleration compared to common randomized alignment techniques. We demonstrate the robustness of our algorithm on several sets of multiple range scans with varying degree of noise, outliers, and extent of overlap.","cites":"70","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"166aa6a7bed04d7c5f8f30792713919dd0bcb1a0","venue_1":"ACM Trans. Graph.","year":"2009","title":"Real-time hand-tracking with a color glove","authors":"Robert Y. Wang, Jovan Popovic","author_ids":"1692433, 1731389","abstract":"Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applications enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality.","cites":"218","conferencePercentile":"99.44751381"},{"venue":"ACM Trans. Graph.","id":"6559e804b92dbf9a746185455c4305bef3f69c72","venue_1":"ACM Trans. Graph.","year":"2014","title":"DecoBrush: drawing structured decorative patterns by example","authors":"Jingwan Lu, Connelly Barnes, Connie Wan, Paul Asente, Radomír Mech, Adam Finkelstein","author_ids":"2054975, 1794537, 3196790, 2934421, 2008027, 1707541","abstract":"Structured decorative patterns are common ornamentations in a variety of media like books, web pages, greeting cards and interior design. Creating such art from scratch using conventional software is time consuming for experts and daunting for novices. We introduce DecoBrush, a data-driven drawing system that generalizes the conventional digital \"painting\" concept beyond the scope of natural media to allow synthesis of structured decorative patterns following user-sketched paths. The user simply selects an example library and draws the overall shape of a pattern. DecoBrush then synthesizes a shape in the style of the exemplars but roughly matching the overall shape. If the designer wishes to alter the result, DecoBrush also supports user-guided refinement via simple drawing and erasing tools. For a variety of example styles, we demonstrate high-quality user-constrained synthesized patterns that visually resemble the exemplars while exhibiting plausible structural variations.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"78932ce8cfe12643aafc5cc75d6b9513619372af","venue_1":"ACM Trans. Graph.","year":"2013","title":"AIREAL: interactive tactile experiences in free air","authors":"Rajinder Sodhi, Ivan Poupyrev, Matthew Glisson, Ali Israr","author_ids":"1924499, 1736819, 2200814, 1769549","abstract":"AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation.","cites":"42","conferencePercentile":"92.98642534"},{"venue":"ACM Trans. Graph.","id":"14b551e411387705f1f152b7cb29a813c940b733","venue_1":"ACM Trans. Graph.","year":"2005","title":"SCAPE: shape completion and animation of people","authors":"Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, James Davis","author_ids":"1838674, 3346614, 1736370, 1738444, 2284975, 3050723","abstract":"We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a <i>pose deformation model</i> that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for <i>shape completion</i> --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture animation. In particular, our method is capable of constructing a high-quality animated surface model of a moving person, with realistic muscle deformation, using just a single static scan and a marker motion capture sequence of the person.","cites":"409","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"aad7826b952e3f1ab17b1dc181f58f3c13bd7216","venue_1":"ACM Trans. Graph.","year":"2016","title":"StyLit: illumination-guided example-based stylization of 3D renderings","authors":"Jakub Fiser, Ondrej Jamriska, Michal Lukác, Eli Shechtman, Paul Asente, Jingwan Lu, Daniel Sýkora","author_ids":"2798088, 3193874, 2834463, 2177801, 2934421, 2054975, 7997286","abstract":"We present an approach to example-based stylization of 3D renderings that better preserves the rich expressiveness of hand-created artwork. Unlike previous techniques, which are mainly guided by colors and normals, our approach is based on light propagation in the scene. This novel type of guidance can distinguish among context-dependent illumination effects, for which artists typically use different stylization techniques, and delivers a look closer to realistic artwork. In addition, we demonstrate that the current state of the art in guided texture synthesis produces artifacts that can significantly decrease the fidelity of the synthesized imagery, and propose an improved algorithm that alleviates them. Finally, we demonstrate our method's effectiveness on a variety of scenes and styles, in applications like interactive shading study or autocompletion.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"477d823fcec9d368d3aabc490daca2def5302891","venue_1":"ACM Trans. Graph.","year":"2004","title":"Supra-threshold control of peripheral LOD","authors":"Benjamin Watson, Neff Walker, Larry F. Hodges","author_ids":"2478056, 8257430, 1710833","abstract":"Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold -- the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be <i>improved</i> as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.","cites":"10","conferencePercentile":"2.173913043"},{"venue":"ACM Trans. Graph.","id":"330e27884f1ec85f813f406b7d1d1f2b1f46b13a","venue_1":"ACM Trans. Graph.","year":"2008","title":"Subdivision shading","authors":"Marc Alexa, Tamy Boubekeur","author_ids":"1751554, 1747280","abstract":"The idea of Phong Shading is applied to subdivision surfaces: normals are associated with vertices and the same construction is used for both locations and normals. This creates vertex positions <i>and</i> normals. The vertex normals are smoother than the normals of the subdivision surface and using vertex normals for shading attenuates the well known visual artifacts of many subdivision schemes. We demonstrate how to apply subdivision to normals and how blend and combine different normals for achieving a variety of effects.","cites":"3","conferencePercentile":"1.851851852"},{"venue":"ACM Trans. Graph.","id":"35515979c44b3deabd36ca7a72530bfec12027b5","venue_1":"ACM Trans. Graph.","year":"2013","title":"Flow reconstruction for data-driven traffic animation","authors":"David Wilkie, Jason Sewall, Ming C. Lin","author_ids":"2293889, 2308989, 1709625","abstract":"Virtualized traffic' reconstructs and displays continuous traffic flows from discrete spatio-temporal traffic sensor data or procedurally generated control input to enhance a sense of immersion in a dynamic virtual environment. In this paper, we introduce a fast technique to reconstruct traffic flows from in-road sensor measurements or procedurally generated data for interactive 3D visual applications. Our algorithm estimates the full state of the traffic flow from sparse sensor measurements (or procedural input) using a statistical inference method and a continuum traffic model. This estimated state then drives an agent-based traffic simulator to produce a 3D animation of vehicle traffic that statistically matches the original traffic conditions. Unlike existing traffic simulation and animation techniques, our method produces a full 3D rendering of individual vehicles as part of continuous traffic flows given discrete spatio-temporal sensor measurements. Instead of using a color map to indicate traffic conditions, users could visualize and fly over the reconstructed traffic in real time over a large digital cityscape.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"7f7442de0f5a0e62304286bccc438d53558f2432","venue_1":"ACM Trans. Graph.","year":"2002","title":"Image based flow visualization","authors":"Jarke J. van Wijk","author_ids":"1709060","abstract":"A new method for the visualization of two-dimensional fluid flow is presented. The method is based on the advection and decay of dye. These processes are simulated by defining each frame of a flow animation as a blend between a warped version of the previous image and a number of background images. For the latter a sequence of filtered white noise images is used: filtered in time and space to remove high frequency components. Because all steps are done using images, the method is named Image Based Flow Visualization (<sc>IBFV</sc>). With <sc>IBFV</sc> a wide variety of visualization techniques can be emulated. Flow can be visualized as moving textures with line integral convolution and spot noise. Arrow plots, streamlines, particles, and topological images can be generated by adding extra dye to the image. Unsteady flows, defined on arbitrary meshes, can be handled. <sc>IBFV</sc> achieves a high performance by using standard features of graphics hardware. Typically fifty frames per second are generated using standard graphics cards on PCs. Finally, <sc>IBFV</sc> is easy to understand, analyse, and implement.","cites":"61","conferencePercentile":"38"},{"venue":"ACM Trans. Graph.","id":"b3fa9b2f63da49ceca2d9c98a1f7cb60ded121f3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Symmetric tiling of closed surfaces: visualization of regular maps","authors":"Jarke J. van Wijk","author_ids":"1709060","abstract":"A regular map is a tiling of a closed surface into faces, bounded by edges that join pairs of vertices, such that these elements exhibit a maximal symmetry. For genus 0 and 1 (spheres and tori) it is well known how to generate and present regular maps, the Platonic solids are a familiar example. We present a method for the generation of space models of regular maps for genus 2 and higher. The method is based on a generalization of the method for tori. Shapes with the proper genus are derived from regular maps by tubification: edges are replaced by tubes. Tessellations are produced using group theory and hyperbolic geometry. The main results are a generic procedure to produce such tilings, and a collection of intriguing shapes and images. Furthermore, we show how to produce shapes of genus 2 and higher with a highly regular structure.","cites":"11","conferencePercentile":"15.46961326"},{"venue":"ACM Trans. Graph.","id":"57c640b55c565632cf6e045f9922d5ee863e9aca","venue_1":"ACM Trans. Graph.","year":"2008","title":"Phong Tessellation","authors":"Tamy Boubekeur, Marc Alexa","author_ids":"1747280, 1751554","abstract":"Modern 3D engines used in real-time applications provide shading that hides the lack of higher order continuity inside the shapes using modulated normals, textures, and tone-mapping -- artifacts remain only on interior contours and silhouettes if the surface geometry is not smooth. The basic idea in this paper is to apply a purely local refinement strategy that inflates the geometry enough to avoid these artifacts. Our technique is a geometric version of Phong normal interpolation, not applied on normals but on the vertex positions. We call this strategy Phong Tessellation.","cites":"10","conferencePercentile":"5.24691358"},{"venue":"ACM Trans. Graph.","id":"b718571a0539f10376e8ef994b145170b8724bcf","venue_1":"ACM Trans. Graph.","year":"2006","title":"Point-sampled cell complexes","authors":"Anders Adamson, Marc Alexa","author_ids":"2032408, 1751554","abstract":"A piecewise smooth surface, possibly with boundaries, sharp edges, corners, or other features is defined by a set of samples. The basic idea is to model surface patches, curve segments and points explicitly, and then to glue them together based on explicit connectivity information. The geometry is defined as the set of stationary points of a projection operator, which is generalized to allow modeling curves with samples, and extended to account for the connectivity information. Additional tangent constraints can be used to model shapes with continuous tangents across edges and corners.","cites":"18","conferencePercentile":"10.64814815"},{"venue":"ACM Trans. Graph.","id":"2878bba483a2c1b00e2188e312cd97be1ecd2325","venue_1":"ACM Trans. Graph.","year":"2004","title":"Interactive modeling of topologically complex geometric detail","authors":"Jianbo Peng, Daniel Kristjansson, Denis Zorin","author_ids":"3029373, 2346540, 1798055","abstract":"Volume textures aligned with a surface can be used to add topologically complex geometric detail to objects in an efficient way, while retaining an underlying simple surface structure.Adding a volume texture to a surface requires more than a conventional two-dimensional parameterization: a part of the space surrounding the surface has to be parameterized. Another problem with using volume textures for adding geometric detail is the difficulty in rendering implicitly represented surfaces, especially when they are changed interactively.In this paper we present algorithms for constructing and rendering volume-textured surfaces. We demonstrate a number of interactive operations that these algorithms enable.","cites":"39","conferencePercentile":"17.93478261"},{"venue":"ACM Trans. Graph.","id":"8ad495ff40be7f02c4a2fa4a0ef0df2a3fb8a455","venue_1":"ACM Trans. Graph.","year":"2004","title":"A simple manifold-based construction of surfaces of arbitrary smoothness","authors":"Lexing Ying, Denis Zorin","author_ids":"1773083, 1798055","abstract":"We present a smooth surface construction based on the manifold approach of Grimm and Hughes. We demonstrate how this approach can relatively easily produce a number of desirable properties which are hard to achieve simultaneously with polynomial patches, subdivision or variational surfaces. Our surfaces are <i>C</i><sup>&#8734;</sup>-continuous with explicit nonsingular <i>C</i><sup>&#8734;</sup> parameterizations, high-order flexible at control vertices, depend linearly on control points, have fixed-size local support for basis functions, and have good visual quality.","cites":"80","conferencePercentile":"40.76086957"},{"venue":"ACM Trans. Graph.","id":"92ce9089f075301d9a78eb01b86be0089ca3c061","venue_1":"ACM Trans. Graph.","year":"2008","title":"Real-time rendering of textures with feature curves","authors":"Evgueni Parilov, Denis Zorin","author_ids":"1960301, 1798055","abstract":"The standard bilinear interpolation on normal maps results in visual artifacts along sharp features, which are common for surfaces with creases, wrinkles, and dents. In many cases, spatially varying features, like the normals near discontinuity curves, are best represented as functions of the distance to the curve and the position along the curve. For high-quality interactive rendering at arbitrary magnifications, one needs to interpolate the distance field preserving discontinuity curves exactly.\n We present a real-time, GPU-based method for distance function and distance gradient interpolation which preserves discontinuity feature curves. The feature curves are represented by a set of quadratic Bezier curves, with minimal restrictions on their intersections. We demonstrate how this technique can be used for real-time rendering of complex feature patterns and blending normal maps with procedurally defined profiles near normal discontinuities.","cites":"11","conferencePercentile":"6.481481481"},{"venue":"ACM Trans. Graph.","id":"4aa02d6140cec740f0cbcf757c6ce46a86204b64","venue_1":"ACM Trans. Graph.","year":"2004","title":"Spacetime faces: high resolution capture for modeling and animation","authors":"Li Zhang, Noah Snavely, Brian Curless, Steven M. Seitz","author_ids":"1735367, 1830653, 1810052, 1679223","abstract":"We present an end-to-end system that goes from video sequences to high resolution, editable, dynamically controllable face models. The capture system employs synchronized video cameras and structured light projectors to record videos of a moving face from multiple viewpoints. A novel spacetime stereo algorithm is introduced to compute depth maps accurately and overcome over-fitting deficiencies in prior work. A new template fitting and tracking procedure fills in missing data and yields point correspondence across the entire sequence without using markers. We demonstrate a data-driven, interactive method for inverse kinematics that draws on the large set of fitted templates and allows for posing new expressions by dragging surface points directly. Finally, we describe new tools that model the dynamics in the input sequence to enable new animations, created via key-framing or texture-synthesis techniques.","cites":"306","conferencePercentile":"90.2173913"},{"venue":"ACM Trans. Graph.","id":"57da453eb7c70b68e37b195f4735246e3863e463","venue_1":"ACM Trans. Graph.","year":"2014","title":"Design and fabrication by example","authors":"Adriana Schulz, Ariel Shamir, David I. W. Levin, Pitchaya Sitthi-amorn, Wojciech Matusik","author_ids":"2711393, 2947946, 1694784, 1936229, 1752521","abstract":"We propose a data-driven method for designing 3D models that can be fabricated. First, our approach converts a collection of expert-created designs to a dataset of parameterized design templates that includes all information necessary for fabrication. The templates are then used in an interactive design system to create new fabri-cable models in a design-by-example manner. A simple interface allows novice users to choose template parts from the database, change their parameters, and combine them to create new models. Using the information in the template database, the system can automatically position, align, and connect parts: the system accomplishes this by adjusting parameters, adding appropriate constraints, and assigning connectors. This process ensures that the created models can be fabricated, saves the user from many tedious but necessary tasks, and makes it possible for non-experts to design and create actual physical objects. To demonstrate our data-driven method, we present several examples of complex functional objects that we designed and manufactured using our system.","cites":"17","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"16ec175ff4d6ffc93731d969525d4b2f567efeda","venue_1":"ACM Trans. Graph.","year":"2014","title":"Modeling and optimizing eye vergence response to stereoscopic cuts","authors":"Krzysztof Templin, Piotr Didyk, Karol Myszkowski, Mohamed Hefeeda, Hans-Peter Seidel, Wojciech Matusik","author_ids":"2668907, 3307078, 1790911, 1711116, 1746884, 1752521","abstract":"Sudden temporal depth changes, such as cuts that are introduced by video edits, can significantly degrade the quality of stereoscopic content. Since usually not encountered in the real world, they are very challenging for the audience. This is because the eye vergence has to constantly adapt to new disparities in spite of conflicting accommodation requirements. Such rapid disparity changes may lead to confusion, reduced understanding of the scene, and overall attractiveness of the content. In most cases the problem cannot be solved by simply matching the depth around the transition, as this would require flattening the scene completely. To better understand this limitation of the human visual system, we conducted a series of eye-tracking experiments. The data obtained allowed us to derive and evaluate a model describing adaptation of vergence to disparity changes on a stereoscopic display. Besides computing user-specific models, we also estimated parameters of an average observer model. This enables a range of strategies for minimizing the adaptation time in the audience.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"3334f53b4e4a44ba5955db421334156aa8a563a8","venue_1":"ACM Trans. Graph.","year":"2014","title":"Improving visual quality of view transitions in automultiscopic displays","authors":"Wojciech Matusik","author_ids":"1752521","abstract":"Automultiscopic screens present different images depending on the viewing direction. This enables glasses-free 3D and provides motion parallax effect. However, due to the limited angular resolution of such displays, they suffer from hot-spotting, i. e., image quality is highly affected by the viewing position. In this paper, we analyze light fields produced by lenticular and parallax-barrier displays, and show that, unlike in real world, the light fields produced by such screens have a repetitive structure. This induces visual artifacts in the form of view discontinuities, depth reversals, and excessive disparities when viewing position is not optimal. Although the problem has been always considered as inherent to the technology, we demonstrate that light fields reproduced on automultiscopic displays have enough degrees of freedom to improve the visual quality. We propose a new technique that modifies light fields using global and local shears followed by stitching to improve their continuity when displayed on a screen. We show that this enhances visual quality significantly, which is demonstrated in a series of user experiments with an automultiscopic display as well as lenticular prints.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"391c71d926c8bc0ea8dcf2ead05d59ef6a1057bf","venue_1":"ACM Trans. Graph.","year":"2006","title":"Photo tourism: exploring photo collections in 3D","authors":"Noah Snavely, Steven M. Seitz, Richard Szeliski","author_ids":"1830653, 1679223, 1717841","abstract":"We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our <i>photo explorer</i> uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.","cites":"1134","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"4a911a4c423caaff9995e8b4521344c4643cf795","venue_1":"ACM Trans. Graph.","year":"2008","title":"Finding paths through the world's photos","authors":"Noah Snavely, Rahul Garg, Steven M. Seitz, Richard Szeliski","author_ids":"1830653, 1779656, 1679223, 1717841","abstract":"When a scene is photographed many times by different people, the viewpoints often cluster along certain paths. These paths are largely specific to the scene being photographed, and follow interesting regions and viewpoints. We seek to discover a range of such paths and turn them into controls for image-based rendering. Our approach takes as input a large set of community or personal photos, reconstructs camera viewpoints, and automatically computes orbits, panoramas, canonical views, and optimal paths between views. The scene can then be interactively browsed in 3D using these controls or with six degree-of-freedom free-viewpoint control. As the user browses the scene, nearby views are continuously selected and transformed, using control-adaptive reprojection techniques.","cites":"99","conferencePercentile":"91.04938272"},{"venue":"ACM Trans. Graph.","id":"d7d21459e681e5d2459d082d4fc6e77ceb241378","venue_1":"ACM Trans. Graph.","year":"2014","title":"Boxelization: folding 3D objects into boxes","authors":"Yahan Zhou, Shinjiro Sueda, Wojciech Matusik, Ariel Shamir","author_ids":"2753846, 7229044, 1752521, 2947946","abstract":"We present a method for transforming a 3D object into a cube or a box using a continuous folding sequence. Our method produces a single, connected object that can be physically fabricated and folded from one shape to the other. We segment the object into voxels and search for a voxel-tree that can fold from the input shape to the target shape. This involves three major steps: finding a good voxelization, finding the tree structure that can form the input and target shapes' configurations, and finding a non-intersecting folding sequence. We demonstrate our results on several input 3D objects and also physically fabricate some using a 3D printer.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"ab2ded5c7e907bcd0781c4a7c4efc2e1c83980c6","venue_1":"ACM Trans. Graph.","year":"2014","title":"Computational Light Routing: 3D Printed Optical Fibers for Sensing and Display","authors":"Thiago Pereira, Szymon Rusinkiewicz, Wojciech Matusik","author_ids":"4497797, 7723706, 1752521","abstract":"Despite recent interest in digital fabrication, there are still few algorithms that provide control over how light propagates inside a solid object. Existing methods either work only on the surface or restrict themselves to light diffusion in volumes. We use multi-material 3D printing to fabricate objects with embedded optical fibers, exploiting total internal reflection to guide light <i>inside</i> an object. We introduce automatic fiber design algorithms together with new manufacturing techniques to route light between two arbitrary surfaces. Our implicit algorithm optimizes light transmission by minimizing fiber curvature and maximizing fiber separation while respecting constraints such as fiber arrival angle. We also discuss the influence of different printable materials and fiber geometry on light propagation in the volume and the light angular distribution when exiting the fiber. Our methods enable new applications such as surface displays of arbitrary shape, touch-based painting of surfaces, and sensing a hemispherical light distribution in a single shot.","cites":"3","conferencePercentile":"13.78600823"},{"venue":"ACM Trans. Graph.","id":"2e3093767d091911a639deca6cae81404516e0fc","venue_1":"ACM Trans. Graph.","year":"2015","title":"Data-driven finite elements for geometry and material design","authors":"Desai Chen, David I. W. Levin, Shinjiro Sueda, Wojciech Matusik","author_ids":"1701750, 1694784, 7229044, 1752521","abstract":"Crafting the behavior of a deformable object is difficult---whether it is a biomechanically accurate character model or a new multimaterial 3D printable design. Getting it right requires constant iteration, performed either manually or driven by an automated system. Unfortunately, Previous algorithms for accelerating three-dimensional finite element analysis of elastic objects suffer from expensive precomputation stages that rely on <i>a priori</i> knowledge of the object's geometry and material composition. In this paper we introduce Data-Driven Finite Elements as a solution to this problem. Given a material palette, our method constructs a metamaterial library which is reusable for subsequent simulations, regardless of object geometry and/or material composition. At runtime, we perform fast coarsening of a simulation mesh using a simple table lookup to select the appropriate metamaterial model for the coarsened elements. When the object's material distribution or geometry changes, we do not need to update the metamaterial library---we simply need to update the metamaterial assignments to the coarsened elements. An important advantage of our approach is that it is applicable to non-linear material models. This is important for designing objects that undergo finite deformation (such as those produced by multimaterial 3D printing). Our method yields speed gains of up to two orders of magnitude while maintaining good accuracy. We demonstrate the effectiveness of the method on both virtual and 3D printed examples in order to show its utility as a tool for deformable object design.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"564ab2b51d49870032b57ede0cce3250bd979662","venue_1":"ACM Trans. Graph.","year":"2015","title":"Phasor Imaging: A Generalization of Correlation-Based Time-of-Flight Imaging","authors":"Mohit Gupta, Shree K. Nayar, Matthias B. Hullin, Jaime Martín","author_ids":"2122821, 1750470, 1899671, 2828054","abstract":"In <i>correlation-based time-of-flight</i> (C-ToF) imaging systems, light sources with temporally varying intensities illuminate the scene. Due to global illumination, the temporally varying radiance received at the sensor is a combination of light received along multiple paths. Recovering scene properties (e.g., scene depths) from the received radiance requires separating these contributions, which is challenging due to the complexity of global illumination and the additional temporal dimension of the radiance.\n We propose phasor imaging, a framework for performing fast inverse light transport analysis using C-ToF sensors. Phasor imaging is based on the idea that, by representing light transport quantities as phasors and light transport events as phasor transformations, light transport analysis can be simplified in the temporal frequency domain. We study the effect of temporal illumination frequencies on light transport and show that, for a broad range of scenes, global radiance (inter-reflections and volumetric scattering) vanishes for frequencies higher than a scene-dependent threshold. We use this observation for developing two novel scene recovery techniques. First, we present micro-ToF imaging, a ToF-based shape recovery technique that is robust to errors due to inter-reflections (multipath interference) and volumetric scattering. Second, we present a technique for separating the direct and global components of radiance. Both techniques require capturing as few as 3--4 images and minimal computations. We demonstrate the validity of the presented techniques via simulations and experiments performed with our hardware prototype.","cites":"14","conferencePercentile":"96.32653061"},{"venue":"ACM Trans. Graph.","id":"8ce351280b8a799cde70a4cf5a694ee2db4d9b72","venue_1":"ACM Trans. Graph.","year":"2007","title":"Prakash: lighting aware motion capture using photosensing markers and multiplexed illuminators","authors":"Ramesh Raskar, Hideaki Nii, Bert de Decker, Yuki Hashimoto, Jay Summet, Dylan Moore, Yong Zhao, Jonathan Westhues, Paul H. Dietz, John Barnwell, Shree K. Nayar, Masahiko Inami, Philippe Bekaert, Michael Noland, Vlad Branzoi, Erich Bruns","author_ids":"1717566, 1736915, 2357248, 2404132, 2585396, 8420558, 1731063, 2711385, 1805795, 4867148, 1750470, 1684930, 7583073, 2317190, 2090570, 2823458","abstract":"In this paper, we present a high speed optical motion capture method that can measure three dimensional motion, orientation, and incident illumination at tagged points in a scene. We use tracking tags that work in natural lighting conditions and can be imperceptibly embedded in attire or other objects. Our system supports an unlimited number of tags in a scene, with each tag uniquely identified to eliminate marker reacquisition issues. Our tags also provide incident illumination data which can be used to match scene lighting when inserting synthetic elements. The technique is therefore ideal for on-set motion capture or real-time broadcasting of virtual sets.\n Unlike previous methods that employ high speed cameras or scanning lasers, we capture the scene appearance using the simplest possible optical devices - a light-emitting diode (LED) with a passive binary mask used as the transmitter and a photosensor used as the receiver. We strategically place a set of optical transmitters to spatio-temporally encode the volume of interest. Photosensors attached to scene points demultiplex the coded optical signals from multiple transmitters, allowing us to compute not only receiver location and orientation but also their incident illumination and the reflectance of the surfaces to which the photosensors are attached. We use our untethered tag system, called Prakash, to demonstrate methods of adding special effects to captured videos that cannot be accomplished using pure vision techniques that rely on camera images.","cites":"35","conferencePercentile":"32.8"},{"venue":"ACM Trans. Graph.","id":"01c096a86d4dbfc2fffb85005b5a92502c7bab2f","venue_1":"ACM Trans. Graph.","year":"2005","title":"Algebraic analysis of high-pass quantization","authors":"Doron Chen, Daniel Cohen-Or, Olga Sorkine-Hornung, Sivan Toledo","author_ids":"2571368, 1701009, 2250001, 1731568","abstract":"This article presents an algebraic analysis of a mesh-compression technique called <i>high-pass quantization</i> [Sorkine et al. 2003]. In high-pass quantization, a rectangular matrix based on the mesh topological Laplacian is applied to the vectors of the Cartesian coordinates of a polygonal mesh. The resulting vectors, called &#916;-coordinates, are then quantized. The applied matrix is a function of the topology of the mesh and the indices of a small set of mesh vertices (anchors) but not of the location of the vertices. An approximation of the geometry can be reconstructed from the quantized &#916;-coordinates and the spatial locations of the anchors. In this article, we show how to algebraically bound the reconstruction error that this method generates. We show that the small singular value of the transformation matrix can be used to bound both the quantization error and the rounding error which is due to the use of floating-point arithmetic. Furthermore, we prove a bound on this singular value. The bound is a function of the topology of the mesh and of the selected anchors. We also propose a new anchor-selection algorithm, inspired by this bound. We show experimentally that the method is effective and that the computed upper bound on the error is not too pessimistic.","cites":"17","conferencePercentile":"8.467741935"},{"venue":"ACM Trans. Graph.","id":"0a25288a5b0eed0677856fd2ee72952420f0a3ea","venue_1":"ACM Trans. Graph.","year":"2004","title":"Lighting sensitive display","authors":"Shree K. Nayar, Peter N. Belhumeur, Terrance E. Boult","author_ids":"1750470, 1767767, 1760117","abstract":"Although display devices have been used for decades, they have functioned without taking into account the illumination of their environment. We present the concept of a lighting sensitive display (LSD)---a display that measures the incident illumination and modifies its content accordingly. An ideal LSD would be able to measure the 4D illumination field incident upon it and generate a 4D light field in response to the illumination. However, current sensing and display technologies do not allow for such an ideal implementation. Our initial LSD prototype uses a 2D measurement of the illumination field and produces a 2D image in response to it. In particular, it renders a 3D scene such that it always appears to be lit by the real environment that the display resides in. The current system is designed to perform best when the light sources in the environment are distant from the display, and a single user in a known location views the display.\n The displayed scene is represented by compressing a very large set of images (acquired or rendered) of the scene that correspond to different lighting conditions. The compression algorithm is a lossy one that exploits not only image correlations over the illumination dimensions but also coherences over the spatial dimensions of the image. This results in a highly compressed representation of the original image set. This representation enables us to achieve high quality relighting of the scene in real time. Our prototype LSD can render 640 &#215; 480 images of scenes under complex and varying illuminations at 15 frames per second using a 2 GHz processor. We conclude with a discussion on the limitations of the current implementation and potential areas for future research.","cites":"32","conferencePercentile":"16.30434783"},{"venue":"ACM Trans. Graph.","id":"4dfddd76148db25a92c91322bb403342543938b8","venue_1":"ACM Trans. Graph.","year":"2009","title":"Robust single-view geometry and motion reconstruction","authors":"Hao Li, Bart Adams, Leonidas J. Guibas, Mark Pauly","author_ids":"1706574, 1700388, 1744254, 1741645","abstract":"We present a framework and algorithms for robust geometry and motion reconstruction of complex deforming shapes. Our method makes use of a smooth template that provides a crude approximation of the scanned object and serves as a geometric and topological prior for reconstruction. Large-scale motion of the acquired object is recovered using a novel space-time adaptive, non-rigid registration method. Fine-scale details such as wrinkles and folds are synthesized with an efficient linear mesh deformation algorithm. Subsequent spatial and temporal filtering of detail coefficients allows transfer of persistent geometric detail to regions not observed by the scanner. We show how this two-scale process allows faithful recovery of small-scale shape and motion features leading to a high-quality reconstruction. We illustrate the robustness and generality of our algorithm on a variety of examples composed of different materials and exhibiting a large range of dynamic deformations.","cites":"117","conferencePercentile":"96.13259669"},{"venue":"ACM Trans. Graph.","id":"e0e379eb4e66289448fd1ddf8841e457d093958f","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fab forms: customizable objects for fabrication with validity and geometry caching","authors":"Maria Shugrina, Ariel Shamir, Wojciech Matusik","author_ids":"2854827, 2947946, 1752521","abstract":"We address the problem of allowing casual users to customize parametric models while maintaining their valid state as 3D-printable functional objects. We define <i>Fab Form</i> as any design representation that lends itself to <i>interactive</i> customization by a novice user, while remaining <i>valid and manufacturable.</i> We propose a method to achieve these <i>Fab Form</i> requirements for general parametric designs tagged with a general set of automated validity tests and a small number of parameters exposed to the casual user. Our solution separates <i>Fab Form</i> evaluation into a precomputation stage and a runtime stage. Parts of the geometry and design validity (such as manufacturability) are evaluated and stored in the precomputation stage by adaptively sampling the design space. At runtime the remainder of the evaluation is performed. This allows interactive navigation in the valid regions of the design space using an automatically generated Web user interface (UI). We evaluate our approach by converting several parametric models into corresponding <i>Fab Forms.</i>","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"59aba9a1372d1eade9d8d71ba3be98d20b4f5dcc","venue_1":"ACM Trans. Graph.","year":"2005","title":"Motion magnification","authors":"Ce Liu, Antonio Torralba, William T. Freeman, Frédo Durand, Edward H. Adelson","author_ids":"1681442, 1690178, 1768236, 1728125, 1788148","abstract":"We present motion magnification, a technique that acts like a microscope for visual motion. It can amplify subtle motions in a video sequence, allowing for visualization of deformations that would otherwise be invisible. To achieve motion magnification, we need to accurately measure visual motions, and group the pixels to be modified. After an initial image registration step, we measure motion by a robust analysis of feature point trajectories, and segment pixels based on similarity of position, color, and motion. A novel measure of motion similarity groups even very small motions according to correlation over time, which often relates to physical cause. An outlier mask marks observations not explained by our layered motion model, and those pixels are simply reproduced on the output from the original registered observations.The motion of any selected layer may be magnified by a user-specified amount; texture synthesis fills-in unseen \"holes\" revealed by the amplified motions. The resulting motion-magnified images can reveal or emphasize small motions in the original sequence, as we demonstrate with deformations in load-bearing structures, subtle motions or balancing corrections of people, and \"rigid\" structures bending under hand pressure.","cites":"73","conferencePercentile":"50.80645161"},{"venue":"ACM Trans. Graph.","id":"0a569e1d427997254cef93d174ef53732cd0eeb0","venue_1":"ACM Trans. Graph.","year":"2015","title":"Hexahedral mesh re-parameterization from aligned base-complex","authors":"Xifeng Gao, Zhigang Deng, Guoning Chen","author_ids":"2180685, 7458303, 1861830","abstract":"Recently, generating a high quality all-hex mesh of a given volume has gained much attention. However, little, if any, effort has been put into the optimization of the hex-mesh structure, which is equally important to the local element quality of a hex-mesh that may influence the performance and accuracy of subsequent computations. In this paper, we present a first and complete pipeline to optimize the global structure of a hex-mesh. Specifically, we first extract the <i>base-complex</i> of a hex-mesh and study the misalignments among its singularities by adapting the previously introduced hexahedral sheets to the base-complex. Second, we identify the valid removal <i>base-complex</i> sheets from the base-complex that contain misaligned singularities. We then propose an effective algorithm to remove these valid removal sheets in order. Finally, we present a structure-aware optimization strategy to improve the geometric quality of the resulting hex-mesh after fixing the misalignments. Our experimental results demonstrate that our pipeline can significantly reduce the number of components of a variety of hex-meshes generated by state-of-the-art methods, while maintaining high geometric quality.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"ff7cafec8249759bd08a8b13ab9725a837b084c3","venue_1":"ACM Trans. Graph.","year":"2016","title":"Interactive mechanism modeling from multi-view images","authors":"Zhigang Deng","author_ids":"7458303","abstract":"In this paper, we present an interactive system for mechanism modeling from multi-view images. Its key feature is that the generated 3D mechanism models contain not only geometric shapes but also internal motion structures: they can be directly animated through kinematic simulation. Our system consists of two steps: interactive 3D modeling and stochastic motion parameter estimation. At the 3D modeling step, our system is designed to integrate the sparse 3D points reconstructed from multi-view images and a sketching interface to achieve accurate 3D modeling of a mechanism. To recover the motion parameters, we record a video clip of the mechanism motion and adopt stochastic optimization to recover its motion parameters by edge matching. Experimental results show that our system can achieve the 3D modeling of a range of mechanisms from simple mechanical toys to complex mechanism objects.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"bd27666a792b976a86b03cbfc3bf371cab62b48e","venue_1":"ACM Trans. Graph.","year":"2014","title":"Spin-it: optimizing moment of inertia for spinnable objects","authors":"Moritz Bächer, Emily Whiting, Bernd Bickel, Olga Sorkine-Hornung","author_ids":"8021864, 2778710, 3083909, 2250001","abstract":"Spinning tops and yo-yos have long fascinated cultures around the world with their unexpected, graceful motions that seemingly elude gravity. We present an algorithm to generate designs for spinning objects by optimizing rotational dynamics properties. As input, the user provides a solid 3D model and a desired axis of rotation. Our approach then modifies the mass distribution such that the principal directions of the moment of inertia align with the target rotation frame. We augment the model by creating voids inside its volume, with interior fill represented by an adaptive multi-resolution voxelization. The discrete voxel fill values are optimized using a continuous, nonlinear formulation. Further, we optimize for rotational stability by maximizing the dominant principal moment. We extend our technique to incorporate deformation and multiple materials for cases where internal voids alone are insufficient. Our method is well-suited for a variety of 3D printed models, ranging from characters to abstract shapes. We demonstrate tops and yo-yos that spin surprisingly stably despite their asymmetric appearance.","cites":"31","conferencePercentile":"96.91358025"},{"venue":"ACM Trans. Graph.","id":"4194b868f56330983a5519d52f1c6b995a0e1238","venue_1":"ACM Trans. Graph.","year":"2007","title":"Image-guided maze construction","authors":"Jie Xu, Craig S. Kaplan","author_ids":"1720015, 2345263","abstract":"We present a set of graphical and combinatorial algorithms for designing mazes based on images. The designer traces regions of interest in an image and annotates the regions with style parameters. They can optionally specify a solution path, which provides a rough guide for laying out the maze's actual solution. The system uses novel extensions to well-known maze construction algorithms to build mazes that approximate the tone of the source image, express the desired style in each region, and conform to the user's solution path.","cites":"21","conferencePercentile":"13.2"},{"venue":"ACM Trans. Graph.","id":"4bd241e324ef02c1f5df997d992cf7b36b07515f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Discontinuity-aware video object cutout","authors":"Fan Zhong, Xueying Qin, Qunsheng Peng, Xiangxu Meng","author_ids":"2579992, 1754175, 1716360, 1759133","abstract":"Existing video object cutout systems can only deal with limited cases. They usually require detailed user interactions to segment real-life videos, which often suffer from both inseparable statistics (similar appearance between foreground and background) and temporal discontinuities (e.g. large movements, newly-exposed regions following disocclusion or topology change).\n In this paper, we present an efficient video cutout system to meet this challenge. A novel directional classifier is proposed to handle temporal discontinuities robustly, and then multiple classifiers are incorporated to cover a variety of cases. The outputs of these classifiers are integrated via another classifier, which is learnt from real examples. The foreground matte is solved by a coherent matting procedure, and remaining errors can be removed easily by additive spatio-temporal local editing. Experiments demonstrate that our system performs more robustly and more intelligently than existing systems in dealing with various input types, thus saving a lot of user labor and time.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"0568422f6c7e89fbde4071098d91f650ffac4fff","venue_1":"ACM Trans. Graph.","year":"2015","title":"Computational design of metallophone contact sounds","authors":"Gaurav Bharaj, David I. W. Levin, James Tompkin, Yun Fei, Hanspeter Pfister, Wojciech Matusik, Changxi Zheng","author_ids":"3090007, 1694784, 1854493, 2547088, 1701371, 1752521, 1797875","abstract":"Metallophones such as glockenspiels produce sounds in response to contact. Building these instruments is a complicated process, limiting their shapes to well-understood designs such as bars. We automatically optimize the shape of arbitrary 2D and 3D objects through deformation and perforation to produce sounds when struck which match user-supplied frequency and amplitude spectra. This optimization requires navigating a complex energy landscape, for which we develop <i>Latin Complement Sampling</i> to both speed up finding minima and provide probabilistic bounds on landscape exploration. Our method produces instruments which perform similarly to those that have been professionally-manufactured, while also expanding the scope of shape and sound that can be realized, e.g., single object chords. Furthermore, we can optimize sound spectra to create overtones and to dampen specific frequencies. Thus our technique allows even novices to design metallophones with unique sound and appearance.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"260b647283d02cbe5e4e07401dc685a1663a6c5b","venue_1":"ACM Trans. Graph.","year":"2008","title":"Extracting depth and matte using a color-filtered aperture","authors":"Yosuke Bando, Bing-Yu Chen, Tomoyuki Nishita","author_ids":"1799420, 1733344, 1696605","abstract":"This paper presents a method for automatically extracting a scene depth map and the alpha matte of a foreground object by capturing a scene through RGB color filters placed in the camera lens aperture. By dividing the aperture into three regions through which only light in one of the RGB color bands can pass, we can acquir three shifted views of a scene in the RGB planes of an image in a single exposure. In other words, a captured image has depth-dependent color misalignment. We develop a color alignment measure to estimate disparities between the RGB planes for depth reconstruction. We also exploit color misalignment cues in our matting algorithm in order to disambiguate between the foreground and background regions even where their colors are similar. Based on the extracted depth and matte, the color misalignment in the captured image can be canceled, and various image editing operations can be applied to the reconstructed image, including novel view synthesis, postexposure refocusing, and composition over different backgrounds.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"d93d34b1ade342207abaaf04a109b317fb2d4e6f","venue_1":"ACM Trans. Graph.","year":"2015","title":"IM6D: magnetic tracking system with 6-DOF passive markers for dexterous 3D interaction and motion","authors":"Jiawei Huang, Tsuyoshi Mori, Kazuki Takashima, Shuichiro Hashi, Yoshifumi Kitamura","author_ids":"2504119, 5163472, 1725740, 2406417, 1690228","abstract":"We propose IM6D, a novel real-time magnetic motion-tracking system using multiple identifiable, tiny, lightweight, wireless and occlusion-free markers. It provides reasonable accuracy and update rates and an appropriate working space for dexterous 3D interaction. Our system follows a novel electromagnetic induction principle to externally excite wireless LC coils and uses an externally located pickup coil array to track each of the LC coils with 5-DOF. We apply this principle to design a practical motion-tracking system using multiple markers with 6-DOF and to achieve reliable tracking with reasonable speed. We also solved the principle's inherent dead-angle problem. Based on this method, we simulated the configuration of parameters for designing a system with scalability for dexterous 3D motion. We implemented an actual system and applied a parallel computation structure to increase the tracking speed. We also built some examples to show how well our system works for actual situations.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"0f7c41018545194e489baaf91e72ef29220471ea","venue_1":"ACM Trans. Graph.","year":"2004","title":"Eyes for relighting","authors":"Ko Nishino, Shree K. Nayar","author_ids":"1708819, 1750470","abstract":"The combination of the cornea of an eye and a camera viewing the eye form a catadioptric (mirror + lens) imaging system with a very wide field of view. We present a detailed analysis of the characteristics of this corneal imaging system. Anatomical studies have shown that the shape of a normal cornea (without major defects) can be approximated with an ellipsoid of fixed eccentricity and size. Using this shape model, we can determine the geometric parameters of the corneal imaging system from the image. Then, an environment map of the scene with a large field of view can be computed from the image. The environment map represents the illumination of the scene with respect to the eye. This use of an eye as a natural light probe is advantageous in many relighting scenarios. For instance, it enables us to insert virtual objects into an image such that they appear consistent with the illumination of the scene. The eye is a particularly useful probe when relighting faces. It allows us to reconstruct the geometry of a face by simply waving a light source in front of the face. Finally, in the case of an already captured image, eyes could be the only direct means for obtaining illumination information. We show how illumination computed from eyes can be used to replace a face in an image with another one. We believe that the eye not only serves as a useful tool for relighting but also makes relighting possible in situations where current approaches are hard to use.","cites":"53","conferencePercentile":"25"},{"venue":"ACM Trans. Graph.","id":"caa3cff434808af54777c1f66300cd587f3875cf","venue_1":"ACM Trans. Graph.","year":"2010","title":"Unbiased, adaptive stochastic sampling for rendering inhomogeneous participating media","authors":"Yonghao Yue, Kei Iwasaki, Bing-Yu Chen, Yoshinori Dobashi, Tomoyuki Nishita","author_ids":"2276806, 1787808, 1733344, 1791538, 1696605","abstract":"Realistic rendering of participating media is one of the major subjects in computer graphics. Monte Carlo techniques are widely used for realistic rendering because they provide unbiased solutions, which converge to exact solutions. Methods based on Monte Carlo techniques generate a number of light paths, each of which consists of a set of randomly selected scattering events. Finding a new scattering event requires free path sampling to determine the distance from the previous scattering event, and is usually a time-consuming process for inhomogeneous participating media. To address this problem, we propose an adaptive and unbiased sampling technique using kd-tree based space partitioning. A key contribution of our method is an automatic scheme that partitions the spatial domain into sub-spaces (partitions) based on a cost model that evaluates the expected sampling cost. The magnitude of performance gain obtained by our method becomes larger for more inhomogeneous media, and rises to two orders compared to traditional free path sampling techniques.","cites":"15","conferencePercentile":"22.22222222"},{"venue":"ACM Trans. Graph.","id":"7bf1bfc569978310b676b2b0652e0f5447d9d667","venue_1":"ACM Trans. Graph.","year":"2010","title":"Example-based facial rigging","authors":"Hao Li, Thibaut Weise, Mark Pauly","author_ids":"1706574, 2246174, 1741645","abstract":"We introduce a method for generating facial blendshape rigs from a set of example poses of a CG character. Our system transfers controller semantics and expression dynamics from a generic template to the target blendshape model, while solving for an optimal reproduction of the training poses. This enables a scalable design process, where the user can iteratively add more training poses to refine the blendshape expression space. However, plausible animations can be obtained even with a single training pose. We show how formulating the optimization in gradient space yields superior results as compared to a direct optimization on blendshape vertices. We provide examples for both hand-crafted characters and 3D scans of a real actor and demonstrate the performance of our system in the context of markerless art-directable facial tracking.","cites":"54","conferencePercentile":"88.01169591"},{"venue":"ACM Trans. Graph.","id":"1b60b7e15ffb72b91df5238a70220ab1d696cc94","venue_1":"ACM Trans. Graph.","year":"2005","title":"A practical analytic single scattering model for real time rendering","authors":"Bo Sun, Ravi Ramamoorthi, Srinivasa G. Narasimhan, Shree K. Nayar","author_ids":"1714316, 1752236, 1779052, 1750470","abstract":"We consider real-time rendering of scenes in participating media, capturing the effects of light scattering in fog, mist and haze. While a number of sophisticated approaches based on Monte Carlo and finite element simulation have been developed, those methods do not work at interactive rates. The most common real-time methods are essentially simple variants of the OpenGL fog model. While easy to use and specify, that model excludes many important qualitative effects like glows around light sources, the impact of volumetric scattering on the appearance of surfaces such as the diffusing of glossy highlights, and the appearance under complex lighting such as environment maps. In this paper, we present an alternative physically based approach that captures these effects while maintaining real time performance and the ease-of-use of the OpenGL fog model. Our method is based on an explicit analytic integration of the single scattering light transport equations for an isotropic point light source in a homogeneous participating medium. We can implement the model in modern programmable graphics hardware using a few small numerical lookup tables stored as texture maps. Our model can also be easily adapted to generate the appearances of materials with arbitrary BRDFs, environment map lighting, and precomputed radiance transfer methods, in the presence of participating media. Hence, our techniques can be widely used in real-time rendering.","cites":"71","conferencePercentile":"48.38709677"},{"venue":"ACM Trans. Graph.","id":"177e876c9e0f1ade353b4744dcee7754d6411837","venue_1":"ACM Trans. Graph.","year":"2011","title":"Realtime performance-based facial animation","authors":"Thibaut Weise, Sofien Bouaziz, Hao Li, Mark Pauly","author_ids":"2246174, 1755579, 1706574, 1741645","abstract":"This paper presents a system for performance-based character animation that enables any user to control the facial expressions of a digital avatar in realtime. The user is recorded in a natural environment using a non-intrusive, commercially available 3D sensor. The simplicity of this acquisition device comes at the cost of high noise levels in the acquired data. To effectively map low-quality 2D images and 3D depth maps to realistic facial expressions, we introduce a novel face tracking algorithm that combines geometry and texture registration with pre-recorded animation priors in a single optimization. Formulated as a maximum a posteriori estimation in a reduced parameter space, our method implicitly exploits temporal coherence to stabilize the tracking. We demonstrate that compelling 3D facial dynamics can be reconstructed in realtime without the use of face markers, intrusive lighting, or complex scanning hardware. This makes our system easy to deploy and facilitates a range of new applications, e.g. in digital gameplay or social interactions.","cites":"170","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"7561c82b3c1574fa162e263255e696f796f22bc5","venue_1":"ACM Trans. Graph.","year":"2013","title":"Online modeling for realtime facial animation","authors":"Sofien Bouaziz, Yangang Wang, Mark Pauly","author_ids":"1755579, 4528986, 1741645","abstract":"We present a new algorithm for realtime face tracking on commodity RGB-D sensing devices. Our method requires no user-specific training or calibration, or any other form of manual assistance, thus enabling a range of new applications in performance-based facial animation and virtual interaction at the consumer level. The key novelty of our approach is an optimization algorithm that jointly solves for a detailed 3D expression model of the user and the corresponding dynamic tracking parameters. Realtime performance and robust computations are facilitated by a novel subspace parameterization of the dynamic facial expression space. We provide a detailed evaluation that shows that our approach significantly simplifies the performance capture workflow, while achieving accurate facial tracking for realtime applications.","cites":"48","conferencePercentile":"96.3800905"},{"venue":"ACM Trans. Graph.","id":"48875a1db07eddb65f86f69099712381d859a91a","venue_1":"ACM Trans. Graph.","year":"2005","title":"Removing photography artifacts using gradient projection and flash-exposure sampling","authors":"Amit K. Agrawal, Ramesh Raskar, Shree K. Nayar, Yuanzhen Li","author_ids":"1985085, 1717566, 1750470, 2549090","abstract":"Flash images are known to suffer from several problems: saturation of nearby objects, poor illumination of distant objects, reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces. We propose to use a flash and no-flash (ambient) image pair to produce better flash images. We present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images. We also present a brightness-ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth. In several practical scenarios, the quality of flash/no-flash images may be limited in terms of dynamic range. In such cases, we advocate using several images taken under different flash intensities and exposures. We analyze the flash intensity-exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene. We present several experimental results that demonstrate the ability of our algorithms to produce improved flash images.","cites":"87","conferencePercentile":"59.27419355"},{"venue":"ACM Trans. Graph.","id":"68f9a8f14acdb11c24bbf8d2e36f829953c1e6c1","venue_1":"ACM Trans. Graph.","year":"2006","title":"Acquiring scattering properties of participating media by dilution","authors":"Srinivasa G. Narasimhan, Mohit Gupta, Craig Donner, Ravi Ramamoorthi, Shree K. Nayar, Henrik Wann Jensen","author_ids":"1779052, 2122821, 2446336, 1752236, 1750470, 1730025","abstract":"The visual world around us displays a rich set of volumetric effects due to participating media. The appearance of these media is governed by several physical properties such as particle densities, shapes and sizes, which must be input (directly or indirectly) to a rendering algorithm to generate realistic images. While there has been significant progress in developing rendering techniques (for instance, volumetric Monte Carlo methods and analytic approximations), there are very few methods that measure or estimate these properties for media that are of relevance to computer graphics. In this paper, we present a simple device and technique for robustly estimating the properties of a broad class of participating media that can be either (a) diluted in water such as juices, beverages, paints and cleaning supplies, or (b) dissolved in water such as powders and sugar/salt crystals, or (c) suspended in water such as impurities. The key idea is to dilute the concentrations of the media so that single scattering effects dominate and multiple scattering becomes negligible, leading to a simple and robust estimation algorithm. Furthermore, unlike previous approaches that require complicated or separate measurement setups for different types or properties of media, our method and setup can be used to measure media with a complete range of absorption and scattering properties from a single HDR photograph. Once the parameters of the diluted medium are estimated, a volumetric Monte Carlo technique may be used to create renderings of any medium concentration and with multiple scattering. We have measured the scattering parameters of forty commonly found materials, that can be immediately used by the computer graphics community. We can also create realistic images of combinations or mixtures of the original measured materials, thus giving the user a wide flexibility in making realistic images of participating media.","cites":"45","conferencePercentile":"34.25925926"},{"venue":"ACM Trans. Graph.","id":"10fa1232eeca50223c592cf7905ed43dd58829f8","venue_1":"ACM Trans. Graph.","year":"2014","title":"Poisson-Based Continuous Surface Generation for Goal-Based Caustics","authors":"Yonghao Yue, Kei Iwasaki, Bing-Yu Chen, Yoshinori Dobashi, Tomoyuki Nishita","author_ids":"2276806, 1787808, 1733344, 1791538, 1696605","abstract":"We present a technique for computing the shape of a transparent object that can generate user-defined caustic patterns. The surface of the object generated using our method is smooth. Thanks to this property, the resulting caustic pattern is smooth, natural, and highly detailed compared to the results btained using previous methods. Our method consists of two processes. First, we use a differential geometry approach to compute a smooth mapping between the distributions of the incident light and the light reaching the screen. Second, we utilize this mapping to compute the surface of the object. We solve Poisson's equation to compute both the mapping and the surface of the object.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"c5380569c8d3e6f8ab6bf5f9b10ec136e212fd9d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Coupled structure-from-motion and 3D symmetry detection for urban facades","authors":"Duygu Ceylan, Niloy J. Mitra, Youyi Zheng, Mark Pauly","author_ids":"5350989, 1710455, 3049304, 1741645","abstract":"Repeated structures are ubiquitous in urban facades. Such repetitions lead to ambiguity in establishing correspondences across sets of unordered images. A decoupled structure-from-motion reconstruction followed by symmetry detection often produces errors: outputs are either noisy and incomplete, or even worse, appear to be valid but actually have a wrong number of repeated elements. We present an optimization framework for extracting repeated elements in images of urban facades, while simultaneously calibrating the input images and recovering the 3D scene geometry using a graph-based global analysis. We evaluate the robustness of the proposed scheme on a range of challenging examples containing widespread repetitions and nondistinctive features. These image sets are common but cannot be handled well with state-of-the-art methods. We show that the recovered symmetry information along with the 3D geometry enables a range of novel image editing operations that maintain consistency across the images.","cites":"10","conferencePercentile":"61.72839506"},{"venue":"ACM Trans. Graph.","id":"182be56948d4e56df01cf84a30fdab7581baa477","venue_1":"ACM Trans. Graph.","year":"2006","title":"Multiview radial catadioptric imaging for scene capture","authors":"Sujit Kuthirummal, Shree K. Nayar","author_ids":"3136389, 1750470","abstract":"In this paper, we present a class of imaging systems, called <i>radial imaging systems</i>, that capture a scene from a large number of view-points within a single image, using a camera and a curved mirror. These systems can recover scene properties such as geometry, reflectance, and texture. We derive analytic expressions that describe the properties of a complete family of radial imaging systems, including their loci of viewpoints, fields of view, and resolution characteristics. We have built radial imaging systems that, from a single image, recover the frontal 3D structure of an object, generate the complete texture map of a convex object, and estimate the parameters of an analytic BRDF model for an isotropic material. In addition, one of our systems can recover the complete geometry of a convex object by capturing only two images. These results show that radial imaging systems are simple, effective, and convenient devices for a wide range of applications in computer graphics and computer vision.","cites":"45","conferencePercentile":"34.25925926"},{"venue":"ACM Trans. Graph.","id":"41b4934be456e0c3687aad759edba200cd2f7b21","venue_1":"ACM Trans. Graph.","year":"2014","title":"A similarity measure for illustration style","authors":"Elena Garces, Aseem Agarwala, Diego Gutierrez, Aaron Hertzmann","author_ids":"1733662, 1696487, 1723695, 1747779","abstract":"This paper presents a method for measuring the similarity in style between two pieces of vector art, independent of content. Similarity is measured by the differences between four types of features: color, shading, texture, and stroke. Feature weightings are learned from crowdsourced experiments. This perceptual similarity enables style-based search. Using our style-based search feature, we demonstrate an application that allows users to create stylistically-coherent clip art mash-ups.","cites":"14","conferencePercentile":"79.218107"},{"venue":"ACM Trans. Graph.","id":"4b3cc6be9aa41e1f0a1fcc6f4b0232d75159d913","venue_1":"ACM Trans. Graph.","year":"2010","title":"Computational rephotography","authors":"Soonmin Bae, Aseem Agarwala, Frédo Durand","author_ids":"3107120, 1696487, 1728125","abstract":"Rephotographers aim to recapture an existing photograph from the same viewpoint. A historical photograph paired with a well-aligned modern rephotograph can serve as a remarkable visualization of the passage of time. However, the task of rephotography is tedious and often imprecise, because reproducing the viewpoint of the original photograph is challenging. The rephotographer must disambiguate between the six degrees of freedom of 3D translation and rotation, and the confounding similarity between the effects of camera zoom and dolly.\n We present a real-time estimation and visualization technique for rephotography that helps users reach a desired viewpoint during capture. The input to our technique is a reference image taken from the desired viewpoint. The user moves through the scene with a camera and follows our visualization to reach the desired viewpoint. We employ computer vision techniques to compute the relative viewpoint difference. We guide 3D movement using two 2D arrows. We demonstrate the success of our technique by rephotographing historical images and conducting user studies.","cites":"13","conferencePercentile":"18.71345029"},{"venue":"ACM Trans. Graph.","id":"9e91832b5fb1c66fe2820550e9c39f4eab74151f","venue_1":"ACM Trans. Graph.","year":"2014","title":"High-contrast computational caustic design","authors":"Yuliy Schwartzburg, Romain Testuz, Andrea Tagliasacchi, Mark Pauly","author_ids":"2984683, 2124668, 1796480, 1741645","abstract":"We present a new algorithm for computational caustic design. Our algorithm solves for the shape of a transparent object such that the refracted light paints a desired caustic image on a receiver screen. We introduce an optimal transport formulation to establish a correspondence between the input geometry and the unknown target shape. A subsequent 3D optimization based on an adaptive discretization scheme then finds the target surface from the correspondence map. Our approach supports <i>piecewise smooth</i> surfaces and non-bijective mappings, which eliminates a number of shortcomings of previous methods. This leads to a significantly richer space of caustic images, including smooth transitions, singularities of infinite light density, and completely black areas. We demonstrate the effectiveness of our approach with several simulated and fabricated examples.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"56222cdc82f16d6acdc34b456b3f12f82917fee7","venue_1":"ACM Trans. Graph.","year":"2010","title":"Paneling architectural freeform surfaces","authors":"Michael Eigensatz, Martin Kilian, Alexander Schiftner, Niloy J. Mitra, Helmut Pottmann, Mark Pauly","author_ids":"2383473, 1837115, 3053682, 1710455, 1734949, 1741645","abstract":"The emergence of large-scale freeform shapes in architecture poses big challenges to the fabrication of such structures. A key problem is the approximation of the design surface by a union of patches, so-called panels, that can be manufactured with a selected technology at reasonable cost, while meeting the design intent and achieving the desired aesthetic quality of panel layout and surface smoothness. The production of curved panels is mostly based on molds. Since the cost of mold fabrication often dominates the panel cost, there is strong incentive to use the same mold for multiple panels. We cast the major practical requirements for architectural surface paneling, including mold reuse, into a global optimization framework that interleaves discrete and continuous optimization steps to minimize production cost while meeting user-specified quality constraints. The search space for optimization is mainly generated through controlled deviation from the design surface and tolerances on positional and normal continuity between neighboring panels. A novel 6-dimensional metric space allows us to quickly compute approximate inter-panel distances, which dramatically improves the performance of the optimization and enables the handling of complex arrangements with thousands of panels. The practical relevance of our system is demonstrated by paneling solutions for real, cutting-edge architectural freeform design projects.","cites":"39","conferencePercentile":"67.54385965"},{"venue":"ACM Trans. Graph.","id":"1e0bf0fbefbb0404c3c18740123a969091c67ef4","venue_1":"ACM Trans. Graph.","year":"1999","title":"Reflectance and Texture of Real-World Surfaces","authors":"Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, Jan J. Koenderink","author_ids":"1710772, 8038506, 1750470, 1716904","abstract":"In this work, we investigate the visual appearance of real-world surfaces and the dependence of appearance on the geometry of imaging conditions. We discuss a new texture representation called the BTF (bidirectional texture function) which captures the variation in texture with illumination and viewing direction. We present a BTF database with image textures from over 60 different samples, each observed with over 200 different combinations of viewing and illumination directions. We describe the methods involved in collecting the database as well as the importqance and uniqueness of this database for computer graphics. A related quantity to the BTF is the familiar BRDF (bidirectional reflectance distribution function). The measurement methods involved in the BTF database are conducive  to simultaneous measurement of the BRDF. Accordingly, we also present a BRDF database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and illumination directions. Both of these unique databases are publicly available and have important implications for computer graphics.","cites":"666","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"4b1b530fe2648949329caa5ecc7d9a8868ca0c19","venue_1":"ACM Trans. Graph.","year":"2008","title":"Discovering structural regularity in 3D geometry","authors":"Mark Pauly, Niloy J. Mitra, Johannes Wallner, Helmut Pottmann, Leonidas J. Guibas","author_ids":"1741645, 1710455, 7379884, 1734949, 1744254","abstract":"We introduce a computational framework for discovering regular or repeated geometric structures in 3D shapes. We describe and classify possible regular structures and present an effective algorithm for detecting such repeated geometric patterns in point- or meshbased models. Our method assumes no prior knowledge of the geometry or spatial location of the individual elements that define the pattern. Structure discovery is made possible by a careful analysis of pairwise similarity transformations that reveals prominent lattice structures in a suitable model of transformation space. We introduce an optimization method for detecting such uniform grids specifically designed to deal with outliers and missing elements. This yields a robust algorithm that successfully discovers complex regular structures amidst clutter, noise, and missing geometry. The accuracy of the extracted generating transformations is further improved using a novel simultaneous registration method in the spatial domain. We demonstrate the effectiveness of our algorithm on a variety of examples and show applications to compression, model repair, and geometry synthesis.","cites":"161","conferencePercentile":"95.0617284"},{"venue":"ACM Trans. Graph.","id":"11ea9ed660efe13f65f82eee3b243c0cd07bb815","venue_1":"ACM Trans. Graph.","year":"2015","title":"MultiFab: a machine vision assisted platform for multi-material 3D printing","authors":"Pitchaya Sitthi-amorn, Javier E. Ramos, Yuwang Wang, Joyce Kwan, Justin T. Lan, Wenshou Wang, Wojciech Matusik","author_ids":"1936229, 2656682, 2439487, 2190853, 2187906, 1828096, 1752521","abstract":"We have developed a multi-material 3D printing platform that is high-resolution, low-cost, and extensible. The key part of our platform is an integrated machine vision system. This system allows for self-calibration of printheads, 3D scanning, and a closed-feedback loop to enable print corrections. The integration of machine vision with 3D printing simplifies the overall platform design and enables new applications such as 3D printing over auxiliary parts. Furthermore, our platform dramatically expands the range of parts that can be 3D printed by simultaneously supporting up to 10 different materials that can interact optically and mechanically. The platform achieves a resolution of at least 40 &mu;m by utilizing piezoelectric inkjet printheads adapted for 3D printing. The hardware is low cost (less than $7,000) since it is built exclusively from off-the-shelf components. The architecture is extensible and modular -- adding, removing, and exchanging printing modules can be done quickly. We provide a detailed analysis of the system's performance. We also demonstrate a variety of fabricated multi-material objects.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"602712c9cfe61f0522f0836757036b9336693d2a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Adaptively sampled particle fluids","authors":"Bart Adams, Mark Pauly, Richard Keiser, Leonidas J. Guibas","author_ids":"1700388, 1741645, 3161356, 1744254","abstract":"We present novel adaptive sampling algorithms for particle-based fluid simulation. We introduce a sampling condition based on geometric local feature size that allows focusing computational resources in geometrically complex regions, while reducing the number of particles deep inside the fluid or near thick flat surfaces. Further performance gains are achieved by varying the sampling density according to visual importance. In addition, we propose a novel fluid surface definition based on approximate particle-to-surface distances that are carried along with the particles and updated appropriately. The resulting surface reconstruction method has several advantages over existing methods, including stability under particle resampling and suitability for representing smooth flat surfaces. We demonstrate how our adaptive sampling and distance-based surface reconstruction algorithms lead to significant improvements in time and memory as compared to single resolution particle simulations, without significantly affecting the fluid flow behavior.","cites":"147","conferencePercentile":"90.8"},{"venue":"ACM Trans. Graph.","id":"3068e30cafa39fcff707347dee1076158eea6ab0","venue_1":"ACM Trans. Graph.","year":"2008","title":"Green Coordinates","authors":"Yaron Lipman, David Levin, Daniel Cohen-Or","author_ids":"3232072, 1689997, 1701009","abstract":"We introduce Green Coordinates for closed polyhedral cages. The coordinates are motivated by Green's third integral identity and respect both the vertices position and faces orientation of the cage. We show that Green Coordinates lead to space deformations with a shape-preserving property. In particular, in 2D they induce conformal mappings, and extend naturally to quasi-conformal mappings in 3D. In both cases we derive closed-form expressions for the coordinates, yielding a simple and fast algorithm for cage-based space deformation. We compare the performance of Green Coordinates with those of Mean Value Coordinates and Harmonic Coordinates and show that the advantage of the shape-preserving property is not achieved at the expense of speed or simplicity. We also show that the new coordinates extend the mapping in a natural analytic manner to the exterior of the cage, allowing the employment of partial cages.","cites":"88","conferencePercentile":"86.11111111"},{"venue":"ACM Trans. Graph.","id":"09bb755693db10ff3e7247c6c2499451027c7d04","venue_1":"ACM Trans. Graph.","year":"2008","title":"Imperfect shadow maps for efficient computation of indirect illumination","authors":"Tobias Ritschel, Thorsten Grosch, Min H. Kim, Hans-Peter Seidel, Carsten Dachsbacher, Jan Kautz","author_ids":"1759347, 8221420, 7591291, 1746884, 1705803, 1690538","abstract":"We present a method for interactive computation of indirect illumination in large and fully dynamic scenes based on approximate visibility queries. While the high-frequency nature of direct lighting requires accurate visibility, indirect illumination mostly consists of smooth gradations, which tend to mask errors due to incorrect visibility. We exploit this by approximating visibility for indirect illumination with <i>imperfect shadow maps</i>---low-resolution shadow maps rendered from a crude point-based representation of the scene. These are used in conjunction with a global illumination algorithm based on virtual point lights enabling indirect illumination of dynamic scenes at real-time frame rates. We demonstrate that imperfect shadow maps are a valid approximation to visibility, which makes the simulation of global illumination an order of magnitude faster than using accurate visibility.","cites":"116","conferencePercentile":"93.82716049"},{"venue":"ACM Trans. Graph.","id":"24295f6dfa2f2d1b32faf3548f49e86539af1d12","venue_1":"ACM Trans. Graph.","year":"2006","title":"Color harmonization","authors":"Daniel Cohen-Or, Olga Sorkine-Hornung, Ran Gal, Tommer Leyvand, Ying-Qing Xu","author_ids":"1701009, 2250001, 2835543, 3316156, 1742571","abstract":"Harmonic colors are sets of colors that are aesthetically pleasing in terms of human visual perception. In this paper, we present a method that enhances the harmony among the colors of a given photograph or of a general image, while remaining faithful, as much as possible, to the original colors. Given a color image, our method finds the best harmonic scheme for the image colors. It then allows a graceful shifting of hue values so as to fit the harmonic scheme while considering spatial coherence among colors of neighboring pixels using an optimization technique. The results demonstrate that our method is capable of automatically enhancing the color \"look-and-feel\" of an ordinary image. In particular, we show the results of harmonizing the background image to accommodate the colors of a foreground image, or the foreground with respect to the background, in a cut-and-paste setting. Our color harmonization technique proves to be useful in adjusting the colors of an image composed of several parts taken from different sources.","cites":"100","conferencePercentile":"73.61111111"},{"venue":"ACM Trans. Graph.","id":"3cd1c2735560a38d0bb7f86c0450b40553772ab4","venue_1":"ACM Trans. Graph.","year":"2008","title":"Interactive relighting of dynamic refractive objects","authors":"Xin Sun, Kun Zhou, Eric J. Stollnitz, Jiaoying Shi, Baining Guo","author_ids":"1788730, 6671887, 3011420, 1700893, 2738456","abstract":"We present a new technique for interactive relighting of dynamic refractive objects with complex material properties. We describe our technique in terms of a rendering pipeline in which each stage runs entirely on the GPU. The rendering pipeline converts surfaces to volumetric data, traces the curved paths of photons as they refract through the volume, and renders arbitrary views of the resulting radiance distribution. Our rendering pipeline is fast enough to permit interactive updates to lighting, materials, geometry, and viewing parameters without any precomputation. Applications of our technique include the visualization of caustics, absorption, and scattering while running physical simulations or while manipulating surfaces in real time.","cites":"36","conferencePercentile":"41.66666667"},{"venue":"ACM Trans. Graph.","id":"6c83a3901b8d7205c88901d50fc1aa59ca1691af","venue_1":"ACM Trans. Graph.","year":"2004","title":"Video tooning","authors":"Jue Wang, Ying-Qing Xu, Harry Shum, Michael F. Cohen","author_ids":"1718812, 1742571, 1698102, 1694613","abstract":"We describe a system for transforming an input video into a highly abstracted, spatio-temporally coherent cartoon animation with a range of styles. To achieve this, we treat video as a space-time volume of image data. We have developed an anisotropic kernel mean shift technique to segment the video data into contiguous volumes. These provide a simple cartoon style in themselves, but more importantly provide the capability to semi-automatically rotoscope semantically meaningful regions.In our system, the user simply outlines objects on keyframes. A mean shift guided interpolation algorithm is then employed to create three dimensional semantic regions by interpolation between the keyframes, while maintaining smooth trajectories along the time dimension. These regions provide the basis for creating smooth two dimensional edge sheets and stroke sheets embedded within the spatio-temporal video volume. The regions, edge sheets, and stroke sheets are rendered by slicing them at particular times. A variety of styles of rendering are shown. The temporal coherence provided by the smoothed semantic regions and sheets results in a temporally consistent non-photorealistic appearance.","cites":"70","conferencePercentile":"32.60869565"},{"venue":"ACM Trans. Graph.","id":"c29dcd65845c76e583a72eba1e98410ffb5f934a","venue_1":"ACM Trans. Graph.","year":"2009","title":"Modeling human color perception under extended luminance levels","authors":"Min H. Kim, Tim Weyrich, Jan Kautz","author_ids":"7591291, 1784306, 1690538","abstract":"Display technology is advancing quickly with peak luminance increasing significantly, enabling high-dynamic-range displays. However, perceptual color appearance under extended luminance levels has not been studied, mainly due to the unavailability of psychophysical data. Therefore, we conduct a psychophysical study in order to acquire appearance data for many different luminance levels (up to 16,860 cd/m<sup>2</sup>) covering most of the dynamic range of the human visual system. These experimental data allow us to quantify human color perception under extended luminance levels, yielding a generalized color appearance model. Our proposed appearance model is efficient, accurate and invertible. It can be used to adapt the tone and color of images to different dynamic ranges for cross-media reproduction while maintaining appearance that is close to human perception.","cites":"20","conferencePercentile":"27.62430939"},{"venue":"ACM Trans. Graph.","id":"2ba316f2d81333b36c8abcd2ff5b90d39f822e82","venue_1":"ACM Trans. Graph.","year":"2002","title":"Modeling and rendering of realistic feathers","authors":"Yanyun Chen, Ying-Qing Xu, Baining Guo, Harry Shum","author_ids":"7377177, 1742571, 2738456, 1698102","abstract":"We present techniques for realistic modeling and rendering of feathers and birds. Our approach is motivated by the observation that a feather is a branching structure that can be described by an L-system. The parametric L-system we derived allows the user to easily create feathers of different types and shapes by changing a few parameters. The randomness in feather geometry is also incorporated into this L-system. To render a feather realistically, we have derived an efficient form of the bidirectional texture function (BTF), which describes the small but visible geometry details on the feather blade. A rendering algorithm combining the L-system and the BTF displays feathers photorealistically while capitalizing on graphics hardware for efficiency. Based on this framework of feather modeling and rendering, we developed a system that can automatically generate appropriate feathers to cover different parts of a bird's body from a few \"key feathers\" supplied by the user, and produce realistic renderings of the bird.","cites":"16","conferencePercentile":"8"},{"venue":"ACM Trans. Graph.","id":"314e5dace3827eb35ff01e097e47df19c69ba0f4","venue_1":"ACM Trans. Graph.","year":"2012","title":"Smooth skinning decomposition with rigid bones","authors":"Binh Huy Le, Zhigang Deng","author_ids":"2664191, 7458303","abstract":"This paper introduces the Smooth Skinning Decomposition with Rigid Bones (SSDR), an automated algorithm to extract the linear blend skinning (LBS) from a set of example poses. The SSDR model can effectively approximate the skin deformation of nearly articulated models as well as highly deformable models by a low number of rigid bones and a sparse, convex bone-vertex weight map. Formulated as a constrained optimization problem where the least squared error of the reconstructed vertices by LBS is minimized, the SSDR model can be solved by a block coordinate descent-based algorithm to iteratively update the weight map and the bone transformations. By employing the sparseness and convex constraints on the weight map, the SSDR model can be used for traditional skinning decomposition tasks such as animation compression and hardware-accelerated rendering. Moreover, by imposing the orthogonal constraints on the bone rotation matrices (rigid bones), the SSDR model can also be applied in motion editing, skeleton extraction, and collision detection tasks. Through qualitative and quantitative evaluations, we show the SSDR model can measurably outperform the state-of-the-art skinning decomposition schemes in terms of accuracy and applicability.","cites":"17","conferencePercentile":"49.24242424"},{"venue":"ACM Trans. Graph.","id":"2a928d5272e77b19c89a57ee6b1c7eba2e96b4bb","venue_1":"ACM Trans. Graph.","year":"2004","title":"Protected interactive 3D graphics via remote rendering","authors":"David Koller, Michael Turitzin, Marc Levoy, Marco Tarini, Giuseppe Croccia, Paolo Cignoni, Roberto Scopigno","author_ids":"2610618, 2548283, 1801789, 3153457, 2972701, 1738697, 7980724","abstract":"Valuable 3D graphical models, such as high-resolution digital scans of cultural heritage objects, may require protection to prevent piracy or misuse, while still allowing for interactive display and manipulation by a widespread audience. We have investigated techniques for protecting 3D graphics content, and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction. The system consists of a 3D viewer client that includes low-resolution versions of the 3D models, and a rendering server that renders and returns images of high-resolution models according to client requests. The server implements a number of defenses to guard against 3D reconstruction attacks, such as monitoring and limiting request streams, and slightly perturbing and distorting the rendered images. We consider several possible types of reconstruction attacks on such a rendering server, and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users.","cites":"60","conferencePercentile":"29.89130435"},{"venue":"ACM Trans. Graph.","id":"a0cc2fddea42f87e4ec01e6704e4c1cbf155cb38","venue_1":"ACM Trans. Graph.","year":"2004","title":"Adaptive tetrapuzzles: efficient out-of-core construction and visualization of gigantic multiresolution polygonal models","authors":"Paolo Cignoni, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, Federico Ponchio, Roberto Scopigno","author_ids":"1738697, 1731043, 1708999, 1685719, 1729580, 7980724","abstract":"We describe an efficient technique for out-of-core construction and accurate view-dependent visualization of very large surface models. The method uses a regular conformal hierarchy of tetrahedra to spatially partition the model. Each tetrahedral cell contains a precomputed simplified version of the original model, represented using cache coherent indexed strips for fast rendering. The representation is constructed during a fine-to-coarse simplification of the surface contained in diamonds (sets of tetrahedral cells sharing their longest edge). The construction preprocess operates out-of-core and parallelizes nicely. Appropriate boundary constraints are introduced in the simplification to ensure that all conforming selective subdivisions of the tetrahedron hierarchy lead to correctly matching surface patches. For each frame at runtime, the hierarchy is traversed coarse-to-fine to select diamonds of the appropriate resolution given the view parameters. The resulting system can interatively render high quality views of out-of-core models of hundreds of millions of triangles at over 40Hz (or 70M triangles/s) on current commodity graphics platforms.","cites":"123","conferencePercentile":"58.69565217"},{"venue":"ACM Trans. Graph.","id":"53a0d0a10946e527ef9cdd094a73f4b7e58b5817","venue_1":"ACM Trans. Graph.","year":"2014","title":"Field-aligned mesh joinery","authors":"Paolo Cignoni, Nico Pietroni, Luigi Malomo, Roberto Scopigno","author_ids":"1738697, 3283805, 2908942, 7980724","abstract":"Mesh joinery is an innovative method to produce illustrative shape approximations suitable for fabrication. Mesh joinery is capable of producing complex fabricable structures in an efficient and visually pleasing manner. We represent an input geometry as a set of planar pieces arranged to compose a rigid structure, by exploiting an efficient slit mechanism. Since slices are planar, to fabricate them a standard 2D cutting system is enough.\n We automatically arrange slices according to a smooth cross-field defined over the surface. Cross-fields allow representing global features that characterize the appearance of the shape. Slice placement conforms to specific manufacturing constraints.","cites":"15","conferencePercentile":"81.48148148"},{"venue":"ACM Trans. Graph.","id":"1475d2d2dcb3017a928a6b4f8865c51665ff8ce9","venue_1":"ACM Trans. Graph.","year":"2013","title":"Two-layer sparse compression of dense-weight blend skinning","authors":"Binh Huy Le, Zhigang Deng","author_ids":"2664191, 7458303","abstract":"Weighted linear interpolation has been widely used in many skinning techniques including linear blend skinning, dual quaternion blend skinning, and cage based deformation. To speed up performance, these skinning models typically employ a sparseness constraint, in which each 3D model vertex has a small fixed number of non-zero weights. However, the sparseness constraint also imposes certain limitations to skinning models and their various applications. This paper introduces an efficient two-layer sparse compression technique to substantially reduce the computational cost of a dense-weight skinning model, with insignificant loss of its visual quality. It can directly work on dense skinning weights or use example-based skinning decomposition to further improve its accuracy. Experiments and comparisons demonstrate that the introduced sparse compression model can significantly outperform state of the art weight reduction algorithms, as well as skinning decomposition algorithms with a sparseness constraint.","cites":"4","conferencePercentile":"9.049773756"},{"venue":"ACM Trans. Graph.","id":"0e444674e6c6046c1c7f01b520bcc34fabe2be62","venue_1":"ACM Trans. Graph.","year":"2001","title":"Real-time texture synthesis by patch-based sampling","authors":"Lin Liang, Ce Liu, Ying-Qing Xu, Baining Guo, Harry Shum","author_ids":"1680293, 1681442, 1742571, 2738456, 1698102","abstract":"We present an algorithm for synthesizing textures from an input sample. This patch-based sampling algorithm is fast and it makes high-quality texture synthesis a real-time process. For generating textures of the same size and comparable quality, patch-based sampling is orders of magnitude faster than existing algorithms. The patch-based sampling algorithm works well for a wide variety of textures ranging from regular to stochastic. By sampling patches according to a nonparametric estimation of the local conditional MRF density function, we avoid mismatching features across patch boundaries. We also experimented with documented cases for which pixel-based nonparametric sampling algorithms cease to be effective but our algorithm continues to work well.","cites":"279","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"24493557fe75018dbb7124dea6a58a250ee02902","venue_1":"ACM Trans. Graph.","year":"2006","title":"Partial and approximate symmetry detection for 3D geometry","authors":"Niloy J. Mitra, Leonidas J. Guibas, Mark Pauly","author_ids":"1710455, 1744254, 1741645","abstract":"\"Symmetry is a complexity-reducing concept [...]; seek it every-where.\" - Alan J. PerlisMany natural and man-made objects exhibit significant symmetries or contain repeated substructures. This paper presents a new algorithm that processes geometric models and efficiently discovers and extracts a compact representation of their Euclidean symmetries. These symmetries can be partial, approximate, or both. The method is based on matching simple local shape signatures in pairs and using these matches to accumulate evidence for symmetries in an appropriate transformation space. A clustering stage extracts potential significant symmetries of the object, followed by a verification step. Based on a statistical sampling analysis, we provide theoretical guarantees on the success rate of our algorithm. The extracted symmetry graph representation captures important high-level information about the structure of a geometric model which in turn enables a large set of further processing operations, including shape compression, segmentation, consistent editing, symmetrization, indexing for retrieval, etc.","cites":"217","conferencePercentile":"95.83333333"},{"venue":"ACM Trans. Graph.","id":"2d91a1903253f6c014419cd31883778a9a94d5d0","venue_1":"ACM Trans. Graph.","year":"2003","title":"VisionWand: interaction techniques for large displays using a passive wand tracked in 3D","authors":"Xiang Cao, Ravin Balakrishnan","author_ids":"7299595, 1748870","abstract":"A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques and visual widgets that exploit the affordances of the wand, resulting in an effective interface for large scale display interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of gestures and postures to track and enable command input.","cites":"78","conferencePercentile":"40.32258065"},{"venue":"ACM Trans. Graph.","id":"5d51f86d6960955f27f550e141356c3ca341bf78","venue_1":"ACM Trans. Graph.","year":"2007","title":"Do HDR displays support LDR content?: a psychophysical evaluation","authors":"Ahmet Oguz Akyüz, Roland W. Fleming, Bernhard E. Riecke, Erik Reinhard, Heinrich H. Bülthoff","author_ids":"1746741, 2436224, 1680478, 1790581, 1747836","abstract":"The development of high dynamic range (HDR) imagery has brought us to the verge of arguably the largest change in image display technologies since the transition from black-and-white to color television. Novel capture and display hardware will soon enable consumers to enjoy the HDR experience in their own homes. The question remains, however, of what to do with existing images and movies, which are intrinsically low dynamic range (LDR). Can this enormous volume of legacy content also be displayed effectively on HDR displays? We have carried out a series of rigorous psychophysical investigations to determine how LDR images are best displayed on a state-of-the-art HDR monitor, and to identify which stages of the HDR imaging pipeline are perceptually most critical. Our main findings are: (1) As expected, HDR displays outperform LDR ones. (2) Surprisingly, HDR images that are tone-mapped for display on standard monitors are often no better than the best single LDR exposure from a bracketed sequence. (3) Most importantly of all, LDR data does not necessarily require sophisticated treatment to produce a compelling HDR experience. Simply boosting the range of an LDR image linearly to fit the HDR display can equal or even surpass the appearance of a true HDR image. Thus the potentially tricky process of <i>inverse tone mapping</i> can be largely circumvented.","cites":"49","conferencePercentile":"50"},{"venue":"ACM Trans. Graph.","id":"3c87fd799675b3879bedc7a7edcc73f9c7cab3f3","venue_1":"ACM Trans. Graph.","year":"2005","title":"Meshless animation of fracturing solids","authors":"Mark Pauly, Richard Keiser, Bart Adams, Philip Dutré, Markus H. Gross, Leonidas J. Guibas","author_ids":"1741645, 3161356, 1700388, 1699581, 1743207, 1744254","abstract":"We present a new meshless animation framework for elastic and plastic materials that fracture. Central to our method is a highly dynamic surface and volume sampling method that supports arbitrary crack initiation, propagation, and termination, while avoiding many of the stability problems of traditional mesh-based techniques. We explicitly model advancing crack fronts and associated fracture surfaces embedded in the simulation volume. When cutting through the material, crack fronts directly affect the coupling between simulation nodes, requiring a dynamic adaptation of the nodal shape functions. We show how local visibility tests and dynamic caching lead to an efficient implementation of these effects based on point collocation. Complex fracture patterns of interacting and branching cracks are handled using a small set of topological operations for splitting, merging, and terminating crack fronts. This allows continuous propagation of cracks with highly detailed fracture surfaces, independent of the spatial resolution of the simulation nodes, and provides effective mechanisms for controlling fracture paths. We demonstrate our method for a wide range of materials, from stiff elastic to highly plastic objects that exhibit brittle and/or ductile fracture.","cites":"120","conferencePercentile":"67.74193548"},{"venue":"ACM Trans. Graph.","id":"1a0d0cf030550bb6f01e0d696ae06c3e34dc8632","venue_1":"ACM Trans. Graph.","year":"2007","title":"Apparent ridges for line drawing","authors":"Tilke Judd, Frédo Durand, Edward H. Adelson","author_ids":"2356732, 1728125, 1788148","abstract":"Three-dimensional shape can be drawn using a variety of feature lines, but none of the current definitions alone seem to capture all visually-relevant lines. We introduce a new definition of feature lines based on two perceptual observations. First, human perception is sensitive to the variation of shading, and since shape perception is little affected by lighting and reflectance modification, we should focus on normal variation. Second, view-dependent lines better convey smooth surfaces. From this we define <i>view-dependent curvature</i> as the variation of the surface normal with respect to a viewing screen plane, and <i>apparent ridges</i> as the loci of points that maximize a view-dependent curvature. We present a formal definition of apparent ridges and an algorithm to render line drawings of 3D meshes. We show that our apparent ridges encompass or enhance aspects of several other feature lines.","cites":"110","conferencePercentile":"82.8"},{"venue":"ACM Trans. Graph.","id":"28d9dcc0387fdc28907e739854e2ed9263c73ccf","venue_1":"ACM Trans. Graph.","year":"2007","title":"Embedded deformation for shape manipulation","authors":"Robert W. Sumner, Johannes Schmid, Mark Pauly","author_ids":"1693475, 5770941, 1741645","abstract":"We present an algorithm that generates natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios. Our method builds a space deformation represented by a collection of affine transformations organized in a graph structure. One transformation is associated with each graph node and applies a deformation to the nearby space. Positional constraints are specified on the points of an embedded object. As the user manipulates the constraints, a nonlinear minimization problem is solved to find optimal values for the affine transformations. Feature preservation is encoded directly in the objective function by measuring the deviation of each transformation from a true rotation. This algorithm addresses the problem of \"embedded deformation\" since it deforms space through direct manipulation of objects embedded within it, while preserving the embedded objects' features. We demonstrate our method by editing meshes, polygon soups, mesh animations, and animated particle systems.","cites":"139","conferencePercentile":"89.6"},{"venue":"ACM Trans. Graph.","id":"9fe7dd90a3d8579b9a55181bc86916f242b17797","venue_1":"ACM Trans. Graph.","year":"2016","title":"Spectral style transfer for human motion between independent actions","authors":"Mehmet Ersin Yümer, Niloy J. Mitra","author_ids":"2396667, 1710455","abstract":"Human motion is complex and difficult to synthesize realistically. Automatic style transfer to transform the mood or identity of a character's motion is a key technology for increasing the value of already synthesized or captured motion data. Typically, state-of-the-art methods require all independent actions observed in the input to be present in a given style database to perform realistic style transfer. We introduce a spectral style transfer method for human motion between independent actions, thereby greatly reducing the required effort and cost of creating such databases. We leverage a spectral domain representation of the human motion to formulate a spatial correspondence free approach. We extract spectral intensity representations of reference and source styles for an arbitrary action, and transfer their difference to a novel motion which may contain previously unseen actions. Building on this core method, we introduce a temporally sliding window filter to perform the same analysis locally in time for heterogeneous motion processing. This immediately allows our approach to serve as a style database enhancement technique to fill-in non-existent actions in order to increase previous style transfer method's performance. We evaluate our method both via quantitative experiments, and through administering controlled user studies with respect to previous work, where significant improvement is observed with our approach.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"32439ad9179ba85a50d93663a6f31c12c05e9179","venue_1":"ACM Trans. Graph.","year":"2014","title":"Robust and accurate skeletal rigging from mesh sequences","authors":"Binh Huy Le, Zhigang Deng","author_ids":"2664191, 7458303","abstract":"We introduce an example-based rigging approach to automatically generate linear blend skinning models with skeletal structure. Based on a set of example poses, our approach can output its skeleton, joint positions, linear blend skinning weights, and corresponding bone transformations. The output can be directly used to set up skeleton-based animation in various 3D modeling and animation software as well as game engines. Specifically, we formulate the solving of a linear blend skinning model with a skeleton as an optimization with joint constraints and weight smoothness regularization, and solve it using an iterative rigging algorithm that (i) alternatively updates skinning weights, joint locations, and bone transformations, and (ii) automatically prunes redundant bones that can be generated by an over-estimated bone initialization. Due to the automatic redundant bone pruning, our approach is more robust than existing example-based rigging approaches. Furthermore, in terms of rigging accuracy, even with a single set of parameters, our approach can soundly outperform state of the art methods on various types of experimental datasets including humans, quadrupled animals, and highly deformable models.","cites":"15","conferencePercentile":"81.48148148"},{"venue":"ACM Trans. Graph.","id":"03682e97ebfdf6f1a3ba08ada217e9df81c08c92","venue_1":"ACM Trans. Graph.","year":"2002","title":"Pointshop 3D: an interactive system for point-based surface editing","authors":"Matthias Zwicker, Mark Pauly, Oliver Knoll, Markus H. Gross","author_ids":"1796846, 1741645, 2684111, 1743207","abstract":"We present a system for interactive shape and appearance editing of 3D point-sampled geometry. By generalizing conventional 2D pixel editors, our system supports a great variety of different interaction techniques to alter shape and appearance of 3D point models, including cleaning, texturing, sculpting, carving, filtering, and resampling. One key ingredient of our framework is a novel concept for interactive point cloud parameterization allowing for distortion minimal and aliasing-free texture mapping. A second one is a dynamic, adaptive resampling method which builds upon a continuous reconstruction of the model surface and its attributes. These techniques allow us to transfer the full functionality of 2D image editing operations to the irregular 3D point setting. Our system reads, processes, and writes point-sampled models without intermediate tesselation. It is intended to complement existing low cost 3D scanners and point rendering pipelines for efficient 3D content creation.","cites":"214","conferencePercentile":"76"},{"venue":"ACM Trans. Graph.","id":"5a58d4b5fdf884a8fdf64159e2509adb9d791a7c","venue_1":"ACM Trans. Graph.","year":"2013","title":"Understanding the role of phase function in translucent appearance","authors":"Ioannis Gkioulekas, Bei Xiao, Shuang Zhao, Edward H. Adelson, Todd E. Zickler, Kavita Bala","author_ids":"2407724, 2665639, 2373908, 1788148, 1713451, 8261370","abstract":"Multiple scattering contributes critically to the characteristic translucent appearance of food, liquids, skin, and crystals; but little is known about how it is perceived by human observers. This article explores the perception of translucency by studying the image effects of variations in one factor of multiple scattering: the phase function. We consider an expanded space of phase functions created by linear combinations of Henyey-Greenstein and von Mises-Fisher lobes, and we study this physical parameter space using computational data analysis and psychophysics.\n Our study identifies a two-dimensional embedding of the physical scattering parameters in a perceptually meaningful appearance space. Through our analysis of this space, we find uniform parameterizations of its two axes by analytical expressions of moments of the phase function, and provide an intuitive characterization of the visual effects that can be achieved at different parts of it. We show that our expansion of the space of phase functions enlarges the range of achievable translucent appearance compared to traditional single-parameter phase function models. Our findings highlight the important role phase function can have in controlling translucent appearance, and provide tools for manipulating its effect in material design applications.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"2dec8923ce85884a72706e9fbd44f36938d492be","venue_1":"ACM Trans. Graph.","year":"2004","title":"Lazy snapping","authors":"Yin Li, Jian Sun, Chi-Keung Tang, Harry Shum","author_ids":"1738814, 1748508, 2546217, 1698102","abstract":"In this paper, we present <i>Lazy Snapping</i>, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment <i>easy</i>. Moreover, Lazy Snapping provides instant visual feedback, <i>snapping</i> the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop.","cites":"343","conferencePercentile":"93.47826087"},{"venue":"ACM Trans. Graph.","id":"7510dadeb0d64d8dbb8e3c0237683db6e1454e01","venue_1":"ACM Trans. Graph.","year":"2004","title":"Pop-up light field: An interactive image-based modeling and rendering system","authors":"Harry Shum, Jian Sun, Shuntaro Yamazaki, Yin Li, Chi-Keung Tang","author_ids":"1698102, 1748508, 2200826, 1738814, 2546217","abstract":"In this article, we present an image-based modeling and rendering system, which we call <i>pop-up light field</i>, that models a sparse light field using a set of <i>coherent layers</i>. In our system, the user specifies how many coherent layers should be modeled or popped up according to the scene complexity. A coherent layer is defined as a collection of corresponding planar regions in the light field images. A coherent layer can be rendered free of aliasing all by itself, or against other background layers. To construct coherent layers, we introduce a Bayesian approach, <i>coherence matting</i>, to estimate alpha matting around segmented layer boundaries by incorporating a coherence prior in order to maintain coherence across images.We have developed an intuitive and easy-to-use user interface (UI) to facilitate pop-up light field construction. The key to our UI is the concept of human-in-the-loop where the user specifies where aliasing occurs in the rendered image. The user input is reflected in the input light field images where pop-up layers can be modified. The user feedback is instant through a hardware-accelerated real-time pop-up light field renderer. Experimental results demonstrate that our system is capable of rendering anti-aliased novel views from a sparse light field.","cites":"45","conferencePercentile":"22.82608696"},{"venue":"ACM Trans. Graph.","id":"0b0f1a779ba69db6fd85cb4ce26afa84cfc09b0e","venue_1":"ACM Trans. Graph.","year":"2005","title":"Video object cut and paste","authors":"Yin Li, Jian Sun, Harry Shum","author_ids":"1738814, 1748508, 1698102","abstract":"In this paper, we present a system for cutting a moving object out from a video clip. The cutout object sequence can be pasted onto another video or a background image. To achieve this, we first apply a new 3D graph cut based segmentation approach on the spatial-temporal video volume. Our algorithm partitions watershed presegmentation regions into foreground and background while preserving temporal coherence. Then, the initial segmentation result is refined locally. Given two frames in the video sequence, we specify two respective windows of interest which are then tracked using a bi-directional feature tracking algorithm. For each frame in between these two given frames, the segmentation in each tracked window is refined using a 2D graph cut that utilizes a local color model. Moreover, we provide brush tools for the user to control the object boundary precisely wherever needed. Based on the accurate binary segmentation result, we apply coherent matting to extract the alpha mattes and foreground colors of the object.","cites":"179","conferencePercentile":"89.51612903"},{"venue":"ACM Trans. Graph.","id":"2705ad6bf722de7a2fc41b7f794e59c7a9bc4ec6","venue_1":"ACM Trans. Graph.","year":"2008","title":"High-quality motion deblurring from a single image","authors":"Qi Shan, Jiaya Jia, Aseem Agarwala","author_ids":"2141964, 1739989, 1696487","abstract":"We present a new algorithm for removing motion blur from a single image. Our method computes a deblurred image using a unified probabilistic model of <i>both</i> blur kernel estimation and unblurred image restoration. We present an analysis of the causes of common artifacts found in current deblurring methods, and then introduce several novel terms within this probabilistic model that are inspired by our analysis. These terms include a model of the spatial randomness of noise in the blurred image, as well a new local smoothness prior that reduces ringing artifacts by constraining contrast in the unblurred image wherever the blurred image exhibits low contrast. Finally, we describe an effficient optimization scheme that alternates between blur kernel estimation and unblurred image restoration until convergence. As a result of these steps, we are able to produce high quality deblurred results in low computation time. We are even able to produce results of comparable quality to techniques that require additional input images beyond a single blurry photograph, and to methods that require additional hardware.","cites":"336","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"01893ac628fdacf9c1e92ee173c30a1ebcf85a66","venue_1":"ACM Trans. Graph.","year":"2003","title":"Poisson image editing","authors":"Patrick Pérez, Michel Gangnet, Andrew Blake","author_ids":"1799777, 1752496, 1745076","abstract":"Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.","cites":"731","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"6df0238868a145e5095571270d565569e8a0bc84","venue_1":"ACM Trans. Graph.","year":"2016","title":"Acoustic voxels: computational optimization of modular acoustic filters","authors":"Dingzeyu Li, David I. W. Levin, Wojciech Matusik, Changxi Zheng","author_ids":"1859449, 1694784, 1752521, 1797875","abstract":"Acoustic filters have a wide range of applications, yet customizing them with desired properties is difficult. Motivated by recent progress in additive manufacturing that allows for fast prototyping of complex shapes, we present a computational approach that automates the design of acoustic filters with complex geometries. In our approach, we construct an acoustic filter comprised of a set of parameterized shape primitives, whose transmission matrices can be precomputed. Using an efficient method of simulating the transmission matrix of an assembly built from these underlying primitives, our method is able to optimize both the arrangement and the parameters of the acoustic shape primitives in order to satisfy target acoustic properties of the filter. We validate our results against industrial laboratory measurements and high-quality off-line simulations. We demonstrate that our method enables a wide range of applications including muffler design, musical wind instrument prototyping, and encoding imperceptible acoustic information into everyday objects.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"6637ec449976278675777c36d560012369933383","venue_1":"ACM Trans. Graph.","year":"2007","title":"Efficient gradient-domain compositing using quadtrees","authors":"Aseem Agarwala","author_ids":"1696487","abstract":"We describe a hierarchical approach to improving the efficiency of <i>gradient-domain compositing</i>, a technique that constructs seamless composites by combining the gradients of images into a vector field that is then integrated to form a composite. While gradient-domain compositing is powerful and widely used, it suffers from poor scalability. Computing an <i>n</i> pixel composite requires solving a linear system with <i>n</i> variables; solving such a large system quickly overwhelms the main memory of a standard computer when performed for multi-megapixel composites, which are common in practice. In this paper we show how to perform gradient-domain compositing approximately by solving an <i>O(p)</i> linear system, where <i>p</i> is the total length of the seams between image regions in the composite; for typical cases, <i>p</i> is <i>O</i>(&radic;<i>n</i>). We achieve this reduction by transforming the problem into a space where much of the solution is smooth, and then utilize the pattern of this smoothness to adaptively subdivide the problem domain using quadtrees. We demonstrate the merits of our approach by performing panoramic stitching and image region copy-and-paste in significantly reduced time and memory while achieving visually identical results.","cites":"63","conferencePercentile":"60.4"},{"venue":"ACM Trans. Graph.","id":"3914c83cbd3a5d5ab826c27e3a0747bb69737ba5","venue_1":"ACM Trans. Graph.","year":"2008","title":"Sketching reality: Realistic interpretation of architectural designs","authors":"Xuejin Chen, Sing Bing Kang, Ying-Qing Xu, Julie Dorsey, Harry Shum","author_ids":"2321274, 1738740, 1742571, 1775220, 1698102","abstract":"In this article, we introduce <i>sketching reality</i>, the process of converting a freehand sketch into a realistic-looking model. We apply this concept to architectural designs. As the sketch is being drawn, our system periodically interprets its 2.5D-geometry by identifying new junctions, edges, and faces, and then analyzing the extracted topology. The user can add detailed geometry and textures through sketches as well. This is possible through the use of databases that match partial sketches to models of detailed geometry and textures. The final product is a realistic texture-mapped 2.5D-model of the building. We show a variety of buildings that have been created using this system.","cites":"38","conferencePercentile":"44.75308642"},{"venue":"ACM Trans. Graph.","id":"670637d0303a863c1548d5b19f705860a23e285c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Face swapping: automatically replacing faces in photographs","authors":"Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter N. Belhumeur, Shree K. Nayar","author_ids":"2085183, 1715147, 2057606, 1767767, 1750470","abstract":"In this paper, we present a complete system for automatic face replacement in images. Our system uses a large library of face images created automatically by downloading images from the internet, extracting faces using face detection software, and aligning each extracted face to a common coordinate system. This library is constructed off-line, once, and can be efficiently accessed during face replacement. Our replacement algorithm has three main stages. First, given an input image, we detect all faces that are present, align them to the coordinate system used by our face library, and select candidate face images from our face library that are similar to the input face in appearance and pose. Second, we adjust the pose, lighting, and color of the candidate face images to match the appearance of those in the input image, and seamlessly blend in the results. Third, we rank the blended candidate replacements by computing a match distance over the overlap region. Our approach requires no 3D model, is fully automatic, and generates highly plausible results across a wide range of skin tones, lighting conditions, and viewpoints. We show how our approach can be used for a variety of applications including face de-identification and the creation of appealing group photographs from a set of images. We conclude with a user study that validates the high quality of our replacement results, and a discussion on the current limitations of our system.","cites":"82","conferencePercentile":"84.56790123"},{"venue":"ACM Trans. Graph.","id":"300e2adb4dfee68f6a866bf726c1ea2ce1c3d47e","venue_1":"ACM Trans. Graph.","year":"2008","title":"Light field transfer: global illumination between real and synthetic objects","authors":"Oliver Cossairt, Shree K. Nayar, Ravi Ramamoorthi","author_ids":"7825128, 1750470, 1752236","abstract":"We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components---thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of interreflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background.","cites":"35","conferencePercentile":"40.12345679"},{"venue":"ACM Trans. Graph.","id":"68a178ff08138ab34168f1a2bb72fcd923d2e971","venue_1":"ACM Trans. Graph.","year":"2010","title":"Ambient point clouds for view interpolation","authors":"Michael Goesele, Jens Ackermann, Simon Fuhrmann, Carsten Haubold, Ronny Klowsky, Drew Steedly, Richard Szeliski","author_ids":"1689293, 1724781, 1743966, 2057581, 1809240, 2833354, 1717841","abstract":"View interpolation and image-based rendering algorithms often produce visual artifacts in regions where the 3D scene geometry is erroneous, uncertain, or incomplete. We introduce ambient point clouds constructed from colored pixels with uncertain depth, which help reduce these artifacts while providing non-photorealistic background coloring and emphasizing reconstructed 3D geometry. Ambient point clouds are created by randomly sampling colored points along the viewing rays associated with uncertain pixels. Our real-time rendering system combines these with more traditional rigid 3D point clouds and colored surface meshes obtained using multiview stereo. Our resulting system can handle larger-range view transitions with fewer visible artifacts than previous approaches.","cites":"21","conferencePercentile":"36.5497076"},{"venue":"ACM Trans. Graph.","id":"3a1a408856dd456fc92b05135c47868a0f8f4cb1","venue_1":"ACM Trans. Graph.","year":"2006","title":"Fitting B-spline curves to point clouds by curvature-based squared distance minimization","authors":"Wenping Wang, Helmut Pottmann, Yang Liu","author_ids":"1698520, 1734949, 1742731","abstract":"Computing a curve to approximate data points is a problem encountered frequently in many applications in computer graphics, computer vision, CAD/CAM, and image processing. We present a novel and efficient method, called <i>squared distance minimization</i> (SDM), for computing a planar B-spline curve, closed or open, to approximate a target shape defined by a <i>point cloud</i>, that is, a set of unorganized, possibly noisy data points. We show that SDM significantly outperforms other optimization methods used currently in common practice of curve fitting. In SDM, a B-spline curve starts from some properly specified initial shape and converges towards the target shape through iterative quadratic minimization of the fitting error. Our contribution is the introduction of a new fitting error term, called the <i>squared distance (SD) error term</i>, defined by a curvature-based quadratic approximant of squared distances from data points to a fitting curve. The SD error term faithfully measures the geometric distance between a fitting curve and a target shape, thus leading to faster and more stable convergence than the point distance (PD) error term, which is commonly used in computer graphics and CAGD, and the tangent distance (TD) error term, which is often adopted in the computer vision community. To provide a theoretical explanation of the superior performance of SDM, we formulate the B-spline curve fitting problem as a nonlinear least squares problem and conclude that SDM is a quasi-Newton method which employs a curvature-based positive definite approximant to the true Hessian of the objective function. Furthermore, we show that the method based on the TD error term is a Gauss-Newton iteration, which is unstable for target shapes with high curvature variations, whereas optimization based on the PD error term is the alternating method that is known to have linear convergence.","cites":"53","conferencePercentile":"46.2962963"},{"venue":"ACM Trans. Graph.","id":"0b7547ef50db1a0fdd38787ec81ced7489c68879","venue_1":"ACM Trans. Graph.","year":"2011","title":"Multigrid and multilevel preconditioners for computational photography","authors":"Dilip Krishnan, Richard Szeliski","author_ids":"1979918, 1717841","abstract":"This paper unifies multigrid and multilevel (hierarchical) preconditioners, two widely-used approaches for solving computational photography and other computer graphics simulation problems. It provides detailed experimental comparisons of these techniques and their variants, including an analysis of relative computational costs and how these impact practical algorithm performance. We derive both theoretical convergence rates based on the condition numbers of the systems and their preconditioners, and empirical convergence rates drawn from real-world problems. We also develop new techniques for sparsifying higher connectivity problems, and compare our techniques to existing and newly developed variants such as algebraic and combinatorial multigrid. Our experimental results demonstrate that, except for highly irregular problems, adaptive hierarchical basis function preconditioners generally outperform alternative multigrid techniques, especially when computational complexity is taken into account.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"124800f138f5c45b3bce284b52e1af59efc0d49a","venue_1":"ACM Trans. Graph.","year":"2012","title":"Image-based rendering for scenes with reflections","authors":"Sudipta N. Sinha, Johannes Kopf, Michael Goesele, Daniel Scharstein, Richard Szeliski","author_ids":"1757937, 2891193, 1689293, 1709053, 1717841","abstract":"We present a system for image-based modeling and rendering of real-world scenes containing reflective and glossy surfaces. Previous approaches to image-based rendering assume that the scene can be approximated by 3D proxies that enable view interpolation using traditional back-to-front or z-buffer compositing. In this work, we show how these can be generalized to multiple layers that are combined in an additive fashion to model the reflection and transmission of light that occurs at specular surfaces such as glass and glossy materials. To simplify the analysis and rendering stages, we model the world using piecewise-planar layers combined using both additive and opaque mixing of light. We also introduce novel techniques for estimating multiple depths in the scene and separating the reflection and transmission components into different layers. We then use our system to model and render a variety of real-world scenes with reflections.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"7a72ed2d41950824ef09ad3541d0d25c62880eb5","venue_1":"ACM Trans. Graph.","year":"1999","title":"Fast and accurate hierarchical radiosity using global visibility","authors":"Frédo Durand, George Drettakis, Claude Puech","author_ids":"1728125, 1721779, 1730178","abstract":"Recent hierarchical global illumination algorithms permit the generation of images with a high degree of realism. Nonetheless, appropriate refinement of light transfers, high quality meshing, and accurate visibility calculation can be challenging tasks. This is particularly true for scenes containing multiple light sources and scenes lit mainly by indirect light. We present solutions to these problems by extending a global visibility data structure, the Visibility Skeleton. This extension allows us to calculate exact point-to-polygon form-factors at vertices created by subdivision. The structures also provides visibility information for all light interactions, allowing intelligent refinement strategies. High-quality meshing is effected based on a perceptualy based ranking strategy  which results in appropriate insertions of discontinuity curves into the meshes representing illumination. We introduce a hierarchy of triangulations that allows the generation of a hierarchical radiosity solution using accurate visibility and meshing. Results of our implementation show that our new algorithm produces high quality view-independent lighting solutions for direct illumination, for scenes with multiple lights and also scenes lit mainly by indirect illumination.","cites":"25","conferencePercentile":"57.14285714"},{"venue":"ACM Trans. Graph.","id":"20dd4ec30c3ee06b56e5763ec096c2f0f482eb8d","venue_1":"ACM Trans. Graph.","year":"1998","title":"Three-Dimensional Distance Field Metamorphosis","authors":"Daniel Cohen-Or, Amira Solomovici, David Levin","author_ids":"1701009, 1902192, 1689997","abstract":"Given two or more objects of general topology, intermediate objects are constructed by a distance field metamorphosis. In the presented method the interpolation of the distance field is guided by a warp function controlled by a set of corresponding anchor points. Some rules for defining a smooth least-distorting warp function are given. To reduce the distortion of the intermediate shapes, the warp function is decomposed into a rigid rotational part and an elastic part. The distance field interpolation method is modified so that the interpolation is done in correlation with the warp function. The method provides the animator with a technique that can be used to create a set of models forming a smooth transition between pairs of a given sequence of keyframe models. The advantage of the  new approach is that it is capable of morphing between objects having a different topological genus where no correspondence between the geometric primitives of the models needs to be established. The desired correspondence is defined by an animator in terms of a relatively small number of anchor points","cites":"125","conferencePercentile":"81.81818182"},{"venue":"ACM Trans. Graph.","id":"125fbf25a39ffab3798f0a187c847753e53b7c9e","venue_1":"ACM Trans. Graph.","year":"2009","title":"An empirical BSSRDF model","authors":"Craig Donner, Jason Lawrence, Ravi Ramamoorthi, Toshiya Hachisuka, Henrik Wann Jensen, Shree K. Nayar","author_ids":"2446336, 1694005, 1752236, 2439297, 1730025, 1750470","abstract":"We present a new model of the homogeneous BSSRDF based on large-scale simulations. Our model captures the appearance of materials that are not accurately represented using existing single scattering models or multiple isotropic scattering models (e.g. the diffusion approximation). We use an analytic function to model the 2D hemispherical distribution of exitant light at a point on the surface, and a table of parameter values of this function computed at uniformly sampled locations over the remaining dimensions of the BSSRDF domain. This analytic function is expressed in elliptic coordinates and has six parameters which vary smoothly with surface position, incident angle, and the underlying optical properties of the material (albedo, mean free path length, phase function and the relative index of refraction). Our model agrees well with measured data, and is compact, requiring only 250MB to represent the full spatial- and angular-distribution of light across a wide spectrum of materials. In practice, rendering a single material requires only about 100KB to represent the BSSRDF.","cites":"20","conferencePercentile":"27.62430939"},{"venue":"ACM Trans. Graph.","id":"91fe45055b3f4a6b5a4161ed08f601887e799f75","venue_1":"ACM Trans. Graph.","year":"2003","title":"Bilateral mesh denoising","authors":"Shachar Fleishman, Iddo Drori, Daniel Cohen-Or","author_ids":"3355053, 2861627, 1701009","abstract":"We present an anisotropic mesh denoising algorithm that is effective, simple and fast. This is accomplished by filtering vertices of the mesh in the normal direction using local neighborhoods. Motivated by the impressive results of bilateral filtering for image denoising, we adopt it to denoise 3D meshes; addressing the specific issues required in the transition from two-dimensions to manifolds in three dimensions. We show that the proposed method successfully removes noise from meshes while preserving features. Furthermore, the presented algorithm excels in its simplicity both in concept and implementation.","cites":"232","conferencePercentile":"86.02150538"},{"venue":"ACM Trans. Graph.","id":"3788cadbdc6724ab1d89b74e5d89dcd40094891d","venue_1":"ACM Trans. Graph.","year":"2007","title":"Geometry of multi-layer freeform structures for architecture","authors":"Helmut Pottmann, Yang Liu, Johannes Wallner, Alexander I. Bobenko, Wenping Wang","author_ids":"1734949, 1742731, 7379884, 1999398, 1698520","abstract":"The geometric challenges in the architectural design of freeform shapes come mainly from the physical realization of beams and nodes. We approach them via the concept of parallel meshes, and present methods of computation and optimization. We discuss planar faces, beams of controlled height, node geometry, and multilayer constructions. Beams of constant height are achieved with the new type of edge offset meshes. Mesh parallelism is also the main ingredient in a novel discrete theory of curvatures. These methods are applied to the construction of quadrilateral, pentagonal and hexagonal meshes, discrete minimal surfaces, discrete constant mean curvature surfaces, and their geometric transforms. We show how to design geometrically optimal shapes, and how to find a meaningful meshing and beam layout for existing shapes.","cites":"83","conferencePercentile":"71.2"},{"venue":"ACM Trans. Graph.","id":"14e46987e4342e2ecd620ba207ff6789e1469539","venue_1":"ACM Trans. Graph.","year":"2006","title":"Geometric modeling with conical meshes and developable surfaces","authors":"Yang Liu, Helmut Pottmann, Johannes Wallner, Yong-Liang Yang, Wenping Wang","author_ids":"1742731, 1734949, 7379884, 6635795, 1698520","abstract":"In architectural freeform design, the relation between shape and fabrication poses new challenges and requires more sophistication from the underlying geometry. The new concept of conical meshes satisfies central requirements for this application: They are quadrilateral meshes with planar faces, and therefore particularly suitable for the design of freeform glass structures. Moreover, they possess a natural offsetting operation and provide a support structure orthogonal to the mesh. Being a discrete analogue of the network of principal curvature lines, they represent fundamental shape characteristics. We show how to optimize a quad mesh such that its faces become planar, or the mesh becomes even conical. Combining this perturbation with subdivision yields a powerful new modeling tool for all types of quad meshes with planar faces, making subdivision attractive for architecture design and providing an elegant way of modeling developable surfaces.","cites":"139","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"6eb28cb2368c5196cb6cc5a144cb3e4593c96db5","venue_1":"ACM Trans. Graph.","year":"2003","title":"Fragment-based image completion","authors":"Iddo Drori, Daniel Cohen-Or, Yehezkel Yeshurun","author_ids":"2861627, 1701009, 2049815","abstract":"We present a new method for completing missing parts caused by the removal of foreground or background elements from an image. Our goal is to synthesize a complete, visually plausible and coherent image. The visible parts of the image serve as a training set to infer the unknown parts. Our method iteratively approximates the unknown regions and composites adaptive image fragments into the image. Values of an inverse matte are used to compute a confidence map and a level set that direct an incremental traversal within the unknown area from high to low confidence. In each step, guided by a fast smooth approximation, an image fragment is selected from the most similar and frequent examples. As the selected fragments are composited, their likelihood increases along with the mean confidence of the image, until reaching a complete image. We demonstrate our method by completion of photographs and paintings.","cites":"225","conferencePercentile":"83.87096774"},{"venue":"ACM Trans. Graph.","id":"19f410f06a01039423c3ed90f1ef97aa5cb44cdf","venue_1":"ACM Trans. Graph.","year":"2002","title":"The 3D visibility complex","authors":"Frédo Durand, George Drettakis, Claude Puech","author_ids":"1728125, 1721779, 1730178","abstract":"Visibility problems are central to many computer graphics applications. The most common examples include hidden-part removal for view computation, shadow boundaries, mutual visibility of objects for lighting simulation. In this paper, we present a theoretical study of 3D visibility properties for scenes of smooth convex objects. We work in the space of light rays, or more precisely, of <i>maximal free segments</i>. We group segments that \"see\" the same object; this defines the <i>3D visibility complex</i>. The boundaries of these groups of segments correspond to the <i>visual events</i> of the scene (limits of shadows, disappearance of an object when the viewpoint is moved, etc.). We provide a worst case analysis of the complexity of the visibility complex of 3D scenes, as well as a probabilistic study under a simple assumption for \"normal\" scenes. We extend the visibility complex to handle temporal visibility. We give an output-sensitive construction algorithm and present applications of our approach.","cites":"44","conferencePercentile":"31"},{"venue":"ACM Trans. Graph.","id":"4610b1e9b18f913fbbdb5bee6502f55a47610ff5","venue_1":"ACM Trans. Graph.","year":"2009","title":"Removing image artifacts due to dirty camera lenses and thin occluders","authors":"Jinwei Gu, Ravi Ramamoorthi, Peter N. Belhumeur, Shree K. Nayar","author_ids":"2931118, 1752236, 1767767, 1750470","abstract":"Dirt on camera lenses, and occlusions from thin objects such as fences, are two important types of artifacts in digital imaging systems. These artifacts are not only an annoyance for photographers, but also a hindrance to computer vision and digital forensics. In this paper, we show that both effects can be described by a single image formation model, wherein an intermediate layer (of dust, dirt or thin occluders) both attenuates the incoming light and scatters stray light towards the camera. Because of camera defocus, these artifacts are low-frequency and either additive or multiplicative, which gives us the power to recover the original scene radiance pointwise. We develop a number of physics-based methods to remove these effects from digital photographs and videos. For dirty camera lenses, we propose two methods to estimate the attenuation and the scattering of the lens dirt and remove the artifacts -- either by taking several pictures of a structured calibration pattern beforehand, or by leveraging natural image statistics for post-processing existing images. For artifacts from thin occluders, we propose a simple yet effective iterative method that recovers the original scene from multiple apertures. The method requires two images if the depths of the scene and the occluder layer are known, or three images if the depths are unknown. The effectiveness of our proposed methods are demonstrated by both simulated and real experimental results.","cites":"18","conferencePercentile":"23.48066298"},{"venue":"ACM Trans. Graph.","id":"19248f682a371f43014d3bd60efda6b3388859b7","venue_1":"ACM Trans. Graph.","year":"2008","title":"Computation of rotation minimizing frames","authors":"Wenping Wang, Bert Jüttler, Dayue Zheng, Yang Liu","author_ids":"1698520, 1737026, 2115579, 1742731","abstract":"Due to its minimal twist, the rotation minimizing frame (RMF) is widely used in computer graphics, including sweep or blending surface modeling, motion design and control in computer animation and robotics, streamline visualization, and tool path planning in CAD/CAM. We present a novel simple and efficient method for accurate and stable computation of RMF of a curve in 3D. This method, called the <i>double reflection method</i>, uses two reflections to compute each frame from its preceding one to yield a sequence of frames to approximate an exact RMF. The double reflection method has the fourth order global approximation error, thus it is much more accurate than the two currently prevailing methods with the second order approximation error&#8212;the projection method by Klok and the rotation method by Bloomenthal, while all these methods have nearly the same per-frame computational cost. Furthermore, the double reflection method is much simpler and faster than using the standard fourth order Runge-Kutta method to integrate the defining ODE of the RMF, though they have the same accuracy. We also investigate further properties and extensions of the double reflection method, and discuss the variational principles in design moving frames with boundary conditions, based on RMF.","cites":"38","conferencePercentile":"44.75308642"},{"venue":"ACM Trans. Graph.","id":"bb28094f21748e1cab7f2095e2c92b30e8fa46a4","venue_1":"ACM Trans. Graph.","year":"2003","title":"Ray space factorization for from-region visibility","authors":"Tommer Leyvand, Olga Sorkine-Hornung, Daniel Cohen-Or","author_ids":"3316156, 2250001, 1701009","abstract":"From-region visibility culling is considered harder than from-point visibility culling, since it is inherently four-dimensional. We present a conservative occlusion culling method based on factorizing the 4D visibility problem into horizontal and vertical components. The visibility of the two components is solved asymmetrically: the horizontal component is based on a parameterization of the ray space, and the visibility of the vertical component is solved by incrementally merging umbrae. The technique is designed so that the horizontal and vertical operations can be efficiently realized together by modern graphics hardware. Similar to image-based from-point methods, we use an occlusion map to encode visibility; however, the image-space occlusion map is in the ray space rather than in the primal space. Our results show that the culling time and the size of the computed potentially visible set depend on the size of the viewcell. For moderate viewcells, conservative occlusion culling of large urban scenes takes less than a second, and the size of the potentially visible set is only about two times larger than the size of the exact visible set.","cites":"33","conferencePercentile":"14.51612903"},{"venue":"ACM Trans. Graph.","id":"254276e646bad8bd9f035741e63ea5991cedbaff","venue_1":"ACM Trans. Graph.","year":"2005","title":"Robust moving least-squares fitting with sharp features","authors":"Shachar Fleishman, Daniel Cohen-Or, Cláudio T. Silva","author_ids":"3355053, 1701009, 1719203","abstract":"We introduce a robust moving least-squares technique for reconstructing a piecewise smooth surface from a potentially noisy point cloud. We use techniques from robust statistics to guide the creation of the neighborhoods used by the moving least squares (MLS) computation. This leads to a conceptually simple approach that provides a unified framework for not only dealing with noise, but also for enabling the modeling of surfaces with sharp features.Our technique is based on a new robust statistics method for outlier detection: the forward-search paradigm. Using this powerful technique, we locally classify regions of a point-set to multiple outlier-free smooth regions. This classification allows us to project points on a locally smooth region rather than a surface that is smooth everywhere, thus defining a piecewise smooth surface and increasing the numerical stability of the projection operator. Furthermore, by treating the points across the discontinuities as outliers, we are able to define sharp features. One of the nice features of our approach is that it automatically disregards outliers during the surface-fitting phase.","cites":"205","conferencePercentile":"91.53225806"},{"venue":"ACM Trans. Graph.","id":"4cf29f464366d30dee9bf7be9cc56963ed42b97f","venue_1":"ACM Trans. Graph.","year":"2005","title":"Action synopsis: pose selection and illustration","authors":"Jackie Assa, Yaron Caspi, Daniel Cohen-Or","author_ids":"2070014, 2289537, 1701009","abstract":"Illustrating motion in still imagery for the purpose of summary, abstraction and motion description is important for a diverse spectrum of fields, ranging from arts to sciences. In this paper, we introduce a method that produces an action synopsis for presenting motion in still images. The method carefully selects key poses based on an analysis of a skeletal animation sequence, to facilitate expressing complex motions in a single image or a small number of concise views. Our approach is to embed the high-dimensional motion curve in a low-dimensional Euclidean space, where the main characteristics of the skeletal action are kept. The lower complexity of the embedded motion curve allows a simple iterative method which analyzes the curve and locates significant points, associated with the key poses of the original motion. We present methods for illustrating the selected poses in an image as a means to convey the action. We applied our methods to a variety of motions of human actions given either as 3D animation sequences or as video clips, and generated images that depict their synopsis.","cites":"87","conferencePercentile":"59.27419355"},{"venue":"ACM Trans. Graph.","id":"900c6e04f115aa4e4a7f9bf6027291d65de0128c","venue_1":"ACM Trans. Graph.","year":"2010","title":"Diffusion coded photography for extended depth of field","authors":"Oliver Cossairt, Changyin Zhou, Shree K. Nayar","author_ids":"7825128, 1739900, 1750470","abstract":"In recent years, several cameras have been introduced which extend depth of field (DOF) by producing a depth-invariant point spread function (PSF). These cameras extend DOF by deblurring a captured image with a single spatially-invariant PSF. For these cameras, the quality of recovered images depends both on the magnitude of the PSF spectrum (MTF) of the camera, and the similarity between PSFs at different depths. While researchers have compared the MTFs of different extended DOF cameras, relatively little attention has been paid to evaluating their depth invariances. In this paper, we compare the depth invariance of several cameras, and introduce a new camera that improves in this regard over existing designs, while still maintaining a good MTF.\n Our technique utilizes a novel optical element placed in the pupil plane of an imaging system. Whereas previous approaches use optical elements characterized by their amplitude or phase profile, our approach utilizes one whose behavior is characterized by its scattering properties. Such an element is commonly referred to as an optical diffuser, and thus we refer to our new approach as <i>diffusion coding</i>. We show that diffusion coding can be analyzed in a simple and intuitive way by modeling the effect of a diffuser as a kernel in light field space. We provide detailed analysis of diffusion coded cameras and show results from an implementation using a custom designed diffuser.","cites":"30","conferencePercentile":"52.63157895"},{"venue":"ACM Trans. Graph.","id":"1c56de705b4752a250a88207d9537adc195e79ba","venue_1":"ACM Trans. Graph.","year":"2010","title":"Lp Centroidal Voronoi Tessellation and its applications","authors":"Bruno Lévy, Yang Liu","author_ids":"1705343, 1742731","abstract":"This paper introduces Lp-Centroidal Voronoi Tessellation (Lp-CVT), a generalization of CVT that minimizes a higher-order moment of the coordinates on the Voronoi cells. This generalization allows for aligning the axes of the Voronoi cells with a prede-fined background tensor field (anisotropy). Lp-CVT is computed by a quasi-Newton optimization framework, based on closed-form derivations of the objective function and its gradient. The derivations are given for both surface meshing (Ω is a triangulated mesh with per-facet anisotropy) and volume meshing (Ω is the interior of a closed triangulated mesh with a 3D anisotropy field). Applications to anisotropic, quad-dominant surface remeshing and to hex-dominant volume meshing are presented. Unlike previous work, Lp-CVT captures sharp features and intersections without requiring any pre-tagging.","cites":"53","conferencePercentile":"87.13450292"},{"venue":"ACM Trans. Graph.","id":"9e641782c04e547c21132c9de6b44a49ecac048f","venue_1":"ACM Trans. Graph.","year":"2011","title":"General planar quadrilateral mesh design using conjugate direction field","authors":"Yang Liu, Weiwei Xu, Jun Wang, Lifeng Zhu, Baining Guo, Falai Chen, Guoping Wang","author_ids":"1742731, 6953977, 1715001, 8301547, 2738456, 1785918, 2896119","abstract":"We present a novel method to approximate a freeform shape with a planar quadrilateral (PQ) mesh for modeling architectural glass structures. Our method is based on the study of conjugate direction fields (CDF) which allow the presence of &#177;&kappa;/4(&kappa; &#949; Z) singularities. Starting with a triangle discretization of a freeform shape, we first compute an as smooth as possible conjugate direction field satisfying the user's directional and angular constraints, then apply mixed-integer quadrangulation and planarization techniques to generate a PQ mesh which approximates the input shape faithfully. We demonstrate that our method is effective and robust on various 3D models.","cites":"29","conferencePercentile":"63.15789474"},{"venue":"ACM Trans. Graph.","id":"031febbedeb0744957b74e1a56e3147c4ba3a292","venue_1":"ACM Trans. Graph.","year":"2005","title":"Linear rotation-invariant coordinates for meshes","authors":"Yaron Lipman, Olga Sorkine-Hornung, David Levin, Daniel Cohen-Or","author_ids":"3232072, 2250001, 1689997, 1701009","abstract":"We introduce a rigid motion invariant mesh representation based on discrete forms defined on the mesh. The reconstruction of mesh geometry from this representation requires solving two sparse linear systems that arise from the discrete forms: the first system defines the relationship between local frames on the mesh, and the second encodes the position of the vertices via the local frames. The reconstructed geometry is unique up to a rigid transformation of the mesh. We define surface editing operations by placing user-defined constraints on the local frames and the vertex positions. These constraints are incorporated in the two linear reconstruction systems, and their solution produces a deformed surface geometry that preserves the local differential properties in the least-squares sense. Linear combination of shapes expressed with our representation enables linear shape interpolation that correctly handles rotations. We demonstrate the effectiveness of the new representation with various detail-preserving editing operators and shape morphing.","cites":"143","conferencePercentile":"75.80645161"},{"venue":"ACM Trans. Graph.","id":"70185b79aca916b570acb446af29f1ae385e3f5f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Robust modeling of constant mean curvature surfaces","authors":"Hao Pan, Yi-King Choi, Yang Liu, Wenchao Hu, Qiang Du, Konrad Polthier, Caiming Zhang, Wenping Wang","author_ids":"1757387, 1686026, 1742731, 5106071, 2522877, 2265027, 6643191, 1698520","abstract":"We present a new method for modeling discrete constant mean curvature (CMC) surfaces, which arise frequently in nature and are highly demanded in architecture and other engineering applications. Our method is based on a novel use of the CVT (<i>centroidal Voronoi tessellation</i>) optimization framework. We devise a CVT-CMC energy function defined as a combination of an extended CVT energy and a volume functional. We show that minimizing the CVT-CMC energy is asymptotically equivalent to minimizing mesh surface area with a fixed volume, thus defining a discrete CMC surface. The CVT term in the energy function ensures high mesh quality throughout the evolution of a CMC surface in an interactive design process for form finding. Our method is capable of modeling CMC surfaces with fixed or free boundaries and is robust with respect to input mesh quality and topology changes. Experiments show that the new method generates discrete CMC surfaces of improved mesh quality over existing methods.","cites":"8","conferencePercentile":"12.37373737"},{"venue":"ACM Trans. Graph.","id":"25a818ad3e6f1b1a619828c74f9f37596a11f8d7","venue_1":"ACM Trans. Graph.","year":"2013","title":"PiCam: an ultra-thin high performance monolithic camera array","authors":"Kartik Venkataraman, Dan Lelescu, Jacques Duparré, Andrew McMahon, Gabriel Molina, Priyam Chatterjee, Robert Mullis, Shree K. Nayar","author_ids":"2275562, 2314446, 2750034, 3131922, 7194062, 2313927, 3205379, 1750470","abstract":"We present <i>PiCam</i> (Pelican Imaging Camera-Array), an ultra-thin high performance monolithic camera array, that captures light fields and synthesizes high resolution images along with a range image (scene depth) through integrated parallax detection and superresolution. The camera is passive, supporting both stills and video, low light capable, and small enough to be included in the next generation of mobile devices including smartphones. Prior works [Rander et al. 1997; Yang et al. 2002; Zhang and Chen 2004; Tanida et al. 2001; Tanida et al. 2003; Duparr&#233; et al. 2004] in camera arrays have explored multiple facets of light field capture - from viewpoint synthesis, synthetic refocus, computing range images, high speed video, and micro-optical aspects of system miniaturization. However, none of these have addressed the modifications needed to achieve the strict form factor and image quality required to make array cameras practical for mobile devices. In our approach, we customize many aspects of the camera array including lenses, pixels, sensors, and software algorithms to achieve imaging performance and form factor comparable to existing mobile phone cameras.\n Our contributions to the post-processing of images from camera arrays include a cost function for parallax detection that integrates across multiple color channels, and a regularized image restoration (superresolution) process that takes into account all the system degradations and adapts to a range of practical imaging conditions. The registration uncertainty from the parallax detection process is integrated into a Maximum-a-Posteriori formulation that synthesizes an estimate of the high resolution image and scene depth. We conclude with some examples of our array capabilities such as postcapture (still) refocus, video refocus, view synthesis to demonstrate motion parallax, 3D range images, and briefly address future work.","cites":"41","conferencePercentile":"92.08144796"},{"venue":"ACM Trans. Graph.","id":"4a75a072aa1b48c68cadbdf5e7095c1a057cf134","venue_1":"ACM Trans. Graph.","year":"2011","title":"Metropolis procedural modeling","authors":"Jerry O. Talton, Yu Lou, Steve Lesser, Jared Duke, Radomír Mech, Vladlen Koltun","author_ids":"2220493, 2211646, 2653451, 2587745, 2008027, 1770944","abstract":"Procedural representations provide powerful means for generating complex geometric structures. They are also notoriously difficult to control. In this article, we present an algorithm for controlling grammar-based procedural models. Given a grammar and a high-level specification of the desired production, the algorithm computes a production from the grammar that conforms to the specification. This production is generated by optimizing over the space of possible productions from the grammar. The algorithm supports specifications of many forms, including geometric shapes and analytical objectives. We demonstrate the algorithm on procedural models of trees, cities, buildings, and Mondrian paintings.","cites":"64","conferencePercentile":"88.94736842"},{"venue":"ACM Trans. Graph.","id":"a5ff25d67a64108a153f1c2bb9307e769e77dec4","venue_1":"ACM Trans. Graph.","year":"2008","title":"A psychophysically validated metric for bidirectional texture data reduction","authors":"Jirí Filip, Mike J. Chantler, Patrick R. Green, Michal Haindl","author_ids":"1759092, 1712844, 3106337, 1765024","abstract":"Bidirectional Texture Functions (BTF) are commonly thought to provide the most realistic perceptual experience of materials from rendered images. The key to providing efficient compression of BTFs is the decision as to how much of the data should be preserved. We use psychophysical experiments to show that this decision depends critically upon the material concerned. Furthermore, we develop a BTF derived metric that enables us to automatically set a material's compression parameters in such a way as to provide users with a predefined perceptual quality. We investigate the correlation of three different BTF metrics with psychophysically derived data. Eight materials were presented to eleven naive observers who were asked to judge the perceived quality of BTF renderings as the amount of preserved data was varied. The metric showing the highest correlation with the thresholds set by the observers was the mean variance of individual BTF images. This metric was then used to automatically determine the material-specific compression parameters used in a vector quantisation scheme. The results were successfully validated in an experiment with six additional materials and eighteen observers. We show that using the psychophysically reduced BTF data significantly improves performance of a PCA-based compression method. On average, we were able to increase the compression ratios, and decrease processing times, by a factor of four without any differences being perceived.","cites":"15","conferencePercentile":"9.87654321"},{"venue":"ACM Trans. Graph.","id":"50d468968eaa32510561355379a8ae3f84926795","venue_1":"ACM Trans. Graph.","year":"2007","title":"Mesh Ensemble Motion Graphs: Data-driven mesh animation with constraints","authors":"Doug L. James, Christopher D. Twigg, Andrew Cove, Robert Y. Wang","author_ids":"1739671, 2665293, 2643598, 1692433","abstract":"We explore the use of space-time cuts to smoothly transition between stochastic mesh animation clips involving numerous deformable mesh groups while subject to physical constraints. These transitions are used to construct <i>Mesh Ensemble Motion Graphs</i> for interactive data-driven animation of high-dimensional mesh animation datasets, such as those arising from expensive physical simulations of deformable objects blowing in the wind. We formulate the transition computation as an integer programming problem, and introduce a novel randomized algorithm to compute transitions subject to geometric nonpenetration constraints. We present examples for several physically based motion datasets, with real-time display and optional interactive control over wind intensity via transitions between wind levels. We discuss challenges and opportunities for future work and practical application.","cites":"18","conferencePercentile":"8.8"},{"venue":"ACM Trans. Graph.","id":"1d6d0928f14cca3db193f1e73ef0f237f0132d58","venue_1":"ACM Trans. Graph.","year":"2008","title":"Edge-preserving decompositions for multi-scale tone and detail manipulation","authors":"Zeev Farbman, Raanan Fattal, Dani Lischinski, Richard Szeliski","author_ids":"1709900, 3230440, 1684384, 1717841","abstract":"Many recent computational photography techniques decompose an image into a piecewise smooth base layer, containing large scale variations in intensity, and a residual detail layer capturing the smaller scale details in the image. In many of these applications, it is important to control the spatial scale of the extracted details, and it is often desirable to manipulate details at multiple scales, while avoiding visual artifacts.\n In this paper we introduce a new way to construct edge-preserving multi-scale image decompositions. We show that current basedetail decomposition techniques, based on the bilateral filter, are limited in their ability to extract detail at arbitrary scales. Instead, we advocate the use of an alternative edge-preserving smoothing operator, based on the weighted least squares optimization framework, which is particularly well suited for progressive coarsening of images and for multi-scale detail extraction. After describing this operator, we show how to use it to construct edge-preserving multi-scale decompositions, and compare it to the bilateral filter, as well as to other schemes. Finally, we demonstrate the effectiveness of our edge-preserving decompositions in the context of LDR and HDR tone mapping, detail enhancement, and other applications.","cites":"220","conferencePercentile":"97.5308642"},{"venue":"ACM Trans. Graph.","id":"003ce24ca00cc925608a6bbc19da86e8083b5e41","venue_1":"ACM Trans. Graph.","year":"2006","title":"Fast separation of direct and global components of a scene using high frequency illumination","authors":"Shree K. Nayar, Gurunandan Krishnan, Michael D. Grossberg, Ramesh Raskar","author_ids":"1750470, 1868772, 1717805, 1717566","abstract":"We present fast methods for separating the direct and global illumination components of a scene measured by a camera and illuminated by a light source. In theory, the separation can be done with just two images taken with a high frequency binary illumination pattern and its complement. In practice, a larger number of images are used to overcome the optical and resolution limitations of the camera and the source. The approach does not require the material properties of objects and media in the scene to be known. However, we require that the illumination frequency is high enough to adequately sample the global components received by scene points. We present separation results for scenes that include complex interreflections, subsurface scattering and volumetric scattering. Several variants of the separation approach are also described. When a sinusoidal illumination pattern is used with different phase shifts, the separation can be done using just three images. When the computed images are of lower resolution than the source and the camera, smoothness constraints are used to perform the separation using a single image. Finally, in the case of a static scene that is lit by a simple point source, such as the sun, a moving occluder and a video camera can be used to do the separation. We also show several simple examples of how novel images of a scene can be computed from the separation results.","cites":"198","conferencePercentile":"93.51851852"},{"venue":"ACM Trans. Graph.","id":"e143e9c666daec637d0c91929bb7bb6ea32c9dc4","venue_1":"ACM Trans. Graph.","year":"2016","title":"GazeStereo3D: seamless disparity manipulations","authors":"Petr Kellnhofer, Piotr Didyk, Karol Myszkowski, Mohamed Hefeeda, Hans-Peter Seidel, Wojciech Matusik","author_ids":"1712418, 3307078, 1790911, 1711116, 1746884, 1752521","abstract":"Producing a high quality stereoscopic impression on current displays is a challenging task. The content has to be carefully prepared in order to maintain visual comfort, which typically affects the quality of depth reproduction. In this work, we show that this problem can be significantly alleviated when the eye fixation regions can be roughly estimated. We propose a new method for stereoscopic depth adjustment that utilizes eye tracking or other gaze prediction information. The key idea that distinguishes our approach from the previous work is to apply gradual depth adjustments at the eye fixation stage, so that they remain unnoticeable. To this end, we measure the limits imposed on the speed of disparity changes in various depth adjustment scenarios, and formulate a new model that can guide such seamless stereoscopic content processing. Based on this model, we propose a real-time controller that applies local manipulations to stereoscopic content to find the optimum between depth reproduction and visual comfort. We show that the controller is mostly immune to the limitations of low-cost eye tracking solutions. We also demonstrate benefits of our model in off-line applications, such as stereoscopic movie production, where skillful directors can reliably guide and predict viewers' attention or where attended image regions are identified during eye tracking sessions. We validate both our model and the controller in a series of user experiments. They show significant improvements in depth perception without sacrificing the visual quality when our techniques are applied.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"fadbe6d068086094fc45fcac6271330f6a2a2b80","venue_1":"ACM Trans. Graph.","year":"2015","title":"Legolization: optimizing LEGO designs","authors":"Sheng-Jie Luo, Yonghao Yue, Chun-Kai Huang, Yu-Huan Chung, Sei Imai, Tomoyuki Nishita, Bing-Yu Chen","author_ids":"2571040, 2276806, 3914550, 3212917, 2658088, 1696605, 1733344","abstract":"Building LEGO sculptures requires accounting for the target object's shape, colors, and stability. In particular, finding a good <i>layout</i> of LEGO bricks that prevents the sculpture from collapsing (due to its own weight) is usually challenging, and it becomes increasingly difficult as the target object becomes larger or more complex. We devise a <i>force-based</i> analysis for estimating physical stability of a given sculpture. Unlike previous techniques for Legolization, which typically use <i>heuristic-based</i> metrics for stability estimation, our <i>force-based</i> metric gives 1) an ordering in the strength so that we know which structure is more stable, and 2) a threshold for stability so that we know which one is stable enough. In addition, our stability analysis tells us the weak portion of the sculpture. Building atop our stability analysis, we present a <i>layout refinement</i> algorithm that iteratively improves the structure around the weak portion, allowing for automatic generation of a LEGO brick layout from a given 3D model, accounting for color information, required workload (in terms of the number of bricks) and physical stability. We demonstrate the success of our method with real LEGO sculptures built up from a wide variety of 3D models, and compare against previous methods.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"2b59e0921798c9543063d51e69e2a478fcbe1858","venue_1":"ACM Trans. Graph.","year":"2016","title":"An interaction-aware, perceptual model for non-linear elastic objects","authors":"Michal Piovarci, David I. W. Levin, Jason Rebello, Desai Chen, Roman Durikovic, Hanspeter Pfister, Wojciech Matusik, Piotr Didyk","author_ids":"2389799, 1694784, 3439629, 1701750, 1775105, 1701371, 1752521, 3307078","abstract":"Everyone, from a shopper buying shoes to a doctor palpating a growth, uses their sense of touch to learn about the world. 3D printing is a powerful technology because it gives us the ability to control the haptic impression an object creates. This is critical for both replicating existing, real-world constructs and designing novel ones. However, each 3D printer has different capabilities and supports different materials, leaving us to ask: How can we best replicate a given haptic result on a particular output device? In this work, we address the problem of mapping a real-world material to its nearest 3D printable counterpart by constructing a perceptual model for the compliance of nonlinearly elastic objects. We begin by building a perceptual space from experimentally obtained user comparisons of twelve 3D-printed metamaterials. By comparing this space to a number of hypothetical computational models, we identify those that can be used to accurately and efficiently evaluate human-perceived differences in nonlinear stiffness. Furthermore, we demonstrate how such models can be applied to complex geometries in an interaction-aware way where the compliance is influenced not only by the material properties from which the object is made but also its geometry. We demonstrate several applications of our method in the context of fabrication and evaluate them in a series of user experiments.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"0ae1db1c5ed3ade3ea1fdf573b10dd793a7ec5c0","venue_1":"ACM Trans. Graph.","year":"2008","title":"Factoring repeated content within and among images","authors":"Huamin Wang, Yonatan Wexler, Eyal Ofek, Hugues Hoppe","author_ids":"6798421, 1743988, 1735652, 1688461","abstract":"We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.","cites":"23","conferencePercentile":"23.45679012"},{"venue":"ACM Trans. Graph.","id":"6d776b0371399b496706dd0e1716300a07e23a39","venue_1":"ACM Trans. Graph.","year":"2006","title":"Photorealistic rendering of rain streaks","authors":"Kshitiz Garg, Shree K. Nayar","author_ids":"2158580, 1750470","abstract":"Photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem. Raindrops undergo rapid shape distortions as they fall, a phenomenon referred to as oscillations. Due to these oscillations, the reflection of light by, and the refraction of light through, a falling raindrop produce complex brightness patterns within a single motion-blurred rain streak captured by a camera or observed by a human. The brightness pattern of a rain streak typically includes speckles, multiple smeared highlights and curved brightness contours. In this work, we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction, the viewing direction and the oscillating shape of the drop. Our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences. We have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops. Using these parameters, we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions. We have developed an efficient image-based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources. The rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources. We have rendered rain in a wide range of scenarios and the results show that our physically-based rain streak model greatly enhances the visual realism of rendered rain.","cites":"27","conferencePercentile":"20.37037037"},{"venue":"ACM Trans. Graph.","id":"11004085643618720cd925e50399f338d9e69743","venue_1":"ACM Trans. Graph.","year":"2008","title":"Image-based façade modeling","authors":"Jianxiong Xiao, Tian Fang, Ping Tan, Peng Zhao, Eyal Ofek, Long Quan","author_ids":"1769328, 2652628, 1911264, 1766535, 1735652, 1722826","abstract":"We propose in this paper a semi-automatic image-based approach to fa&#231;ade modeling that uses images captured along streets and relies on structure from motion to recover camera positions and point clouds automatically as the initial stage for modeling. We start by considering a building fa&#231;ade as a flat rectangular plane or a developable surface with an associated texture image composited from the multiple visible images. A fa&#231;ade is then decomposed and structured into a Directed Acyclic Graph of rectilinear elementary patches. The decomposition is carried out top-down by a recursive subdivision, and followed by a bottom-up merging with the detection of the architectural bilateral symmetry and repetitive patterns. Each subdivided patch of the flat fa&#231;ade is augmented with a depth optimized using the 3D points cloud. Our system also allows for an easy user feedback in the 2D image space for the proposed decomposition and augmentation. Finally, our approach is demonstrated on a large number of fa&#231;ades from a variety of street-side images.","cites":"79","conferencePercentile":"83.33333333"},{"venue":"ACM Trans. Graph.","id":"6dbfb2a0bee1b7c01c1e302e3432c3aafec49efd","venue_1":"ACM Trans. Graph.","year":"2011","title":"C1x6: a stereoscopic six-user display for co-located collaboration in shared virtual environments","authors":"Alexander Kulik, André Kunert, Stephan Beck, Roman Reichel, Roland Blach, Armin Zink, Bernd Fröhlich","author_ids":"1712907, 2843613, 2497404, 2059069, 2872976, 2144558, 5399927","abstract":"Stereoscopic multi-user systems provide multiple users with individual views of a virtual environment. We developed a new projection-based stereoscopic display for six users, which employs six customized DLP projectors for fast time-sequential image display in combination with polarization. Our intelligent high-speed shutter glasses can be programmed from the application to adapt to the situation. For instance, it does this by staying open if users do not look at the projection screen or switch to a VIP high brightness mode if less than six users use the system. Each user is tracked and can move freely in front of the display while perceiving perspectively correct views of the virtual environment.\n Navigating a group of six users through a virtual world leads to situations in which the group will not fit through spatial constrictions. Our augmented group navigation techniques ameliorate this situation by fading out obstacles or by slightly redirecting individual users along a collision-free path. While redirection goes mostly unnoticed, both techniques temporarily give up the notion of a consistent shared space. Our user study confirms that users generally prefer this trade-off over na&#239;ve approaches.","cites":"13","conferencePercentile":"30.26315789"},{"venue":"ACM Trans. Graph.","id":"12e275baabc6da547136dbff4132404f2593d1b9","venue_1":"ACM Trans. Graph.","year":"2006","title":"Projection defocus analysis for scene capture and image display","authors":"Li Zhang, Shree K. Nayar","author_ids":"1735367, 1750470","abstract":"In order to produce bright images, projectors have large apertures and hence narrow depths of field. In this paper, we present methods for robust scene capture and enhanced image display based on projection defocus analysis. We model a projector's defocus using a linear system. This model is used to develop a novel temporal defocus analysis method to recover depth at each camera pixel by estimating the parameters of its projection defocus kemel in frequency domain. Compared to most depth recovery methods, our approach is more accurate near depth discontinuities. Furthermore, by using a coaxial projector-camera system, we ensure that depth is computed at all camera pixels, without any missing parts. We show that the recovered scene geometry can be used for refocus synthesis and for depth-based image composition. Using the same projector defocus model and estimation technique, we also propose a defocus compensation method that filters a projection image in a spatially-varying, depth-dependent manner to minimize its defocus blur after it is projected onto the scene. This method effectively increases the depth of field of a projector without modifying its optics. Finally, we present an algorithm that exploits projector defocus to reduce the strong pixelation artifacts produced by digital projectors, while preserving the quality of the projected image. We have experimentally verified each of our methods using real scenes.","cites":"83","conferencePercentile":"62.03703704"},{"venue":"ACM Trans. Graph.","id":"010089afa43956cca86e65bb841443865ccd8053","venue_1":"ACM Trans. Graph.","year":"2014","title":"Projective dynamics: fusing constraint projections for fast simulation","authors":"Sofien Bouaziz, Sebastian Martin, Tiantian Liu, Ladislav Kavan, Mark Pauly","author_ids":"1755579, 3341690, 1735519, 1771758, 1741645","abstract":"We present a new method for implicit time integration of physical systems. Our approach builds a bridge between nodal Finite Element methods and Position Based Dynamics, leading to a simple, efficient, robust, yet accurate solver that supports many different types of constraints. We propose specially designed energy potentials that can be solved efficiently using an alternating optimization approach. Inspired by continuum mechanics, we derive a set of continuum-based potentials that can be efficiently incorporated within our solver. We demonstrate the generality and robustness of our approach in many different applications ranging from the simulation of solids, cloths, and shells, to example-based simulation. Comparisons to Newton-based and Position Based Dynamics solvers highlight the benefits of our formulation.","cites":"32","conferencePercentile":"97.5308642"},{"venue":"ACM Trans. Graph.","id":"67234ca10a4b8589c7073eaf821b7866cb441fc4","venue_1":"ACM Trans. Graph.","year":"2016","title":"Cinema 3D: large scale automultiscopic display","authors":"Netalee Efrat, Piotr Didyk, Mike Foshey, Wojciech Matusik, Anat Levin","author_ids":"2082481, 3307078, 3440369, 1752521, 1801055","abstract":"While 3D movies are gaining popularity, viewers in a 3D cinema still need to wear cumbersome glasses in order to enjoy them. Automultiscopic displays provide a better alternative to the display of 3D content, as they present multiple angular images of the same scene without the need for special eyewear. However, automultiscopic displays cannot be directly implemented in a wide cinema setting due to variants of two main problems: (i) The range of angles at which the screen is observed in a large cinema is usually very wide, and there is an unavoidable tradeoff between the range of angular images supported by the display and its spatial or angular resolutions. (ii) Parallax is usually observed only when a viewer is positioned at a limited range of distances from the screen. This work proposes a new display concept, which supports automultiscopic content in a wide cinema setting. It builds on the typical structure of cinemas, such as the fixed seat positions and the fact that different rows are located on a slope at different heights. Rather than attempting to display many angular images spanning the full range of viewing angles in a wide cinema, our design only displays the narrow angular range observed within the limited width of a single seat. The same narrow range content is then replicated to all rows and seats in the cinema. To achieve this, it uses an optical construction based on two sets of parallax barriers, or lenslets, placed in front of a standard screen. This paper derives the geometry of such a display, analyzes its limitations, and demonstrates a proof-of-concept prototype.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"086d2543f4aa74266542db3266d253e91a16b932","venue_1":"ACM Trans. Graph.","year":"2015","title":"Dynamic 3D avatar creation from hand-held video input","authors":"Alexandru Eugen Ichim, Sofien Bouaziz, Mark Pauly","author_ids":"2645057, 1755579, 1741645","abstract":"We present a complete pipeline for creating fully rigged, personalized 3D facial avatars from hand-held video. Our system faithfully recovers facial expression dynamics of the user by adapting a blendshape template to an image sequence of recorded expressions using an optimization that integrates feature tracking, optical flow, and shape from shading. Fine-scale details such as wrinkles are captured separately in normal maps and ambient occlusion maps. From this user- and expression-specific data, we learn a regressor for on-the-fly detail synthesis during animation to enhance the perceptual realism of the avatars. Our system demonstrates that the use of appropriate reconstruction priors yields compelling face rigs even with a minimalistic acquisition system and limited user assistance. This facilitates a range of new applications in computer animation and consumer-level online communication based on personalized avatars. We present realtime application demos to validate our method.","cites":"18","conferencePercentile":"97.95918367"},{"venue":"ACM Trans. Graph.","id":"6eb2bb8b3970985d914c015a32a8827dfe501034","venue_1":"ACM Trans. Graph.","year":"1991","title":"A General Framework for Visualizing Abstract Objects and Relations","authors":"Tomihisa Kamada, Satoru Kawai","author_ids":"3237240, 2348378","abstract":"Pictorial representations significantly enhance our ability to understand complicated relations and structures, which means that information systems strongly require user interfaces that support the visualization of many kinds of information with a wide variety of graphical forms. At present, however, these difficult visualization problems have not been solved. We present a visualization framework for translating abstract objects and relations, typically represented in textual forms, into pictorial representations, and describe a general visualization interface based on this framework. In our framework, abstract objects and relations are mapped to graphical objects and relations by user-defined mapping rules. The kernel of our visualization process is to determine a layout of  graphical objects under geometric constraints. A constraint-based object layout system named COOL has been developed to handle this layout problem. COOL introduces the concept of rigidity of constraints in order to reasonably handle, a set of conflicting constraints by use of the least squares method. As applications of our system, we show the generation of kinship diagrams, list diagrams, Nassi-Shneiderman diagrams, and entity-relationship diagrams.","cites":"37","conferencePercentile":"80"},{"venue":"ACM Trans. Graph.","id":"b923ee56da306c2cefd1484c32cc4f08255395c0","venue_1":"ACM Trans. Graph.","year":"2006","title":"Animating Chinese paintings through stroke-based decomposition","authors":"Songhua Xu, Ying-Qing Xu, Sing Bing Kang, David Salesin, Yunhe Pan, Harry Shum","author_ids":"3231501, 1742571, 1738740, 1745260, 1778259, 1698102","abstract":"This article proposes a technique to animate a Chinese style painting given its image. We first extract descriptions of the brush strokes that hypothetically produced it. The key to the extraction process is the use of a brush stroke library, which is obtained by digitizing single brush strokes drawn by an experienced artist. The steps in our extraction technique are first to segment the input image, then to find the best set of brush strokes that fit the regions, and, finally, to refine these strokes to account for local appearance. We model a single brush stroke using its skeleton and contour, and we characterize texture variation within each stroke by sampling perpendicularly along its skeleton. Once these brush descriptions have been obtained, the painting can be animated at the brush stroke level. In this article, we focus on Chinese paintings with relatively sparse strokes. The animation is produced using a graphical application we developed. We present several animations of real paintings using our technique.","cites":"17","conferencePercentile":"8.796296296"},{"venue":"ACM Trans. Graph.","id":"40adc648850ee1afc8c4e5c17d43429ae6184296","venue_1":"ACM Trans. Graph.","year":"2016","title":"Beyond developable: computational design and fabrication with auxetic materials","authors":"Mina Konakovic, Keenan Crane, Bailin Deng, Sofien Bouaziz, Daniel Piker, Mark Pauly","author_ids":"3430061, 3284915, 2964129, 1755579, 1870849, 1741645","abstract":"We present a computational method for interactive 3D design and rationalization of surfaces via <i>auxetic</i> materials, i.e., flat flexible material that can stretch uniformly up to a certain extent. A key motivation for studying such material is that one can approximate doubly-curved surfaces (such as the sphere) using only flat pieces, making it attractive for fabrication. We physically realize surfaces by introducing cuts into approximately inextensible material such as sheet metal, plastic, or leather. The cutting pattern is modeled as a regular triangular linkage that yields hexagonal openings of spatially-varying radius when stretched. In the same way that isometry is fundamental to modeling developable surfaces, we leverage <i>conformal</i> geometry to understand auxetic design. In particular, we compute a global conformal map with bounded scale factor to initialize an otherwise intractable non-linear optimization. We demonstrate that this global approach can handle non-trivial topology and non-local dependencies inherent in auxetic material. Design studies and physical prototypes are used to illustrate a wide range of possible applications.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"92fbd4293638342124c25aa3bd9adc12285b238b","venue_1":"ACM Trans. Graph.","year":"2007","title":"Active refocusing of images and videos","authors":"Francesc Moreno-Noguer, Peter N. Belhumeur, Shree K. Nayar","author_ids":"1994318, 1767767, 1750470","abstract":"We present a system for refocusing images and videos of dynamic scenes using a novel, single-view depth estimation method. Our method for obtaining depth is based on the defocus of a sparse set of dots projected onto the scene. In contrast to other active illumination techniques, the projected pattern of dots can be removed from each captured image and its brightness easily controlled in order to avoid under- or over-exposure. The depths corresponding to the projected dots and a color segmentation of the image are used to compute an approximate depth map of the scene with clean region boundaries. The depth map is used to refocus the acquired image after the dots are removed, simulating realistic depth of field effects. Experiments on a wide variety of scenes, including close-ups and live action, demonstrate the effectiveness of our method.","cites":"42","conferencePercentile":"42"},{"venue":"ACM Trans. Graph.","id":"31630b94c0d302f9d2a50e0469c7b5013b7b8481","venue_1":"ACM Trans. Graph.","year":"2016","title":"Sphere-meshes for real-time hand modeling and tracking","authors":"Anastasia Tkach, Mark Pauly, Andrea Tagliasacchi","author_ids":"2768713, 1741645, 1796480","abstract":"Modern systems for real-time hand tracking rely on a combination of discriminative and generative approaches to robustly recover hand poses. Generative approaches require the specification of a geometric model. In this paper, we propose a the use of sphere-meshes as a novel geometric representation for real-time generative hand tracking. How tightly this model fits a specific user heavily affects tracking precision. We derive an optimization to non-rigidly deform a template model to fit the user data in a number of poses. This optimization jointly captures the user's static and dynamic hand geometry, thus facilitating high-precision registration. At the same time, the limited number of primitives in the tracking template allows us to retain excellent computational performance. We confirm this by embedding our models in an open source real-time registration algorithm to obtain a tracker steadily running at 60Hz. We demonstrate the effectiveness of our solution by qualitatively and quantitatively evaluating tracking precision on a variety of complex motions. We show that the improved tracking accuracy at high frame-rate enables stable tracking of extended and complex motion sequences without the need for per-frame re-initialization. To enable further research in the area of high-precision hand tracking, we publicly release source code and evaluation datasets.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"33f2465fd69bfb8ed9172ca40f15154368ac6539","venue_1":"ACM Trans. Graph.","year":"2012","title":"Enabling warping on stereoscopic images","authors":"Yuzhen Niu, Wu-chi Feng, Feng Liu","author_ids":"8368489, 1789317, 1734409","abstract":"Warping is one of the basic image processing techniques. Directly applying existing monocular image warping techniques to stereoscopic images is problematic as it often introduces vertical disparities and damages the original disparity distribution. In this paper, we show that these problems can be solved by appropriately warping both the disparity map and the two images of a stereoscopic image. We accordingly develop a technique for extending existing image warping algorithms to stereoscopic images. This technique divides stereoscopic image warping into three steps. Our method first applies the user-specified warping to one of the two images. Our method then computes the target disparity map according to the user specified warping. The target disparity map is optimized to preserve the perceived 3D shape of image content after image warping. Our method finally warps the other image using a spatially-varying warping method guided by the target disparity map. Our experiments show that our technique enables existing warping methods to be effectively applied to stereoscopic images, ranging from parametric global warping to non-parametric spatially-varying warping.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"fd60be5dc272f349d79c1685e527b0616326b997","venue_1":"ACM Trans. Graph.","year":"2013","title":"InfraStructs: fabricating information inside physical objects for imaging in the terahertz region","authors":"Karl D. D. Willis, Andrew D. Wilson","author_ids":"2269914, 1767449","abstract":"We introduce <i>InfraStructs</i>, material-based tags that embed information inside digitally fabricated objects for imaging in the Terahertz region. Terahertz imaging can safely penetrate many common materials, opening up new possibilities for encoding hidden information as part of the fabrication process. We outline the design, fabrication, imaging, and data processing steps to fabricate information inside physical objects. Prototype tag designs are presented for location encoding, pose estimation, object identification, data storage, and authentication. We provide detailed analysis of the constraints and performance considerations for designing <i>InfraStruct</i> tags. Future application scenarios range from production line inventory, to customized game accessories, to mobile robotics.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"ae4a6b2802acb178b78e817b48260094b8259fa2","venue_1":"ACM Trans. Graph.","year":"2016","title":"Stochastic structural analysis for context-aware design and fabrication","authors":"Wojciech Matusik","author_ids":"1752521","abstract":"In this paper we propose failure probabilities as a semantically and mechanically meaningful measure of object fragility. We present a stochastic finite element method which exploits fast rigid body simulation and reduced-space approaches to compute spatially varying failure probabilities. We use an explicit rigid body simulation to emulate the real-world loading conditions an object might experience, including persistent and transient frictional contact, while allowing us to combine several such scenarios together. Thus, our estimates better reflect real-world failure modes than previous methods. We validate our results using a series of real-world tests. Finally, we show how to embed failure probabilities into a stress constrained topology optimization which we use to design objects such as weight bearing brackets and robust 3D printable objects.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"ad6c92e7d0e9588d5f5064379fb2df65f99b10a2","venue_1":"ACM Trans. Graph.","year":"2016","title":"Computational multicopter design","authors":"Wojciech Matusik","author_ids":"1752521","abstract":"We present an interactive system for computational design, optimization, and fabrication of multicopters. Our computational approach allows non-experts to design, explore, and evaluate a wide range of different multicopters. We provide users with an intuitive interface for assembling a multicopter from a collection of components (e.g., propellers, motors, and carbon fiber rods). Our algorithm interactively optimizes shape and controller parameters of the current design to ensure its proper operation. In addition, we allow incorporating a variety of other metrics (such as payload, battery usage, size, and cost) into the design process and exploring tradeoffs between them. We show the efficacy of our method and system by designing, optimizing, fabricating, and operating multicopters with complex geometries and propeller configurations. We also demonstrate the ability of our optimization algorithm to improve the multicopter performance under different metrics.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"27b2decb0bc3cfaa1e66c898247a874fcb31119b","venue_1":"ACM Trans. Graph.","year":"2016","title":"Simit: A Language for Physical Simulation","authors":"Fredrik Kjolstad, Shoaib Kamil, Jonathan Ragan-Kelley, David I. W. Levin, Shinjiro Sueda, Desai Chen, Etienne Vouga, Danny M. Kaufman, Gurtej Kanwar, Wojciech Matusik, Saman P. Amarasinghe","author_ids":"2290932, 2853477, 2488277, 1694784, 7229044, 1701750, 1778579, 2972719, 3411661, 1752521, 1709150","abstract":"With existing programming tools, writing high-performance simulation code is labor intensive and requires sacrificing readability and portability. The alternative is to prototype simulations in a high-level language like Matlab, thereby sacrificing performance. The Matlab programming model naturally describes the behavior of an entire physical system using the language of linear algebra. However, simulations also manipulate individual geometric elements, which are best represented using linked data structures like meshes. Translating between the linked data structures and linear algebra comes at significant cost, both to the programmer and to the machine. High-performance implementations avoid the cost by rephrasing the computation in terms of linked or index data structures, leaving the code complicated and monolithic, often increasing its size by an order of magnitude.\n In this article, we present Simit, a new language for physical simulations that lets the programmer view the system both as a linked data structure in the form of a hypergraph and as a set of global vectors, matrices, and tensors depending on what is convenient at any given time. Simit provides a novel assembly construct that makes it conceptually easy and computationally efficient to move between the two abstractions. Using the information provided by the assembly construct, the compiler generates efficient in-place computation on the graph. We demonstrate that Simit is easy to use: a Simit program is typically shorter than a Matlab program; that it is high performance: a Simit program running sequentially on a CPU performs comparably to hand-optimized simulations; and that it is portable: Simit programs can be compiled for GPUs with no change to the program, delivering 4 to 20&#215; speedups over our optimized CPU code.","cites":"3","conferencePercentile":"91.7721519"},{"venue":"ACM Trans. Graph.","id":"8354adf7092db767712b9b95c8c58cd62752df9f","venue_1":"ACM Trans. Graph.","year":"2015","title":"Facial performance sensing head-mounted display","authors":"Hao Li, Laura C. Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, Chongyang Ma","author_ids":"1706574, 2048839, 8452501, 7563742, 2476056, 2519072, 2158289, 1797422","abstract":"There are currently no solutions for enabling direct face-to-face interaction between virtual reality (VR) users wearing head-mounted displays (HMDs). The main challenge is that the headset obstructs a significant portion of a user's face, preventing effective facial capture with traditional techniques. To advance virtual reality as a next-generation communication platform, we develop a novel HMD that enables 3D facial performance-driven animation in real-time. Our wearable system uses ultra-thin flexible electronic materials that are mounted on the foam liner of the headset to measure surface strain signals corresponding to upper face expressions. These strain signals are combined with a head-mounted RGB-D camera to enhance the tracking in the mouth region and to account for inaccurate HMD placement. To map the input signals to a 3D face model, we perform a single-instance offline training session for each person. For reusable and accurate online operation, we propose a short calibration step to readjust the Gaussian mixture distribution of the mapping before each use. The resulting animations are visually on par with cutting-edge depth sensor-driven facial performance capture systems and hence, are suitable for social interactions in virtual worlds.","cites":"20","conferencePercentile":"98.57142857"},{"venue":"ACM Trans. Graph.","id":"58d4eeb98cb10be6c1077ce415132c6b46eaaa6a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Real-time enveloping with rotational regression","authors":"Robert Y. Wang, Kari Pulli, Jovan Popovic","author_ids":"1692433, 1704409, 1731389","abstract":"Enveloping, or the mapping of skeletal controls to the deformations of a surface, is key to driving realistic animated characters. Despite its widespread use, enveloping still relies on slow or inaccurate deformation methods. We propose a method that is both fast, accurate and example-based. Our technique introduces a rotational regression model that captures common skinning deformations such as muscle bulging, twisting, and challenging areas such as the shoulders. Our improved treatment of rotational quantities is made practical by model reduction that ensures real-time solution of least-squares problems, independent of the mesh size. Our method is significantly more accurate than linear blend skinning and almost as fast, suggesting its use as a replacement for linear blend skinning when examples are available.","cites":"78","conferencePercentile":"68.8"},{"venue":"ACM Trans. Graph.","id":"89feb0d7d2cb4ba125ce2a776b03f5c451051e1f","venue_1":"ACM Trans. Graph.","year":"2014","title":"The visual microphone: passive recovery of sound from video","authors":"Abe Davis, Michael Rubinstein, Neal Wadhwa, Gautham J. Mysore, Frédo Durand, William T. Freeman","author_ids":"1825995, 1836449, 2103475, 1781063, 1728125, 1768236","abstract":"When sound hits an object, it causes small vibrations of the object's surface. We show how, using only high-speed video of the object, we can extract those minute vibrations and partially recover the sound that produced them, allowing us to turn everyday objects---a glass of water, a potted plant, a box of tissues, or a bag of chips---into visual microphones. We recover sounds from high-speed footage of a variety of objects with different properties, and use both real and simulated data to examine some of the factors that affect our ability to visually recover sound. We evaluate the quality of recovered sounds using intelligibility and SNR metrics and provide input and recovered audio samples for direct comparison. We also explore how to leverage the rolling shutter in regular consumer cameras to recover audio from standard frame-rate videos, and use the spatial resolution of our method to visualize how sound-related vibrations vary over an object's surface, which we can use to recover the vibration modes of an object.","cites":"22","conferencePercentile":"91.56378601"},{"venue":"ACM Trans. Graph.","id":"42749cfba35dfa51449a61050b3ce750b7518c1e","venue_1":"ACM Trans. Graph.","year":"2006","title":"Locally adapted hierarchical basis preconditioning","authors":"Richard Szeliski","author_ids":"1717841","abstract":"This paper develops locally adapted hierarchical basis functions for effectively preconditioning large optimization problems that arise in computer graphics applications such as tone mapping, gradient-domain blending, colorization, and scattered data interpolation. By looking at the local structure of the coefficient matrix and performing a recursive set of variable eliminations, combined with a simplification of the resulting coarse level problems, we obtain bases better suited for problems with inhomogeneous (spatially varying) data, smoothness, and boundary constraints. Our approach removes the need to heuristically adjust the optimal number of preconditioning levels, significantly outperforms previously proposed approaches, and also maps cleanly onto data-parallel architectures such as modern GPUs.","cites":"56","conferencePercentile":"48.14814815"},{"venue":"ACM Trans. Graph.","id":"01856460de5a5263d810899afb5795e06141cbcb","venue_1":"ACM Trans. Graph.","year":"2008","title":"Sketch-based tree modeling using Markov random field","authors":"Xuejin Chen, Boris Neubert, Ying-Qing Xu, Oliver Deussen, Sing Bing Kang","author_ids":"2321274, 2466324, 1742571, 1850438, 1738740","abstract":"In this paper, we describe a new system for converting a user's freehand sketch of a tree into a full 3D model that is both complex and realistic-looking. Our system does this by probabilistic optimization based on parameters obtained from a database of tree models. The best matching model is selected by comparing its 2D projections with the sketch. Branch interaction is modeled by a Markov random field, subject to the constraint of 3D projection to sketch. Our system then uses the notion of self-similarity to add new branches before finally populating all branches with leaves of the user's choice. We show a variety of natural-looking tree models generated from freehand sketches with only a few strokes.","cites":"49","conferencePercentile":"60.18518519"},{"venue":"ACM Trans. Graph.","id":"10ddf971879d2ac3963f9398124af1a3db209e2c","venue_1":"ACM Trans. Graph.","year":"2014","title":"Wire mesh design","authors":"Akash Garg, Andrew O. Sageman-Furnas, Bailin Deng, Yonghao Yue, Eitan Grinspun, Mark Pauly, Max Wardetzky","author_ids":"2004417, 2451427, 2964129, 2276806, 7522998, 1741645, 1763180","abstract":"We present a computational approach for designing <i>wire meshes</i>, i.e., freeform surfaces composed of woven wires arranged in a regular grid. To facilitate shape exploration, we map material properties of wire meshes to the geometric model of <i>Chebyshev nets</i>. This abstraction is exploited to build an efficient optimization scheme. While the theory of Chebyshev nets suggests a highly constrained design space, we show that allowing controlled deviations from the underlying surface provides a rich shape space for design exploration. Our algorithm balances globally coupled material constraints with aesthetic and geometric design objectives that can be specified by the user in an interactive design session. In addition to sculptural art, wire meshes represent an innovative medium for industrial applications including composite materials and architectural fa&#231;ades. We demonstrate the effectiveness of our approach using a variety of digital and physical prototypes with a level of shape complexity unobtainable using previous methods.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"d3a7eb838da3bf4f22dd0ca4a818ad0468c06ae4","venue_1":"ACM Trans. Graph.","year":"2012","title":"Calibrated image appearance reproduction","authors":"Erik Reinhard, Tania Pouli, Timo Kunkel, Benjamin Long, Anders Ballestad, Gerwin Damberg","author_ids":"1790581, 1763977, 1762402, 2826191, 3098896, 1906085","abstract":"Managing the appearance of images across different display environments is a difficult problem, exacerbated by the proliferation of high dynamic range imaging technologies. Tone reproduction is often limited to luminance adjustment and is rarely calibrated against psychophysical data, while color appearance modeling addresses color reproduction in a calibrated manner, albeit over a limited luminance range. Only a few image appearance models bridge the gap, borrowing ideas from both areas. Our take on scene reproduction reduces computational complexity with respect to the state-of-the-art, and adds a spatially varying model of lightness perception. The predictive capabilities of the model are validated against all psychophysical data known to us, and visual comparisons show accurate and robust reproduction for challenging high dynamic range scenes.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"542f6484cfb155a92ea6a400c63f935cee07c7a1","venue_1":"ACM Trans. Graph.","year":"2009","title":"Preserving topology and elasticity for embedded deformable models","authors":"Matthieu Nesme, Paul G. Kry, Lenka Jerábková, François Faure","author_ids":"2417582, 1970147, 1971465, 1769618","abstract":"In this paper we introduce a new approach for the embedding of linear elastic deformable models. Our technique results in significant improvements in the efficient physically based simulation of highly detailed objects. First, our embedding takes into account topological details, that is, disconnected parts that fall into the same coarse element are simulated independently. Second, we account for the varying material properties by computing stiffness and interpolation functions for coarse elements which accurately approximate the behaviour of the embedded material. Finally, we also take into account empty space in the coarse embeddings, which provides a better simulation of the boundary. The result is a straightforward approach to simulating complex deformable models with the ease and speed associated with a coarse regular embedding, and with a quality of detail that would only be possible at much finer resolution.","cites":"67","conferencePercentile":"85.91160221"},{"venue":"ACM Trans. Graph.","id":"aa498837e35136d903b97d8844c93c6fa99a3118","venue_1":"ACM Trans. Graph.","year":"2014","title":"Robust Simulation of Sparsely Sampled Thin Features in SPH-Based Free Surface Flows","authors":"Xiaowei He, Huamin Wang, Fengjun Zhang, Hongan Wang, Guoping Wang, Kun Zhou","author_ids":"2433407, 6798421, 3280032, 7643981, 2896119, 6671887","abstract":"Smoothed particle hydrodynamics (SPH) is efficient, mass preserving, and flexible in handling topological changes. However, sparsely sampled thin features are difficult to simulate in SPH-based free surface flows, due to a number of robustness and stability issues. In this article, we address this problem from two perspectives: the robustness of surface forces and the numerical instability of thin features. We present a new surface tension force scheme based on a free surface energy functional, under the diffuse interface model. We develop an efficient way to calculate the air pressure force for free surface flows, without using air particles. Compared with previous surface force formulae, our formulae are more robust against particle sparsity in thin feature cases. To avoid numerical instability on thin features, we propose to adjust the internal pressure force by estimating the internal pressure at two scales and filtering the force using a geometry-aware anisotropic kernel. Our result demonstrates the effectiveness of our algorithms in handling a variety of sparsely sampled thin liquid features, including thin sheets, thin jets, and water splashes.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"c1a7f8587b3a9f4f404e64083f750a76c4265b35","venue_1":"ACM Trans. Graph.","year":"2012","title":"Example-based synthesis of 3D object arrangements","authors":"Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas A. Funkhouser, Pat Hanrahan","author_ids":"2676553, 3106018, 2531433, 1807080, 4982303","abstract":"We present a method for synthesizing 3D object arrangements from examples. Given a few user-provided examples, our system can synthesize a diverse set of plausible new scenes by learning from a larger scene database. We rely on three novel contributions. First, we introduce a <i>probabilistic model for scenes</i> based on Bayesian networks and Gaussian mixtures that can be trained from a small number of input examples. Second, we develop a clustering algorithm that groups objects occurring in a database of scenes according to their local scene neighborhoods. These <i>contextual categories</i> allow the synthesis process to treat a wider variety of objects as interchangeable. Third, we train our probabilistic model on a mix of user-provided examples and relevant scenes retrieved from the database. This <i>mixed model</i> learning process can be controlled to introduce additional variety into the synthesized scenes. We evaluate our algorithm through qualitative results and a perceptual study in which participants judged synthesized scenes to be highly plausible, as compared to hand-created scenes.","cites":"55","conferencePercentile":"92.42424242"},{"venue":"ACM Trans. Graph.","id":"14fb05f3dd2dec91e8fa211ce799d1de18015d39","venue_1":"ACM Trans. Graph.","year":"2015","title":"Time-lapse mining from internet photos","authors":"Ricardo Martin-Brualla, David Gallup, Steven M. Seitz","author_ids":"2328243, 1796345, 1679223","abstract":"We introduce an approach for synthesizing time-lapse videos of popular landmarks from large community photo collections. The approach is completely automated and leverages the vast quantity of photos available online. First, we cluster 86 million photos into landmarks and popular viewpoints. Then, we sort the photos by date and warp each photo onto a common viewpoint. Finally, we stabilize the appearance of the sequence to compensate for lighting effects and minimize flicker. Our resulting time-lapses show diverse changes in the world's most popular sites, like glaciers shrinking, skyscrapers being constructed, and waterfalls changing course.","cites":"12","conferencePercentile":"93.67346939"},{"venue":"ACM Trans. Graph.","id":"54c95347aee5bcaae473208650a6a4107c148b84","venue_1":"ACM Trans. Graph.","year":"2015","title":"Controlling procedural modeling programs with stochastically-ordered sequential Monte Carlo","authors":"Daniel Ritchie, Ben Mildenhall, Noah D. Goodman, Pat Hanrahan","author_ids":"3106018, 2577533, 1945655, 4982303","abstract":"We present a method for controlling the output of procedural modeling programs using Sequential Monte Carlo (SMC). Previous probabilistic methods for controlling procedural models use Markov Chain Monte Carlo (MCMC), which receives control feedback only for completely-generated models. In contrast, SMC receives feedback incrementally on incomplete models, allowing it to reallocate computational resources and converge quickly. To handle the many possible sequentializations of a structured, recursive procedural modeling program, we develop and prove the correctness of a new SMC variant, Stochastically-Ordered Sequential Monte Carlo (SOSMC). We implement SOSMC for general-purpose programs using a new programming primitive: the stochastic future. Finally, we show that SOSMC reliably generates high-quality outputs for a variety of programs and control scoring functions. For small computational budgets, SOSMC's outputs often score nearly twice as high as those of MCMC or normal SMC.","cites":"8","conferencePercentile":"85.30612245"},{"venue":"ACM Trans. Graph.","id":"63f3e84f3157ef782b915b238dccc069b8d24ff0","venue_1":"ACM Trans. Graph.","year":"2013","title":"Co-hierarchical analysis of shape structures","authors":"Oliver van Kaick, Kai Xu, Hao Zhang, Yanzhen Wang, Shuyang Sun, Ariel Shamir, Daniel Cohen-Or","author_ids":"3276873, 1723225, 1682058, 7709255, 1837024, 2947946, 1701009","abstract":"We introduce an unsupervised <i>co-hierarchical</i> analysis of a <i>set</i> of shapes, aimed at discovering their hierarchical part structures and revealing relations between geometrically dissimilar yet functionally equivalent shape parts across the set. The core problem is that of <i>representative co-selection</i>. For each shape in the set, one representative hierarchy (tree) is selected from among many possible interpretations of the hierarchical structure of the shape. Collectively, the selected tree representatives maximize the <i>within-cluster</i> structural similarity among them. We develop an iterative algorithm for representative co-selection. At each step, a novel <i>cluster-and-select</i> scheme is applied to a set of candidate trees for all the shapes. The tree-to-tree distance for clustering caters to structural shape analysis by focusing on spatial arrangement of shape parts, rather than their geometric details. The final set of representative trees are unified to form a structural co-hierarchy. We demonstrate co-hierarchical analysis on families of man-made shapes exhibiting high degrees of geometric and finer-scale structural variabilities.","cites":"18","conferencePercentile":"70.13574661"},{"venue":"ACM Trans. Graph.","id":"1d7d4f6b121fa4c8f6bb52becc05d6a875206a86","venue_1":"ACM Trans. Graph.","year":"2007","title":"Image-based tree modeling","authors":"Ping Tan, Gang Zeng, Jingdong Wang, Sing Bing Kang, Long Quan","author_ids":"1911264, 3269098, 1688516, 1738740, 1722826","abstract":"In this paper, we propose an approach for generating 3D models of natural-looking trees from images that has the additional benefit of requiring little user intervention. While our approach is primarily image-based, we do not model each leaf directly from images due to the large leaf count, small image footprint, and widespread occlusions. Instead, we populate the tree with leaf replicas from segmented source images to reconstruct the overall tree shape. In addition, we use the shape patterns of visible branches to predict those of obscured branches. We demonstrate our approach on a variety of trees.","cites":"70","conferencePercentile":"64.8"},{"venue":"ACM Trans. Graph.","id":"2b60980e468050a36678b0f5aecbec842d1da709","venue_1":"ACM Trans. Graph.","year":"2006","title":"Flash matting","authors":"Jian Sun, Yin Li, Sing Bing Kang, Harry Shum","author_ids":"1748508, 1738814, 1738740, 1698102","abstract":"In this paper, we propose a novel approach to extract mattes using a pair of flash/no-flash images. Our approach, which we call <i>flash matting</i>, was inspired by the simple observation that the most noticeable difference between the flash and no-flash images is the foreground object if the background scene is sufficiently distant. We apply a new matting algorithm called <i>joint Bayesian flash matting</i> to robustly recover the matte from flash/no-flash images, even for scenes in which the foreground and the background are similar or the background is complex. Experimental results involving a variety of complex indoors and outdoors scenes show that it is easy to extract high-quality mattes using an off-the-shelf, flash-equipped camera. We also describe extensions to flash matting for handling more general scenes.","cites":"23","conferencePercentile":"14.35185185"},{"venue":"ACM Trans. Graph.","id":"8841d8c0952b2ba8d1030493c065a5efbbd79d69","venue_1":"ACM Trans. Graph.","year":"2005","title":"Far voxels: a multiresolution framework for interactive rendering of huge complex 3D models on commodity graphics platforms","authors":"Enrico Gobbetti, Fabio Marton","author_ids":"1708999, 1685719","abstract":"We present an efficient approach for end-to-end out-of-core construction and interactive inspection of very large arbitrary surface models. The method tightly integrates visibility culling and out-of-core data management with a level-of-detail framework. At preprocessing time, we generate a coarse volume hierarchy by binary space partitioning the input triangle soup. Leaf nodes partition the original data into chunks of a fixed maximum number of triangles, while inner nodes are discretized into a fixed number of cubical voxels. Each voxel contains a compact direction dependent approximation of the appearance of the associated volumetric subpart of the model when viewed from a distance. The approximation is constructed by a visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset. At rendering time, the volumetric structure, maintained off-core, is refined and rendered in front-to-back order, exploiting vertex programs for GPU evaluation of view-dependent voxel representations, hardware occlusion queries for culling occluded subtrees, and asynchronous I/O for detecting and avoiding data access latencies. Since the granularity of the multiresolution structure is coarse, data management, traversal and occlusion culling cost is amortized over many graphics primitives. The efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms.","cites":"64","conferencePercentile":"44.35483871"},{"venue":"ACM Trans. Graph.","id":"24d517878f0d9d6d1c3836a2a63ad3c9d1ca1a4f","venue_1":"ACM Trans. Graph.","year":"2006","title":"Image-based plant modeling","authors":"Long Quan, Ping Tan, Gang Zeng, Lu Yuan, Jingdong Wang, Sing Bing Kang","author_ids":"1722826, 1911264, 3269098, 2803558, 1688516, 1738740","abstract":"In this paper, we propose a semi-automatic technique for modeling plants directly from images. Our image-based approach has the distinct advantage that the resulting model inherits the realistic shape and complexity of a real plant. We designed our modeling system to be interactive, automating the process of shape recovery while relying on the user to provide simple hints on segmentation. Segmentation is performed in both image and 3D spaces, allowing the user to easily visualize its effect immediately. Using the segmented image and 3D data, the geometry of each leaf is then automatically recovered from the multiple views by fitting a deformable leaf model. Our system also allows the user to easily reconstruct branches in a similar manner. We show realistic reconstructions of a variety of plants, and demonstrate examples of plant editing.","cites":"88","conferencePercentile":"65.27777778"},{"venue":"ACM Trans. Graph.","id":"723bba1a6a6ce6a13347eda8f5af6c4dd481d826","venue_1":"ACM Trans. Graph.","year":"2014","title":"Fast burst images denoising","authors":"Ziwei Liu, Lu Yuan, Xiaoou Tang, Matthew Uyttendaele, Jian Sun","author_ids":"3243969, 2803558, 1741901, 2262291, 1748508","abstract":"This paper presents a fast denoising method that produces a clean image from a burst of noisy images. We accelerate alignment of the images by introducing a lightweight camera motion representation called <i>homography flow</i>. The aligned images are then fused to create a denoised output with rapid <i>per-pixel</i> operations in temporal and spatial domains. To handle scene motion during the capture, a mechanism of selecting <i>consistent pixels</i> for temporal fusion is proposed to \"synthesize\" a clean, ghost-free image, which can largely reduce the computation of tracking motion between frames. Combined with these efficient solutions, our method runs several orders of magnitude faster than previous work, while the denoising quality is comparable. A smartphone prototype demonstrates that our method is practical and works well on a large variety of real examples.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"1bf3ad795c2452addc2621664948fd3144c82bb7","venue_1":"ACM Trans. Graph.","year":"2006","title":"Interactive local adjustment of tonal values","authors":"Dani Lischinski, Zeev Farbman, Matthew Uyttendaele, Richard Szeliski","author_ids":"1684384, 1709900, 2262291, 1717841","abstract":"This paper presents a new interactive tool for making local adjustments of tonal values and other visual parameters in an image. Rather than carefully selecting regions or hand-painting layer masks, the user quickly indicates regions of interest by drawing a few simple brush strokes and then uses sliders to adjust the brightness, contrast, and other parameters in these regions. The effects of the user's sparse set of constraints are interpolated to the entire image using an edge-preserving energy minimization method designed to prevent the propagation of tonal adjustments to regions of significantly different luminance. The resulting system is suitable for adjusting ordinary and high dynamic range images, and provides the user with much more creative control than existing tone mapping algorithms. Our tool is also able to produce a tone mapping automatically, which may serve as a basis for further local adjustments, if so desired. The constraint propagation approach developed in this paper is a general one, and may also be used to interactively control a variety of other adjustments commonly performed in the digital darkroom.","cites":"141","conferencePercentile":"87.5"},{"venue":"ACM Trans. Graph.","id":"957005a6bc71580cc295a7d087ee8c378c815cc8","venue_1":"ACM Trans. Graph.","year":"2016","title":"Generating dynamically feasible trajectories for quadrotor cameras","authors":"Mike Roberts, Pat Hanrahan","author_ids":"3311425, 4982303","abstract":"When designing trajectories for quadrotor cameras, it is important that the trajectories respect the dynamics and physical limits of quadrotor hardware. We refer to such trajectories as being <i>feasible</i>. In this paper, we introduce a fast and user-friendly algorithm for generating feasible quadrotor camera trajectories. Our algorithm takes as input an infeasible trajectory designed by a user, and produces as output a feasible trajectory that is as similar as possible to the user's input. By design, our algorithm does not change the spatial layout or visual contents of the input trajectory. Instead, our algorithm guarantees the feasibility of the output trajectory by <i>re-timing</i> the input trajectory, perturbing its timing as little as possible while remaining within velocity and control force limits. Our choice to perturb the timing of a shot, while leaving the spatial layout and visual contents of the shot intact, leads to a well-behaved non-convex optimization problem that can be solved at interactive rates.\n We implement our algorithm in an open-source tool for designing quadrotor camera shots, where we achieve interactive performance across a wide range of camera trajectories. We demonstrate that our algorithm is between 25x and 45x faster than a spacetime constraints approach implemented using a commercially available solver. As we scale to more finely discretized trajectories, this performance gap widens, with our algorithm outperforming spacetime constraints by between 90x and 180x. Finally, we fly 5 feasible trajectories generated by our algorithm on a real quadrotor camera, producing video footage that is faithful to Google Earth shot previews, even when the trajectories are at the quadrotor's physical limits.","cites":"3","conferencePercentile":"91.7721519"},{"venue":"ACM Trans. Graph.","id":"375be99ccb9c6dde02bd0e92635bcf47613252db","venue_1":"ACM Trans. Graph.","year":"2014","title":"Tangible and modular input device for character articulation","authors":"Alec Jacobson, Daniele Panozzo, Oliver Glauser, Cédric Pradalier, Otmar Hilliges, Olga Sorkine-Hornung","author_ids":"2574283, 3241132, 2180671, 1689803, 2531379, 2250001","abstract":"Articulation of 3D characters requires control over many degrees of freedom: a difficult task with standard 2D interfaces. We present a tangible input device composed of interchangeable, hot-pluggable parts. Embedded sensors measure the device's pose at rates suitable for real-time editing and animation. Splitter parts allow branching to accommodate any skeletal tree. During assembly, the device recognizes topological changes as individual parts or pre-assembled subtrees are plugged and unplugged. A novel semi-automatic registration approach helps the user quickly map the device's degrees of freedom to a virtual skeleton inside the character. User studies report favorable comparisons to mouse and keyboard interfaces for the tasks of target acquisition and pose replication. Our device provides input for character rigging and automatic weight computation, direct skeletal deformation, interaction with physical simulations, and handle-based variational geometric modeling.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"305c1981524f672d09212972649f215fcadb3168","venue_1":"ACM Trans. Graph.","year":"2016","title":"Rig animation with a tangible and modular input device","authors":"Oliver Glauser, Wan-Chun Ma, Alec Jacobson, Daniele Panozzo, Otmar Hilliges, Olga Sorkine-Hornung","author_ids":"2180671, 1899617, 2574283, 3241132, 2531379, 2250001","abstract":"We propose a novel approach to digital character animation, combining the benefits of modular and tangible input devices and sophisticated rig animation algorithms. With a symbiotic software and hardware approach, we overcome limitations inherent to all previous tangible devices. It allows users to directly control complex rigs with 5-10 physical controls only. These compact input device configurations - optimized for a specific rig and a set of sample poses - are automatically generated by our algorithm. This avoids oversimplification of the pose space and excessively bulky devices.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"2e56a4f8ed0b39b1e8734d834dec148c0fe30169","venue_1":"ACM Trans. Graph.","year":"2016","title":"Rigel: flexible multi-rate image processing hardware","authors":"James Hegarty, Ross Daly, Zach DeVito, Mark Horowitz, Pat Hanrahan, Jonathan Ragan-Kelley","author_ids":"2624550, 3136796, 2375710, 1764167, 4982303, 2488277","abstract":"Image processing algorithms implemented using custom hardware or FPGAs of can be orders-of-magnitude more energy efficient and performant than software. Unfortunately, converting an algorithm by hand to a hardware description language suitable for compilation on these platforms is frequently too time consuming to be practical. Recent work on hardware synthesis of high-level image processing languages demonstrated that a single-rate pipeline of stencil kernels can be synthesized into hardware with provably minimal buffering. Unfortunately, few advanced image processing or vision algorithms fit into this highly-restricted programming model.\n In this paper, we present Rigel, which takes pipelines specified in our new multi-rate architecture and lowers them to FPGA implementations. Our flexible multi-rate architecture supports pyramid image processing, sparse computations, and space-time implementation tradeoffs. We demonstrate depth from stereo, Lucas-Kanade, the SIFT descriptor, and a Gaussian pyramid running on two FPGA boards. Our system can synthesize hardware for FPGAs with up to 436 Megapixels/second throughput, and up to 297x faster runtime than a tablet-class ARM CPU.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"8c35e199c66f1d27e7a861964e88257b2bc11e97","venue_1":"ACM Trans. Graph.","year":"2014","title":"Darkroom: compiling high-level image processing code into hardware pipelines","authors":"James Hegarty, John Brunhaver, Zach DeVito, Jonathan Ragan-Kelley, Noy Cohen, Steven Bell, Artem Vasilyev, Mark Horowitz, Pat Hanrahan","author_ids":"2624550, 2207916, 2375710, 2488277, 2673411, 3650723, 3337200, 1764167, 4982303","abstract":"Specialized image signal processors (ISPs) exploit the structure of image processing pipelines to minimize memory bandwidth using the architectural pattern of <i>line-buffering</i>, where all intermediate data between each stage is stored in small on-chip buffers. This provides high energy efficiency, allowing long pipelines with tera-op/sec. image processing in battery-powered devices, but traditionally requires painstaking manual design in hardware. Based on this pattern, we present Darkroom, a language and compiler for image processing. The semantics of the Darkroom language allow it to compile programs directly into line-buffered pipelines, with all intermediate values in local line-buffer storage, eliminating unnecessary communication with off-chip DRAM. We formulate the problem of optimally scheduling line-buffered pipelines to minimize buffering as an integer linear program. Finally, given an optimally scheduled pipeline, Darkroom synthesizes hardware descriptions for ASIC or FPGA, or fast CPU code. We evaluate Darkroom implementations of a range of applications, including a camera pipeline, low-level feature detection algorithms, and deblurring. For many applications, we demonstrate gigapixel/sec. performance in under 0.5mm<sup>2</sup> of ASIC silicon at 250 mW (simulated on a 45nm foundry process), real-time 1080p/60 video processing using a fraction of the resources of a modern FPGA, and tens of megapixels/sec. of throughput on a quad-core x86 processor.","cites":"16","conferencePercentile":"83.12757202"},{"venue":"ACM Trans. Graph.","id":"46bd49f717e0645e7410551a42dbeb8f5e18fbcc","venue_1":"ACM Trans. Graph.","year":"2013","title":"Embedded thin shells for wrinkle simulation","authors":"Olivier Rémillard, Paul G. Kry","author_ids":"3013080, 1970147","abstract":"We present a new technique for simulating high resolution surface wrinkling deformations of composite objects consisting of a soft interior and a harder skin. We combine high resolution thin shells with coarse finite element lattices and define frequency based constraints that allow the formation of wrinkles with properties matching those predicted by the physical parameters of the composite object. Our two-way coupled model produces the expected wrinkling behavior without the computational expense of a large number of volumetric elements to model deformations under the surface. We use <i>C</i><sup>1</sup> quadratic shape functions for the interior deformations, allowing very coarse resolutions to model the overall global deformation efficiently, while avoiding visual artifacts of wrinkling at discretization boundaries. We demonstrate that our model produces wrinkle wavelengths that match both theoretical predictions and high resolution volumetric simulations. We also show example applications in simulating wrinkles on passive objects, such as furniture, and for wrinkles on faces in character animation.","cites":"8","conferencePercentile":"25.56561086"},{"venue":"ACM Trans. Graph.","id":"1817e2285bfa850d01d92b65b0fd2e1525c727c9","venue_1":"ACM Trans. Graph.","year":"2014","title":"SceneGrok: inferring action maps in 3D environments","authors":"Pat Hanrahan","author_ids":"4982303","abstract":"With modern computer graphics, we can generate enormous amounts of 3D scene data. It is now possible to capture high-quality 3D representations of large real-world environments. Large shape and scene databases, such as the Trimble 3D Warehouse, are publicly accessible and constantly growing. Unfortunately, while a great amount of 3D content exists, most of it is detached from the semantics and functionality of the objects it represents. In this paper, we present a method to establish a correlation between the geometry and the functionality of 3D environments. Using RGB-D sensors, we capture dense 3D reconstructions of real-world scenes, and observe and track people as they interact with the environment. With these observations, we train a classifier which can transfer interaction knowledge to unobserved 3D scenes. We predict a likelihood of a given action taking place over all locations in a 3D environment and refer to this representation as an <i>action map</i> over the scene. We demonstrate prediction of action maps in both 3D scans and virtual scenes. We evaluate our predictions against ground truth annotations by people, and present an approach for characterizing 3D scenes by functional similarity using action maps.","cites":"15","conferencePercentile":"81.48148148"},{"venue":"ACM Trans. Graph.","id":"7017b2615b068eded6e74effbd3c1b8b493c4bcb","venue_1":"ACM Trans. Graph.","year":"2010","title":"Dynamic local remeshing for elastoplastic simulation","authors":"Martin Wicke, Daniel Ritchie, Bryan Matthew Klingner, Sebastian Burke, Jonathan Richard Shewchuk, James F. O'Brien","author_ids":"3185736, 3106018, 2552390, 2300895, 2096070, 1692280","abstract":"We propose a finite element simulation method that addresses the full range of material behavior, from purely elastic to highly plastic, for physical domains that are substantially reshaped by plastic flow, fracture, or large elastic deformations. To mitigate artificial plasticity, we maintain a simulation mesh in both the current state and the rest shape, and store plastic offsets only to represent the non-embeddable portion of the plastic deformation. To maintain high element quality in a tetrahedral mesh undergoing gross changes, we use a dynamic meshing algorithm that attempts to replace as few tetrahedra as possible, and thereby limits the visual artifacts and artificial diffusion that would otherwise be introduced by repeatedly remeshing the domain from scratch. Our dynamic mesher also locally refines and coarsens a mesh, and even creates anisotropic tetrahedra, wherever a simulation requests it. We illustrate these features with animations of elastic and plastic behavior, extreme deformations, and fracture.","cites":"61","conferencePercentile":"91.22807018"},{"venue":"ACM Trans. Graph.","id":"8d454f78c28e3936d51fafc999055c2286c87ec8","venue_1":"ACM Trans. Graph.","year":"2016","title":"Multi-scale label-map extraction for texture synthesis","authors":"Yitzchak David Lockerman, Basile Sauvage, Rémi Allègre, Jean-Michel Dischler, Julie Dorsey, Holly E. Rushmeier","author_ids":"2236536, 3294455, 3308830, 1753136, 1775220, 1690595","abstract":"Texture synthesis is a well-established area, with many important applications in computer graphics and vision. However, despite their success, synthesis techniques are not used widely in practice because the creation of good exemplars remains challenging and extremely tedious. In this paper, we introduce an unsupervised method for analyzing texture content across multiple scales that automatically extracts good exemplars from natural images. Unlike existing methods, which require extensive manual tuning, our method is fully automatic. This allows the user to focus on using texture palettes derived from their own images, rather than on manual interactions dictated by the needs of an underlying algorithm.\n Most natural textures exhibit patterns at multiple scales that may vary according to the location (non-stationarity). To handle such textures many synthesis algorithms rely on an analysis of the input and a guidance of the synthesis. Our new analysis is based on a labeling of texture patterns that is both (i) multi-scale and (ii) unsupervised -- that is, patterns are labeled at multiple scales, and the scales and the number of labeled clusters are selected automatically. Our method works in two stages. The first builds a hierarchical extension of superpixels and the second labels the superpixels based on random walk in a graph of similarity between superpixels and a nonnegative matrix factorization. Our label-maps provide descriptors for pixels and regions that benefit state-of-the-art texture synthesis algorithms. We show several applications including guidance of non-stationary synthesis, content selection and texture painting. Our method is designed to treat large inputs and can scale to many megapixels. In addition to traditional exemplar inputs, our method can also handle natural images containing different textured regions.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"0a7f3552102c302548e5d5b2d6b1564e73e0ab2f","venue_1":"ACM Trans. Graph.","year":"2013","title":"\"Mind the gap\": tele-registration for structure-driven image completion","authors":"Hui Huang, Kangxue Yin, Minglun Gong, Dani Lischinski, Daniel Cohen-Or, Uri M. Ascher, Baoquan Chen","author_ids":"1927737, 2042641, 2834751, 1684384, 1701009, 1966230, 1748939","abstract":"Concocting a plausible composition from several non-overlapping image pieces, whose relative positions are not fixed in advance and without having the benefit of priors, can be a daunting task. Here we propose such a method, starting with a set of sloppily pasted image pieces with gaps between them. We first extract salient curves that approach the gaps from non-tangential directions, and use likely correspondences between pairs of such curves to guide a novel <i>tele-registration</i> method that simultaneously aligns all the pieces together. A <i>structure-driven</i> image completion technique is then proposed to fill the gaps, allowing the subsequent employment of standard in-painting tools to finish the job.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"02c4c6dcc5f9e6a0341803450dfc3feb9b8901a2","venue_1":"ACM Trans. Graph.","year":"2013","title":"Projective analysis for 3D shape segmentation","authors":"Yunhai Wang, Minglun Gong, Tianhua Wang, Daniel Cohen-Or, Hao Zhang, Baoquan Chen","author_ids":"2008043, 2834751, 2526224, 1701009, 1682058, 1748939","abstract":"We introduce <i>projective analysis</i> for semantic segmentation and labeling of 3D shapes. The analysis treats an input 3D shape as a collection of 2D projections, labels each projection by transferring knowledge from existing labeled images, and back-projects and fuses the labelings on the 3D shape. The image-space analysis involves matching projected binary images of 3D objects based on a novel <i>bi-class Hausdorff distance</i>. The distance is topology-aware by accounting for internal holes in the 2D figures and it is applied to <i>piecewise-linearly warped</i> object projections to compensate for part scaling and view discrepancies. Projective analysis simplifies the processing task by working in a lower-dimensional space, circumvents the requirement of having complete and well-modeled 3D shapes, and addresses the data challenge for 3D shape analysis by leveraging the massive available image data. A large and dense labeled set ensures that the labeling of a given projected image can be inferred from closely matched labeled images. We demonstrate semantic labeling of imperfect (e.g., incomplete or self-intersecting) 3D models which would be otherwise difficult to analyze without taking the projective analysis approach.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"13b43d86e4c5e69f8a2aa9edb6fcc81cd0244eb4","venue_1":"ACM Trans. Graph.","year":"2010","title":"Anisotropic blue noise sampling","authors":"Hongwei Li, Li-Yi Wei, Pedro V. Sander, Chi-Wing Fu","author_ids":"3018623, 2420851, 1730301, 1699457","abstract":"Blue noise sampling is widely employed for a variety of imaging, geometry, and rendering applications. However, existing research so far has focused mainly on isotropic sampling, and challenges remain for the anisotropic scenario both in sample generation and quality verification. We present <i>anisotropic blue noise sampling</i> to address these issues. On the generation side, we extend dart throwing and relaxation, the two classical methods for isotropic blue noise sampling, for the anisotropic setting, while ensuring both high-quality results and efficient computation. On the verification side, although Fourier spectrum analysis has been one of the most powerful and widely adopted tools, so far it has been applied only to uniform isotropic samples. We introduce approaches based on warping and sphere sampling that allow us to extend Fourier spectrum analysis for adaptive and/or anisotropic samples; thus, we can detect problems in alternative anisotropic sampling techniques that were not yet found via prior verification. We present several applications of our technique, including stippling, visualization, surface texturing, and object distribution.","cites":"33","conferencePercentile":"58.47953216"},{"venue":"ACM Trans. Graph.","id":"121489ded314019721354d1ba3bd33b36ce615b4","venue_1":"ACM Trans. Graph.","year":"2014","title":"Approximate pyramidal shape decomposition","authors":"Ruizhen Hu, Honghua Li, Hao Zhang, Daniel Cohen-Or","author_ids":"2154334, 1829406, 1682058, 1701009","abstract":"A shape is pyramidal if it has a flat base with the remaining boundary forming a height function over the base. Pyramidal shapes are optimal for molding, casting, and layered 3D printing. However, many common objects are not pyramidal. We introduce an algorithm for <i>approximate pyramidal shape decomposition</i>. The general exact pyramidal decomposition problem is NP-hard. We turn this problem into an NP-complete problem which admits a practical solution. Specifically, we link pyramidal decomposition to the <i>Exact Cover Problem</i> (ECP). Given an input shape <i>S</i>, we develop clustering schemes to derive a set of building blocks for approximate pyramidal parts of <i>S</i>. The building blocks are then combined to yield a set of candidate pyramidal parts. Finally, we employ Knuth's Algorithm X over the candidate parts to obtain solutions to ECP as pyramidal shape decompositions. Our solution is equally applicable to 2D or 3D shapes, and to shapes with polygonal or smooth boundaries, with or without holes. We demonstrate our algorithm on numerous shapes and evaluate its performance.","cites":"16","conferencePercentile":"83.12757202"},{"venue":"ACM Trans. Graph.","id":"ce08c4d620c45715224b2645e419708e4efef4d3","venue_1":"ACM Trans. Graph.","year":"2013","title":"Probabilistic color-by-numbers: suggesting pattern colorizations using factor graphs","authors":"Sharon Lin, Daniel Ritchie, Matthew Fisher, Pat Hanrahan","author_ids":"5581342, 3106018, 2676553, 4982303","abstract":"We present a probabilistic factor graph model for automatically coloring 2D patterns. The model is trained on example patterns to statistically capture their stylistic properties. It incorporates terms for enforcing both color compatibility and spatial arrangements of colors that are consistent with the training examples. Using Markov Chain Monte Carlo, the model can be sampled to generate a diverse set of new colorings for a target pattern. This general probabilistic framework allows users to guide the generated suggestions via conditional inference or additional soft constraints. We demonstrate results on a variety of coloring tasks, and we evaluate the model through a perceptual study in which participants judged sampled colorings to be significantly preferable to other automatic baselines.","cites":"17","conferencePercentile":"67.19457014"},{"venue":"ACM Trans. Graph.","id":"1045cc9599944e0088ccc6f6c13ee9f960ecffd7","venue_1":"ACM Trans. Graph.","year":"2014","title":"Shape Segmentation by Approximate Convexity Analysis","authors":"Oliver van Kaick, Noa Fish, Yanir Kleiman, Shmuel Asafi, Daniel Cohen-Or","author_ids":"3276873, 2787217, 3119575, 3216105, 1701009","abstract":"We present a shape segmentation method for complete and incomplete shapes. The key idea is to directly optimize the decomposition based on a characterization of the expected geometry of a part in a shape. Rather than setting the number of parts in advance, we search for the smallest number of parts that admit the geometric characterization of the parts. The segmentation is based on an <i>intermediate-level</i> analysis, where first the shape is decomposed into approximate convex components, which are then merged into consistent parts based on a nonlocal geometric signature. Our method is designed to handle incomplete shapes, represented by point clouds. We show segmentation results on shapes acquired by a range scanner, and an analysis of the robustness of our method to missing regions. Moreover, our method yields results that are comparable to state-of-the-art techniques evaluated on complete shapes.","cites":"12","conferencePercentile":"73.45679012"},{"venue":"ACM Trans. Graph.","id":"5355b5785dbab64f510d01cda9e8aa69052ef5af","venue_1":"ACM Trans. Graph.","year":"2016","title":"Pyramid of arclength descriptor for generating collage of shapes","authors":"Kin Chung Kwan, Lok Tsun Sinn, Chu Han, Tien-Tsin Wong, Chi-Wing Fu","author_ids":"3447478, 8593872, 1946028, 1720633, 1699457","abstract":"This paper tackles a challenging 2D collage generation problem, focusing on shapes: we aim to fill a given region by packing irregular and reasonably-sized shapes with minimized gaps and overlaps. To achieve this nontrivial problem, we first have to analyze the boundary of individual shapes and then couple the shapes with partially-matched boundary to reduce gaps and overlaps in the collages. Second, the search space in identifying a good coupling of shapes is highly enormous, since arranging a shape in a collage involves a position, an orientation, and a scale factor. Yet, this matching step needs to be performed for every single shape when we pack it into a collage. Existing shape descriptors are simply infeasible for computation in a reasonable amount of time. To overcome this, we present a brand new, scale- and rotation-invariant 2D shape descriptor, namely <i>pyramid of arclength descriptor</i> (PAD). Its formulation is locally supported, scalable, and yet simple to construct and compute. These properties make PAD efficient for performing the partial-shape matching. Hence, we can prune away most search space with simple calculation, and efficiently identify candidate shapes. We evaluate our method using a large variety of shapes with different types and contours. Convincing collage results in terms of visual quality and time performance are obtained.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"2988e38a5587238a814331c36cd9268c4bc075da","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fluid volume modeling from sparse multi-view images by appearance transfer","authors":"Makoto Okabe, Yoshinori Dobashi, Ken-ichi Anjyo, Rikio Onai","author_ids":"3102663, 1791538, 3002982, 2086194","abstract":"We propose a method of three-dimensional (3D) modeling of volumetric fluid phenomena from sparse multi-view images (e.g., only a single-view input or a pair of front- and side-view inputs). The volume determined from such sparse inputs using previous methods appears blurry and unnatural with novel views; however, our method preserves the appearance of novel viewing angles by transferring the appearance information from input images to novel viewing angles. For appearance information, we use histograms of image intensities and steerable coefficients. We formulate the volume modeling as an energy minimization problem with statistical hard constraints, which is solved using an expectation maximization (EM)-like iterative algorithm. Our algorithm begins with a rough estimate of the initial volume modeled from the input images, followed by an iterative process whereby we first render the images of the current volume with novel viewing angles. Then, we modify the rendered images by transferring the appearance information from the input images, and we thereafter model the improved volume based on the modified images. We iterate these operations until the volume converges. We demonstrate our method successfully provides natural-looking volume sequences of fluids (i.e., fire, smoke, explosions, and a water splash) from sparse multi-view videos. To create production-ready fluid animations, we further propose a method of rendering and editing fluids using a commercially available fluid simulator.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"69f54bc15f2fbc8c81daa0d3ef3987460881c55b","venue_1":"ACM Trans. Graph.","year":"2008","title":"Accelerometer-based user interfaces for the control of a physically simulated character","authors":"Takaaki Shiratori, Jessica K. Hodgins","author_ids":"2463857, 1788773","abstract":"In late 2006, Nintendo released a new game controller, the Wiimote, which included a three-axis accelerometer. Since then, a large variety of novel applications for these controllers have been developed by both independent and commercial developers. We add to this growing library with three performance interfaces that allow the user to control the motion of a dynamically simulated, animated character through the motion of his or her arms, wrists, or legs. For comparison, we also implement a traditional joystick/button interface. We assess these interfaces by having users test them on a set of tracks containing turns and pits. Two of the interfaces (legs and wrists) were judged to be more immersive and were better liked than the joystick/button interface by our subjects. All three of the Wiimote interfaces provided better control than the joystick interface based on an analysis of the failures seen during the user study.","cites":"51","conferencePercentile":"62.96296296"},{"venue":"ACM Trans. Graph.","id":"1f72279fd7d40fc068559ece016f6be4d9aa1606","venue_1":"ACM Trans. Graph.","year":"2014","title":"Build-to-last: strength to weight 3D printed objects","authors":"Lin Lu, Andrei Sharf, Haisen Zhao, Yuan Wei, Qingnan Fan, Xuelin Chen, Yann Savoye, Changhe Tu, Daniel Cohen-Or, Baoquan Chen","author_ids":"1687087, 2120270, 3266088, 2873392, 2115998, 1885508, 2513846, 3327879, 1701009, 1748939","abstract":"The emergence of low-cost 3D printers steers the investigation of new geometric problems that control the quality of the fabricated object. In this paper, we present a method to reduce the material cost and weight of a given object while providing a durable printed model that is resistant to impact and external forces.\n We introduce a hollowing optimization algorithm based on the concept of <i>honeycomb-cells</i> structure. Honeycombs structures are known to be of minimal material cost while providing strength in tension. We utilize the Voronoi diagram to compute irregular honeycomb-like volume tessellations which define the inner structure. We formulate our problem as a <i>strength--to--weight</i> optimization and cast it as mutually finding an optimal interior tessellation and its maximal <i>hollowing</i> subject to relieve the interior stress. Thus, our system allows to <i>build-to-last</i> 3D printed objects with large control over their strength-to-weight ratio and easily model various interior structures. We demonstrate our method on a collection of 3D objects from different categories. Furthermore, we evaluate our method by printing our hollowed models and measure their stress and weights.","cites":"37","conferencePercentile":"98.14814815"},{"venue":"ACM Trans. Graph.","id":"120058dde6a26fefd9909d7ed76bb1e9dd80889d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Slippage-free background replacement for hand-held video","authors":"Fan Zhong, Song Yang, Xueying Qin, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen","author_ids":"2579992, 1792382, 1754175, 1684384, 1701009, 1748939","abstract":"We introduce a method for replacing the background in a video of a moving foreground subject, when both the <i>source video</i> capturing the subject, and the <i>target video</i> capturing the new background scene, are natural videos, casually captured using a freely moving hand-held camera. We assume that the foreground subject has already been extracted, and focus on the challenging task of generating a video with a new background, such that the new background motion appears compatible with the original one. Failure to match the motion results in disturbing <i>slippage</i> or <i>moonwalk</i> artifacts, where the subject's feet appear to slide or slip over the ground. While matching the motion across the entire frame is impossible for scenes with differing geometry, we aim to match the <i>local motion</i> of the ground in the vicinity of the subject. This is achieved by reordering and warping the available target background frames in a manner that optimizes a suitably designed objective function.","cites":"1","conferencePercentile":"4.938271605"},{"venue":"ACM Trans. Graph.","id":"5a8d85470b6f6de653ad888cdb72d3c20f752db7","venue_1":"ACM Trans. Graph.","year":"2014","title":"Meta-representation of shape families","authors":"Noa Fish, Melinos Averkiou, Oliver van Kaick, Olga Sorkine-Hornung, Daniel Cohen-Or, Niloy J. Mitra","author_ids":"2787217, 2695364, 3276873, 2250001, 1701009, 1710455","abstract":"We introduce a meta-representation that represents the <i>essence</i> of a family of shapes. The meta-representation learns the configurations of shape parts that are common across the family, and encapsulates this knowledge with a system of geometric distributions that encode relative arrangements of parts. Thus, instead of predefined priors, what characterizes a shape family is directly learned from the set of input shapes. The meta-representation is constructed from a set of co-segmented shapes with known correspondence. It can then be used in several applications where we seek to preserve the identity of the shapes as members of the family. We demonstrate applications of the meta-representation in exploration of shape repositories, where interesting shape configurations can be examined in the set; guided editing, where models can be edited while maintaining their familial traits; and coupled editing, where several shapes can be collectively deformed by directly manipulating the distributions in the meta-representation. We evaluate the efficacy of the proposed representation on a variety of shape collections.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"8bfe6b0c59350540955262fdebc9301a2c19c0ae","venue_1":"ACM Trans. Graph.","year":"2011","title":"Motion capture from body-mounted cameras","authors":"Takaaki Shiratori, Hyun Soo Park, Leonid Sigal, Yaser Sheikh, Jessica K. Hodgins","author_ids":"2463857, 1806522, 2956921, 1774867, 1788773","abstract":"Motion capture technology generally requires that recordings be performed in a laboratory or closed stage setting with controlled lighting. This restriction precludes the capture of motions that require an outdoor setting or the traversal of large areas. In this paper, we present the theory and practice of using body-mounted cameras to reconstruct the motion of a subject. Outward-looking cameras are attached to the limbs of the subject, and the joint angles and root pose are estimated through non-linear optimization. The optimization objective function incorporates terms for image matching error and temporal continuity of motion. Structure-from-motion is used to estimate the skeleton structure and to provide initialization for the non-linear optimization procedure. Global motion is estimated and drift is controlled by matching the captured set of videos to reference imagery. We show results in settings where capture would be difficult or impossible with traditional motion capture systems, including walking outside and swinging on monkey bars. The quality of the motion reconstruction is evaluated by comparing our results against motion capture data produced by a commercially available optical system.","cites":"34","conferencePercentile":"70.26315789"},{"venue":"ACM Trans. Graph.","id":"ba19b9076d52a130db480a1beca903c10c349f47","venue_1":"ACM Trans. Graph.","year":"2014","title":"Morfit: interactive surface reconstruction from incomplete point clouds with curve-driven topology and geometry control","authors":"Kangxue Yin, Hui Huang, Hao Zhang, Minglun Gong, Daniel Cohen-Or, Baoquan Chen","author_ids":"2042641, 1927737, 1682058, 2834751, 1701009, 1748939","abstract":"With significant data missing in a point scan, reconstructing a complete surface with sufficient geometric and topological fidelity is highly challenging. We present an interactive technique for surface reconstruction from incomplete and sparse scans of 3D objects possessing sharp features. A fundamental premise of our interaction paradigm is that directly editing data in 3D is not only counterintuitive but also ineffective, while working with 1D entities (i.e., curves) is a lot more manageable. To this end, we factor 3D editing into two \"orthogonal\" interactions acting on skeletal and profile curves of the underlying shape, controlling its topology and geometric features, respectively. For surface completion, we introduce a novel skeleton-driven <i>morph-to-fit</i>, or <i>morfit</i>, scheme which reconstructs the shape as an ensemble of generalized cylinders. Morfit is a hybrid operator which optimally interpolates between adjacent curve profiles (the \"morph\") and snaps the surface to input points (the \"fit\"). The interactive reconstruction iterates between user edits and morfit to converge to a desired final surface. We demonstrate various interactive reconstructions from point scans with sharp features and significant missing data.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"b198c2f898c3c7a005ae07f16a3c1f7d94aff6f1","venue_1":"ACM Trans. Graph.","year":"2016","title":"Printing arbitrary meshes with a 5DOF wireframe printer","authors":"Rundong Wu, Huaishu Peng, François Guimbretière, Steve Marschner","author_ids":"3396534, 1817818, 2539134, 2593798","abstract":"Traditional 3D printers fabricate objects by depositing material to build up the model layer by layer. Instead printing only wireframes can reduce printing time and the cost of material while producing effective depictions of shape. However, wireframe printing requires the printer to undergo arbitrary 3D motions, rather than slice-wise 2D motions, which can lead to collisions with already-printed parts of the model. Previous work has either limited itself to restricted meshes that are collision free by construction, or simply dropped unreachable parts of the model, but in this paper we present a method to print arbitrary meshes on a 5DOF wireframe printer. We formalize the collision avoidance problem using a directed graph, and propose an algorithm that finds a locally minimal set of constraints on the order of edges that guarantees there will be no collisions. Then a second algorithm orders the edges so that the printing progresses smoothly. Though meshes do exist that still cannot be printed, our method prints a wide range of models that previous methods cannot, and it provides a fundamental enabling algorithm for future development of wireframe printing.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"b2b82cc571d2a39b6e74871325436acb855e06a5","venue_1":"ACM Trans. Graph.","year":"2014","title":"Organizing heterogeneous scene collections through contextual focal points","authors":"Kai Xu, Rui Ma, Hao Zhang, Chenyang Zhu, Ariel Shamir, Daniel Cohen-Or, Hui Huang","author_ids":"1723225, 5888590, 1682058, 2041096, 2947946, 1701009, 1927737","abstract":"We introduce <i>focal points</i> for characterizing, comparing, and organizing collections of complex and heterogeneous data and apply the concepts and algorithms developed to collections of 3D indoor scenes. We represent each scene by a graph of its constituent objects and define focal points as <i>representative</i> substructures in a scene collection. To organize a heterogeneous scene collection, we cluster the scenes based on a set of extracted focal points: scenes in a cluster are closely connected when viewed from the perspective of the representative focal points of that cluster. The key concept of representativity requires that the focal points occur <i>frequently</i> in the cluster and that they result in a <i>compact</i> cluster. Hence, the problem of focal point extraction is intermixed with the problem of clustering groups of scenes based on their representative focal points. We present a <i>co-analysis</i> algorithm which interleaves frequent pattern mining and subspace clustering to extract a set of <i>contextual</i> focal points which guide the clustering of the scene collection. We demonstrate advantages of <i>focal-centric</i> scene comparison and organization over existing approaches, particularly in dealing with <i>hybrid</i> scenes, scenes consisting of elements which suggest membership in different semantic categories.","cites":"12","conferencePercentile":"73.45679012"},{"venue":"ACM Trans. Graph.","id":"9f5de7d4e35ada605afb413ae81628905ed411e5","venue_1":"ACM Trans. Graph.","year":"2016","title":"Efficient GPU rendering of subdivision surfaces using adaptive quadtrees","authors":"Wade Brainerd, Tim Foley, Manuel Kraemer, Henry Moreton, Matthias Nießner","author_ids":"2907796, 2151035, 3430988, 2907854, 2209612","abstract":"We present a novel method for real-time rendering of subdivision surfaces whose goal is to make subdivision faces as easy to render as triangles, points, or lines. Our approach uses standard GPU tessellation hardware and processes each face of a base mesh independently, thus allowing an entire model to be rendered in a single pass. The key idea of our method is to subdivide the <i>u, v</i> domain of each face ahead of time, generating a quadtree structure, and then submit one tessellated primitive per input face. By traversing the quadtree for each post-tessellation vertex, we are able to accurately and efficiently evaluate the limit surface. Our method yields a more uniform tessellation of the surface, and faster rendering, as fewer primitives are submitted. We evaluate our method on a variety of assets, and realize performance that can be three times faster than state-of-the-art approaches. In addition, our streaming formulation makes it easier to integrate subdivision surfaces into applications and shader code written for polygonal models. We illustrate integration of our technique into a full-featured video game engine.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"41f4df98eadfdf6e0b933bfd65021b93a9c74e68","venue_1":"ACM Trans. Graph.","year":"2013","title":"Qualitative organization of collections of shapes via quartet analysis","authors":"Shi-Sheng Huang, Ariel Shamir, Chao-Hui Shen, Hao Zhang, Alla Sheffer, Shi-Min Hu, Daniel Cohen-Or","author_ids":"2550389, 2947946, 2795523, 1682058, 3354923, 1686809, 1701009","abstract":"We present a method for organizing a heterogeneous collection of 3D shapes for overview and exploration. Instead of relying on quantitative distances, which may become unreliable between dissimilar shapes, we introduce a <i>qualitative</i> analysis which utilizes multiple distance measures but only in cases where the measures can be reliably compared. Our analysis is based on the notion of <i>quartets</i>, each defined by two pairs of shapes, where the shapes in each pair are close to each other, but far apart from the shapes in the other pair. Combining the information from many quartets computed across a shape collection using several distance measures, we create a hierarchical structure we call <i>categorization tree</i> of the shape collection. This tree satisfies the topological (qualitative) constraints imposed by the quartets creating an effective organization of the shapes. We present categorization trees computed on various collections of shapes and compare them to ground truth data from human categorization. We further introduce the concept of <i>degree of separation</i> chart for every shape in the collection and show the effectiveness of using it for interactive shapes exploration.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"2bd7993184cdd42b3570d2ec7b01bfec7193dd08","venue_1":"ACM Trans. Graph.","year":"2013","title":"L1-medial skeleton of point cloud","authors":"Hui Huang, Shihao Wu, Daniel Cohen-Or, Minglun Gong, Hao Zhang, Guiqing Li, Baoquan Chen","author_ids":"1927737, 8175431, 1701009, 2834751, 1682058, 4882057, 1748939","abstract":"We introduce <i>L</i><sub>1</sub>-<i>medial skeleton</i> as a curve skeleton representation for 3D point cloud data. The <i>L</i><sub>1</sub>-median is well-known as a robust global center of an arbitrary set of points. We make the key observation that adapting <i>L</i><sub>1</sub>-medians <i>locally</i> to a point set representing a 3D shape gives rise to a one-dimensional structure, which can be seen as a localized center of the shape. The primary advantage of our approach is that it does not place strong requirements on the quality of the input point cloud nor on the geometry or topology of the captured shape. We develop a <i>L</i><sub>1</sub>-medial skeleton construction algorithm, which can be directly applied to an unoriented raw point scan with significant noise, outliers, and large areas of missing data. We demonstrate <i>L</i><sub>1</sub>-medial skeletons extracted from raw scans of a variety of shapes, including those modeling high-genus 3D objects, plant-like structures, and curve networks.","cites":"27","conferencePercentile":"81.67420814"},{"venue":"ACM Trans. Graph.","id":"0b62641e4d5c662b51370146e6d9c3e2f653780b","venue_1":"ACM Trans. Graph.","year":"2009","title":"Interactive simulation of surgical needle insertion and steering","authors":"Nuttapong Chentanez, Ron Alterovitz, Daniel Ritchie, Lita Cho, Kris K. Hauser, Kenneth Y. Goldberg, Jonathan Richard Shewchuk, James F. O'Brien","author_ids":"1696373, 1682091, 3106018, 2902971, 2961657, 1733062, 2096070, 1692280","abstract":"We present algorithms for simulating and visualizing the insertion and steering of needles through deformable tissues for surgical training and planning. Needle insertion is an essential component of many clinical procedures such as biopsies, injections, neurosurgery, and brachytherapy cancer treatment. The success of these procedures depends on accurate guidance of the needle tip to a clinical target while avoiding vital tissues. Needle insertion deforms body tissues, making accurate placement difficult. Our interactive needle insertion simulator models the coupling between a steerable needle and deformable tissue. We introduce (1) a novel algorithm for local remeshing that quickly enforces the conformity of a tetrahedral mesh to a curvilinear needle path, enabling accurate computation of contact forces, (2) an efficient method for coupling a 3D finite element simulation with a 1D inextensible rod with stick-slip friction, and (3) optimizations that reduce the computation time for physically based simulations. We can realistically and interactively simulate needle insertion into a prostate mesh of 13,375 tetrahedra and 2,763 vertices at a 25 Hz frame rate on an 8-core 3.0 GHz Intel Xeon PC. The simulation models prostate brachytherapy with needles of varying stiffness, steering needles around obstacles, and supports motion planning for robotic needle insertion. We evaluate the accuracy of the simulation by comparing against real-world experiments in which flexible, steerable needles were inserted into gel tissue phantoms.","cites":"55","conferencePercentile":"75.69060773"},{"venue":"ACM Trans. Graph.","id":"50b848b1fa5a15044dfd865e745ebc1347a2bf26","venue_1":"ACM Trans. Graph.","year":"2011","title":"Spark: modular, composable shaders for graphics hardware","authors":"Tim Foley, Pat Hanrahan","author_ids":"2151035, 4982303","abstract":"In creating complex real-time shaders, programmers should be able to decompose code into independent, localized modules of their choosing. Current real-time shading languages, however, enforce a fixed decomposition into per-pipeline-stage procedures. Program concerns at other scales -- including those that <i>cross-cut</i> multiple pipeline stages -- cannot be expressed as reusable modules.\n We present a shading language, Spark, and its implementation for modern graphics hardware that improves support for separation of concerns into modules. A Spark <i>shader class</i> can encapsulate code that maps to more than one pipeline stage, and can be extended and composed using object-oriented inheritance. In our tests, shaders written in Spark achieve performance within 2% of HLSL.","cites":"15","conferencePercentile":"34.47368421"},{"venue":"ACM Trans. Graph.","id":"2185378d3ac617da71f02f7a9b1fb79416f2e124","venue_1":"ACM Trans. Graph.","year":"2013","title":"Inverse bi-scale material design","authors":"Hongzhi Wu, Julie Dorsey, Holly E. Rushmeier","author_ids":"1880838, 1775220, 1690595","abstract":"One major shortcoming of existing bi-scale material design systems is the lack of support for inverse design: there is no way to directly edit the large-scale appearance and then rapidly solve for the small-scale details that approximate that look. Prior work is either too slow to provide quick feedback, or limited in the types of small-scale details that can be handled. We present a novel computational framework for inverse bi-scale material design. The key idea is to convert the challenging inverse appearance computation into efficient search in two precomputed large libraries: one including a wide range of measured and analytical materials, and the other procedurally generated and height-map-based geometries. We demonstrate a variety of editing operations, including finding visually equivalent details that produce similar large-scale appearance, which can be useful in applications such as physical fabrication of materials.","cites":"2","conferencePercentile":"4.07239819"},{"venue":"ACM Trans. Graph.","id":"24ceedaea33e2ff8387bc262d74d4d82beef14a3","venue_1":"ACM Trans. Graph.","year":"2010","title":"Reducing shading on GPUs using quad-fragment merging","authors":"Kayvon Fatahalian, Solomon Boulos, James Hegarty, Kurt Akeley, William R. Mark, Henry Moreton, Pat Hanrahan","author_ids":"2789576, 3164352, 2624550, 3221156, 1913999, 2907854, 4982303","abstract":"Current GPUs perform a significant amount of redundant shading when surfaces are tessellated into small triangles. We address this inefficiency by augmenting the GPU pipeline to gather and merge rasterized fragments from adjacent triangles in a mesh. This approach has minimal impact on output image quality, is amenable to implementation in fixed-function hardware, and, when rendering pixel-sized triangles, requires only a small amount of buffering to reduce overall pipeline shading work by a factor of eight. We find that a fragment-shading pipeline with this optimization is competitive with the REYES pipeline approach of shading at micropolygon vertices and, in cases of complex occlusion, can perform up to two times less shading work.","cites":"20","conferencePercentile":"33.62573099"},{"venue":"ACM Trans. Graph.","id":"333efd19bdd4d0c386f109eb984a54844a24b816","venue_1":"ACM Trans. Graph.","year":"2011","title":"Image-guided weathering: A new approach applied to flow phenomena","authors":"Carles Bosch, Pierre-Yves Laffont, Holly E. Rushmeier, Julie Dorsey, George Drettakis","author_ids":"2463339, 2615801, 1690595, 1775220, 1721779","abstract":"The simulation of weathered appearance is essential in the realistic modeling of urban environments. A representative and particularly difficult effect to produce on a large scale is the effect of fluid flow. Changes in appearance due to flow are the result of both the global effect of large-scale shape, and local effects, such as the detailed roughness of a surface. With digital photography and Internet image collections, visual examples of flow effects are readily available. These images, however, mix the appearance of flows with the specific local context. We present a methodology to extract parameters and detail maps from existing imagery in a form that allows new target-specific flow effects to be produced, with natural variations in the effects as they are applied in different locations in a new scene. In this article, we focus on producing a library of parameters and detail maps for generating flow patterns; and this methodology can be used to extend the library with additional image exemplars. To illustrate our methodology, we show a rich collection of patterns applied to urban models.","cites":"9","conferencePercentile":"15.78947368"},{"venue":"ACM Trans. Graph.","id":"5c8a3341c8ba8d3be218938955277e8543b2a287","venue_1":"ACM Trans. Graph.","year":"2007","title":"Context-aware textures","authors":"Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-Yi Wei, Baining Guo, Julie Dorsey, Holly E. Rushmeier","author_ids":"2711910, 3230391, 3066387, 1880838, 2420851, 2738456, 1775220, 1690595","abstract":"Interesting textures form on the surfaces of objects as the result of external chemical, mechanical, and biological agents. Simulating these textures is necessary to generate models for realistic image synthesis. The textures formed are progressively variant, with the variations depending on the global and local geometric context. We present a method for capturing progressively varying textures and the relevant context parameters that control them. By relating textures and context parameters, we are able to transfer the textures to novel synthetic objects. We present examples of capturing chemical effects, such as rusting; mechanical effects, such as paint cracking; and biological effects, such as the growth of mold on a surface. We demonstrate a user interface that provides a method for specifying where an object is exposed to external agents. We show the results of complex, geometry-dependent textures evolving on synthetic objects.","cites":"32","conferencePercentile":"32"},{"venue":"ACM Trans. Graph.","id":"6de9bccfd72648be5fe8cc31e4739c6e2ee67e50","venue_1":"ACM Trans. Graph.","year":"2004","title":"Stereological techniques for solid textures","authors":"Robert Jagnow, Julie Dorsey, Holly E. Rushmeier","author_ids":"1818576, 1775220, 1690595","abstract":"We describe the use of traditional stereological methods to synthesize 3D solid textures from 2D images of existing materials. We first illustrate our approach for aggregate materials of spherical particles, and then extend the technique to apply to particles of arbitrary shapes. We demonstrate the effectiveness of the approach with side-by-side comparisons of a real material and a synthetic model with its appearance parameters derived from its physical counterpart. Unlike ad hoc methods for texture synthesis, stereology provides a disciplined, systematic basis for predicting material structure with well-defined assumptions.","cites":"44","conferencePercentile":"21.73913043"},{"venue":"ACM Trans. Graph.","id":"45989abc3f55f7c8107aff7a642116a0bdf3888a","venue_1":"ACM Trans. Graph.","year":"2003","title":"Billboard clouds for extreme model simplification","authors":"Xavier Décoret, Frédo Durand, François X. Sillion, Julie Dorsey","author_ids":"2293423, 1728125, 1708617, 1775220","abstract":"We introduce <i>billboard clouds</i> -- a new approach for extreme simplification in the context of real-time rendering. 3D models are simplified onto a set of planes with texture and transparency maps. We present an optimization approach to build a billboard cloud given a geometric error threshold. After computing an appropriate density function in plane space, a greedy approach is used to select suitable representative planes. A good surface approximation is ensured by favoring planes that are \"nearly tangent\" to the model. This method does not require connectivity information, but instead avoids cracks by projecting primitives onto multiple planes when needed. For extreme simplification, our approach combines the strengths of mesh decimation and image-based impostors. We demonstrate our technique on a large class of models, including smooth manifolds and composite objects.","cites":"93","conferencePercentile":"52.68817204"},{"venue":"ACM Trans. Graph.","id":"c807d3c82c3485b3cacb980305d7b652ca12ec58","venue_1":"ACM Trans. Graph.","year":"1991","title":"Automating the Lexical and Syntactic Design of Graphical User Interfaces: The UofA* UIMS","authors":"Gurminder Singh, Mark Green","author_ids":"7000352, 3081643","abstract":"The primary goal of the UofA* UIMS is to address a key problem with UIMSS: their inability to help in the initial design of user interfaces, Because of this inability, most existing UIMSS require the interface designer to work with low level syntactic and lexical detail, which can be very time-consuming and expensive in terms of effort required. The UofA* approach to this problem is to produce the initial design specification and implementation of the user interface automatically y, and then enable the interface designer to improve its appearance and effectiveness through an interactive refinement process, The interface designer, in this approach, works at the conceptual and semantic levels of the user interface and produces a high-level description of the commands the interface is to support. Based on this description the syntactic and lexical levels of the interface are automatically designed and implemented. This interface can be refined by the designer to improve the resulting interaction with the user. The UofA* UIMS facilitates exploration in interface design by using user's preferences and designer's guidelines as optional inputs. It allows the creation of interfaces in which many different types of syntaxes can coexist.","cites":"25","conferencePercentile":"73.33333333"},{"venue":"ACM Trans. Graph.","id":"0263dda6941177cce85600bf9c449eab7db2fdcc","venue_1":"ACM Trans. Graph.","year":"2002","title":"A procedural approach to authoring solid models","authors":"Barbara Cutler, Julie Dorsey, Leonard McMillan, Matthias Müller, Robert Jagnow","author_ids":"1781349, 1775220, 1748115, 6295817, 1818576","abstract":"We present a procedural approach to authoring layered, solid models. Using a simple scripting language, we define the internal structure of a volume from one or more input meshes. Sculpting and simulation operators are applied within the context of the language to shape and modify the model. Our framework treats simulation as a modeling operator rather than simply as a tool for animation, thereby suggesting a new paradigm for modeling as well as a new level of abstraction for interacting with simulation environments.Capturing real-world effects with standard modeling techniques is extremely challenging. Our key contribution is a concise procedural approach for seamlessly building and modifying complex solid geometry. We present an implementation of our language using a flexible tetrahedral representation. We show a variety of complex objects modeled in our system using tools that interface with finite element method and particle system simulations.","cites":"66","conferencePercentile":"40"},{"venue":"ACM Trans. Graph.","id":"50b6aab9be83dde08dcfa519a8f2b55b0f757800","venue_1":"ACM Trans. Graph.","year":"2005","title":"Texture design using a simplicial complex of morphable textures","authors":"Wojciech Matusik, Matthias Zwicker, Frédo Durand","author_ids":"1752521, 1796846, 1728125","abstract":"We present a system for designing novel textures in the space of textures induced by an input database. We capture the structure of the induced space by a simplicial complex where vertices of the simplices represent input textures. A user can generate new textures by interpolating within individual simplices. We propose a morphable interpolation for textures, which also defines a metric used to build the simplicial complex. To guarantee sharpness in interpolated textures, we enforce histograms of high-frequency content using a novel method for histogram interpolation. We allow users to continuously navigate in the simplicial complex and design new textures using a simple and efficient user interface. We demonstrate the usefulness of our system by integrating it with a 3D texture painting application, where the user interactively designs desired textures.","cites":"83","conferencePercentile":"54.83870968"},{"venue":"ACM Trans. Graph.","id":"fc20827f1890daaba086735c41b48fdf05f3a4a8","venue_1":"ACM Trans. Graph.","year":"1995","title":"Device-Sirected Rendering","authors":"Andrew S. Glassner, Kenneth P. Fishkin, David H. Marimont, Maureen C. Stone","author_ids":"1731467, 1719980, 2433345, 1725665","abstract":"Rendering systems can produce images that include the entire range of visible colors. Imaging hardware, however, can reproduce only a subset of these colors: the device gamut. An image can only be correctly displayed if all of its colors lie inside of the gamut of the target device. Current solutions to this problem are either to correct the scene colors by hand, or to apply gamut mapping techniques to the final image. We propose a methodology called <italic>device-directed rendering</italic> that performs scene color adjustments automatically. Device-directed rendering applies classic minimization techniques to a symbolic representation of the image that describes the relationship of the scene lights and surfaces to the pixel colors. This representation can then be evaluated to produce an image that is guaranteed to be in gamut. Although our primary application has been correcting out-of-gamut colors, this methodology can be generally applied to the problem of adjusting a scene description to accommodate constraints on the output image pixel values.","cites":"1","conferencePercentile":"10.71428571"},{"venue":"ACM Trans. Graph.","id":"308ab33ea1519911045a50aa019b9ccc292b33da","venue_1":"ACM Trans. Graph.","year":"2013","title":"Analyzing growing plants from 4D point cloud data","authors":"Yangyan Li, Xiaochen Fan, Niloy J. Mitra, Daniel A. Chamovitz, Daniel Cohen-Or, Baoquan Chen","author_ids":"1920864, 2061063, 1710455, 8739874, 1701009, 1748939","abstract":"Studying growth and development of plants is of central importance in botany. Current quantitative are either limited to tedious and sparse manual measurements, or coarse image-based 2D measurements. Availability of cheap and portable 3D acquisition devices has the potential to automate this process and easily provide scientists with volumes of accurate data, at a scale much beyond the realms of existing methods. However, during their development, plants grow new parts (e.g., vegetative buds) and bifurcate to different components --- violating the central incompressibility assumption made by existing acquisition algorithms, which makes these algorithms unsuited for analyzing growth. We introduce a framework to study plant growth, particularly focusing on accurate localization and tracking topological events like budding and bifurcation. This is achieved by a novel forward-backward analysis, wherein we track robustly detected plant components back in time to ensure correct spatio-temporal event detection using a locally adapting threshold. We evaluate our approach on several groups of time lapse scans, often ranging from days to weeks, on a diverse set of plant species and use the results to animate static virtual plants or directly attach them to physical simulators.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"4488548631a75e2923160092d9031c6757d52980","venue_1":"ACM Trans. Graph.","year":"2006","title":"A compact factored representation of heterogeneous subsurface scattering","authors":"Pieter Peers, Karl vom Berge, Wojciech Matusik, Ravi Ramamoorthi, Jason Lawrence, Szymon Rusinkiewicz, Philip Dutré","author_ids":"1808270, 3268867, 1752521, 1752236, 1694005, 7723706, 1699581","abstract":"Many translucent materials exhibit heterogeneous subsurface scattering, which arises from complex internal structures. The acquisition and representation of these scattering functions is a complex problem that has been only partially addressed in previous techniques. Unlike homogeneous materials, the spatial component of heterogeneous subsurface scattering can vary arbitrarily over surface locations. Storing the spatial component without compression leads to impractically large datasets. In this paper, we address the problem of acquiring and compactly representing the spatial component of heterogeneous subsurface scattering functions. We propose a material model based on matrix factorization that can be mapped onto arbitrary geometry, and, due to its compact form, can be incorporated into most visualization systems with little overhead. We present results of several real-world datasets that are acquired using a projector and a digital camera.","cites":"47","conferencePercentile":"37.5"},{"venue":"ACM Trans. Graph.","id":"92621b4b961b30331e15ec759fcfd61d4ec55209","venue_1":"ACM Trans. Graph.","year":"2013","title":"Layered analysis of irregular facades via symmetry maximization","authors":"Hao Zhang, Kai Xu, Wei Jiang, Jinjie Lin, Daniel Cohen-Or, Baoquan Chen","author_ids":"1682058, 1723225, 4344679, 2817530, 1701009, 1748939","abstract":"We present an algorithm for <i>hierarchical</i> and <i>layered</i> analysis of <i>irregular</i> facades, seeking a high-level understanding of facade structures. By introducing layering into the analysis, we no longer view a facade as a flat structure, but allow it to be structurally separated into depth layers, enabling more compact and natural interpretations of building facades. Computationally, we perform a symmetry-driven search for an optimal hierarchical decomposition defined by split and layering operations applied to an input facade. The objective is <i>symmetry maximization</i>, i.e., to maximize the sum of symmetry of the substructures resulting from recursive decomposition. To this end, we propose a novel <i>integral symmetry</i> measure, which behaves well at both ends of the symmetry spectrum by accounting for all partial symmetries in a discrete structure. Our analysis results in a structural representation, which can be utilized for structural editing and exploration of building facades.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"9a50d5f34f999ac75daaeba2e7db281832584049","venue_1":"ACM Trans. Graph.","year":"2015","title":"BendFields: Regularized Curvature Fields from Rough Concept Sketches","authors":"Emmanuel Iarussi, David Bommes, Adrien Bousseau","author_ids":"1859777, 1825432, 2149814","abstract":"Designers frequently draw curvature lines to convey bending of smooth surfaces in concept sketches. We present a method to extrapolate curvature lines in a rough concept sketch, recovering the intended 3D curvature field and surface normal at each pixel of the sketch. This 3D information allows to enrich the sketch with 3D-looking shading and texturing.\n We first introduce the concept of <i>regularized curvature lines</i> that model the lines designers draw over curved surfaces, encompassing curvature lines and their extension as geodesics over flat or umbilical regions. We build on this concept to define the orthogonal cross field that assigns two regularized curvature lines to each point of a 3D surface. Our algorithm first estimates the projection of this cross field in the drawing, which is nonorthogonal due to foreshortening. We formulate this estimation as a scattered interpolation of the strokes drawn in the sketch, which makes our method robust to sketchy lines that are typical for design sketches. Our interpolation relies on a novel smoothness energy that we derive from our definition of regularized curvature lines. Optimizing this energy subject to the stroke constraints produces a dense nonorthogonal 2D cross field which we then lift to 3D by imposing orthogonality. Thus, one central concept of our approach is the generalization of existing cross field algorithms to the nonorthogonal case.\n We demonstrate our algorithm on a variety of concept sketches with various levels of sketchiness. We also compare our approach with existing work that takes clean vector drawings as input.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"5a52fa394a1d3100a7c4c038ab8cd6a5e19f9f41","venue_1":"ACM Trans. Graph.","year":"2006","title":"Analysis of human faces using a measurement-based skin reflectance model","authors":"Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, Markus H. Gross","author_ids":"1784306, 1752521, 1701371, 3083909, 2446336, 3211818, 3113446, 4083586, 3138665, 1730025, 1743207","abstract":"We have measured 3D face geometry, skin reflectance, and subsurface scattering using custom-built devices for 149 subjects of varying age, gender, and race. We developed a novel skin reflectance model whose parameters can be estimated from measurements. The model decomposes the large amount of measured skin data into a spatially-varying analytic BRDF, a diffuse albedo map, and diffuse subsurface scattering. Our model is intuitive, physically plausible, and -- since we do not use the original measured data -- easy to edit as well. High-quality renderings come close to reproducing real photographs. The analysis of the model parameters for our sample population reveals variations according to subject age, gender, skin type, and external factors (e.g., sweat, cold, or makeup). Using our statistics, a user can edit the overall appearance of a face (e.g., changing skin type and age) or change small-scale features using texture synthesis (e.g., adding moles and freckles). We are making the collected statistics publicly available to the research community for applications in face synthesis and analysis.","cites":"104","conferencePercentile":"78.24074074"},{"venue":"ACM Trans. Graph.","id":"68a677a326a290a82bc08686465019414ebe1d98","venue_1":"ACM Trans. Graph.","year":"2014","title":"ImageSpirit: Verbal Guided Image Parsing","authors":"Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Vibhav Vineet, Paul Sturgess, Nigel Crook, Niloy J. Mitra, Philip H. S. Torr","author_ids":"3184935, 2202027, 1730822, 1683102, 3274976, 1880894, 1710455, 7194119","abstract":"Humans describe images in terms of nouns and adjectives while algorithms operate on images represented as sets of pixels. Bridging this gap between how humans would like to access images versus their typical representation is the goal of image parsing, which involves assigning object and attribute labels to pixels. In this article we propose treating nouns as object labels and adjectives as visual attribute labels. This allows us to formulate the image parsing problem as one of jointly estimating per-pixel object and attribute labels from a set of training images. We propose an efficient (interactive time) solution. Using the extracted labels as handles, our system empowers a user to verbally refine the results. This enables hands-free parsing of an image into pixel-wise object/attribute labels that correspond to human semantics. Verbally selecting objects of interest enables a novel and natural interaction modality that can possibly be used to interact with new generation devices (e.g., smartphones, Google Glass, livingroom devices). We demonstrate our system on a large number of real-world images with varying complexity. To help understand the trade-offs compared to traditional mouse-based interactions, results are reported for both a large-scale quantitative evaluation and a user study.","cites":"12","conferencePercentile":"73.45679012"},{"venue":"ACM Trans. Graph.","id":"b17179ddbc2efaff6f0999868dec2447132bd087","venue_1":"ACM Trans. Graph.","year":"2006","title":"Natural video matting using camera arrays","authors":"Neel Joshi, Wojciech Matusik, Shai Avidan","author_ids":"2641664, 1752521, 2740179","abstract":"We present an algorithm and a system for high-quality natural video matting using a camera array. The system uses high frequencies present in natural scenes to compute mattes by creating a synthetic aperture image that is focused on the foreground object, which reduces the variance of pixels reprojected from the foreground while increasing the variance of pixels reprojected from the background. We modify the standard matting equation to work directly with variance measurements and show how these statistics can be used to construct a trimap that is later upgraded to an alpha matte. The entire process is completely automatic, including an automatic method for focusing the synthetic aperture image on the foreground object and an automatic method to compute the trimap and the alpha matte. The proposed algorithm is very efficient and has a per-pixel running time that is linear in the number of cameras. Our current system runs at several frames per second, and we believe that it is the first system capable of computing high-quality alpha mattes at near real-time rates without the use of active illumination or special backgrounds.","cites":"50","conferencePercentile":"41.2037037"},{"venue":"ACM Trans. Graph.","id":"4f90f3864b9fe1d09ca36ec69d77a8ba9c16a8a0","venue_1":"ACM Trans. Graph.","year":"2006","title":"Inverse shade trees for non-parametric material representation and editing","authors":"Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech Matusik, Hanspeter Pfister, Ravi Ramamoorthi, Szymon Rusinkiewicz","author_ids":"1694005, 2822153, 1762101, 1752521, 1701371, 1752236, 7723706","abstract":"Recent progress in the measurement of surface reflectance has created a demand for non-parametric appearance representations that are accurate, compact, and easy to use for rendering. Another crucial goal, which has so far received little attention, is <i>editability:</i> for practical use, we must be able to change both the directional and spatial behavior of surface reflectance (e.g., making one material shinier, another more anisotropic, and changing the spatial \"texture maps\" indicating where each material appears). We introduce an Inverse <i>Shade Tree</i> framework that provides a general approach to estimating the \"leaves\" of a user-specified shade tree from high-dimensional measured datasets of appearance. These leaves are sampled 1- and 2-dimensional functions that capture both the directional behavior of individual materials and their spatial mixing patterns. In order to compute these shade trees automatically, we map the problem to matrix factorization and introduce a flexible new algorithm that allows for constraints such as non-negativity, sparsity, and energy conservation. Although we cannot infer every type of shade tree, we demonstrate the ability to reduce multi-gigabyte measured datasets of the Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) into a compact representation that may be edited in real time.","cites":"104","conferencePercentile":"78.24074074"},{"venue":"ACM Trans. Graph.","id":"312fecf1ae2ede240779b9d7124eeb56b9ab740d","venue_1":"ACM Trans. Graph.","year":"2006","title":"A statistical model for synthesis of detailed facial geometry","authors":"Aleksey Golovinskiy, Wojciech Matusik, Hanspeter Pfister, Szymon Rusinkiewicz, Thomas A. Funkhouser","author_ids":"3139470, 1752521, 1701371, 7723706, 1807080","abstract":"Detailed surface geometry contributes greatly to the visual realism of 3D face models. However, acquiring high-resolution face geometry is often tedious and expensive. Consequently, most face models used in games, virtual reality, or computer vision look unrealistically smooth. In this paper, we introduce a new statistical technique for the analysis and synthesis of small three-dimensional facial features, such as wrinkles and pores. We acquire high-resolution face geometry for people across a wide range of ages, genders, and races. For each scan, we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces. Then, we analyze the resulting displacement maps using the texture analysis/synthesis framework of Heeger and Bergen, adapted to capture statistics that vary spatially across a face. Finally, we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects. We demonstrate the effectiveness of this method in several applications, including analysis of facial texture in subjects with different ages and genders, interpolation between high-resolution face scans, adding detail to low-resolution face scans, and adjusting the apparent age of faces. In all cases, we are able to re-produce fine geometric details consistent with those observed in high resolution scans.","cites":"35","conferencePercentile":"26.85185185"},{"venue":"ACM Trans. Graph.","id":"247863dc199aebc5d20a6111ef1c743785ccc8ed","venue_1":"ACM Trans. Graph.","year":"2005","title":"Defocus video matting","authors":"Morgan McGuire, Wojciech Matusik, Hanspeter Pfister, John F. Hughes, Frédo Durand","author_ids":"1708541, 1752521, 1701371, 2057964, 1728125","abstract":"Video matting is the process of pulling a high-quality alpha matte and foreground from a video sequence. Current techniques require either a known background (e.g., a blue screen) or extensive user interaction (e.g., to specify known foreground and background elements). The matting problem is generally under-constrained, since not enough information has been collected at capture time. We propose a novel, fully autonomous method for pulling a matte using multiple synchronized video streams that share a point of view but differ in their plane of focus. The solution is obtained by directly minimizing the error in filter-based image formation equations, which are over-constrained by our rich data stream. Our system solves the fully dynamic video matting problem without user assistance: both the foreground and background may be high frequency and have dynamic content, the foreground may resemble the background, and the scene is lit by natural (as opposed to polarized or collimated) illumination.","cites":"84","conferencePercentile":"56.0483871"},{"venue":"ACM Trans. Graph.","id":"0d076e616c0846b5510dabb4b2df2fbd09c2230f","venue_1":"ACM Trans. Graph.","year":"2003","title":"Shadow silhouette maps","authors":"Pradeep Sen, Mike Cammarano, Pat Hanrahan","author_ids":"2258791, 3314843, 4982303","abstract":"The most popular techniques for interactive rendering of hard shadows are <i>shadow maps</i> and <i>shadow volumes</i>. Shadow maps work well in regions that are completely in light or in shadow but result in objectionable artifacts near shadow boundaries. In contrast, shadow volumes generate precise shadow boundaries but require high fill rates. In this paper, we propose the method of <i>silhouette maps</i>, in which a shadow depth map is augmented by storing the location of points on the geometric silhouette. This allows the shader to construct a piecewise linear approximation to the true shadow silhouette, improving the visual quality over the piecewise constant approximation of conventional shadow maps. We demonstrate an implementation of our approach running on programmable graphics hardware in real-time.","cites":"75","conferencePercentile":"37.6344086"},{"venue":"ACM Trans. Graph.","id":"44f0b63ff297fede8486942b3d74a42be53c8049","venue_1":"ACM Trans. Graph.","year":"2012","title":"Foveated 3D graphics","authors":"Brian K. Guenter, Mark Finch, Steven Drucker, Desney Tan, John Snyder","author_ids":"1842844, 3354969, 2909389, 2339622, 6314473","abstract":"We exploit the falloff of acuity in the visual periphery to accelerate graphics computation by a factor of 5-6 on a desktop HD display (1920x1080). Our method tracks the user's gaze point and renders three image layers around it at progressively higher angular size but lower sampling rate. The three layers are then magnified to display resolution and smoothly composited. We develop a general and efficient antialiasing algorithm easily retrofitted into existing graphics code to minimize \"twinkling\" artifacts in the lower-resolution layers. A standard psychophysical model for acuity falloff assumes that minimum detectable angular size increases linearly as a function of eccentricity. Given the slope characterizing this falloff, we automatically compute layer sizes and sampling rates. The result looks like a full-resolution image but reduces the number of pixels shaded by a factor of 10-15.\n We performed a user study to validate these results. It identifies two levels of foveation quality: a more conservative one in which users reported foveated rendering quality as equivalent to or better than non-foveated when directly shown both, and a more aggressive one in which users were unable to correctly label as increasing or decreasing a short quality progression relative to a high-quality foveated reference. Based on this user study, we obtain a slope value for the model of 1.32-1.65 arc minutes per degree of eccentricity. This allows us to predict two future advantages of foveated rendering: (1) bigger savings with larger, sharper displays than exist currently (e.g. 100 times speedup at a field of view of 70&#176; and resolution matching foveal acuity), and (2) a roughly linear (rather than quadratic or worse) increase in rendering cost with increasing display field of view, for planar displays at a constant sharpness.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"04b75f63e3f53e440641bee42fe8b07aacad3493","venue_1":"ACM Trans. Graph.","year":"2003","title":"Light scattering from human hair fibers","authors":"Steve Marschner, Henrik Wann Jensen, Mike Cammarano, Steve Worley, Pat Hanrahan","author_ids":"2593798, 1730025, 3314843, 2964196, 4982303","abstract":"Light scattering from hair is normally simulated in computer graphics using Kajiya and Kay's classic phenomenological model. We have made new measurements of scattering from individual hair fibers that exhibit visually significant effects not predicted by Kajiya and Kay's model. Our measurements go beyond previous hair measurements by examining out-of-plane scattering, and together with this previous work they show a multiple specular highlight and variation in scattering with rotation about the fiber axis. We explain the sources of these effects using a model of a hair fiber as a transparent elliptical cylinder with an absorbing interior and a surface covered with tilted scales. Based on an analytical scattering function for a circular cylinder, we propose a practical shading model for hair that qualitatively matches the scattering behavior shown in the measurements. In a comparison between a photograph and rendered images, we demonstrate the new model's ability to match the appearance of real hair.","cites":"133","conferencePercentile":"70.96774194"},{"venue":"ACM Trans. Graph.","id":"c17febb88fa23485352ad71584ce02b1b7d3e3f8","venue_1":"ACM Trans. Graph.","year":"2010","title":"Context-based search for 3D models","authors":"Matthew Fisher, Pat Hanrahan","author_ids":"2676553, 4982303","abstract":"Large corpora of 3D models, such as Google 3D Warehouse, are now becoming available on the web. It is possible to search these databases using a keyword search. This makes it possible for designers to easily include existing content into new scenes. In this paper, we describe a method for context-based search of 3D scenes. We first downloaded a large set of scene graphs from Google 3D Warehouse. These scene graphs were segmented into individual objects. We also extracted tags from the names of the models. Given the object shape, tags, and spatial relationship between pairs of objects, we can predict the strength of a relationship between a candidate model and an existing object in the scene. Using this function, we can perform context-based queries. The user specifies a region in the scene they are modeling using a 3D bounding box, and the system returns a list of related objects. We show that context-based queries perform better than keyword queries alone, and that without any keywords our algorithm still returns a relevant set of models.","cites":"48","conferencePercentile":"82.16374269"},{"venue":"ACM Trans. Graph.","id":"0a278b97889659b0c831826d64f6c3ce0332cd73","venue_1":"ACM Trans. Graph.","year":"2009","title":"DiagSplit: parallel, crack-free, adaptive tessellation for micropolygon rendering","authors":"Matthew Fisher, Kayvon Fatahalian, Solomon Boulos, Kurt Akeley, William R. Mark, Pat Hanrahan","author_ids":"2676553, 2789576, 3164352, 3221156, 1913999, 4982303","abstract":"We present DiagSplit, a parallel algorithm for adaptively tessellating displaced parametric surfaces into high-quality, crack-free micropolygon meshes. DiagSplit modifies the split-dice tessellation algorithm to allow splits along non-isoparametric directions in the surface's parametric domain, and uses a dicing scheme that supports unique tessellation factors for each subpatch edge. Edge tessellation factors are computed using only information local to subpatch edges. These modifications allow all subpatches generated by DiagSplit to be processed independently without introducing T-junctions or mesh cracks and without incurring the tessellation overhead of binary dicing. We demonstrate that DiagSplit produces output that is better (in terms of image quality and number of micropolygons produced) than existing parallel tessellation schemes, and as good as highly adaptive split-dice implementations that are less amenable to parallelization.","cites":"26","conferencePercentile":"38.95027624"},{"venue":"ACM Trans. Graph.","id":"7fb6fd4300d56e71469b64d9ffc4b2034431ae46","venue_1":"ACM Trans. Graph.","year":"2012","title":"Understanding and improving the realism of image composites","authors":"Su Xue, Aseem Agarwala, Julie Dorsey, Holly E. Rushmeier","author_ids":"2204029, 1696487, 1775220, 1690595","abstract":"Compositing is one of the most commonly performed operations in computer graphics. A realistic composite requires adjusting the appearance of the foreground and background so that they appear compatible; unfortunately, this task is challenging and poorly understood. We use statistical and visual perception experiments to study the realism of image composites. First, we evaluate a number of standard 2D image statistical measures, and identify those that are most significant in determining the realism of a composite. Then, we perform a human subjects experiment to determine how the changes in these key statistics influence human judgements of composite realism. Finally, we describe a data-driven algorithm that automatically adjusts these statistical measures in a foreground to make it more compatible with its background in a composite. We show a number of compositing results, and evaluate the performance of both our algorithm and previous work with a human subjects study.","cites":"15","conferencePercentile":"42.67676768"},{"venue":"ACM Trans. Graph.","id":"1daa9321768295d39d562f69d4347739f6f8931e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Autocomplete hand-drawn animations","authors":"Jun Xing, Li-Yi Wei, Takaaki Shiratori, Koji Yatani","author_ids":"8262577, 2420851, 2463857, 3260704","abstract":"Hand-drawn animation is a major art form and communication medium, but can be challenging to produce. We present a system to help people create frame-by-frame animations through manual sketches. We design our interface to be minimalistic: it contains only a canvas and a few controls. When users draw on the canvas, our system silently analyzes all past sketches and predicts what might be drawn in the future across spatial locations and temporal frames. The interface also offers suggestions to beautify existing drawings. Our system can reduce manual workload and improve output quality without compromising natural drawing flow and control: users can accept, ignore, or modify such predictions visualized on the canvas by simple gestures. Our key idea is to extend the local similarity method in [Xing et al. 2014], which handles only low-level spatial repetitions such as hatches within a single frame, to a global similarity that can capture high-level structures across multiple frames such as dynamic objects. We evaluate our system through a preliminary user study and confirm that it can enhance both users' objective performance and subjective satisfaction.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"2df268efd1b6740ab20f6e862fc02a7dc6ee8ef1","venue_1":"ACM Trans. Graph.","year":"2011","title":"Physically-based interactive bi-scale material design","authors":"Hongzhi Wu, Julie Dorsey, Holly E. Rushmeier","author_ids":"1880838, 1775220, 1690595","abstract":"We present the first physically-based interactive system to facilitate the appearance design at <i>different scales</i> consistently, through manipulations of both small-scale geometry and materials. The core of our system is a novel reflectance filtering algorithm, which rapidly computes the large-scale appearance from small-scale details, by exploiting the low-rank structures of the Bidirectional Visible Normal Distribution Function and pre-rotated BRDFs in the matrix formulation of our rendering problem. Our algorithm is three orders of magnitude faster than a ground-truth method. We demonstrate various editing results of different small-scale geometry with analytical and measured BRDFs. In addition, we show the applications of our system to physical realization of appearance, as well as modeling of real-world materials using very sparse measurements.","cites":"4","conferencePercentile":"5.789473684"},{"venue":"ACM Trans. Graph.","id":"7f24577bf972612a4d909407cd221aaca3db6acd","venue_1":"ACM Trans. Graph.","year":"2003","title":"A data-driven reflectance model","authors":"Wojciech Matusik, Hanspeter Pfister, Matthew Brand, Leonard McMillan","author_ids":"1752521, 1701371, 1740065, 1748115","abstract":"We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.","cites":"325","conferencePercentile":"92.47311828"},{"venue":"ACM Trans. Graph.","id":"61608f5a57007fbba2b0ed201fc3e2c61acddbd9","venue_1":"ACM Trans. Graph.","year":"2009","title":"GRAMPS: A programming model for graphics pipelines","authors":"Jeremy Sugerman, Kayvon Fatahalian, Solomon Boulos, Kurt Akeley, Pat Hanrahan","author_ids":"3200943, 2789576, 3164352, 3221156, 4982303","abstract":"We introduce GRAMPS, a programming model that generalizes concepts from modern real-time graphics pipelines by exposing a model of execution containing both fixed-function and application-programmable processing stages that exchange data via queues. GRAMPS allows the number, type, and connectivity of these processing stages to be defined by software, permitting arbitrary processing pipelines or even processing graphs. Applications achieve high performance using GRAMPS by expressing advanced rendering algorithms as custom pipelines, then using the pipeline as a rendering engine. We describe the design of GRAMPS, then evaluate it by implementing three pipelines, that is, Direct3D, a ray tracer, and a hybridization of the two, and running them on emulations of two different GRAMPS implementations: a traditional GPU-like architecture and a CPU-like multicore architecture. In our tests, our GRAMPS schedulers run our pipelines with 500 to 1500KB of queue usage at their peaks.","cites":"32","conferencePercentile":"48.06629834"},{"venue":"ACM Trans. Graph.","id":"6516a7337a096335db44d86c3db152d68e525541","venue_1":"ACM Trans. Graph.","year":"2004","title":"3D TV: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes","authors":"Wojciech Matusik, Hanspeter Pfister","author_ids":"1752521, 1701371","abstract":"Three-dimensional TV is expected to be the next revolution in the history of television. We implemented a 3D TV prototype system with real-time acquisition, transmission, and 3D display of dynamic scenes. We developed a distributed, scalable architecture to manage the high computation and bandwidth demands. Our system consists of an array of cameras, clusters of network-connected PCs, and a multi-projector 3D display. Multiple video streams are individually encoded and sent over a broadband network to the display. The 3D display shows high-resolution (1024 &#215; 768) stereoscopic color images for multiple viewpoints without special glasses. We implemented systems with rear-projection and front-projection lenticular screens. In this paper, we provide a detailed overview of our 3D TV system, including an examination of design choices and tradeoffs. We present the calibration and image alignment procedures that are necessary to achieve good image quality. We present qualitative results and some early user feedback. We believe this is the first real-time end-to-end 3D TV system with enough views and resolution to provide a truly immersive 3D experience.","cites":"213","conferencePercentile":"82.60869565"},{"venue":"ACM Trans. Graph.","id":"3c53638c74ac08bb1f3f2b51e360ac53b9479a0a","venue_1":"ACM Trans. Graph.","year":"2014","title":"Quality-driven poisson-guided autoscanning","authors":"Shihao Wu, Wei Sun, Pinxin Long, Hui Huang, Daniel Cohen-Or, Minglun Gong, Oliver Deussen, Baoquan Chen","author_ids":"8175431, 1712625, 3089602, 1927737, 1701009, 2834751, 1850438, 1748939","abstract":"We present a quality-driven, Poisson-guided autonomous scanning method. Unlike previous scan planning techniques, we do not aim to minimize the number of scans needed to cover the object's surface, but rather to ensure the high quality scanning of the model. This goal is achieved by placing the scanner at strategically selected Next-Best-Views (NBVs) to ensure progressively capturing the geometric details of the object, until both completeness and high fidelity are reached. The technique is based on the analysis of a Poisson field and its geometric relation with an input scan. We generate a confidence map that reflects the quality/fidelity of the estimated Poisson iso-surface. The confidence map guides the generation of a viewing vector field, which is then used for computing a set of NBVs. We applied the algorithm on two different robotic platforms, a PR2 mobile robot and a one-arm industry robot. We demonstrated the advantages of our method through a number of autonomous high quality scannings of complex physical objects, as well as performance comparisons against state-of-the-art methods.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"2264baf912786e105f697d5704c0a896bc89842a","venue_1":"ACM Trans. Graph.","year":"2004","title":"A signal-processing framework for reflection","authors":"Ravi Ramamoorthi, Pat Hanrahan","author_ids":"1752236, 4982303","abstract":"We present a signal-processing framework for analyzing the reflected light field from a homogeneous convex curved surface under distant illumination. This analysis is of theoretical interest in both graphics and vision and is also of practical importance in many computer graphics problems---for instance, in determining lighting distributions and bidirectional reflectance distribution functions (BRDFs), in rendering with environment maps, and in image-based rendering. It is well known that under our assumptions, the reflection operator behaves qualitatively like a convolution. In this paper, we formalize these notions, showing that the reflected light field can be thought of in a precise quantitative way as obtained by convolving the lighting and BRDF, i.e. by filtering the incident illumination using the BRDF. Mathematically, we are able to express the frequency-space coefficients of the reflected light field as a product of the spherical harmonic coefficients of the illumination and the BRDF. These results are of practical importance in determining the well-posedness and conditioning of problems in &#60;i> inverse rendering&#60;/i>---estimation of BRDF and lighting parameters from real photographs. Furthermore, we are able to derive analytic formulae for the spherical harmonic coefficients of many common BRDF and lighting models. From this formal analysis, we are able to determine precise conditions under which estimation of BRDFs and lighting distributions are well posed and well-conditioned. Our mathematical analysis also has implications for &#60;i>forward rendering&#60;/i>---especially the efficient rendering of objects under complex lighting conditions specified by environment maps. The results, especially the analytic formulae derived for Lambertian surfaces, are also relevant in &#60;i>computer vision&#60;/i> in the areas of recognition, photometric stereo and structure from motion.","cites":"58","conferencePercentile":"27.7173913"},{"venue":"ACM Trans. Graph.","id":"3b631d9c54c75be9a30bb7c6564b0036417d40ff","venue_1":"ACM Trans. Graph.","year":"2009","title":"Ray casting of multiple volumetric datasets with polyhedral boundaries on manycore GPUs","authors":"Bernhard Kainz, Markus Grabner, Alexander Bornik, Stefan Hauswiesner, Judith Muehl, Dieter Schmalstieg","author_ids":"2015193, 2173443, 1742270, 1774095, 2688598, 1742819","abstract":"We present a new GPU-based rendering system for ray casting of multiple volumes. Our approach supports a large number of volumes, complex translucent and concave polyhedral objects as well as CSG intersections of volumes and geometry in any combination. The system (including the rasterization stage) is implemented entirely in CUDA, which allows full control of the memory hierarchy, in particular access to high bandwidth and low latency shared memory. High depth complexity, which is problematic for conventional approaches based on depth peeling, can be handled successfully. As far as we know, our approach is the first framework for multivolume rendering which provides interactive frame rates when concurrently rendering more than 50 arbitrarily overlapping volumes on current graphics hardware.","cites":"18","conferencePercentile":"23.48066298"},{"venue":"ACM Trans. Graph.","id":"22534750dbf90b05f07941f4b369adafe1df60d3","venue_1":"ACM Trans. Graph.","year":"2012","title":"Softshell: dynamic scheduling on GPUs","authors":"Markus Steinberger, Bernhard Kainz, Bernhard Kerbl, Stefan Hauswiesner, Michael Kenzel, Dieter Schmalstieg","author_ids":"1681353, 2015193, 2454128, 1774095, 2072046, 1742819","abstract":"In this paper we present Softshell, a novel execution model for devices composed of multiple processing cores operating in a single instruction, multiple data fashion, such as graphics processing units (GPUs). The Softshell model is intuitive and more flexible than the kernel-based adaption of the stream processing model, which is currently the dominant model for general purpose GPU computation. Using the Softshell model, algorithms with a relatively low local degree of parallelism can execute efficiently on massively parallel architectures. Softshell has the following distinct advantages: (<b>1</b>) work can be dynamically issued directly on the device, eliminating the need for synchronization with an external source, <i>i.e</i>., the CPU; (<b>2</b>) its three-tier dynamic scheduler supports arbitrary scheduling strategies, including dynamic priorities and real-time scheduling; and (<b>3</b>) the user can influence, pause, and cancel work already submitted for parallel execution. The Softshell processing model thus brings capabilities to GPU architectures that were previously only known from operating-system designs and reserved for CPU programming. As a proof of our claims, we present a publicly available implementation of the Softshell processing model realized on top of CUDA. The benchmarks of this implementation demonstrate that our processing model is easy to use and also performs substantially better than the state-of-the-art kernel-based processing model for problems that have been difficult to parallelize in the past.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"0c75806bfe62a119e1aa580327c2f8db01b898aa","venue_1":"ACM Trans. Graph.","year":"2014","title":"Whippletree: task-based scheduling of dynamic workloads on the GPU","authors":"Markus Steinberger, Michael Kenzel, Pedro Boechat, Bernhard Kerbl, Mark Dokter, Dieter Schmalstieg","author_ids":"1681353, 2072046, 2670918, 2454128, 2099071, 1742819","abstract":"In this paper, we present Whippletree, a novel approach to scheduling dynamic, irregular workloads on the GPU. We introduce a new programming model which offers the simplicity and expressiveness of task-based parallelism while retaining all aspects of the multi-level execution hierarchy essential to unlocking the full potential of a modern GPU. At the same time, our programming model lends itself to efficient implementation on the SIMD-based architecture typical of a current GPU. We demonstrate the practical utility of our model by providing a reference implementation on top of current CUDA hardware. Furthermore, we show that our model compares favorably to traditional approaches in terms of both performance as well as the range of applications that can be covered. We demonstrate the benefits of our model for recursive Reyes rendering, procedural geometry generation and volume rendering with concurrent irradiance caching.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"7685df27b12f71c9c743af7b59576f3ff51b8a89","venue_1":"ACM Trans. Graph.","year":"2016","title":"Representing and scheduling procedural generation using operator graphs","authors":"Pedro Boechat, Mark Dokter, Michael Kenzel, Hans-Peter Seidel, Dieter Schmalstieg, Markus Steinberger","author_ids":"2670918, 2099071, 2072046, 1746884, 1742819, 1681353","abstract":"In this paper, we present the concept of operator graph scheduling for high performance procedural generation on the graphics processing unit (GPU). The operator graph forms an intermediate representation that describes all possible operations and objects that can arise during a specific procedural generation. While previous methods have focused on parallelizing a specific procedural approach, the operator graph is applicable to all procedural generation methods that can be described by a graph, such as L-systems, shape grammars, or stack based generation methods. Using the operator graph, we show that all partitions of the graph correspond to possible ways of scheduling a procedural generation on the GPU, including the scheduling strategies of previous work. As the space of possible partitions is very large, we describe three search heuristics, aiding an optimizer in finding the fastest valid schedule for any given operator graph. The best partitions found by our optimizer increase performance of 8 to 30x over the previous state of the art in GPU shape grammar and L-system generation.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"2d40ad54d78a1c2a26aedca972b144bf2f5ce85f","venue_1":"ACM Trans. Graph.","year":"2004","title":"Triple product wavelet integrals for all-frequency relighting","authors":"Ren Ng, Ravi Ramamoorthi, Pat Hanrahan","author_ids":"2180699, 1752236, 4982303","abstract":"This paper focuses on efficient rendering based on pre-computed light transport, with realistic materials and shadows under all-frequency direct lighting such an environment maps. The basic difficulty is representation and computation in the 6D space of light direction, view direction, and surface position. While image-based and synthetic methods for real-time rendering have been proposed, they do not scale to high sampling rates with variation of both lighting and viewpoint. Current approaches are therefore limited to lower dimensionality (only lighting or viewpoint variation, not both) or lower sampling rates (low frequency lighting and materials). We propose a new mathematical and computational analysis of pre-computed light transport. We use <i>factored</i> forms, separately pre-computing and representing visibility and material properties. Rendering then requires computing <i>triple product integrals</i> at each vertex, involving the lighting, visibility and BRDF. Our main contribution is a general analysis of these triple product integrals, which are likely to have broad applicability in computer graphics and numerical analysis. We first determine the computational complexity in a number of bases like point samples, spherical harmonics and wavelets. We then give efficient linear and sublinear-time algorithms for Haar wavelets, incorporating non-linear wavelet approximation of lighting and BRDFs. Practically, we demonstrate rendering of images under new lighting and viewing conditions in a few seconds, significantly faster than previous techniques.","cites":"153","conferencePercentile":"75"},{"venue":"ACM Trans. Graph.","id":"cc4d20accca6580fce7a913faecb20037d8b9a1e","venue_1":"ACM Trans. Graph.","year":"2013","title":"PolyCut: monotone graph-cuts for PolyCube base-complex construction","authors":"Marco Livesu, Nicholas Vining, Alla Sheffer, James Gregson, Riccardo Scateni","author_ids":"3243460, 1983439, 3354923, 2200882, 3204853","abstract":"PolyCubes, or orthogonal polyhedra, are useful as parameterization base-complexes for various operations in computer graphics. However, computing quality PolyCube base-complexes for general shapes, providing a good trade-off between mapping distortion and singularity counts, remains a challenge. Our work improves on the state-of-the-art in PolyCube computation by adopting a graph-cut inspired approach. We observe that, given an arbitrary input mesh, the computation of a suitable PolyCube base-complex can be formulated as associating, or labeling, each input mesh triangle with one of six signed principal axis directions. Most of the criteria for a desirable PolyCube labeling can be satisfied using a multi-label graph-cut optimization with suitable <i>local</i> unary and pairwise terms. However, the highly constrained nature of PolyCubes, imposed by the need to align each chart with one of the principal axes, enforces additional <i>global</i> constraints that the labeling must satisfy. To enforce these constraints, we develop a constrained discrete optimization technique, <i>PolyCut</i>, which embeds a graph-cut multi-label optimization within a hill-climbing local search framework that looks for solutions that minimize the cut energy while satisfying the global constraints. We further optimize our generated PolyCube base-complexes through a combination of distortion-minimizing deformation, followed by a labeling update and a final PolyCube parameterization step. Our <i>PolyCut</i> formulation captures the desired properties of a PolyCube base-complex, balancing parameterization distortion against singularity count, and produces demonstrably better PolyCube base-complexes then previous work.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"323aca9e3e0e20272bb9d8c2b71bc48585d37262","venue_1":"ACM Trans. Graph.","year":"2003","title":"All-frequency shadows using non-linear wavelet lighting approximation","authors":"Ren Ng, Ravi Ramamoorthi, Pat Hanrahan","author_ids":"2180699, 1752236, 4982303","abstract":"We present a method, based on pre-computed light transport, for real-time rendering of objects under all-frequency, time-varying illumination represented as a high-resolution environment map. Current techniques are limited to small area lights, with sharp shadows, or large low-frequency lights, with very soft shadows. Our main contribution is to approximate the environment map in a wavelet basis, keeping only the largest terms (this is known as a <i>non-linear approximation</i>). We obtain further compression by encoding the light transport matrix sparsely but accurately in the same basis. Rendering is performed by multiplying a sparse light vector by a sparse transport matrix, which is very fast. For accurate rendering, using non-linear wavelets is an order of magnitude faster than using linear spherical harmonics, the current best technique.","cites":"229","conferencePercentile":"84.94623656"},{"venue":"ACM Trans. Graph.","id":"c91752386cf5be7cf4e9c7d59f3ad86c600d4075","venue_1":"ACM Trans. Graph.","year":"2015","title":"Extraction of the Quad Layout of a Triangle Mesh Guided by Its Curve Skeleton","authors":"Francesco Usai, Marco Livesu, Enrico Puppo, Marco Tarini, Riccardo Scateni","author_ids":"1879198, 3243460, 1698966, 3153457, 3204853","abstract":"Starting from the triangle mesh of a digital shape, that is, mainly an articulated object, we produce a coarse quad layout that can be used in character modeling and animation. Our quad layout follows the intrinsic object structure described by its curve skeleton; it contains few irregular vertices of low degree; it can be immediately refined into a semiregular quad mesh; it provides a structured domain for UV mapping and parametrization. Our method is fast, one-click, and does not require any parameter setting. The user can steer and refine the process through simple interactive tools during the construction of the quad layout.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"45b6965c4869d23a218d5222be7bd43ff7752cda","venue_1":"ACM Trans. Graph.","year":"2004","title":"Brook for GPUs: stream computing on graphics hardware","authors":"Ian Buck, Tim Foley, Daniel Reiter Horn, Jeremy Sugerman, Kayvon Fatahalian, Mike Houston, Pat Hanrahan","author_ids":"2999741, 2151035, 2810695, 3200943, 2789576, 2376915, 4982303","abstract":"In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.","cites":"667","conferencePercentile":"98.91304348"},{"venue":"ACM Trans. Graph.","id":"1589a204fc92a583ebc2a1ddd1aaafc780809f4e","venue_1":"ACM Trans. Graph.","year":"2007","title":"VideoTrace: rapid interactive scene modelling from video","authors":"Anton van den Hengel, Anthony R. Dick, Thorsten Thormählen, Ben Ward, Philip H. S. Torr","author_ids":"7265081, 2699095, 2543070, 6699129, 7194119","abstract":"VideoTrace is a system for interactively generating realistic 3D models of objects from video---models that might be inserted into a video game, a simulation environment, or another video sequence. The user interacts with VideoTrace by tracing the shape of the object to be modelled over one or more frames of the video. By interpreting the sketch drawn by the user in light of 3D information obtained from computer vision techniques, a small number of simple 2D interactions can be used to generate a realistic 3D model. Each of the sketching operations in VideoTrace provides an intuitive and powerful means of modelling shape from video, and executes quickly enough to be used interactively. Immediate feedback allows the user to model rapidly those parts of the scene which are of interest and to the level of detail required. The combination of automated and manual reconstruction allows VideoTrace to model parts of the scene not visible, and to succeed in cases where purely automated approaches would fail.","cites":"79","conferencePercentile":"69.6"},{"venue":"ACM Trans. Graph.","id":"9a1b100a0f75f00d94dde538265b9eb965162926","venue_1":"ACM Trans. Graph.","year":"2010","title":"Modeling and rendering of impossible figures","authors":"Tai-Pang Wu, Chi-Wing Fu, Sai Kit Yeung, Jiaya Jia, Chi-Keung Tang","author_ids":"1817118, 1699457, 1698659, 1739989, 2546217","abstract":"This article introduces an optimization approach for modeling and rendering impossible figures. Our solution is inspired by how modeling artists construct physical 3D models to produce a valid 2D view of an impossible figure. Given a set of 3D locally possible parts of the figure, our algorithm automatically optimizes a view-dependent 3D model, subject to the necessary 3D constraints for rendering the impossible figure at the desired novel viewpoint. A linear and constrained least-squares solution to the optimization problem is derived, thereby allowing an efficient computation and rendering new views of impossible figures at interactive rates. Once the optimized model is available, a variety of compelling rendering effects can be applied to the impossible figure.","cites":"4","conferencePercentile":"2.339181287"},{"venue":"ACM Trans. Graph.","id":"8b647752c3ac11ba707458406bda1ad91debe26c","venue_1":"ACM Trans. Graph.","year":"2015","title":"JumpCut: non-successive mask transfer and interpolation for video cutout","authors":"Qingnan Fan, Fan Zhong, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen","author_ids":"2115998, 2579992, 1684384, 1701009, 1748939","abstract":"We introduce JumpCut, a new mask transfer and interpolation method for interactive video cutout. Given a source frame for which a foreground mask is already available, we compute an estimate of the foreground mask at another, typically non-successive, target frame. Observing that the background and foreground regions typically exhibit different motions, we leverage these differences by computing two separate nearest-neighbor fields (split-NNF) from the target to the source frame. These NNFs are then used to jointly predict a coherent labeling of the pixels in the target frame. The same split-NNF is also used to aid a novel edge classifier in detecting silhouette edges (S-edges) that separate the foreground from the background. A modified level set method is then applied to produce a clean mask, based on the pixel labels and the S-edges computed by the previous two steps. The resulting mask transfer method may also be used for coherently interpolating the foreground masks between two distant source frames. Our results demonstrate that the proposed method is significantly more accurate than the existing state-of-the-art on a wide variety of video sequences. Thus, it reduces the required amount of user effort, and provides a basis for an effective interactive video object cutout tool.","cites":"8","conferencePercentile":"85.30612245"},{"venue":"ACM Trans. Graph.","id":"79fac5fb9f230e951f1bc9d2d7eb463efa1b5c2e","venue_1":"ACM Trans. Graph.","year":"2012","title":"Learning hatching for pen-and-ink illustration of surfaces","authors":"Evangelos Kalogerakis, Derek Nowrouzezahrai, Simon Breslav, Aaron Hertzmann","author_ids":"2808670, 1795014, 1984863, 1747779","abstract":"This article presents an algorithm for learning hatching styles from line drawings. An artist draws a single hatching illustration of a 3D object. Her strokes are analyzed to extract the following per-pixel properties: hatching level (hatching, cross-hatching, or no strokes), stroke orientation, spacing, intensity, length, and thickness. A mapping is learned from input geometric, contextual, and shading features of the 3D object to these hatching properties, using classification, regression, and clustering techniques. Then, a new illustration can be generated in the artist's style, as follows. First, given a new view of a 3D object, the learned mapping is applied to synthesize target stroke properties for each pixel. A new illustration is then generated by synthesizing hatching strokes according to the target properties.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"0b000cc274bcf655fe9e4add165d4b860a9da718","venue_1":"ACM Trans. Graph.","year":"2011","title":"Probabilistic reasoning for assembly-based 3D modeling","authors":"Siddhartha Chaudhuri, Evangelos Kalogerakis, Leonidas J. Guibas, Vladlen Koltun","author_ids":"7218171, 2808670, 1744254, 1770944","abstract":"Assembly-based modeling is a promising approach to broadening the accessibility of 3D modeling. In assembly-based modeling, new models are assembled from shape components extracted from a database. A key challenge in assembly-based modeling is the identification of relevant components to be presented to the user. In this paper, we introduce a probabilistic reasoning approach to this problem. Given a repository of shapes, our approach learns a probabilistic graphical model that encodes semantic and geometric relationships among shape components. The probabilistic model is used to present components that are semantically and stylistically compatible with the 3D model that is being assembled. Our experiments indicate that the probabilistic model increases the relevance of presented components.","cites":"71","conferencePercentile":"92.10526316"},{"venue":"ACM Trans. Graph.","id":"0bf390e2a14f74bcc8838d5fb1c0c4cc60e92eb7","venue_1":"ACM Trans. Graph.","year":"2010","title":"Learning 3D mesh segmentation and labeling","authors":"Evangelos Kalogerakis, Aaron Hertzmann, Karan Singh","author_ids":"2808670, 1747779, 1682205","abstract":"This paper presents a data-driven approach to simultaneous segmentation and labeling of parts in 3D meshes. An objective function is formulated as a Conditional Random Field model, with terms assessing the consistency of faces with labels, and terms between labels of neighboring faces. The objective function is learned from a collection of labeled training meshes. The algorithm uses hundreds of geometric and contextual label features and learns different types of segmentations for different tasks, without requiring manual parameter tuning. Our algorithm achieves a significant improvement in results over the state-of-the-art when evaluated on the Princeton Segmentation Benchmark, often producing segmentations and labelings comparable to those produced by humans.","cites":"128","conferencePercentile":"99.41520468"},{"venue":"ACM Trans. Graph.","id":"06bde65e46a8abd5419d4b92f8b11424180154c9","venue_1":"ACM Trans. Graph.","year":"2011","title":"Image smoothing via L0 gradient minimization","authors":"Li Xu, Cewu Lu, Yi Xu, Jiaya Jia","author_ids":"6736904, 1830034, 1734114, 1739989","abstract":"Figure 1: L 0 smoothing accomplished by global small-magnitude gradient removal. Our method suppresses low-amplitude details. Meanwhile it globally retains and sharpens salient edges. Even the high-contrast thin edges on the tower are preserved. Abstract We present a new image editing method, particularly effective for sharpening major edges by increasing the steepness of transition while eliminating a manageable degree of low-amplitude structures. The seemingly contradictive effect is achieved in an optimization framework making use of L 0 gradient minimization, which can globally control how many non-zero gradients are resulted in to approximate prominent structure in a sparsity-control manner. Unlike other edge-preserving smoothing approaches, our method does not depend on local features, but instead globally locates important edges. It, as a fundamental tool, finds many applications and is particularly beneficial to edge extraction, clip-art JPEG artifact removal, and non-photorealistic effect generation.","cites":"147","conferencePercentile":"98.94736842"},{"venue":"ACM Trans. Graph.","id":"3a4d5c8a274e233012a94d0d83cbd8b06d5648a7","venue_1":"ACM Trans. Graph.","year":"2014","title":"Functional map networks for analyzing and exploring large shape collections","authors":"Qi-Xing Huang, Fan Wang, Leonidas J. Guibas","author_ids":"1734006, 6701355, 1744254","abstract":"The construction of networks of maps among shapes in a collection enables a variety of applications in data-driven geometry processing. A key task in network construction is to make the maps consistent with each other. This consistency constraint, when properly defined, leads not only to a concise representation of such networks, but more importantly, it serves as a strong regularizer for correcting and improving noisy initial maps computed between pairs of shapes in isolation. Up-to-now, however, the consistency constraint has only been fully formulated for point-based maps or for shape collections that are fully similar.\n In this paper, we introduce a framework for computing consistent functional maps within heterogeneous shape collections. In such collections not all shapes share the same structure --- different types of shared structure may be present within different (but possibly overlapping) sub-collections. Unlike point-based maps, functional maps can encode similarities at multiple levels of detail (points or parts), and thus are particularly suitable for coping with such diversity within a shape collection. We show how to rigorously formulate the consistency constraint in the functional map setting. The formulation leads to a powerful tool for computing consistent functional maps, and also for discovering shared structures, such as meaningful shape parts. We also show how to adapt the procedure for handling very large-scale shape collections. Experimental results on benchmark datasets show that the proposed framework significantly improves upon state-of-the-art data-driven techniques. We demonstrate the usefulness of the framework in shape co-segmentation and various shape exploration tasks.","cites":"21","conferencePercentile":"90.12345679"},{"venue":"ACM Trans. Graph.","id":"236ae3cad39a3e57fe0f8850fc3428e42cd8bfad","venue_1":"ACM Trans. Graph.","year":"2004","title":"\"GrabCut\": interactive foreground extraction using iterated graph cuts","authors":"Carsten Rother, Vladimir Kolmogorov, Andrew Blake","author_ids":"7699610, 1697050, 1745076","abstract":"The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.","cites":"1725","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"a6902db7972a7631d186bbf59c5ef116c205b1e8","venue_1":"ACM Trans. Graph.","year":"2007","title":"Photo clip art","authors":"Jean-François Lalonde, Derek Hoiem, Alexei A. Efros, Carsten Rother, John M. Winn, Antonio Criminisi","author_ids":"1727433, 2433269, 1763086, 7699610, 1928603, 1716777","abstract":"We present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged.","cites":"115","conferencePercentile":"86"},{"venue":"ACM Trans. Graph.","id":"92268ee417def276af4d896d9bbe816803e3db1a","venue_1":"ACM Trans. Graph.","year":"2012","title":"3D imaging spectroscopy for measuring hyperspectral patterns on solid objects","authors":"Min H. Kim, Holly E. Rushmeier, Julie Dorsey, Todd Alan Harvey, Richard O. Prum, David S. Kittle, David J. Brady","author_ids":"7591291, 1690595, 1775220, 3336068, 2837651, 8101882, 5561932","abstract":"Sophisticated methods for true spectral rendering have been developed in computer graphics to produce highly accurate images. In addition to traditional applications in visualizing appearance, such methods have potential applications in many areas of scientific study. In particular, we are motivated by the application of studying avian vision and appearance. An obstacle to using graphics in this application is the lack of reliable input data. We introduce an end-to-end measurement system for capturing spectral data on 3D objects. We present the modification of a recently developed hyperspectral imager to make it suitable for acquiring such data in a wide spectral range at high spectral and spatial resolution. We capture four megapixel images, with data at each pixel from the near-ultraviolet (359 nm) to near-infrared (1,003 nm) at 12 nm spectral resolution. We fully characterize the imaging system, and document its accuracy. This imager is integrated into a 3D scanning system to enable the measurement of the diffuse spectral reflectance and fluorescence of specimens. We demonstrate the use of this measurement system in the study of the interplay between the visual capabilities and appearance of birds. We show further the use of the system in gaining insight into artifacts from geology and cultural heritage.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"2aab78ffb245dd1294459f7a0f5b069e13e393ed","venue_1":"ACM Trans. Graph.","year":"2011","title":"Insitu: sketching architectural designs in context","authors":"Patrick Paczkowski, Min H. Kim, Yann Morvan, Julie Dorsey, Holly E. Rushmeier, Carol O'Sullivan","author_ids":"1818289, 7591291, 2574352, 1775220, 1690595, 7711112","abstract":"Architecture is design in spatial context. The only current methods for representing context involve designing in a heavyweight computer-aided design system, using a full model of existing buildings and landscape, or sketching on a panoramic photo. The former is too cumbersome; the latter is too restrictive in viewpoint and in the handling of occlusions and topography. We introduce a novel approach to presenting context such that it is an integral component in a lightweight conceptual design system. We represent sites through a fusion of data available from different sources. We derive a site model from geographic elevation data, on-site point-to-point distance measurements, and images of the site. To acquire and process the data, we use publicly available data sources, multidimensional scaling techniques and refinements of recent bundle adjustment techniques. We offer a suite of interactive tools to acquire, process, and combine the data into a lightweight stroke and image-billboard representation. We create multiple and linked <i>pop-ups</i> derived from images, forming a lightweight representation of a three-dimensional environment. We implemented our techniques in a stroke-based conceptual design system we call <i>Insitu</i>. We developed our work through continuous interaction with professional designers. We present designs created with our new techniques integrated in a conceptual design system.","cites":"11","conferencePercentile":"22.10526316"},{"venue":"ACM Trans. Graph.","id":"4b7f821f9b5f989ef55ca709060661bc13985955","venue_1":"ACM Trans. Graph.","year":"2011","title":"Edge-aware color appearance","authors":"Min H. Kim, Tobias Ritschel, Jan Kautz","author_ids":"7591291, 1759347, 1690538","abstract":"Color perception is recognized to vary with surrounding spatial structure, but the impact of edge smoothness on color has not been studied in color appearance modeling. In this work, we study the appearance of color under different degrees of edge smoothness. A psychophysical experiment was conducted to quantify the change in perceived lightness, colorfulness, and hue with respect to edge smoothness. We confirm that color appearance, in particular lightness, changes noticeably with increased smoothness. Based on our experimental data, we have developed a computational model that predicts this appearance change. The model can be integrated into existing color appearance models. We demonstrate the applicability of our model on a number of examples.","cites":"5","conferencePercentile":"8.157894737"},{"venue":"ACM Trans. Graph.","id":"cdb9a964d2b08cc7578eae031600bfa75e7e8c0c","venue_1":"ACM Trans. Graph.","year":"2016","title":"Temporally coherent completion of dynamic video","authors":"Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, Johannes Kopf","author_ids":"3068086, 1738740, 1752333, 2891193","abstract":"We present an automatic video completion algorithm that synthesizes missing regions in videos in a temporally coherent fashion. Our algorithm can handle dynamic scenes captured using a moving camera. State-of-the-art approaches have difficulties handling such videos because viewpoint changes cause image-space motion vectors in the missing and known regions to be inconsistent. We address this problem by jointly estimating optical flow and color in the missing regions. Using pixel-wise forward/backward flow fields enables us to synthesize temporally coherent colors. We formulate the problem as a non-parametric patch-based optimization. We demonstrate our technique on numerous challenging videos.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"1f46ed7166bb9d9a3cf44126b9b8058bf1f14428","venue_1":"ACM Trans. Graph.","year":"2014","title":"Image completion using planar structure guidance","authors":"Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, Johannes Kopf","author_ids":"3068086, 1738740, 1752333, 2891193","abstract":"We propose a method for automatically guiding patch-based image completion using mid-level structural cues. Our method first estimates planar projection parameters, softly segments the known region into planes, and discovers translational regularity <i>within</i> these planes. This information is then converted into soft constraints for the low-level completion algorithm by defining prior probabilities for patch offsets and transformations. Our method handles multiple planes, and in the absence of any detected planes falls back to a baseline fronto-parallel image completion algorithm. We validate our technique through extensive comparisons with state-of-the-art algorithms on a variety of scenes.","cites":"14","conferencePercentile":"79.218107"},{"venue":"ACM Trans. Graph.","id":"5747079a8d0a6aa490e0e063b096d2f3166b6a66","venue_1":"ACM Trans. Graph.","year":"2012","title":"Quality prediction for image completion","authors":"Johannes Kopf, Wolf Kienzle, Steven M. Drucker, Sing Bing Kang","author_ids":"2891193, 1932429, 2311676, 1738740","abstract":"We present a data-driven method to predict the quality of an image completion method. Our method is based on the state-of-the-art non-parametric framework of Wexler <i>et al</i>. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape <i>before</i> the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"3e23f380c254c93e7aee6e89776f2b2d30467ed3","venue_1":"ACM Trans. Graph.","year":"2005","title":"Texture optimization for example-based synthesis","authors":"Vivek Kwatra, Irfan A. Essa, Aaron F. Bobick, Nipun Kwatra","author_ids":"1706355, 1714295, 1688328, 2741003","abstract":"We present a novel technique for texture synthesis using optimization. We define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. This allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm. In contrast to most example-based techniques that do region-growing, ours is a joint optimization approach that progressively refines the entire texture. Additionally, our approach is ideally suited to allow for controllable synthesis of textures. Specifically, we demonstrate controllability by animating image textures using flow fields. We allow for general two-dimensional flow fields that may dynamically change over time. Applications of this technique include dynamic texturing of fluid animations and texture-based flow visualization.","cites":"238","conferencePercentile":"94.35483871"},{"venue":"ACM Trans. Graph.","id":"296af88333ff7f695e802dd620015ae60af78c25","venue_1":"ACM Trans. Graph.","year":"2003","title":"Graphcut textures: image and video synthesis using graph cuts","authors":"Vivek Kwatra, Arno Schödl, Irfan A. Essa, Greg Turk, Aaron F. Bobick","author_ids":"1706355, 1734769, 1714295, 1713189, 1688328","abstract":"In this paper we introduce a new algorithm for image and video texture synthesis. In our approach, patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output. In contrast to other techniques, the size of the patch is not chosen <i>a-priori</i>, but instead a <i>graph cut</i> technique is used to determine the optimal patch region for any given offset between the input and output texture. Unlike dynamic programming, our graph cut technique for seam optimization is applicable in any dimension. We specifically explore it in 2D and 3D to perform video texture synthesis in addition to regular image synthesis. We present approximative offset search techniques that work well in conjunction with the presented patch size optimization. We show results for synthesizing regular, random, and natural images and videos. We also demonstrate how this method can be used to interactively merge different images to generate new scenes.","cites":"651","conferencePercentile":"98.92473118"},{"venue":"ACM Trans. Graph.","id":"58e3a93c9c3f4171d3286901312f662e02ba8003","venue_1":"ACM Trans. Graph.","year":"2015","title":"AA patterns for point sets with controlled spectral properties","authors":"Abdalla G. M. Ahmed, Hui Huang, Oliver Deussen","author_ids":"2683313, 1927737, 1850438","abstract":"We describe a novel technique for the fast production of large point sets with different spectral properties. In contrast to tile-based methods we use so-called AA Patterns: ornamental point sets obtained from quantization errors. These patterns have a discrete and structured number-theoretic nature, can be produced at very low costs, and possess an inherent structural indexing mechanism equivalent to those used in recursive tiling techniques. This allows us to generate, manipulate and store point sets very efficiently. The technique outperforms existing methods in speed, memory footprint, quality, and flexibility. This is demonstrated by a number of measurements and comparisons to existing point generation algorithms.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"123905a6b69d00856d5b26302203ae28435deca5","venue_1":"ACM Trans. Graph.","year":"2014","title":"Windy trees: computing stress response for developmental tree models","authors":"Sören Pirk, Till Niese, Torsten Hädrich, Bedrich Benes, Oliver Deussen","author_ids":"2604835, 1844922, 2565867, 7590480, 1850438","abstract":"We present a novel method for combining developmental tree models with turbulent wind fields. The tree geometry is created from internal growth functions of the developmental model and its response to external stress is induced by a physically-plausible wind field that is simulated by Smoothed Particle Hydrodynamics (SPH). Our tree models are dynamically evolving complex systems that (1) react in real-time to high-frequent changes of the wind simulation; and (2) adapt to long-term wind stress. We extend this process by wind-related effects such as branch breaking as well as bud abrasion and drying. In our interactive system the user can adjust the parameters of the growth model, modify wind properties and resulting forces, and define the tree's long-term response to wind. By using graphics hardware, our implementation runs at interactive rates for moderately large scenes composed of up to 20 tree models.","cites":"2","conferencePercentile":"8.436213992"},{"venue":"ACM Trans. Graph.","id":"0cf71398f7d26a0e7bb757c4dfc35f0570c2f7b8","venue_1":"ACM Trans. Graph.","year":"2012","title":"A probabilistic model for component-based shape synthesis","authors":"Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, Vladlen Koltun","author_ids":"2808670, 7218171, 1736370, 1770944","abstract":"We present an approach to synthesizing shapes from complex domains, by identifying new plausible combinations of components from existing shapes. Our primary contribution is a new generative model of component-based shape structure. The model represents probabilistic relationships between properties of shape components, and relates them to learned underlying causes of structural variability within the domain. These causes are treated as latent variables, leading to a compact representation that can be effectively learned without supervision from a set of compatibly segmented shapes. We evaluate the model on a number of shape datasets with complex structural variability and demonstrate its application to amplification of shape databases and to interactive shape synthesis.","cites":"74","conferencePercentile":"97.97979798"},{"venue":"ACM Trans. Graph.","id":"5cc301e66427ec26f9ec154de6732c5d9b4ad036","venue_1":"ACM Trans. Graph.","year":"2013","title":"Blue noise sampling with controlled aliasing","authors":"Daniel Heck, Thomas Schlömer, Oliver Deussen","author_ids":"1887328, 2283817, 1850438","abstract":"In this article we revisit the problem of blue noise sampling with a strong focus on the spectral properties of the sampling patterns. Starting from the observation that oscillations in the power spectrum of a sampling pattern can cause aliasing artifacts in the resulting images, we synthesize two new types of blue noise patterns: <i>step blue noise</i> with a power spectrum in the form of a step function and <i>single-peak blue noise</i> with a wide zero-region and no oscillations except for a single peak. We study the mathematical relationship of the radial power spectrum to a spatial statistic known as the radial distribution function to determine which power spectra can actually be realized and to construct the corresponding point sets. Finally, we show that both proposed sampling patterns effectively prevent structured aliasing at low sampling rates and perform well at high sampling rates.","cites":"17","conferencePercentile":"67.19457014"},{"venue":"ACM Trans. Graph.","id":"242cd62c931162605fb94fae2d884135cdf4ff89","venue_1":"ACM Trans. Graph.","year":"2003","title":"Suggestive contours for conveying shape","authors":"Douglas DeCarlo, Adam Finkelstein, Szymon Rusinkiewicz, Anthony Santella","author_ids":"2476753, 1707541, 7723706, 2971168","abstract":"In this paper, we describe a non-photorealistic rendering system that conveys shape using lines. We go beyond contours and creases by developing a new type of line to draw: the <i>suggestive contour</i>. Suggestive contours are lines drawn on clearly visible parts of the surface, where a true contour would first appear with a minimal change in viewpoint. We provide two methods for calculating suggestive contours, including an algorithm that finds the zero crossings of the radial curvature. We show that suggestive contours can be drawn consistently with true contours, because they anticipate and extend them. We present a variety of results, arguing that these images convey shape more effectively than contour alone.","cites":"274","conferencePercentile":"90.32258065"},{"venue":"ACM Trans. Graph.","id":"1dbaf046a9a2b4c417dd537713de3963d67233ce","venue_1":"ACM Trans. Graph.","year":"2008","title":"Two-way coupling of fluids to rigid and deformable solids and shells","authors":"Avi Robinson-Mosher, Tamar Shinar, Jon Gretarsson, Jonathan Su, Ronald Fedkiw","author_ids":"2982297, 2357146, 2186595, 2510810, 1688994","abstract":"We propose a novel solid/fluid coupling method that treats the coupled system in a fully implicit manner making it stable for arbitrary time steps, large density ratios, etc. In contrast to previous work in computer graphics, we derive our method using a simple back-of-the-envelope approach which lumps the solid and fluid momenta together, and which we show exactly conserves the momentum of the coupled system. Notably, our method uses the standard Cartesian fluid discretization and does not require (moving) conforming tetrahedral meshes or ALE frameworks. Furthermore, we use a standard Lagrangian framework for the solid, thus supporting arbitrary solid constitutive models, both implicit and explicit time integration, etc. The method is quite general, working for smoke, water, and multiphase fluids as well as both rigid and deformable solids, and both volumes and thin shells. Rigid shells and cloth are handled automatically without special treatment, and we support fully one-sided discretizations without leaking. Our equations are fully symmetric, allowing for the use of fast solvers, which is a natural result of properly conserving momentum. Finally, for simple explicit time integration of rigid bodies, we show that our equations reduce to form similar to previous work via a single block Gaussian elimination operation, but that this approach scales poorly, i.e. as though four spatial dimensions rather than three.","cites":"52","conferencePercentile":"64.19753086"},{"venue":"ACM Trans. Graph.","id":"201fed356fc9d39d3b9d81b029ba71cc798cc7ec","venue_1":"ACM Trans. Graph.","year":"2005","title":"Video enhancement using per-pixel virtual exposures","authors":"Eric P. Bennett, Leonard McMillan","author_ids":"3037279, 1748115","abstract":"We enhance underexposed, low dynamic range videos by adaptively and independently varying the exposure at each photoreceptor in a post-process. This virtual exposure is a dynamic function of both the spatial neighborhood and temporal history at each pixel. Temporal integration enables us to expand the image's dynamic range while simultaneously reducing noise. Our non-linear exposure variation and denoising filters smoothly transition from temporal to spatial for moving scene elements. Our virtual exposure framework also supports temporally coherent per frame tone mapping. Our system outputs restored video sequences with significantly reduced noise, increased exposure time of dark pixels, intact motion, and improved details.","cites":"113","conferencePercentile":"65.32258065"},{"venue":"ACM Trans. Graph.","id":"1027d335627072de19264c10ef71d3a29e65c05e","venue_1":"ACM Trans. Graph.","year":"2007","title":"Image vectorization using optimized gradient meshes","authors":"Jian Sun, Lin Liang, Fang Wen, Harry Shum","author_ids":"1748508, 1680293, 1716835, 1698102","abstract":"Recently, gradient meshes have been introduced as a powerful vector graphics representation to draw multicolored mesh objects with smooth transitions. Using tools from Abode Illustrator and Corel CorelDraw, a user can manually create gradient meshes even for photo-realistic vector arts, which can be further edited, stylized and animated.\n In this paper, we present an easy-to-use interactive tool, called <i>optimized gradient mesh</i>, to semi-automatically and quickly create gradient meshes from a raster image. We obtain the optimized gradient mesh by formulating an energy minimization problem. The user can also interactively specify a few vector lines to guide the mesh generation. The resulting optimized gradient mesh is an editable and scalable mesh that otherwise would have taken many hours for a user to manually create.","cites":"45","conferencePercentile":"44"},{"venue":"ACM Trans. Graph.","id":"2a559491cbe9bbac86301b579c3203d5f2bc22e6","venue_1":"ACM Trans. Graph.","year":"2007","title":"Computational time-lapse video","authors":"Eric P. Bennett, Leonard McMillan","author_ids":"3037279, 1748115","abstract":"We present methods for generating novel time-lapse videos that address the inherent sampling issues that arise with traditional photographic techniques. Starting with video-rate footage as input, our post-process downsamples the source material into a time-lapse video and provides user controls for retaining, removing, and resampling events. We employ two techniques for selecting and combining source frames to form the output. First, we present a non-uniform sampling method, based on dynamic programming, which optimizes the sampling of the input video to match the user's desired duration and visual objectives. We present multiple error metrics for this optimization, each resulting in different sampling characteristics. To complement the non-uniform sampling, we present the <i>virtual shutter</i>, a non-linear filtering technique that synthetically extends the exposure time of time-lapse frames.","cites":"28","conferencePercentile":"24.8"},{"venue":"ACM Trans. Graph.","id":"12c047c711f1657144cc9a5b5ba65ef663005a0e","venue_1":"ACM Trans. Graph.","year":"2010","title":"Simulating virtual environments within virtual environments as the basis for a psychophysics of presence","authors":"Mel Slater, Bernhard Spanlang, David Corominas","author_ids":"1703354, 2891686, 3264597","abstract":"A new definition of immersion with respect to virtual environment (VE) systems has been proposed in earlier work, based on the concept of simulation. One system (<i>A</i>) is said to be more immersive than another (<i>B</i>) if <i>A</i> can be used to simulate an application as if it were running on <i>B.</i> Here we show how this concept can be used as the basis for a psychophysics of presence in VEs, the sensation of being in the place depicted by the virtual environment displays (Place Illusion, PI), and also the illusion that events occurring in the virtual environment are real (Plausibility Illusion, Psi). The new methodology involves matching experiments akin to those in color science. Twenty participants first experienced PI or Psi in the initial highest level immersive system, and then in 5 different trials chose transitions from lower to higher order systems and declared a match whenever they felt the same level of PI or Psi as they had in the initial system. In each transition they could change the type of illumination model used, or the field-of-view, or the display type (powerwall or HMD) or the extent of self-representation by an avatar. The results showed that the 10 participants instructed to choose transitions to attain a level of PI corresponding to that in the initial system tended to first choose a wide field-of-view and head-mounted display, and then ensure that they had a virtual body that moved as they did. The other 10 in the Psi group concentrated far more on achieving a higher level of illumination realism, although having a virtual body representation was important for both groups. This methodology is offered as a way forward in the evaluation of the responses of people to immersive virtual environments, a unified theory and methodology for psychophysical measurement.","cites":"23","conferencePercentile":"39.76608187"},{"venue":"ACM Trans. Graph.","id":"560b0cf5ab18a325a7a76fb9589177ab55fff1b2","venue_1":"ACM Trans. Graph.","year":"1993","title":"Experimental Comparison of Splines Using the Shape-Matching Paradigm","authors":"Richard H. Bartels, John C. Beatty, Kellogg S. Booth, Eric G. Bosch, Pierre Jolicoeur","author_ids":"1862534, 3041827, 1800617, 2576598, 1707099","abstract":"There are a variety of spline formulations for specifying smooth curves. In theory, many of these are equivalent in terms of the families of curves that can be obtained. In practice, each formulation encourages the implementation of certain styles of interaction in a curve editor and discourages other styles. This paper presents the design, implementation, and analysis of four experiments to illustrate a general shape-matching paradigm for studying interactive curve manipulation. We introduce the new shape-matching paradigm as a methodology for analyzing user performance. We then report on a series of studies that employ this paradigm to explore the relationship between a commonly used interaction style for curve design and various spline formulations, Controlled experiments were performed to test the hypothesis that the style of interaction implicit in the movement of single design points will influence the ease with which curve manipulation can be performed using a variety of spline formulations. Although such interactions are of a simple form, the experience gained in these studies has already enabled us to undertake more extensive experiments with more sophisticated interaction styles. The specific experiments reported here compare five formulations of cubic splines (B-splines, B6zier splines, Catmull-Rom splines, and two versions of interpolating splines with C 2 continuity), Data from the experiments show that, for the particular shape-matching tasks investigated, there is a significant performance difference among the five formulations. The B-spline formulation was best for the task, both in terms of match quality and in terms of the time required to Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Asscwiation for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.","cites":"12","conferencePercentile":"28.57142857"},{"venue":"ACM Trans. Graph.","id":"6ce38185c346e6798abff9d454e7b18c7e0352d1","venue_1":"ACM Trans. Graph.","year":"2016","title":"Functionality preserving shape style transfer","authors":"Zhaoliang Lun, Evangelos Kalogerakis, Rui Wang, Alla Sheffer","author_ids":"2257363, 2808670, 1699697, 3354923","abstract":"When geometric models with a desired combination of style and functionality are not available, they currently need to be created manually. We facilitate algorithmic synthesis of 3D models of man-made shapes which combines user-specified style, described via an <i>exemplar</i> shape, and functionality, encoded by a functionally different <i>target</i> shape. Our method automatically transfers the style of the exemplar to the target, creating the desired combination. The main challenge in performing cross-functional style transfer is to implicitly separate an object's style from its function: while stylistically the output shapes should be as close as possible to the exemplar, their original functionality and structure, as encoded by the target, should be strictly preserved. Recent literature point to the presence of similarly shaped, salient <i>geometric elements</i> as a main indicator of stylistic similarity between 3D shapes. We therefore transfer the exemplar style to the target via a sequence of element-level operations. We allow only <i>compatible</i> operations, ones that do not affect the target functionality. To this end, we introduce a cross-structural element compatibility metric that estimates the impact of each operation on the edited shape. Our metric is based on the global context and coarse geometry of evaluated elements, and is trained on databases of 3D objects. We use this metric to cast style transfer as a tabu search, which incrementally updates the target shape using compatible operations, progressively increasing its style similarity to the exemplar while strictly maintaining its functionality at each step. We evaluate our framework across a range of man-made objects including furniture, light fixtures, and tableware, and perform a number of user studies confirming that it produces convincing outputs combining the desired style and function.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"51fffafb5c009e952a0dcc94c5217f971906d445","venue_1":"ACM Trans. Graph.","year":"2016","title":"A compiler for 3D machine knitting","authors":"James McCann, Lea Albaugh, Vidya Narayanan, April Grow, Wojciech Matusik, Jennifer Mankoff, Jessica K. Hodgins","author_ids":"2400939, 3396145, 2265362, 1899862, 1752521, 3055754, 1788773","abstract":"Industrial knitting machines can produce finely detailed, seamless, 3D surfaces quickly and without human intervention. However, the tools used to program them require detailed manipulation and understanding of low-level knitting operations. We present a compiler that can automatically turn assemblies of high-level shape primitives (tubes, sheets) into low-level machine instructions. These high-level shape primitives allow knit objects to be scheduled, scaled, and otherwise shaped in ways that require thousands of edits to low-level instructions. At the core of our compiler is a heuristic transfer planning algorithm for knit cycles, which we prove is both sound and complete. This algorithm enables the translation of high-level shaping and scheduling operations into needle-level operations. We show a wide range of examples produced with our compiler and demonstrate a basic visual design interface that uses our compiler as a backend.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"0145eb17c46c1d6472e75f115f23dcfeabaf96fc","venue_1":"ACM Trans. Graph.","year":"2004","title":"VisualIDs: automatic distinctive icons for desktop interfaces","authors":"John P. Lewis, Ruth Rosenholtz, Nickson Fong, Ulrich Neumann","author_ids":"2731407, 7379768, 1848947, 1745263","abstract":"Although existing GUIs have a sense of space, they provide no sense of place. Numerous studies report that users misplace files and have trouble wayfinding in virtual worlds despite the fact that people have remarkable visual and spatial abilities. This issue is considered in the human-computer interface field and has been addressed with alternate display/navigation schemes. Our paper presents a fundamentally graphics based approach to this 'lost in hyperspace' problem. Specifically, we propose that spatial display of files is not sufficient to engage our visual skills; <i>scenery</i> (distinctive visual appearance) is needed as well. While scenery (in the form of custom icon assignments) is already possible in current operating systems, few if any users take the time to manually assign icons to all their files. As such, our proposal is to generate visually distinctive icons (\"VisualIDs\") <i>automatically</i>, while allowing the user to replace the icon if desired. The paper discusses psychological and conceptual issues relating to icons, visual memory, and the necessary relation of scenery to data. A particular icon generation algorithm is described; subjects using these icons in simulated file search and recall tasks show significantly improved performance with little effort. Although the incorporation of scenery in a graphical user interface will introduce many new (and interesting) design problems that cannot be addressed in this paper, we show that automatically created scenery is both beneficial and feasible.","cites":"20","conferencePercentile":"7.065217391"},{"venue":"ACM Trans. Graph.","id":"aa6810ca55072e64a5d5034d96f81b269eeca827","venue_1":"ACM Trans. Graph.","year":"2016","title":"FrameFab: robotic fabrication of frame shapes","authors":"Yijiang Huang, Juyong Zhang, Xin Hu, Guoxian Song, Zhongyuan Liu, Lei Yu, Ligang Liu","author_ids":"7524935, 2938279, 5232730, 7436326, 2297688, 1739927, 1724542","abstract":"Frame shapes, which are made of struts, have been widely used in many fields, such as art, sculpture, architecture, and geometric modeling, etc. An interest in robotic fabrication of frame shapes via spatial thermoplastic extrusion has been increasingly growing in recent years. In this paper, we present a novel algorithm to generate a feasible fabrication sequence for general frame shapes. To solve this non-trivial combinatorial problem, we develop a divide-and-conquer strategy that first decomposes the input frame shape into stable layers via a constrained sparse optimization model. Then we search a feasible sequence for each layer via a local optimization method together with a backtracking strategy. The generated sequence guarantees that the already-printed part is in a stable equilibrium state at all stages of fabrication, and that the 3D printing extrusion head does not collide with the printed part during the fabrication. Our algorithm has been validated by a built prototype robotic fabrication system made by a 6-axis KUKA robotic arm with a customized extrusion head. Experimental results demonstrate the feasibility and applicability of our algorithm.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"9544d444a6896b4ab3ba6646990d67cb1ac0e6e7","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by Focused Femtosecond Laser Combined with Computational Holographic Fields","authors":"Yoichi Ochiai, Kota Kumagai, Takayuki Hoshi, Jun Rekimoto, Satoshi Hasegawa, Yoshio Hayasaki","author_ids":"1734002, 2807730, 1789842, 1685962, 1688161, 2052969","abstract":"We envision a laser-induced plasma technology in general applications for public use. If laser-induced plasma aerial images were made available, many useful applications such as spatial aerial AR, aerial user interfaces, volumetric images could be produced. This would be a highly effective display for the expression of three-dimensional information. Volumetric expression has considerable merit because the content scale corresponds to the human body; therefore, this technology could be usefully applied to wearable materials and spatial user interactions. Further, laser focusing technology can add an additional dimension to conventional projection technology, which is designed for surface mapping, while laser focusing technology is capable of volumetric mapping. This technology can be effectively used in real-world-oriented user interfaces.","cites":"11","conferencePercentile":"91.63265306"},{"venue":"ACM Trans. Graph.","id":"5b70beeadc31ac8421bd9fe54fbe696b90eba1cf","venue_1":"ACM Trans. Graph.","year":"2012","title":"Three-dimensional proxies for hand-drawn characters","authors":"Eakta Jain, Yaser Sheikh, Moshe Mahler, Jessica K. Hodgins","author_ids":"2617152, 1774867, 3156160, 1788773","abstract":"Drawing shapes by hand and manipulating computer-generated objects are the two dominant forms of animation. Though each medium has its own advantages, the techniques developed for one medium are not easily leveraged in the other medium because hand animation is two-dimensional, and inferring the third dimension is mathematically ambiguous. A second challenge is that the character is a consistent three-dimensional (3D) object in computer animation while hand animators introduce geometric inconsistencies in the two-dimensional (2D) shapes to better convey a character's emotional state and personality. In this work, we identify 3D proxies to connect hand-drawn animation and 3D computer animation. We present an integrated approach to generate three levels of 3D proxies: single-points, polygonal shapes, and a full joint hierarchy. We demonstrate how this approach enables one medium to take advantage of techniques developed for the other; for example, 3D physical simulation is used to create clothes for a hand-animated character, and a traditionally trained animator is able to influence the performance of a 3D character while drawing with paper and pencil.","cites":"6","conferencePercentile":"7.323232323"},{"venue":"ACM Trans. Graph.","id":"151f7e6eeb4a15e3f6f4a47e6ff2b3bb67a6a73e","venue_1":"ACM Trans. Graph.","year":"2013","title":"Style and abstraction in portrait sketching","authors":"Itamar Berger, Ariel Shamir, Moshe Mahler, Elizabeth J. Carter, Jessica K. Hodgins","author_ids":"2672654, 2947946, 3156160, 3316296, 1788773","abstract":"We use a data-driven approach to study both style and abstraction in sketching of a human face. We gather and analyze data from a number of artists as they sketch a human face from a reference photograph. To achieve different levels of abstraction in the sketches, decreasing time limits were imposed -- from four and a half minutes to fifteen seconds. We analyzed the data at two levels: strokes and geometric shape. In each, we create a model that captures both the style of the different artists and the process of abstraction. These models are then used for a portrait sketch synthesis application. Starting from a novel face photograph, we can synthesize a sketch in the various artistic styles and in different levels of abstraction.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"a81cc726d1f73521de45a59c9f439a31bb341423","venue_1":"ACM Trans. Graph.","year":"2015","title":"A perceptual control space for garment simulation","authors":"Leonid Sigal, Moshe Mahler, Spencer Diaz, Kyna McIntosh, Elizabeth J. Carter, Timothy Richards, Jessica K. Hodgins","author_ids":"2956921, 3156160, 3275145, 1955171, 3316296, 1886233, 1788773","abstract":"We present a perceptual control space for simulation of cloth that works with any physical simulator, treating it as a black box. The perceptual control space provides intuitive, art-directable control over the simulation behavior based on a learned mapping from common descriptors for cloth (<i>e.g.</i>, flowiness, softness) to the parameters of the simulation. To learn the mapping, we perform a series of perceptual experiments in which the simulation parameters are varied and participants assess the values of the common terms of the cloth on a scale. A multi-dimensional sub-space regression is performed on the results to build a perceptual generative model over the simulator parameters. We evaluate the perceptual control space by demonstrating that the generative model does in fact create simulated clothing that is rated by participants as having the expected properties. We also show that this perceptual control space generalizes to garments and motions not in the original experiments.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"35b5dbbf4455a247652675cadc445e8410e2b2d1","venue_1":"ACM Trans. Graph.","year":"2003","title":"Twister: a space-warp operator for the two-handed editing of 3D shapes","authors":"Ignacio Llamas, ByungMoon Kim, Joshua Gargus, Jarek Rossignac, Chris Shaw","author_ids":"2072664, 2259682, 2259772, 1701940, 1698197","abstract":"A free-form deformation that warps a surface or solid may be specified in terms of one or several point-displacement constraints that must be interpolated by the deformation. The Twister approach introduced here, adds the capability to impose an orientation change, adding three rotational constraints, at each displaced point. Furthermore, it solves for a space warp that simultaneously interpolates two sets of such displacement and orientation constraints. With a 6 DoF magnetic tracker in each hand, the user may grab two points on or near the surface of an object and simultaneously drag them to new locations while rotating the trackers to tilt, bend, or twist the shape near the displaced points. Using a new formalism based on a weighted average of <i>screw displacements</i>, Twister computes in realtime a smooth deformation, whose effect decays with distance from the grabbed points, simultaneously interpolating the 12 constraints. It is continuously applied to the shape, providing realtime graphic feedback. The two-hand interface and the resulting deformation are intuitive and hence offer an effective direct manipulation tool for creating or modifying 3D shapes.","cites":"78","conferencePercentile":"40.32258065"},{"venue":"ACM Trans. Graph.","id":"ce44df002463102ec17df252ad03ba65c9ee8b3b","venue_1":"ACM Trans. Graph.","year":"2016","title":"Synthesis of filigrees for digital fabrication","authors":"Weikai Chen, Xiaolong Zhang, Shi-Qing Xin, Yang Xia, Sylvain Lefebvre, Wenping Wang","author_ids":"2535947, 1799079, 2012376, 1773901, 2757631, 1698520","abstract":"Filigrees are thin patterns found in jewelry, ornaments and lace fabrics. They are often formed of repeated base elements manually composed into larger, delicate patterns. Digital fabrication simplifies the process of turning a virtual model of a filigree into a physical object. However, designing a virtual model of a filigree remains a time consuming and challenging task. The difficulty lies in tightly packing together the base elements while covering a target surface. In addition, the filigree has to be well connected and sufficiently robust to be fabricated. We propose a novel approach automating this task. Our technique covers a target surface with a set of input base elements, forming a filigree strong enough to be fabricated. We exploit two properties of filigrees to make this possible. First, as filigrees form delicate traceries they are well captured by their skeleton. This affords for a simpler definition of operators such as matching and deformation. Second, instead of seeking for a perfect packing of the base elements we relax the problem by allowing appearance preserving partial overlaps. We optimize a filigree by a stochastic search, further improved by a novel boosting algorithm that records and reuses good configurations discovered during the process.\n We illustrate our technique on a number of challenging examples reproducing filigrees on large objects, which we manufacture by 3D printing. Our technique affords for several user controls, such as the scale and orientation of the elements.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"00f05770d02bb8251479245450f35f971fe0ed66","venue_1":"ACM Trans. Graph.","year":"2015","title":"Elements of style: learning perceptual shape style similarity","authors":"Zhaoliang Lun, Evangelos Kalogerakis, Alla Sheffer","author_ids":"2257363, 2808670, 3354923","abstract":"The human perception of stylistic similarity transcends structure and function: for instance, a bed and a dresser may share a common style. An algorithmically computed style similarity measure that mimics human perception can benefit a range of computer graphics applications. Previous work in style analysis focused on shapes within the same class, and leveraged structural similarity between these shapes to facilitate analysis. In contrast, we introduce the first <i>structure-transcending</i> style similarity measure and validate it to be well aligned with human perception of stylistic similarity. Our measure is inspired by observations about style similarity in art history literature, which point to the presence of similarly shaped, salient, <i>geometric elements</i> as one of the key indicators of stylistic similarity. We translate these observations into an algorithmic measure by first quantifying the geometric properties that make humans perceive geometric elements as similarly shaped and salient in the context of style, then employing this quantification to detect pairs of matching style related elements on the analyzed models, and finally collating the element-level geometric similarity measurements into an object-level style measure consistent with human perception. To achieve this consistency we employ crowdsourcing to quantify the different components of our measure; we learn the relative perceptual importance of a range of elementary shape distances and other parameters used in our measurement from 50K responses to cross-structure style similarity queries provided by over 2500 participants.We train and validate our method on this dataset, showing it to successfully predict relative style similarity with near 90% accuracy based on 10-fold cross-validation.","cites":"11","conferencePercentile":"91.63265306"},{"venue":"ACM Trans. Graph.","id":"243116bc6d7512f0fb60aa793ac14f528a174781","venue_1":"ACM Trans. Graph.","year":"2002","title":"Stylization and abstraction of photographs","authors":"Douglas DeCarlo, Anthony Santella","author_ids":"2476753, 2971168","abstract":"Good information design depends on clarifying the meaningful structure in an image. We describe a computational approach to stylizing and abstracting photographs that explicitly responds to this design goal. Our system transforms images into a line-drawing style using bold edges and large regions of constant color. To do this, it represents images as a hierarchical structure of parts and boundaries computed using state-of-the-art computer vision. Our system identifies the meaningful elements of this structure using a model of human perception and a record of a user's eye movements in looking at the photo; the system renders a new image using transformations that preserve and highlight these visual elements. Our method thus represents a new alternative for non-photorealistic rendering both in its visual style, in its approach to visual form, and in its techniques for interaction.","cites":"300","conferencePercentile":"86"},{"venue":"ACM Trans. Graph.","id":"112550a048cbdee9accd520ff676945cc0c7fac5","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fast multiple-fluid simulation using Helmholtz free energy","authors":"Tao Yang, Jian Chang, Bo Ren, Ming C. Lin, Jian J. Zhang, Shi-Min Hu","author_ids":"1720792, 2804499, 2879613, 1709625, 1679327, 1686809","abstract":"Multiple-fluid interaction is an interesting and common visual phenomenon we often observe. In this paper, we present an energy-based Lagrangian method that expands the capability of existing multiple-fluid methods to handle various phenomena, such as extraction, partial dissolution, etc. Based on our user-adjusted Helmholtz free energy functions, the simulated fluid evolves from high-energy states to low-energy states, allowing flexible capture of various mixing and unmixing processes. We also extend the original Cahn-Hilliard equation to be better able to simulate complex fluid-fluid interaction and rich visual phenomena such as motion-related mixing and position based pattern. Our approach is easily integrated with existing state-of-the-art smooth particle hydrodynamic (SPH) solvers and can be further implemented on top of the position based dynamics (PBD) method, improving the stability and incompressibility of the fluid during Lagrangian simulation under large time steps. Performance analysis shows that our method is at least 4 times faster than the state-of-the-art multiple-fluid method. Examples are provided to demonstrate the new capability and effectiveness of our approach.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"1909fdb87f8da28bc5cdcbc747dd1f7f8ecaff09","venue_1":"ACM Trans. Graph.","year":"2005","title":"A data-driven approach to quantifying natural human motion","authors":"Liu Ren, Alton Patrick, Alexei A. Efros, Jessica K. Hodgins, James M. Rehg","author_ids":"3334600, 2822667, 1763086, 1788773, 1692956","abstract":"In this paper, we investigate whether it is possible to develop a measure that quantifies the naturalness of human motion (as defined by a large database). Such a measure might prove useful in verifying that a motion editing operation had not destroyed the naturalness of a motion capture clip or that a synthetic motion transition was within the space of those seen in natural human motion. We explore the performance of mixture of Gaussians (MoG), hidden Markov models (HMM), and switching linear dynamic systems (SLDS) on this problem. We use each of these statistical models alone and as part of an ensemble of smaller statistical models. We also implement a Naive Bayes (NB) model for a baseline comparison. We test these techniques on motion capture data held out from a database, keyframed motions, edited motions, motions with noise added, and synthetic motion transitions. We present the results as receiver operating characteristic (ROC) curves and compare the results to the judgments made by subjects in a user study.","cites":"78","conferencePercentile":"52.01612903"},{"venue":"ACM Trans. Graph.","id":"338c2a8284f8072a5d89e6a22f9abdf20eb97d9c","venue_1":"ACM Trans. Graph.","year":"2010","title":"Volume contact constraints at arbitrary resolution","authors":"Jérémie Allard, François Faure, Hadrien Courtecuisse, Florent Falipou, Christian Duriez, Paul G. Kry","author_ids":"1702759, 1769618, 2241016, 2803339, 3305697, 1970147","abstract":"We introduce a new method for simulating frictional contact between volumetric objects using interpenetration volume constraints. When applied to complex geometries, our formulation results in dramatically simpler systems of equations than those of traditional mesh contact models. Contact between highly detailed meshes can be simplified to a single unilateral constraint equation, or accurately processed at arbitrary geometry-independent resolution with simultaneous sticking and sliding across contact patches. We exploit fast GPU methods for computing layered depth images, which provides us with the intersection volumes and gradients necessary to formulate the contact equations as linear complementarity problems. Straightforward and popular numerical methods, such as projected Gauss-Seidel, can be used to solve the system. We demonstrate our method in a number of scenarios and present results involving both rigid and deformable objects at interactive rates.","cites":"39","conferencePercentile":"67.54385965"},{"venue":"ACM Trans. Graph.","id":"258589dff05ad42a3238f502c8a2ecc04908f85d","venue_1":"ACM Trans. Graph.","year":"2016","title":"Emptying, refurnishing, and relighting indoor spaces","authors":"Brian Curless","author_ids":"1810052","abstract":"Visualizing changes to indoor scenes is important for many applications. When looking for a new place to live, we want to see how the interior looks not with the current inhabitant's belongings, but with our own furniture. Before purchasing a new sofa, we want to visualize how it would look in our living room. In this paper, we present a system that takes an RGBD scan of an indoor scene and produces a scene model of the empty room, including light emitters, materials, and the geometry of the non-cluttered room. Our system enables realistic rendering not only of the empty room under the original lighting conditions, but also with various scene edits, including adding furniture, changing the material properties of the walls, and relighting. These types of scene edits enable many mixed reality applications in areas such as real estate, furniture retail, and interior design. Our system contains two novel technical contributions: a 3D radiometric calibration process that recovers the appearance of the scene in high dynamic range, and a global-illumination-aware inverse rendering framework that simultaneously recovers reflectance properties of scene surfaces and lighting properties for several light source types, including generalized point and line lights.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"c61c0c2c64b27e4b8e9a0159273b78e7631b0e7f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Learning a manifold of fonts","authors":"Neill D. F. Campbell, Jan Kautz","author_ids":"2036351, 1690538","abstract":"The design and manipulation of typefaces and fonts is an area requiring substantial expertise; it can take many years of study to become a proficient typographer. At the same time, the use of typefaces is ubiquitous; there are many users who, while not experts, would like to be more involved in tweaking or changing existing fonts without suffering the learning curve of professional typography packages.\n Given the wealth of fonts that are available today, we would like to exploit the expertise used to produce these fonts, and to enable everyday users to create, explore, and edit fonts. To this end, we build a generative manifold of standard fonts. Every location on the manifold corresponds to a unique and novel typeface, and is obtained by learning a non-linear mapping that intelligently interpolates and extrapolates existing fonts. Using the manifold, we can smoothly interpolate and move between existing fonts. We can also use the manifold as a constraint that makes a variety of new applications possible. For instance, when editing a single character, we can update all the other glyphs in a font simultaneously to keep them compatible with our changes.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"0feabf6cd2f5417f003c1ece35f863a027fb9cba","venue_1":"ACM Trans. Graph.","year":"1992","title":"A Practical Approach to Calculating Luminance Contrast on a CRT","authors":"Blair MacIntyre, William B. Cowan","author_ids":"1768774, 1750885","abstract":"Luminance contrast is the basis of text legibility, and maintaining luminance contrast is essential for any color selection algorithm. In principle, it can be calculated precisely on a sufficiently well-calibrated display surface, but calibration is very expensive. Consequently, most current systems deal with contrast using heuristics. However, the usual CRT setup puts the display surface into a state that is relatively predictable. Luminance values can be estimated based on this state, and these luminance values have been used to calculate contrast using the Michelson definition. This paper proposes a method for determining the contrast of colored areas displayed on a CRT. It uses a contrast metric that is in wide use in visual psychophysics and shows that the metric can be approximated reasonably without display measurement, as long as it is possible to assume that the CRT has been adjusted according to usual CRT setup standards.","cites":"14","conferencePercentile":"31.57894737"},{"venue":"ACM Trans. Graph.","id":"f6e3638e4b3684f40b39c7316c29539b2c0d2357","venue_1":"ACM Trans. Graph.","year":"2015","title":"RingIt: Ring-Ordering Casual Photos of a Temporal Event","authors":"Hadar Averbuch-Elor, Daniel Cohen-Or","author_ids":"1793313, 1701009","abstract":"The multitude of cameras constantly present nowadays redefines the meaning of capturing an event and the meaning of sharing this event with others. The images are frequently uploaded to a common platform, and the image navigation challenge naturally arises. We introduce <i>RingIt</i>: a spectral technique for recovering the spatial order of a set of still images capturing an event taken by a group of people situated around the event. We assume a nearly instantaneous event, such as an interesting moment in a performance captured by the digital cameras and smartphones of the surrounding crowd. The ordering method extracts the <i>K-nearest neighbors</i> (KNN) of each image from a rough all-pairs dissimilarity estimate. The KNN dissimilarities are refined to form a sparse weighted Laplacian, and a spectral analysis then yields a ring angle for each image. The spatial order is recovered by sorting the obtained ring angles. The ordering of the unorganized set of images allows for a sequential display of the captured object. We demonstrate our technique on a number of sets capturing momentary events, where the images were acquired with low-quality consumer cameras by a group of people positioned around the event.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"b7bbc703782207652a81d5b2dc3f17dbe528a90f","venue_1":"ACM Trans. Graph.","year":"2011","title":"Nonlinear revision control for images","authors":"Hsiang-Ting Chen, Li-Yi Wei, Chun-Fa Chang","author_ids":"3327016, 2420851, 2305933","abstract":"Revision control is a vital component of digital project management and has been widely deployed for text files. Binary files, on the other hand, have received relatively less attention. This can be inconvenient for graphics applications that use a significant amount of binary data, such as images, videos, meshes, and animations. Existing strategies such as storing whole files for individual revisions or simple binary deltas could consume significant storage and obscure vital semantic information. We present a nonlinear revision control system for images, designed with the common digital editing and sketching workflows in mind. We use DAG (directed acyclic graph) as the core structure, with DAG nodes representing editing operations and DAG edges the corresponding spatial, temporal and semantic relationships. We visualize our DAG in RevG (revision graph), which provides not only as a meaningful display of the revision history but also an intuitive interface for common revision control operations such as review, replay, diff, addition, branching, merging, and conflict resolving. Beyond revision control, our system also facilitates artistic creation processes in common image editing and digital painting workflows. We have built a prototype system upon GIMP, an open source image editor, and demonstrate its effectiveness through formative user study and comparisons with alternative revision control systems.","cites":"26","conferencePercentile":"57.36842105"},{"venue":"ACM Trans. Graph.","id":"90b0b63e3dd29cd5c4763d31f4bb599efed7dfb0","venue_1":"ACM Trans. Graph.","year":"2014","title":"Autocomplete painting repetitions","authors":"Jun Xing, Hsiang-Ting Chen, Li-Yi Wei","author_ids":"8262577, 3327016, 2420851","abstract":"Painting is a major form of content creation, offering unlimited control and freedom of expression. However, it can involve tedious manual repetitions, such as stippling large regions or hatching complex contours. Thus, a central goal in digital painting research is to automate tedious repetitions while allowing user control. Existing methods impose a sequential order, in which a small exemplar is prepared and then cloned through additional gestures. Such sequential mode may break the continuous, spontaneous flow of painting. Moreover, it is more suitable for homogeneous areas than nuanced variations common in real paintings.\n We present an interactive digital painting system that auto-completes tedious repetitions while preserving nuanced variations and maintaining natural flows. Specifically, users paint as usual, while our system records and analyzes their workflows. When potential repetition is detected, our system predicts what the user might want to draw and offers auto-completes that adjust to the existing shape-color context. Our method eliminates the need for sequential creation-cloning and better adapts to the local painting contexts. Furthermore, users can choose to accept, ignore, or modify those predictions and thus maintain full control. Our method can be considered as the painting analogy of auto-completes in common typing and IDE systems. We demonstrate the quality and usability of our system through painting results and a pilot user study.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"32d3b3822aed38a0daa361d40a1af6ac2ae56d8a","venue_1":"ACM Trans. Graph.","year":"2015","title":"SHED: shape edit distance for fine-grained shape similarity","authors":"Yanir Kleiman, Oliver van Kaick, Olga Sorkine-Hornung, Daniel Cohen-Or","author_ids":"3119575, 3276873, 2250001, 1701009","abstract":"Computing similarities or distances between 3D shapes is a crucial building block for numerous tasks, including shape retrieval, exploration and classification. Current state-of-the-art distance measures mostly consider the overall appearance of the shapes and are less sensitive to fine changes in shape structure or geometry. We present <i>shape edit distance</i> (SHED) that measures the amount of effort needed to transform one shape into the other, in terms of re-arranging the parts of one shape to match the parts of the other shape, as well as possibly adding and removing parts. The shape edit distance takes into account both the similarity of the overall shape structure and the similarity of individual parts of the shapes. We show that SHED is favorable to state-of-the-art distance measures in a variety of applications and datasets, and is especially successful in scenarios where detecting fine details of the shapes is important, such as shape retrieval and exploration.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"22b7cfc288914fcbe32f973ccd8f80c14a57d1b3","venue_1":"ACM Trans. Graph.","year":"2015","title":"Deep points consolidation","authors":"Shihao Wu, Hui Huang, Minglun Gong, Matthias Zwicker, Daniel Cohen-Or","author_ids":"8175431, 1927737, 2834751, 1796846, 1701009","abstract":"In this paper, we present a consolidation method that is based on a new representation of 3D point sets. The key idea is to augment each surface point into a <i>deep point</i> by associating it with an inner point that resides on the meso-skeleton, which consists of a mixture of skeletal curves and sheets. The deep points representation is a result of a joint optimization applied to both ends of the deep points. The optimization objective is to fairly distribute the end points across the surface and the meso-skeleton, such that the deep point orientations agree with the surface normals. The optimization converges where the inner points form a coherent meso-skeleton, and the surface points are consolidated with the missing regions completed. The strength of this new representation stems from the fact that it is comprised of both local and non-local geometric information. We demonstrate the advantages of the deep points consolidation technique by employing it to consolidate and complete noisy point-sampled geometry with large missing parts.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"58c5f37ceeb5406892920cff1cc276bca0feca88","venue_1":"ACM Trans. Graph.","year":"2015","title":"Generalized cylinder decomposition","authors":"Yang Zhou, Kangxue Yin, Hui Huang, Hao Zhang, Minglun Gong, Daniel Cohen-Or","author_ids":"1686687, 2042641, 1927737, 1682058, 2834751, 1701009","abstract":"Decomposing a complex shape into geometrically simple primitives is a fundamental problem in geometry processing. We are interested in a shape decomposition problem where the simple primitives sought are <i>generalized cylinders</i>, which are ubiquitous in both organic forms and man-made artifacts. We introduce a quantitative measure of <i>cylindricity</i> for a shape part and develop a cylindricity-driven optimization algorithm, with a global objective function, for <i>generalized cylinder decomposition.</i> As a measure of geometric simplicity and following the minimum description length principle, cylindricity is defined as the cost of representing a cylinder through skeletal and cross-section profile curves. Our decomposition algorithm progressively builds local to non-local cylinders, which form over-complete covers of the input shape. The over-completeness of the cylinder covers ensures a conservative buildup of the cylindrical parts, leaving the final decision on decomposition to global optimization. We solve the global optimization by finding an exact cover, which optimizes the global objective function. We demonstrate results of our optimal decomposition algorithm on numerous examples and compare with other alternatives.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"34ded18ce80acede8f7e3780a088b56859917146","venue_1":"ACM Trans. Graph.","year":"2015","title":"Joint embeddings of shapes and images via CNN image purification","authors":"Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, Leonidas J. Guibas","author_ids":"1920864, 2888806, 1898455, 2787217, 1701009, 1744254","abstract":"Both 3D models and 2D images contain a wealth of information about everyday objects in our environment. However, it is difficult to semantically link together these two media forms, even when they feature identical or very similar objects. We propose a <i>joint</i> embedding space populated by both 3D shapes and 2D images of objects, where the distances between embedded entities reflect similarity between the underlying objects. This joint embedding space facilitates comparison between entities of either form, and allows for cross-modality retrieval. We construct the embedding space using 3D shape similarity measure, as 3D shapes are more pure and complete than their appearance in images, leading to more robust distance metrics. We then employ a Convolutional Neural Network (CNN) to \"purify\" images by muting distracting factors. The CNN is trained to map an image to a point in the embedding space, so that it is close to a point attributed to a 3D model of a similar object to the one depicted in the image. This purifying capability of the CNN is accomplished with the help of a large amount of training data consisting of images synthesized from 3D shapes. Our joint embedding allows cross-view image retrieval, image-based shape retrieval, as well as shape-based image retrieval. We evaluate our method on these retrieval tasks and show that it consistently out-performs state-of-the-art methods, and demonstrate the usability of a joint embedding in a number of additional applications.","cites":"22","conferencePercentile":"99.3877551"},{"venue":"ACM Trans. Graph.","id":"067f212f01449753c3bbd01bf4cd5bac12a209e6","venue_1":"ACM Trans. Graph.","year":"2009","title":"On centroidal voronoi tessellation - energy smoothness and fast computation","authors":"Yang Liu, Wenping Wang, Bruno Lévy, Feng Sun, Dong-Ming Yan, Lin Lu, Chenglei Yang","author_ids":"1742731, 1698520, 1705343, 4803694, 1709357, 1687087, 8100457","abstract":"Centroidal Voronoi tessellation (CVT) is a particular type of Voronoi tessellation that has many applications in computational sciences and engineering, including computer graphics. The prevailing method for computing CVT is Lloyd's method, which has linear convergence and is inefficient in practice. We develop new efficient methods for CVT computation and demonstrate the fast convergence of these methods. Specifically, we show that the CVT energy function has 2nd order smoothness for convex domains with smooth density, as well as in most situations encountered in optimization. Due to the 2nd order smoothness, it is possible to minimize the CVT energy functions using Newton-like optimization methods and expect fast convergence. We propose a quasi-Newton method to compute CVT and demonstrate its faster convergence than Lloyd's method with various numerical examples. It is also significantly faster and more robust than the Lloyd-Newton method, a previous attempt to accelerate CVT. We also demonstrate surface remeshing as a possible application.","cites":"95","conferencePercentile":"93.37016575"},{"venue":"ACM Trans. Graph.","id":"5cac68a233a4cc691f9345631527f5d6f146a298","venue_1":"ACM Trans. Graph.","year":"2002","title":"Ordered and quantum treemaps: Making effective use of 2D space to display hierarchies","authors":"Benjamin B. Bederson, Ben Shneiderman, Martin Wattenberg","author_ids":"1799187, 1740403, 1764291","abstract":"Treemaps, a space-filling method for visualizing large hierarchical data sets, are receiving increasing attention. Several algorithms have been previously proposed to create more useful displays by controlling the aspect ratios of the rectangles that make up a treemap. While these algorithms do improve visibility of small items in a single layout, they introduce instability over time in the display of dynamically changing data, fail to preserve order of the underlying data, and create layouts that are difficult to visually search. In addition, continuous treemap algorithms are not suitable for displaying fixed-sized objects within them, such as images.This paper introduces a new \"strip\" treemap algorithm which addresses these shortcomings, and analyzes other \"pivot\" algorithms we recently developed showing the trade-offs between them. These ordered treemap algorithms ensure that items near each other in the given order will be near each other in the treemap layout. Using experimental evidence from Monte Carlo trials and from actual stock market data, we show that, compared to other layout algorithms, ordered treemaps are more stable, while maintaining relatively favorable aspect ratios of the constituent rectangles. A user study with 20 participants clarifies the human performance benefits of the new algorithms. Finally, we present quantum treemap algorithms, which modify the layout of the continuous treemap algorithms to generate rectangles that are integral multiples of an input object size. The quantum treemap algorithm has been applied to PhotoMesa, an application that supports browsing of large numbers of images.","cites":"215","conferencePercentile":"78"},{"venue":"ACM Trans. Graph.","id":"0a272343fffc082642a7adbbb26dbdec7a518296","venue_1":"ACM Trans. Graph.","year":"2011","title":"Slices: a shape-proxy based on planar sections","authors":"James McCrae, Karan Singh, Niloy J. Mitra","author_ids":"2256139, 1682205, 1710455","abstract":"Minimalist object representations or <i>shape-proxies</i> that spark and inspire human perception of shape remain an incompletely understood, yet powerful aspect of visual communication. We explore the use of planar sections, i.e., the contours of intersection of planes with a 3D object, for creating shape abstractions, motivated by their popularity in art and engineering. We first perform a user study to show that humans do define consistent and similar planar section proxies for common objects. Interestingly, we observe a strong correlation between user-defined planes and geometric features of objects. Further we show that the problem of finding the minimum set of planes that capture a set of 3D geometric shape features is both NP-hard and not always the proxy a user would pick. Guided by the principles inferred from our user study, we present an algorithm that progressively selects planes to maximize feature coverage, which in turn influence the selection of subsequent planes. The algorithmic framework easily incorporates various shape features, while their relative importance values are computed and validated from the user study data. We use our algorithm to compute planar slices for various objects, validate their utility towards object abstraction using a second user study, and conclude showing the potential applications of the extracted planar slice shape proxies.","cites":"32","conferencePercentile":"68.15789474"},{"venue":"ACM Trans. Graph.","id":"02086be014c4a276663e66ffde4d14f9c4cebe7e","venue_1":"ACM Trans. Graph.","year":"2014","title":"BiggerPicture: data-driven image extrapolation using graph matching","authors":"Miao Wang, Yu-Kun Lai, Yuan Liang, Ralph R. Martin, Shi-Min Hu","author_ids":"1693285, 7827503, 4758461, 4326042, 1686809","abstract":"Filling a small hole in an image with plausible content is well studied. Extrapolating an image to give a distinctly larger one is much more challenging---a significant amount of additional content is needed which matches the original image, especially near its boundaries. We propose a data-driven approach to this problem. Given a source image, and the amount and direction(s) in which it is to be extrapolated, our system determines visually consistent content for the extrapolated regions using library images. As well as considering low-level matching, we achieve consistency at a higher level by using graph proxies for regions of source and library images. Treating images as graphs allows us to find candidates for image extrapolation in a feasible time. Consistency of subgraphs in source and library images is used to find good candidates for the additional content; these are then further filtered. Region boundary curves are aligned to ensure consistency where image parts are joined using a photomontage method. We demonstrate the power of our method in image editing applications.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"001e8b3de9e1dcd5ac2b1186e3534e94706915cf","venue_1":"ACM Trans. Graph.","year":"2009","title":"3D polyomino puzzle","authors":"Kui-Yip Lo, Chi-Wing Fu, Hongwei Li","author_ids":"2835414, 1699457, 3018623","abstract":"This paper presents a computer-aided geometric design approach to realize a new genre of 3D puzzle, namely the <i>3D Polyomino puzzle</i>. We base our puzzle pieces on the family of 2D shapes known as <i>polyominoes</i> in recreational mathematics, and construct the 3D puzzle model by covering its geometry with polyominolike shapes. We first apply quad-based surface parametrization to the input solid, and tile the parametrized surface with polyominoes. Then, we construct a nonintersecting offset surface inside the input solid and shape the puzzle pieces to fit inside a thick shell volume. Finally, we develop a family of associated techniques for precisely constructing the geometry of individual puzzle pieces, including the ring-based ordering scheme, the motion space analysis technique, and the tab and blank construction method. The final completed puzzle model is guaranteed to be not only buildable, but also interlocking and maintainable.","cites":"12","conferencePercentile":"17.40331492"},{"venue":"ACM Trans. Graph.","id":"15d3398120eb2410e3fbc58f7f356432f3090e26","venue_1":"ACM Trans. Graph.","year":"2013","title":"Content-adaptive image downscaling","authors":"Johannes Kopf, Ariel Shamir, Pieter Peers","author_ids":"2891193, 2947946, 1808270","abstract":"This paper introduces a novel <i>content-adaptive</i> image downscaling method. The key idea is to optimize the shape and locations of the downsampling kernels to better align with local image features. Our content-adaptive kernels are formed as a bilateral combination of two Gaussian kernels defined over space and color, respectively. This yields a continuum ranging from smoothing to edge/detail preserving kernels driven by image content. We optimize these kernels to represent the input image well, by finding an output image from which the input can be well reconstructed. This is technically realized as an iterative maximum-likelihood optimization using a constrained variation of the Expectation-Maximization algorithm. In comparison to previous downscaling algorithms, our results remain crisper without suffering from ringing artifacts. Besides natural images, our algorithm is also effective for creating <i>pixel art</i> images from vector graphics inputs, due to its ability to keep linear features sharp <i>and</i> connected.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"30b1d85d97e5e50af01a77984d8430c22cb90d60","venue_1":"ACM Trans. Graph.","year":"2012","title":"Digital reconstruction of halftoned color comics","authors":"Johannes Kopf, Dani Lischinski","author_ids":"2891193, 1684384","abstract":"We introduce a method for automated conversion of scanned color comic books and graphical novels into a new high-fidelity rescalable digital representation. Since crisp black line artwork and lettering are the most important structural and stylistic elements in this important genre of color illustrations, our digitization process is geared towards faithful reconstruction of these elements. This is a challenging task, because commercial presses perform halftoning (screening) to approximate continuous tones and colors with overlapping grids of dots. Although a large number of inverse haftoning (descreening) methods exist, they typically blur the intricate black artwork. Our approach is specifically designed to descreen color comics, which typically reproduce color using screened CMY inks, but print the black artwork using non-screened solid black ink. After separating the scanned image into three screening grids, one for each of the CMY process inks, we use non-linear optimization to fit a parametric model describing each grid, and simultaneously recover the non-screened black ink layer, which is then vectorized. The result of this process is a high quality, compact, and rescalable digital representation of the original artwork.","cites":"7","conferencePercentile":"9.848484848"},{"venue":"ACM Trans. Graph.","id":"13c71c7450aa27a9f926adb6cead1f338c7967c5","venue_1":"ACM Trans. Graph.","year":"2011","title":"Depixelizing pixel art","authors":"Johannes Kopf, Dani Lischinski","author_ids":"2891193, 1684384","abstract":"We describe a novel algorithm for extracting a resolution-independent vector representation from <i>pixel art</i> images, which enables magnifying the results by an arbitrary amount without image degradation. Our algorithm resolves pixel-scale features in the input and converts them into regions with smoothly varying shading that are crisply separated by piecewise-smooth contour curves. In the original image, pixels are represented on a square pixel lattice, where diagonal neighbors are only connected through a single point. This causes thin features to become visually disconnected under magnification by conventional means, and creates ambiguities in the connectedness and separation of diagonal neighbors. The key to our algorithm is in resolving these ambiguities. This enables us to reshape the pixel cells so that neighboring pixels belonging to the same feature are connected through edges, thereby preserving the feature connectivity under magnification. We reduce pixel aliasing artifacts and improve smoothness by fitting spline curves to contours in the image and optimizing their control points.","cites":"23","conferencePercentile":"50.52631579"},{"venue":"ACM Trans. Graph.","id":"70bbd9159e9021509c89a0b94655c5741bfb7fd1","venue_1":"ACM Trans. Graph.","year":"2013","title":"Fourier analysis of stochastic sampling strategies for assessing bias and variance in integration","authors":"Kartic Subr, Jan Kautz","author_ids":"2740413, 1690538","abstract":"Each pixel in a photorealistic, computer generated picture is calculated by approximately integrating all the light arriving at the pixel, from the virtual scene. A common strategy to calculate these high-dimensional integrals is to average the estimates at stochastically sampled locations. The strategy with which the sampled locations are chosen is of utmost importance in deciding the quality of the approximation, and hence rendered image.\n We derive connections between the spectral properties of stochastic sampling patterns and the first and second order statistics of estimates of integration using the samples. Our equations provide insight into the assessment of stochastic sampling strategies for integration. We show that the amplitude of the expected Fourier spectrum of sampling patterns is a useful indicator of the bias when used in numerical integration. We deduce that estimator variance is directly dependent on the variance of the sampling spectrum over multiple realizations of the sampling pattern. We then analyse Gaussian jittered sampling, a simple variant of jittered sampling, that allows a smooth trade-off of bias for variance in uniform (regular grid) sampling. We verify our predictions using spectral measurement, quantitative integration experiments and qualitative comparisons of rendered images.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"7f951c2ca59bd4074bbe660be3b397d9d843c673","venue_1":"ACM Trans. Graph.","year":"2010","title":"Acquisition and analysis of bispectral bidirectional reflectance and reradiation distribution functions","authors":"Matthias B. Hullin, Johannes Hanika, Boris Ajdin, Hans-Peter Seidel, Jan Kautz, Hendrik P. A. Lensch","author_ids":"1899671, 3230881, 2027612, 1746884, 1690538, 1809190","abstract":"In fluorescent materials, light from a certain band of incident wavelengths is reradiated at longer wavelengths, i.e., with a reduced per-photon energy. While fluorescent materials are common in everyday life, they have received little attention in computer graphics. Especially, no bidirectional reradiation measurements of fluorescent materials have been available so far. In this paper, we extend the well-known concept of the bidirectional reflectance distribution function (BRDF) to account for energy transfer between wavelengths, resulting in a <i>Bispectral Bidirectional Reflectance and Reradiation Distribution Function (bispectral BRRDF).</i> Using a bidirectional and bispectral measurement setup, we acquire reflectance and reradiation data of a variety of fluorescent materials, including vehicle paints, paper and fabric, and compare their renderings with RGB, RGBxRGB, and spectral BRDFs. Our acquisition is guided by a principal component analysis on complete bispectral data taken under a sparse set of angles. We show that in order to faithfully reproduce the full bispectral information for all other angles, only a very small number of wavelength pairs needs to be measured at a high angular resolution.","cites":"18","conferencePercentile":"28.3625731"},{"venue":"ACM Trans. Graph.","id":"477bcc0b02e71d2a113ce0ef330c8dd2bd13bbd5","venue_1":"ACM Trans. Graph.","year":"2010","title":"Interactive on-surface signal deformation","authors":"Tobias Ritschel, Thorsten Thormählen, Carsten Dachsbacher, Jan Kautz, Hans-Peter Seidel","author_ids":"1759347, 2543070, 1705803, 1690538, 1746884","abstract":"We present an interactive system for the artistic control of visual phenomena visible on surfaces. Our method allows the user to intuitively reposition shadows, caustics, and indirect illumination using a simple click-and-drag user interface working directly on surfaces. In contrast to previous approaches, the positions of the lights or objects in the scene remain unchanged, enabling localized edits of individual shading components. Our method facilitates the editing by computing a mapping from one surface location to another. Based on this mapping, we can not only edit shadows, caustics, and indirect illumination but also other surface properties, such as color or texture, in a unified way. This is achieved using an intuitive user-interface that allows the user to specify position constraints with drag-and-drop or sketching operations directly on the surface. Our approach requires no explicit surface parametrization and handles scenes with arbitrary topology. We demonstrate the applicability of the approach to interactive editing of shadows, reflections, refractions, textures, caustics, and diffuse indirect light. The effectiveness of the system to achieve an artistic goal is evaluated by a user study.","cites":"11","conferencePercentile":"14.03508772"},{"venue":"ACM Trans. Graph.","id":"e263962c5365333817890e667d08cb4d263dd146","venue_1":"ACM Trans. Graph.","year":"2009","title":"Visio-lization: generating novel facial images","authors":"Umar Mohammed, Simon Prince, Jan Kautz","author_ids":"2711159, 2814165, 1690538","abstract":"Our goal is to generate novel realistic images of faces using a model trained from real examples. This model consists of two components: First we consider face images as samples from a texture with spatially varying statistics and describe this texture with a local non-parametric model. Second, we learn a parametric global model of all of the pixel values. To generate realistic faces, we combine the strengths of both approaches and condition the local non-parametric model on the global parametric model. We demonstrate that with appropriate choice of local and global models it is possible to reliably generate new realistic face images that do not correspond to any individual in the training data. We extend the model to cope with considerable intra-class variation (pose and illumination). Finally, we apply our model to editing real facial images: we demonstrate image in-painting, interactive techniques for improving synthesized images and modifying facial expressions.","cites":"22","conferencePercentile":"31.7679558"},{"venue":"ACM Trans. Graph.","id":"59b8c2f6111d8b61d6609e6068362e114de10da3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Data-driven curvature for real-time line drawing of dynamic scenes","authors":"Evangelos Kalogerakis, Derek Nowrouzezahrai, Patricio D. Simari, James McCrae, Aaron Hertzmann, Karan Singh","author_ids":"2808670, 1795014, 2876838, 2256139, 1747779, 1682205","abstract":"This article presents a method for real-time line drawing of deforming objects. Object-space line drawing algorithms for many types of curves, including suggestive contours, highlights, ridges, and valleys, rely on surface curvature and curvature derivatives. Unfortunately, these curvatures and their derivatives cannot be computed in real-time for animated, deforming objects. In a preprocessing step, our method learns the mapping from a low-dimensional set of animation parameters (e.g., joint angles) to surface curvatures for a deforming 3D mesh. The learned model can then accurately and efficiently predict curvatures and their derivatives, enabling real-time object-space rendering of suggestive contours and other such curves. This represents an order-of-magnitude speedup over the fastest existing algorithm capable of estimating curvatures and their derivatives accurately enough for many different types of line drawings. The learned model can generalize to novel animation sequences and is also very compact, typically requiring a few megabytes of storage at runtime. We demonstrate our method for various types of animated objects, including skeleton-based characters, cloth simulation, and blend-shape facial animation, using a variety of nonphotorealistic rendering styles.\n An important component of our system is the use of dimensionality reduction for differential mesh data. We show that Independent Component Analysis (ICA) yields localized basis functions, and gives superior generalization performance to that of Principal Component Analysis (PCA).","cites":"12","conferencePercentile":"17.40331492"},{"venue":"ACM Trans. Graph.","id":"9486056dd59ec33f05657af0c03d04a188900a4e","venue_1":"ACM Trans. Graph.","year":"2011","title":"Candid portrait selection from video","authors":"Juliet Fiss, Aseem Agarwala, Brian Curless","author_ids":"1763981, 1696487, 1810052","abstract":"In this paper, we train a computer to select still frames from video that work well as candid portraits. Because of the subjective nature of this task, we conduct a human subjects study to collect ratings of video frames across multiple videos. Then, we compute a number of features and train a model to predict the average rating of a video frame. We evaluate our model with cross-validation, and show that it is better able to select quality still frames than previous techniques, such as simply omitting frames that contain blinking or motion blur, or selecting only smiles. We also evaluate our technique qualitatively on videos that were not part of our validation set, and were taken outdoors and under different lighting conditions.","cites":"20","conferencePercentile":"44.21052632"},{"venue":"ACM Trans. Graph.","id":"20fdaa1383b1b879f47dc10d79a1c0e6463b8535","venue_1":"ACM Trans. Graph.","year":"2011","title":"Modeling and generating moving trees from video","authors":"Chuan Li, Oliver Deussen, Yi-Zhe Song, Philip J. Willis, Peter M. Hall","author_ids":"3238349, 1850438, 1705408, 7424533, 2067332","abstract":"We present a probabilistic approach for the automatic production of tree models with convincing 3D appearance and motion. The only input is a video of a moving tree that provides us an initial dynamic tree model, which is used to generate new individual trees of the same type. Our approach combines global and local constraints to construct a dynamic 3D tree model from a 2D skeleton. Our modeling takes into account factors such as the shape of branches, the overall shape of the tree, and physically plausible motion. Furthermore, we provide a generative model that creates multiple trees in 3D, given a single example model. This means that users no longer have to make each tree individually, or specify rules to make new trees. Results with different species are presented and compared to both reference input data and state of the art alternatives.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"3f71d3034157e99001a3ac983f65113b55719061","venue_1":"ACM Trans. Graph.","year":"2016","title":"3D attention-driven depth acquisition for object identification","authors":"Kai Xu, Yifei Shi, Lintao Zheng, Junyu Zhang, Min Liu, Hui Huang, Hao Su, Daniel Cohen-Or, Baoquan Chen","author_ids":"1723225, 8064820, 3387947, 8214836, 1721760, 1754290, 2888806, 1701009, 1748939","abstract":"We address the problem of autonomously exploring unknown objects in a scene by consecutive depth acquisitions. The goal is to reconstruct the scene while online identifying the objects from among a large collection of 3D shapes. Fine-grained shape identification demands a meticulous series of observations attending to varying views and parts of the object of interest. Inspired by the recent success of attention-based models for 2D recognition, we develop a <i>3D Attention Model</i> that selects the best views to scan from, as well as the most informative regions in each view to focus on, to achieve efficient object recognition. The region-level attention leads to focus-driven features which are quite robust against object occlusion. The attention model, trained with the 3D shape collection, encodes the temporal dependencies among consecutive views with deep recurrent networks. This facilitates order-aware view planning accounting for robot movement cost. In achieving instance identification, the shape collection is organized into a hierarchy, associated with pre-trained hierarchical classifiers. The effectiveness of our method is demonstrated on an autonomous robot (PR) that explores a scene and identifies the objects to construct a 3D scene model.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"149e2d40c8890806be2282cbe163d1b131a3e2ce","venue_1":"ACM Trans. Graph.","year":"2016","title":"Time-varying weathering in texture space","authors":"Rachele Bellini, Yanir Kleiman, Daniel Cohen-Or","author_ids":"2143738, 3119575, 1701009","abstract":"We present a technique to synthesize time-varying weathered textures. Given a single texture image as input, the degree of weathering at different regions of the input texture is estimated by prevalence analysis of texture patches. This information then allows to gracefully increase or decrease the popularity of weathered patches, simulating the evolution of texture appearance both backward and forward in time. Our method can be applied to a wide variety of different textures since the reaction of the material to weathering effects is physically-oblivious and learned from the input texture itself. The weathering process evolves new structures as well as color variations, providing rich and natural results. In contrast with existing methods, our method does not require any user interaction or assistance. We demonstrate our technique on various textures, and their application to time-varying weathering of 3D scenes. We also extend our method to handle multi-layered textures, weathering transfer, and interactive weathering painting.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"0455b7bf43a8883d414938017748f6b45d99e3ca","venue_1":"ACM Trans. Graph.","year":"2009","title":"Capacity-constrained point distributions: a variant of Lloyd's method","authors":"Michael Balzer, Thomas Schlömer, Oliver Deussen","author_ids":"2034021, 2283817, 1850438","abstract":"We present a new general-purpose method for optimizing existing point sets. The resulting distributions possess high-quality blue noise characteristics and adapt precisely to given density functions. Our method is similar to the commonly used Lloyd's method while avoiding its drawbacks. We achieve our results by utilizing the concept of capacity, which for each point is determined by the area of its Voronoi region weighted with an underlying density function. We demand that each point has the same capacity. In combination with a dedicated optimization algorithm, this capacity constraint enforces that each point obtains equal importance in the distribution. Our method can be used as a drop-in replacement for Lloyd's method, and combines enhancement of blue noise characteristics and density function adaptation in one operation.","cites":"63","conferencePercentile":"82.04419889"},{"venue":"ACM Trans. Graph.","id":"15dddf86956dd722942abdafa6bc0fa0d834cdba","venue_1":"ACM Trans. Graph.","year":"2016","title":"Structure-oriented networks of shape collections","authors":"Noa Fish, Oliver van Kaick, Amit Bermano, Daniel Cohen-Or","author_ids":"2787217, 3276873, 1755628, 1701009","abstract":"We introduce a co-analysis technique designed for correspondence inference within large shape collections. Such collections are naturally rich in variation, adding ambiguity to the notoriously difficult problem of correspondence computation. We leverage the robustness of correspondences between similar shapes to address the difficulties associated with this problem. In our approach, pairs of similar shapes are extracted from the collection, analyzed and matched in an efficient and reliable manner, culminating in the construction of a network of correspondences that connects the entire collection. The correspondence between any pair of shapes then amounts to a simple propagation along the minimax path between the two shapes in the network. At the heart of our approach is the introduction of a robust, structure-oriented shape matching method. Leveraging the idea of projective analysis, we partition 2D projections of a shape to obtain a set of 1D ordered regions, which are both simple and efficient to match. We lift the matched projections back to the 3D domain to obtain a pairwise shape correspondence. The emphasis given to structural compatibility is a central tool in estimating the reliability and completeness of a computed correspondence, uncovering any non-negligible semantic discrepancies that may exist between shapes. These detected differences are a deciding factor in the establishment of a network aiming to capture local similarities. We demonstrate that the combination of the presented observations into a co-analysis method allows us to establish reliable correspondences among shapes within large collections.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"3aaed9fa8f90b2bc1f49bb042c83d47b147c5249","venue_1":"ACM Trans. Graph.","year":"2003","title":"Continuous contact simulation for smooth surfaces","authors":"Paul G. Kry, Dinesh K. Pai","author_ids":"1970147, 1694975","abstract":"Dynamics simulation of smooth surfaced rigid bodies in contact is a critical problem in physically based animation and interactive virtual environments. We describe a technique that uses reduced coordinates to evolve a single continuous contact between smooth piecewise parametric surfaces. The incorporation of friction into our algorithm is straightforward. The dynamics equations, although slightly more complex due to the reduced coordinate formulation, can be integrated easily using explicit integrators without the need for constraint stabilization. Reduced coordinates confine integration errors to the constraint manifold, thereby permitting a wide choice of step sizes with visually acceptable results. We demonstrate these results using Loop Subdivision surfaces with parametric evaluation.","cites":"19","conferencePercentile":"4.301075269"},{"venue":"ACM Trans. Graph.","id":"12ad462341296288797bb12cd4c5b8041a079b2a","venue_1":"ACM Trans. Graph.","year":"2014","title":"True2Form: 3D curve networks from 2D sketches via selective regularization","authors":"Bao-Xuan Xu, William Chang, Alla Sheffer, Adrien Bousseau, James McCrae, Karan Singh","author_ids":"2536033, 2540201, 3354923, 2149814, 2256139, 1682205","abstract":"<i>True2Form</i> is a sketch-based modeling system that reconstructs 3D curves from typical design sketches. Our approach to infer 3D form from 2D drawings is a novel mathematical framework of insights derived from perception and design literature. We note that designers favor viewpoints that maximally reveal 3D shape information, and strategically sketch descriptive curves that convey intrinsic shape properties, such as curvature, symmetry, or parallelism. Studies indicate that viewers apply these properties selectively to envision a globally consistent 3D shape. We mimic this selective regularization algorithmically, by progressively detecting and enforcing applicable properties, accounting for their global impact on an evolving 3D curve network. Balancing regularity enforcement against sketch fidelity at each step allows us to correct for inaccuracy inherent in free-hand sketching. We perceptually validate our approach by showing agreement between our algorithm and viewers in selecting applicable regularities. We further evaluate our solution by: reconstructing a range of 3D models from diversely sourced sketches; comparisons to prior art; and visual comparison to both ground-truth and 3D reconstructions by designers.","cites":"26","conferencePercentile":"95.0617284"},{"venue":"ACM Trans. Graph.","id":"0bda148be82128180fb40c38a27e7c1aedadb823","venue_1":"ACM Trans. Graph.","year":"2011","title":"Matting and compositing of transparent and refractive objects","authors":"Sai Kit Yeung, Chi-Keung Tang, Michael S. Brown, Sing Bing Kang","author_ids":"1698659, 2546217, 1742730, 1738740","abstract":"This article introduces a new approach for matting and compositing transparent and refractive objects in photographs. The key to our work is an image-based matting model, termed the <i>Attenuation-Refraction Matte</i> (ARM), that encodes plausible refractive properties of a transparent object along with its observed specularities and transmissive properties. We show that an object's ARM can be extracted directly from a photograph using simple user markup. Once extracted, the ARM is used to paste the object onto a new background with a variety of effects, including compound compositing, Fresnel effect, scene depth, and even caustic shadows. User studies find our results favorable to those obtained with Photoshop as well as perceptually valid in most cases. Our approach allows photo editing of transparent and refractive objects in a manner that produces realistic effects previously only possible via 3D models or environment matting.","cites":"9","conferencePercentile":"15.78947368"},{"venue":"ACM Trans. Graph.","id":"2df32452eeb42cb100b3af22d792c5abdb1ca654","venue_1":"ACM Trans. Graph.","year":"2016","title":"Connected fermat spirals for layered fabrication","authors":"Haisen Zhao, Fanglin Gu, Qi-Xing Huang, Jorge A. Garcia Galicia, Yong Chen, Changhe Tu, Bedrich Benes, Hao Zhang, Daniel Cohen-Or, Baoquan Chen","author_ids":"3266088, 1752408, 1734006, 8160377, 5511325, 3327879, 7590480, 1682058, 1701009, 1748939","abstract":"We develop a new kind of \"space-filling\" curves, <i>connected Fermat spirals</i>, and show their compelling properties as a tool path fill pattern for layered fabrication. Unlike classical space-filling curves such as the Peano or Hilbert curves, which constantly wind and bind to preserve locality, connected Fermat spirals are formed mostly by <i>long, low-curvature</i> paths. This geometric property, along with continuity, influences the quality and efficiency of layered fabrication. Given a connected 2D region, we first decompose it into a set of sub-regions, each of which can be filled with a single continuous Fermat spiral. We show that it is always possible to start and end a Fermat spiral fill at approximately the same location on the outer boundary of the filled region. This special property allows the Fermat spiral fills to be joined systematically along a graph traversal of the decomposed sub-regions. The result is a globally continuous curve. We demonstrate that printing 2D layers following tool paths as connected Fermat spirals leads to efficient and quality fabrication, compared to conventional fill patterns.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"213c067c531d7aca0cb4bc3a27496e9ca8659ed9","venue_1":"ACM Trans. Graph.","year":"2006","title":"Image enhancement by unsharp masking the depth buffer","authors":"Thomas Luft, Carsten Colditz, Oliver Deussen","author_ids":"1972918, 2448311, 1850438","abstract":"We present a simple and efficient method to enhance the perceptual quality of images that contain depth information. Similar to an unsharp mask, the difference between the original depth buffer content and a low-pass filtered copy is utilized to determine information about spatially important areas in a scene. Based on this information we locally enhance the contrast, color, and other parameters of the image. Our technique aims at improving the perception of complex scenes by introducing additional depth cues. The idea is motivated by artwork and findings in the field of neurology, and can be applied to images of any kind, ranging from complex landscape data and technical artifacts, to volume rendering, photograph, and video with depth information.","cites":"102","conferencePercentile":"76.85185185"},{"venue":"ACM Trans. Graph.","id":"8fa6dd7100e70147d72d79ef1e866988ff5e7765","venue_1":"ACM Trans. Graph.","year":"2003","title":"TreeJuxtaposer: scalable tree comparison using Focus+Context with guaranteed visibility","authors":"Tamara Munzner, François Guimbretière, Serdar Tasiran, Li Zhang, Yunhong Zhou","author_ids":"1732016, 2539134, 1797515, 1735174, 1772745","abstract":"Structural comparison of large trees is a difficult task that is only partially supported by current visualization techniques, which are mainly designed for browsing. We present TreeJuxtaposer, a system designed to support the comparison task for large trees of several hundred thousand nodes. We introduce the idea of \"guaranteed visibility\", where highlighted areas are treated as landmarks that must remain visually apparent at all times. We propose a new methodology for detailed structural comparison between two trees and provide a new nearly-linear algorithm for computing the best corresponding node from one tree to another. In addition, we present a new rectilinear Focus+Context technique for navigation that is well suited to the dynamic linking of side-by-side views while guaranteeing landmark visibility and constant frame rates. These three contributions result in a system delivering a fluid exploration experience that scales both in the size of the dataset and the number of pixels in the display. We have based the design decisions for our system on the needs of a target audience of biologists who must understand the structural details of many phylogenetic, or evolutionary, trees. Our tool is also useful in many other application domains where tree comparison is needed, ranging from network management to call graph optimization to genealogy.","cites":"7","conferencePercentile":"2.150537634"},{"venue":"ACM Trans. Graph.","id":"05c168c323c3c4bd1cb4f03b2d463ad433c96777","venue_1":"ACM Trans. Graph.","year":"2003","title":"Image-based reconstruction of spatial appearance and geometric detail","authors":"Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, Hans-Peter Seidel","author_ids":"1809190, 1690538, 1689293, 1752192, 1746884","abstract":"Real-world objects are usually composed of a number of different materials that often show subtle changes even within a single material. Photorealistic rendering of such objects requires accurate measurements of the reflection properties of each material, as well as the spatially varying effects. We present an image-based measuring method that robustly detects the different materials of real objects and fits an average bidirectional reflectance distribution function (BRDF) to each of them. In order to model local changes as well, we project the measured data for each surface point into a basis formed by the recovered BRDFs leading to a truly spatially varying BRDF representation. Real-world objects often also have fine geometric detail that is not represented in an acquired mesh. To increase the detail, we derive normal maps even for non-Lambertian surfaces using our measured BRDFs. A high quality model of a real object can be generated with relatively little input data. The generated model allows for rendering under arbitrary viewing and lighting conditions and realistically reproduces the appearance of the original object.","cites":"195","conferencePercentile":"79.03225806"},{"venue":"ACM Trans. Graph.","id":"5764807291f90f24da8eb7e77ae28dc42161e713","venue_1":"ACM Trans. Graph.","year":"2007","title":"Interactive editing and modeling of bidirectional texture functions","authors":"Jan Kautz, Solomon Boulos, Frédo Durand","author_ids":"1690538, 3164352, 1728125","abstract":"While measured Bidirectional Texture Functions (BTF) enable impressive realism in material appearance, they offer little control, which limits their use for content creation. In this work, we interactively manipulate BTFs and create new BTFs from flat textures. We present an out-of-core approach to manage the size of BTFs and introduce new editing operations that modify the appearance of a material. These tools achieve their full potential when selectively applied to subsets of the BTF through the use of new selection operators. We further analyze the use of our editing operators for the modification of important visual characteristics such as highlights, roughness, and fuzziness. Results compare favorably to the direct alteration of micro-geometry and reflectances of synthetic reference data.","cites":"25","conferencePercentile":"19.6"},{"venue":"ACM Trans. Graph.","id":"dad9e36c4dfe94ccd4a71dee64d3572fdfa1431e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Dapper: decompose-and-pack for 3D printing","authors":"Xuelin Chen, Hao Zhang, Jinjie Lin, Ruizhen Hu, Lin Lu, Qi-Xing Huang, Bedrich Benes, Daniel Cohen-Or, Baoquan Chen","author_ids":"1885508, 1682058, 2817530, 2154334, 1687087, 1734006, 7590480, 1701009, 1748939","abstract":"We pose the <i>decompose-and-pack</i> or <i>DAP</i> problem, which tightly combines shape decomposition and packing. While in general, DAP seeks to decompose an input shape into a <i>small number</i> of parts which can be <i>efficiently</i> packed, our focus is geared towards 3D printing. The goal is to optimally decompose-and-pack a 3D object into a printing volume to minimize support material, build time, and assembly cost. We present <i>Dapper</i>, a global optimization algorithm for the DAP problem which can be applied to both powder- and FDM-based 3D printing. The solution search is top-down and iterative. Starting with a coarse decomposition of the input shape into few initial parts, we progressively pack a pile in the printing volume, by iteratively docking parts, possibly while introducing cuts, onto the pile. Exploration of the search space is via a <i>prioritized</i> and <i>bounded beam search</i>, with breadth and depth pruning guided by local and global DAP objectives. A key feature of Dapper is that it works with <i>pyramidal</i> primitives, which are packing- and printing-friendly. Pyramidal shapes are also more general than boxes to reduce part counts, while still maintaining a suitable level of simplicity to facilitate DAP optimization. We demonstrate printing efficiency gains achieved by Dapper, compare to state-of-the-art alternatives, and show how fabrication criteria such as cut area and part size can be easily incorporated into our solution framework to produce more physically plausible fabrications.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"455d52ddf700f10ce133c9ac32f0a4e327b145a0","venue_1":"ACM Trans. Graph.","year":"2008","title":"Real-time, all-frequency shadows in dynamic scenes","authors":"Thomas Annen, Zhao Dong, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, Jan Kautz","author_ids":"2954835, 3385123, 1736035, 7583073, 1746884, 1690538","abstract":"Shadow computation in dynamic scenes under complex illumination is a challenging problem. Methods based on precomputation provide accurate, real-time solutions, but are hard to extend to dynamic scenes. Specialized approaches for soft shadows can deal with dynamic objects but are not fast enough to handle more than one light source. In this paper, we present a technique for rendering dynamic objects under arbitrary environment illumination, which does not require any precomputation. The key ingredient is a fast, approximate technique for computing soft shadows, which achieves several hundred frames per second for a single light source. This allows for approximating environment illumination with a sparse collection of area light sources and yields real-time frame rates.","cites":"59","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"4b9e123a7a1e50b73d99d8341365fcd0ce8e0b44","venue_1":"ACM Trans. Graph.","year":"2009","title":"Micro-rendering for scalable, parallel final gathering","authors":"Tobias Ritschel, Thomas Engelhardt, Thorsten Grosch, Hans-Peter Seidel, Jan Kautz, Carsten Dachsbacher","author_ids":"1759347, 4209266, 8221420, 1746884, 1690538, 1705803","abstract":"Recent approaches to global illumination for dynamic scenes achieve interactive frame rates by using coarse approximations to geometry, lighting, or both, which limits scene complexity and rendering quality. High-quality global illumination renderings of complex scenes are still limited to methods based on ray tracing. While conceptually simple, these techniques are computationally expensive. We present an efficient and scalable method to compute global illumination solutions at interactive rates for complex and dynamic scenes. Our method is based on parallel final gathering running entirely on the GPU. At each final gathering location we perform <i>micro-rendering:</i> we traverse and rasterize a hierarchical point-based scene representation into an importance-warped <i>micro-buffer</i>, which allows for BRDF importance sampling. The final reflected radiance is computed at each gathering location using the micro-buffers and is then stored in image-space. We can trade quality for speed by reducing the sampling rate of the gathering locations in conjunction with bilateral upsampling. We demonstrate the applicability of our method to interactive global illumination, the simulation of multiple indirect bounces, and to final gathering from photon maps.","cites":"57","conferencePercentile":"76.79558011"},{"venue":"ACM Trans. Graph.","id":"07dd722a7d65eac28a30a9207d7e8d4f4004924e","venue_1":"ACM Trans. Graph.","year":"2016","title":"Printed Perforated Lampshades for Continuous Projective Images","authors":"Haisen Zhao, Lin Lu, Yuan Wei, Dani Lischinski, Andrei Sharf, Daniel Cohen-Or, Baoquan Chen","author_ids":"3266088, 1687087, 2873392, 1684384, 2120270, 1701009, 1748939","abstract":"We present a technique for designing three-dimensional- (3D) printed perforated lampshades that project continuous grayscale images onto the surrounding walls. Given the geometry of the lampshade and a target grayscale image, our method computes a distribution of tiny holes over the shell, such that the combined footprints of the light emanating through the holes form the target image on a nearby diffuse surface. Our objective is to approximate the continuous tones and the spatial detail of the target image to the extent possible within the constraints of the fabrication process.\n To ensure structural integrity, there are lower bounds on the thickness of the shell, the radii of the holes, and the minimal distances between adjacent holes. Thus, the holes are realized as thin tubes distributed over the lampshade surface. The amount of light passing through a single tube may be controlled by the tube&#8217;s radius and by its orientation (tilt angle). The core of our technique thus consists of determining a suitable configuration of the tubes: their distribution across the relevant portion of the lampshade, as well as the parameters (radius, tilt angle) of each tube. This is achieved by computing a capacity-constrained Voronoi tessellation over a suitably defined density function and embedding a tube inside the maximal inscribed circle of each tessellation cell.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"1e896c9b3badcb16de0999568c8effd0e9cd3f8f","venue_1":"ACM Trans. Graph.","year":"2006","title":"Time-varying surface appearance: acquisition, modeling and rendering","authors":"Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter N. Belhumeur, Wojciech Matusik, Shree K. Nayar","author_ids":"2931118, 2600928, 1752236, 1767767, 1752521, 1750470","abstract":"For computer graphics rendering, we generally assume that the appearance of surfaces remains static over time. Yet, there are a number of natural processes that cause surface appearance to vary dramatically, such as burning of wood, wetting and drying of rock and fabric, decay of fruit skins, and corrosion and rusting of steel and copper. In this paper, we take a significant step towards measuring, modeling, and rendering time-varying surface appearance. We describe the acquisition of the first time-varying database of 26 samples, encompassing a variety of natural processes including burning, drying, decay, and corrosion. Our main technical contribution is a Space-Time Appearance Factorization (STAF). This model factors space and time-varying effects. We derive an overall temporal appearance variation characteristic curve of the specific process, as well as space-dependent textures, rates, and offsets. This overall temporal curve controls different spatial locations evolve at the different rates, causing spatial patterns on the surface over time. We show that the model accurately represents a variety of phenomena. Moreover, it enables a number of novel rendering applications, such as transfer of the time-varying effect to a new static surface, control to accelerate time evolution in certain areas, extrapolation beyond the acquired sequence, and texture synthesis of time-varying appearance.","cites":"52","conferencePercentile":"44.44444444"},{"venue":"ACM Trans. Graph.","id":"57f58413579c1da8563f885d04512b0125e8ce1a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Post-production facial performance relighting using reflectance transfer","authors":"Pieter Peers, Naoki Tamura, Wojciech Matusik, Paul E. Debevec","author_ids":"1808270, 3131985, 1752521, 1778676","abstract":"We propose a novel post-production facial performance relighting system for human actors. Our system uses just a dataset of view-dependent facial appearances with a neutral expression, captured for a static subject using a Light Stage apparatus. For the actual performance, however, a potentially different actor is captured under known, but static, illumination. During post-production, the reflectance field of the reference dataset actor is transferred onto the dynamic performance, enabling image-based relighting of the entire sequence. Our approach makes post-production relighting more practical and could easily be incorporated in a traditional production pipeline since it does not require additional hardware during principal photography. Additionally, we show that our system is suitable for real-time post-production illumination editing.","cites":"30","conferencePercentile":"29.2"},{"venue":"ACM Trans. Graph.","id":"4efe9d0bb2da9d6159af7641d2b9c97a4969a76a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Multi-aperture photography","authors":"Paul Green, Wenyang Sun, Wojciech Matusik, Frédo Durand","author_ids":"2535850, 2084693, 1752521, 1728125","abstract":"The emergent field of computational photography is proving that, by coupling generalized imaging optics with software processing, the quality and flexibility of imaging systems can be increased. In this paper, we capture and manipulate multiple images of a scene taken with different aperture settings (<i>f</i>-numbers). We design and implement a prototype optical system and associated algorithms to capture four images of the scene in a single exposure, each taken with a different aperture setting. Our system can be used with commercially available DSLR cameras and photographic lenses without modification to either. We leverage the fact that defocus blur is a function of scene depth and <i>f</i>/# to estimate a depth map. We demonstrate several applications of our multi-aperture camera, such as post-exposure editing of the depth of field, including extrapolation beyond the physical limits of the lens, synthetic refocusing, and depth-guided deconvolution.","cites":"24","conferencePercentile":"18"},{"venue":"ACM Trans. Graph.","id":"4d311926ec2436cc02f671286ad6ea3f35b7f7a8","venue_1":"ACM Trans. Graph.","year":"2011","title":"Animated construction of line drawings","authors":"Hongbo Fu, Shizhe Zhou, Ligang Liu, Niloy J. Mitra","author_ids":"1691065, 2835857, 1724542, 1710455","abstract":"Revealing the sketching sequence of a line drawing can be visually intriguing and used for video-based storytelling. Typically this is enabled based on tedious recording of artists' drawing process. We demonstrate that it is often possible to estimate a reasonable drawing order from a static line drawing with clearly defined shape geometry, which looks plausible to a human viewer. We map the key principles of drawing order from drawing cognition to computational procedures in our framework. Our system produces plausible animated constructions of input line drawings, with no or little user intervention. We test our algorithm on a range of input sketches, with varying degree of complexity and structure, and evaluate the results via a user study. We also present applications to gesture drawing synthesis and drawing animation creation especially in the context of video scribing.","cites":"19","conferencePercentile":"42.89473684"},{"venue":"ACM Trans. Graph.","id":"b9465e1182ed6d221e0d0dfdf893c29b92a908ae","venue_1":"ACM Trans. Graph.","year":"2012","title":"Image melding: combining inconsistent images using patch-based synthesis","authors":"Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B. Goldman, Pradeep Sen","author_ids":"2600320, 2177801, 1794537, 1976171, 2258791","abstract":"Current methods for combining two different images produce visible artifacts when the sources have very different textures and structures. We present a new method for synthesizing a transition region between two source images, such that inconsistent color, texture, and structural properties all change gradually from one source to the other. We call this process <i>image melding</i>. Our method builds upon a patch-based optimization foundation with three key generalizations: First, we enrich the patch search space with additional geometric and photometric transformations. Second, we integrate image gradients into the patch representation and replace the usual color averaging with a screened Poisson equation solver. And third, we propose a new energy based on mixed <i>L</i><sub><i>2</i></sub><i>/L</i><sub><i>0</i></sub> norms for colors and gradients that produces a gradual transition between sources without sacrificing texture sharpness. Together, all three generalizations enable patch-based solutions to a broad class of image melding problems involving inconsistent sources: object cloning, stitching challenging panoramas, hole filling from multiple photos, and image harmonization. In several cases, our unified method outperforms previous state-of-the-art methods specifically designed for those applications.","cites":"69","conferencePercentile":"97.47474747"},{"venue":"ACM Trans. Graph.","id":"5c85eecf7fb8ffa9ded6590b1cea5ab655c8a7bc","venue_1":"ACM Trans. Graph.","year":"2012","title":"Lazy selection: a scribble-based tool for smart shape elements selection","authors":"Pengfei Xu, Hongbo Fu, Oscar Kin-Chung Au, Chiew-Lan Tai","author_ids":"8334787, 1691065, 1752997, 1702674","abstract":"This paper presents <i>Lazy Selection</i>, a scribble-based tool for quick selection of one or more desired shape elements by roughly stroking through the elements. Our algorithm automatically refines the selection and reveals the user's intention. To give the user maximum flexibility but least ambiguity, our technique first extracts selection candidates from the scribble-covered elements by examining the underlying patterns and then ranks them based on their location and shape with respect to the user-sketched scribble. Such a design makes our tool tolerant to imprecise input systems and applicable to touch systems without suffering from the fat finger problem. A preliminary evaluation shows that compared to the standard click and lasso selection tools, which are the most commonly used, our technique provides significant improvements in efficiency and flexibility for many selection scenarios.","cites":"5","conferencePercentile":"5.555555556"},{"venue":"ACM Trans. Graph.","id":"c802c5f223d294d58600f80f8c83517e9acb5e68","venue_1":"ACM Trans. Graph.","year":"2011","title":"Non-rigid dense correspondence with applications for image enhancement","authors":"Yoav HaCohen, Eli Shechtman, Dan B. Goldman, Dani Lischinski","author_ids":"2661824, 2177801, 1976171, 1684384","abstract":"This paper presents a new efficient method for recovering reliable local sets of dense correspondences between two images with some shared content. Our method is designed for pairs of images depicting similar regions acquired by different cameras and lenses, under non-rigid transformations, under different lighting, and over different backgrounds. We utilize a new coarse-to-fine scheme in which nearest-neighbor field computations using Generalized PatchMatch [Barnes et al. 2010] are interleaved with fitting a global non-linear parametric color model and aggregating consistent matching regions using locally adaptive constraints. Compared to previous correspondence approaches, our method combines the best of two worlds: It is dense, like optical flow and stereo reconstruction methods, and it is also robust to geometric and photometric variations, like sparse feature matching. We demonstrate the usefulness of our method using three applications for automatic example-based photograph enhancement: adjusting the tonal characteristics of a source image to match a reference, transferring a known mask to a new image, and kernel estimation for image deblurring.","cites":"97","conferencePercentile":"96.31578947"},{"venue":"ACM Trans. Graph.","id":"12e649656e833dcbd74e45b91ed0c4fd0afbfbb3","venue_1":"ACM Trans. Graph.","year":"2010","title":"Video tapestries with continuous temporal zoom","authors":"Connelly Barnes, Dan B. Goldman, Eli Shechtman, Adam Finkelstein","author_ids":"1794537, 1976171, 2177801, 1707541","abstract":"We present a novel approach for summarizing video in the form of a multiscale image that is continuous in both the spatial domain and across the scale dimension: There are no hard borders between discrete moments in time, and a user can zoom smoothly into the image to reveal additional temporal details. We call these artifacts <i>tapestries</i> because their continuous nature is akin to medieval tapestries and other narrative depictions predating the advent of motion pictures. We propose a set of criteria for such a summarization, and a series of optimizations motivated by these criteria. These can be performed as an entirely offline computation to produce high quality renderings, or by adjusting some optimization parameters the later stages can be solved in real time, enabling an interactive interface for video navigation. Our video tapestries combine the best aspects of two common visualizations, providing the visual clarity of DVD chapter menus with the information density and multiple scales of a video editing timeline representation. In addition, they provide continuous transitions between zoom levels. In a user study, participants preferred both the aesthetics and efficiency of tapestries over other interfaces for visual browsing.","cites":"42","conferencePercentile":"73.68421053"},{"venue":"ACM Trans. Graph.","id":"18ad3529e7d5841eabc4ad66ef12a50f77295766","venue_1":"ACM Trans. Graph.","year":"2009","title":"PatchMatch: a randomized correspondence algorithm for structural image editing","authors":"Connelly Barnes, Eli Shechtman, Adam Finkelstein, Dan B. Goldman","author_ids":"1794537, 2177801, 1707541, 1976171","abstract":"This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest-neighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However, the cost of computing a field of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x), enabling its use in interactive editing tools. The key insights driving the algorithm are that some good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas. We offer theoretical analysis of the convergence properties of the algorithm, as well as empirical and practical evidence for its high quality and performance. This one simple algorithm forms the basis for a variety of tools -- image retargeting, completion and reshuffling -- that can be used together in the context of a high-level image editing application. Finally, we propose additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods.","cites":"398","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"ee24322f7ee8af71b1449945a19c6e8e4e1a3293","venue_1":"ACM Trans. Graph.","year":"2015","title":"OmniAD: data-driven omni-directional aerodynamics","authors":"Tobias Martin, Nobuyuki Umetani, Bernd Bickel","author_ids":"2506525, 2065148, 3083909","abstract":"This paper introduces \"OmniAD,\" a novel data-driven pipeline to model and acquire the aerodynamics of three-dimensional rigid objects. Traditionally, aerodynamics are examined through elaborate wind tunnel experiments or expensive fluid dynamics computations, and are only measured for a small number of discrete wind directions. OmniAD allows the evaluation of aerodynamic forces, such as drag and lift, for any incoming wind direction using a novel representation based on spherical harmonics. Our data-driven technique acquires the aerodynamic properties of an object simply by capturing its falling motion using a single camera. Once model parameters are estimated, OmniAD enables realistic real-time simulation of rigid bodies, such as the tumbling and gliding of leaves, without simulating the surrounding air. In addition, we propose an intuitive user interface based on OmniAD to interactively design three-dimensional kites that actually fly. Various non-traditional kites were designed to demonstrate the physical validity of our model.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"625525e3f95a8230d17bd055b0c46134a3314a4c","venue_1":"ACM Trans. Graph.","year":"2013","title":"Gloss perception in painterly and cartoon rendering","authors":"Adrien Bousseau, James P. O'Shea, Frédo Durand, Ravi Ramamoorthi, Maneesh Agrawala","author_ids":"2149814, 2584086, 1728125, 1752236, 1820412","abstract":"Depictions with traditional media such as painting and drawing represent scene content in a stylized manner. It is unclear, however, how well stylized images depict scene properties like shape, material, and lighting. In this article, we describe the first study of material perception in stylized images (specifically painting and cartoon) and use nonphotorealistic rendering algorithms to evaluate how such stylization alters the perception of gloss. Our study reveals a compression of the range of representable gloss in stylized images so that shiny materials appear more diffuse in painterly rendering, while diffuse materials appear shinier in cartoon images. From our measurements we estimate the function that maps realistic gloss parameters to their perception in a stylized rendering. This mapping allows users of NPR algorithms to predict the perception of gloss in their images. The inverse of this function exaggerates gloss properties to make the contrast between materials in a stylized image more faithful. We have conducted our experiment both in a lab and on a crowdsourcing Web site. While crowdsourcing allows us to quickly design our pilot study, a lab experiment provides more control on how subjects perform the task. We provide a detailed comparison of the results obtained with the two approaches and discuss their advantages and drawbacks for studies like ours.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"b66200f79d67e385d3159852ed75b638c13f6605","venue_1":"ACM Trans. Graph.","year":"2012","title":"Structure recovery by part assembly","authors":"Chao-Hui Shen, Hongbo Fu, Kang Chen, Shi-Min Hu","author_ids":"2795523, 1691065, 1723702, 1686809","abstract":"This paper presents a technique that allows quick conversion of acquired low-quality data from consumer-level scanning devices to high-quality 3D models with labeled semantic parts and meanwhile their assembly reasonably close to the underlying geometry. This is achieved by a novel structure recovery approach that is essentially local to global and bottom up, enabling the creation of new structures by assembling existing labeled parts with respect to the acquired data. We demonstrate that using only a small-scale shape repository, our part assembly approach is able to faithfully recover a variety of high-level structures from only a single-view scan of man-made objects acquired by the Kinect system, containing a highly noisy, incomplete 3D point cloud and a corresponding RGB image.","cites":"39","conferencePercentile":"83.08080808"},{"venue":"ACM Trans. Graph.","id":"0ee820225936784429e371d22a1fe037eb8abc7b","venue_1":"ACM Trans. Graph.","year":"2013","title":"Sketch2Scene: sketch-based co-retrieval and co-placement of 3D models","authors":"Kun Xu, Kang Chen, Hongbo Fu, Wei-Lun Sun, Shi-Min Hu","author_ids":"1750039, 1723702, 1691065, 3295158, 1686809","abstract":"This work presents <i>Sketch2Scene</i>, a framework that automatically turns a freehand sketch drawing inferring multiple scene objects to semantically valid, well arranged scenes of 3D models. Unlike the existing works on sketch-based search and composition of 3D models, which typically process individual sketched objects one by one, our technique performs <i>co-retrieval</i> and <i>co-placement</i> of 3D relevant models by jointly processing the sketched objects. This is enabled by summarizing functional and spatial relationships among models in a large collection of 3D scenes as <i>structural groups</i>. Our technique greatly reduces the amount of user intervention needed for sketch-based modeling of 3D scenes and fits well into the traditional production pipeline involving concept design followed by 3D modeling. A pilot study indicates that it is promising to use our technique as an alternative but more efficient tool of standard 3D modeling for 3D scene construction.","cites":"32","conferencePercentile":"87.55656109"},{"venue":"ACM Trans. Graph.","id":"924462af4f90643a91327a9c466d2cfa905c6fef","venue_1":"ACM Trans. Graph.","year":"2014","title":"Data-driven segmentation and labeling of freehand sketches","authors":"Zhe Huang, Hongbo Fu, Rynson W. H. Lau","author_ids":"4169506, 1691065, 1726262","abstract":"We present a data-driven approach to derive part-level segmentation and labeling of free-hand sketches, which depict single objects with multiple parts. Our method performs segmentation and labeling simultaneously, by inferring a structure that best fits the input sketch, through selecting and connecting 3D components in the database. The problem is formulated using Mixed Integer Programming, which optimizes over both the local fitness of the selected components and the global plausibility of the connected structure. Evaluations show that our algorithm is significantly better than the straightforward approaches based on direct retrieval or part assembly, and can effectively handle challenging variations in the sketch.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"2db2af51237197cf744351e705c664db1abb2cae","venue_1":"ACM Trans. Graph.","year":"2013","title":"Depicting stylized materials with vector shade trees","authors":"Jorge Lopez-Moreno, Stefan Popov, Adrien Bousseau, Maneesh Agrawala, George Drettakis","author_ids":"1752874, 1721543, 2149814, 1820412, 1721779","abstract":"Vector graphics represent images with compact, editable and scalable primitives. Skillful vector artists employ these primitives to produce vivid depictions of material appearance and lighting. However, such stylized imagery often requires building complex multi-layered combinations of colored fills and gradient meshes. We facilitate this task by introducing <i>vector shade trees</i> that bring to vector graphics the flexibility of modular shading representations as known in the 3D rendering community. In contrast to traditional shade trees that combine pixel and vertex shaders, our shade nodes encapsulate the creation and blending of vector primitives that vector artists routinely use. We propose a set of basic shade nodes that we design to respect the traditional guidelines on material depiction described in drawing books and tutorials. We integrate our representation as an Adobe Illustrator plug-in that allows even inexperienced users to take a line drawing, apply a few clicks and obtain a fully colored illustration. More experienced artists can easily refine the illustration, adding more details and visual features, while using all the vector drawing tools they are already familiar with. We demonstrate the power of our representation by quickly generating illustrations of complex objects and materials.","cites":"4","conferencePercentile":"9.049773756"},{"venue":"ACM Trans. Graph.","id":"22cdde337a3e6411affa262b8788578416605260","venue_1":"ACM Trans. Graph.","year":"2015","title":"Simulating the Visual Experience of Very Bright and Very Dark Scenes","authors":"David E. Jacobs, Orazio Gallo, Emily A. Cooper, Kari Pulli, Marc Levoy","author_ids":"3049679, 3218156, 3288147, 1704409, 1801789","abstract":"The human visual system can operate in a wide range of illumination levels due to several adaptation processes working in concert. For the most part, these adaptation mechanisms are transparent, leaving the observer unaware of his or her absolute adaptation state. At extreme illumination levels, however, some of these mechanisms produce perceivable secondary effects, or epiphenomena. In bright light, these include bleaching afterimages and adaptation afterimages, while in dark conditions these include desaturation, loss of acuity, mesopic hue shift, and the Purkinje effect. In this work we examine whether displaying these effects explicitly can be used to extend the apparent dynamic range of a conventional computer display. We present phenomenological models for each effect, describe efficient computer graphics methods for rendering our models, and propose a gaze-adaptive display that injects the effects into imagery on a standard computer monitor. Finally, we report the results of psychophysical experiments which reveal that, while mesopic epiphenomena are a strong cue that a stimulus is very dark, afterimages have little impact on the perception that a stimulus is very bright.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"51ac22e75ec69666e2b7fd04c1a531b831475cfe","venue_1":"ACM Trans. Graph.","year":"2013","title":"Perception of perspective distortions in image-based rendering","authors":"Peter Vangorp, Christian Richardt, Emily A. Cooper, Gaurav Chaurasia, Martin S. Banks, George Drettakis","author_ids":"2533751, 7240032, 3288147, 2585067, 2971790, 1721779","abstract":"Image-based rendering (IBR) creates realistic images by enriching simple geometries with photographs, e.g., mapping the photograph of a building fa&#231;ade onto a plane. However, as soon as the viewer moves away from the correct viewpoint, the image in the retina becomes distorted, sometimes leading to gross misperceptions of the original geometry. Two hypotheses from vision science state how viewers perceive such image distortions, one claiming that they can compensate for them (and therefore perceive scene geometry reasonably correctly), and one claiming that they cannot compensate (and therefore can perceive rather significant distortions). We modified the latter hypothesis so that it extends to street-level IBR. We then conducted a rigorous experiment that measured the magnitude of perceptual distortions that occur with IBR for fa&#231;ade viewing. We also conducted a rating experiment that assessed the acceptability of the distortions. The results of the two experiments were consistent with one another. They showed that viewers' percepts are indeed distorted, but not as severely as predicted by the modified vision science hypothesis. From our experimental results, we develop a predictive model of distortion for street-level IBR, which we use to provide guidelines for acceptability of virtual views and for capture camera density. We perform a confirmatory study to validate our predictions, and illustrate their use with an application that guides users in IBR navigation to stay in regions where virtual views yield acceptable perceptual distortions.","cites":"5","conferencePercentile":"12.66968326"},{"venue":"ACM Trans. Graph.","id":"59cc21c98bf971ff2b4f985dd0326cbef9a6eaf9","venue_1":"ACM Trans. Graph.","year":"2010","title":"Using blur to affect perceived distance and size","authors":"Robert T. Held, Emily A. Cooper, James F. O'Brien, Martin S. Banks","author_ids":"2549479, 3288147, 1692280, 2971790","abstract":"We present a probabilistic model of how viewers may use defocus blur in conjunction with other pictorial cues to estimate the absolute distances to objects in a scene. Our model explains how the pattern of blur in an image together with relative depth cues indicates the apparent scale of the image's contents. From the model, we develop a semiautomated algorithm that applies blur to a sharply rendered image and thereby changes the apparent distance and scale of the scene's contents. To examine the correspondence between the model/algorithm and actual viewer experience, we conducted an experiment with human viewers and compared their estimates of absolute distance to the model's predictions. We did this for images with geometrically correct blur due to defocus and for images with commonly used approximations to the correct blur. The agreement between the experimental data and model predictions was excellent. The model predicts that some approximations should work well and that others should not. Human viewers responded to the various types of blur in much the way the model predicts. The model and algorithm allow one to manipulate blur precisely and to achieve the desired perceived scale efficiently.","cites":"44","conferencePercentile":"77.77777778"},{"venue":"ACM Trans. Graph.","id":"2d2aed1f2079264c8329dedde139ffd00b963bb3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Consolidation of unorganized point clouds for surface reconstruction","authors":"Hui Huang, Dan Li, Hao Zhang, Uri M. Ascher, Daniel Cohen-Or","author_ids":"1927737, 1716114, 1682058, 1966230, 1701009","abstract":"We consolidate an unorganized point cloud with noise, outliers, non-uniformities, and in particular interference between close-by surface sheets as a preprocess to surface generation, focusing on reliable normal estimation. Our algorithm includes two new developments. First, a <i>weighted locally optimal projection</i> operator produces a set of denoised, outlier-free and evenly distributed particles over the original dense point cloud, so as to improve the reliability of local PCA for initial estimate of normals. Next, an iterative framework for robust normal estimation is introduced, where a priority-driven normal propagation scheme based on a new priority measure and an orientation-aware PCA work complementarily and iteratively to consolidate particle normals. The priority setting is reinforced with front stopping at thin surface features and normal flipping to enable robust handling of the close-by surface sheet problem. We demonstrate how a point cloud that is well-consolidated by our method steers conventional surface generation schemes towards a proper interpretation of the input data.","cites":"62","conferencePercentile":"81.21546961"},{"venue":"ACM Trans. Graph.","id":"580f4c8a389532bb12590e4e715ea2a0003d4eaf","venue_1":"ACM Trans. Graph.","year":"2009","title":"Feature-aligned shape texturing","authors":"Kai Xu, Daniel Cohen-Or, Tao Ju, Ligang Liu, Hao Zhang, Shizhe Zhou, Yueshan Xiong","author_ids":"1723225, 1701009, 1787371, 1724542, 1682058, 2835857, 2992810","abstract":"The essence of a 3D shape can often be well captured by its salient feature curves. In this paper, we explore the use of salient curves in synthesizing intuitive, shape-revealing textures on surfaces. Our texture synthesis is guided by two principles: matching the direction of the texture patterns to those of the salient curves, and aligning the prominent feature lines in the texture to the salient curves exactly. We have observed that textures synthesized by these principles not only fit naturally to the surface geometry, but also visually reveal, even reinforce, the shape's essential characteristics. We call these <i>feature-aligned shape texturing</i>. Our technique is fully automatic, and introduces two novel technical components in vector-field-guided texture synthesis: an algorithm that orients the salient curves on a surface for constrained vector field generation, and a feature-to-feature texture optimization.","cites":"22","conferencePercentile":"31.7679558"},{"venue":"ACM Trans. Graph.","id":"ca00c5c77b99bfe50ce581528b5b3da06b0219f7","venue_1":"ACM Trans. Graph.","year":"2011","title":"Adaptive partitioning of urban facades","authors":"Chao-Hui Shen, Shi-Sheng Huang, Hongbo Fu, Shi-Min Hu","author_ids":"2795523, 2550389, 1691065, 1686809","abstract":"Automatically discovering high-level facade structures in unorganized 3D point clouds of urban scenes is crucial for applications like digitalization of real cities. However, this problem is challenging due to poor-quality input data, contaminated with severe missing areas, noise and outliers. This work introduces the concept of <i>adaptive partitioning</i> to automatically derive a flexible and hierarchical representation of 3D urban facades. Our key observation is that urban facades are largely governed by concatenated and/or interlaced grids. Hence, unlike previous automatic facade analysis works which are typically restricted to globally rectilinear grids, we propose to automatically partition the facade in an adaptive manner, in which the splitting direction, the number and location of splitting planes are all adaptively determined. Such an adaptive partition operation is performed recursively to generate a hierarchical representation of the facade. We show that the concept of adaptive partitioning is also applicable to flexible and robust analysis of image facades. We evaluate our method on a dozen of LiDAR scans of various complexity and styles, and the image facades from the eTRIMS database and the Ecole Centrale Paris database. A series of applications that benefit from our approach are also demonstrated.","cites":"26","conferencePercentile":"57.36842105"},{"venue":"ACM Trans. Graph.","id":"2bb0b4bfadf1cf7931f02a94f349e94175860dbd","venue_1":"ACM Trans. Graph.","year":"2009","title":"Layered shape synthesis: automatic generation of control maps for non-stationary textures","authors":"Amir Rosenberger, Daniel Cohen-Or, Dani Lischinski","author_ids":"2664998, 1701009, 1684384","abstract":"Many inhomogeneous real-world textures are non-stationary and exhibit various large scale patterns that are easily perceived by a human observer. Such textures violate the assumptions underlying most state-of-the-art example-based synthesis methods. Consequently, they cannot be properly reproduced by these methods, unless a suitable control map is provided to guide the synthesis process. Such control maps are typically either user specified or generated by a simulation. In this paper, we present an alternative: a method for automatic example-based generation of control maps, geared at synthesis of natural, highly inhomogeneous textures, such as those resulting from natural aging or weathering processes. Our method is based on the observation that an appropriate control map for many of these textures may be modeled as a superposition of several layers, where the visible parts of each layer are occupied by a more homogeneous texture. Thus, given a decomposition of a texture exemplar into a small number of such layers, we employ a novel example-based shape synthesis algorithm to automatically generate a new set of layers. Our shape synthesis algorithm is designed to preserve both local and global characteristics of the exemplar's layer map. This process results in a new control map, which then may be used to guide the subsequent texture synthesis process.","cites":"18","conferencePercentile":"23.48066298"},{"venue":"ACM Trans. Graph.","id":"7d2852c854e590e69c95403c1e174dc4f93d8a6f","venue_1":"ACM Trans. Graph.","year":"2010","title":"Camouflage images","authors":"Hung-Kuo Chu, Wei-Hsin Hsu, Niloy J. Mitra, Daniel Cohen-Or, Tien-Tsin Wong, Tong-Yee Lee","author_ids":"1771281, 3321837, 1710455, 1701009, 1720633, 1724980","abstract":"Camouflage images contain one or more hidden figures that remain imperceptible or unnoticed for a while. In one possible explanation, the ability to delay the perception of the hidden figures is attributed to the theory that human perception works in two main phases: feature search and conjunction search. Effective camouflage images make feature based recognition difficult, and thus force the recognition process to employ conjunction search, which takes considerable effort and time. In this paper, we present a technique for creating camouflage images. To foil the feature search, we remove the original subtle texture details of the hidden figures and replace them by that of the surrounding apparent image. To leave an appropriate degree of clues for the conjunction search, we compute and assign new tones to regions in the embedded figures by performing an optimization between two conflicting terms, which we call <i>immersion</i> and <i>standout</i>, corresponding to hiding and leaving clues, respectively. We show a large number of camouflage images generated by our technique, with or without user guidance. We have tested the quality of the images in an extensive user study, showing a good control of the difficulty levels.","cites":"9","conferencePercentile":"9.649122807"},{"venue":"ACM Trans. Graph.","id":"42b4b44e2127e4ab8be3ca44b859779c8ca31fdd","venue_1":"ACM Trans. Graph.","year":"2010","title":"Parametric reshaping of human bodies in images","authors":"Shizhe Zhou, Hongbo Fu, Ligang Liu, Daniel Cohen-Or, Xiaoguang Han","author_ids":"2835857, 1691065, 1724542, 1701009, 1763245","abstract":"We present an easy-to-use image retouching technique for realistic reshaping of human bodies in a single image. A <i>model-based</i> approach is taken by integrating a 3D whole-body morphable model into the reshaping process to achieve globally consistent editing effects. A novel <i>body-aware image warping</i> approach is introduced to reliably transfer the reshaping effects from the model to the image, even under moderate fitting errors. Thanks to the parametric nature of the model, our technique parameterizes the degree of reshaping by a small set of semantic attributes, such as weight and height. It allows easy creation of desired reshaping effects by changing the full-body attributes, while producing visually pleasing results even for loosely-dressed humans in casual photographs with a variety of poses and shapes.","cites":"51","conferencePercentile":"85.96491228"},{"venue":"ACM Trans. Graph.","id":"40c68937c98ccf35e3895dde660124627c4e8aeb","venue_1":"ACM Trans. Graph.","year":"2004","title":"Islamic star patterns in absolute geometry","authors":"Craig S. Kaplan, David Salesin","author_ids":"2345263, 1745260","abstract":"We present <i>Najm</i>, a set of tools built on the axioms of absolute geometry for exploring the design space of Islamic star patterns. Our approach makes use of a novel family of tilings, called \"inflation tilings,\" which are particularly well suited as guides for creating star patterns. We describe a method for creating a parameterized set of motifs that can be used to fill the many regular polygons that comprise these tilings, as well as an algorithm to infer geometry for any irregular polygons that remain. Erasing the underlying tiling and joining together the inferred motifs produces the star patterns. By choice, <i>Najm</i> is build upon the subset of geometry that makes no assumption about the behavior of parallel lines. As a consequence, star patterns created by <i>Najm</i> can be designed equally well to fit the Euclidean plane, the hyperbolic plane, or the surface of a sphere.","cites":"25","conferencePercentile":"10.86956522"},{"venue":"ACM Trans. Graph.","id":"4df1380c925687a4dd2e328badee8ef19b54f655","venue_1":"ACM Trans. Graph.","year":"2003","title":"Shadow matting and compositing","authors":"Yung-Yu Chuang, Dan B. Goldman, Brian Curless, David Salesin, Richard Szeliski","author_ids":"3032320, 1976171, 1810052, 1745260, 1717841","abstract":"In this paper, we describe a method for extracting shadows from one natural scene and inserting them into another. We develop physically-based shadow matting and compositing equations and use these to pull a <i>shadow matte</i> from a source scene in which the shadow is cast onto an arbitrary planar background. We then acquire the photometric and geometric properties of the target scene by sweeping oriented linear shadows (cast by a straight object) across it. From these shadow scans, we can construct a shadow displacement map without requiring camera or light source calibration. This map can then be used to deform the original shadow matte. We demonstrate our approach for both indoor scenes with controlled lighting and for outdoor scenes using natural lighting.","cites":"40","conferencePercentile":"21.50537634"},{"venue":"ACM Trans. Graph.","id":"86aae06409f84028697f6aede2e28394d03c8723","venue_1":"ACM Trans. Graph.","year":"2008","title":"Gesture modeling and animation based on a probabilistic re-creation of speaker style","authors":"Michael Neff, Michael Kipp, Irene Albrecht, Hans-Peter Seidel","author_ids":"3623112, 1711113, 1737620, 1746884","abstract":"Animated characters that move and gesticulate appropriately with spoken text are useful in a wide range of applications. Unfortunately, this class of movement is very difficult to generate, even more so when a unique, individual movement style is required. We present a system that, with a focus on arm gestures, is capable of producing full-body gesture animation for given input text in the style of a particular performer. Our process starts with video of a person whose gesturing style we wish to animate. A tool-assisted annotation process is performed on the video, from which a statistical model of the person's particular gesturing style is built. Using this model and input text tagged with theme, rheme and focus, our generation algorithm creates a gesture script. As opposed to isolated singleton gestures, our gesture script specifies a stream of continuous gestures coordinated with speech. This script is passed to an animation system, which enhances the gesture description with additional detail. It then generates either kinematic or physically simulated motion based on this description. The system is capable of generating gesture animations for novel text that are consistent with a given performer's style, as was successfully validated in an empirical user study.","cites":"94","conferencePercentile":"88.27160494"},{"venue":"ACM Trans. Graph.","id":"b837fc64442b072a0ad1426e033cf40277eecfc4","venue_1":"ACM Trans. Graph.","year":"2010","title":"The Frankencamera: an experimental platform for computational photography","authors":"Andrew Adams, David E. Jacobs, Jennifer Dolson, Marius Tico, Kari Pulli, Eino-Ville Talvala, Boris Ajdin, Daniel A. Vaquero, Hendrik P. A. Lensch, Mark Horowitz, Sung Hee Park, Natasha Gelfand, Jongmin Baek, Wojciech Matusik, Marc Levoy","author_ids":"4030789, 3049679, 2889257, 1717236, 1704409, 3129635, 2027612, 2000950, 1809190, 1764167, 2816492, 1683095, 2155413, 1752521, 1801789","abstract":"Although there has been much interest in computational photography within the research and photography communities, progress has been hampered by the lack of a portable, programmable camera with sufficient image quality and computing power. To address this problem, we have designed and implemented an open architecture and API for such cameras: the Frankencamera. It consists of a base hardware specification, a software stack based on Linux, and an API for C++. Our architecture permits control and synchronization of the sensor and image processing pipeline at the microsecond time scale, as well as the ability to incorporate and synchronize external hardware like lenses and flashes. This paper specifies our architecture and API, and it describes two reference implementations we have built. Using these implementations we demonstrate six computational photography applications: HDR viewfinding and capture, low-light viewfinding and capture, automated acquisition of extended dynamic range panoramas, foveal imaging, IMU-based hand shake detection, and rephotography. Our goal is to standardize the architecture and distribute Frankencameras to researchers and students, as a step towards creating a community of photographer-programmers who develop algorithms, applications, and hardware for computational cameras.","cites":"78","conferencePercentile":"95.32163743"},{"venue":"ACM Trans. Graph.","id":"65095e4580b0a75605ebf9574f22ce7115cd6f67","venue_1":"ACM Trans. Graph.","year":"2010","title":"Computational highlight holography","authors":"Christian Regg, Szymon Rusinkiewicz, Wojciech Matusik, Markus H. Gross","author_ids":"3039289, 7723706, 1752521, 1743207","abstract":"Computational highlight holography converts three-dimensional computer models into mechanical \"holograms\" fabricated on (specular) reflective or refractive materials. The surface consists of small grooves with patches of paraboloids or hyperboloids, each of which produces a highlight when illuminated by a directional light. Each highlight appears in different places for different view directions, with the correct binocular and motion parallax corresponding to a virtual 3D point position. Our computational pipeline begins with a 3D model and desired view position, samples the model to generate points that depict its features accurately, and computes a maximal set of non-overlapping patches to be embedded in the surface. We provide a preview of the hologram for the user, then fabricate the surface using a computer-controlled engraving machine. We show a variety of different fabricated holograms: reflective, transmissive, and holograms with color and proper shading. We also present extensions to stationary and animated 2D stippled images.","cites":"5","conferencePercentile":"4.678362573"},{"venue":"ACM Trans. Graph.","id":"12fcaec8cca81728f70e5be9c997b3a0f7d7b1cd","venue_1":"ACM Trans. Graph.","year":"2010","title":"Non-local scan consolidation for 3D urban scenes","authors":"Qian Zheng, Andrei Sharf, Guowei Wan, Yangyan Li, Niloy J. Mitra, Daniel Cohen-Or, Baoquan Chen","author_ids":"2780662, 2120270, 1932247, 1920864, 1710455, 1701009, 1748939","abstract":"Recent advances in scanning technologies, in particular devices that extract depth through active sensing, allow fast scanning of urban scenes. Such rapid acquisition incurs imperfections: large regions remain missing, significant variation in sampling density is common, and the data is often corrupted with noise and outliers. However, buildings often exhibit large scale repetitions and self-similarities. Detecting, extracting, and utilizing such large scale repetitions provide powerful means to consolidate the imperfect data. Our key observation is that the same geometry, when scanned multiple times over reoccurrences of instances, allow application of a simple yet effective non-local filtering. The multiplicity of the geometry is fused together and projected to a <i>base-geometry</i> defined by clustering corresponding surfaces. Denoising is applied by separating the process into off-plane and in-plane phases. We show that the consolidation of the reoccurrences provides robust denoising and allow reliable completion of missing parts. We present evaluation results of the algorithm on several LiDAR scans of buildings of varying complexity and styles.","cites":"49","conferencePercentile":"84.21052632"},{"venue":"ACM Trans. Graph.","id":"215c1e2484e2ceceb5343a8950b18053a299345e","venue_1":"ACM Trans. Graph.","year":"2010","title":"l1-Sparse reconstruction of sharp point set surfaces","authors":"Haim Avron, Andrei Sharf, Chen Greif, Daniel Cohen-Or","author_ids":"1922753, 2120270, 1805855, 1701009","abstract":"We introduce an &ell;<sub>1</sub>-sparse method for the reconstruction of a piecewise smooth point set surface. The technique is motivated by recent advancements in sparse signal reconstruction. The assumption underlying our work is that common objects, even geometrically complex ones, can typically be characterized by a rather small number of features. This, in turn, naturally lends itself to incorporating the powerful notion of sparsity into the model. The sparse reconstruction principle gives rise to a reconstructed point set surface that consists mainly of smooth modes, with the residual of the objective function strongly concentrated near sharp features. Our technique is capable of recovering orientation and positions of highly noisy point sets. The global nature of the optimization yields a sparse solution and avoids local minima. Using an interior-point log-barrier solver with a customized preconditioning scheme, the solver for the corresponding convex optimization problem is competitive and the results are of high quality.","cites":"28","conferencePercentile":"48.83040936"},{"venue":"ACM Trans. Graph.","id":"343050e42a7909da7b04c22d7125552ad84877a3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Edge-based image coarsening","authors":"Raanan Fattal, Robert Carroll, Maneesh Agrawala","author_ids":"3230440, 3149497, 1820412","abstract":"This article presents a new dimensionally-reduced linear image space that allows a number of recent image manipulation techniques to be performed efficiently and robustly. The basis vectors spanning this space are constructed from a scale-adaptive image decomposition, based on kernels of the bilateral filter. Each of these vectors locally binds together pixels in smooth regions and leaves pixels across edges independent. Despite the drastic reduction in the number of degrees of freedom, this representation can be used to perform a number of recent gradient-based tonemapping techniques. In addition to reducing computation time, this space can prevent the bleeding artifacts which are common to Poisson-based integration methods. In addition, we show that this reduced representation is useful for energy-minimization methods in achieving efficient processing and providing better matrix conditioning at a minimal quality sacrifice.","cites":"7","conferencePercentile":"9.116022099"},{"venue":"ACM Trans. Graph.","id":"60ca20396cae916e0737bfbde74af6cbcced2dca","venue_1":"ACM Trans. Graph.","year":"2014","title":"Sensitivity-optimized rigging for example-based real-time clothing synthesis","authors":"Weiwei Xu, Nobuyuki Umetani, Qianwen Chao, Jie Mao, Xiaogang Jin, Xin Tong","author_ids":"6953977, 2065148, 3219746, 2124791, 1735879, 1743927","abstract":"We present a real-time solution for generating detailed clothing deformations from pre-computed clothing shape examples. Given an input pose, it synthesizes a clothing deformation by blending skinned clothing deformations of nearby examples controlled by the body skeleton. Observing that cloth deformation can be well modeled with sensitivity analysis driven by the underlying skeleton, we introduce a sensitivity based method to construct a pose-dependent rigging solution from sparse examples. We also develop a sensitivity based blending scheme to find nearby examples for the input pose and evaluate their contributions to the result. Finally, we propose a stochastic optimization based greedy scheme for sampling the pose space and generating example clothing shapes. Our solution is fast, compact and can generate realistic clothing animation results for various kinds of clothes in real time.","cites":"3","conferencePercentile":"13.78600823"},{"venue":"ACM Trans. Graph.","id":"c3d6235ef98c11f4669cfaf37b3b0eef9ec22536","venue_1":"ACM Trans. Graph.","year":"2010","title":"Image warps for artistic perspective manipulation","authors":"Robert Carroll, Aseem Agarwala, Maneesh Agrawala","author_ids":"3149497, 1696487, 1820412","abstract":"Painters and illustrators commonly sketch vanishing points and lines to guide the construction of perspective images. We present a tool that gives users the ability to manipulate perspective in photographs using image space controls similar to those used by artists. Our approach computes a 2D warp guided by constraints based on projective geometry. A user annotates an image by marking a number of image space constraints including planar regions of the scene, straight lines, and associated vanishing points. The user can then use the lines, vanishing points, and other point constraints as handles to control the warp. Our system optimizes the warp such that straight lines remain straight, planar regions transform according to a homography, and the entire mapping is as shape-preserving as possible. While the result of this warp is not necessarily an accurate perspective projection of the scene, it is often visually plausible. We demonstrate how this approach can be used to produce a variety of effects, such as changing the perspective composition of a scene, exploring artistic perspectives not realizable with a camera, and matching perspectives of objects from different images so that they appear consistent for compositing.","cites":"25","conferencePercentile":"43.85964912"},{"venue":"ACM Trans. Graph.","id":"a2ca4387e6c1ded596e0b67ba17eb2f089d7f252","venue_1":"ACM Trans. Graph.","year":"2010","title":"Reliefs as images","authors":"Marc Alexa, Wojciech Matusik","author_ids":"1751554, 1752521","abstract":"We describe how to create relief surfaces whose diffuse reflection approximates given images under known directional illumination. This allows using any surface with a significant diffuse reflection component as an image display. We propose a discrete model for the area in the relief surface that corresponds to a pixel in the desired image. This model introduces the necessary degrees of freedom to overcome theoretical limitations in shape from shading and practical requirements such as stability of the image under changes in viewing condition and limited overall variation in depth. The discrete surface is determined using an iterative least squares optimization. We show several resulting relief surfaces conveying one image for varying lighting directions as well as two images for two specific lighting directions.","cites":"42","conferencePercentile":"73.68421053"},{"venue":"ACM Trans. Graph.","id":"b112c21bb526f0c43be688fec60dd2186de0957d","venue_1":"ACM Trans. Graph.","year":"2011","title":"Illumination decomposition for material recoloring with consistent interreflections","authors":"Robert Carroll, Ravi Ramamoorthi, Maneesh Agrawala","author_ids":"3149497, 1752236, 1820412","abstract":"Changing the color of an object is a basic image editing operation, but a high quality result must also preserve natural shading. A common approach is to first compute reflectance and illumination intrinsic images. Reflectances can then be edited independently, and recomposed with the illumination. However, manipulating only the reflectance color does not account for diffuse interreflections, and can result in inconsistent shading in the edited image. We propose an approach for further decomposing illumination into direct lighting, and indirect diffuse illumination from each material. This decomposition allows us to change indirect illumination from an individual material independently, so it matches the modified reflectance color. To address the underconstrained problem of decomposing illumination into multiple components, we take advantage of its smooth nature, as well as user-provided constraints. We demonstrate our approach on a number of examples, where we consistently edit material colors and the associated interreflections.","cites":"16","conferencePercentile":"37.63157895"},{"venue":"ACM Trans. Graph.","id":"aa9e8f6112d2a57306a1e15e355e60f9751b4d52","venue_1":"ACM Trans. Graph.","year":"2011","title":"Interactive furniture layout using interior design guidelines","authors":"Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, Vladlen Koltun","author_ids":"7778343, 1945094, 3021501, 1820412, 1770944","abstract":"We present an interactive furniture layout system that assists users by suggesting furniture arrangements that are based on interior design guidelines. Our system incorporates the layout guidelines as terms in a density function and generates layout suggestions by rapidly sampling the density function using a hardware-accelerated Monte Carlo sampler. Our results demonstrate that the suggestion generation functionality measurably increases the quality of furniture arrangements produced by participants with no prior training in interior design.","cites":"75","conferencePercentile":"94.21052632"},{"venue":"ACM Trans. Graph.","id":"33849580955cbf73fa174bc82cb18f6d362f11f3","venue_1":"ACM Trans. Graph.","year":"2012","title":"Selectively de-animating video","authors":"Jiamin Bai, Aseem Agarwala, Maneesh Agrawala, Ravi Ramamoorthi","author_ids":"2420518, 1696487, 1820412, 1752236","abstract":"We present a semi-automated technique for selectively <i>deanimating</i> video to remove the large-scale motions of one or more objects so that other motions are easier to see. The user draws strokes to indicate the regions of the video that should be immobilized, and our algorithm warps the video to remove the large-scale motion of these regions while leaving finer-scale, relative motions intact. However, such warps may introduce unnatural motions in previously motionless areas, such as background regions. We therefore use a graph-cut-based optimization to composite the warped video regions with still frames from the input video; we also optionally loop the output in a seamless manner. Our technique enables a number of applications such as clearer motion visualization, simpler creation of artistic <i>cinemagraphs</i> (photos that include looping motions in some regions), and new ways to edit appearance and complicated motion paths in video by manipulating a de-animated representation. We demonstrate the success of our technique with a number of motion visualizations, cinemagraphs and video editing examples created from a variety of short input videos, as well as visual and numerical comparison to previous techniques.","cites":"13","conferencePercentile":"35.1010101"},{"venue":"ACM Trans. Graph.","id":"380ac77c76aebd064ef8dd351492dcb727bc8aba","venue_1":"ACM Trans. Graph.","year":"2013","title":"Optimizing color consistency in photo collections","authors":"Yoav HaCohen, Eli Shechtman, Dan B. Goldman, Dani Lischinski","author_ids":"2661824, 2177801, 1976171, 1684384","abstract":"With dozens or even hundreds of photos in today's digital photo albums, editing an entire album can be a daunting task. Existing automatic tools operate on individual photos without ensuring consistency of appearance between photographs that share content. In this paper, we present a new method for consistent editing of photo collections. Our method automatically enforces consistent appearance of images that share content without any user input. When the user does make changes to selected images, these changes automatically propagate to other images in the collection, while still maintaining as much consistency as possible. This makes it possible to interactively adjust an entire photo album in a consistent manner by manipulating only a few images.\n Our method operates by efficiently constructing a graph with edges linking photo pairs that share content. Consistent appearance of connected photos is achieved by globally optimizing a quadratic cost function over the entire graph, treating user-specified edits as constraints in the optimization. The optimization is fast enough to provide interactive visual feedback to the user. We demonstrate the usefulness of our approach using a number of personal and professional photo collections, as well as internet collections.","cites":"17","conferencePercentile":"67.19457014"},{"venue":"ACM Trans. Graph.","id":"b23a1397ad7518c0a27905bf1449afb8a609fdce","venue_1":"ACM Trans. Graph.","year":"2012","title":"Robust patch-based hdr reconstruction of dynamic scenes","authors":"Pradeep Sen, Nima Khademi Kalantari, Maziar Yaesoubi, Soheil Darabi, Dan B. Goldman, Eli Shechtman","author_ids":"2258791, 1717070, 3264806, 2600320, 1976171, 2177801","abstract":"High dynamic range (HDR) imaging from a set of sequential exposures is an easy way to capture high-quality images of static scenes, but suffers from artifacts for scenes with significant motion. In this paper, we propose a new approach to HDR reconstruction that draws information from all the exposures but is more robust to camera/scene motion than previous techniques. Our algorithm is based on a novel patch-based energy-minimization formulation that integrates alignment and reconstruction in a joint optimization through an equation we call the HDR image synthesis equation. This allows us to produce an HDR result that is aligned to one of the exposures yet contains information from all of them. We present results that show considerable improvement over previous approaches.","cites":"35","conferencePercentile":"79.7979798"},{"venue":"ACM Trans. Graph.","id":"b2195a35ae4b107ef5af184fee2a57755630e7a8","venue_1":"ACM Trans. Graph.","year":"2003","title":"Adaptive grid-based document layout","authors":"Charles E. Jacobs, Wilmot Li, Evan Schrier, David Bargeron, David Salesin","author_ids":"3202932, 2812691, 3101123, 1787634, 1745260","abstract":"Grid-based page designs are ubiquitous in commercially printed publications, such as newspapers and magazines. Yet, to date, no one has invented a good way to easily and automatically adapt such designs to arbitrarily-sized electronic displays. The difficulty of generalizing grid-based designs explains the generally inferior nature of on-screen layouts when compared to their printed counterparts, and is arguably one of the greatest remaining impediments to creating on-line reading experiences that rival those of ink on paper. In this work, we present a new approach to adaptive grid-based document layout, which attempts to bridge this gap. In our approach, an adaptive layout style is encoded as a set of grid-based templates that know how to adapt to a range of page sizes and other viewing conditions. These templates include various types of layout elements (such as text, figures, etc.) and define, through constraint-based relationships, just how these elements are to be laid out together as a function of both the properties of the content itself, such as a figure's size and aspect ratio, and the properties of the viewing conditions under which the content is being displayed. We describe an XML-based representation for our templates and content, which maintains a clean separation between the two. We also describe the various parts of our research prototype system: a layout engine for formatting the page; a paginator for determining a globally optimal allocation of content amongst the pages, as well as an optimal pairing of templates with content; and a graphical user interface for interactively creating adaptive templates. We also provide numerous examples demonstrating the capabilities of this prototype, including this paper, itself, which has been laid out with our system.","cites":"64","conferencePercentile":"32.25806452"},{"venue":"ACM Trans. Graph.","id":"5ca5c6eb4d9cb96c9187ced073cd177f7005fbc0","venue_1":"ACM Trans. Graph.","year":"2009","title":"Motion-aware temporal coherence for video resizing","authors":"Yu-Shuen Wang, Hongbo Fu, Olga Sorkine-Hornung, Tong-Yee Lee, Hans-Peter Seidel","author_ids":"3133234, 1691065, 2250001, 1724980, 1746884","abstract":"Temporal coherence is crucial in content-aware video retargeting. To date, this problem has been addressed by constraining temporally adjacent pixels to be transformed coherently. However, due to the <i>motion-oblivious</i> nature of this simple constraint, the retargeted videos often exhibit flickering or waving artifacts, especially when significant camera or object motions are involved. Since the feature correspondence across frames varies spatially with both camera and object motion, <i>motion-aware</i> treatment of features is required for video resizing. This motivated us to align consecutive frames by estimating interframe camera motion and to constrain relative positions in the aligned frames. To preserve object motion, we detect distinct moving areas of objects across multiple frames and constrain each of them to be resized consistently. We build a complete video resizing framework by incorporating our motion-aware constraints with an adaptation of the scale-and-stretch optimization recently proposed by Wang and colleagues. Our streaming implementation of the framework allows efficient resizing of long video sequences with low memory cost. Experiments demonstrate that our method produces spatiotemporally coherent retargeting results even for challenging examples with complex camera and object motion, which are difficult to handle with previous techniques.","cites":"49","conferencePercentile":"70.99447514"},{"venue":"ACM Trans. Graph.","id":"62f8e7542cf884c265fd76adcf7bc6859047875c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Video puppetry: a performative interface for cutout animation","authors":"Connelly Barnes, David E. Jacobs, Jason Sanders, Dan B. Goldman, Szymon Rusinkiewicz, Adam Finkelstein, Maneesh Agrawala","author_ids":"1794537, 3049679, 8303989, 1976171, 7723706, 1707541, 1820412","abstract":"We present a video-based interface that allows users of all skill levels to quickly create cutout-style animations by performing the character motions. The puppeteer first creates a cast of physical puppets using paper, markers and scissors. He then physically moves these puppets to tell a story. Using an inexpensive overhead camera our system tracks the motions of the puppets and renders them on a new background while removing the puppeteer's hands. Our system runs in real-time (at 30 fps) so that the puppeteer and the audience can immediately see the animation that is created. Our system also supports a variety of constraints and effects including articulated characters, multi-track animation, scene changes, camera controls, 2 1/2-D environments, shadows, and animation cycles. Users have evaluated our system both quantitatively and qualitatively: In tests of low-level dexterity, our system has similar accuracy to a mouse interface. For simple story telling, users prefer our system over either a mouse interface or traditional puppetry. We demonstrate that even first-time users, including an eleven-year-old, can use our system to quickly turn an original story idea into an animation.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"5d5e933725ae8567900601f637d5e3a89350a738","venue_1":"ACM Trans. Graph.","year":"2011","title":"Multi-perspective stereoscopy from light fields","authors":"Changil Kim, Alexander Sorkine-Hornung, Simon Heinzle, Wojciech Matusik, Markus H. Gross","author_ids":"2692728, 2893744, 1782613, 1752521, 1743207","abstract":"This paper addresses stereoscopic view generation from a light field. We present a framework that allows for the generation of stereoscopic image pairs with per-pixel control over disparity, based on multi-perspective imaging from light fields. The proposed framework is novel and useful for stereoscopic image processing and post-production. The stereoscopic images are computed as piecewise continuous cuts through a light field, minimizing an energy reflecting prescribed parameters such as depth budget, maximum disparity gradient, desired stereoscopic baseline, and so on. As demonstrated in our results, this technique can be used for efficient and flexible stereoscopic post-processing, such as reducing excessive disparity while preserving perceived depth, or retargeting of already captured scenes to various view settings. Moreover, we generalize our method to multiple cuts, which is highly useful for content creation in the context of multi-view autostereoscopic displays. We present several results on computer-generated content as well as live-action content.","cites":"18","conferencePercentile":"41.31578947"},{"venue":"ACM Trans. Graph.","id":"27158a836c1be2ed549bdee8412fbc7b56318e6b","venue_1":"ACM Trans. Graph.","year":"2009","title":"Emerging images","authors":"Niloy J. Mitra, Hung-Kuo Chu, Tong-Yee Lee, Lior Wolf, Yehezkel Yeshurun, Daniel Cohen-Or","author_ids":"1710455, 1771281, 1724980, 1776343, 2049815, 1701009","abstract":"Emergence refers to the unique human ability to aggregate information from seemingly meaningless pieces, and to perceive a whole that is meaningful. This special skill of humans can constitute an effective scheme to tell humans and machines apart. This paper presents a synthesis technique to generate images of 3D objects that are detectable by humans, but difficult for an automatic algorithm to recognize. The technique allows generating an infinite number of images with emerging figures. Our algorithm is designed so that locally the synthesized images divulge little useful information or cues to assist any segmentation or recognition procedure. Therefore, as we demonstrate, computer vision algorithms are incapable of effectively processing such images. However, when a human observer is presented with an emergence image, synthesized using an object she is familiar with, the figure emerges when observed as a whole. We can control the difficulty level of perceiving the emergence effect through a limited set of parameters. A procedure that synthesizes emergence images can be an effective tool for exploring and understanding the factors affecting computer vision techniques.","cites":"8","conferencePercentile":"11.32596685"},{"venue":"ACM Trans. Graph.","id":"0faa62a76eedb2a981fd5bddf3d1a87d809ddc92","venue_1":"ACM Trans. Graph.","year":"2008","title":"Reusable skinning templates using cage-based deformations","authors":"Tao Ju, Qian-Yi Zhou, Michiel van de Panne, Daniel Cohen-Or, Ulrich Neumann","author_ids":"1787371, 7451623, 1745029, 1701009, 1745263","abstract":"Character skinning determines how the shape of the surface geometry changes as a function of the pose of the underlying skeleton. In this paper we describe skinning templates, which define common deformation behaviors for common joint types. This abstraction allows skinning solutions to be shared and reused, and they allow a user to quickly explore many possible alternatives for the skinning behavior of a character. The skinning templates are implemented using cage-based deformations, which offer a flexible design space within which to develop reusable skinning behaviors. We demonstrate the interactive use of skinning templates to quickly explore alternate skinning behaviors for 3D models.","cites":"21","conferencePercentile":"21.2962963"},{"venue":"ACM Trans. Graph.","id":"1bc117114d18e47e8a0c878e75a86b3936e6abdd","venue_1":"ACM Trans. Graph.","year":"2016","title":"Bijective maps from simplicial foliations","authors":"Marcel Campen, Cláudio T. Silva, Denis Zorin","author_ids":"2304114, 1719203, 1798055","abstract":"This paper presents a method for bijective parametrization of 2D and 3D objects over canonical domains. While a range of solutions for the two-dimensional case are well-known, our method guarantees bijectivity of mappings also for a large, combinatorially-defined class of tetrahedral meshes (shellable meshes). The key concept in our method is the piecewise-linear (PL) foliation, decomposing the mesh into one-dimensional submanifolds and reducing the mapping problem to parametrization of a lower-dimensional manifold (a foliation section). The maps resulting from these foliations are proved to be bijective and continuous, and shown to have provably bijective PL approximations. We describe exact, numerically robust evaluation methods and demonstrate our implementation's capabilities on a large variety of meshes.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"6d1e285059e47890d965bd194b7cc2337f48f2cf","venue_1":"ACM Trans. Graph.","year":"2015","title":"Elastic textures for additive fabrication","authors":"Julian Panetta, Qingnan Zhou, Luigi Malomo, Nico Pietroni, Paolo Cignoni, Denis Zorin","author_ids":"1964417, 2760811, 2908942, 3283805, 1738697, 1798055","abstract":"We introduce <i>elastic textures:</i> a set of parametric, tileable, printable, cubic patterns achieving a broad range of isotropic elastic material properties: the softest pattern is over a thousand times softer than the stiffest, and the Poisson's ratios range from below zero to nearly 0.5. Using a combinatorial search over topologies followed by shape optimization, we explore a wide space of truss-like, symmetric 3D patterns to obtain a small family. This pattern family can be printed without internal support structure on a single-material 3D printer and can be used to fabricate objects with prescribed mechanical behavior. The family can be extended easily to create anisotropic patterns with target orthotropic properties. We demonstrate that our elastic textures are able to achieve a user-supplied varying material property distribution. We also present a material optimization algorithm to choose material properties at each point within an object to best fit a target deformation under a prescribed scenario. We show that, by fabricating these spatially varying materials with elastic textures, the desired behavior is achieved.","cites":"12","conferencePercentile":"93.67346939"},{"venue":"ACM Trans. Graph.","id":"2d2f2bc65c53d2b004c9cf9b8c6d1c9b6c011b13","venue_1":"ACM Trans. Graph.","year":"2013","title":"OpenFab: a programmable pipeline for multi-material fabrication","authors":"Kiril Vidimce, Szu-Po Wang, Jonathan Ragan-Kelley, Wojciech Matusik","author_ids":"2891782, 3245779, 2488277, 1752521","abstract":"3D printing hardware is rapidly scaling up to output continuous mixtures of multiple materials at increasing resolution over ever larger print volumes. This poses an enormous computational challenge: large high-resolution prints comprise trillions of voxels and petabytes of data and simply modeling and describing the input with spatially varying material mixtures at this scale is challenging. Existing 3D printing software is insufficient; in particular, most software is designed to support only a few million primitives, with discrete material choices per object. We present OpenFab, a programmable pipeline for synthesis of multi-material 3D printed objects that is inspired by RenderMan and modern GPU pipelines. The pipeline supports procedural evaluation of geometric detail and material composition, using shader-like <i>fablets</i>, allowing models to be specified easily and efficiently. We describe a streaming architecture for OpenFab; only a small fraction of the final volume is stored in memory and output is fed to the printer with little startup delay. We demonstrate it on a variety of multi-material objects.","cites":"36","conferencePercentile":"89.36651584"},{"venue":"ACM Trans. Graph.","id":"4414c1b5b43264df9f41b38eb592d55f57a4decb","venue_1":"ACM Trans. Graph.","year":"2013","title":"Spec2Fab: a reducer-tuner model for translating specifications to 3D prints","authors":"Desai Chen, David I. W. Levin, Piotr Didyk, Pitchaya Sitthi-amorn, Wojciech Matusik","author_ids":"1701750, 1694784, 3307078, 1936229, 1752521","abstract":"Multi-material 3D printing allows objects to be composed of complex, heterogenous arrangements of materials. It is often more natural to define a functional goal than to define the material composition of an object. Translating these functional requirements to fabri-cable 3D prints is still an open research problem. Recently, several specific instances of this problem have been explored (e.g., appearance or elastic deformation), but they exist as isolated, monolithic algorithms. In this paper, we propose an abstraction mechanism that simplifies the design, development, implementation, and reuse of these algorithms. Our solution relies on two new data structures: a <i>reducer tree</i> that efficiently parameterizes the space of material assignments and a <i>tuner network</i> that describes the optimization process used to compute material arrangement. We provide an application programming interface for specifying the desired object and for defining parameters for the <i>reducer tree</i> and <i>tuner network</i>. We illustrate the utility of our framework by implementing several fabrication algorithms as well as demonstrating the manufactured results.","cites":"29","conferencePercentile":"84.84162896"},{"venue":"ACM Trans. Graph.","id":"35a02fc2b1a94348c3756ae08a05262beafc0d27","venue_1":"ACM Trans. Graph.","year":"2015","title":"Dyadic T-mesh subdivision","authors":"Denis Kovacs, Justin Bisceglio, Denis Zorin","author_ids":"2075157, 2325946, 1798055","abstract":"Meshes with T-joints (T-meshes) and related high-order surfaces have many advantages in situations where flexible local refinement is needed. At the same time, designing subdivision rules and bases for T-meshes is much more difficult, and fewer options are available. For common geometric modeling tasks it is desirable to retain the simplicity and flexibility of commonly used subdivision surfaces, and extend them to handle T-meshes.\n We propose a subdivision scheme extending Catmull-Clark and NURSS to a special class of quad T-meshes, dyadic T-meshes, which have no more than one T-joint per edge. Our scheme is based on a factorization with the same structure as Catmull-Clark subdivision. On regular T-meshes it is a refinement scheme for a subset of standard T-splines. While we use more variations of subdivision masks compared to Catmull-Clark and NURSS, the minimal size of the stencil is maintained, and all variations in formulas are due to simple changes in coefficients.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"98043f343c88d98ee1526a81cccd97f59fbccc79","venue_1":"ACM Trans. Graph.","year":"2013","title":"Fabricating translucent materials using continuous pigment mixtures","authors":"Marios Papas, Christian Regg, Wojciech Jarosz, Bernd Bickel, Philip Jackson, Wojciech Matusik, Steve Marschner, Markus H. Gross","author_ids":"3112836, 3039289, 1953515, 3083909, 7256581, 1752521, 2593798, 1743207","abstract":"We present a method for practical physical reproduction and design of homogeneous materials with desired subsurface scattering. Our process uses a collection of different pigments that can be suspended in a clear base material. Our goal is to determine pigment concentrations that best reproduce the appearance and subsurface scattering of a given target material. In order to achieve this task we first fabricate a collection of material samples composed of known mixtures of the available pigments with the base material. We then acquire their reflectance profiles using a custom-built measurement device. We use the same device to measure the reflectance profile of a target material. Based on the database of mappings from pigment concentrations to reflectance profiles, we use an optimization process to compute the concentration of pigments to best replicate the target material appearance. We demonstrate the practicality of our method by reproducing a variety of different translucent materials. We also present a tool that allows the user to explore the range of achievable appearances for a given set of pigments.","cites":"4","conferencePercentile":"9.049773756"},{"venue":"ACM Trans. Graph.","id":"4946f29cf4c7752d742d144adcf64c865150877a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Modeling and estimation of internal friction in cloth","authors":"Eder Miguel, Rasmus Tamstorf, Derek Bradley, Sara C. Schvartzman, Bernhard Thomaszewski, Bernd Bickel, Wojciech Matusik, Steve Marschner, Miguel A. Otaduy","author_ids":"1885083, 2236138, 1745149, 2592999, 1784345, 3083909, 1752521, 2593798, 1704342","abstract":"Force-deformation measurements of cloth exhibit significant hysteresis, and many researchers have identified internal friction as the source of this effect. However, it has not been incorporated into computer animation models of cloth. In this paper, we propose a model of internal friction based on an augmented reparameterization of Dahl's model, and we show that this model provides a good match to several important features of cloth hysteresis even with a minimal set of parameters. We also propose novel parameter estimation procedures that are based on simple and inexpensive setups and need only sparse data, as opposed to the complex hardware and dense data acquisition of previous methods. Finally, we provide an algorithm for the efficient simulation of internal friction, and we demonstrate it on simulation examples that show disparate behavior with and without internal friction.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"09e3433dbf131ddf50517616fe6c7677ebdfe0b3","venue_1":"ACM Trans. Graph.","year":"2014","title":"Strict minimizers for geometric optimization","authors":"Denis Zorin","author_ids":"1798055","abstract":"We introduce the idea of <i>strict minimizers</i> for geometric distortion measures used in shape interpolation, deformation, parametrization, and other applications involving geometric mappings. The <i>L</i><sub>&#8734;</sub>-norm ensures the tightest possible control on the worst-case distortion. Unfortunately, it does not yield a unique solution and does not distinguish between solutions with high or low distortion below the maximum. The strict minimizer is a minimal <i>L</i><sub>&#8734;</sub>-norm solution, which always prioritizes higher distortion reduction. We propose practical algorithms for computing strict minimizers. We also offer an efficient algorithm for <i>L</i><sub>&#8734;</sub> optimization based on the ARAP energy. This algorithm can be used on its own or as a building block for an ARAP strict minimizer. We demonstrate that these algorithms lead to significant improvements in quality.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"e2b01fa1663223296008f5452003a39047073296","venue_1":"ACM Trans. Graph.","year":"2014","title":"Locally injective parametrization with arbitrary fixed boundaries","authors":"Ofir Weber, Denis Zorin","author_ids":"3053534, 1798055","abstract":"We present an algorithm for mapping a triangle mesh, which is homeomorphic to a disk, to a planar domain with arbitrary fixed boundaries. The algorithm is guaranteed to produce a globally bijective map when the boundary is fixed to a shape that does not self-intersect. Obtaining a one-to-one map is of paramount importance for many graphics applications such as texture mapping. However, for other applications, such as quadrangulation, remeshing, and planar deformations, global bijectively may be unnecessarily constraining and requires significant increase on map distortion. For that reason, our algorithm allows the fixed boundary to intersect itself, and is <i>guaranteed</i> to produce a map that is injective locally (if such a map exists). We also extend the basic ideas of the algorithm to support the computation of discrete approximation for extremal quasiconformal maps. The algorithm is conceptually simple and fast. We demonstrate the superior robustness of our algorithm in various settings and configurations in which state-of-the-art algorithms fail to produce injective maps.","cites":"17","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"7fdfe6938156ec3a8db5674d893f653c5803334a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Computational design of mechanical characters","authors":"Stelian Coros, Bernhard Thomaszewski, Gioacchino Noris, Shinjiro Sueda, Moira Forberg, Robert W. Sumner, Wojciech Matusik, Bernd Bickel","author_ids":"1783776, 1784345, 3206366, 7229044, 2970807, 1693475, 1752521, 3083909","abstract":"We present an interactive design system that allows non-expert users to create animated mechanical characters. Given an articulated character as input, the user iteratively creates an animation by sketching motion curves indicating how different parts of the character should move. For each motion curve, our framework creates an optimized mechanism that reproduces it as closely as possible. The resulting mechanisms are attached to the character and then connected to each other using gear trains, which are created in a semi-automated fashion. The mechanical assemblies generated with our system can be driven with a single input driver, such as a hand-operated crank or an electric motor, and they can be fabricated using rapid prototyping devices. We demonstrate the versatility of our approach by designing a wide range of mechanical characters, several of which we manufactured using 3D printing. While our pipeline is designed for characters driven by planar mechanisms, significant parts of it extend directly to non-planar mechanisms, allowing us to create characters with compelling 3D motions.","cites":"53","conferencePercentile":"97.28506787"},{"venue":"ACM Trans. Graph.","id":"1d43fac12ceaaa43e57a6610e16bbc7fd36f70bc","venue_1":"ACM Trans. Graph.","year":"1997","title":"Clustering for Glossy Global Illumination","authors":"Per H. Christensen, Dani Lischinski, Eric J. Stollnitz, David Salesin","author_ids":"6729766, 1684384, 3011420, 1745260","abstract":"We present a new clustering algorithm for global illumination in complex environments. The new algorithm extends provious work on clustering for radiosity to allow for nondiffuse (glossy) reflectors. We represent clusters as points with directional distributions of outgoing and incoming radiance and importance, and we derive an error bound for transfers between these clusters. The algorithm groups input surfaces into a hierarchy of clusters, and then permits clusters to interact only if the error bound is below an acceptable tolerance. We show that the algorithm is asymptotically more efficient than previous clustering algorithms even when restricted to ideally diffuse environments. Finally, we demonstrate the performance of our method on two complex glossy environments.","cites":"74","conferencePercentile":"75"},{"venue":"ACM Trans. Graph.","id":"dcc1815236e6b8b47a0b10f1fa7b379cef582bee","venue_1":"ACM Trans. Graph.","year":"2014","title":"Robust field-aligned global parametrization","authors":"Ashish Myles, Nico Pietroni, Denis Zorin","author_ids":"2103823, 3283805, 1798055","abstract":"We present a robust method for computing locally bijective global parametrizations aligned with a given cross-field. The singularities of the parametrization in general agree with singularities of the field, except in a small number of cases when several additional cones need to be added in a controlled way. Parametric lines can be constrained to follow an arbitrary set of feature lines on the surface. Our method is based on constructing an initial quad patch partition using robust cross-field integral line tracing. This process is followed by an algorithm modifying the quad layout structure to ensure that consistent parametric lengths can be assigned to the edges. For most meshes, the layout modification algorithm does not add new singularities; a small number of singularities may be added to resolve an explicitly described set of layouts. We demonstrate that our algorithm succeeds on a test data set of over a hundred meshes.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"04cfe597af51a270cd4dc0b58afe0830912d3361","venue_1":"ACM Trans. Graph.","year":"2008","title":"Motion overview of human actions","authors":"Jackie Assa, Daniel Cohen-Or, I-Cheng Yeh, Tong-Yee Lee","author_ids":"2070014, 1701009, 1725404, 1724980","abstract":"During the last decade, motion capture data has emerged and gained a leading role in animations, games and 3D environments. Many of these applications require the creation of expressive overview video clips capturing the human motion, however sufficient attention has not been given to this problem. In this paper, we present a technique that generates an overview video based on the analysis of motion capture data. Our method is targeted for applications of 3D character based animations, automating, for example, the action summary and gameplay overview in simulations and computer games. We base our method on quantum annealing optimization with an objective function that respects the analysis of the character motion and the camera movement constraints. It automatically generates a smooth camera control path, splitting it to several shots if required. To evaluate our method, we introduce a novel camera placement metric which is evaluated against previous work and conduct a user study comparing our results with the various systems.","cites":"17","conferencePercentile":"15.43209877"},{"venue":"ACM Trans. Graph.","id":"502954c0c5ba93c248fa5ac9a7ae52d9f6a0cd03","venue_1":"ACM Trans. Graph.","year":"1996","title":"Global Illumination of Glossy Environments Using Wavelets and Importance","authors":"Per H. Christensen, Eric J. Stollnitz, David Salesin, Tony DeRose","author_ids":"6729766, 3011420, 1745260, 1792251","abstract":"We show how importance-driven refinement and a wavelet basis can be combined to provide an efficient solution to the global illumination problem with glossy and diffuse reflections. Importance is used to focus the computation on the interactions having the greatest impact on the visible solution. Wavelets are used to provide an efficient representation of radiance, importance, and the transport operator. We discuss a number of choices that must be made when constructing a finite element algorithm for glossy global illumination. Our algorithm is based on the standard wavelet decomposition of the transport operator and makes use of a four-dimensional wavelet representation for spatially and angularly varying radiance distributions. We use a final gathering step to improve the visual  quality of the solution. Features of our implementation include support for curved surfaces as well as texture-mapped anisotropic emission and reflection functions.","cites":"57","conferencePercentile":"66.66666667"},{"venue":"ACM Trans. Graph.","id":"d3f4454e9606d60e38d19669278792e9e1852871","venue_1":"ACM Trans. Graph.","year":"1993","title":"Adjustable Tools: An Object-Oriented Interaction Metaphor","authors":"David Salesin, Ronen Barzel","author_ids":"1745260, 1716832","abstract":"Purpose: To provide a simpler metaphor for representing, selecting, and adjusting collections of attributes for interactive operations. Backgrounds: Typical painting and graphic design programs present the user with a menu of operations, such as drawing, erasing, and text placement. Each of these operations, in turn, can have many attributes, such as pen width, color, and fill pattern. Often, a user may wish to alternate between several configurations of attribute settings. For instance, an artist might want to use an opaque blue line, then a semitransparent red wash, and then continue with the blue line. Unfortunately, alternating between sets of attributes may require performing many tedious menu operations. Moreover, it may be difficult or impossible to duplicate a previous set of attributes exactly. The practice of grouping collections of attributes together is becoming increasingly common. For example, the PHIGS graphics standard [2] uses \" bundles \" of drawing attributes, while Microsoft Word [5] allows the user to group parameters controlling the appearance of text, such as font, size, and spacing, into a single text \" style. \" In our work we apply the notion of grouped attributes to a graphical interaction method,","cites":"3","conferencePercentile":"7.142857143"},{"venue":"ACM Trans. Graph.","id":"8c47d887076d69e925aa7b2b86859f667f4576ac","venue_1":"ACM Trans. Graph.","year":"2008","title":"Upright orientation of man-made objects","authors":"Hongbo Fu, Daniel Cohen-Or, Gideon Dror, Alla Sheffer","author_ids":"1691065, 1701009, 2088846, 3354923","abstract":"Humans usually associate an upright orientation with objects, placing them in a way that they are most commonly seen in our surroundings. While it is an open challenge to recover the functionality of a shape from its geometry alone, this paper shows that it is often possible to infer its upright orientation by analyzing its geometry. Our key idea is to reduce the two-dimensional (spherical) orientation space to a small set of orientation candidates using functionality-related geometric properties of the object, and then determine the best orientation using an assessment function of several functional geometric attributes defined with respect to each candidate. Specifically we focus on obtaining the upright orientation for man-made objects that typically stand on some flat surface (ground, floor, table, etc.), which include the vast majority of objects in our everyday surroundings. For these types of models orientation candidates can be defined according to static equilibrium. For each candidate, we introduce a set of discriminative attributes linking shape to function. We learn an assessment function of these attributes from a training set using a combination of Random Forest classifier and Support Vector Machine classifier. Experiments demonstrate that our method generalizes well and achieves about 90% prediction accuracy for both a 10-fold cross-validation over the training set and a validation with an independent test set.","cites":"59","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"1c7400b65aed27242e9f8fcebdd7505682c2249d","venue_1":"ACM Trans. Graph.","year":"2008","title":"Skeleton extraction by mesh contraction","authors":"Oscar Kin-Chung Au, Chiew-Lan Tai, Hung-Kuo Chu, Daniel Cohen-Or, Tong-Yee Lee","author_ids":"1752997, 1702674, 1771281, 1701009, 1724980","abstract":"Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. In addition to producing a curve skeleton, the method generates other valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.","cites":"143","conferencePercentile":"94.44444444"},{"venue":"ACM Trans. Graph.","id":"44f18ef0800e276617e458bc21502947f35a7f94","venue_1":"ACM Trans. Graph.","year":"2016","title":"EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras","authors":"Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt","author_ids":"2933543, 7240032, 8188478, 3205238, 2712174, 1746884, 1697100, 1680185","abstract":"Marker-based and marker-less optical skeletal motion-capture methods use an <i>outside-in</i> arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort with marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. Alternative suit-based systems use several inertial measurement units or an exoskeleton to capture motion with an <i>inside-in</i> setup, i.e. without external sensors. This makes capture independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. Therefore, we propose a new method for real-time, marker-less, and egocentric motion capture: estimating the full-body skeleton pose from a lightweight stereo pair of fisheye cameras attached to a helmet or virtual reality headset - an <i>optical inside-in</i> method, so to speak. This allows full-body motion capture in general indoor and outdoor scenes, including crowded scenes with many people nearby, which enables reconstruction in larger-scale activities. Our approach combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. It is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"73c4963d308bd54e61bc46544f4552aad5dd1d91","venue_1":"ACM Trans. Graph.","year":"2008","title":"Data-driven enhancement of facial attractiveness","authors":"Tommer Leyvand, Daniel Cohen-Or, Gideon Dror, Dani Lischinski","author_ids":"3316156, 1701009, 2088846, 1684384","abstract":"When human raters are presented with a collection of shapes and asked to rank them according to their aesthetic appeal, the results often indicate that there is a statistical consensus among the raters. Yet it might be difficult to define a succinct set of rules that capture the aesthetic preferences of the raters. In this work, we explore a data-driven approach to aesthetic enhancement of such shapes. Specifically, we focus on the challenging problem of enhancing the aesthetic appeal (or the <i>attractiveness</i>) of human faces in frontal photographs (portraits), while maintaining close similarity with the original.\n The key component in our approach is an automatic facial attractiveness engine trained on datasets of faces with accompanying facial attractiveness ratings collected from groups of human raters. Given a new face, we extract a set of distances between a variety of facial feature locations, which define a point in a high-dimensional \"face space\". We then search the face space for a nearby point with a higher predicted attractiveness rating. Once such a point is found, the corresponding facial distances are embedded in the plane and serve as a target to define a 2D warp field which maps the original facial features to their adjusted locations. The effectiveness of our technique was experimentally validated by independent rating experiments, which indicate that it is indeed capable of increasing the facial attractiveness of most portraits that we have experimented with.","cites":"64","conferencePercentile":"73.45679012"},{"venue":"ACM Trans. Graph.","id":"a5e73ba1beb27ecd58612698818b7df391a63016","venue_1":"ACM Trans. Graph.","year":"2016","title":"Mesh arrangements for solid geometry","authors":"Qingnan Zhou, Eitan Grinspun, Denis Zorin, Alec Jacobson","author_ids":"2760811, 7522998, 1798055, 2574283","abstract":"Many high-level geometry processing tasks rely on low-level constructive solid geometry operations. Though trivial for implicit representations, boolean operations are notoriously difficult to execute robustly for explicit boundary representations. Existing methods for 3D triangle meshes fall short in one way or another. Some methods are fast but fail to produce closed, self-intersection free output. Other methods are robust but place prohibitively strict assumptions on the input, e.g., no hollow cavities, non-manifold edges or self-intersections. We propose a systematic recipe for conducting a family of exact constructive solid geometry operations. The two-stage method makes no general position assumptions and does not resort to numerical perturbation. The method is <i>variadic</i>, operating on any number of input meshes. This generalizes <i>unary</i> mesh-repair operations, classic <i>binary</i> boolean differencing, and <i>n</i>-ary operations such as finding all regions inside at least <i>k</i> out of <i>n</i> inputs. We demonstrate the superior effectiveness and robustness of our method on a dataset of 10,000 \"real-world\" meshes from a popular online repository. To encourage development, validation, and comparison, we release both our code and dataset to the public.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"3a3466bddadb183d43ef7dc9786ae17f7fe24c6c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Unwrap mosaics: a new representation for video editing","authors":"Alex Rav-Acha, Pushmeet Kohli, Carsten Rother, Andrew W. Fitzgibbon","author_ids":"2499069, 1685185, 7699610, 1708974","abstract":"We introduce a new representation for video which facilitates a number of common editing tasks. The representation has some of the power of a full reconstruction of 3D surface models from video, but is designed to be easy to recover from <i>a priori</i> unseen and uncalibrated footage. By modelling the image-formation process as a 2D-to-2D transformation from an object's texture map to the image, modulated by an object-space occlusion mask, we can recover a representation which we term the \"unwrap mosaic\". Many editing operations can be performed on the unwrap mosaic, and then re-composited into the original sequence, for example resizing objects, repainting textures, copying/cutting/pasting objects, and attaching effects layers to deforming objects.","cites":"35","conferencePercentile":"40.12345679"},{"venue":"ACM Trans. Graph.","id":"31df3d4ea3b57a8976fdbc9ba40e630c1e1ad417","venue_1":"ACM Trans. Graph.","year":"2009","title":"iWIRES: an analyze-and-edit approach to shape manipulation","authors":"Ran Gal, Olga Sorkine-Hornung, Niloy J. Mitra, Daniel Cohen-Or","author_ids":"2835543, 2250001, 1710455, 1701009","abstract":"Man-made objects are largely dominated by a few typical features that carry special characteristics and engineered meanings. State-of-the-art deformation tools fall short at preserving such characteristic features and global structure. We introduce iWIRES, a novel approach based on the argument that man-made models can be distilled using a few special 1D <i>wires</i> and their mutual relations. We hypothesize that maintaining the properties of such a small number of wires allows preserving the defining characteristics of the entire object. We introduce an <i>analyze-and-edit</i> approach, where prior to editing, we perform a light-weight analysis of the input shape to extract a descriptive set of wires. Analyzing the individual and mutual properties of the wires, and augmenting them with geometric attributes makes them intelligent and ready to be manipulated. Editing the object by modifying the intelligent wires leads to a powerful editing framework that retains the original design intent and object characteristics. We show numerous results of manipulation of man-made shapes using our editing technique.","cites":"92","conferencePercentile":"92.81767956"},{"venue":"ACM Trans. Graph.","id":"5e45ac8f94225ece5898b5b40cf276b0b822241a","venue_1":"ACM Trans. Graph.","year":"2009","title":"Curve skeleton extraction from incomplete point cloud","authors":"Andrea Tagliasacchi, Hao Zhang, Daniel Cohen-Or","author_ids":"1796480, 1682058, 1701009","abstract":"We present an algorithm for curve skeleton extraction from imperfect point clouds where large portions of the data may be missing. Our construction is primarily based on a novel notion of generalized <i>rotational symmetry axis</i> (ROSA) of an oriented point set. Specifically, given a subset <i>S</i> of oriented points, we introduce a variational definition for an oriented point that is most rotationally symmetric with respect to <i>S</i>. Our formulation effectively utilizes normal information to compensate for the missing data and leads to robust curve skeleton computation over regions of a shape that are generally cylindrical. We present an iterative algorithm via planar cuts to compute the ROSA of a point cloud. This is complemented by special handling of non-cylindrical joint regions to obtain a centered, topologically clean, and complete 1D skeleton. We demonstrate that quality curve skeletons can be extracted from a variety of shapes captured by incomplete point clouds. Finally, we show how our algorithm assists in shape completion under these challenges by developing a skeleton-driven point cloud completion scheme.","cites":"63","conferencePercentile":"82.04419889"},{"venue":"ACM Trans. Graph.","id":"2b9865a739b1a6ad2dd5fb06f9098db193c34a10","venue_1":"ACM Trans. Graph.","year":"2009","title":"Coordinates for instant image cloning","authors":"Zeev Farbman, Gil Hoffer, Yaron Lipman, Daniel Cohen-Or, Dani Lischinski","author_ids":"1709900, 2141652, 3232072, 1701009, 1684384","abstract":"Seamless cloning of a source image patch into a target image is an important and useful image editing operation, which has received considerable research attention in recent years. This operation is typically carried out by solving a Poisson equation with Dirichlet boundary conditions, which smoothly interpolates the discrepancies between the boundary of the source patch and the target across the entire cloned area. In this paper we introduce an alternative, <i>coordinate-based</i> approach, where rather than solving a large linear system to perform the aforementioned interpolation, the value of the interpolant at each interior pixel is given by a weighted combination of values along the boundary. More specifically, our approach is based on Mean-Value Coordinates (MVC). The use of coordinates is advantageous in terms of speed, ease of implementation, small memory footprint, and parallelizability, enabling real-time cloning of large regions, and interactive cloning of video streams. We demonstrate a number of applications and extensions of the coordinate-based framework.","cites":"81","conferencePercentile":"91.16022099"},{"venue":"ACM Trans. Graph.","id":"14f201426bcb10d3dba014e50679e9a80927a667","venue_1":"ACM Trans. Graph.","year":"2002","title":"Linear combination of transformations","authors":"Marc Alexa","author_ids":"1751554","abstract":"Geometric transformations are most commonly represented as square matrices in computer graphics. Following simple geometric arguments we derive a natural and geometrically meaningful definition of scalar multiples and a commutative addition of transformations based on the matrix representation, given that the matrices have no negative real eigenvalues. Together, these operations allow the linear combination of transformations. This provides the ability to create weighted combination of transformations, interpolate between transformations, and to construct or use arbitrary transformations in a structure similar to a basis of a vector space. These basic techniques are useful for synthesis and analysis of motions or animations. Animations through a set of key transformations are generated using standard techniques such as subdivision curves. For analysis and progressive compression a PCA can be applied to sequences of transformations. We describe an implementation of the techniques that enables an easy-to-use and transparent way of dealing with geometric transformations in graphics software. We compare and relate our approach to other techniques such as matrix decomposition and quaternion interpolation.","cites":"131","conferencePercentile":"58"},{"venue":"ACM Trans. Graph.","id":"3135746d065105e06f419ba71b77fe200404a242","venue_1":"ACM Trans. Graph.","year":"2002","title":"Video matting of complex scenes","authors":"Yung-Yu Chuang, Aseem Agarwala, Brian Curless, David Salesin, Richard Szeliski","author_ids":"3032320, 1696487, 1810052, 1745260, 1717841","abstract":"This paper describes a new framework for <i>video matting,</i> the process of pulling a high-quality alpha matte and foreground from a video sequence. The framework builds upon techniques in natural image matting, optical flow computation, and background estimation. User interaction is comprised of garbage matte specification if background estimation is needed, and hand-drawn keyframe segmentations into \"foreground,\" \"background\" and \"unknown\". The segmentations, called <i>trimaps,</i> are interpolated across the video volume using forward and backward optical flow. Competing flow estimates are combined based on information about where flow is likely to be accurate. A Bayesian matting technique uses the flowed trimaps to yield high-quality mattes of moving foreground elements with complex boundaries filmed by a moving camera. A novel technique for smoke matte extraction is also demonstrated.","cites":"191","conferencePercentile":"69"},{"venue":"ACM Trans. Graph.","id":"2d3b9b0108d0645e1528f3881a830f6d900638fd","venue_1":"ACM Trans. Graph.","year":"2003","title":"Progressive point set surfaces","authors":"Shachar Fleishman, Daniel Cohen-Or, Marc Alexa, Cláudio T. Silva","author_ids":"3355053, 1701009, 1751554, 1719203","abstract":"Progressive point set surfaces (PPSS) are a multilevel point-based surface representation. They combine the usability of multilevel scalar displacement maps (e.g., compression, filtering, geometric modeling) with the generality of point-based surface representations (i.e., no fixed homology group or continuity class). The multiscale nature of PPSS fosters the idea of <i>point-based modeling.</i> The basic building block for the construction of PPSS is a projection operator, which maps points in the proximity of the shape onto local polynomial surface approximations. The projection operator allows the computing of displacements from smoother to more detailed levels. Based on the properties of the projection operator we derive an algorithm to construct a base point set. Starting from this base point set, a refinement rule using the projection operator constructs a PPSS from any given manifold surface.","cites":"94","conferencePercentile":"53.76344086"},{"venue":"ACM Trans. Graph.","id":"0ae405508764fc63019fe4fc0b91744c5fd381e2","venue_1":"ACM Trans. Graph.","year":"2008","title":"Space-time surface reconstruction using incompressible flow","authors":"Andrei Sharf, Dan A. Alcantara, Thomas Lewiner, Chen Greif, Alla Sheffer, Nina Amenta, Daniel Cohen-Or","author_ids":"2120270, 2796359, 3212265, 1805855, 3354923, 1764578, 1701009","abstract":"We introduce a volumetric space-time technique for the reconstruction of moving and deforming objects from point data. The output of our method is a four-dimensional space-time solid, made up of spatial slices, each of which is a three-dimensional solid bounded by a watertight manifold. The motion of the object is described as an incompressible flow of material through time. We optimize the flow so that the distance material moves from one time frame to the next is bounded, the density of material remains constant, and the object remains compact. This formulation overcomes deficiencies in the acquired data, such as persistent occlusions, errors, and missing frames. We demonstrate the performance of our flow-based technique by reconstructing coherent sequences of watertight models from incomplete scanner data.","cites":"50","conferencePercentile":"61.72839506"},{"venue":"ACM Trans. Graph.","id":"064585db7573aef0c0f3b0618f1ae51667e78348","venue_1":"ACM Trans. Graph.","year":"2013","title":"Patch-based high dynamic range video","authors":"Nima Khademi Kalantari, Eli Shechtman, Connelly Barnes, Soheil Darabi, Dan B. Goldman, Pradeep Sen","author_ids":"1717070, 2177801, 1794537, 2600320, 1976171, 2258791","abstract":"Despite significant progress in high dynamic range (HDR) imaging over the years, it is still difficult to capture high-quality HDR video with a conventional, off-the-shelf camera. The most practical way to do this is to capture alternating exposures for every LDR frame and then use an alignment method based on optical flow to register the exposures together. However, this results in objectionable artifacts whenever there is complex motion and optical flow fails. To address this problem, we propose a new approach for HDR reconstruction from alternating exposure video sequences that combines the advantages of optical flow and recently introduced patch-based synthesis for HDR images. We use patch-based synthesis to enforce similarity between adjacent frames, increasing temporal continuity. To synthesize visually plausible solutions, we enforce constraints from motion estimation coupled with a search window map that guides the patch-based synthesis. This results in a novel reconstruction algorithm that can produce high-quality HDR videos with a standard camera. Furthermore, our method is able to synthesize plausible texture and motion in fast-moving regions, where either patch-based synthesis or optical flow alone would exhibit artifacts. We present results of our reconstructed HDR video sequences that are superior to those produced by current approaches.","cites":"20","conferencePercentile":"73.98190045"},{"venue":"ACM Trans. Graph.","id":"0a59923dc47606da816a94c9080f4c2e28c919e2","venue_1":"ACM Trans. Graph.","year":"2011","title":"Video face replacement","authors":"Kevin Dale, Kalyan Sunkavalli, Micah K. Johnson, Daniel Vlasic, Wojciech Matusik, Hanspeter Pfister","author_ids":"1734877, 2454127, 1744375, 1880628, 1752521, 1701371","abstract":"We present a method for replacing facial performances in video. Our approach accounts for differences in identity, visual appearance, speech, and timing between source and target videos. Unlike prior work, it does not require substantial manual operation or complex acquisition hardware, only single-camera video. We use a 3D multilinear model to track the facial performance in both videos. Using the corresponding 3D geometry, we warp the source to the target face and retime the source to match the target performance. We then compute an optimal seam through the video volume that maintains temporal consistency in the final composite. We showcase the use of our method on a variety of examples and present the result of a user study that suggests our results are difficult to distinguish from real video footage.","cites":"40","conferencePercentile":"78.94736842"},{"venue":"ACM Trans. Graph.","id":"7ac13529d9d23e7921e6a4f179bd2f8a307254ff","venue_1":"ACM Trans. Graph.","year":"2011","title":"Computational stereo camera system with programmable control loop","authors":"Simon Heinzle, Pierre Greisen, David Gallup, Christine Chen, Daniel Saner, Aljoscha Smolic, Andreas Peter Burg, Wojciech Matusik, Markus H. Gross","author_ids":"1782613, 1724398, 1796345, 6067993, 3258486, 1741139, 8063237, 1752521, 1743207","abstract":"Stereoscopic 3D has gained significant importance in the entertainment industry. However, production of high quality stereoscopic content is still a challenging art that requires mastering the complex interplay of human perception, 3D display properties, and artistic intent. In this paper, we present a computational stereo camera system that closes the control loop from capture and analysis to automatic adjustment of physical parameters. Intuitive interaction metaphors are developed that replace cumbersome handling of rig parameters using a touch screen interface with 3D visualization. Our system is designed to make stereoscopic 3D production as easy, intuitive, flexible, and reliable as possible. Captured signals are processed and analyzed in real-time on a stream processor. Stereoscopy and user settings define programmable control functionalities, which are executed in real-time on a control processor. Computational power and flexibility is enabled by a dedicated software and hardware architecture. We show that even traditionally difficult shots can be easily captured using our system.","cites":"29","conferencePercentile":"63.15789474"},{"venue":"ACM Trans. Graph.","id":"c77064f410f557ce752713523201f10ad04177fc","venue_1":"ACM Trans. Graph.","year":"2016","title":"Printone: interactive resonance simulation for free-form print-wind instrument design","authors":"Nobuyuki Umetani, Athina Panotopoulou, Ryan Schmidt, Emily Whiting","author_ids":"2065148, 1940815, 2291899, 2778710","abstract":"This paper presents an interactive design interface for three-dimensional free-form musical wind instruments. The sound of a wind instrument is governed by the acoustic resonance as a result of complicated interactions of sound waves and internal geometries of the instrument. Thus, creating an original free-form wind instrument by manual methods is a challenging problem. Our interface provides interactive sound simulation feedback as the user edits, allowing exploration of original wind instrument designs. Sound simulation of a 3D wind musical instrument is known to be computationally expensive. To overcome this problem, we first model the wind instruments as a passive resonator, where we ignore coupled oscillation excitation from the mouthpiece. Then we present a novel efficient method to estimate the resonance frequency based on the boundary element method by formulating the resonance problem as a minimum eigenvalue problem. Furthermore, we can efficiently compute an approximate resonance frequency using a new technique based on a generalized eigenvalue problem. The designs can be fabricated using a 3D printer, thus we call the results \"print-wind instruments\" in association with woodwind instruments. We demonstrate our approach with examples of unconventional shapes performing familiar songs.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"086d596371a812ed904032bfcdb63e5f24c55b11","venue_1":"ACM Trans. Graph.","year":"2012","title":"A luminance-contrast-aware disparity model and applications","authors":"Piotr Didyk, Tobias Ritschel, Elmar Eisemann, Karol Myszkowski, Hans-Peter Seidel, Wojciech Matusik","author_ids":"3307078, 1759347, 1737690, 1790911, 1746884, 1752521","abstract":"Binocular disparity is one of the most important depth cues used by the human visual system. Recently developed stereo-perception models allow us to successfully manipulate disparity in order to improve viewing comfort, depth discrimination as well as stereo content compression and display. Nonetheless, all existing models neglect the substantial influence of luminance on stereo perception. Our work is the first to account for the interplay of luminance contrast (magnitude/frequency) and disparity and our model predicts the human response to complex stereo-luminance images. Besides improving existing disparity-model applications (e.g., difference metrics or compression), our approach offers new possibilities, such as joint luminance contrast and disparity manipulation or the optimization of auto-stereoscopic content. We validate our results in a user study, which also reveals the advantage of considering luminance contrast and its significant impact on disparity manipulation techniques.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"d270eebc34741489e9d264a746897318dc5e6a44","venue_1":"ACM Trans. Graph.","year":"2012","title":"Physical face cloning","authors":"Bernd Bickel, Peter Kaufmann, Mélina Skouras, Bernhard Thomaszewski, Derek Bradley, Thabo Beeler, Philip Jackson, Steve Marschner, Wojciech Matusik, Markus H. Gross","author_ids":"3083909, 2027140, 2529055, 1784345, 1745149, 2486770, 7256581, 2593798, 1752521, 1743207","abstract":"We propose a complete process for designing, simulating, and fabricating synthetic skin for an animatronics character that mimics the face of a given subject and its expressions. The process starts with measuring the elastic properties of a material used to manufacture synthetic soft tissue. Given these measurements we use physics-based simulation to predict the behavior of a face when it is driven by the underlying robotic actuation. Next, we capture 3D facial expressions for a given target subject. As the key component of our process, we present a novel optimization scheme that determines the shape of the synthetic skin as well as the actuation parameters that provide the best match to the target expressions. We demonstrate this computational skin design by physically cloning a real human face onto an animatronics figure.","cites":"19","conferencePercentile":"56.81818182"},{"venue":"ACM Trans. Graph.","id":"9b01a133ca48612adc94ae120bfbf36611caac3a","venue_1":"ACM Trans. Graph.","year":"2012","title":"Chopper: partitioning models into 3D-printable parts","authors":"Linjie Luo, Ilya Baran, Szymon Rusinkiewicz, Wojciech Matusik","author_ids":"1702459, 1789898, 7723706, 1752521","abstract":"3D printing technology is rapidly maturing and becoming ubiquitous. One of the remaining obstacles to wide-scale adoption is that the object to be printed must fit into the working volume of the 3D printer. We propose a framework, called Chopper, to decompose a large 3D object into smaller parts so that each part fits into the printing volume. These parts can then be assembled to form the original object. We formulate a number of desirable criteria for the partition, including assemblability, having few components, unobtrusiveness of the seams, and structural soundness. Chopper optimizes these criteria and generates a partition either automatically or with user guidance. Our prototype outputs the final decomposed parts with customized connectors on the interfaces. We demonstrate the effectiveness of Chopper on a variety of non-trivial real-world objects.","cites":"41","conferencePercentile":"86.11111111"},{"venue":"ACM Trans. Graph.","id":"5529a212cdcda6ed3ba416bb10482f3c47f191a9","venue_1":"ACM Trans. Graph.","year":"2004","title":"Context-based surface completion","authors":"Andrei Sharf, Marc Alexa, Daniel Cohen-Or","author_ids":"2120270, 1751554, 1701009","abstract":"Sampling complex, real-world geometry with range scanning devices almost always yields imperfect surface samplings. These \"holes\" in the surface are commonly filled with a smooth patch that conforms with the boundary. We introduce a context-based method: the characteristics of the given surface are analyzed, and the hole is iteratively filled by copying patches from valid regions of the given surface. In particular, the method needs to determine best matching patches, and then, fit imported patches by aligning them with the surrounding surface. The completion process works top down, where details refine intermediate coarser approximations. To align an imported patch with the existing surface, we apply a rigid transformation followed by an iterative closest point procedure with non-rigid transformations. The surface is essentially treated as a point set, and local implicit approximations aid in measuring the similarity between two point set patches. We demonstrate the method at several point-sampled surfaces, where the holes either result from imperfect sampling during range scanning or manual removal.","cites":"104","conferencePercentile":"50.54347826"},{"venue":"ACM Trans. Graph.","id":"55fd368087a67be324c1b02e45ca81dd0607f595","venue_1":"ACM Trans. Graph.","year":"2008","title":"Non-homogeneous resizing of complex models","authors":"Vladislav Kraevoy, Alla Sheffer, Ariel Shamir, Daniel Cohen-Or","author_ids":"2406481, 3354923, 2947946, 1701009","abstract":"Resizing of 3D models can be very useful when creating new models or placing models inside different scenes. However, uniform scaling is limited in its applicability while straightforward non-uniform scaling can destroy features and lead to serious visual artifacts. Our goal is to define a method that protects model features and structures during resizing. We observe that typically, during scaling some parts of the models are more vulnerable than others, undergoing undesirable deformation. We automatically detect vulnerable regions and carry this information to a protective grid defined around the object, defining a vulnerability map. The 3D model is then resized by a space-deformation technique which scales the grid non-homogeneously while respecting this map. Using space-deformation allows processing of common models of man-made objects that consist of multiple components and contain non-manifold structures. We show that our technique resizes models while suppressing undesirable distortion, creating models that preserve the structure and features of the original ones.","cites":"46","conferencePercentile":"57.40740741"},{"venue":"ACM Trans. Graph.","id":"23ef80a68a8958938c79315e4eb5dd3544ba43f7","venue_1":"ACM Trans. Graph.","year":"2005","title":"A sketch-based interface for detail-preserving mesh editing","authors":"Andrew Nealen, Olga Sorkine-Hornung, Marc Alexa, Daniel Cohen-Or","author_ids":"1729677, 2250001, 1751554, 1701009","abstract":"In this paper we present a method for the intuitive editing of surface meshes by means of view-dependent sketching. In most existing shape deformation work, editing is carried out by selecting and moving a <i>handle</i>, usually a set of vertices. Our system lets the user easily determine the handle, either by silhouette selection and cropping, or by sketching directly onto the surface. Subsequently, an edit is carried out by sketching a new, view-dependent handle position or by indirectly influencing differential properties along the sketch. Combined, these editing and handle metaphors greatly simplify otherwise complex shape modeling tasks.","cites":"131","conferencePercentile":"72.58064516"},{"venue":"ACM Trans. Graph.","id":"0d73da84ffc0ef18bb69af9a02fa265edaf5157c","venue_1":"ACM Trans. Graph.","year":"2011","title":"Computing and fabricating multilayer models","authors":"Michael Holroyd, Ilya Baran, Jason Lawrence, Wojciech Matusik","author_ids":"3151083, 1789898, 1694005, 1752521","abstract":"We present a method for automatically converting a digital 3D model into a <i>multilayer model</i>: a parallel stack of high-resolution 2D images embedded within a semi-transparent medium. Multilayer models can be produced quickly and cheaply and provide a strong sense of an object's 3D shape and texture over a wide range of viewing directions. Our method is designed to minimize visible cracks and other artifacts that can arise when projecting an input model onto a small number of parallel planes, and avoid layer transitions that cut the model along important surface features. We demonstrate multilayer models fabricated with glass and acrylic tiles using commercially available printers.","cites":"13","conferencePercentile":"30.26315789"},{"venue":"ACM Trans. Graph.","id":"41cfc9edbf36754746991c2a1e9a47c0d129d105","venue_1":"ACM Trans. Graph.","year":"2016","title":"Perspective-aware manipulation of portrait photos","authors":"Ohad Fried, Eli Shechtman, Dan B. Goldman, Adam Finkelstein","author_ids":"2416503, 2177801, 1976171, 1707541","abstract":"This paper introduces a method to modify the apparent relative pose and distance between camera and subject given a single portrait photo. Our approach fits a full perspective camera and a parametric 3D head model to the portrait, and then builds a 2D warp in the image plane to approximate the effect of a desired change in 3D. We show that this model is capable of correcting objectionable artifacts such as the large noses sometimes seen in \"selfies,\" or to deliberately bring a distant camera closer to the subject. This framework can also be used to re-pose the subject, as well as to create stereo pairs from an input portrait. We show convincing results on both an existing dataset as well as a new dataset we captured to validate our method.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"646c0b975be8090c12fbd0e082d8c03b25669f1a","venue_1":"ACM Trans. Graph.","year":"2010","title":"Personal photo enhancement using example images","authors":"Neel Joshi, Wojciech Matusik, Edward H. Adelson, David J. Kriegman","author_ids":"2641664, 1752521, 1788148, 8404973","abstract":"We describe a framework for improving the quality of personal photos by using a person's favorite photographs as examples. We observe that the majority of a person's photographs include the faces of a photographer's family and friends and often the errors in these photographs are the most disconcerting. We focus on correcting these types of images and use common faces across images to automatically perform both global and face-specific corrections. Our system achieves this by using face detection to align faces between &#8220;good&#8221; and &#8220;bad&#8221; photos such that properties of the good examples can be used to correct a bad photo. These &#8220;personal&#8221; photos provide strong guidance for a number of operations and, as a result, enable a number of high-quality image processing operations. We illustrate the power and generality of our approach by presenting a novel deblurring algorithm, and we show corrections that perform sharpening, superresolution, in-painting of over- and underexposured regions, and white-balancing.","cites":"31","conferencePercentile":"54.09356725"},{"venue":"ACM Trans. Graph.","id":"9d8e1e1e7511ed17db4dec4ee60ed284e5516e60","venue_1":"ACM Trans. Graph.","year":"2009","title":"Optimizing content-preserving projections for wide-angle images","authors":"Robert Carroll, Maneesh Agrawala, Aseem Agarwala","author_ids":"3149497, 1820412, 1696487","abstract":"Any projection of a 3D scene into a wide-angle image unavoidably results in distortion. Current projection methods either bend straight lines in the scene, or locally distort the shapes of scene objects. We present a method that minimizes this distortion by adapting the projection to content in the scene, such as salient scene regions and lines, in order to preserve their shape. Our optimization technique computes a spatially-varying projection that respects user-specified constraints while minimizing a set of energy terms that measure wide-angle image distortion. We demonstrate the effectiveness of our approach by showing results on a variety of wide-angle photographs, as well as comparisons to standard projections.","cites":"34","conferencePercentile":"50.82872928"},{"venue":"ACM Trans. Graph.","id":"390457b940fc8a11e582f096fe18c244550f268a","venue_1":"ACM Trans. Graph.","year":"2009","title":"Approximating subdivision surfaces with Gregory patches for hardware tessellation","authors":"Charles T. Loop, Scott Schaefer, Tianyun Ni, Ignacio Castaño","author_ids":"1839367, 1687097, 1896574, 3035617","abstract":"We present a new method for approximating subdivision surfaces with hardware accelerated parametric patches. Our method improves the memory bandwidth requirements for patch control points, translating into superior performance compared to existing methods. Our input is general, allowing for meshes that contain both quadrilateral and triangular faces in the input control mesh, as well as control meshes with boundary. We present two implementations of our scheme designed to run on Direct3D 11 class hardware equipped with a tessellator unit.","cites":"35","conferencePercentile":"52.76243094"},{"venue":"ACM Trans. Graph.","id":"2b6d835b729682f1d21b58ef29778f7db59bb319","venue_1":"ACM Trans. Graph.","year":"2012","title":"Feature-adaptive GPU rendering of Catmull-Clark subdivision surfaces","authors":"Matthias Nießner, Charles T. Loop, Mark Meyer, Tony DeRose","author_ids":"2209612, 1839367, 8735841, 1792251","abstract":"We present a novel method for high-performance GPU-based rendering of Catmull-Clark subdivision surfaces. Unlike previous methods, our algorithm computes the true limit surface up to machine precision, and is capable of rendering surfaces that conform to the full RenderMan specification for Catmull-Clark surfaces. Specifically, our algorithm can accommodate base meshes consisting of arbitrary valence vertices and faces, and the surface can contain any number and arrangement of semisharp creases and hierarchically defined detail. We also present a variant of the algorithm which guarantees watertight positions and normals, meaning that even displaced surfaces can be rendered in a crack-free manner. Finally, we describe a view-dependent level-of-detail scheme which adapts to both the depth of subdivision and the patch tessellation density. Though considerably more general, the performance of our algorithm is comparable to the best approximating method, and is considerably faster than Stam's exact method.","cites":"29","conferencePercentile":"76.26262626"},{"venue":"ACM Trans. Graph.","id":"1aaeab33bbc8660b2578556bd8c2e34debb99644","venue_1":"ACM Trans. Graph.","year":"2009","title":"Capture and modeling of non-linear heterogeneous soft tissue","authors":"Bernd Bickel, Moritz Bächer, Miguel A. Otaduy, Wojciech Matusik, Hanspeter Pfister, Markus H. Gross","author_ids":"3083909, 8021864, 1704342, 1752521, 1701371, 1743207","abstract":"This paper introduces a data-driven representation and modeling technique for simulating non-linear heterogeneous soft tissue. It simplifies the construction of convincing deformable models by avoiding complex selection and tuning of physical material parameters, yet retaining the richness of non-linear heterogeneous behavior. We acquire a set of example deformations of a real object, and represent each of them as a spatially varying stress-strain relationship in a finite-element model. We then model the material by non-linear interpolation of these stress-strain relationships in strain-space. Our method relies on a simple-to-build capture system and an efficient run-time simulation algorithm based on incremental loading, making it suitable for interactive computer graphics applications. We present the results of our approach for several non-linear materials and biological soft tissue, with accurate agreement of our model to the measured data.","cites":"44","conferencePercentile":"65.19337017"},{"venue":"ACM Trans. Graph.","id":"c7d8b8288af0bbd2e5f4bb3f48b140a09a52ec60","venue_1":"ACM Trans. Graph.","year":"2009","title":"Printing spatially-varying reflectance","authors":"Wojciech Matusik, Boris Ajdin, Jinwei Gu, Jason Lawrence, Hendrik P. A. Lensch, Fabio Pellacini, Szymon Rusinkiewicz","author_ids":"1752521, 2027612, 2931118, 1694005, 1809190, 1757883, 7723706","abstract":"Although real-world surfaces can exhibit significant variation in materials --- glossy, diffuse, metallic, etc. --- printers are usually used to reproduce color or gray-scale images. We propose a complete system that uses appropriate inks and foils to print documents with a variety of material properties. Given a set of inks with known Bidirectional Reflectance Distribution Functions (BRDFs), our system automatically finds the optimal linear combinations to approximate the BRDFs of the target documents. Novel gamut-mapping algorithms preserve the relative glossiness between different BRDFs, and halftoning is used to produce patterns to be sent to the printer. We demonstrate the effectiveness of this approach with printed samples of a number of measured spatially-varying BRDFs.","cites":"39","conferencePercentile":"58.83977901"},{"venue":"ACM Trans. Graph.","id":"a0d3f59fb847466e677efe853ff2771c14c58ca2","venue_1":"ACM Trans. Graph.","year":"2013","title":"3-Sweep: extracting editable objects from a single photo","authors":"Tao Chen, Zhe Zhu, Ariel Shamir, Shi-Min Hu, Daniel Cohen-Or","author_ids":"4725884, 3231305, 2947946, 1686809, 1701009","abstract":"We introduce an interactive technique for manipulating simple 3D shapes based on extracting them from a single photograph. Such extraction requires understanding of the components of the shape, their projections, and relations. These simple cognitive tasks for humans are particularly difficult for automatic algorithms. Thus, our approach combines the cognitive abilities of humans with the computational accuracy of the machine to solve this problem. Our technique provides the user the means to quickly create editable 3D parts---human assistance implicitly segments a complex object into its components, and positions them in space. In our interface, three strokes are used to generate a 3D component that snaps to the shape's outline in the photograph, where each stroke defines one dimension of the component. The computer reshapes the component to fit the image of the object in the photograph as well as to satisfy various inferred geometric constraints imposed by its global 3D structure. We show that with this intelligent interactive modeling tool, the daunting task of object extraction is made simple. Once the 3D object has been extracted, it can be quickly edited and placed back into photos or 3D scenes, permitting object-driven photo editing tasks which are impossible to perform in image-space. We show several examples and present a user study illustrating the usefulness of our technique.","cites":"32","conferencePercentile":"87.55656109"},{"venue":"ACM Trans. Graph.","id":"badb198c67b9000c55bfec09fd4d27afc1bd1464","venue_1":"ACM Trans. Graph.","year":"2012","title":"Active co-analysis of a set of shapes","authors":"Yunhai Wang, Shmulik Asafi, Oliver van Kaick, Hao Zhang, Daniel Cohen-Or, Baoquan Chen","author_ids":"2008043, 2252217, 3276873, 1682058, 1701009, 1748939","abstract":"Unsupervised co-analysis of a set of shapes is a difficult problem since the geometry of the shapes alone cannot always fully describe the semantics of the shape parts. In this paper, we propose a semi-supervised learning method where the user actively assists in the co-analysis by iteratively providing inputs that progressively constrain the system. We introduce a novel constrained clustering method based on a spring system which embeds elements to better respect their inter-distances in feature space together with the user-given set of constraints. We also present an active learning method that suggests to the user where his input is likely to be the most effective in refining the results. We show that each single pair of constraints affects many relations across the set. Thus, the method requires only a sparse set of constraints to quickly converge toward a consistent and error-free semantic labeling of the set.","cites":"44","conferencePercentile":"88.38383838"},{"venue":"ACM Trans. Graph.","id":"5854663c261585192ac9594dc95352f77aa18768","venue_1":"ACM Trans. Graph.","year":"2013","title":"Analytic displacement mapping using hardware tessellation","authors":"Matthias Nießner, Charles T. Loop","author_ids":"2209612, 1839367","abstract":"Displacement mapping is ideal for modern GPUs since it enables high-frequency geometric surface detail on models with low memory I/O. However, problems such as texture seams, normal recomputation, and undersampling artifacts have limited its adoption. We provide a comprehensive solution to these problems by introducing a smooth analytic displacement function. Coefficients are stored in a GPU-friendly tile-based texture format, and a multiresolution mip hierarchy of this function is formed. We propose a novel level-of-detail scheme by computing per-vertex adaptive tessellation factors and select the appropriate prefiltered mip levels of the displacement function. Our method obviates the need for a precomputed normal map since normals are directly derived from the displacements. Thus, we are able to perform authoring and rendering simultaneously without typical displacement map extraction from a dense triangle mesh. This not only is more flexible than the traditional combination of discrete displacements and normal maps, but also provides faster runtime due to reduced memory I/O.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"5d3b6eb6b929e50c228c73423287a353c3a75880","venue_1":"ACM Trans. Graph.","year":"2014","title":"Real-time non-rigid reconstruction using an RGB-D camera","authors":"Michael Zollhöfer, Matthias Nießner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew W. Fitzgibbon, Charles T. Loop, Christian Theobalt, Marc Stamminger","author_ids":"1699058, 2209612, 1699068, 2086328, 1713941, 2676553, 1682672, 1708974, 1839367, 1680185, 1712970","abstract":"We present a combined hardware and software solution for markerless reconstruction of non-rigidly deforming physical objects with arbitrary shape in <i>real-time</i>. Our system uses a single self-contained stereo camera unit built from off-the-shelf components and consumer graphics hardware to generate spatio-temporally coherent 3D models at 30 Hz. A new stereo matching algorithm estimates real-time RGB-D data. We start by scanning a smooth template model of the subject as they move rigidly. This geometric surface prior avoids strong scene assumptions, such as a kinematic human skeleton or a parametric shape model. Next, a novel GPU pipeline performs non-rigid registration of live RGB-D data to the smooth template using an extended non-linear as-rigid-as-possible (ARAP) framework. High-frequency details are fused onto the final mesh using a linear deformation model. The system is an order of magnitude faster than state-of-the-art methods, while matching the quality and robustness of many offline algorithms. We show precise real-time reconstructions of diverse scenes, including: large deformations of users' heads, hands, and upper bodies; fine-scale wrinkles and folds of skin and clothing; and non-rigid interactions performed by users on flexible objects such as toys. We demonstrate how acquired models can be used for many interactive scenarios, including re-texturing, online performance capture and preview, and real-time shape and motion re-targeting.","cites":"46","conferencePercentile":"98.97119342"},{"venue":"ACM Trans. Graph.","id":"6f60846bfbd4dce3edd4f6ee35df412981abd6e9","venue_1":"ACM Trans. Graph.","year":"2009","title":"Compressive light transport sensing","authors":"Pieter Peers, Dhruv Kumar Mahajan, Bruce Lamond, Abhijeet Ghosh, Wojciech Matusik, Ravi Ramamoorthi, Paul E. Debevec","author_ids":"1808270, 1962364, 2134940, 1740273, 1752521, 1752236, 1778676","abstract":"In this article we propose a new framework for capturing light transport data of a real scene, based on the recently developed theory of compressive sensing. Compressive sensing offers a solid mathematical framework to infer a sparse signal from a limited number of nonadaptive measurements. Besides introducing compressive sensing for fast acquisition of light transport to computer graphics, we develop several innovations that address specific challenges for image-based relighting, and which may have broader implications. We develop a novel hierarchical decoding algorithm that improves reconstruction quality by exploiting interpixel coherency relations. Additionally, we design new nonadaptive illumination patterns that minimize measurement noise and further improve reconstruction quality. We illustrate our framework by capturing detailed high-resolution reflectance fields for image-based relighting.","cites":"53","conferencePercentile":"74.03314917"},{"venue":"ACM Trans. Graph.","id":"0ba4cd78e8fbace0858fc6ab6f3346266b95a854","venue_1":"ACM Trans. Graph.","year":"2012","title":"Fit and diverse: set evolution for inspiring 3D shape galleries","authors":"Kai Xu, Hao Zhang, Daniel Cohen-Or, Baoquan Chen","author_ids":"1723225, 1682058, 1701009, 1748939","abstract":"We introduce <i>set evolution</i> as a means for creative 3D shape modeling, where an initial population of 3D models is evolved to produce generations of novel shapes. Part of the evolving set is presented to a user as a shape gallery to offer modeling suggestions. User preferences define the fitness for the evolution so that over time, the shape population will mainly consist of individuals with good fitness. However, to inspire the user's creativity, we must also keep the evolving set diverse. Hence the evolution is \"<i>fit and diverse</i>\", drawing motivation from evolution theory. We introduce a novel part crossover operator which works at the finer-level part structures of the shapes, leading to significant variations and thus increased diversity in the evolved shape structures. Diversity is also achieved by explicitly compromising the fitness scores on a portion of the evolving population. We demonstrate the effectiveness of set evolution on man-made shapes. We show that selecting only models with high fitness leads to an elite population with low diversity. By keeping the population fit and diverse, the evolution can generate inspiring, and sometimes unexpected, shapes.","cites":"56","conferencePercentile":"93.18181818"},{"venue":"ACM Trans. Graph.","id":"172c2658aed32f13cffeec3ffacbb68368797ddb","venue_1":"ACM Trans. Graph.","year":"2012","title":"Field-guided registration for feature-conforming shape composition","authors":"Hui Huang, Minglun Gong, Daniel Cohen-Or, Yaobin Ouyang, Fuwen Tan, Hao Zhang","author_ids":"1927737, 2834751, 1701009, 2877945, 2218741, 1682058","abstract":"We present an automatic shape composition method to fuse two shape parts which may not overlap and possibly contain sharp features, a scenario often encountered when modeling man-made objects. At the core of our method is a novel <i>field-guided</i> approach to automatically align two input parts in a <i>feature-conforming</i> manner. The key to our field-guided shape registration is a <i>natural continuation</i> of one part into the ambient field as a means to introduce an overlap with the distant part, which then allows a surface-to-field registration. The ambient vector field we compute is feature-conforming; it characterizes a piecewise smooth field which respects and naturally extrapolates the surface features. Once the two parts are aligned, gap filling is carried out by spline interpolation between matching feature curves followed by piecewise smooth least-squares surface reconstruction. We apply our algorithm to obtain feature-conforming shape composition on a variety of models and demonstrate generality of the method with results on parts with or without overlap and with or without salient features.","cites":"4","conferencePercentile":"3.535353535"},{"venue":"ACM Trans. Graph.","id":"3260ea9154952860749b25285d44a521f8d886a2","venue_1":"ACM Trans. Graph.","year":"2011","title":"Photo-inspired model-driven 3D object modeling","authors":"Kai Xu, Hanlin Zheng, Hao Zhang, Daniel Cohen-Or, Ligang Liu, Yueshan Xiong","author_ids":"1723225, 1898386, 1682058, 1701009, 1724542, 2992810","abstract":"We introduce an algorithm for 3D object modeling where the user draws creative inspiration from an object captured in a single photograph. Our method leverages the rich source of photographs for creative 3D modeling. However, with only a photo as a guide, creating a 3D model from scratch is a daunting task. We support the modeling process by utilizing an available set of 3D candidate models. Specifically, the user creates a digital 3D model as a geometric variation from a 3D candidate. Our modeling technique consists of two major steps. The first step is a user-guided image-space object segmentation to reveal the structure of the photographed object. The core step is the second one, in which a 3D candidate is automatically deformed to fit the photographed target under the guidance of silhouette correspondence. The set of candidate models have been pre-analyzed to possess useful high-level structural information, which is heavily utilized in both steps to compensate for the ill-posedness of the analysis and modeling problems based only on content in a single image. Equally important, the structural information is preserved by the geometric variation so that the final product is coherent with its inherited structural information readily usable for subsequent model refinement or processing.","cites":"37","conferencePercentile":"76.31578947"},{"venue":"ACM Trans. Graph.","id":"d11a25a68e5e98631aeb67a895e366e53b5d50fb","venue_1":"ACM Trans. Graph.","year":"2011","title":"Unsupervised co-segmentation of a set of shapes via descriptor-space spectral clustering","authors":"Oana Sidi, Oliver van Kaick, Yanir Kleiman, Hao Zhang, Daniel Cohen-Or","author_ids":"3215309, 3276873, 3119575, 1682058, 1701009","abstract":"We introduce an algorithm for unsupervised co-segmentation of a set of shapes so as to reveal the semantic shape parts and establish their correspondence across the set. The input set may exhibit significant shape variability where the shapes do not admit proper spatial alignment and the corresponding parts in any pair of shapes may be geometrically dissimilar. Our algorithm can handle such challenging input sets since, first, we perform co-analysis in a <i>descriptor space</i>, where a combination of shape descriptors relates the parts independently of their pose, location, and cardinality. Secondly, we exploit a key enabling feature of the input set, namely, dissimilar parts may be \"linked\" through third-parties present in the set. The links are derived from the pairwise similarities between the parts' descriptors. To reveal such linkages, which may manifest themselves as anisotropic and non-linear structures in the descriptor space, we perform spectral clustering with the aid of diffusion maps. We show that with our approach, we are able to co-segment sets of shapes that possess significant variability, achieving results that are close to those of a supervised approach.","cites":"65","conferencePercentile":"89.47368421"},{"venue":"ACM Trans. Graph.","id":"1761b92d3c00f21596e406dbea1a35f4f30772b3","venue_1":"ACM Trans. Graph.","year":"2010","title":"Spatial relationship preserving character motion adaptation","authors":"Edmond S. L. Ho, Taku Komura, Chiew-Lan Tai","author_ids":"2882496, 2254293, 1702674","abstract":"This paper presents a new method for editing and retargeting motions that involve close interactions between body parts of single or multiple articulated characters, such as dancing, wrestling, and sword fighting, or between characters and a restricted environment, such as getting into a car. In such motions, the implicit spatial relationships between body parts/objects are important for capturing the scene semantics. We introduce a simple structure called an interaction mesh to represent such spatial relationships. By minimizing the local deformation of the interaction meshes of animation frames, such relationships are preserved during motion editing while reducing the number of inappropriate interpenetrations. The interaction mesh representation is general and applicable to various kinds of close interactions. It also works well for interactions involving contacts and tangles as well as those without any contacts. The method is computationally efficient, allowing real-time character control. We demonstrate its effectiveness and versatility in synthesizing a wide variety of motions with close interactions.","cites":"44","conferencePercentile":"77.77777778"},{"venue":"ACM Trans. Graph.","id":"2dc4b6f50e054611165052f7a88d653a7ce09d58","venue_1":"ACM Trans. Graph.","year":"2006","title":"Real-time video abstraction","authors":"Holger Winnemöller, Sven C. Olsen, Bruce Gooch","author_ids":"2168852, 2737807, 2963263","abstract":"We present an automatic, real-time video and image abstraction framework that abstracts imagery by modifying the contrast of visually important features, namely luminance and color opponency. We reduce contrast in low-contrast regions using an approximation to anisotropic diffusion, and artificially increase contrast in higher contrast regions with difference-of-Gaussian edges. The abstraction step is extensible and allows for artistic or data-driven control. Abstracted images can optionally be stylized using soft color quantization to create cartoon-like effects with good temporal coherence. Our framework design is highly parallel, allowing for a GPU-based, real-time implementation. We evaluate the effectiveness of our abstraction framework with a user-study and find that participants are faster at naming abstracted faces of known persons compared to photographs. Participants are also better at remembering abstracted images of arbitrary scenes in a memory task.","cites":"153","conferencePercentile":"90.74074074"},{"venue":"ACM Trans. Graph.","id":"11e81a235c34ddaac8c37bf3cdca12c1c95df1f7","venue_1":"ACM Trans. Graph.","year":"2008","title":"Optimized scale-and-stretch for image resizing","authors":"Yu-Shuen Wang, Chiew-Lan Tai, Olga Sorkine-Hornung, Tong-Yee Lee","author_ids":"3133234, 1702674, 2250001, 1724980","abstract":"We present a \"scale-and-stretch\" warping method that allows resizing images into arbitrary aspect ratios while preserving visually prominent features. The method operates by iteratively computing optimal local scaling factors for each local region and updating a warped image that matches these scaling factors as closely as possible. The amount of deformation of the image content is guided by a significance map that characterizes the visual attractiveness of each pixel; this significance map is computed automatically using a novel combination of gradient and salience-based measures. Our technique allows diverting the distortion due to resizing to image regions with homogeneous content, such that the impact on perceptually important features is minimized. Unlike previous approaches, our method distributes the distortion in all spatial directions, even when the resizing operation is only applied horizontally or vertically, thus fully utilizing the available homogeneous regions to absorb the distortion. We develop an efficient formulation for the nonlinear optimization involved in the warping function computation, allowing interactive image resizing.","cites":"204","conferencePercentile":"96.91358025"},{"venue":"ACM Trans. Graph.","id":"6b3dacecf4b119b66df47f5582ddefacfb7eaabf","venue_1":"ACM Trans. Graph.","year":"2014","title":"Online motion synthesis using sequential Monte Carlo","authors":"Perttu Hämäläinen, Sebastian Eriksson, Esa Tanskanen, Ville Kyrki, Jaakko Lehtinen","author_ids":"1777495, 2327354, 2311678, 2717428, 1780788","abstract":"We present a Model-Predictive Control (MPC) system for online synthesis of interactive and physically valid character motion. Our system enables a complex (36-DOF) 3D human character model to balance in a given pose, dodge projectiles, and improvise a get up strategy if forced to lose balance, all in a dynamic and unpredictable environment. Such contact-rich, predictive and reactive motions have previously only been generated offline or using a handcrafted state machine or a dataset of reference motions, which our system does not require.\n For each animation frame, our system generates trajectories of character control parameters for the near future --- a few seconds --- using Sequential Monte Carlo sampling. Our main technical contribution is a multimodal, tree-based sampler that simultaneously explores multiple different near-term control strategies represented as parameter splines. The strategies represented by each sample are evaluated in parallel using a causal physics engine. The best strategy, as determined by an objective function measuring goal achievement, fluidity of motion, etc., is used as the control signal for the current frame, but maintaining multiple hypotheses is crucial for adapting to dynamically changing environments.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"9aa2abf7829de849628ffaad322153794ac99c3f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Edge-aware point set resampling","authors":"Hui Huang, Shihao Wu, Minglun Gong, Daniel Cohen-Or, Uri M. Ascher, Hao Zhang","author_ids":"1927737, 8175431, 2834751, 1701009, 1966230, 1682058","abstract":"Points acquired by laser scanners are not intrinsically equipped with normals, which are essential to surface reconstruction and point set rendering using surfels. Normal estimation is notoriously sensitive to noise. Near sharp features, the computation of noise-free normals becomes even more challenging due to the inherent undersampling problem at edge singularities. As a result, common edge-aware consolidation techniques such as bilateral smoothing may still produce erroneous normals near the edges. We propose a resampling approach to process a noisy and possibly outlier-ridden point set in an edge-aware manner. Our key idea is to first resample away from the edges so that reliable normals can be computed at the samples, and then based on reliable data, we progressively resample the point set while approaching the edge singularities. We demonstrate that our Edge-Aware Resampling (EAR) algorithm is capable of producing consolidated point sets with noise-free normals and clean preservation of sharp features. We also show that EAR leads to improved performance of edge-aware reconstruction methods and point set rendering techniques.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"cb33ef9a2c7520d27c307ec4a8ea6e51146d2cd0","venue_1":"ACM Trans. Graph.","year":"2009","title":"Dynamic shape capture using multi-view photometric stereo","authors":"Daniel Vlasic, Pieter Peers, Ilya Baran, Paul E. Debevec, Jovan Popovic, Szymon Rusinkiewicz, Wojciech Matusik","author_ids":"1880628, 1808270, 1789898, 1778676, 1731389, 7723706, 1752521","abstract":"We describe a system for high-resolution capture of moving 3D geometry, beginning with dynamic normal maps from multiple views. The normal maps are captured using active shape-from-shading (photometric stereo), with a large lighting dome providing a series of novel spherical lighting configurations. To compensate for low-frequency deformation, we perform multi-view matching and thin-plate spline deformation on the initial surfaces obtained by integrating the normal maps. Next, the corrected meshes are merged into a single mesh using a volumetric method. The final output is a set of meshes, which were impossible to produce with previous methods. The meshes exhibit details on the order of a few millimeters, and represent the performance over human-size working volumes at a temporal resolution of 60Hz.","cites":"90","conferencePercentile":"92.26519337"},{"venue":"ACM Trans. Graph.","id":"09fa5031bbe24e8d0c00f3560d126e5ce222cf00","venue_1":"ACM Trans. Graph.","year":"2009","title":"Fabricating microgeometry for custom surface reflectance","authors":"Tim Weyrich, Pieter Peers, Wojciech Matusik, Szymon Rusinkiewicz","author_ids":"1784306, 1808270, 1752521, 7723706","abstract":"We propose a system for manufacturing physical surfaces that, in aggregate, exhibit a desired surface appearance. Our system begins with a user specification of a BRDF, or simply a highlight shape, and infers the required distribution of surface slopes. We sample this distribution, optimize for a maximally-continuous and valley-minimizing height field, and finally mill the surface using a computer-controlled machine tool. We demonstrate a variety of surfaces, ranging from reproductions of measured BRDFs to materials with unconventional highlights.","cites":"58","conferencePercentile":"77.62430939"},{"venue":"ACM Trans. Graph.","id":"05c8d4f9dff10acd1084617957e4a63c720075fa","venue_1":"ACM Trans. Graph.","year":"2008","title":"Approximating Catmull-Clark subdivision surfaces with bicubic patches","authors":"Charles T. Loop, Scott Schaefer","author_ids":"1839367, 1687097","abstract":"We present a simple and computationally efficient algorithm for approximating Catmull-Clark subdivision surfaces using a minimal set of bicubic patches. For each quadrilateral face of the control mesh, we construct a geometry patch and a pair of tangent patches. The geometry patches approximate the shape and silhouette of the Catmull-Clark surface and are smooth everywhere except along patch edges containing an extraordinary vertex where the patches are <i>C</i><sup>0</sup>. To make the patch surface appear smooth, we provide a pair of tangent patches that approximate the tangent fields of the Catmull-Clark surface. These tangent patches are used to construct a continuous normal field (through their cross-product) for shading and displacement mapping. Using this bifurcated representation, we are able to define an accurate proxy for Catmull-Clark surfaces that is efficient to evaluate on next-generation GPU architectures that expose a programmable tessellation unit.","cites":"65","conferencePercentile":"74.07407407"},{"venue":"ACM Trans. Graph.","id":"2644d5ab6c901d821c938e2bc57b4a62490fa9a0","venue_1":"ACM Trans. Graph.","year":"2007","title":"Practical motion capture in everyday surroundings","authors":"Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John Barnwell, Markus H. Gross, Wojciech Matusik, Jovan Popovic","author_ids":"1880628, 2998887, 2335502, 4867148, 1743207, 1752521, 1731389","abstract":"Commercial motion-capture systems produce excellent in-studio reconstructions, but offer no comparable solution for acquisition in everyday environments. We present a system for acquiring motions almost anywhere. This wearable system gathers ultrasonic time-of-flight and inertial measurements with a set of inexpensive miniature sensors worn on the garment. After recording, the information is combined using an Extended Kalman Filter to reconstruct joint configurations of a body. Experimental results show that even motions that are traditionally difficult to acquire are recorded with ease within their natural settings. Although our prototype does not reliably recover the global transformation, we show that the resulting motions are visually similar to the original ones, and that the combined acoustic and intertial system reduces the drift commonly observed in purely inertial systems. Our final results suggest that this system could become a versatile input device for a variety of augmented-reality applications.","cites":"85","conferencePercentile":"73.6"},{"venue":"ACM Trans. Graph.","id":"8a398d51d5db08395888f55c5a51db8e26f0a5fe","venue_1":"ACM Trans. Graph.","year":"2007","title":"Multi-scale capture of facial geometry and motion","authors":"Bernd Bickel, Mario Botsch, Roland Angst, Wojciech Matusik, Miguel A. Otaduy, Hanspeter Pfister, Markus H. Gross","author_ids":"3083909, 1716234, 2746423, 1752521, 1704342, 1701371, 1743207","abstract":"We present a novel multi-scale representation and acquisition method for the animation of high-resolution facial geometry and wrinkles. We first acquire a static scan of the face including reflectance data at the highest possible quality. We then augment a traditional marker-based facial motion-capture system by two synchronized video cameras to track expression wrinkles. The resulting model consists of high-resolution geometry, motion-capture data, and expression wrinkles in 2D parametric form. This combination represents the facial shape and its salient features at multiple scales. During motion synthesis the motion-capture data deforms the high-resolution geometry using a linear shell-based mesh-deformation method. The wrinkle geometry is added to the facial base mesh using nonlinear energy optimization. We present the results of our approach for performance replay as well as for wrinkle editing.","cites":"88","conferencePercentile":"76.4"},{"venue":"ACM Trans. Graph.","id":"bf61b2c1b96c41f9dad669b6f155b88f05c3b696","venue_1":"ACM Trans. Graph.","year":"2012","title":"Coupled 3D reconstruction of sparse facial hair and skin","authors":"Thabo Beeler, Bernd Bickel, Gioacchino Noris, Paul A. Beardsley, Steve Marschner, Robert W. Sumner, Markus H. Gross","author_ids":"2486770, 3083909, 3206366, 1777539, 2593798, 1693475, 1743207","abstract":"Although facial hair plays an important role in individual expression, facial-hair reconstruction is not addressed by current face-capture systems. Our research addresses this limitation with an algorithm that treats hair and skin surface capture together in a coupled fashion so that a high-quality representation of hair fibers as well as the underlying skin surface can be reconstructed. We propose a passive, camera-based system that is robust against arbitrary motion since all data is acquired within the time period of a single exposure. Our reconstruction algorithm detects and traces hairs in the captured images and reconstructs them in 3D using a multiview stereo approach. Our coupled skin-reconstruction algorithm uses information about the detected hairs to deliver a skin surface that lies underneath all hairs irrespective of occlusions. In dense regions like eyebrows, we employ a hair-synthesis method to create hair fibers that plausibly match the image data. We demonstrate our scanning system on a number of individuals and show that it can successfully reconstruct a variety of facial-hair styles together with the underlying skin surface.","cites":"15","conferencePercentile":"42.67676768"},{"venue":"ACM Trans. Graph.","id":"e0acd5f322837547d63f7ed65670e6854bd96a70","venue_1":"ACM Trans. Graph.","year":"2011","title":"High-quality passive facial performance capture using anchor frames","authors":"Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel, Paul A. Beardsley, Craig Gotsman, Robert W. Sumner, Markus H. Gross","author_ids":"2486770, 1757129, 1745149, 3083909, 1777539, 1724072, 1693475, 1743207","abstract":"We present a new technique for passive and markerless facial performance capture based on <i>anchor frames</i>. Our method starts with high resolution per-frame geometry acquisition using state-of-the-art stereo reconstruction, and proceeds to establish a single triangle mesh that is propagated through the entire performance. Leveraging the fact that facial performances often contain repetitive subsequences, we identify <i>anchor frames</i> as those which contain similar facial expressions to a manually chosen reference expression. Anchor frames are automatically computed over one or even multiple performances. We introduce a robust image-space tracking method that computes pixel matches directly from the reference frame to all anchor frames, and thereby to the remaining frames in the sequence via sequential matching. This allows us to propagate one reconstructed frame to an entire sequence in parallel, in contrast to previous sequential methods. Our anchored reconstruction approach also limits tracker drift and robustly handles occlusions and motion blur. The parallel tracking and mesh propagation offer low computation times. Our technique will even automatically match anchor frames across different sequences captured on different occasions, propagating a single mesh to all performances.","cites":"94","conferencePercentile":"95.78947368"},{"venue":"ACM Trans. Graph.","id":"c2ebef19f3dc1dd01890e5698248c1e93e05dba8","venue_1":"ACM Trans. Graph.","year":"2010","title":"High-quality single-shot capture of facial geometry","authors":"Thabo Beeler, Bernd Bickel, Paul A. Beardsley, Bob Sumner, Markus H. Gross","author_ids":"2486770, 3083909, 1777539, 2298061, 1743207","abstract":"This paper describes a passive stereo system for capturing the 3D geometry of a face in a single-shot under standard light sources. The system is low-cost and easy to deploy. Results are submillimeter accurate and commensurate with those from state-of-the-art systems based on active lighting, and the models meet the quality requirements of a demanding domain like the movie industry. Recovered models are shown for captures from both high-end cameras in a studio setting and from a consumer binocular-stereo camera, demonstrating scalability across a spectrum of camera deployments, and showing the potential for 3D face modeling to move beyond the professional arena and into the emerging consumer market in stereoscopic photography.\n Our primary technical contribution is a modification of standard stereo refinement methods to capture pore-scale geometry, using a qualitative approach that produces visually realistic results. The second technical contribution is a calibration method suited to face capture systems. The systemic contribution includes multiple demonstrations of system robustness and quality. These include capture in a studio setup, capture off a consumer binocular-stereo camera, scanning of faces of varying gender and ethnicity and age, capture of highly-transient facial expression, and scanning a physical mask to provide ground-truth validation.","cites":"105","conferencePercentile":"97.66081871"},{"venue":"ACM Trans. Graph.","id":"2c2f96b530f644f7cd62b8dc83fad44f30b7b34c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Intrinsic colorization","authors":"Xiaopei Liu, Liang Wan, Yingge Qu, Tien-Tsin Wong, Stephen Lin, Andrew Chi-Sing Leung, Pheng-Ann Heng","author_ids":"3260708, 7533280, 2549934, 1720633, 1686911, 1734177, 1714602","abstract":"In this paper, we present an example-based colorization technique robust to illumination differences between grayscale target and color reference images. To achieve this goal, our method performs color transfer in an illumination-independent domain that is relatively free of shadows and highlights. It first recovers an illumination-independent <i>intrinsic reflectance image</i> of the target scene from multiple color references obtained by web search. The reference images from the web search may be taken from different vantage points, under different illumination conditions, and with different cameras. Grayscale versions of these reference images are then used in decomposing the grayscale target image into its intrinsic reflectance and illumination components. We transfer color from the color reflectance image to the grayscale reflectance image, and obtain the final result by relighting with the illumination component of the target image. We demonstrate via several examples that our method generates results with excellent color consistency.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"5cd2be1bfd8a1b0bb6071a9b7519dfb19b0fad02","venue_1":"ACM Trans. Graph.","year":"2008","title":"Animating animal motion from still","authors":"Xuemiao Xu, Liang Wan, Xiaopei Liu, Tien-Tsin Wong, Liansheng Wang, Andrew Chi-Sing Leung","author_ids":"2947755, 7533280, 3260708, 1720633, 5323201, 1734177","abstract":"Even though the temporal information is lost, a still picture of moving animals hints at their motion. In this paper, we infer motion cycle of animals from the \"motion snapshots\" (snapshots of different individuals) captured in a still picture. By finding the motion path in the graph connecting motion snapshots, we can infer the order of motion snapshots with respect to time, and hence the motion cycle. Both \"half-cycle\" and \"full-cycle\" motions can be inferred in a unified manner. Therefore, we can animate a still picture of a moving animal group by morphing among the ordered snapshots. By refining the pose, morphology, and appearance consistencies, smooth and realistic animal motion can be synthesized. Our results demonstrate the applicability of the proposed method to a wide range of species, including birds, fishes, mammals, and reptiles.","cites":"16","conferencePercentile":"12.65432099"},{"venue":"ACM Trans. Graph.","id":"7ded7befc1c517404d581c9ca02bee763de73b9a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Wave-ray coupling for interactive sound propagation in large complex scenes","authors":"Hengchin Yeh, Ravish Mehra, Zhimin Ren, Lakulish Antani, Dinesh Manocha, Ming C. Lin","author_ids":"3352671, 2040242, 2961841, 2747880, 1699159, 1709625","abstract":"We present a novel hybrid approach that couples geometric and numerical acoustic techniques for interactive sound propagation in complex environments. Our formulation is based on a combination of spatial and frequency decomposition of the sound field. We use numerical wave-based techniques to precompute the pressure field in the near-object regions and geometric propagation techniques in the far-field regions to model sound propagation. We present a novel two-way pressure coupling technique at the interface of near-object and far-field regions. At runtime, the impulse response at the listener position is computed at interactive rates based on the stored pressure field and interpolation techniques. Our system is able to simulate high-fidelity acoustic effects such as diffraction, scattering, low-pass filtering behind obstruction, reverberation, and high-order reflections in large, complex indoor and outdoor environments and Half-Life 2 game engine. The pressure computation requires orders of magnitude lower memory than standard wave-based numerical techniques.","cites":"8","conferencePercentile":"25.56561086"},{"venue":"ACM Trans. Graph.","id":"677e3b162c15ab508f22527d15aa9f66f8f752da","venue_1":"ACM Trans. Graph.","year":"2013","title":"Example-guided physically based modal sound synthesis","authors":"Zhimin Ren, Hengchin Yeh, Ming C. Lin","author_ids":"2961841, 3352671, 1709625","abstract":"Linear modal synthesis methods have often been used to generate sounds for rigid bodies. One of the key challenges in widely adopting such techniques is the lack of automatic determination of satisfactory material parameters that recreate realistic audio quality of sounding materials. We introduce a novel method using prerecorded audio clips to estimate material parameters that capture the inherent quality of recorded sounding materials. Our method extracts perceptually salient features from audio examples. Based on psychoacoustic principles, we design a parameter estimation algorithm using an optimization framework and these salient features to guide the search of the best material parameters for modal synthesis. We also present a method that compensates for the differences between the real-world recording and sound synthesized using solely linear modal synthesis models to create the final synthesized audio. The resulting audio generated from this sound synthesis pipeline well preserves the same sense of material as a recorded audio example. Moreover, both the estimated material parameters and the residual compensation naturally transfer to virtual objects of different sizes and shapes, while the synthesized sounds vary accordingly. A perceptual study shows the results of this system compare well with real-world recordings in terms of material perception.","cites":"12","conferencePercentile":"45.47511312"},{"venue":"ACM Trans. Graph.","id":"747fda73716bbca512d28e6d6b0d3d70f4f92664","venue_1":"ACM Trans. Graph.","year":"2007","title":"Factored time-lapse video","authors":"Kalyan Sunkavalli, Wojciech Matusik, Hanspeter Pfister, Szymon Rusinkiewicz","author_ids":"2454127, 1752521, 1701371, 7723706","abstract":"We describe a method for converting time-lapse photography captured with outdoor cameras into <i>Factored Time-Lapse Video</i> (FTLV): a video in which time appears to move faster (i.e., lapsing) and where data at each pixel has been factored into shadow, illumination, and reflectance components. The factorization allows a user to easily relight the scene, recover a portion of the scene geometry (normals), and to perform advanced image editing operations. Our method is easy to implement, robust, and provides a compact representation with good reconstruction characteristics. We show results using several publicly available time-lapse sequences.","cites":"46","conferencePercentile":"45.2"},{"venue":"ACM Trans. Graph.","id":"5e1d57ae4b8af32a9eefb77a42e568dfc63ef663","venue_1":"ACM Trans. Graph.","year":"2002","title":"Physiological measures of presence in stressful virtual environments","authors":"Michael Meehan, Brent Insko, Mary C. Whitton, Frederick P. Brooks","author_ids":"2000470, 1837250, 1761150, 1795780","abstract":"A common measure of the quality or effectiveness of a virtual environment (VE) is the mount of <i>presence</i> it evokes in users. Presence is often defined as the sense of <i>being there</i> in a VE. There has been much debate about the best way to measure presence, and presence researchers need, and have sought, a measure that is <b>reliable, valid, sensitive, and objective.</b>We hypothesized that to the degree that a VE seems real, it would evoke physiological responses similar to those evoked by the corresponding real environment, and that greater presence would evoke a greater response. To examine this, we conducted three experiments, the results of which support the use of physiological reaction as a reliable, valid, sensitive, and objective presence measure. The experiments compared participants' physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual height situation. We found that change in heart rate satisfied our requirements for a measure of presence, change in skin conductance did to a lesser extent, and that change in skin temperature did not. Moreover, the results showed that inclusion of a passive haptic element in the VE significantly increased presence and that for presence evoked: 30FPS &gt; 20FPS &gt; 15FPS.","cites":"130","conferencePercentile":"56"},{"venue":"ACM Trans. Graph.","id":"29e087cd0ab8e43853f1daa81247a798e7140fe0","venue_1":"ACM Trans. Graph.","year":"2011","title":"Characterizing structural relationships in scenes using graph kernels","authors":"Matthew Fisher, Manolis Savva, Pat Hanrahan","author_ids":"2676553, 2531433, 4982303","abstract":"Modeling virtual environments is a time consuming and expensive task that is becoming increasingly popular for both professional and casual artists. The model density and complexity of the scenes representing these virtual environments is rising rapidly. This trend suggests that data-mining a 3D scene corpus could be a very powerful tool enabling more efficient scene design. In this paper, we show how to represent scenes as graphs that encode models and their semantic relationships. We then define a kernel between these relationship graphs that compares common virtual substructures in two graphs and captures the similarity between their corresponding scenes. We apply this framework to several scene modeling problems, such as finding similar scenes, relevance feedback, and context-based model search. We show that incorporating structural relationships allows our method to provide a more relevant set of results when compared against previous approaches to model context search.","cites":"66","conferencePercentile":"90.52631579"},{"venue":"ACM Trans. Graph.","id":"6a1ded20e33b799685c7e521ecd3222e23bfcc34","venue_1":"ACM Trans. Graph.","year":"2015","title":"Activity-centric scene synthesis for functional 3D scene modeling","authors":"Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan, Matthias Nießner","author_ids":"2676553, 2531433, 1920864, 4982303, 2209612","abstract":"We present a novel method to generate 3D scenes that allow the same activities as real environments captured through noisy and incomplete 3D scans. As robust object detection and instance retrieval from low-quality depth data is challenging, our algorithm aims to model semantically-correct rather than geometrically-accurate object arrangements. Our core contribution is a new scene synthesis technique which, conditioned on a coarse geometric scene representation, models functionally similar scenes using prior knowledge learned from a scene database. The key insight underlying our scene synthesis approach is that many real-world environments are structured to facilitate specific human activities, such as sleeping or eating. We represent scene functionalities through virtual agents that associate object arrangements with the activities for which they are typically used. When modeling a scene, we first identify the activities supported by a scanned environment. We then determine semantically-plausible arrangements of virtual objects -- retrieved from a shape database -- constrained by the observed scene geometry. For a given 3D scan, our algorithm produces a variety of synthesized scenes which support the activities of the captured real environments. In a perceptual evaluation study, we demonstrate that our results are judged to be visually appealing and functionally comparable to manually designed scenes.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"3b5b7a92b4039783c098543a66b4b345d32f41e3","venue_1":"ACM Trans. Graph.","year":"2008","title":"Articulated mesh animation from multi-view silhouettes","authors":"Daniel Vlasic, Ilya Baran, Wojciech Matusik, Jovan Popovic","author_ids":"1880628, 1789898, 1752521, 1731389","abstract":"Details in mesh animations are difficult to generate but they have great impact on visual quality. In this work, we demonstrate a practical software system for capturing such details from multi-view video recordings. Given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer, our system captures the motion of both the skeleton and the shape. The output mesh animation is enhanced with the details observed in the image silhouettes. For example, a performance in casual loose-fitting clothes will generate mesh animations with flowing garment motions. We accomplish this with a fast pose tracking method followed by nonrigid deformation of the template to fit the silhouettes. The entire process takes less than sixteen seconds per frame and requires no markers or texture cues. Captured meshes are in full correspondence making them readily usable for editing operations including texturing, deformation transfer, and deformation model learning.","cites":"228","conferencePercentile":"98.14814815"},{"venue":"ACM Trans. Graph.","id":"42374503590b336d88ead84f3e345bdffa6712c3","venue_1":"ACM Trans. Graph.","year":"2016","title":"PiGraphs: learning interaction snapshots from observations","authors":"Manolis Savva, Angel X. Chang, Pat Hanrahan, Matthew Fisher, Matthias Nießner","author_ids":"2531433, 3317599, 4982303, 2676553, 2209612","abstract":"Computer graphics has made great progress in enabling people to create visual content. However, we still face a big content creation bottleneck. In particular, designing 3D scenes and virtual character interactions within them is still a time---consuming task requiring expertise and much manual effort. A common theme in addressing the content creation challenge in various subfields of graphics has been to leverage data in order to build statistical methods for automated content generation.","cites":"3","conferencePercentile":"91.7721519"},{"venue":"ACM Trans. Graph.","id":"b1ede039edb3d58755534e8f5278565d0fb0e557","venue_1":"ACM Trans. Graph.","year":"2008","title":"Bubbles alive","authors":"Jeong-Mo Hong, Ho-Young Lee, Jong-Chul Yoon, Chang-Hun Kim","author_ids":"2674978, 1717704, 3132648, 1921569","abstract":"We propose a hybrid method for simulating multiphase fluids such as bubbly water. The appearance of subgrid visual details is improved by incorporating a new bubble model based on smoothed particle hydrodynamics (SPH) into an Eulerian grid-based simulation that handles background flows of large bodies of water and air. To overcome the difficulty in simulating small bubbles in the context of the multiphase flows on a coarse grid, we heuristically model the interphase properties of water and air by means of the interactions between bubble particles. As a result, we can animate lively motion of bubbly water with small scale details efficiently.","cites":"25","conferencePercentile":"26.2345679"},{"venue":"ACM Trans. Graph.","id":"9f2be13f74ad153de8052864317e887faaba6989","venue_1":"ACM Trans. Graph.","year":"2005","title":"Resolution independent curve rendering using programmable graphics hardware","authors":"Charles T. Loop, James F. Blinn","author_ids":"1839367, 1685272","abstract":"We present a method for resolution independent rendering of paths and bounded regions, defined by quadratic and cubic spline curves, that leverages the parallelism of programmable graphics hardware to achieve high performance. A simple implicit equation for a parametric curve is found in a space that can be thought of as an analog to texture space. The image of a curve's B&#233;zier control points are found in this space and assigned to the control points as texture coordinates. When the triangle(s) corresponding to the B&#233;zier curve control hull are rendered, a pixel shader program evaluates the implicit equation for a pixel's interpolated texture coordinates to determine an inside/outside test for the curve. We extend our technique to handle anti-aliasing of boundaries. We also construct a vector image from mosaics of triangulated B&#233;zier control points and show how to deform such images to create resolution independent texture on three dimensional objects.","cites":"51","conferencePercentile":"28.22580645"},{"venue":"ACM Trans. Graph.","id":"154cc6c351ab61bbe98b6175813ea88f6e12a643","venue_1":"ACM Trans. Graph.","year":"2006","title":"Real-time GPU rendering of piecewise algebraic surfaces","authors":"Charles T. Loop, James F. Blinn","author_ids":"1839367, 1685272","abstract":"We consider the problem of real-time GPU rendering of algebraic surfaces defined by B&#233;zier tetrahedra. These surfaces are rendered directly in terms of their polynomial representations, as opposed to a collection of approximating triangles, thereby eliminating tessellation artifacts and reducing memory usage. A key step in such algorithms is the computation of univariate polynomial coefficients at each pixel; real roots of this polynomial correspond to possibly visible points on the surface. Our approach leverages the strengths of GPU computation and is highly efficient. Furthermore, we compute these coefficients in Bernstein form to maximize the stability of root finding, and to provide shader instances with an early exit test based on the sign of these coefficients. Solving for roots is done using analytic techniques that map well to a SIMD architecture, but limits us to fourth order algebraic surfaces. The general framework could be extended to higher order with numerical root finding.","cites":"69","conferencePercentile":"53.7037037"},{"venue":"ACM Trans. Graph.","id":"31c03b35c72a04e27dbf649ecb92bb008e3899e9","venue_1":"ACM Trans. Graph.","year":"2008","title":"Hair photobooth: geometric and photometric acquisition of real hairstyles","authors":"Sylvain Paris, Will Chang, Oleg I. Kozhushnyan, Wojciech Jarosz, Wojciech Matusik, Matthias Zwicker, Frédo Durand","author_ids":"1720990, 1730495, 3215712, 1953515, 1752521, 1796846, 1728125","abstract":"We accurately capture the shape and appearance of a person's hairstyle. We use triangulation and a sweep with planes of light for the geometry. Multiple projectors and cameras address the challenges raised by the reflectance and intricate geometry of hair. We introduce the use of structure tensors to infer the hidden geometry between the hair surface and the scalp. Our triangulation approach affords substantial accuracy improvement and we are able to measure elaborate hair geometry including complex curls and concavities. To reproduce the hair appearance, we capture a six-dimensional reflectance field. We introduce a new reflectance interpolation technique that leverages an analytical reflectance model to alleviate cross-fading artifacts caused by linear methods. Our results closely match the real hairstyles and can be used for animation.","cites":"52","conferencePercentile":"64.19753086"},{"venue":"ACM Trans. Graph.","id":"7f20af11ebd0423f63532ddf04bad458609c3879","venue_1":"ACM Trans. Graph.","year":"2014","title":"Rendering volumetric haptic shapes in mid-air using ultrasound","authors":"Benjamin Long, Sue Ann Seah, Tom Carter, Sriram Subramanian","author_ids":"2826191, 8546038, 2484657, 1702794","abstract":"We present a method for creating three-dimensional haptic shapes in mid-air using focused ultrasound. This approach applies the principles of <i>acoustic radiation force</i>, whereby the non-linear effects of sound produce forces on the skin which are strong enough to generate tactile sensations. This mid-air haptic feedback eliminates the need for any attachment of actuators or contact with physical devices. The user perceives a discernible haptic shape when the corresponding acoustic interference pattern is generated above a precisely controlled two-dimensional phased array of ultrasound transducers. In this paper, we outline our algorithm for controlling the volumetric distribution of the acoustic radiation force field in the form of a three-dimensional shape. We demonstrate how we create this acoustic radiation force field and how we interact with it. We then describe our implementation of the system and provide evidence from both visual and technical evaluations of its ability to render different shapes. We conclude with a subjective user evaluation to examine users' performance for different shapes.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"efa775a35670f53bfeb023576973b53570b125bb","venue_1":"ACM Trans. Graph.","year":"2016","title":"Real-time skeletal skinning with optimized centers of rotation","authors":"Binh Huy Le, Jessica K. Hodgins","author_ids":"2664191, 1788773","abstract":"Skinning algorithms that work across a broad range of character designs and poses are crucial to creating compelling animations. Currently, linear blend skinning (LBS) and dual quaternion skinning (DQS) are the most widely used, especially for real-time applications. Both techniques are efficient to compute and are effective for many purposes. However, they also have many well-known artifacts, such as collapsing elbows, candy wrapper twists, and bulging around the joints. Due to the popularity of LBS and DQS, it would be of great benefit to reduce these artifacts without changing the animation pipeline or increasing the computational cost significantly. In this paper, we introduce a new direct skinning method that addresses this problem. Our key idea is to pre-compute the optimized center of rotation for each vertex from the rest pose and skinning weights. At runtime, these centers of rotation are used to interpolate the rigid transformation for each vertex. Compared to other direct skinning methods, our method significantly reduces the artifacts of LBS and DQS while maintaining real-time performance and backwards compatibility with the animation pipeline.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"17a121789c205dbc4ee4b6bba7d7f51e3cfeae88","venue_1":"ACM Trans. Graph.","year":"2010","title":"Data-driven suggestions for creativity support in 3D modeling","authors":"Siddhartha Chaudhuri, Vladlen Koltun","author_ids":"7218171, 1770944","abstract":"We introduce data-driven suggestions for 3D modeling. Data-driven suggestions support open-ended stages in the 3D modeling process, when the appearance of the desired model is ill-defined and the artist can benefit from customized examples that stimulate creativity. Our approach computes and presents components that can be added to the artist's current shape. We describe shape retrieval and shape correspondence techniques that support the generation of data-driven suggestions, and report preliminary experiments with a tool for creative prototyping of 3D models.","cites":"45","conferencePercentile":"79.53216374"},{"venue":"ACM Trans. Graph.","id":"6723dde31ddd81671d8bd7a9fb309acf0a8f6a0b","venue_1":"ACM Trans. Graph.","year":"2004","title":"Video matching","authors":"Peter Sand, Seth J. Teller","author_ids":"2259071, 1720894","abstract":"This paper describes a method for bringing two videos (recorded at different times) into spatiotemporal alignment, then comparing and combining corresponding pixels for applications such as background subtraction, compositing, and increasing dynamic range. We align a pair of videos by searching for frames that best match according to a robust image registration process. This process uses locally weighted regression to interpolate and extrapolate high-likelihood image correspondences, allowing new correspondences to be discovered and refined. Image regions that cannot be matched are detected and ignored, providing robustness to changes in scene content and lighting, which allows a variety of new applications.","cites":"73","conferencePercentile":"35.86956522"},{"venue":"ACM Trans. Graph.","id":"214b0c078cd0958514b2150130a50b6241fe7b72","venue_1":"ACM Trans. Graph.","year":"1999","title":"Radiance interpolants for accelerated bounded-error ray tracing","authors":"Kavita Bala, Julie Dorsey, Seth J. Teller","author_ids":"8261370, 1775220, 1720894","abstract":"Ray tracers, which sample radiance, are usually regarded as offline rendering algorithms that are too slow for interactive use. In this article we present a system that exploits object-space, ray-space, image-space, and temporal coherence to accelerate ray tracing. Our system uses <italic>per-surface interpolants</italic> to approximate radiance both interactive and batch ray tracers.Our approach explicity decouples the two primary operations of a ray tracer&#8212;shading and visibility determination&#8212;and accelerates each of them independently. Shading is accelerated  by quadrilinearily interpolating lazily acquired radiance samples. Interpolation error does not exceed a user-specified bound, allowing the user to control performance/quality tradeoffs. Error is bounded by  adaptive sampling at discontinuities and radiance nonlinearities.Visibility determination at pixels is accelerated by <italic>reprojecting</italic> interpolants as the user's viewpoint changes. A fast scan-line alogoithm then achieves high performance without sacrificing image quality. For a smoothly varying viewpoint, the combination of lazy interpolants and projection substantially accelerates the ray tracer. Additionally, an efficient cache management algorithm keeps the memory footprint of the system small with negilgible overhead.","cites":"55","conferencePercentile":"85.71428571"},{"venue":"ACM Trans. Graph.","id":"02be27212e26e388fb602ee09f0b069e70e14660","venue_1":"ACM Trans. Graph.","year":"2011","title":"Structure-preserving retargeting of irregular 3D architecture","authors":"Jinjie Lin, Daniel Cohen-Or, Hao Zhang, Cheng Liang, Andrei Sharf, Oliver Deussen, Baoquan Chen","author_ids":"2817530, 1701009, 1682058, 2844690, 2120270, 1850438, 1748939","abstract":"We present an algorithm for interactive <i>structure-preserving</i> retargeting of <i>irregular</i> 3D architecture models, offering the modeler an easy-to-use tool to quickly generate a variety of 3D models that resemble an input piece in its structural style. Working on a more global and structural level of the input, our technique allows and even encourages replication of its structural elements, while taking into account their semantics and expected geometric interrelations such as alignments and adjacency. The algorithm performs automatic replication and scaling of these elements while preserving their structures. Instead of formulating and solving a complex constrained optimization, we decompose the input model into a set of sequences, each of which is a 1D structure that is relatively straightforward to retarget. As the sequences are retargeted in turn, they progressively constrain the retargeting of the remaining sequences. We demonstrate interactivity and variability of results from our retargeting algorithm using many examples modeled after real-world architectures exhibiting various forms of irregularity.","cites":"29","conferencePercentile":"63.15789474"},{"venue":"ACM Trans. Graph.","id":"7088462e6ecc99e2decc43c3b4d272cfb5f6983b","venue_1":"ACM Trans. Graph.","year":"1994","title":"Smart telepointers: maintaining telepointer consistency in the presence of user interface customization","authors":"Kenneth J. Rodham, Dan R. Olsen","author_ids":"2755607, 1733794","abstract":"Conventional methods for maintaining telepointer consistency in shared windows do not work in the presence of per-user window customizations. This article presents the notion of a &#8220;smart telepointer,&#8221; which is a telepointer that works correctly in spite of such customizations. Methods for smart-telepointer implementation are discussed.","cites":"11","conferencePercentile":"31.25"},{"venue":"ACM Trans. Graph.","id":"5fbe1c4f9ac275865c7b507ea130fb3090ed5bae","venue_1":"ACM Trans. Graph.","year":"2011","title":"Conjoining Gestalt rules for abstraction of architectural drawings","authors":"Liangliang Nan, Andrei Sharf, Ke Xie, Tien-Tsin Wong, Oliver Deussen, Daniel Cohen-Or, Baoquan Chen","author_ids":"2792746, 2120270, 1707301, 1720633, 1850438, 1701009, 1748939","abstract":"We present a method for structural summarization and abstraction of complex spatial arrangements found in architectural drawings. The method is based on the well-known Gestalt rules, which summarize how forms, patterns, and semantics are perceived by humans from bits and pieces of geometric information. Although defining a computational model for each rule alone has been extensively studied, modeling a conjoint of Gestalt rules remains a challenge. In this work, we develop a computational framework which models Gestalt rules and more importantly, their complex interactions. We apply <i>conjoining rules</i> to line drawings, to detect groups of objects and repetitions that conform to Gestalt principles. We summarize and abstract such groups in ways that maintain structural semantics by displaying only a reduced number of repeated elements, or by replacing them with simpler shapes. We show an application of our method to line drawings of architectural models of various styles, and the potential of extending the technique to other computer-generated illustrations, and three-dimensional models.","cites":"16","conferencePercentile":"37.63157895"},{"venue":"ACM Trans. Graph.","id":"215868783e880ffe7e69493c9a72d748af549800","venue_1":"ACM Trans. Graph.","year":"2010","title":"Analysis, reconstruction and manipulation using arterial snakes","authors":"Guo Li, Ligang Liu, Hanlin Zheng, Niloy J. Mitra","author_ids":"7137023, 1724542, 1898386, 1710455","abstract":"Man-made objects often consist of detailed and interleaving structures, which are created using cane, coils, metal wires, rods, etc. The delicate structures, although manufactured using simple procedures, are challenging to scan and reconstruct. We observe that such structures are inherently 1D, and hence are naturally represented using an arrangement of generating curves. We refer to the resultant surfaces as <i>arterial surfaces.</i> In this paper we approach for analyzing, reconstructing, and manipulating such arterial surfaces.\n The core of the algorithm is a novel deformable model, called <i>arterial snake</i>, that simultaneously captures the topology and geometry of the arterial objects. The recovered snakes produce a natural decomposition of the raw scans, with the decomposed parts often capturing meaningful object sections. We demonstrate the robustness of our algorithm on a variety of arterial objects corrupted with noise, outliers, and with large parts missing. We present a range of applications including reconstruction, topology repairing, and manipulation of arterial surfaces by directly controlling the underlying curve network and the associated sectional profiles, which are otherwise challenging to perform.","cites":"20","conferencePercentile":"33.62573099"},{"venue":"ACM Trans. Graph.","id":"1243360fc5592f35fc8b57086809932f9e6b0ce8","venue_1":"ACM Trans. Graph.","year":"2009","title":"Partial intrinsic reflectional symmetry of 3D shapes","authors":"Kai Xu, Hao Zhang, Andrea Tagliasacchi, Ligang Liu, Guo Li, Min Meng, Yueshan Xiong","author_ids":"1723225, 1682058, 1796480, 1724542, 7137023, 2738386, 2992810","abstract":"While many 3D objects exhibit various forms of global symmetries, prominent intrinsic symmetries which exist only on parts of an object are also well recognized. Such partial symmetries are often seen as more natural than a global one, even when the symmetric parts are under complex pose. We introduce an algorithm to extract <i>partial intrinsic reflectional symmetries</i> (PIRS) of a 3D shape. Given a closed 2-manifold mesh, we develop a voting scheme to obtain an intrinsic reflectional symmetry axis (IRSA) transform, which is a scalar field over the mesh that accentuates prominent IRSAs of the shape. We then extract a set of explicit IRSA curves on the shape based on a refined measure of local reflectional symmetry support along a curve. The iterative refinement procedure combines IRSA-induced region growing and region-constrained symmetry support refinement to improve accuracy and address potential issues arising from rotational symmetries in the shape. We show how the extracted IRSA curves can be incorporated into a conventional mesh segmentation scheme so that the implied symmetry cues can be utilized to obtain more meaningful results. We also demonstrate the use of IRSA curves for symmetry-driven part repair.","cites":"42","conferencePercentile":"62.70718232"},{"venue":"ACM Trans. Graph.","id":"01f8635d00f3a8d8a01ee852a3fce6daa88b61d6","venue_1":"ACM Trans. Graph.","year":"2010","title":"Style-content separation by anisotropic part scales","authors":"Kai Xu, Honghua Li, Hao Zhang, Daniel Cohen-Or, Yueshan Xiong, Zhi-Quan Cheng","author_ids":"1723225, 1829406, 1682058, 1701009, 2992810, 1729485","abstract":"We perform co-analysis of a set of man-made 3D objects to allow the creation of novel instances derived from the set. We analyze the objects at the part level and treat the <i>anisotropic part scales</i> as a shape style. The co-analysis then allows <i>style transfer</i> to synthesize new objects. The key to co-analysis is part correspondence, where a major challenge is the handling of large style variations and diverse geometric content in the shape set. We propose <i>style-content separation</i> as a means to address this challenge. Specifically, we define a <i>correspondence-free</i> style signature for style clustering. We show that confining analysis to within a style cluster facilitates tasks such as co-segmentation, content classification, and deformation-driven part correspondence. With part correspondence between each pair of shapes in the set, style transfer can be easily performed. We demonstrate our analysis and synthesis results on several sets of man-made objects with style and content variations.","cites":"47","conferencePercentile":"80.70175439"},{"venue":"ACM Trans. Graph.","id":"5fa49c3b4760892a7fe9abcb866f496af94397a8","venue_1":"ACM Trans. Graph.","year":"2010","title":"Cone carving for surface reconstruction","authors":"Shy Shalom, Ariel Shamir, Hao Zhang, Daniel Cohen-Or","author_ids":"2544662, 2947946, 1682058, 1701009","abstract":"We present cone carving, a novel space carving technique supporting topologically correct surface reconstruction from an incomplete scanned point cloud. The technique utilizes the point samples not only for local surface position estimation but also to obtain global visibility information under the assumption that each acquired point is visible from a point lying outside the shape. This enables associating each point with a generalized cone, called the <i>visibility cone</i>, that carves a portion of the outside ambient space of the shape from the inside out. These cones collectively provide a means to better approximate the signed distances to the shape specifically near regions containing large holes in the scan, allowing one to infer the correct surface topology. Combining the new distance measure with conventional RBF, we define an implicit function whose zero level set defines the surface of the shape. We demonstrate the utility of cone carving in coping with significant missing data and raw scans from a commercial 3D scanner as well as synthetic input.","cites":"21","conferencePercentile":"36.5497076"},{"venue":"ACM Trans. Graph.","id":"2f7df25933d3a92d49124703af4e3edec5732a65","venue_1":"ACM Trans. Graph.","year":"2007","title":"Multiscale shape and detail enhancement from multi-light image collections","authors":"Raanan Fattal, Maneesh Agrawala, Szymon Rusinkiewicz","author_ids":"3230440, 1820412, 7723706","abstract":"We present a new image-based technique for enhancing the shape and surface details of an object. The input to our system is a small set of photographs taken from a fixed viewpoint, but under varying lighting conditions. For each image we compute a multiscale decomposition based on the bilateral filter and then reconstruct an enhanced image that combines detail information at each scale across all the input images. Our approach does not require any information about light source positions, or camera calibration, and can produce good results with 3 to 5 input images. In addition our system provides a few high-level parameters for controlling the amount of enhancement and does not require pixel-level user input. We show that the bilateral filter is a good choice for our multiscale algorithm because it avoids the halo artifacts commonly associated with the traditional Laplacian image pyramid. We also develop a new scheme for computing our multiscale bilateral decomposition that is simple to implement, fast <i>O</i>(<i>N</i><sup>2</sup> log <i>N</i>) and accurate.","cites":"106","conferencePercentile":"80.8"},{"venue":"ACM Trans. Graph.","id":"1b5927621f708042b86eea40e71036279f4cf5e8","venue_1":"ACM Trans. Graph.","year":"2008","title":"Interactive 3D architectural modeling from unordered photo collections","authors":"Sudipta N. Sinha, Drew Steedly, Richard Szeliski, Maneesh Agrawala, Marc Pollefeys","author_ids":"1757937, 2833354, 1717841, 1820412, 1742208","abstract":"We present an interactive system for generating photorealistic, textured, piecewise-planar 3D models of architectural structures and urban scenes from unordered sets of photographs. To reconstruct 3D geometry in our system, the user draws outlines overlaid on 2D photographs. The 3D structure is then automatically computed by combining the 2D interaction with the multi-view geometric information recovered by performing structure from motion analysis on the input photographs. We utilize vanishing point constraints at multiple stages during the reconstruction, which is particularly useful for architectural scenes where parallel lines are abundant. Our approach enables us to accurately model polygonal faces from 2D interactions in a single image. Our system also supports useful operations such as edge snapping and extrusions.\n Seamless texture maps are automatically generated by combining multiple input photographs using graph cut optimization and Poisson blending. The user can add brush strokes as hints during the texture generation stage to remove artifacts caused by unmodeled geometric structures. We build models for a variety of architectural scenes from collections of up to about a hundred photographs.","cites":"102","conferencePercentile":"91.97530864"},{"venue":"ACM Trans. Graph.","id":"29be7afe3b59e454439a5b9576f7b8a35a0ca3ef","venue_1":"ACM Trans. Graph.","year":"2008","title":"Automatic generation of tourist maps","authors":"Floraine Grabler, Maneesh Agrawala, Robert W. Sumner, Mark Pauly","author_ids":"2224711, 1820412, 1693475, 1741645","abstract":"Tourist maps are essential resources for visitors to an unfamiliar city because they visually highlight landmarks and other points of interest. Yet, hand-designed maps are static representations that cannot adapt to the needs and tastes of the individual tourist. In this paper we present an automated system for designing tourist maps that selects and highlights the information that is most important to tourists. Our system determines the salience of map elements using bottom-up vision-based image analysis and top-down web-based information extraction techniques. It then generates a map that emphasizes the most important elements, using a combination of multiperspective rendering to increase visibility of streets and landmarks, and cartographic generalization techniques such as simplification, deformation, and displacement to emphasize landmarks and de-emphasize less important buildings. We show a number of automatically generated tourist maps of San Francisco and compare them to existing automated and manual approaches.","cites":"37","conferencePercentile":"42.90123457"},{"venue":"ACM Trans. Graph.","id":"146f970e65615b0df366d0a830ef4a977b94c337","venue_1":"ACM Trans. Graph.","year":"2010","title":"Design and fabrication of materials with desired deformation behavior","authors":"Bernd Bickel, Moritz Bächer, Miguel A. Otaduy, Hyunho Richard Lee, Hanspeter Pfister, Markus H. Gross, Wojciech Matusik","author_ids":"3083909, 8021864, 1704342, 2904356, 1701371, 1743207, 1752521","abstract":"This paper introduces a data-driven process for designing and fabricating materials with desired deformation behavior. Our process starts with measuring deformation properties of base materials. For each base material we acquire a set of example deformations, and we represent the material as a non-linear stress-strain relationship in a finite-element model. We have validated our material measurement process by comparing simulations of arbitrary stacks of base materials with measured deformations of fabricated material stacks. After material measurement, our process continues with designing stacked layers of base materials. We introduce an optimization process that finds the best combination of stacked layers that meets a user's criteria specified by example deformations. Our algorithm employs a number of strategies to prune poor solutions from the combinatorial search space. We demonstrate the complete process by designing and fabricating objects with complex heterogeneous materials using modern multi-material 3D printers.","cites":"72","conferencePercentile":"92.69005848"},{"venue":"ACM Trans. Graph.","id":"4694524ffda87ebc63aebb55e8e858946d02edcd","venue_1":"ACM Trans. Graph.","year":"2010","title":"SmartBoxes for interactive urban reconstruction","authors":"Liangliang Nan, Andrei Sharf, Hao Zhang, Daniel Cohen-Or, Baoquan Chen","author_ids":"2792746, 2120270, 1682058, 1701009, 1748939","abstract":"We introduce an interactive tool which enables a user to quickly assemble an architectural model directly over a 3D point cloud acquired from large-scale scanning of an urban scene. The user loosely defines and manipulates simple building blocks, which we call SmartBoxes, over the point samples. These boxes quickly snap to their proper locations to conform to common architectural structures. The key idea is that the building blocks are smart in the sense that their locations and sizes are automatically adjusted on-the-fly to fit well to the point data, while at the same time respecting contextual relations with nearby similar blocks. SmartBoxes are assembled through a discrete optimization to balance between two snapping forces defined respectively by a data-fitting term and a contextual term, which together assist the user in reconstructing the architectural model from a sparse and noisy point cloud. We show that a combination of the user's interactive guidance and high-level knowledge about the semantics of the underlying model, together with the snapping forces, allows the reconstruction of structures which are partially or even completely missing from the input.","cites":"58","conferencePercentile":"90.64327485"},{"venue":"ACM Trans. Graph.","id":"217af49622a4e51b6d1b9b6c75726eaf1355a903","venue_1":"ACM Trans. Graph.","year":"2005","title":"Animating pictures with stochastic motion textures","authors":"Yung-Yu Chuang, Dan B. Goldman, Ke Colin Zheng, Brian Curless, David Salesin, Richard Szeliski","author_ids":"3032320, 1976171, 1903504, 1810052, 1745260, 1717841","abstract":"In this paper, we explore the problem of enhancing still pictures with subtly animated motions. We limit our domain to scenes containing passive elements that respond to natural forces in some fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. Then, a \"stochastic motion texture\" is automatically synthesized using a spectral method, i.e., the inverse Fourier transform of a filtered noise spectrum. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are then recomposited to form the animated frames. The result is a looping video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings.","cites":"57","conferencePercentile":"35.88709677"},{"venue":"ACM Trans. Graph.","id":"88349aea6c84771be835b98d19067149378407cc","venue_1":"ACM Trans. Graph.","year":"2004","title":"Keyframe-based tracking for rotoscoping and animation","authors":"Aseem Agarwala, Aaron Hertzmann, David Salesin, Steven M. Seitz","author_ids":"1696487, 1747779, 1745260, 1679223","abstract":"We describe a new approach to rotoscoping --- the process of tracking contours in a video sequence --- that combines computer vision with user interaction. In order to track contours in video, the user specifies curves in two or more frames; these curves are used as keyframes by a computer-vision-based tracking algorithm. The user may interactively refine the curves and then restart the tracking algorithm. Combining computer vision with user interaction allows our system to track any sequence with significantly less effort than interpolation-based systems --- and with better reliability than \"pure\" computer vision systems. Our tracking algorithm is cast as a spacetime optimization problem that solves for time-varying curve shapes based on an input video sequence and user-specified constraints. We demonstrate our system with several rotoscoped examples. Additionally, we show how these rotoscoped contours can be used to help create cartoon animation by attaching user-drawn strokes to the tracked contours.","cites":"132","conferencePercentile":"64.13043478"},{"venue":"ACM Trans. Graph.","id":"3415672f90b35e15bffa3c12d5b33f3d47fe2559","venue_1":"ACM Trans. Graph.","year":"2004","title":"Interactive digital photomontage","authors":"Aseem Agarwala, Mira Dontcheva, Maneesh Agrawala, Steven M. Drucker, Alex Colburn, Brian Curless, David Salesin, Michael F. Cohen","author_ids":"1696487, 2875493, 1820412, 2311676, 5945803, 1810052, 1745260, 1694613","abstract":"We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call \"digital photomontage.\" Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as \"maximum contrast\") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including \"selective composites\" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.","cites":"355","conferencePercentile":"94.56521739"},{"venue":"ACM Trans. Graph.","id":"11736edc4c23c8fe79ba13f3ec2258cf779b3765","venue_1":"ACM Trans. Graph.","year":"2003","title":"Non-invasive interactive visualization of dynamic architectural environments","authors":"Christopher Niederauer, Mike Houston, Maneesh Agrawala, Greg Humphreys","author_ids":"2217938, 2376915, 1820412, 1734552","abstract":"We present a system for interactively producing exploded views of 3D architectural environments such as multi-story buildings. These exploded views allow viewers to simultaneously see the internal and external structures of such environments. To create an exploded view we analyze the geometry of the environment to locate individual stories. We then use clipping planes and multipass rendering to separately render each story of the environment in exploded form. Our system operates at the graphics driver level and therefore can be applied to existing OpenGL applications, such as first-person multi-player video games, without modification. The resulting visualization allows users to understand the global structure of architectural environments and to observe the actions of dynamic characters and objects interacting within such environments.","cites":"23","conferencePercentile":"5.913978495"},{"venue":"ACM Trans. Graph.","id":"0a557426544e917ed3deaaed4ff309d0fc14186e","venue_1":"ACM Trans. Graph.","year":"2010","title":"Multi-scale image harmonization","authors":"Kalyan Sunkavalli, Micah K. Johnson, Wojciech Matusik, Hanspeter Pfister","author_ids":"2454127, 1744375, 1752521, 1701371","abstract":"Traditional image compositing techniques, such as alpha matting and gradient domain compositing, are used to create composites that have plausible boundaries. But when applied to images taken from different sources or shot under different conditions, these techniques can produce unrealistic results. In this work, we present a framework that explicitly matches the visual appearance of images through a process we call <i>image harmonization</i>, before blending them. At the heart of this framework is a multi-scale technique that allows us to transfer the appearance of one image to another. We show that by carefully manipulating the scales of a pyramid decomposition of an image, we can match contrast, texture, noise, and blur, while avoiding image artifacts. The output composite can then be reconstructed from the modified pyramid coefficients while enforcing both alpha-based and seamless boundary constraints. We show how the proposed framework can be used to produce realistic composites with minimal user interaction in a number of different scenarios.","cites":"32","conferencePercentile":"55.84795322"},{"venue":"ACM Trans. Graph.","id":"7f09a40787a76484fc98520564d34242f43529cc","venue_1":"ACM Trans. Graph.","year":"2005","title":"Panoramic video textures","authors":"Aseem Agarwala, Ke Colin Zheng, Christopher Joseph Pal, Maneesh Agrawala, Michael F. Cohen, Brian Curless, David Salesin, Richard Szeliski","author_ids":"1696487, 1903504, 2571211, 1820412, 1694613, 1810052, 1745260, 1717841","abstract":"This paper describes a mostly automatic method for taking the output of a single panning video camera and creating a <i>panoramic video texture</i> (PVT): a video that has been stitched into a single, wide field of view and that appears to play continuously and indefinitely. The key problem in creating a PVT is that although only a portion of the scene has been imaged at any given time, the output must simultaneously portray motion throughout the scene. Like previous work in video textures, our method employs min-cut optimization to select fragments of video that can be stitched together both spatially and temporally. However, it differs from earlier work in that the optimization must take place over a much larger set of data. Thus, to create PVTs, we introduce a dynamic programming step, followed by a novel hierarchical min-cut optimization algorithm. We also use gradient-domain compositing to further smooth boundaries between video fragments. We demonstrate our results with an interactive viewer in which users can interactively pan and zoom on high-resolution PVTs.","cites":"89","conferencePercentile":"60.48387097"},{"venue":"ACM Trans. Graph.","id":"14fba545bbedbeeb16010825e616aed7de253511","venue_1":"ACM Trans. Graph.","year":"2010","title":"Physical reproduction of materials with specified subsurface scattering","authors":"Milos Hasan, Martin Fuchs, Wojciech Matusik, Hanspeter Pfister, Szymon Rusinkiewicz","author_ids":"2545444, 3100139, 1752521, 1701371, 7723706","abstract":"We investigate a complete pipeline for measuring, modeling, and fabricating objects with specified subsurface scattering behaviors. The process starts with measuring the scattering properties of a given set of base materials, determining their radial reflection and transmission profiles. We describe a mathematical model that predicts the profiles of different stackings of base materials, at arbitrary thicknesses. In an inverse process, we can then specify a desired reflection profile and compute a layered composite material that best approximates it. Our algorithm efficiently searches the space of possible combinations of base materials, pruning unsatisfactory states imposed by physical constraints. We validate our process by producing both homogeneous and heterogeneous composites fabricated using a multi-material 3D printer. We demonstrate reproductions that have scattering properties approximating complex materials.","cites":"56","conferencePercentile":"89.18128655"},{"venue":"ACM Trans. Graph.","id":"7748de0e2aaf80555eb1e346b619db66fa8d4a74","venue_1":"ACM Trans. Graph.","year":"2001","title":"On numerical solutions to one-dimensional integration problems with applications to linear light sources","authors":"Marc J. Ouellette, Eugene Fiume","author_ids":"2242444, 3018043","abstract":"Many key problems in computer graphics require the computation of integrals. Due to the nature of the integrand and of the domain of integration, these integrals seldom can be computed analytically. As a result, numerical techniques are used to find approximate solutions to these problems. While the numerical analysis literature offers many integration techniques, the choice of which method to use for specific computer graphic problems is a difficult one. This choice must be driven by the numerical efficiency of the method, and ultimately, by its visual impact on the computed image. In this paper, we begin to address these issues by methodically analyzing deterministic and stochastic numerical techniques and their application to the type of one-dimensional problems that occur in computer graphics, especially in the context of linear light source integration. In addition to traditional methods such as Gauss-Legendre quadratures, we also examine Voronoi diagram-based sampling, jittered quadratures, random offset quadratures, weighted Monte Carlo, and a newly introduced method of compounding known as a <i>difficulty driven compound quadrature</i>.We compare the effectiveness of these methods using a three-pronged approach. First, we compare the frequency domain characteristics of all the methods using periodograms. Next, applying ideas found in the numerical analysis literature, we examine the numerical and visual performance profiles of these methods for seven different one-parameter problem families. We then present results from the application of the methods for the example of linear light sources. Finally, we summarize the relative effectiveness of the methods surveyed, showing the potential power of difficulty-driven compound quadratures.","cites":"7","conferencePercentile":"20"},{"venue":"ACM Trans. Graph.","id":"293a01458a6f085306ec0603ca20fd9964c70328","venue_1":"ACM Trans. Graph.","year":"2014","title":"Shape2Pose: human-centric shape analysis","authors":"Vladimir G. Kim, Siddhartha Chaudhuri, Leonidas J. Guibas, Thomas A. Funkhouser","author_ids":"3082383, 7218171, 1744254, 1807080","abstract":"As 3D acquisition devices and modeling tools become widely available there is a growing need for automatic algorithms that analyze the semantics and functionality of digitized shapes. Most recent research has focused on analyzing geometric structures of shapes. Our work is motivated by the observation that a majority of man-made shapes are designed to be used by people. Thus, in order to fully understand their semantics, one needs to answer a fundamental question: \"how do people interact with these objects?\" As an initial step towards this goal, we offer a novel algorithm for automatically predicting a static pose that a person would need to adopt in order to use an object. Specifically, given an input 3D shape, the goal of our analysis is to predict a corresponding human pose, including contact points and kinematic parameters. This is especially challenging for man-made objects that commonly exhibit a lot of variance in their geometric structure. We address this challenge by observing that contact points usually share consistent local geometric features related to the anthropometric properties of corresponding parts and that human body is subject to kinematic constraints and priors. Accordingly, our method effectively combines local region classification and global kinematically-constrained search to successfully predict poses for various objects. We also evaluate our algorithm on six diverse collections of 3D polygonal models (chairs, gym equipment, cockpits, carts, bicycles, and bipedal devices) containing a total of 147 models. Finally, we demonstrate that the poses predicted by our algorithm can be used in several shape analysis problems, such as establishing correspondences between objects, detecting salient regions, finding informative viewpoints, and retrieving functionally-similar shapes.","cites":"22","conferencePercentile":"91.56378601"},{"venue":"ACM Trans. Graph.","id":"17805d0f45e4f6fdfa30e309e1e68ac8e683fe6b","venue_1":"ACM Trans. Graph.","year":"2008","title":"SOHO: Orthogonal and symmetric Haar wavelets on the sphere","authors":"Christian Lessig, Eugene Fiume","author_ids":"1777246, 3018043","abstract":"We propose the SOHO wavelet basis&#8212;the first spherical Haar wavelet basis that is both orthogonal and symmetric, making it particularly well suited for the approximation and processing of all-frequency signals on the sphere. We obtain the basis with a novel spherical subdivision scheme that defines a partition acting as the domain of the basis functions. Our construction refutes earlier claims doubting the existence of a basis that is both orthogonal and symmetric. Experimental results for the representation of spherical signals verify that the superior theoretical properties of the SOHO wavelet basis are also relevant in practice.","cites":"8","conferencePercentile":"4.012345679"},{"venue":"ACM Trans. Graph.","id":"6b03cd9a472ddab100b8dbd2ead219fe79c157e7","venue_1":"ACM Trans. Graph.","year":"2015","title":"Semantic shape editing using deformation handles","authors":"Mehmet Ersin Yümer, Siddhartha Chaudhuri, Jessica K. Hodgins, Levent Burak Kara","author_ids":"2396667, 7218171, 1788773, 1808848","abstract":"We propose a shape editing method where the user creates geometric deformations using a set of semantic attributes, thus avoiding the need for detailed geometric manipulations. In contrast to prior work, we focus on continuous deformations instead of discrete part substitutions. Our method provides a platform for quick design explorations and allows non-experts to produce semantically guided shape variations that are otherwise difficult to attain. We crowdsource a large set of pairwise comparisons between the semantic attributes and geometry and use this data to learn a continuous mapping from the semantic attributes to geometry. The resulting map enables simple and intuitive shape manipulations based solely on the learned attributes. We demonstrate our method on large datasets using two different user interaction modes and evaluate its usability with a set of user studies.","cites":"9","conferencePercentile":"88.16326531"},{"venue":"ACM Trans. Graph.","id":"53c73106d861f7dcac9fbdb678e488b811a4e1a4","venue_1":"ACM Trans. Graph.","year":"2012","title":"Fluid simulation using Laplacian eigenfunctions","authors":"Tyler de Witt, Christian Lessig, Eugene Fiume","author_ids":"2131331, 1777246, 3018043","abstract":"We present an algorithm for the simulation of incompressible fluid phenomena that is computationally efficient and leads to visually convincing simulations with far fewer degrees of freedom than existing approaches. Rather than using an Eulerian grid or Lagrangian elements, we represent vorticity and velocity using a basis of global functions defined over the entire simulation domain. We show that choosing Laplacian eigenfunctions for this basis provides benefits, including correspondence with spatial scales of vorticity and precise energy control at each scale. We perform Galerkin projection of the Navier-Stokes equations to derive a time evolution equation in the space of basis coefficients. Our method admits closed-form solutions on simple domains but can also be implemented efficiently on arbitrary meshes.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"182b53b6823605f2a9b6fa6135227a303493b4c4","venue_1":"ACM Trans. Graph.","year":"2010","title":"Automatic generation of destination maps","authors":"Johannes Kopf, Maneesh Agrawala, David Bargeron, David Salesin, Michael F. Cohen","author_ids":"2891193, 1820412, 1787634, 1745260, 1694613","abstract":"Destination maps are navigational aids designed to show anyone within a region how to reach a location (the destination). Hand-designed destination maps include only the most important roads in the region and are non-uniformly scaled to ensure that all of the important roads from the highways to the residential streets are visible. We present the first automated system for creating such destination maps based on the design principles used by mapmakers. Our system includes novel algorithms for selecting the important roads based on mental representations of road networks, and for laying out the roads based on a non-linear optimization procedure. The final layouts are labeled and rendered in a variety of styles ranging from informal to more formal map styles. The system has been used to generate over 57,000 destination maps by thousands of users. We report feedback from both a formal and informal user study, as well as provide quantitative measures of success.","cites":"14","conferencePercentile":"20.1754386"},{"venue":"ACM Trans. Graph.","id":"63e4b6bc405b263f17ad55ea9ba0c4ddd4a2d62f","venue_1":"ACM Trans. Graph.","year":"2008","title":"Diffusion curves: a vector representation for smooth-shaded images","authors":"Alexandrina Orzan, Adrien Bousseau, Holger Winnemöller, Pascal Barla, Joëlle Thollot, David Salesin","author_ids":"2879283, 2149814, 2168852, 1692862, 2800344, 1745260","abstract":"We describe a new vector-based primitive for creating smooth-shaded images, called the <i>diffusion curve</i>. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution-independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in realtime. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. The system is simple and intuitive: we show results created by artists after just a few minutes of instruction. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.","cites":"99","conferencePercentile":"91.04938272"},{"venue":"ACM Trans. Graph.","id":"886ecfd186b5b7ed9fbdb4b563ff62f65efb3747","venue_1":"ACM Trans. Graph.","year":"2008","title":"Automated generation of interactive 3D exploded view diagrams","authors":"Wilmot Li, Maneesh Agrawala, Brian Curless, David Salesin","author_ids":"2812691, 1820412, 1810052, 1745260","abstract":"We present a system for creating and viewing interactive exploded views of complex 3D models. In our approach, a 3D input model is organized into an <i>explosion graph</i> that encodes how parts explode with respect to each other. We present an automatic method for computing explosion graphs that takes into account part hierarchies in the input models and handles common classes of interlocking parts. Our system also includes an interface that allows users to interactively explore our exploded views using both direct controls and higher-level interaction modes.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"181d665ac7f2f8297df5ece68483a87fff77369d","venue_1":"ACM Trans. Graph.","year":"2015","title":"Online control of simulated humanoids using particle belief propagation","authors":"Perttu Hämäläinen, Joose Rajamäki, C. Karen Liu","author_ids":"1777495, 1996406, 1688533","abstract":"We present a novel, general-purpose Model-Predictive Control (MPC) algorithm that we call Control Particle Belief Propagation (C-PBP). C-PBP combines multimodal, gradient-free sampling and a Markov Random Field factorization to effectively perform simultaneous path finding and smoothing in high-dimensional spaces. We demonstrate the method in online synthesis of interactive and physically valid humanoid movements, including balancing, recovery from both small and extreme disturbances, reaching, balancing on a ball, juggling a ball, and fully steerable locomotion in an environment with obstacles. Such a large repertoire of movements has not been demonstrated before at interactive frame rates, especially considering that all our movement emerges from simple cost functions. Furthermore, we abstain from using any precomputation to train a control policy offline, reference data such as motion capture clips, or state machines that break the movements down into more manageable subtasks. Operating under these conditions enables rapid and convenient iteration when designing the cost functions.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"64888b62b1719fa19f7e42575cf6c321131b5932","venue_1":"ACM Trans. Graph.","year":"2007","title":"Video watercolorization using bidirectional texture advection","authors":"Adrien Bousseau, Fabrice Neyret, Joëlle Thollot, David Salesin","author_ids":"2149814, 1708868, 2800344, 1745260","abstract":"In this paper, we present a method for creating watercolor-like animation, starting from video as input. The method involves two main steps: applying textures that simulate a watercolor appearance; and creating a simplified, abstracted version of the video to which the texturing operations are applied. Both of these steps are subject to highly visible temporal artifacts, so the primary technical contributions of the paper are extensions of previous methods for texturing and abstraction to provide temporal coherence when applied to video sequences. To maintain coherence for textures, we employ texture advection along lines of optical flow. We furthermore extend previous approaches by incorporating advection in both forward and reverse directions through the video, which allows for minimal texture distortion, particularly in areas of disocclusion that are otherwise highly problematic. To maintain coherence for abstraction, we employ mathematical morphology extended to the temporal domain, using filters whose temporal extents are locally controlled by the degree of distortions in the optical flow. Together, these techniques provide the first practical and robust approach for producing watercolor animations from video, which we demonstrate with a number of examples.","cites":"66","conferencePercentile":"61.6"},{"venue":"ACM Trans. Graph.","id":"1c492a9842865eba651ccbd98fe88e8ca823ea88","venue_1":"ACM Trans. Graph.","year":"2006","title":"Photographing long scenes with multi-viewpoint panoramas","authors":"Aseem Agarwala, Maneesh Agrawala, Michael F. Cohen, David Salesin, Richard Szeliski","author_ids":"1696487, 1820412, 1694613, 1745260, 1717841","abstract":"We present a system for producing multi-viewpoint panoramas of long, roughly planar scenes, such as the facades of buildings along a city street, from a relatively sparse set of photographs captured with a handheld still camera that is moved along the scene. Our work is a significant departure from previous methods for creating multi-viewpoint panoramas, which composite thin vertical strips from a video sequence captured by a translating video camera, in that the resulting panoramas are composed of relatively large regions of ordinary perspective. In our system, the only user input required beyond capturing the photographs themselves is to identify the dominant plane of the photographed scene; our system then computes a panorama automatically using Markov Random Field optimization. Users may exert additional control over the appearance of the result by drawing rough strokes that indicate various high-level goals. We demonstrate the results of our system on several scenes, including urban streets, a river bank, and a grocery store aisle.","cites":"110","conferencePercentile":"80.55555556"},{"venue":"ACM Trans. Graph.","id":"c1ead86f67f582bfae7a8d68accf569a2c93748c","venue_1":"ACM Trans. Graph.","year":"2014","title":"A constructive theory of sampling for image synthesis using reproducing Kernel bases","authors":"Christian Lessig, Mathieu Desbrun, Eugene Fiume","author_ids":"1777246, 1716096, 3018043","abstract":"Sampling a scene by tracing rays and reconstructing an image from such pointwise samples is fundamental to computer graphics. To improve the efficacy of these computations, we propose an alternative theory of sampling. In contrast to traditional formulations for image synthesis, which appeal to nonconstructive Dirac deltas, our theory employs constructive reproducing kernels for the correspondence between continuous functions and pointwise samples. Conceptually, this allows us to obtain a common mathematical formulation of almost all existing numerical techniques for image synthesis. Practically, it enables novel sampling based numerical techniques designed for light transport that provide considerably improved performance per sample. We exemplify the practical benefits of our formulation with three applications: pointwise transport of color spectra, projection of the light energy density into spherical harmonics, and approximation of the shading equation from a photon map. Experimental results verify the utility of our sampling formulation, with lower numerical error rates and enhanced visual quality compared to existing techniques.","cites":"1","conferencePercentile":"4.938271605"},{"venue":"ACM Trans. Graph.","id":"3c5afa833d290adb2341fe2554a8b3cf18dfcd96","venue_1":"ACM Trans. Graph.","year":"2011","title":"Texture-lobes for tree modelling","authors":"Yotam Livny, Sören Pirk, Zhang-Lin Cheng, Feilong Yan, Oliver Deussen, Daniel Cohen-Or, Baoquan Chen","author_ids":"2210567, 2604835, 1829075, 2020843, 1850438, 1701009, 1748939","abstract":"We present a lobe-based tree representation for modeling trees. The new representation is based on the observation that the tree's foliage details can be abstracted into canonical geometry structures, termed lobe-textures. We introduce techniques to (i) approximate the geometry of given tree data and encode it into a lobe-based representation, (ii) decode the representation and synthesize a fully detailed tree model that visually resembles the input. The encoded tree serves as a light intermediate representation, which facilitates efficient storage and transmission of massive amounts of trees, e.g., from a server to clients for interactive applications in urban environments. The method is evaluated by both reconstructing laser scanned trees (given as point sets) as well as re-representing existing tree models (given as polygons).","cites":"25","conferencePercentile":"55.26315789"},{"venue":"ACM Trans. Graph.","id":"748edafd7cdf41e78d49816a71fb1e40ea07e6ae","venue_1":"ACM Trans. Graph.","year":"2009","title":"Moving gradients: a path-based method for plausible image interpolation","authors":"Dhruv Kumar Mahajan, Fu-Chung Huang, Wojciech Matusik, Ravi Ramamoorthi, Peter N. Belhumeur","author_ids":"1962364, 1798019, 1752521, 1752236, 1767767","abstract":"We describe a method for plausible interpolation of images, with a wide range of applications like temporal up-sampling for smooth playback of lower frame rate video, smooth view interpolation, and animation of still images. The method is based on the intuitive idea, that a given pixel in the interpolated frames traces out a <i>path</i> in the source images. Therefore, we simply move and copy pixel gradients from the input images along this path. A key innovation is to allow arbitrary (asymmetric) <i>transition points</i>, where the path moves from one image to the other. This flexible transition preserves the frequency content of the originals without ghosting or blurring, and maintains temporal coherence. Perhaps most importantly, our framework makes occlusion handling particularly simple. The transition points allow for matches away from the occluded regions, at any suitable point along the path. Indeed, occlusions do not need to be handled explicitly at all in our initial graph-cut optimization. Moreover, a simple comparison of computed path lengths <i>after</i> the optimization, allows us to robustly identify occluded regions, and compute the most plausible interpolation in those areas. Finally, we show that significant improvements are obtained by moving gradients and using Poisson reconstruction.","cites":"64","conferencePercentile":"83.42541436"},{"venue":"ACM Trans. Graph.","id":"0df0acbd41f388b6212b2b9f52febceb91253937","venue_1":"ACM Trans. Graph.","year":"2006","title":"Schematic storyboarding for video visualization and editing","authors":"Dan B. Goldman, Brian Curless, David Salesin, Steven M. Seitz","author_ids":"1976171, 1810052, 1745260, 1679223","abstract":"We present a method for visualizing short video clips in a single static image, using the visual language of storyboards. These <i>schematic storyboards</i> are composed from multiple input frames and annotated using outlines, arrows, and text describing the motion in the scene. The principal advantage of this storyboard representation over standard representations of video -- generally either a static thumbnail image or a playback of the video clip in its entirety -- is that it requires only a moment to observe and comprehend but at the same time retains much of the detail of the source video. Our system renders a schematic storyboard layout based on a small amount of user interaction. We also demonstrate an interaction technique to scrub through time using the natural spatial dimensions of the storyboard. Potential applications include video editing, surveillance summarization, assembly instructions, composition of graphic novels, and illustration of camera technique for film studies.","cites":"90","conferencePercentile":"67.12962963"},{"venue":"ACM Trans. Graph.","id":"1b4024ed60e99f66a494dfed64b3687d26d3c502","venue_1":"ACM Trans. Graph.","year":"2011","title":"GlobFit: consistently fitting primitives by discovering global relations","authors":"Yangyan Li, Xiaokun Wu, Yiorgos Chrysanthou, Andrei Sharf, Daniel Cohen-Or, Niloy J. Mitra","author_ids":"1920864, 2356009, 1706408, 2120270, 1701009, 1710455","abstract":"Given a noisy and incomplete point set, we introduce a method that simultaneously recovers a set of locally fitted primitives along with their global mutual relations. We operate under the assumption that the data corresponds to a man-made engineering object consisting of basic primitives, possibly repeated and globally aligned under common relations. We introduce an algorithm to directly couple the local and global aspects of the problem. The local fit of the model is determined by how well the inferred model agrees to the observed data, while the global relations are iteratively learned and enforced through a constrained optimization. Starting with a set of initial RANSAC based locally fitted primitives, relations across the primitives such as orientation, placement, and equality are progressively learned and conformed to. In each stage, a set of feasible relations are extracted among the candidate relations, and then aligned to, while best fitting to the input data. The global coupling corrects the primitives obtained in the local RANSAC stage, and brings them to precise global alignment. We test the robustness of our algorithm on a range of synthesized and scanned data, with varying amounts of noise, outliers, and non-uniform sampling, and validate the results against ground truth, where available.","cites":"74","conferencePercentile":"93.68421053"},{"venue":"ACM Trans. Graph.","id":"1a50ba87aac9b109c54c9ca8b38b5edfaaaa9b6f","venue_1":"ACM Trans. Graph.","year":"2016","title":"JALI: an animator-centric viseme model for expressive lip synchronization","authors":"Pif Edwards, Chris Landreth, Eugene Fiume, Karan Singh","author_ids":"2625904, 2483922, 3018043, 1682205","abstract":"The rich signals we extract from facial expressions imposes high expectations for the science and art of facial animation. While the advent of high-resolution performance capture has greatly improved realism, the utility of procedural animation warrants a prominent place in facial animation workflow. We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output. Because of the diversity of ways we produce sound, the mapping from phonemes to visual depictions as visemes is many-valued. We draw from psycholinguistics to capture this variation using two visually distinct anatomical actions: <b>Ja</b>w and <b>L</b>ip, wheresound is primarily controlled by jaw articulation and lower-face muscles, respectively. We describe the construction of a transferable template jali 3D facial rig, built upon the popular facial muscle action unit representation facs. We show that acoustic properties in a speech signal map naturally to the dynamic degree of <i>jaw</i> and <i>lip</i> in visual speech. We provide an array of compelling animation clips, compare against performance capture and existing procedural animation, and report on a brief user study.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"127ba34771d2518b826f2893ff44961eb097bc33","venue_1":"ACM Trans. Graph.","year":"2003","title":"Designing effective step-by-step assembly instructions","authors":"Maneesh Agrawala, Doantam Phan, Julie Heiser, John Haymaker, Jeff Klingner, Pat Hanrahan, Barbara Tversky","author_ids":"1820412, 2797343, 3342679, 2725819, 2367620, 4982303, 1743610","abstract":"We present design principles for creating effective assembly instructions and a system that is based on these principles. The principles are drawn from cognitive psychology research which investigated people's conceptual models of assembly and effective methods to visually communicate assembly information. Our system is inspired by earlier work in robotics on assembly planning and in visualization on automated presentation design. Although other systems have considered presentation and planning independently, we believe it is necessary to address the two problems simultaneously in order to create effective assembly instructions. We describe the algorithmic techniques used to produce assembly instructions given object geometry, orientation, and optional grouping and ordering constraints on the object's parts. Our results demonstrate that it is possible to produce aesthetically pleasing and easy to follow instructions for many everyday objects.","cites":"92","conferencePercentile":"51.61290323"},{"venue":"ACM Trans. Graph.","id":"161e6ff49e6ec696b3eddfff28f2423badafd101","venue_1":"ACM Trans. Graph.","year":"2007","title":"Interactive cutaway illustrations of complex 3D models","authors":"Wilmot Li, Lincoln Ritter, Maneesh Agrawala, Brian Curless, David Salesin","author_ids":"2812691, 2874548, 1820412, 1810052, 1745260","abstract":"We present a system for authoring and viewing interactive cutaway illustrations of complex 3D models using conventions of traditional scientific and technical illustration. Our approach is based on the two key ideas that 1) cuts should respect the geometry of the parts being cut, and 2) cutaway illustrations should support interactive exploration. In our approach, an author instruments a 3D model with auxiliary parameters, which we call \"rigging,\" that define how cutaways of that structure are formed. We provide an authoring interface that automates most of the rigging process. We also provide a viewing interface that allows viewers to explore rigged models using high-level interactions. In particular, the viewer can just select a set of target structures, and the system will automatically generate a cutaway illustration that exposes those parts. We have tested our system on a variety of CAD and anatomical models, and our results demonstrate that our approach can be used to create and view effective interactive cutaway illustrations for a variety of complex objects with little user effort.","cites":"50","conferencePercentile":"52"},{"venue":"ACM Trans. Graph.","id":"cede088d37f4c38eccca2a6666a850931bf346af","venue_1":"ACM Trans. Graph.","year":"2010","title":"Discrete scale axis representations for 3D geometry","authors":"Balint Miklos, Joachim Giesen, Mark Pauly","author_ids":"2864102, 1695806, 1741645","abstract":"This paper addresses the fundamental problem of computing stable medial representations of 3D shapes. We propose a <i>spatially adaptive</i> classification of geometric features that yields a robust algorithm for generating medial representations at different levels of abstraction. The recently introduced continuous scale axis transform serves as the mathematical foundation of our algorithm. We show how geometric and topological properties of the continuous setting carry over to discrete shape representations. Our method combines scaling operations of medial balls for geometric simplification with filtrations of the medial axis and provably good conversion steps to and from union of balls, to enable efficient processing of a wide variety shape representations including polygon meshes, 3D images, implicit surfaces, and point clouds. We demonstrate the robustness and versatility of our algorithm with an extensive validation on hundreds of shapes including complex geometries consisting of millions of triangles.","cites":"42","conferencePercentile":"73.68421053"},{"venue":"ACM Trans. Graph.","id":"73187aa1b0f13d2c07c1a25e7b830980d0a35747","venue_1":"ACM Trans. Graph.","year":"2015","title":"The light field stereoscope: immersive computer graphics via factored near-eye light field displays with focus cues","authors":"Fu-Chung Huang, Kevin Chen, Gordon Wetzstein","author_ids":"1798019, 1792429, 1731170","abstract":"Over the last few years, virtual reality (VR) has re-emerged as a technology that is now feasible at low cost via inexpensive cellphone components. In particular, advances of high-resolution micro displays, low-latency orientation trackers, and modern GPUs facilitate immersive experiences at low cost. One of the remaining challenges to further improve visual comfort in VR experiences is the vergence-accommodation conflict inherent to all stereoscopic displays. Accurate reproduction of all depth cues is crucial for visual comfort. By combining well-known stereoscopic display principles with emerging factored light field technology, we present the first wearable VR display supporting high image resolution as well as focus cues. A light field is presented to each eye, which provides more natural viewing experiences than conventional near-eye displays. Since the eye box is just slightly larger than the pupil size, rank-1 light field factorizations are sufficient to produce correct or nearly-correct focus cues; <i>no time-multiplexed image display or gaze tracking is required.</i> We analyze lens distortions in 4D light field space and correct them using the afforded high-dimensional image formation. We also demonstrate significant improvements in resolution and retinal blur quality over related near-eye displays. Finally, we analyze diffraction limits of these types of displays.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"5a7fab702861a47a879443b4f468564fd39507a9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Designing structurally-sound ornamental curve networks","authors":"Stelian Coros, Jonas Zehnder, Bernhard Thomaszewski","author_ids":"1783776, 3439662, 1784345","abstract":"We present a computational tool for designing ornamental curve networks---structurally-sound physical surfaces with user-controlled aesthetics. In contrast to approaches that leverage texture synthesis for creating decorative surface patterns, our method relies on user-defined spline curves as central design primitives. More specifically, we build on the physically-inspired metaphor of an embedded elastic curve that can move on a smooth surface, deform, and connect with other curves. We formalize this idea as a globally coupled energy-minimization problem, discretized with piece-wise linear curves that are optimized in the parametric space of a smooth surface. Building on this technical core, we propose a set of interactive design and editing tools that we demonstrate on manually-created layouts and semi-automated deformable packings. In order to prevent excessive compliance, we furthermore propose a structural analysis tool that uses eigenanalysis to identify potentially large deformations between geodesically-close curves and guide the user in strengthening the corresponding regions. We used our approach to create a variety of designs in simulation, validated with a set of 3D-printed physical prototypes.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"f3654759578d8a1c32c5534637b5ea2f6ed11aeb","venue_1":"ACM Trans. Graph.","year":"2012","title":"Motion-driven concatenative synthesis of cloth sounds","authors":"Steven S. An, Doug L. James, Steve Marschner","author_ids":"2457460, 1739671, 2593798","abstract":"We present a practical data-driven method for automatically synthesizing plausible soundtracks for physics-based cloth animations running at graphics rates. Given a cloth animation, we analyze the deformations and use motion events to drive crumpling and friction sound models estimated from cloth measurements. We synthesize a low-quality sound signal, which is then used as a target signal for a concatenative sound synthesis (CSS) process. CSS selects a sequence of microsound units, very short segments, from a database of recorded cloth sounds, which best match the synthesized target sound in a low-dimensional feature-space after applying a hand-tuned warping function. The selected microsound units are concatenated together to produce the final cloth sound with minimal filtering. Our approach avoids expensive physics-based synthesis of cloth sound, instead relying on cloth recordings and our motion-driven CSS approach for realism. We demonstrate its effectiveness on a variety of cloth animations involving various materials and character motions, including first-person virtual clothing with binaural sound.","cites":"1","conferencePercentile":"0.757575758"},{"venue":"ACM Trans. Graph.","id":"3e9bc5fab367e5c3252c6fd7f6463a4ea9b8d3aa","venue_1":"ACM Trans. Graph.","year":"2011","title":"Building volumetric appearance models of fabric using micro CT imaging","authors":"Shuang Zhao, Wenzel Jakob, Steve Marschner, Kavita Bala","author_ids":"2373908, 1780369, 2593798, 8261370","abstract":"The appearance of complex, thick materials like textiles is determined by their 3D structure, and they are incompletely described by surface reflection models alone. While volume scattering can produce highly realistic images of such materials, creating the required volume density models is difficult. Procedural approaches require significant programmer effort and intuition to design specialpurpose algorithms for each material. Further, the resulting models lack the visual complexity of real materials with their naturally-arising irregularities.\n This paper proposes a new approach to acquiring volume models, based on density data from X-ray computed tomography (CT) scans and appearance data from photographs under uncontrolled illumination. To model a material, a CT scan is made, resulting in a scalar density volume. This 3D data is processed to extract orientation information and remove noise. The resulting density and orientation fields are used in an appearance matching procedure to define scattering properties in the volume that, when rendered, produce images with texture statistics that match the photographs. As our results show, this approach can easily produce volume appearance models with extreme detail, and at larger scales the distinctive textures and highlights of a range of very different fabrics like satin and velvet emerge automatically---all based simply on having accurate mesoscale geometry.","cites":"26","conferencePercentile":"57.36842105"},{"venue":"ACM Trans. Graph.","id":"05b14b7b3a4ef9449bfca9dea36981f385b2c531","venue_1":"ACM Trans. Graph.","year":"2010","title":"Apparent layer operations for the manipulation of deformable objects","authors":"Takeo Igarashi, Jun Mitani","author_ids":"1717356, 2618827","abstract":"We introduce layer operations for single-view 3D deformable object manipulation, in which the user can control the depth order of layered 3D objects resting on a flat ground with simple clicks and drags, as in 2D drawing systems. We present two interaction techniques based on this idea and describe their implementation. The first technique is explicit layer swap. The user clicks the target layer, and the system swaps the layer with the one directly underneath it. The second technique is layer-aware dragging. As the user drags the object, the system adjusts its depth automatically to pass over or under a colliding object in the screen space, according to user control. Although the user interface is 2.5D, all scene representations are true 3D, and thus the system naturally supports local layering, self-occlusions, and folds. Internally, the system dynamically computes the apparent layer structure in the current configuration and makes appropriate depth adjustments to obtain the desired results. We demonstrate the effectiveness of this approach in cloth and rope manipulation systems.","cites":"11","conferencePercentile":"14.03508772"},{"venue":"ACM Trans. Graph.","id":"1a11766c06cdeec7b0858111ebfc596212bac2d3","venue_1":"ACM Trans. Graph.","year":"2008","title":"Lapped solid textures: filling a model with anisotropic textures","authors":"Kenshi Takayama, Makoto Okabe, Takashi Ijiri, Takeo Igarashi","author_ids":"1996486, 3102663, 2524158, 1717356","abstract":"We present a method for representing solid objects with spatially-varying oriented textures by repeatedly pasting solid texture exemplars. The underlying concept is to extend the 2D texture patch-pasting approach of lapped textures to 3D solids using a tetrahedral mesh and 3D texture patches. The system places texture patches according to the user-defined volumetric tensor fields over the mesh to represent oriented textures. We have also extended the original technique to handle nonhomogeneous textures for creating solid models whose textural patterns change gradually along the depth fields. We identify several texture types considering the amount of anisotropy and spatial variation and provide a tailored user interface for each. With our simple framework, large-scale realistic solid models can be created easily with little memory and computational cost. We demonstrate the effectiveness of our approach with several examples including trees, fruits, and vegetables.","cites":"40","conferencePercentile":"47.5308642"},{"venue":"ACM Trans. Graph.","id":"4dd0a1bfe4e07e87e58c3cbe57016b82b47ba619","venue_1":"ACM Trans. Graph.","year":"2007","title":"Locally controllable stylized shading","authors":"Hideki Todo, Ken-ichi Anjyo, William V. Baxter, Takeo Igarashi","author_ids":"1768302, 3002982, 1928493, 1717356","abstract":"Recent progress in non-photorealistic rendering (NPR) has led to many stylized shading techniques that efficiently convey visual information about the objects depicted. Another crucial goal of NPR is to give artists simple and direct ways to express the abstract ideas born of their imaginations. In particular, the ability to add intentional, but often unrealistic, shading effects is indispensable for many applications. We propose a set of simple stylized shading algorithms that allow the user to freely add localized light and shade to a model in a manner that is consistent and seamlessly integrated with conventional lighting techniques. The algorithms provide an intuitive, direct manipulation method based on a paint-brush metaphor, to control and edit the light and shade locally as desired. Our prototype system demonstrates how our method can enhance both the quality and range of applicability of conventional stylized shading for offline animation and interactive applications.","cites":"30","conferencePercentile":"29.2"},{"venue":"ACM Trans. Graph.","id":"25b7f1b06e60a39113ebc25049e13b58f84fec1b","venue_1":"ACM Trans. Graph.","year":"2007","title":"FiberMesh: designing freeform surfaces with 3D curves","authors":"Andrew Nealen, Takeo Igarashi, Olga Sorkine-Hornung, Marc Alexa","author_ids":"1729677, 1717356, 2250001, 1751554","abstract":"This paper presents a system for designing freeform surfaces with a collection of 3D curves. The user first creates a rough 3D model by using a sketching interface. Unlike previous sketching systems, the user-drawn strokes stay on the model surface and serve as handles for controlling the geometry. The user can add, remove, and deform these control curves easily, as if working with a 2D line drawing. The curves can have arbitrary topology; they need not be connected to each other. For a given set of curves, the system automatically constructs a smooth surface embedding by applying functional optimization. Our system provides real-time algorithms for both control curve deformation and the subsequent surface optimization. We show that one can create sophisticated models using this system, which have not yet been seen in previous sketching or functional optimization systems.","cites":"176","conferencePercentile":"92"},{"venue":"ACM Trans. Graph.","id":"64998f2123cb44fad7bb8c407af329a628a1443e","venue_1":"ACM Trans. Graph.","year":"2010","title":"Efficient yarn-based cloth with adaptive contact linearization","authors":"Jonathan M. Kaldor, Doug L. James, Steve Marschner","author_ids":"3316406, 1739671, 2593798","abstract":"Yarn-based cloth simulation can improve visual quality but at high computational costs due to the reliance on numerous persistent yarn-yarn contacts to generate material behavior. Finding so many contacts in densely interlinked geometry is a pathological case for traditional collision detection, and the sheer number of contact interactions makes contact processing the simulation bottleneck. In this paper, we propose a method for approximating penalty-based contact forces in yarn-yarn collisions by computing the exact contact response at one time step, then using a rotated linear force model to approximate forces in nearby deformed configurations. Because contacts internal to the cloth exhibit good temporal coherence, sufficient accuracy can be obtained with infrequent updates to the approximation, which are done adaptively in space and time. Furthermore, by tracking contact models we reduce the time to detect new contacts. The end result is a 7- to 9-fold speedup in contact processing and a 4- to 5-fold overall speedup, enabling simulation of character-scale garments.","cites":"21","conferencePercentile":"36.5497076"},{"venue":"ACM Trans. Graph.","id":"179155cc131b4c8b617bcaacba849c3b4ece67b5","venue_1":"ACM Trans. Graph.","year":"2007","title":"Plushie: an interactive design system for plush toys","authors":"Yuki Mori, Takeo Igarashi","author_ids":"1872128, 1717356","abstract":"We introduce Plushie, an interactive system that allows nonprofessional users to design their own original plush toys. To design a plush toy, one needs to construct an appropriate two-dimensional (2D) pattern. However, it is difficult for non-professional users to appropriately design a 2D pattern. Some recent systems automatically generate a 2D pattern for a given three-dimensional (3D) model, but constructing a 3D model is itself a challenge. Furthermore, an arbitrary 3D model cannot necessarily be realized as a real plush toy, and the final sewn result can be very different from the original 3D model. We avoid this mismatch by constructing appropriate 2D patterns and applying simple physical simulation to it on the fly during 3D modeling. In this way, the model on the screen is always a good approximation of the final sewn result, which makes the design process much more efficient. We use a sketching interface for 3D modeling and also provide various editing operations tailored for plush toy design. Internally, the system constructs a 2D cloth pattern in such a way that the simulation result matches the user's input stroke. Our goal is to show that relatively simple algorithms can provide fast, satisfactory results to the user whereas the pursuit of optimal layout and simulation accuracy lies outside this paper's scope. We successfully demonstrated that non-professional users could design plush toys or balloon easily using Plushie.","cites":"91","conferencePercentile":"77.6"},{"venue":"ACM Trans. Graph.","id":"a6e575436acc53f7fc3432c10f9abb36bb966283","venue_1":"ACM Trans. Graph.","year":"2012","title":"Manifold exploration: a Markov Chain Monte Carlo technique for rendering scenes with difficult specular transport","authors":"Wenzel Jakob, Steve Marschner","author_ids":"1780369, 2593798","abstract":"It is a long-standing problem in unbiased Monte Carlo methods for rendering that certain difficult types of light transport paths, particularly those involving viewing and illumination along paths containing specular or glossy surfaces, cause unusably slow convergence. In this paper we introduce Manifold Exploration, a new way of handling specular paths in rendering. It is based on the idea that sets of paths contributing to the image naturally form manifolds in path space, which can be explored locally by a simple equation-solving iteration. This paper shows how to formulate and solve the required equations using only geometric information that is already generally available in ray tracing systems, and how to use this method in in two different Markov Chain Monte Carlo frameworks to accurately compute illumination from general families of paths. The resulting rendering algorithms handle specular, near-specular, glossy, and diffuse surface interactions as well as isotropic or highly anisotropic volume scattering interactions, all using the same fundamental algorithm. An implementation is demonstrated on a range of challenging scenes and evaluated against previous methods.","cites":"33","conferencePercentile":"78.78787879"},{"venue":"ACM Trans. Graph.","id":"3b01bd66079f7ffa40783452ae4d63f53e205fc5","venue_1":"ACM Trans. Graph.","year":"2005","title":"Floral diagrams and inflorescences: interactive flower modeling using botanical structural constraints","authors":"Takashi Ijiri, Shigeru Owada, Makoto Okabe, Takeo Igarashi","author_ids":"2524158, 2452580, 3102663, 1717356","abstract":"We present a system for modeling flowers in three dimensions quickly and easily while preserving correct botanical structures. We use <i>floral diagrams</i> and <i>inflorescences</i>, which were developed by botanists to concisely describe structural information of flowers. Floral diagrams represent the layout of floral components on a single flower, while inflorescences are arrangements of multiple flowers. Based on these notions, we created a simple user interface that is specially tailored to flower editing, while retaining a maximum variety of generable models. We also provide sketching interfaces to define the geometries of floral components. Separation of structural editing and editing of geometry makes the authoring process more flexible and efficient. We found that even novice users could easily design various flower models using our technique. Our system is an example of application-customized sketching, illustrating the potential power of a sketching interface that is carefully designed for a specific application.","cites":"59","conferencePercentile":"38.30645161"},{"venue":"ACM Trans. Graph.","id":"50249b0398700c255a6e25aa4ff64ed518851eee","venue_1":"ACM Trans. Graph.","year":"2004","title":"Volumetric illustration: designing 3D models with internal textures","authors":"Shigeru Owada, Frank Nielsen, Makoto Okabe, Takeo Igarashi","author_ids":"2452580, 1751894, 3102663, 1717356","abstract":"This paper presents an interactive system for designing and browsing volumetric illustrations. Volumetric illustrations are 3D models with internal textures that the user can browse by cutting the models at desired locations. To assign internal textures to a surface mesh, the designer cuts the mesh and provides simple guiding information to specify the correspondence between the cross-section and a reference 2D image. The guiding information is stored with the geometry and used during the synthesis of cross-sectional textures. The key idea is to synthesize a plausible cross-sectional image using a 2D texture-synthesis technique, instead of sampling from a complete 3D RGB volumetric representation directly. This simplifies the design interface and reduces the amount of data, making it possible for non-experts to rapidly design and use volumetric illustrations. We believe that our system can enrich human communications in various domains, such as medicine, biology, and geology.","cites":"58","conferencePercentile":"27.7173913"},{"venue":"ACM Trans. Graph.","id":"24c3537cec099e887e1e298a21fbd8885b77f062","venue_1":"ACM Trans. Graph.","year":"2012","title":"Co-abstraction of shape collections","authors":"Mehmet Ersin Yümer, Levent Burak Kara","author_ids":"2396667, 1808848","abstract":"We present a co-abstraction method that takes as input a collection of 3D objects, and produces a mutually consistent and individually identity-preserving abstraction of each object. In general, an abstraction is a simpler version of a shape that preserves its main characteristics. We hypothesize, however, that there is no single abstraction of an object. Instead, there is a variety of possible abstractions, and an admissible one can only be chosen conjointly with other objects' abstractions. To this end, we introduce a new approach that hierarchically generates a spectrum of abstractions for each model in a shape collection. Given the spectra, we compute the appropriate abstraction level for each model such that shape simplification and inter-set consistency are collectively maximized, while individual shape identities are preserved.","cites":"19","conferencePercentile":"56.81818182"},{"venue":"ACM Trans. Graph.","id":"825b5de1a2f23fd46533eb49ed7f816b27f069a4","venue_1":"ACM Trans. Graph.","year":"2014","title":"Co-constrained handles for deformation in shape collections","authors":"Mehmet Ersin Yümer, Levent Burak Kara","author_ids":"2396667, 1808848","abstract":"We present a method for learning custom deformation handles for an object, from a co-analysis of similar objects. Our approach identifies the geometric and spatial constraints among the different parts of an object, and makes this information available through abstract shape handles. These handles allow the user to prescribe arbitrary deformation directives including free-form surface deformations. However, only a subset of admissible deformations is enabled to the user as learned from the constraint space. Example applications are presented in shape editing, co-deformation and style transfer.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"32f14eb039865fc6cf6ba0b40fb55d16e826c2f3","venue_1":"ACM Trans. Graph.","year":"2010","title":"A radiative transfer framework for rendering materials with anisotropic structure","authors":"Wenzel Jakob, Adam Arbree, Jonathan T. Moon, Kavita Bala, Steve Marschner","author_ids":"1780369, 3146982, 7303700, 8261370, 2593798","abstract":"The radiative transfer framework that underlies all current rendering of volumes is limited to scattering media whose properties are invariant to rotation. Many systems allow for \"anisotropic scattering,\" in the sense that scattered intensity depends on the scattering angle, but the standard equation assumes that the structure of the medium is isotropic. This limitation impedes physics-based rendering of volume models of cloth, hair, skin, and other important volumetric or translucent materials that do have anisotropic structure. This paper presents an end-to-end formulation of physics-based volume rendering of anisotropic scattering structures, allowing these materials to become full participants in global illumination simulations.\n We begin with a generalized radiative transfer equation, derived from scattering by oriented non-spherical particles. Within this framework, we propose a new volume scattering model analogous to the well-known family of microfacet surface reflection models; we derive an anisotropic diffusion approximation, including the weak form required for finite element solution and a way to compute the diffusion matrix from the parameters of the scattering model; and we also derive a new anisotropic dipole BSSRDF for anisotropic translucent materials. We demonstrate results from Monte Carlo, finite element, and dipole simulations. All these contributions are readily implemented in existing rendering systems for volumes and translucent materials, and they all reduce to the standard practice in the isotropic case.","cites":"33","conferencePercentile":"58.47953216"},{"venue":"ACM Trans. Graph.","id":"162e8c137b7c2f877133e93e1ba871485c9423a3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Capturing hair assemblies fiber by fiber","authors":"Wenzel Jakob, Jonathan T. Moon, Steve Marschner","author_ids":"1780369, 7303700, 2593798","abstract":"Hair models for computer graphics consist of many curves representing individual hair fibers. In current practice these curves are generated by ad hoc random processes, and in close-up views their arrangement appears plainly different from real hair. To begin improving this situation, this paper presents a new method for measuring the detailed arrangement of fibers in a hair assembly. Many macrophotographs with shallow depth of field are taken of a sample of hair, sweeping the plane of focus through the hair's volume. The shallow depth of field helps isolate the fibers and reduces occlusion. Several sweeps are performed with the hair at different orientations, resulting in multiple observations of most of the clearly visible fibers. The images are filtered to detect the fibers, and the resulting feature data from all images is used jointly in a hair growing process to construct smooth curves along the observed fibers. Finally, additional hairs are generated to fill in the unseen volume inside the hair. The method is demonstrated on both straight and wavy hair, with results suitable for realistic close-up renderings. These models provide the first views we know of into the 3D arrangement of hair fibers in real hair assemblies.","cites":"23","conferencePercentile":"33.70165746"},{"venue":"ACM Trans. Graph.","id":"0fd71e41ccd3ae752ed3e1137ddc456409662968","venue_1":"ACM Trans. Graph.","year":"2008","title":"Simulating knitted cloth at the yarn level","authors":"Jonathan M. Kaldor, Doug L. James, Steve Marschner","author_ids":"3316406, 1739671, 2593798","abstract":"Knitted fabric is widely used in clothing because of its unique and stretchy behavior, which is fundamentally different from the behavior of woven cloth. The properties of knits come from the nonlinear, three-dimensional kinematics of long, inter-looping yarns, and despite significant advances in cloth animation we still do not know how to simulate knitted fabric faithfully. Existing cloth simulators mainly adopt elastic-sheet mechanical models inspired by woven materials, focusing less on the model itself than on important simulation challenges such as efficiency, stability, and robustness. We define a new computational model for knits in terms of the motion of yarns, rather than the motion of a sheet. Each yarn is modeled as an inextensible, yet otherwise flexible, B-spline tube. To simulate complex knitted garments, we propose an implicit-explicit integrator, with yarn inextensibility constraints imposed using efficient projections. Friction among yarns is approximated using rigid-body velocity filters, and key yarn-yarn interactions are mediated by stiff penalty forces. Our results show that this simple model predicts the key mechanical properties of different knits, as demonstrated by qualitative comparisons to observed deformations of actual samples in the laboratory, and that the simulator can scale up to substantial animations with complex dynamic motion.","cites":"42","conferencePercentile":"52.77777778"},{"venue":"ACM Trans. Graph.","id":"1e9f15de3ec3ef8200f7c42b681b7b1382485e96","venue_1":"ACM Trans. Graph.","year":"2008","title":"Efficient multiple scattering in hair using spherical harmonics","authors":"Jonathan T. Moon, Bruce Walter, Steve Marschner","author_ids":"7303700, 2540721, 2593798","abstract":"Previous research has shown that a global multiple scattering simulation is needed to achieve physically realistic renderings of hair, particularly light-colored hair with low absorption. However, previous methods have either sacrificed accuracy or have been too computationally expensive for practical use. In this paper we describe a physically based, volumetric rendering method that computes multiple scattering solutions, including directional effects, much faster than previous accurate methods. Our two-pass method first traces light paths through a volumetric representation of the hair, contributing power to a 3D grid of spherical harmonic coefficients that store the directional distribution of scattered radiance everywhere in the hair volume. Then, in a ray tracing pass, multiple scattering is computed by integrating the stored radiance against the scattering functions of visible fibers using an efficient matrix multiplication. Single scattering is computed using conventional direct illumination methods. In our comparisons the new method produces quality similar to that of the best previous methods, but computes multiple scattering more than 10 times faster.","cites":"27","conferencePercentile":"28.08641975"},{"venue":"ACM Trans. Graph.","id":"99627d1d7a8074c837e967e4b7b979abaf0b09f9","venue_1":"ACM Trans. Graph.","year":"2006","title":"Simulating multiple scattering in hair using a photon mapping approach","authors":"Jonathan T. Moon, Steve Marschner","author_ids":"7303700, 2593798","abstract":"Simulating multiple scattering correctly is important for accurate rendering of hair. However, a volume of hair is a difficult scene to simulate because scattering from an individual fiber is very structured and forward directed, and because the radiance distributions that arise from many such scattering events remain quite directional. For these reasons, previous methods cannot compute accurate images substantially faster than Monte Carlo path tracing.This paper proposes a new physically accurate method for rendering hair that is based on previous volumetric photon mapping methods. The first pass generates a photon map by tracing particles through the hair geometry, depositing them along paths rather than at scattering events. The second pass ray traces the hair, computing direct illumination and looking up indirect radiance in the photon map. Photons are stored and looked up in 5D position-direction space to allow for the very directional radiance distributions that occur in hair. Together with a new radiance caching method for fibers, our method simulates difficult scattering problems in hair efficiently and with low noise.The new algorithm is validated against path tracing and also compared with a photograph of light scattering in real hair.","cites":"44","conferencePercentile":"32.40740741"},{"venue":"ACM Trans. Graph.","id":"238f7fc96bd33a7e99843a72c2cec5b035b3ec5b","venue_1":"ACM Trans. Graph.","year":"2005","title":"As-rigid-as-possible shape manipulation","authors":"Takeo Igarashi, Tomer Moscovich, John F. Hughes","author_ids":"1717356, 2966785, 2057964","abstract":"We present an interactive system that lets a user move and deform a two-dimensional shape without manually establishing a skeleton or freeform deformation (FFD) domain beforehand. The shape is represented by a triangle mesh and the user moves several vertices of the mesh as constrained handles. The system then computes the positions of the remaining free vertices by minimizing the distortion of each triangle. While physically based simulation or iterative refinement can also be used for this purpose, they tend to be slow. We present a two-step closed-form algorithm that achieves real-time interaction. The first step finds an appropriate rotation for each triangle and the second step adjusts its scale. The key idea is to use quadratic error metrics so that each minimization problem becomes a system of linear equations. After solving the simultaneous equations at the beginning of interaction, we can quickly find the positions of free vertices during interactive manipulation. Our approach successfully conveys a sense of rigidity of the shape, which is difficult in space-warp approaches. With a multiple-point input device, even beginners can easily move, rotate, and deform shapes at will.","cites":"272","conferencePercentile":"97.58064516"},{"venue":"ACM Trans. Graph.","id":"04e9edfe06647c6b0ce716f7cab97cecb57890ce","venue_1":"ACM Trans. Graph.","year":"2010","title":"3D modeling with silhouettes","authors":"Alec R. Rivers, Frédo Durand, Takeo Igarashi","author_ids":"1913819, 1728125, 1717356","abstract":"We present a new sketch-based modeling approach in which models are interactively designed by drawing their 2D silhouettes from different views. The core idea of our paper is to limit the input to 2D silhouettes, removing the need to explicitly create or position 3D elements. Arbitrarily complex models can be constructed by assembling them out of parts defined by their silhouettes, which can be combined using CSG operations. We introduce a new simplified algorithm to compute CSG solids that leverages special properties of silhouette cylinders to convert the 3D CSG problem into one that can be handled entirely with 2D operations, making implementation simpler and more robust. We evaluate our approach by modeling a random sampling of man-made objects taken from the words in WordNet, and show that all of the tested man-made objects can be modeled quickly and easily using our approach.","cites":"26","conferencePercentile":"46.19883041"},{"venue":"ACM Trans. Graph.","id":"44e1b348e35ac871929a8d765904f901dfc93c94","venue_1":"ACM Trans. Graph.","year":"2010","title":"Volumetric modeling with diffusion surfaces","authors":"Kenshi Takayama, Olga Sorkine-Hornung, Andrew Nealen, Takeo Igarashi","author_ids":"1996486, 2250001, 1729677, 1717356","abstract":"The modeling of volumetric objects is still a difficult problem. Solid texture synthesis methods enable the design of volumes with homogeneous textures, but global features such as smoothly varying colors seen in vegetables and fruits are difficult to model. In this paper, we propose a representation called <i>diffusion surfaces</i> (DSs) to enable modeling such objects. DSs consist of 3D surfaces with colors defined on both sides, such that the interior colors in the volume are obtained by diffusing colors from nearby surfaces. A straightforward way to compute color diffusion is to solve a volumetric Poisson equation with the colors of the DSs as boundary conditions, but it requires expensive volumetric meshing which is not appropriate for interactive modeling. We therefore propose to interpolate colors only locally at user-defined cross-sections using a modified version of the positive mean value coordinates algorithm to avoid volumetric meshing. DSs are generally applicable to model many different kinds of objects with internal structures. As a case study, we present a simple sketch-based interface for modeling objects with rotational symmetries that can also generate random variations of models. We demonstrate the effectiveness of our approach through various DSs models with simple non-photorealistic rendering techniques enabled by DSs.","cites":"28","conferencePercentile":"48.83040936"},{"venue":"ACM Trans. Graph.","id":"3c6e21da03d8bd474681d52bd76adc624f285867","venue_1":"ACM Trans. Graph.","year":"2012","title":"Structure-aware synthesis for predictive woven fabric appearance","authors":"Shuang Zhao, Wenzel Jakob, Steve Marschner, Kavita Bala","author_ids":"2373908, 1780369, 2593798, 8261370","abstract":"Woven fabrics have a wide range of appearance determined by their small-scale 3D structure. Accurately modeling this structural detail can produce highly realistic renderings of fabrics and is critical for predictive rendering of fabric appearance. But building these yarn-level volumetric models is challenging. Procedural techniques are manually intensive, and fail to capture the naturally arising irregularities which contribute significantly to the overall appearance of cloth. Techniques that acquire the detailed 3D structure of real fabric samples are constrained only to model the scanned samples and cannot represent different fabric designs.\n This paper presents a new approach to creating volumetric models of woven cloth, which starts with user-specified fabric designs and produces models that correctly capture the yarn-level structural details of cloth. We create a small database of volumetric exemplars by scanning fabric samples with simple weave structures. To build an output model, our method synthesizes a new volume by copying data from the exemplars at each yarn crossing to match a weave pattern that specifies the desired output structure. Our results demonstrate that our approach generalizes well to complex designs and can produce highly realistic results at both large and small scales.","cites":"9","conferencePercentile":"15.15151515"},{"venue":"ACM Trans. Graph.","id":"7b43040dd2fbf5b27de4179a623bb0db9a78682a","venue_1":"ACM Trans. Graph.","year":"2011","title":"Modular Radiance Transfer","authors":"Brad Loos, Lakulish Antani, Kenny Mitchell, Derek Nowrouzezahrai, Wojciech Jarosz, Peter-Pike J. Sloan","author_ids":"3089153, 2747880, 3315742, 1795014, 1953515, 2838682","abstract":"Many rendering algorithms willingly sacrifice accuracy, favoring plausible shading with high-performance. Modular Radiance Transfer (MRT) models coarse-scale, distant indirect lighting effects in scene geometry that scales from high-end GPUs to low-end mobile platforms. MRT eliminates scene-dependent precomputation by storing compact transport on simple shapes, akin to bounce cards used in film production. These shapes' modular transport can be instanced, warped and connected on-the-fly to yield approximate light transport in large scenes. We introduce a prior on incident lighting distributions and perform all computations in low-dimensional subspaces. An <i>implicit lighting environment</i> induced from the low-rank approximations is in turn used to model secondary effects, such as volumetric transport variation, higher-order irradiance, and transport through lightfields. MRT is a new approach to precomputed lighting that uses a novel low-dimensional subspace simulation of light transport to uniquely balance the need for high-performance and portable solutions, low memory usage, and fast authoring iteration.","cites":"15","conferencePercentile":"34.47368421"},{"venue":"ACM Trans. Graph.","id":"4ca77ca0fd8a96df886f9cce6a17598442955744","venue_1":"ACM Trans. Graph.","year":"2014","title":"Anisotropic simplicial meshing using local convex functions","authors":"Xiao-Ming Fu, Yang Liu, John Snyder, Baining Guo","author_ids":"7780643, 1750084, 6314473, 2738456","abstract":"We present a novel method to generate high-quality simplicial meshes with specified anisotropy. Given a surface or volumetric domain equipped with a Riemannian metric that encodes the desired anisotropy, we transform the problem to one of functional approximation. We construct a convex function over each mesh simplex whose Hessian locally matches the Riemannian metric, and iteratively adapt vertex positions and mesh connectivity to minimize the difference between the target convex functions and their piecewise-linear interpolation over the mesh. Our method generalizes optimal Delaunay triangulation and leads to a simple and efficient algorithm. We demonstrate its quality and speed compared to state-of-the-art methods on a variety of domains and metrics.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"2b24bb63b38e14a404c0d4839aa3ff23ab185e76","venue_1":"ACM Trans. Graph.","year":"2005","title":"Modeling hair from multiple views","authors":"Yichen Wei, Eyal Ofek, Long Quan, Harry Shum","author_ids":"1732264, 1735652, 1722826, 1698102","abstract":"In this paper, we propose a novel image-based approach to model hair geometry from images taken at multiple viewpoints. Unlike previous hair modeling techniques that require intensive user interactions or rely on special capturing setup under controlled illumination conditions, we use a handheld camera to capture hair images under uncontrolled illumination conditions. Our multi-view approach is natural and flexible for capturing. It also provides inherent strong and accurate geometric constraints to recover hair models.In our approach, the hair fibers are synthesized from local image orientations. Each synthesized fiber segment is validated and optimally triangulated from all visible views. The hair volume and the visibility of synthesized fibers can also be reliably estimated from multiple views. Flexibility of acquisition, little user interaction, and high quality results of recovered complex hair models are the key advantages of our method.","cites":"56","conferencePercentile":"34.27419355"},{"venue":"ACM Trans. Graph.","id":"78bd14e4324db6498e88491d4d3698ed9d75bac4","venue_1":"ACM Trans. Graph.","year":"2015","title":"Parametric self-supporting surfaces via direct computation of airy stress functions","authors":"Masaaki Miki, Takeo Igarashi, Philippe Block","author_ids":"2179083, 1717356, 2068197","abstract":"This paper presents a method that employs parametric surfaces as surface geometry representations at any stage of a computational process to compute self-supporting surfaces. This approach can be differentiated from existing relevant methods because such methods represent surfaces by a triangulated mesh surface or a network consisting of lines. The proposed method is based on the theory of Airy stress functions. Although some existing methods are also based on this theory, they apply its discrete version to discrete geometries. The proposed method simultaneously applies the theory to parametric surfaces directly and the discrete theory to the edges of parametric patches. The discontinuous boundary between continuous patches naturally corresponds to ribs seen in traditional vault masonry buildings. We use nonuniform rational B-spline surfaces in this study; however, the basic idea can be applied to other parametric surfaces. A variety of self-supporting surfaces obtained by the proposed computational scheme is presented.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"bcacdd245b8e792083f09d1773577ba010df4253","venue_1":"ACM Trans. Graph.","year":"2015","title":"Matching Real Fabrics with Micro-Appearance Models","authors":"Pramook Khungurn, Daniel Schroeder, Shuang Zhao, Kavita Bala, Steve Marschner","author_ids":"2934153, 8085736, 2373908, 8261370, 2593798","abstract":"<i>Micro-appearance models</i> explicitly model the interaction of light with microgeometry at the fiber scale to produce realistic appearance. To effectively match them to real fabrics, we introduce a new appearance matching framework to determine their parameters. Given a micro-appearance model and photographs of the fabric under many different lighting conditions, we optimize for parameters that best match the photographs using a method based on calculating derivatives during rendering. This highly applicable framework, we believe, is a useful research tool because it simplifies development and testing of new models.\n Using the framework, we systematically compare several types of micro-appearance models. We acquired computed microtomography (micro CT) scans of several fabrics, photographed the fabrics under many viewing/illumination conditions, and matched several appearance models to this data. We compare a new fiber-based light scattering model to the previously used microflake model. We also compare representing cloth microgeometry using volumes derived directly from the micro CT data to using explicit fibers reconstructed from the volumes. From our comparisons, we make the following conclusions: (1) given a fiber-based scattering model, volume- and fiber-based microgeometry representations are capable of very similar quality, and (2) using a fiber-specific scattering model is crucial to good results as it achieves considerably higher accuracy than prior work.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"2c38aa009ced53f8d65a8f445395077577851c98","venue_1":"ACM Trans. Graph.","year":"2015","title":"Architecture-scale human-assisted additive manufacturing","authors":"Hironori Yoshida, Takeo Igarashi, Yusuke Obuchi, Yosuke Takami, Jun Sato, Mika Araki, Masaaki Miki, Kosuke Nagata, Kazuhide Sakai, Syunsuke Igarashi","author_ids":"3145265, 1717356, 2017025, 3261268, 1736478, 2926584, 2179083, 1986809, 8731825, 2444499","abstract":"Recent digital fabrication tools have opened up accessibility to personalized rapid prototyping; however, such tools are limited to product-scale objects. The materials currently available for use in 3D printing are too fine for large-scale objects, and CNC gantry sizes limit the scope of printable objects. In this paper, we propose a new method for printing architecture-scale objects. Our proposal includes three developments: (i) a construction material consisting of chopsticks and glue, (ii) a handheld chopstick dispenser, and (iii) a printing guidance system that uses projection mapping. The proposed chopstickglue material is cost effective, environmentally sustainable, and can be printed more quickly than conventional materials. The developed handheld dispenser enables consistent feeding of the chopstickglue material composite. The printing guidance system --- consisting of a depth camera and a projector --- evaluates a given shape in real time and indicates where humans should deposit chopsticks by projecting a simple color code onto the form under construction. Given the mechanical specifications of the stickglue composite, an experimental pavilion was designed as a case study of the proposed method and built without scaffoldings and formworks. The case study also revealed several fundamental limitations, such as the projector does not work in daylight, which requires future investigations.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"0ec9a616c16198403da7311acd72ab0b91da8416","venue_1":"ACM Trans. Graph.","year":"2014","title":"Flower modeling via X-ray computed tomography","authors":"Takashi Ijiri, Shin Yoshizawa, Hideo Yokota, Takeo Igarashi","author_ids":"2524158, 2262972, 2604837, 1717356","abstract":"This paper presents a novel three dimensional (3D) flower modeling technique that utilizes an X-ray computed tomography (CT) system and real-world flowers. Although a CT system provides volume data that captures the internal structures of flowers, it is difficult to accurately segment them into regions of particular organs and model them as smooth surfaces because a flower consists of thin organs that contact one another. We thus introduce a semi-automatic modeling technique that is based on a new active contour model with energy functionals designed for flower CT. Our key idea is to approximate flower components by two important primitives, a shaft and a sheet. Based on our active contour model, we also provide novel user interfaces and a numerical scheme to fit these primitives so as to reconstruct realistic thin flower organs efficiently. To demonstrate the feasibility of our technique, we provide various flower models reconstructed from CT volumes.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"e397f35ebcec07f4b258c0cdbb1ce2f893e3d70e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Multi-scale modeling and rendering of granular materials","authors":"Johannes Meng, Marios Papas, Ralf Habel, Carsten Dachsbacher, Steve Marschner, Markus H. Gross, Wojciech Jarosz","author_ids":"1945844, 3112836, 3346419, 1705803, 2593798, 1743207, 1953515","abstract":"We address the problem of modeling and rendering granular materials---such as large structures made of sand, snow, or sugar---where an aggregate object is composed of many randomly oriented, but discernible grains. These materials pose a particular challenge as the complex scattering properties of individual grains, and their packing arrangement, can have a dramatic effect on the large-scale appearance of the aggregate object. We propose a multi-scale modeling and rendering framework that adapts to the structure of scattered light at different scales. We rely on path tracing the individual grains only at the finest scale, and---by decoupling individual grains from their arrangement---we develop a modular approach for simulating longer-scale light transport. We model light interactions within and across grains as separate processes and leverage this decomposition to derive parameters for classical radiative transport, including standard volumetric path tracing and a diffusion method that can quickly summarize the large scale transport due to many grain interactions. We require only a one-time precomputation per exemplar grain, which we can then reuse for arbitrary aggregate shapes and a continuum of different packing rates and scales of grains. We demonstrate our method on scenes containing mixtures of tens of millions of individual, complex, specular grains that would be otherwise infeasible to render with standard techniques.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"4c4cf358dc0eaec0f3a9f4ffd46e20d5fc7e122f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Pteromys: interactive design and optimization of free-formed free-flight model airplanes","authors":"Nobuyuki Umetani, Yuki Koyama, Ryan Schmidt, Takeo Igarashi","author_ids":"2065148, 2196816, 2291899, 1717356","abstract":"This paper introduces novel interactive techniques for designing original hand-launched free-flight glider airplanes which can actually fly. The aerodynamic properties of a glider aircraft depend on their shape, imposing significant design constraints. We present a compact and efficient representation of glider aerodynamics that can be fit to real-world conditions using a data-driven method. To do so, we acquire a sample set of glider flight trajectories using a video camera and the system learns a nonlinear relationship between forces on the wing and wing shape. Our acquisition system is much simpler to construct than a wind tunnel, but using it we can efficiently discover a wing model for simple gliding aircraft. Our resulting model can handle general free-form wing shapes and yet agrees sufficiently well with the acquired airplane flight trajectories. Based on this compact aerodynamics model, we present a design tool in which the wing configuration created by a user is interactively optimized to maximize flight-ability. To demonstrate the effectiveness of our tool for glider design by novice users, we compare it with a traditional design workflow.","cites":"20","conferencePercentile":"88.88888889"},{"venue":"ACM Trans. Graph.","id":"36fa36bef94239ccffda5c8424438d7453a582ab","venue_1":"ACM Trans. Graph.","year":"2011","title":"Sensitive couture for interactive garment modeling and editing","authors":"Nobuyuki Umetani, Danny M. Kaufman, Takeo Igarashi, Eitan Grinspun","author_ids":"2065148, 2972719, 1717356, 7522998","abstract":"We present a novel interactive tool for garment design that enables, for the first time, interactive bidirectional editing between 2D patterns and 3D high-fidelity simulated draped forms. This provides a continuous, interactive, and natural design modality in which 2D and 3D representations are simultaneously visible and seamlessly maintain correspondence. Artists can now interactively edit 2D pattern designs and immediately obtain stable accurate feedback online, thus enabling rapid prototyping and an intuitive understanding of complex drape form.","cites":"52","conferencePercentile":"86.57894737"},{"venue":"ACM Trans. Graph.","id":"562d3e9cc3573bb49df1bf257f742e21e907b039","venue_1":"ACM Trans. Graph.","year":"2011","title":"Beady: interactive beadwork design and construction","authors":"Yuki Igarashi, Takeo Igarashi, Jun Mitani","author_ids":"1694743, 1717356, 2618827","abstract":"We introduce the interactive system \"Beady\" to assist the design and construction of customized 3D beadwork. The user first creates a polygonal mesh model called the design model that represents the overall structure of the beadwork. Each edge of the mesh model corresponds to a bead in the beadwork. We provide two methods to create the design model. One is interactive modeling from scratch. The user defines the mesh topology with gestural interaction and the system continuously adjusts edge lengths by considering the physical constraints among neighboring beads. The other is automatic conversion that takes an existing polygonal model as input and generates a near-hexagonal mesh model with a near-uniform edge length as output. The system then converts the design model into a beadwork model with the appropriate wiring. Computation of an appropriate wiring path requires careful consideration, and we present an algorithm based on face stripification of the mesh. The system also provides a visual step-by-step guide to assist the manual beadwork construction process. We show several beadwork designs constructed by the authors and by test users using the system.","cites":"15","conferencePercentile":"34.47368421"},{"venue":"ACM Trans. Graph.","id":"7726963f67d3a36e379582fcbcc5102bc9f38829","venue_1":"ACM Trans. Graph.","year":"2014","title":"Rendering glints on high-resolution normal-mapped specular surfaces","authors":"Ling-Qi Yan, Milos Hasan, Wenzel Jakob, Jason Lawrence, Steve Marschner, Ravi Ramamoorthi","author_ids":"2162776, 2545444, 1780369, 1694005, 2593798, 1752236","abstract":"Complex specular surfaces under sharp point lighting show a fascinating glinty appearance, but rendering it is an unsolved problem. Using Monte Carlo pixel sampling for this purpose is impractical: the energy is concentrated in tiny highlights that take up a minuscule fraction of the pixel. We instead compute an accurate solution using a completely different deterministic approach. Our method considers the true distribution of normals on a surface patch seen through a single pixel, which can be highly complex. We show how to evaluate this distribution efficiently, assuming a Gaussian pixel footprint and Gaussian intrinsic roughness. We also take advantage of hierarchical pruning of position-normal space to rapidly find texels that might contribute to a given normal distribution evaluation. Our results show complex, temporally varying glints from materials such as bumpy plastics, brushed and scratched metals, metallic paint and ocean waves.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"02234ba940f3da9d0422af59737b70d8203dbab7","venue_1":"ACM Trans. Graph.","year":"2010","title":"Fabricating spatially-varying subsurface scattering","authors":"Yue Dong, Jiaping Wang, Fabio Pellacini, Xin Tong, Baining Guo","author_ids":"1744268, 4912907, 1757883, 1743927, 2738456","abstract":"Many real world surfaces exhibit translucent appearance due to subsurface scattering. Although various methods exists to measure, edit and render subsurface scattering effects, no solution exists for manufacturing physical objects with desired translucent appearance. In this paper, we present a complete solution for fabricating a material volume with a desired surface BSSRDF. We stack layers from a fixed set of manufacturing materials whose thickness is varied spatially to reproduce the heterogeneity of the input BSSRDF. Given an input BSSRDF and the optical properties of the manufacturing materials, our system efficiently determines the optimal order and thickness of the layers. We demonstrate our approach by printing a variety of homogenous and heterogenous BSSRDFs using two hardware setups: a milling machine and a 3D printer.","cites":"52","conferencePercentile":"86.5497076"},{"venue":"ACM Trans. Graph.","id":"483c595e97b1f47668e5fa6d04c5fb36de3cf09b","venue_1":"ACM Trans. Graph.","year":"2011","title":"Sketch-based Dynamic Illustration of Fluid Systems","authors":"Bo Zhu, Michiaki Iwata, Ryo Haraguchi, Takashi Ashihara, Nobuyuki Umetani, Takeo Igarashi, Kazuo Nakazawa","author_ids":"1707836, 1918600, 2907297, 2698744, 2065148, 1717356, 3105581","abstract":"This paper presents a lightweight sketching system that enables interactive illustration of complex fluid systems. Users can sketch on a 2.5-dimensional (2.5D) canvas to design the shapes and connections of a fluid circuit. These input sketches are automatically analyzed and abstracted into a hydraulic graph, and a new hybrid fluid model is used in the background to enhance the illustrations. The system provides rich simple operations for users to edit the fluid system incrementally, and the new internal flow patterns can be simulated in real time. Our system is used to illustrate various fluid systems in medicine, biology, and engineering. We asked professional medical doctors to try our system and obtained positive feedback from them.","cites":"3","conferencePercentile":"3.947368421"},{"venue":"ACM Trans. Graph.","id":"1a7e1f6e2f7d1061b8e8596485fe2ccdf11f4024","venue_1":"ACM Trans. Graph.","year":"2014","title":"Discrete stochastic microfacet models","authors":"Wenzel Jakob, Milos Hasan, Ling-Qi Yan, Jason Lawrence, Ravi Ramamoorthi, Steve Marschner","author_ids":"1780369, 2545444, 2162776, 1694005, 1752236, 2593798","abstract":"This paper investigates rendering glittery surfaces, ones which exhibit shifting random patterns of glints as the surface or viewer moves. It applies both to dramatically glittery surfaces that contain mirror-like flakes and also to rough surfaces that exhibit more subtle small scale glitter, without which most glossy surfaces appear too smooth in close-up. These phenomena can in principle be simulated by high-resolution normal maps, but maps with tiny features create severe aliasing problems under narrow-angle illumination. In this paper we present a stochastic model for the effects of random subpixel structures that generates glitter and spatial noise that behave correctly under different illumination conditions and viewing distances, while also being temporally coherent so that they look right in motion. The model is based on microfacet theory, but it replaces the usual continuous microfacet distribution with a discrete distribution of scattering particles on the surface. A novel stochastic hierarchy allows efficient evaluation in the presence of large numbers of random particles, without ever having to consider the particles individually. This leads to a multiscale procedural BRDF that is readily implemented in standard rendering systems, and which converges back to the smooth case in the limit.","cites":"10","conferencePercentile":"61.72839506"},{"venue":"ACM Trans. Graph.","id":"87d9b850bec271c9884724238ea9a16b536c471e","venue_1":"ACM Trans. Graph.","year":"2010","title":"2.5D cartoon models","authors":"Alec R. Rivers, Takeo Igarashi, Frédo Durand","author_ids":"1913819, 1717356, 1728125","abstract":"We present a way to bring cartoon objects and characters into the third dimension, by giving them the ability to rotate and be viewed from any angle. We show how 2D vector art drawings of a cartoon from different views can be used to generate a novel structure, the 2.5D cartoon model, which can be used to simulate 3D rotations and generate plausible renderings of the cartoon from any view. 2.5D cartoon models are easier to create than a full 3D model, and retain the 2D nature of hand-drawn vector art, supporting a wide range of stylizations that need not correspond to any real 3D shape.","cites":"24","conferencePercentile":"41.8128655"},{"venue":"ACM Trans. Graph.","id":"5c75696de7c9937f9b010efc274da7b456f8064d","venue_1":"ACM Trans. Graph.","year":"2014","title":"A comprehensive framework for rendering layered materials","authors":"Wenzel Jakob, Eugene d'Eon, Otto Jakob, Steve Marschner","author_ids":"1780369, 1712604, 2198748, 2593798","abstract":"We present a general and practical method for computing BSDFs of layered materials. Its ingredients are transport-theoretical models of isotropic or anisotropic scattering layers and smooth or rough boundaries of conductors and dielectrics. Following expansion into a directional basis that supports arbitrary composition, we are able to efficiently and accurately synthesize BSDFs for a great variety of layered structures.\n Reflectance models created by our system correctly account for multiple scattering within and between layers, and in the context of a rendering system they are efficient to evaluate and support texturing and exact importance sampling. Although our approach essentially involves tabulating reflectance functions in a Fourier basis, the generated models are compact to store due to the inherent sparsity of our representation, and are accurate even for narrowly peaked functions. While methods for rendering general layered surfaces have been investigated in the past, ours is the first system that supports arbitrary layer structures while remaining both efficient and accurate.\n We validate our model by comparing to measurements of real-world examples of layered materials, and we demonstrate an interactive visual design tool that enables easy exploration of the space of layered materials. We provide a fully practical, high-performance implementation in an open-source rendering system.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"962ad20c7bf01304b06fedd9b8b4ffc56dd61ee4","venue_1":"ACM Trans. Graph.","year":"2012","title":"Stitch meshes for modeling knitted clothing with yarn-level detail","authors":"Cem Yuksel, Jonathan M. Kaldor, Doug L. James, Steve Marschner","author_ids":"1775014, 3316406, 1739671, 2593798","abstract":"Recent yarn-based simulation techniques permit realistic and efficient dynamic simulation of knitted clothing, but producing the required yarn-level models remains a challenge. The lack of practical modeling techniques significantly limits the diversity and complexity of knitted garments that can be simulated. We propose a new modeling technique that builds yarn-level models of complex knitted garments for virtual characters. We start with a polygonal model that represents the large-scale surface of the knitted cloth. Using this mesh as an input, our interactive modeling tool produces a finer mesh representing the layout of stitches in the garment, which we call the <i>stitch mesh</i>. By manipulating this mesh and assigning stitch types to its faces, the user can replicate a variety of complicated knitting patterns. The curve model representing the yarn is generated from the stitch mesh, then the final shape is computed by a yarn-level physical simulation that locally relaxes the yarn into realistic shape while preserving global shape of the garment and avoiding \"yarn pull-through,\" thereby producing valid yarn geometry suitable for dynamic simulation. Using our system, we can efficiently create yarn-level models of knitted clothing with a rich variety of patterns that would be completely impractical to model using traditional techniques. We show a variety of example knitting patterns and full-scale garments produced using our system.","cites":"11","conferencePercentile":"25.25252525"},{"venue":"ACM Trans. Graph.","id":"be506d36254663a8f32d44b2b0b1a9fbe0bdeb22","venue_1":"ACM Trans. Graph.","year":"2012","title":"Specular reflection from woven cloth","authors":"Piti Irawan, Steve Marschner","author_ids":"2366112, 2593798","abstract":"The appearance of a particular fabric is produced by variations in both large-scale reflectance and small-scale texture as the viewing and illumination angles change across the surface. This article presents a study of the reflectance and texture of woven cloth that aims to identify and model important optical features of cloth appearance. New measurements are reported for a range of fabrics including natural and synthetic fibers as well as staple and filament yarns. A new scattering model for woven cloth is introduced that describes the reflectance and the texture based on an analysis of specular reflection from the fibers. Unlike data-based models, our procedural model doesn't require image data. It can handle a wide range of fabrics using a small set of physically meaningful parameters that describe the characteristics of the fibers, the geometry of the yarns, and the pattern of the weave. The model is validated against the measurements and evaluated by comparisons to high-resolution video of the real fabrics and to BTF models of two of the fabrics.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"9bbd6aafe0efc447ea70245cc717e04f95a11423","venue_1":"ACM Trans. Graph.","year":"2016","title":"Efficient GPU path rendering using scanline rasterization","authors":"Rui Li, Qiming Hou, Kun Zhou","author_ids":"1704992, 7939453, 6671887","abstract":"We introduce a novel GPU path rendering method based on scan-line rasterization, which is highly work-efficient but traditionally considered as GPU hostile. Our method is parallelized over <i>boundary fragments</i>, i.e., pixels directly intersecting the path boundary. Non-boundary pixels are processed in bulk as horizontal spans like in CPU scanline rasterizers, which saves a significant amount of winding number computation workload. The distinction also allows the majority of our algorithmic steps to focus on boundary fragments only, which leads to highly balanced workload among the GPU threads. In addition, we develop a ray shooting pattern that minimizes the global data dependency when computing winding numbers at anti-aliasing samples. This allows us to shift the majority of winding-number-related workload to the same kernel that consumes its result, which saves a significant amount of GPU memory bandwidth. Experiments show that our method gives a consistent 2.5X speedup over state-of-the-art alternatives for high-quality rendering at Ultra HD resolution, which can increase to more than 30X in extreme cases. We can also get a consistent 10X speedup on animated input.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"a5d4bfc978576d28b50413b85f5bc7eb1dd06bc8","venue_1":"ACM Trans. Graph.","year":"2011","title":"A programmable system for artistic volumetric lighting","authors":"Derek Nowrouzezahrai, Jared M. Johnson, Andrew Selle, Dylan Lacewell, Michael Kaschalk, Wojciech Jarosz","author_ids":"1795014, 3139688, 1714992, 1938537, 1705027, 1953515","abstract":"We present a method for generating art-directable volumetric effects, ranging from physically-accurate to non-physical results. Our system mimics the way experienced artists think about volumetric effects by using an intuitive lighting primitive, and decoupling the <i>modeling</i> and <i>shading</i> of this primitive. To accomplish this, we generalize the physically-based photon beams method to allow arbitrarily programmable simulation and shading phases. This provides an intuitive design space for artists to rapidly explore a wide range of physically-based as well as plausible, but exaggerated, volumetric effects. We integrate our approach into a real-world production pipeline and couple our volumetric effects to surface shading.","cites":"9","conferencePercentile":"15.78947368"},{"venue":"ACM Trans. Graph.","id":"4328d3be9e27f29997bf608f0206068a655c8a7f","venue_1":"ACM Trans. Graph.","year":"2005","title":"Measuring and modeling the appearance of finished wood","authors":"Steve Marschner, Stephen H. Westin, Adam Arbree, Jonathan T. Moon","author_ids":"2593798, 2035584, 3146982, 7303700","abstract":"Wood coated with transparent finish has a beautiful and distinctive appearance that is familiar to everyone. Woods with unusual grain patterns. such as tiger, burl, and birdseye figures, have a strikingly unusual directional reflectance that is prized for decorative applications. With new, high resolution measurements of spatially varying BRDFs. we show that this distinctive appearance is due to light scattering that does not conform to the usual notion of anisotropic surface reflection. The behavior can be explained by scattering from the matrix of wood fibers below the surface, resulting in a subsurface highlight that occurs on a cone with an out-of-plane axis. We propose a new shading model component to handle reflection from subsurface fibers, which is combined with the standard diffuse and specular components to make a complete shading model. Rendered results from fits of our model to the measurement data demonstrate that this new model captures the distinctive appearance of wood.","cites":"44","conferencePercentile":"20.16129032"},{"venue":"ACM Trans. Graph.","id":"760007cd0443d9211b8deb31b587e5976d1aa359","venue_1":"ACM Trans. Graph.","year":"2012","title":"Tools for placing cuts and transitions in interview video","authors":"Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala","author_ids":"2842099, 2812691, 1820412","abstract":"We present a set of tools designed to help editors place cuts and create transitions in interview video. To help place cuts, our interface links a text transcript of the video to the corresponding locations in the raw footage. It also visualizes the suitability of cut locations by analyzing the audio/visual features of the raw footage to find frames where the speaker is relatively quiet and still. With these tools editors can directly highlight segments of text, check if the endpoints are suitable cut locations and if so, simply delete the text to make the edit. For each cut our system generates visible (e.g. jump-cut, fade, etc.) and seamless, hidden transitions. We present a hierarchical, graph-based algorithm for efficiently generating hidden transitions that considers visual features specific to interview footage. We also describe a new data-driven technique for setting the timing of the hidden transition. Finally, our tools offer a one click method for seamlessly removing 'ums' and repeated words as well as inserting natural-looking pauses to emphasize semantic content. We apply our tools to edit a variety of interviews and also show how they can be used to quickly compose multiple takes of an actor narrating a story.","cites":"20","conferencePercentile":"59.84848485"},{"venue":"ACM Trans. Graph.","id":"6ea826f8b8d5b092e639f54adbeb8d65b32a3725","venue_1":"ACM Trans. Graph.","year":"2011","title":"Pocket reflectometry","authors":"Peiran Ren, Jiaping Wang, John Snyder, Xin Tong, Baining Guo","author_ids":"3246404, 4912907, 6314473, 1743927, 2738456","abstract":"We present a simple, fast solution for reflectance acquisition using tools that fit into a pocket. Our method captures video of a flat target surface from a fixed video camera lit by a hand-held, moving, linear light source. After processing, we obtain an SVBRDF.\n We introduce a <i>BRDF chart</i>, analogous to a color \"checker\" chart, which arranges a set of known-BRDF reference tiles over a small card. A sequence of light responses from the chart tiles as well as from points on the target is captured and matched to reconstruct the target's appearance.\n We develop a new algorithm for BRDF reconstruction which works directly on these LDR responses, without knowing the light or camera position, or acquiring HDR lighting. It compensates for spatial variation caused by the local (finite distance) camera and light position by warping responses over time to align them to a specular reference. After alignment, we find an optimal linear combination of the Lambertian and purely specular reference responses to match each target point's response. The same weights are then applied to the corresponding (known) reference BRDFs to reconstruct the target point's BRDF. We extend the basic algorithm to also recover varying surface normals by adding two spherical caps for diffuse and specular references to the BRDF chart.\n We demonstrate convincing results obtained after less than 30 seconds of data capture, using commercial mobile phone cameras in a casual environment.","cites":"8","conferencePercentile":"12.10526316"},{"venue":"ACM Trans. Graph.","id":"31cd7fc13459df315298c20de9661e1b24d91328","venue_1":"ACM Trans. Graph.","year":"2011","title":"Freeform vector graphics with controlled thin-plate splines","authors":"Mark Finch, John Snyder, Hugues Hoppe","author_ids":"3354969, 6314473, 1688461","abstract":"Recent work defines vector graphics using diffusion between colored curves. We explore higher-order fairing to enable more natural interpolation and greater expressive control. Specifically, we build on thin-plate splines which provide smoothness everywhere except at user-specified tears and creases (discontinuities in value and derivative respectively). Our system lets a user sketch discontinuity curves without fixing their colors, and sprinkle color constraints at sparse interior points to obtain smooth interpolation subject to the outlines. We refine the representation with novel contour and slope curves, which anisotropically constrain interpolation derivatives. Compound curves further increase editing power by expanding a single curve into multiple offsets of various basic types (value, tear, crease, slope, and contour). The vector constraints are discretized over an image grid, and satisfied using a hierarchical solver. We demonstrate interactive authoring on a desktop CPU.","cites":"24","conferencePercentile":"53.42105263"},{"venue":"ACM Trans. Graph.","id":"0748bdcdeccc7a29964e27f0be40766fe35529c4","venue_1":"ACM Trans. Graph.","year":"2016","title":"Soli: ubiquitous gesture sensing with millimeter wave radar","authors":"Jaime Lien, Nicholas Edward Gillian, Mustafa Emre Karagozler, Patrick Amihood, Carsten Schwesig, Erik Olson, Hakim Raja, Ivan Poupyrev","author_ids":"3352068, 1998265, 2439281, 1865002, 2270984, 8669784, 2895895, 1736819","abstract":"This paper presents <i>Soli</i>, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.","cites":"5","conferencePercentile":"97.46835443"},{"venue":"ACM Trans. Graph.","id":"a8be2596cbc0a857b33c90a9ae34214833cc76cc","venue_1":"ACM Trans. Graph.","year":"2011","title":"Errata for GPU-Efficient Recursive Filtering and Summed-Area Tables","authors":"Diego F. Nehab, André Maximo, Rodolfo S. Lima, Hugues Hoppe","author_ids":"1764421, 1714345, 1805736, 1688461","abstract":"We have found a few typos in the original descriptions of algorithms 5 and SAT [Nehab et al. 2011]. These mistakes are present only in the textual description, and not in the source code or in the equations. Therefore, no results are affected. Nevertheless, to prevent future confusion, we correct them in red below. 5.2 In parallel for all n, sequentially for each m, compute and store the P m,n (Y) according to (24), using the previously computed P m,n (¯ Y). 5.3 In parallel for all n, sequentially for each m, compute and store E m,n (Z) according to (34) and using the previously computed P m−1,n (Y) and E m,n (ˆ Z). Algorithm SAT S.4 In parallel for all m and n, compute B m,n (Y) then compute and store B m,n (V) according to (41) and using the previously computed P m−1,n (Y) and P T m,n−1 (V).efficient recursive filtering and summed-area tables. ACM Trans.","cites":"1","conferencePercentile":"1.578947368"},{"venue":"ACM Trans. Graph.","id":"26c26cd1529f8010d89cadf1cfe024561d76c2ad","venue_1":"ACM Trans. Graph.","year":"2011","title":"Antialiasing recovery","authors":"Lei Yang, Pedro V. Sander, Jason Lawrence, Hugues Hoppe","author_ids":"3449835, 1730301, 1694005, 1688461","abstract":"We present a method for restoring antialiased edges that are damaged by certain types of nonlinear image filters. This problem arises with many common operations such as intensity thresholding, tone mapping, gamma correction, histogram equalization, bilateral filters, unsharp masking, and certain nonphotorealistic filters. We present a simple algorithm that selectively adjusts the local gradients in affected regions of the filtered image so that they are consistent with those in the original image. Our algorithm is highly parallel and is therefore easily implemented on a GPU. Our prototype system can process up to 500 megapixels per second and we present results for a number of different image filters.","cites":"8","conferencePercentile":"12.10526316"},{"venue":"ACM Trans. Graph.","id":"438525c98a504cdae855fe0ec91754a5d659b2e1","venue_1":"ACM Trans. Graph.","year":"2011","title":"Image-based bidirectional scene reprojection","authors":"Lei Yang, Yu-Chiu Tse, Pedro V. Sander, Jason Lawrence, Diego F. Nehab, Hugues Hoppe, Clara L. Wilkins","author_ids":"3449835, 2323123, 1730301, 1694005, 1764421, 1688461, 3282524","abstract":"We introduce a method for increasing the framerate of real-time rendering applications. Whereas many existing temporal upsampling strategies only reuse information from previous frames, our bidirectional technique reconstructs intermediate frames from a pair of consecutive rendered frames. This significantly improves the accuracy and efficiency of data reuse since very few pixels are simultaneously occluded in both frames. We present two versions of this basic algorithm. The first is appropriate for fill-bound scenes as it limits the number of expensive shading calculations, but involves rasterization of scene geometry at each intermediate frame. The second version, our more significant contribution, reduces both shading and geometry computations by performing reprojection using only image-based buffers. It warps and combines the adjacent rendered frames using an efficient iterative search on their stored scene depth and flow. Bidirectional reprojection introduces a small amount of lag. We perform a user study to investigate this lag, and find that its effect is minor. We demonstrate substantial performance improvements (3--4x) for a variety of applications, including vertex-bound and fill-bound scenes, multi-pass effects, and motion blur.","cites":"13","conferencePercentile":"30.26315789"},{"venue":"ACM Trans. Graph.","id":"e656deb595c384026d901f3c23b4e9dcafb02182","venue_1":"ACM Trans. Graph.","year":"2015","title":"Video-audio driven real-time facial animation","authors":"Yilong Liu, Feng Xu, Jinxiang Chai, Xin Tong, Lijuan Wang, Qiang Huo","author_ids":"2935689, 1770633, 1759700, 1743927, 4688667, 2316043","abstract":"We present a real-time facial tracking and animation system based on a Kinect sensor with video and audio input. Our method requires no user-specific training and is robust to occlusions, large head rotations, and background noise. Given the color, depth and speech audio frames captured from an actor, our system first reconstructs 3D facial expressions and 3D mouth shapes from color and depth input with a multi-linear model. Concurrently a speaker-independent DNN acoustic model is applied to extract phoneme state posterior probabilities (PSPP) from the audio frames. After that, a lip motion regressor refines the 3D mouth shape based on both PSPP and expression weights of the 3D mouth shapes, as well as their confidences. Finally, the refined 3D mouth shape is combined with other parts of the 3D face to generate the final result. The whole process is fully automatic and executed in real time.\n The key component of our system is a data-driven regresor for modeling the correlation between speech data and mouth shapes. Based on a precaptured database of accurate 3D mouth shapes and associated speech audio from one speaker, the regressor jointly uses the input speech and visual features to refine the mouth shape of a new actor. We also present an improved DNN acoustic model. It not only preserves accuracy but also achieves real-time performance.\n Our method efficiently fuses visual and acoustic information for 3D facial performance capture. It generates more accurate 3D mouth motions than other approaches that are based on audio or video input only. It also supports video or audio only input for real-time facial animation. We evaluate the performance of our system with speech and facial expressions captured from different actors. Results demonstrate the efficiency and robustness of our method.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"12b0713eb6ec54a6d8f6a40bfe512947d42962e7","venue_1":"ACM Trans. Graph.","year":"2013","title":"Screened poisson surface reconstruction","authors":"Michael M. Kazhdan, Hugues Hoppe","author_ids":"1690653, 1688461","abstract":"Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface reconstructions.","cites":"88","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"11752a0ce6e86c55e67075ee8510ac0592e9866f","venue_1":"ACM Trans. Graph.","year":"2010","title":"Metric-aware processing of spherical imagery","authors":"Michael M. Kazhdan, Hugues Hoppe","author_ids":"1690653, 1688461","abstract":"Processing spherical images is challenging. Because no spherical parameterization is globally uniform, an accurate solver must account for the spatially varying metric. We present the first efficient metric-aware solver for Laplacian processing of spherical data. Our approach builds on the commonly used equirectangular parameterization, which provides differentiability, axial symmetry, and grid sampling. Crucially, axial symmetry lets us discretize the Laplacian operator just once per grid row. One difficulty is that anisotropy near the poles leads to a poorly conditioned system. Our solution is to construct an adapted hierarchy of finite elements, adjusted at the poles to maintain derivative continuity, and selectively coarsened to bound element anisotropy. The resulting elements are nested both within and across resolution levels. A streaming multigrid solver over this hierarchy achieves excellent convergence rate and scales to huge images. We demonstrate applications in reaction-diffusion texture synthesis and panorama stitching and sharpening.","cites":"7","conferencePercentile":"7.30994152"},{"venue":"ACM Trans. Graph.","id":"4c41aaa2cd20a4110221967a8b13bba9eeee309b","venue_1":"ACM Trans. Graph.","year":"2014","title":"Automating Image Morphing Using Structural Similarity on a Halfway Domain","authors":"Jing Liao, Rodolfo S. Lima, Diego F. Nehab, Hugues Hoppe, Pedro V. Sander, Jinhui Yu","author_ids":"5792126, 1805736, 1764421, 1688461, 1730301, 2213679","abstract":"The main challenge in achieving good image morphs is to create a map that aligns corresponding image elements. Our aim is to help automate this often tedious task. We compute the map by optimizing the compatibility of corresponding warped image neighborhoods using an adaptation of structural similarity. The optimization is regularized by a thin-plate spline and may be guided by a few user-drawn points. We parameterize the map over a halfway domain and show that this representation offers many benefits. The map is able to treat the image pair symmetrically, model simple occlusions continuously, span partially overlapping images, and define extrapolated correspondences. Moreover, it enables direct evaluation of the morph in a pixel shader without mesh rasterization. We improve the morphs by optimizing quadratic motion paths and by seamlessly extending content beyond the image boundaries. We parallelize the algorithm on a GPU to achieve a responsive interface and demonstrate challenging morphs obtained with little effort.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"98ff5fa477503ebf8443f816336e4c7f0693f2bc","venue_1":"ACM Trans. Graph.","year":"2015","title":"High-quality streamable free-viewpoint video","authors":"Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam G. Kirk, Steve Sullivan","author_ids":"2879725, 1707915, 2681417, 2380516, 2015592, 3003709, 1688461, 1758270, 8307604","abstract":"We present the first end-to-end solution to create high-quality free-viewpoint video encoded as a compact data stream. Our system records performances using a dense set of RGB and IR video cameras, generates dynamic textured surfaces, and compresses these to a streamable 3D video format. Four technical advances contribute to high fidelity and robustness: multimodal multi-view stereo fusing RGB, IR, and silhouette information; adaptive meshing guided by automatic detection of perceptually salient areas; mesh tracking to create temporally coherent subsequences; and encoding of tracked textured meshes as an MPEG video stream. Quantitative experiments demonstrate geometric accuracy, texture fidelity, and encoding efficiency. We release several datasets with calibrated inputs and processed results to foster future research.","cites":"13","conferencePercentile":"95.10204082"},{"venue":"ACM Trans. Graph.","id":"8493941ae2fe909363d2b93f47753d65a744749c","venue_1":"ACM Trans. Graph.","year":"2016","title":"Motion graphs for unstructured textured meshes","authors":"Fabian Prada, Michael M. Kazhdan, Ming Chuang, Alvaro Collet, Hugues Hoppe","author_ids":"2020154, 1690653, 1707915, 2879725, 1688461","abstract":"Scanned performances are commonly represented in virtual environments as sequences of textured triangle meshes. Detailed shapes deforming over time benefit from meshes with dynamically evolving connectivity. We analyze these unstructured mesh sequences to automatically synthesize motion graphs with new smooth transitions between compatible poses and actions. Such motion graphs enable natural periodic motions, stochastic playback, and user-directed animations. The main challenge of unstructured sequences is that the meshes differ not only in connectivity but also in alignment, shape, and texture. We introduce new geometry processing techniques to address these problems and demonstrate visually seamless transitions on high-quality captures.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"890120bf1255a511f527f7766567a2b70883b4c2","venue_1":"ACM Trans. Graph.","year":"2014","title":"Controllable high-fidelity facial performance transfer","authors":"Feng Xu, Jinxiang Chai, Yilong Liu, Xin Tong","author_ids":"1770633, 1759700, 2935689, 1743927","abstract":"Recent technological advances in facial capture have made it possible to acquire high-fidelity 3D facial performance data with stunningly high spatial-temporal resolution. Current methods for facial expression transfer, however, are often limited to large-scale facial deformation. This paper introduces a novel facial expression transfer and editing technique for high-fidelity facial performance data. The key idea of our approach is to decompose high-fidelity facial performances into high-level facial feature lines, large-scale facial deformation and fine-scale motion details and transfer them appropriately to reconstruct the retargeted facial animation in an efficient optimization framework. The system also allows the user to quickly modify and control the retargeted facial sequences in the spatial-temporal domain. We demonstrate the power of our approach by transferring and editing high-fidelity facial animation data from high-resolution source models to a wide range of target models, including both human faces and non-human faces such as \"monster\" and \"dog\".","cites":"3","conferencePercentile":"13.78600823"},{"venue":"ACM Trans. Graph.","id":"be843d498131fc64ce1b1dc6790f158e4b37fe58","venue_1":"ACM Trans. Graph.","year":"2014","title":"Reflectance scanning: estimating shading frame and BRDF with generalized linear light sources","authors":"Guojun Chen, Yue Dong, Pieter Peers, Jiawan Zhang, Xin Tong","author_ids":"8018112, 1744268, 1808270, 8214269, 1743927","abstract":"We present a generalized linear light source solution to estimate both the local shading frame and anisotropic surface reflectance of a planar spatially varying material sample.\n We generalize linear light source reflectometry by modulating the intensity along the linear light source, and show that a constant and two sinusoidal lighting patterns are sufficient for estimating the local shading frame and anisotropic surface reflectance. We propose a novel reconstruction algorithm based on the key observation that after factoring out the tangent rotation, the anisotropic surface reflectance lies in a low rank subspace. We exploit the differences in tangent rotation between surface points to infer the low rank subspace and fit each surface point's reflectance function in the projected low rank subspace to the observations. We propose two prototype acquisition devices for capturing surface reflectance that differ on whether the camera is fixed with respect to the linear light source or fixed with respect to the material sample.\n We demonstrate convincing results obtained from reflectance scans of surfaces with different reflectance and shading frame variations.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"af80bc5dbe9c9f18de0918acdf080e319c8e4494","venue_1":"ACM Trans. Graph.","year":"2011","title":"Discrete element textures","authors":"Chongyang Ma, Li-Yi Wei, Xin Tong","author_ids":"1797422, 2420851, 1743927","abstract":"A variety of phenomena can be characterized by repetitive small scale elements within a large scale domain. Examples include a stack of fresh produce, a plate of spaghetti, or a mosaic pattern. Although certain results can be produced via manual placement or procedural/physical simulation, these methods can be labor intensive, difficult to control, or limited to specific phenomena.\n We present discrete element textures, a data-driven method for synthesizing repetitive elements according to a small input exemplar and a large output domain. Our method preserves both individual element properties and their aggregate distributions. It is also general and applicable to a variety of phenomena, including different dimensionalities, different element properties and distributions, and different effects including both artistic and physically realistic ones. We represent each element by one or multiple samples whose positions encode relevant element attributes including position, size, shape, and orientation. We propose a sample-based neighborhood similarity metric and an energy optimization solver to synthesize desired outputs that observe not only input exemplars and output domains but also optional constraints such as physics, orientation fields, and boundary conditions. As a further benefit, our method can also be applied for editing existing element distributions.","cites":"35","conferencePercentile":"72.10526316"},{"venue":"ACM Trans. Graph.","id":"10fbecfc7272ff28c737be8c194b089463b5c33b","venue_1":"ACM Trans. Graph.","year":"2012","title":"High-quality image deblurring with panchromatic pixels","authors":"Sen Wang, Tingbo Hou, John Border, Hong Qin, Rodney L. Miller","author_ids":"6646390, 2634894, 3114528, 1695999, 2126277","abstract":"Image deblurring has been a very challenging problem in recent decades. In this article, we propose a high-quality image deblurring method with a novel image prior based on a new imaging system. The imaging system has a newly designed sensor pattern achieved by adding panchromatic (pan) pixels to the conventional Bayer pattern. Since these pan pixels are sensitive to all wavelengths of visible light, they collect a significantly higher proportion of the light striking the sensor. A new demosaicing algorithm is also proposed to restore full-resolution images from pixels on the sensor. The shutter speed of pan pixels is controllable to users. Therefore, we can produce multiple images with different exposures. When long exposure is needed under dim light, we read pan pixels twice in one shot: one with short exposure and the other with long exposure. The long-exposure image is often blurred, while the short-exposure image can be sharp and noisy. The short-exposure image plays an important role in deblurring, since it is sharp and there is no alignment problem for the one-shot image pair. For the algorithmic aspect, our method runs in a two-step maximum-a-posteriori (MAP) fashion under a joint minimization of the blur kernel and the deblurred image. The algorithm exploits a combined image prior with a statistical part and a spatial part, which is powerful in ringing controls. Extensive experiments under various conditions and settings are conducted to demonstrate the performance of our method.","cites":"4","conferencePercentile":"3.535353535"},{"venue":"ACM Trans. Graph.","id":"4bb0d8d3a753d5b8521df1743cc0e907659e04a7","venue_1":"ACM Trans. Graph.","year":"2016","title":"Manifold differential evolution (MDE): a global optimization method for geodesic centroidal voronoi tessellations on meshes","authors":"Yong-Jin Liu, Chunxu Xu, Ran Yi, Dian Fan, Ying He","author_ids":"1715826, 2250021, 1908480, 3244211, 1734129","abstract":"Computing centroidal Voronoi tessellations (CVT) has many applications in computer graphics. The existing methods, such as the Lloyd algorithm and the quasi-Newton solver, are efficient and easy to implement; however, they compute only the local optimal solutions due to the highly non-linear nature of the CVT energy. This paper presents a novel method, called manifold differential evolution (MDE), for computing globally optimal geodesic CVT energy on triangle meshes. Formulating the mutation operator using discrete geodesics, MDE naturally extends the powerful differential evolution framework from Euclidean spaces to manifold domains. Under mild assumptions, we show that MDE has a provable probabilistic convergence to the global optimum. Experiments on a wide range of 3D models show that MDE consistently out-performs the existing methods by producing results with lower energy. Thanks to its intrinsic and global nature, MDE is insensitive to initialization and mesh tessellation. Moreover, it is able to handle multiply-connected Voronoi cells, which are challenging to the existing geodesic CVT methods.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"2760e681e9962fe56f4849aff25aecbecbe89ec6","venue_1":"ACM Trans. Graph.","year":"2007","title":"Real-time edge-aware image processing with the bilateral grid","authors":"Jiawen Chen, Sylvain Paris, Frédo Durand","author_ids":"1967685, 1720990, 1728125","abstract":"We present a new data structure---the <i>bilateral grid</i>, that enables fast edge-aware image processing. By working in the bilateral grid, algorithms such as bilateral filtering, edge-aware painting, and local histogram equalization become simple manipulations that are both local and independent. We parallelize our algorithms on modern GPUs to achieve real-time frame rates on high-definition video. We demonstrate our method on a variety of applications such as image editing, transfer of photographic look, and contrast enhancement of medical images.","cites":"182","conferencePercentile":"94.4"},{"venue":"ACM Trans. Graph.","id":"1b3ca9a9dbb5ec118cfd9211958302313e8d987c","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fast computation of seamless video loops","authors":"Jing Liao, Mark Finch, Hugues Hoppe","author_ids":"5792126, 3354969, 1688461","abstract":"Short looping videos concisely capture the dynamism of natural scenes. Creating seamless loops usually involves maximizing spatiotemporal consistency and applying Poisson blending. We take an end-to-end view of the problem and present new techniques that jointly improve loop quality while also significantly reducing processing time. A key idea is to relax the consistency constraints to anticipate the subsequent blending, thereby enabling looping of low-frequency content like moving clouds and changing illumination. We also analyze the input video to remove an undesired bias toward short loops. The quality gains are demonstrated visually and confirmed quantitatively using a new gradient-domain consistency metric. We improve system performance by classifying potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based on dominant periods, and optimizing on a coarse grid while retaining finer detail. Together these techniques reduce computation times from tens of minutes to nearly real-time.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"0f5a6fc6a6ffe844625f47eac430b642b094a30a","venue_1":"ACM Trans. Graph.","year":"2010","title":"Optimizing continuity in multiscale imagery","authors":"Charles Han, Hugues Hoppe","author_ids":"1861679, 1688461","abstract":"Multiscale imagery often combines several sources with differing appearance. For instance, Internet-based maps contain satellite and aerial photography. Zooming within these maps may reveal jarring transitions. We present a scheme that creates a visually smooth mipmap pyramid from stitched imagery at several scales. The scheme involves two new techniques. The first, <i>structure transfer</i>, is a nonlinear operator that combines the detail of one image with the local appearance of another. We use this operator to inject detail from the fine image into the coarse one while retaining color consistency. The improved structural similarity greatly reduces inter-level ghosting artifacts. The second, <i>clipped Laplacian blending</i>, is an efficient construction to minimize blur when creating intermediate levels. It considers the sum of all inter-level image differences within the pyramid. We demonstrate continuous zooming of map imagery from space to ground level.","cites":"4","conferencePercentile":"2.339181287"},{"venue":"ACM Trans. Graph.","id":"0d861b6a26bb946e0667da2905ddf70e9d60eb17","venue_1":"ACM Trans. Graph.","year":"2010","title":"Distributed gradient-domain processing of planar and spherical images","authors":"Michael M. Kazhdan, Dinoj Surendran, Hugues Hoppe","author_ids":"1690653, 2848879, 1688461","abstract":"Gradient-domain processing is widely used to edit and combine images. In this article we extend the framework in two directions. First, we adapt the gradient-domain approach to operate on a spherical domain, to enable operations such as seamless stitching, dynamic-range compression, and gradient-based sharpening over spherical imagery. An efficient streaming computation is obtained using a new spherical parameterization with bounded distortion and localized boundary constraints. Second, we design a distributed solver to efficiently process large planar or spherical images. The solver partitions images into bands, streams through these bands in parallel within a networked cluster, and schedules computation to hide the necessary synchronization latency. We demonstrate our contributions on several datasets including the Digitized Sky Survey, a terapixel spherical scan of the night sky.","cites":"12","conferencePercentile":"16.66666667"},{"venue":"ACM Trans. Graph.","id":"12615d6d808b8334e45742bbc63ea1d01b6571e0","venue_1":"ACM Trans. Graph.","year":"2009","title":"Amortized supersampling","authors":"Lei Yang, Diego F. Nehab, Pedro V. Sander, Pitchaya Sitthi-amorn, Jason Lawrence, Hugues Hoppe","author_ids":"3449835, 1764421, 1730301, 1936229, 1694005, 1688461","abstract":"We present a real-time rendering scheme that reuses shading samples from earlier time frames to achieve practical antialiasing of procedural shaders. Using a reprojection strategy, we maintain several sets of shading estimates at subpixel precision, and incrementally update these such that for most pixels only one new shaded sample is evaluated per frame. The key difficulty is to prevent accumulated blurring during successive reprojections. We present a theoretical analysis of the blur introduced by reprojection methods. Based on this analysis, we introduce a nonuniform spatial filter, an adaptive recursive temporal filter, and a principled scheme for locally estimating the spatial blur. Our scheme is appropriate for antialiasing shading attributes that vary slowly over time. It works in a single rendering pass on commodity graphics hardware, and offers results that surpass 4x4 stratified supersampling in quality, at a fraction of the cost.","cites":"4","conferencePercentile":"5.524861878"},{"venue":"ACM Trans. Graph.","id":"92249a337a01f3fc6c61d26d26a2ed815f5c1a41","venue_1":"ACM Trans. Graph.","year":"2012","title":"Resolution enhancement by vibrating displays","authors":"Floraine Berthouzoz, Raanan Fattal","author_ids":"2842099, 3230440","abstract":"We present a method that makes use of the retinal integration time in the human visual system for increasing the resolution of displays. Given an input image with a resolution higher than the display resolution, we compute several images that match the display's native resolution. We then render these low-resolution images in a sequence that repeats itself on a high refresh-rate display. The period of the sequence falls below the retinal integration time and therefore the eye integrates the images temporally and perceives them as one image. In order to achieve resolution enhancement we apply small-amplitude vibrations to the display panel and synchronize them with the screen refresh cycles. We derive the perceived image model and use it to compute the low-resolution images that are optimized to enhance the apparent resolution of the perceived image. This approach achieves resolution enhancement without having to move the displayed content across the screen and hence offers a more practical solution than existing approaches. Moreover, we use our model to establish limitations on the amount of resolution enhancement achievable by such display systems. In this analysis we draw a formal connection between our display and super-resolution techniques and find that both methods share the same limitation, yet this limitation stems from different sources. Finally, we describe in detail a simple physical realization of our display system and demonstrate its ability to match most of the spectrum displayable on a screen with twice the resolution.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"4f662a9cdff3aa814d2b68527bb6a0b9ea4e629f","venue_1":"ACM Trans. Graph.","year":"2011","title":"A Framework for content-adaptive photo manipulation macros: Application to face, landscape, and global manipulations","authors":"Floraine Berthouzoz, Wilmot Li, Mira Dontcheva, Maneesh Agrawala","author_ids":"2842099, 2812691, 2875493, 1820412","abstract":"We present a framework for generating content-adaptive macros that can transfer complex photo manipulations to new target images. We demonstrate applications of our framework to face, landscape, and global manipulations. To create a content-adaptive macro, we make use of multiple training demonstrations. Specifically, we use automated image labeling and machine learning techniques to learn the dependencies between image features and the parameters of each selection, brush stroke, and image processing operation in the macro. Although our approach is limited to learning manipulations where there is a direct dependency between image features and operation parameters, we show that our framework is able to learn a large class of the most commonly used manipulations using as few as 20 training demonstrations. Our framework also provides interactive controls to help macro authors and users generate training demonstrations and correct errors due to incorrect labeling or poor parameter estimation. We ask viewers to compare images generated using our content-adaptive macros with and without corrections to manually generated ground-truth images and find that they consistently rate both our automatic and corrected results as close in appearance to the ground truth. We also evaluate the utility of our proposed macro generation workflow via a small informal lab study with professional photographers. The study suggests that our workflow is effective and practical in the context of real-world photo editing.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"a62edbcc35df14e2f5549192a204a453a2389ca5","venue_1":"ACM Trans. Graph.","year":"1995","title":"Using Visual Texture for Information Display","authors":"Colin Ware, William Knight","author_ids":"1720078, 3811078","abstract":"Results from vision research are applied to the synthesis of visual texture for the purposes of information display. The literature surveyed suggests that the human visual system processes spatial information by means of parallel arrays of neurons that can be modeled by Gabor functions. Based on the Gabor model, it is argued that the fundamental dimensions of texture for human perception are orientation, size (1/frequency), and contrast. It is shown that there are a number of trade-offs in the density with which information can be displayed using texture. Two of these are (1) a trade-off between the size of the texture elements and the precision with which the location can be specified, and (2) the precision with which texture orientation can be specified and the precision with which texture size can be specified. Two algorithms for generating texture are included.","cites":"61","conferencePercentile":"71.42857143"},{"venue":"ACM Trans. Graph.","id":"725e9f7022c22e21999d90c8a40d3801dbe6fd82","venue_1":"ACM Trans. Graph.","year":"2015","title":"Solving trigonometric moment problems for fast transient imaging","authors":"Christoph Peters, Jonathan Klein, Matthias B. Hullin, Reinhard Klein","author_ids":"1681772, 3942493, 1899671, 1742928","abstract":"Transient images help to analyze light transport in scenes. Besides two spatial dimensions, they are resolved in time of flight. Cost-efficient approaches for their capture use amplitude modulated continuous wave lidar systems but typically take more than a minute of capture time. We propose new techniques for measurement and reconstruction of transient images, which drastically reduce this capture time. To this end, we pose the problem of reconstruction as a trigonometric moment problem. A vast body of mathematical literature provides powerful solutions to such problems. In particular, the maximum entropy spectral estimate and the Pisarenko estimate provide two closed-form solutions for reconstruction using continuous densities or sparse distributions, respectively. Both methods can separate <i>m</i> distinct returns using measurements at <i>m</i> modulation frequencies. For <i>m</i> = 3 our experiments with measured data confirm this. Our GPU-accelerated implementation can reconstruct more than 100000 frames of a transient image per second. Additionally, we propose modifications of the capture routine to achieve the required sinusoidal modulation without increasing the capture time. This allows us to capture up to 18.6 transient images per second, leading to transient video. An important byproduct is a method for removal of multipath interference in range imaging.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"0b98e6e1f21bf335c68f671e5b8e94dc5dbb6a25","venue_1":"ACM Trans. Graph.","year":"2010","title":"Light reallocation for high contrast projection using an analog micromirror array","authors":"Reynald Hoskinson, Boris Stoeber, Wolfgang Heidrich, Sidney Fels","author_ids":"2924938, 1949301, 1752192, 1749457","abstract":"We demonstrate for the first time a proof of concept projector with a secondary array of individually controllable, analog micromirrors added to improve the contrast and peak brightness of conventional projectors. The micromirrors reallocate the light of the projector lamp from the dark parts towards the light parts of the image, before it reaches the primary image modulator. Each element of the analog micromirror array can be tipped/tilted to divert portions of the light from the lamp in two dimensions. By directing these mirrors on an image-dependent basis, we can increase both the peak intensity of the projected image as well as its contrast.\n In this paper, we describe and analyze the optical design for projectors using this light reallocation approach. We also discuss software algorithms to compute the best light reallocation pattern for a given input image, using the constraints of real hardware. We perform extensive simulations of this process to evaluate image quality and performance characteristics of this process. Finally, we present a first proof-of-concept implementation of this approach using a prototype analog micromirror device.","cites":"6","conferencePercentile":"6.432748538"},{"venue":"ACM Trans. Graph.","id":"6318714027d58d7b402314c3c42b466d1307c33e","venue_1":"ACM Trans. Graph.","year":"2011","title":"T&I engine: traversal and intersection engine for hardware accelerated ray tracing","authors":"Jae-Ho Nah, Jeong-Soo Park, Chanmin Park, Jin-Woo Kim, Yun-Hye Jung, Woo-Chan Park, Tack-Don Han","author_ids":"1714542, 1702049, 4384845, 5701930, 2144050, 1700180, 2184681","abstract":"Ray tracing naturally supports high-quality global illumination effects, but it is computationally costly. Traversal and intersection operations dominate the computation of ray tracing. To accelerate these two operations, we propose a hardware architecture integrating three novel approaches. First, we present an ordered depth-first layout and a traversal architecture using this layout to reduce the required memory bandwidth. Second, we propose a three-phase ray-triangle intersection architecture that takes advantage of early exit. Third, we propose a latency hiding architecture defined as the ray accumulation unit. Cycle-accurate simulation results indicate our architecture can achieve interactive distributed ray tracing.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"4edee7b5234e2c5289572c09423f25771f4fe5ad","venue_1":"ACM Trans. Graph.","year":"1999","title":"Model and representation: the effect of visual feedback on human performance in a color picker interface","authors":"Sarah A. Douglas, Arthur E. Kirkpatrick","author_ids":"3057136, 7428034","abstract":"User interfaces for color selection consist of a visible screen representation, an input method, and the underlying conceptual organization of the color model. We report a two-way factorial, between-subjects variable experiment that tested the effect of high and low visual feedback interfaces on speed and accuracy of color matching for RGB and HSV color models. The only significant effect was improved accuracy due to increased visual feedback. Using color groups as a within-subjects variable, we found differences in performance of both speed and accuracy. We recommend that experimental tests adopt a color test set that does not show bias toward a particular model, but is based instead on a range of colors that would be most likely matched in practice by people using color  selection software. We recomment the Macbeth Color Checker naturals, primaries, and grays. As a follow-up study, a qualitative case analysis of the way users navigated through the color space indicates that feedback helps users with limited knowledge of the model, allowing them to refine their match to a higher degree of accuracy. Users with very little or a lot of knowledge of the color model do not appear to be aided by increased feedback. In conclusion, we suggest that visual feedback and design of the interface may be a more important factor in improving the usability of a color selection interface than the particular color model used.","cites":"17","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"3165c199667ede69b85360eaaa5b01e662eb3149","venue_1":"ACM Trans. Graph.","year":"2014","title":"RayCore: A Ray-Tracing Hardware Architecture for Mobile Devices","authors":"Jae-Ho Nah, Hyuck-Joo Kwon, Dong-Seok Kim, Cheol-Ho Jeong, Jin-Hong Park, Tack-Don Han, Dinesh Manocha, Woo-Chan Park","author_ids":"1714542, 2806901, 3114815, 3058526, 2269112, 2184681, 1699159, 1700180","abstract":"We present RayCore, a mobile ray-tracing hardware architecture. RayCore facilitates high-quality rendering effects, such as reflection, refraction, and shadows, on mobile devices by performing real-time Whitted ray tracing. RayCore consists of two major components: ray-tracing units (RTUs) based on a unified traversal and intersection pipeline and a tree-building unit (TBU) for dynamic scenes. The overall RayCore architecture offers considerable benefits in terms of die area, memory access, and power consumption. We have evaluated our architecture based on FPGA and ASIC evaluations and demonstrate its performance on different benchmarks. According to the results, our architecture demonstrates high performance per unit area and unit energy, making it highly suitable for use in mobile devices.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"23d6450b88eefeecb4b095f2aa4a242bd4d88d7a","venue_1":"ACM Trans. Graph.","year":"2002","title":"Geometry images","authors":"Xianfeng Gu, Steven J. Gortler, Hugues Hoppe","author_ids":"1678267, 2415843, 1688461","abstract":"Surface geometry is often modeled with irregular triangle meshes. The process of remeshing refers to approximating such geometry using a mesh with (semi)-regular connectivity, which has advantages for many graphics applications. However, current techniques for remeshing arbitrary surfaces create only <i>semi-regular</i> meshes. The original mesh is typically decomposed into a set of disk-like charts, onto which the geometry is parametrized and sampled. In this paper, we propose to remesh an arbitrary surface onto a <i>completely regular</i> structure we call a <i>geometry image.</i> It captures geometry as a simple 2D array of quantized points. Surface signals like normals and colors are stored in similar 2D arrays using the same implicit surface parametrization --- texture coordinates are absent. To create a geometry image, we cut an arbitrary mesh along a network of edge paths, and parametrize the resulting single chart onto a square. Geometry images can be encoded using traditional image compression algorithms, such as wavelet-based coders.","cites":"262","conferencePercentile":"84"},{"venue":"ACM Trans. Graph.","id":"027c2120574a0dfb6ff6d140826e70b8880cff25","venue_1":"ACM Trans. Graph.","year":"2003","title":"Spherical parametrization and remeshing","authors":"Emil Praun, Hugues Hoppe","author_ids":"2807811, 1688461","abstract":"The traditional approach for parametrizing a surface involves cutting it into charts and mapping these piecewise onto a planar domain. We introduce a robust technique for directly parametrizing a genus-zero surface onto a spherical domain. A key ingredient for making such a parametrization practical is the minimization of a stretch-based measure, to reduce scale-distortion and thereby prevent undersampling. Our second contribution is a scheme for sampling the spherical domain using uniformly subdivided polyhedral domains, namely the tetrahedron, octahedron, and cube. We show that these particular semi-regular samplings can be conveniently represented as completely regular 2D grids, i.e. geometry images. Moreover, these images have simple boundary extension rules that aid many processing operations. Applications include geometry remeshing, level-of-detail, morphing, compression, and smooth surface subdivision.","cites":"215","conferencePercentile":"81.72043011"},{"venue":"ACM Trans. Graph.","id":"91d6ec816316dc41de489d7e2b92524e626e9201","venue_1":"ACM Trans. Graph.","year":"2004","title":"Geometry clipmaps: terrain rendering using nested regular grids","authors":"Frank Losasso, Hugues Hoppe","author_ids":"1967534, 1688461","abstract":"Rendering throughput has reached a level that enables a novel approach to level-of-detail (LOD) control in terrain rendering. We introduce the geometry clipmap, which caches the terrain in a set of nested regular grids centered about the viewer. The grids are stored as vertex buffers in fast video memory, and are incrementally refilled as the viewpoint moves. This simple framework provides visual continuity, uniform frame rate, complexity throttling, and graceful degradation. Moreover it allows two new exciting real-time functionalities: decompression and synthesis. Our main dataset is a 40GB height map of the United States. A compressed image pyramid reduces the size by a remarkable factor of 100, so that it fits entirely in memory. This compressed data also contributes normal maps for shading. As the viewer approaches the surface, we synthesize grid levels finer than the stored terrain using fractal noise displacement. Decompression, synthesis, and normal-map computations are incremental, thereby allowing interactive flight at 60 frames/sec.","cites":"210","conferencePercentile":"81.52173913"},{"venue":"ACM Trans. Graph.","id":"877716923b5c2c95e6f33956b0b1355342f87e64","venue_1":"ACM Trans. Graph.","year":"2004","title":"Inter-surface mapping","authors":"John Schreiner, Arul Asirvatham, Emil Praun, Hugues Hoppe","author_ids":"1952258, 2427662, 2807811, 1688461","abstract":"We consider the problem of creating a map between two arbitrary triangle meshes. Whereas previous approaches compose parametrizations over a simpler intermediate domain, we directly create and optimize a continuous map between the meshes. Map distortion is measured with a new symmetric metric, and is minimized during interleaved coarse-to-fine refinement of both meshes. By explicitly favoring low inter-surface distortion, we obtain maps that naturally align corresponding shape elements. Typically, the user need only specify a handful of feature correspondences for initial registration, and even these constraints can be removed during optimization. Our method robustly satisfies hard constraints if desired. Inter-surface mapping is shown using geometric and attribute morphs. Our general framework can also be applied to parametrize surfaces onto simplicial domains, such as coarse meshes (for semi-regular remeshing), and octahedron and toroidal domains (for geometry image remeshing). In these settings, we obtain better parametrizations than with previous specialized techniques, thanks to our fine-grain optimization.","cites":"138","conferencePercentile":"69.56521739"},{"venue":"ACM Trans. Graph.","id":"12ed62b2ea04e1b3849f8be5f8a2d8e6f7410de8","venue_1":"ACM Trans. Graph.","year":"2005","title":"Fast exact and approximate geodesics on meshes","authors":"Vitaly Surazhsky, Tatiana Surazhsky, Danil Kirsanov, Steven J. Gortler, Hugues Hoppe","author_ids":"1713344, 3150776, 1873177, 2415843, 1688461","abstract":"The computation of geodesic paths and distances on triangle meshes is a common operation in many computer graphics applications. We present several practical algorithms for computing such geodesics from a source point to one or all other points efficiently. First, we describe an implementation of the exact \"single source, all destination\" algorithm presented by Mitchell, Mount, and Papadimitriou (MMP). We show that the algorithm runs much faster in practice than suggested by worst case analysis. Next, we extend the algorithm with a merging operation to obtain computationally efficient and accurate approximations with bounded error. Finally, to compute the shortest path between two given points, we use a lower-bound property of our approximate geodesic algorithm to efficiently prune the frontier of the MMP algorithm. thereby obtaining an exact solution even more quickly.","cites":"169","conferencePercentile":"86.69354839"},{"venue":"ACM Trans. Graph.","id":"379c30e3e185478a16705c8828e628ad50d22981","venue_1":"ACM Trans. Graph.","year":"2005","title":"Parallel controllable texture synthesis","authors":"Sylvain Lefebvre, Hugues Hoppe","author_ids":"2757631, 1688461","abstract":"We present a texture synthesis scheme based on neighborhood matching, with contributions in two areas: parallelism and control. Our scheme defines an infinite, deterministic, aperiodic texture, from which windows can be computed in real-time on a GPU. We attain high-quality synthesis using a new analysis structure called the Gaussian stack, together with a coordinate upsampling step and a subpass correction approach. Texture variation is achieved by multiresolution jittering of exemplar coordinates. Combined with the local support of parallel synthesis, the jitter enables intuitive user controls including multiscale randomness, spatial modulation over both exemplar and output, feature drag-and-drop, and periodicity constraints. We also introduce synthesis magnification, a fast method for amplifying coarse synthesis results to higher resolution.","cites":"145","conferencePercentile":"77.41935484"},{"venue":"ACM Trans. Graph.","id":"219aaf0402340bb815d467305b9a298acdc3f29c","venue_1":"ACM Trans. Graph.","year":"2006","title":"Appearance-space texture synthesis","authors":"Sylvain Lefebvre, Hugues Hoppe","author_ids":"2757631, 1688461","abstract":"The traditional approach in texture synthesis is to compare color neighborhoods with those of an exemplar. We show that quality is greatly improved if pointwise colors are replaced by appearance vectors that incorporate nonlocal information such as feature and radiance-transfer data. We perform dimensionality reduction on these vectors prior to synthesis, to create a new appearance-space exemplar. Unlike a texton space, our appearance space is low-dimensional and Euclidean. Synthesis in this information-rich space lets us reduce runtime neighborhood vectors from 5x5 grids to just 4 locations. Building on this unifying framework, we introduce novel techniques for coherent anisometric synthesis, surface texture synthesis directly in an ordinary atlas, and texture advection. Remarkably, we achieve all these functionalities in real-time, or 3 to 4 orders of magnitude faster than prior work.","cites":"143","conferencePercentile":"88.88888889"},{"venue":"ACM Trans. Graph.","id":"0747d01592a8883dc38686995b41fa23fe62b348","venue_1":"ACM Trans. Graph.","year":"2006","title":"Perfect spatial hashing","authors":"Sylvain Lefebvre, Hugues Hoppe","author_ids":"2757631, 1688461","abstract":"We explore using hashing to pack sparse data into a compact table while retaining efficient random access. Specifically, we design a perfect multidimensional hash function -- one that is precomputed on static data to have no hash collisions. Because our hash function makes a single reference to a small offset table, queries always involve exactly two memory accesses and are thus ideally suited for parallel SIMD evaluation on graphics hardware. Whereas prior hashing work strives for pseudorandom mappings, we instead design the hash function to preserve spatial coherence and thereby improve runtime locality of reference. We demonstrate numerous graphics applications including vector images, texture sprites, alpha channel compression, 3D-parameterized textures, 3D painting, simulation, and collision detection.","cites":"83","conferencePercentile":"62.03703704"},{"venue":"ACM Trans. Graph.","id":"3c3a0a820cb59d047af513c3d1835b7f80b229d2","venue_1":"ACM Trans. Graph.","year":"2008","title":"Streaming multigrid for gradient-domain operations on large images","authors":"Michael M. Kazhdan, Hugues Hoppe","author_ids":"1690653, 1688461","abstract":"We introduce a new tool to solve the large linear systems arising from gradient-domain image processing. Specifically, we develop a streaming multigrid solver, which needs just two sequential passes over out-of-core data. This fast solution is enabled by a combination of three techniques: (1) use of second-order finite elements (rather than traditional finite differences) to reach sufficient accuracy in a single V-cycle, (2) temporally blocked relaxation, and (3) multi-level streaming to pipeline the restriction and prolongation phases into single streaming passes. A key contribution is the extension of the B-spline finite-element method to be compatible with the forward-difference gradient representation commonly used with images. Our streaming solver is also efficient for in-memory images, due to its fast convergence and excellent cache behavior. Remarkably, it can outperform spatially adaptive solvers that exploit application-specific knowledge. We demonstrate seamless stitching and tone-mapping of gigapixel images in about an hour on a notebook PC.","cites":"59","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"09084003e51b1704905cb4eb0f95c781ea2bcf6f","venue_1":"ACM Trans. Graph.","year":"2008","title":"Efficient traversal of mesh edges using adjacency primitives","authors":"Pedro V. Sander, Diego F. Nehab, Eden Chlamtác, Hugues Hoppe","author_ids":"1730301, 1764421, 2050395, 1688461","abstract":"Processing of mesh edges lies at the core of many advanced realtime rendering techniques, ranging from shadow and silhouette computations, to motion blur and fur rendering. We present a scheme for efficient traversal of mesh edges that builds on the adjacency primitives and programmable geometry shaders introduced in recent graphics hardware. Our scheme aims to minimize the number of primitives while maximizing SIMD parallelism. These objectives reduce to a set of discrete optimization problems on the dual graph of the mesh, and we develop practical solutions to these graph problems. In addition, we extend two existing vertex cache optimization algorithms to produce cache-efficient traversal orderings for adjacency primitives. We demonstrate significant runtime speedups for several practical real-time rendering algorithms.","cites":"15","conferencePercentile":"9.87654321"},{"venue":"ACM Trans. Graph.","id":"3c35a1ca28ac2417b149b29031a1e28a06510eb9","venue_1":"ACM Trans. Graph.","year":"2008","title":"Random-access rendering of general vector graphics","authors":"Diego F. Nehab, Hugues Hoppe","author_ids":"1764421, 1688461","abstract":"We introduce a novel representation for random-access rendering of antialiased vector graphics on the GPU, along with efficient encoding and rendering algorithms. The representation supports a broad class of vector primitives, including multiple layers of semitransparent filled and stroked shapes, with quadratic outlines and color gradients. Our approach is to create a coarse lattice in which each cell contains a variable-length encoding of the graphics primitives it overlaps. These cell-specialized encodings are interpreted at runtime within a pixel shader. Advantages include localized memory access and the ability to map vector graphics onto arbitrary surfaces, or under arbitrary deformations. Most importantly, we perform both prefiltering and supersampling within a single pixel shader invocation, achieving inter-primitive antialiasing at no added memory bandwidth cost. We present an efficient encoding algorithm, and demonstrate high-quality real-time rendering of complex, real-world examples.","cites":"20","conferencePercentile":"19.44444444"},{"venue":"ACM Trans. Graph.","id":"2d7142ea07a5b5d6c3ce121bd00e61c8f3b9968e","venue_1":"ACM Trans. Graph.","year":"1996","title":"Evaluating Stereo and Motion Cues for Visualizing Information Nets in Three Dimensions","authors":"Colin Ware, Glenn Franck","author_ids":"1720078, 2186278","abstract":"This article concerns the benefits of presenting abstract data in 3D. Two experiments show that motion cues combined with stereo viewing can substantially increase the size of the graph that can be preceived. The first experiment was designed to provide quantitiative measurements of how much more (or less) can be understood in 3D than in 2D. The 3D display used was configured so that the image on the monitor was coupled to the user's actual eye positons (and it was updated in real-time as the user moved) as well as bring in stereo. Thus the effect was like a local &#8220;virtual reality&#8221; display located in the vicinity of the computer monitor. The results from this study show that head-coupled stereo viewing can increase the size of an abstract graph that can be understood by a factor of three; using stereo alone provided an increase by a factor of 1.6 and head coupling along produced an increase by a factor of 2.2. The second experiment examined a variety of motion cues provided by head-coupled perspective (as in virtual reality displays), head-guided motion and automatic rotation, respectively, both with and without stereo in each case. The results show that structured 3D motion and stereo viewing both help in understanding, but that the kind of motion is not particularly important; all improve performance, and all are more significant than stereo cues. These results provide strong reasons for using advanced 3D graphics for interacting with a large variety of information structures.","cites":"219","conferencePercentile":"86.66666667"},{"venue":"ACM Trans. Graph.","id":"b68be66c6c4f77afc9d2f48b175d067434012e01","venue_1":"ACM Trans. Graph.","year":"2010","title":"A hierarchical volumetric shadow algorithm for single scattering","authors":"Ilya Baran, Jiawen Chen, Jonathan Ragan-Kelley, Frédo Durand, Jaakko Lehtinen","author_ids":"1789898, 1967685, 2488277, 1728125, 1780788","abstract":"Volumetric effects such as beams of light through participating media are an important component in the appearance of the natural world. Many such effects can be faithfully modeled by a single scattering medium. In the presence of shadows, rendering these effects can be prohibitively expensive: current algorithms are based on ray marching, i.e., integrating the illumination scattered towards the camera along each view ray, modulated by visibility to the light source at each sample. Visibility must be determined for each sample using shadow rays or shadow-map lookups. We observe that in a suitably chosen coordinate system, the visibility function has a regular structure that we can exploit for significant acceleration compared to brute force sampling. We propose an efficient algorithm based on partial sum trees for computing the scattering integrals in a single-scattering homogeneous medium. On a CPU, we achieve speedups of 17--120x over ray marching.","cites":"17","conferencePercentile":"26.02339181"},{"venue":"ACM Trans. Graph.","id":"ebeceea06b3e911cb31b3ac28947bc8817b27160","venue_1":"ACM Trans. Graph.","year":"2015","title":"Predicting Appearance from Measured Microgeometry of Metal Surfaces","authors":"Zhao Dong, Bruce Walter, Steve Marschner, Donald P. Greenberg","author_ids":"3385123, 2540721, 2593798, 1748054","abstract":"The visual appearance of many materials is created by micro-scale details of their surface geometry. In this article, we investigate a new approach to capturing the appearance of metal surfaces without reflectance measurements, by deriving microfacet distributions directly from measured surface topography. Modern profilometers are capable of measuring surfaces with subwavelength resolution at increasingly rapid rates. We consider both wave- and geometric-optics methods for predicting BRDFs of measured surfaces and compare the results to optical measurements from a gonioreflectometer for five rough metal samples. Surface measurements are also used to predict spatial variation, or texture, which is especially important for the appearance of our anisotropic brushed metal samples.\n Profilometer-based BRDF acquisition offers many potential advantages over traditional techniques, including speed and easy handling of anisotropic, highly directional materials. We also introduce a new generalized normal distribution function, the ellipsoidal NDF, to compactly represent nonsymmetric features in our measured data and texture synthesis.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"344ec434457b5888057114dc816da77196661fea","venue_1":"ACM Trans. Graph.","year":"2011","title":"A comprehensive theory of volumetric radiance estimation using photon points and beams","authors":"Wojciech Jarosz, Derek Nowrouzezahrai, Iman Sadeghi, Henrik Wann Jensen","author_ids":"1953515, 1795014, 7474590, 1730025","abstract":"We present two contributions to the area of volumetric rendering. We develop a novel, comprehensive theory of volumetric radiance estimation that leads to several new insights and includes all previously published estimates as special cases. This theory allows for estimating in-scattered radiance at a point, or accumulated radiance along a camera ray, with the standard photon particle representation used in previous work. Furthermore, we generalize these operations to include a more compact, and more expressive intermediate representation of lighting in participating media, which we call &#8220;photon beams.&#8221; The combination of these representations and their respective query operations results in a collection of nine distinct volumetric radiance estimates.\n Our second contribution is a more efficient rendering method for participating media based on photon beams. Even when shooting and storing less photons and using less computation time, our method significantly reduces both bias (blur) and variance in volumetric radiance estimation. This enables us to render sharp lighting details (e.g., volume caustics) using just tens of thousands of photon beams, instead of the millions to billions of photon points required with previous methods.","cites":"24","conferencePercentile":"53.42105263"},{"venue":"ACM Trans. Graph.","id":"b3ecfdab462201e00c18ae3270c1358e1234d9c5","venue_1":"ACM Trans. Graph.","year":"2004","title":"Digital photography with flash and no-flash image pairs","authors":"Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael F. Cohen, Hugues Hoppe, Kentaro Toyama","author_ids":"1964338, 1717841, 1820412, 1694613, 1688461, 1769685","abstract":"Digital photography has made it possible to quickly and easily take a pair of images of low-light environments: one with flash to capture detail and one without flash to capture ambient illumination. We present a variety of applications that analyze and combine the strengths of such flash/no-flash image pairs. Our applications include denoising and detail transfer (to merge the ambient qualities of the no-flash image with the high-frequency flash detail), white-balancing (to change the color tone of the ambient image), continuous flash (to interactively adjust flash intensity), and red-eye removal (to repair artifacts in the flash image). We demonstrate how these applications can synthesize new images that are of higher quality than either of the originals.","cites":"303","conferencePercentile":"89.13043478"},{"venue":"ACM Trans. Graph.","id":"43c31a928f2a34065d65dadf750428fd1b99d11b","venue_1":"ACM Trans. Graph.","year":"2003","title":"Wang Tiles for image and texture generation","authors":"Michael F. Cohen, Jonathan Shade, Stefan Hiller, Oliver Deussen","author_ids":"1694613, 3185683, 3111506, 1850438","abstract":"We present a simple stochastic system for non-periodically tiling the plane with a small set of Wang Tiles. The tiles may be filled with texture, patterns, or geometry that when assembled create a continuous representation. The primary advantage of using Wang Tiles is that once the tiles are filled, large expanses of non-periodic texture (or patterns or geometry) can be created as needed very efficiently at runtime.Wang Tiles are squares in which each edge is assigned a color. A valid tiling requires all shared edges between tiles to have matching colors. We present a new stochastic algorithm to non-periodically tile the plane with a small set of Wang Tiles at runtime.Furthermore, we present new methods to fill the tiles with 2D texture, 2D Poisson distributions, or 3D geometry to efficiently create at runtime as much non-periodic texture (or distributions, or geometry) as needed. We leverage previous texture synthesis work and adapt it to fill Wang Tiles. We demonstrate how to fill individual tiles with Poisson distributions that maintain their statistical properties when combined. These are used to generate a large arrangement of plants or other objects on a terrain. We show how such environments can be rendered efficiently by pre-lighting the individual Wang Tiles containing the geometry.We also extend the definition of Wang Tiles to include a coding of the tile corners to allow discrete objects to overlap more than one edge. The larger set of tiles provides increased degrees of freedom.","cites":"217","conferencePercentile":"82.79569892"},{"venue":"ACM Trans. Graph.","id":"f3229752eb207b8b1cb1fa26b075d764d93020e1","venue_1":"ACM Trans. Graph.","year":"2008","title":"Facial performance synthesis using deformation-driven polynomial displacement maps","authors":"Wan-Chun Ma, Andrew Jones, Jen-Yuan Chiang, Tim Hawkins, Sune Frederiksen, Pieter Peers, Marko Vukovic, Ming Ouhyoung, Paul E. Debevec","author_ids":"1899617, 4240397, 2220196, 1684297, 3040984, 1808270, 3018416, 1744863, 1778676","abstract":"We present a novel method for acquisition, modeling, compression, and synthesis of realistic facial deformations using polynomial displacement maps. Our method consists of an analysis phase where the relationship between motion capture markers and detailed facial geometry is inferred, and a synthesis phase where novel detailed animated facial geometry is driven solely by a sparse set of motion capture markers. For analysis, we record the actor wearing facial markers while performing a set of training expression clips. We capture real-time high-resolution facial deformations, including dynamic wrinkle and pore detail, using interleaved structured light 3D scanning and photometric stereo. Next, we compute displacements between a neutral mesh driven by the motion capture markers and the high-resolution captured expressions. These geometric displacements are stored in a <i>polynomial displacement map</i> which is parameterized according to the local deformations of the motion capture dots. For synthesis, we drive the polynomial displacement map with new motion capture data. This allows the recreation of large-scale muscle deformation, medium and fine wrinkles, and dynamic skin pore detail. Applications include the compression of existing performance data and the synthesis of new performances. Our technique is independent of the underlying geometry capture system and can be used to automatically generate high-frequency wrinkle and pore details on top of many existing facial animation systems.","cites":"67","conferencePercentile":"74.69135802"},{"venue":"ACM Trans. Graph.","id":"5e27fd1e25b1d7c700454cd5f19a5801fbdefac9","venue_1":"ACM Trans. Graph.","year":"2014","title":"Dual strip weaving: interactive design of quad layouts using elastica strips","authors":"Marcel Campen, Leif Kobbelt","author_ids":"2304114, 1763010","abstract":"We introduce <i>Dual Strip Weaving</i>, a novel concept for the interactive design of quad layouts, i.e. partitionings of freeform surfaces into quadrilateral patch networks. In contrast to established tools for the design of quad layouts or subdivision base meshes, which are often based on creating individual vertices, edges, and quads, our method takes a more global perspective, operating on a higher level of abstraction: the atomic operation of our method is the creation of an entire cyclic strip, delineating a large number of quad patches at once. The global consistency-preserving nature of this approach reduces demands on the user's expertise by requiring less advance planning. Efficiency is achieved using a novel method at the heart of our system, which automatically proposes geometrically and topologically suitable strips to the user. Based on this we provide interaction tools to influence the design process to any desired degree and visual guides to support the user in this task.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"aa387cfc110013f344cf1014bad80e956f802169","venue_1":"ACM Trans. Graph.","year":"2009","title":"Automatic and topology-preserving gradient mesh generation for image vectorization","authors":"Yu-Kun Lai, Shi-Min Hu, Ralph R. Martin","author_ids":"7827503, 1686809, 4326042","abstract":"<i>Gradient mesh</i> vector graphics representation, used in commercial software, is a regular grid with specified position and color, and their gradients, at each grid point. Gradient meshes can compactly represent smoothly changing data, and are typically used for single objects. This paper advances the state of the art for gradient meshes in several significant ways. Firstly, we introduce a <i>topology-preserving</i> gradient mesh representation which allows an arbitrary number of <i>holes</i>. This is important, as objects in images often have holes, either due to occlusion, or their 3D structure. Secondly, our algorithm uses the concept of image manifolds, adapting surface parameterization and fitting techniques to generate the gradient mesh in a <i>fully automatic</i> manner. Existing gradient-mesh algorithms require manual interaction to guide grid construction, and to cut objects with holes into disk-like regions. Our new algorithm is empirically at least 10 times <i>faster</i> than previous approaches. Furthermore, image segmentation can be used with our new algorithm to provide automatic gradient mesh generation for a <i>whole image</i>. Finally, fitting errors can be simply controlled to balance quality with storage.","cites":"33","conferencePercentile":"49.44751381"},{"venue":"ACM Trans. Graph.","id":"6ed7ba552166e4027730df69493f717a532bc32e","venue_1":"ACM Trans. Graph.","year":"2014","title":"Mesh saliency via spectral processing","authors":"Ran Song, Yonghuai Liu, Ralph R. Martin, Paul L. Rosin","author_ids":"3094070, 1990125, 4326042, 1734823","abstract":"We propose a novel method for detecting mesh saliency, a perceptually-based measure of the importance of a local region on a 3D surface mesh. Our method incorporates <i>global</i> considerations by making use of spectral attributes of the mesh, unlike most existing methods which are typically based on <i>local</i> geometric cues. We first consider the properties of the log-Laplacian spectrum of the mesh. Those frequencies which show differences from expected behaviour capture saliency in the frequency domain. Information about these frequencies is considered in the spatial domain at multiple spatial scales to localise the salient features and give the final salient areas. The effectiveness and robustness of our approach are demonstrated by comparisons to previous approaches on a range of test models. The benefits of the proposed method are further evaluated in applications such as mesh simplification, mesh segmentation, and scan integration, where we show how incorporating mesh saliency can provide improved results.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"81f2b7a85fde6ad8846fc399dc07e5ebc72bac30","venue_1":"ACM Trans. Graph.","year":"2013","title":"Integer-grid maps for reliable quad meshing","authors":"David Bommes, Marcel Campen, Hans-Christian Ebke, Pierre Alliez, Leif Kobbelt","author_ids":"1825432, 2304114, 2889614, 1782393, 1763010","abstract":"Quadrilateral remeshing approaches based on global parametrization enable many desirable mesh properties. Two of the most important ones are (1) high regularity due to explicit control over irregular vertices and (2) smooth distribution of distortion achieved by convex variational formulations. Apart from these strengths, state-of-the-art techniques suffer from limited reliability on real-world input data, i.e. the determined map might have degeneracies like (local) non-injectivities and consequently often cannot be used directly to generate a quadrilateral mesh. In this paper we propose a novel convex Mixed-Integer Quadratic Programming (MIQP) formulation which ensures by construction that the resulting map is within the class of so called Integer-Grid Maps that are guaranteed to imply a quad mesh. In order to overcome the NP-hardness of MIQP and to be able to remesh typical input geometries in acceptable time we propose two additional problem specific optimizations: a complexity reduction algorithm and singularity separating conditions. While the former decouples the dimension of the MIQP search space from the input complexity of the triangle mesh and thus is able to dramatically speed up the computation without inducing inaccuracies, the latter improves the continuous relaxation, which is crucial for the success of modern MIQP optimizers. Our experiments show that the reliability of the resulting algorithm does not only annihilate the main drawback of parametrization based quad-remeshing but moreover enables the global search for high-quality coarse quad layouts - a difficult task solely tackled by greedy methodologies before.","cites":"39","conferencePercentile":"90.49773756"},{"venue":"ACM Trans. Graph.","id":"7c031ae048c58a4fa48d5fc547d0d95268fa5890","venue_1":"ACM Trans. Graph.","year":"2014","title":"Diffusion pruning for rapidly and robustly selecting global correspondences using local isometry","authors":"Gary K. L. Tam, Ralph R. Martin, Paul L. Rosin, Yu-Kun Lai","author_ids":"2988923, 4326042, 1734823, 7827503","abstract":"Finding correspondences between two surfaces is a fundamental operation in various applications in computer graphics and related fields. Candidate correspondences can be found by matching local signatures, but as they only consider local geometry, many are globally inconsistent. We provide a novel algorithm to prune a set of candidate correspondences to those most likely to be globally consistent. Our approach can handle articulated surfaces, and ones related by a deformation which is globally nonisometric, provided that the deformation is locally approximately isometric. Our approach uses an efficient diffusion framework, and only requires geodesic distance calculations in small neighbourhoods, unlike many existing techniques which require computation of global geodesic distances. We demonstrate that, for typical examples, our approach provides significant improvements in accuracy, yet also reduces time and memory costs by a factor of several hundred compared to existing pruning techniques. Our method is furthermore insensitive to holes, unlike many other methods.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"5fa66e8c4047fc55695f1321ed57d2c23a8bd861","venue_1":"ACM Trans. Graph.","year":"2007","title":"Joint bilateral upsampling","authors":"Johannes Kopf, Michael F. Cohen, Dani Lischinski, Matthew Uyttendaele","author_ids":"2891193, 1694613, 1684384, 2262291","abstract":"Image analysis and enhancement tasks such as tone mapping, colorization, stereo depth, and photomontage, often require computing a solution (e.g., for exposure, chromaticity, disparity, labels) over the pixel grid. Computational and memory costs often require that a smaller solution be run over a downsampled image. Although general purpose upsampling methods can be used to interpolate the low resolution solution to the full resolution, these methods generally assume a smoothness prior for the interpolation.\n We demonstrate that in cases, such as those above, the available high resolution input image may be leveraged as a prior in the context of a joint bilateral upsampling procedure to produce a better high resolution solution. We show results for each of the applications above and compare them to traditional upsampling methods.","cites":"277","conferencePercentile":"98.4"},{"venue":"ACM Trans. Graph.","id":"84ba078292adf148662cd18661b920c189124875","venue_1":"ACM Trans. Graph.","year":"2014","title":"Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information","authors":"Kang Chen, Yu-Kun Lai, Yu-Xin Wu, Ralph R. Martin, Shi-Min Hu","author_ids":"1723702, 7827503, 1971886, 4326042, 1686809","abstract":"We present a novel solution to automatic semantic modeling of indoor scenes from a sparse set of low-quality RGB-D images. Such data presents challenges due to noise, low resolution, occlusion and missing depth information. We exploit the knowledge in a scene database containing 100s of indoor scenes with over 10,000 manually segmented and labeled mesh models of objects. In seconds, we output a visually plausible 3D scene, adapting these models and their parts to fit the input scans. Contextual relationships learned from the database are used to constrain reconstruction, ensuring semantic compatibility between both object models and parts. Small objects and objects with incomplete depth information which are difficult to recover reliably are processed with a two-stage approach. Major objects are recognized first, providing a known scene structure. 2D contour-based model retrieval is then used to recover smaller objects. Evaluations using our own data and two public datasets show that our approach can model typical real-world indoor scenes efficiently and robustly.","cites":"17","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"99cdc11621f728869f4bfe971752bf26d698abe4","venue_1":"ACM Trans. Graph.","year":"2015","title":"Real-time hyperlapse creation via optimal frame selection","authors":"Neel Joshi, Wolf Kienzle, Mike Toelle, Matthew Uyttendaele, Michael F. Cohen","author_ids":"2641664, 1932429, 2441615, 2262291, 1694613","abstract":"Long videos can be played much faster than real-time by recording only one frame per second or by dropping all but one frame each second, i.e., by creating a <i>timelapse.</i> Unstable hand-held moving videos can be <i>stabilized</i> with a number of recently described methods. Unfortunately, creating a stabilized timelapse, or <i>hyperlapse</i>, cannot be achieved through a simple combination of these two methods. Two hyperlapse methods have been previously demonstrated: one with high computational complexity and one requiring special sensors. We present an algorithm for creating hyperlapse videos that can handle significant high-frequency camera motion and runs in real-time on HD video. Our approach does not require sensor data, thus can be run on videos captured on any camera. We optimally select frames from the input video that best match a desired target speed-up while also resulting in the smoothest possible camera motion. We evaluate our approach using several input videos from a range of cameras and compare these results to existing methods.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"45392756fd0d437091d172e4cbbc37a66650555f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Automated video looping with progressive dynamism","authors":"Zicheng Liao, Neel Joshi, Hugues Hoppe","author_ids":"2928799, 2641664, 1688461","abstract":"Given a short video we create a representation that captures a spectrum of looping videos with varying levels of dynamism, ranging from a static image to a highly animated loop. In such a progressively dynamic video, scene liveliness can be adjusted interactively using a slider control. Applications include background images and slideshows, where the desired level of activity may depend on personal taste or mood. The representation also provides a segmentation of the scene into independently looping regions, enabling interactive local adjustment over dynamism. For a landscape scene, this control might correspond to selective animation and deanimation of grass motion, water ripples, and swaying trees. Converting arbitrary video to looping content is a challenging research problem. Unlike prior work, we explore an optimization in which each pixel automatically determines its own looping period. The resulting nested segmentation of static and dynamic scene regions forms an extremely compact representation.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"0b8dc050e191df6237d38841390af6fb5b0b6020","venue_1":"ACM Trans. Graph.","year":"2010","title":"Image deblurring using inertial measurement sensors","authors":"Neel Joshi, Sing Bing Kang, C. Lawrence Zitnick, Richard Szeliski","author_ids":"2641664, 1738740, 1699161, 1717841","abstract":"We present a deblurring algorithm that uses a hardware attachment coupled with a natural image prior to deblur images from consumer cameras. Our approach uses a combination of inexpensive gyroscopes and accelerometers in an energy optimization framework to estimate a blur function from the camera's acceleration and angular velocity during an exposure. We solve for the camera motion at a high sampling rate <i>during</i> an exposure and infer the latent image using a joint optimization. Our method is completely automatic, handles per-pixel, spatially-varying blur, and out-performs the current leading image-based methods. Our experiments show that it handles large kernels -- up to at least 100 pixels, with a typical size of 30 pixels. We also present a method to perform \"ground-truth\" measurements of camera motion blur. We use this method to validate our hardware and deconvolution approach. To the best of our knowledge, this is the first work that uses 6 DOF inertial sensors for dense, per-pixel spatially-varying image deblurring and the first work to gather dense ground-truth measurements for camera-shake blur.","cites":"119","conferencePercentile":"98.83040936"},{"venue":"ACM Trans. Graph.","id":"7e6bb2eb15e75c333426273f1266ad49a376421f","venue_1":"ACM Trans. Graph.","year":"2005","title":"High performance imaging using large camera arrays","authors":"Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala, Emilio R. Antúnez, Adam Barth, Andrew Adams, Mark Horowitz, Marc Levoy","author_ids":"2391035, 2641664, 2146731, 3129635, 2483141, 2908330, 4030789, 1764167, 1801789","abstract":"The advent of inexpensive digital image sensors and the ability to create photographs that combine information from a number of sensed images are changing the way we think about photography. In this paper, we describe a unique array of 100 custom video cameras that we have built, and we summarize our experiences using this array in a range of imaging applications. Our goal was to explore the capabilities of a system that would be inexpensive to produce in the future. With this in mind, we used simple cameras, lenses, and mountings, and we assumed that processing large numbers of images would eventually be easy and cheap. The applications we have explored include approximating a conventional single center of projection video camera with high performance along one or more axes, such as resolution, dynamic range, frame rate, and/or large aperture, and using multiple cameras to approximate a video camera with a large synthetic aperture. This permits us to capture a video light field, to which we can apply spatiotemporal view interpolation algorithms in order to digitally simulate time dilation and camera motion. It also permits us to create video sequences using custom non-uniform synthetic apertures.","cites":"290","conferencePercentile":"99.19354839"},{"venue":"ACM Trans. Graph.","id":"560861191aa0eaa5417284a0f1781026f7ab29a6","venue_1":"ACM Trans. Graph.","year":"2013","title":"QEx: robust quad mesh extraction","authors":"Hans-Christian Ebke, David Bommes, Marcel Campen, Leif Kobbelt","author_ids":"2889614, 1825432, 2304114, 1763010","abstract":"The most popular and actively researched class of quad remeshing techniques is the family of <i>parametrization based quad meshing methods</i>. They all strive to generate an <i>integer-grid map</i>, i.e. a parametrization of the input surface into R<sup>2</sup> such that the canonical grid of integer iso-lines forms a quad mesh when mapped back onto the surface in R<sup>3</sup>. An essential, albeit broadly neglected aspect of these methods is the <i>quad extraction</i> step, i.e. the materialization of an actual quad mesh from the mere \"quad texture\". Quad (mesh) extraction is often believed to be a trivial matter but quite the opposite is true: numerous special cases, ambiguities induced by numerical inaccuracies and limited solver precision, as well as imperfections in the maps produced by most methods (unless costly countermeasures are taken) pose significant challenges to the quad extractor. We present a method to sanitize a provided parametrization such that it becomes numerically consistent even in a limited precision floating point representation. Based on this we are able to provide a comprehensive and sound description of how to perform quad extraction robustly and without the need for any complex tolerance thresholds or disambiguation rules. On top of that we develop a novel strategy to cope with common local fold-overs in the parametrization. This allows our method, dubbed <i>QEx</i>, to generate all-quadrilateral meshes where otherwise holes, non-quad polygons or no output at all would have been produced. We thus enable the practical use of an entire class of maps that was previously considered defective. Since state of the art quad meshing methods spend a significant share of their run time solely to prevent local fold-overs, using our method it is now possible to obtain quad meshes significantly quicker than before. We also provide libQEx, an open source C++ reference implementation of our method and thus significantly lower the bar to enter the field of quad meshing.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"77e6f776727a6fd35722d89a8ce03833b6a318bd","venue_1":"ACM Trans. Graph.","year":"2012","title":"Dual loops meshing: quality quad layouts on manifolds","authors":"Marcel Campen, David Bommes, Leif Kobbelt","author_ids":"2304114, 1825432, 1763010","abstract":"We present a theoretical framework and practical method for the automatic construction of simple, all-quadrilateral patch layouts on manifold surfaces. The resulting layouts are coarse, surface-embedded cell complexes well adapted to the geometric structure, hence they are ideally suited as domains and base complexes for surface parameterization, spline fitting, or subdivision surfaces and can be used to generate quad meshes with a high-level patch structure that are advantageous in many application scenarios. Our approach is based on the careful construction of the layout graph's combinatorial dual. In contrast to the primal this dual perspective provides direct control over the globally interdependent structural constraints inherent to quad layouts. The dual layout is built from curvature-guided, crossing loops on the surface. A novel method to construct these efficiently in a geometry- and structure-aware manner constitutes the core of our approach.","cites":"24","conferencePercentile":"69.6969697"},{"venue":"ACM Trans. Graph.","id":"28b05f31d8f53accdcec7150a134146e8c85e56f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Theory, analysis and applications of 2D global illumination","authors":"Wojciech Jarosz, Volker Schönefeld, Leif Kobbelt, Henrik Wann Jensen","author_ids":"1953515, 2941281, 1763010, 1730025","abstract":"We investigate global illumination in 2D and show how this simplified problem domain leads to practical insights for 3D rendering.\n We first derive a full theory of 2D light transport by introducing 2D analogs to radiometric quantities such as flux and radiance, and deriving a 2D rendering equation. We use our theory to show how to implement algorithms such as Monte Carlo raytracing, path tracing, irradiance caching, and photon mapping in 2D, and demonstrate that these algorithms can be analyzed more easily in this domain while still providing insights for 3D rendering.\n We apply our theory to develop several practical improvements to the irradiance caching algorithm. We perform a full second-order analysis of diffuse indirect illumination, first in 2D, and then in 3D by deriving the irradiance Hessian, and show how this leads to increased accuracy and performance for irradiance caching. We propose second-order Taylor expansion from cache points, which results in more accurate irradiance reconstruction. We also introduce a novel error metric to guide cache point placement by analyzing the error produced by irradiance caching. Our error metric naturally supports anisotropic reconstruction and, in our preliminary study, resulted in an order of magnitude less error than the &#8220;split-sphere&#8221; heuristic when using the same number of cache points.","cites":"9","conferencePercentile":"15.15151515"},{"venue":"ACM Trans. Graph.","id":"598929ba3dd09ed7f263d6026512f8bfd513988a","venue_1":"ACM Trans. Graph.","year":"2009","title":"Mixed-integer quadrangulation","authors":"David Bommes, Henrik Zimmer, Leif Kobbelt","author_ids":"1825432, 3160435, 1763010","abstract":"We present a novel method for quadrangulating a given triangle mesh. After constructing an as smooth as possible symmetric cross field satisfying a sparse set of directional constraints (to capture the geometric structure of the surface), the mesh is cut open in order to enable a low distortion unfolding. Then a seamless globally smooth parametrization is computed whose iso-parameter lines follow the cross field directions. In contrast to previous methods, sparsely distributed directional constraints are sufficient to automatically determine the appropriate number, type and position of singularities in the quadrangulation. Both steps of the algorithm (cross field and parametrization) can be formulated as a mixed-integer problem which we solve very efficiently by an adaptive greedy solver. We show several complex examples where high quality quad meshes are generated in a fully automatic manner.","cites":"76","conferencePercentile":"88.67403315"},{"venue":"ACM Trans. Graph.","id":"13a3d0a3d7e41d4bd564e6a8d99cf3ddeb3b0ec5","venue_1":"ACM Trans. Graph.","year":"2016","title":"Multiphase SPH simulation for interactive fluids and solids","authors":"Xiao Yan, Yun-Tao Jiang, Chen-Feng Li, Ralph R. Martin, Shi-Min Hu","author_ids":"3932013, 7912180, 7431679, 4326042, 1686809","abstract":"This work extends existing multiphase-fluid SPH frameworks to cover solid phases, including deformable bodies and granular materials. In our extended multiphase SPH framework, the distribution and shapes of all phases, both fluids and solids, are uniformly represented by their volume fraction functions. The dynamics of the multiphase system is governed by conservation of mass and momentum within different phases. The behavior of individual phases and the interactions between them are represented by corresponding constitutive laws, which are functions of the volume fraction fields and the velocity fields. Our generalized multiphase SPH framework does not require separate equations for specific phases or tedious interface tracking. As the distribution, shape and motion of each phase is represented and resolved in the same way, the proposed approach is robust, efficient and easy to implement. Various simulation results are presented to demonstrate the capabilities of our new multiphase SPH framework, including deformable bodies, granular materials, interaction between multiple fluids and deformable solids, flow in porous media, and dissolution of deformable solids.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"060c9b790f1e3cc407e9ece638671e1850b3855a","venue_1":"ACM Trans. Graph.","year":"2007","title":"Capturing and viewing gigapixel images","authors":"Johannes Kopf, Matthew Uyttendaele, Oliver Deussen, Michael F. Cohen","author_ids":"2891193, 2262291, 1850438, 1694613","abstract":"We present a system to capture and view \"Gigapixel images\": very high resolution, high dynamic range, and wide angle imagery consisting of several billion pixels each. A specialized camera mount, in combination with an automated pipeline for alignment, exposure compensation, and stitching, provide the means to acquire Gigapixel images with a standard camera and lens. More importantly, our novel viewer enables exploration of such images at interactive rates over a network, while dynamically and smoothly interpolating the projection between perspective and curved projections, and simultaneously modifying the tone-mapping to ensure an optimal view of the portion of the scene being viewed.","cites":"75","conferencePercentile":"67.2"},{"venue":"ACM Trans. Graph.","id":"33141f1be8f0fc562d3111780c6c2a3ac462bc32","venue_1":"ACM Trans. Graph.","year":"2010","title":"GradientShop: A gradient-domain optimization framework for image and video filtering","authors":"Pravin Bhat, C. Lawrence Zitnick, Michael F. Cohen, Brian Curless","author_ids":"2300778, 1699161, 1694613, 1810052","abstract":"We present an optimization framework for exploring gradient-domain solutions for image and video processing. The proposed framework unifies many of the key ideas in the gradient-domain literature under a single optimization formulation. Our hope is that this generalized framework will allow the reader to quickly gain a general understanding of the field and contribute new ideas of their own.\n We propose a novel metric for measuring local gradient saliency that identifies salient gradients that give rise to long, coherent edges, even when the individual gradients are faint. We present a general weighting scheme for gradient constraints that improves the visual appearance of results. We also provide a solution for applying gradient-domain filters to videos and video streams in a coherent manner.\n Finally, we demonstrate the utility of our formulation in creating effective yet simple to implement solutions for various image-processing tasks. To exercise our formulation we have created a new saliency-based sharpen filter and a pseudo image-relighting application. We also revisit and improve upon previously defined filters such as nonphotorealistic rendering, image deblocking, and sparse data interpolation over images (e.g., colorization using optimization).","cites":"72","conferencePercentile":"92.69005848"},{"venue":"ACM Trans. Graph.","id":"064399fa688b76c57864f24cf70cfc6410a9ad59","venue_1":"ACM Trans. Graph.","year":"2014","title":"Level-of-detail quad meshing","authors":"Hans-Christian Ebke, Marcel Campen, David Bommes, Leif Kobbelt","author_ids":"2889614, 2304114, 1825432, 1763010","abstract":"The most effective and popular tools for obtaining feature aligned quad meshes from triangular input meshes are based on cross field guided parametrization. These methods are incarnations of a conceptual three-step pipeline: (1) cross field computation, (2) field-guided surface parametrization, (3) quad mesh extraction. While in most meshing scenarios the user prescribes a desired target quad size or edge length, this information is typically taken into account from step 2 onwards only, but not in the cross field computation step. This turns into a problem in the presence of small scale geometric or topological features or noise in the input mesh: closely placed singularities are induced in the cross field, which are not properly reproducible by vertices in a quad mesh with the prescribed edge length, causing severe distortions or even failure of the meshing algorithm. We reformulate the construction of cross fields as well as field-guided parametrizations in a scale-aware manner which effectively suppresses densely spaced features and noise of geometric as well as topological kind. Dominant large-scale features are adequately preserved in the output by relying on the unaltered input mesh as the computational domain.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"3072998af1ce4b2128ced58af000348a3b2a667f","venue_1":"ACM Trans. Graph.","year":"2004","title":"Flash photography enhancement via intrinsic relighting","authors":"Elmar Eisemann, Frédo Durand","author_ids":"1737690, 1728125","abstract":"We enhance photographs shot in dark environments by combining a picture taken with the available light and one taken with the flash. We preserve the ambiance of the original lighting and insert the sharpness from the flash image. We use the bilateral filter to decompose the images into detail and large scale. We reconstruct the image using the large scale of the available lighting and the detail of the flash. We detect and correct flash shadows. This combines the advantages of available illumination and flash photography.","cites":"219","conferencePercentile":"83.69565217"},{"venue":"ACM Trans. Graph.","id":"3ffa280bbba607ba3dbc0f6adcc557c85453016c","venue_1":"ACM Trans. Graph.","year":"2003","title":"Non-iterative, feature-preserving mesh smoothing","authors":"Thouis R. Jones, Frédo Durand, Mathieu Desbrun","author_ids":"2646100, 1728125, 1716096","abstract":"With the increasing use of geometry scanners to create 3D models, there is a rising need for fast and robust mesh smoothing to remove inevitable noise in the measurements. While most previous work has favored diffusion-based iterative techniques for feature-preserving smoothing, we propose a radically different approach, based on robust statistics and local first-order predictors of the surface. The robustness of our local estimates allows us to derive a<i> non-iterative</i> feature-preserving filtering technique applicable to arbitrary \"triangle soups\". We demonstrate its simplicity of implementation and its efficiency, which make it an excellent solution for smoothing large, noisy, and non-manifold meshes.","cites":"183","conferencePercentile":"77.41935484"},{"venue":"ACM Trans. Graph.","id":"bb9970e633331129190c8504cbcc82422da6dd11","venue_1":"ACM Trans. Graph.","year":"2006","title":"Deringing cartoons by image analogies","authors":"Guangyu Wang, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"8350127, 1720633, 1714602","abstract":"In this article, we propose a novel method to reduce ringing artifacts in BDCT-encoded cartoon images using image analogies. The quantization procedure of BDCT compression (such as JPEG and MPEG) introduces annoying visual artifacts. Our main focus is on the removal of ringing artifacts that is seldom addressed by existing methods. In the proposed method, the contaminated image is modeled as a Markov random field (MRF). We &#8220;learn&#8221; the behavior of contamination by extracting massive numbers of artifact patterns from a training set, and organizing them using tree-structured vector quantization (TSVQ). Instead of <i>postfiltering</i> the input contaminated image, we <i>synthesize</i> an artifact-reduced image. Our method is noniterative and hence, can remove artifacts within a very short period of time. We show that substantial improvement is achieved using the proposed method in terms of visual quality and statistics.","cites":"10","conferencePercentile":"6.944444444"},{"venue":"ACM Trans. Graph.","id":"c67192cb7c82d2a0516b656909985823a5b2aba0","venue_1":"ACM Trans. Graph.","year":"2014","title":"First-person hyper-lapse videos","authors":"Johannes Kopf, Michael F. Cohen, Richard Szeliski","author_ids":"2891193, 1694613, 1717841","abstract":"We present a method for converting first-person videos, for example, captured with a helmet camera during activities such as rock climbing or bicycling, into <i>hyper-lapse</i> videos, i.e., time-lapse videos with a smoothly moving camera. At high speed-up rates, simple frame sub-sampling coupled with existing video stabilization methods does not work, because the erratic camera shake present in first-person videos is amplified by the speed-up. Our algorithm first reconstructs the 3D input camera path as well as dense, per-frame proxy geometries. We then optimize a novel camera path for the output video that passes near the input cameras while ensuring that the virtual camera looks in directions that can be rendered well from the input. Finally, we generate the novel smoothed, time-lapse video by rendering, stitching, and blending appropriately selected source frames for each output frame. We present a number of results for challenging videos that cannot be processed using traditional techniques.","cites":"26","conferencePercentile":"95.0617284"},{"venue":"ACM Trans. Graph.","id":"d7d0acfdeb365c85d414bd536cb68cc2c06c0ab1","venue_1":"ACM Trans. Graph.","year":"2006","title":"Manga colorization","authors":"Yingge Qu, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"2549934, 1720633, 1714602","abstract":"This paper proposes a novel colorization technique that propagates color over regions exhibiting pattern-continuity as well as intensity-continuity. The proposed method works effectively on colorizing black-and-white manga which contains intensive amount of strokes, hatching, halftoning and screening. Such fine details and discontinuities in intensity introduce many difficulties to intensity-based colorization methods. Once the user scribbles on the drawing, a local, statistical based pattern feature obtained with Gabor wavelet filters is applied to measure the pattern-continuity. The boundary is then propagated by the level set method that monitors the pattern-continuity. Regions with open boundaries or multiple disjointed regions with similar patterns can be sensibly segmented by a single scribble. With the segmented regions, various colorization techniques can be applied to replace colors, colorize with stroke preservation, or even convert pattern to shading. Several results are shown to demonstrate the effectiveness and convenience of the proposed method.","cites":"31","conferencePercentile":"25.92592593"},{"venue":"ACM Trans. Graph.","id":"87b8ce02e57879951b3389d0d655ca242d2e291c","venue_1":"ACM Trans. Graph.","year":"2013","title":"Putting holes in holey geometry: topology change for arbitrary surfaces","authors":"Gilbert Louis Bernstein, Christopher Wojtan","author_ids":"2541837, 2106076","abstract":"This paper presents a method for computing topology changes for triangle meshes in an interactive geometric modeling environment. Most triangle meshes in practice do not exhibit desirable geometric properties, so we develop a solution that is independent of standard assumptions and robust to geometric errors. Specifically, we provide the first method for topology change applicable to arbitrary non-solid, non-manifold, non-closed, self-intersecting surfaces. We prove that this new method for topology change produces the expected conventional results when applied to solid (closed, manifold, non-self-intersecting) surfaces---that is, we prove a backwards-compatibility property relative to prior work. Beyond solid surfaces, we present empirical evidence that our method remains tolerant to a variety of surface aberrations through the incorporation of a novel error correction scheme. Finally, we demonstrate how topology change applied to non-solid objects enables wholly new and useful behaviors.","cites":"9","conferencePercentile":"29.63800905"},{"venue":"ACM Trans. Graph.","id":"4a3a10f83b48f15651187b30177f0ef4d2217c97","venue_1":"ACM Trans. Graph.","year":"2015","title":"Lillicon: using transient widgets to create scale variations of icons","authors":"Gilbert Louis Bernstein, Wilmot Li","author_ids":"2541837, 2812691","abstract":"Good icons are legible, and legible icons are scale-dependent. Experienced icon designers use a set of common strategies to create legible scale variations of icons, but executing those strategies with current tools can be challenging. In part, this is because many apparent objects, like hairlines formed by negative space, are not explicitly represented as objects in vector drawings. We present transient widgets as a mechanism for selecting and manipulating apparent objects that is independent of the underlying drawing representation. We implement transient widgets using a constraint-based editing framework; demonstrate their utility for performing the kinds of edits most common when producing scale variations of icons; and report qualitative feedback on the system from professional icon designers.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"2eac47ed4e8c81b71cc196f7fc0f7a472deed933","venue_1":"ACM Trans. Graph.","year":"2016","title":"Ebb: A DSL for Physical Simulation on CPUs and GPUs","authors":"Gilbert Louis Bernstein, Chinmayee Shah, Crystal Lemire, Zach DeVito, Matthew Fisher, Philip Levis, Pat Hanrahan","author_ids":"2541837, 1691018, 3086294, 2375710, 2676553, 4862966, 4982303","abstract":"Designing programming environments for physical simulation is challenging because simulations rely on diverse algorithms and geometric domains. These challenges are compounded when we try to run efficiently on heterogeneous parallel architectures. We present Ebb, a Domain-Specific Language (DSL) for simulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs, Ebb uses a three-layer architecture to separate (1) simulation code, (2) definition of data structures for geometric domains, and (3) runtimes supporting parallel architectures. Different geometric domains are implemented as libraries that use a common, unified, relational data model. By structuring the simulation framework in this way, programmers implementing simulations can focus on the physics and algorithms for each simulation without worrying about their implementation on parallel computers. Because the geometric domain libraries are all implemented using a common runtime based on relations, new geometric domains can be added as needed, without specifying the details of memory management, mapping to different parallel architectures, or having to expand the runtime&#8217;s interface.\n We evaluate Ebb by comparing it to several widely used simulations, demonstrating comparable performance to handwritten GPU code where available, and surpassing existing CPU performance optimizations by up to 9 &#215; when no GPU code exists.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"fe7f8e3fb97ca0e98c0d3552ede19f2ad4e99e81","venue_1":"ACM Trans. Graph.","year":"2015","title":"Realtime style transfer for unlabeled heterogeneous human motion","authors":"Shihong Xia, Congyi Wang, Jinxiang Chai, Jessica K. Hodgins","author_ids":"1688183, 2768528, 1759700, 1788773","abstract":"This paper presents a novel solution for realtime generation of stylistic human motion that automatically transforms unlabeled, heterogeneous motion data into new styles. The key idea of our approach is an <i>online learning</i> algorithm that automatically constructs a series of local mixtures of autoregressive models (MAR) to capture the complex relationships between styles of motion. We construct local MAR models on the fly by searching for the closest examples of each input pose in the database. Once the model parameters are estimated from the training data, the model adapts the current pose with simple linear transformations. In addition, we introduce an efficient local regression model to predict the timings of synthesized poses in the output style. We demonstrate the power of our approach by transferring stylistic human motion for a wide variety of actions, including walking, running, punching, kicking, jumping and transitions between those behaviors. Our method achieves superior performance in a comparison against alternative methods. We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system.","cites":"7","conferencePercentile":"81.2244898"},{"venue":"ACM Trans. Graph.","id":"0e18f6725d1a55a8e16382c34d369a1b95e3175c","venue_1":"ACM Trans. Graph.","year":"2002","title":"Improving noise","authors":"Ken Perlin","author_ids":"2307214","abstract":"Two deficiencies in the original Noise algorithm are corrected: second order interpolation discontinuity and unoptimal gradient computation. With these defects corrected, Noise both looks better and runs faster. The latter change also makes it easier to define a uniform mathematical reference standard.","cites":"125","conferencePercentile":"54"},{"venue":"ACM Trans. Graph.","id":"9d2fcb7c57bb99584bb8fb841ccc12b1b3fb0bf0","venue_1":"ACM Trans. Graph.","year":"2003","title":"Measuring bidirectional texture reflectance with a kaleidoscope","authors":"Jefferson Y. Han, Ken Perlin","author_ids":"2183554, 2307214","abstract":"We describe a new technique for measuring the bidirectional texture function (BTF) of a surface that requires no mechanical movement, can measure surfaces <i>in situ</i> under arbitrary lighting conditions, and can be made small, portable and inexpensive. The enabling innovation is the use of a tapered kaleidoscope, which allows a camera to view the same surface sample simultaneously from many directions. Similarly, the surface can be simultaneously illuminated from many directions, using only a single structured light source. We describe the techniques of construction and measurement, and we show experimental results.","cites":"74","conferencePercentile":"36.55913978"},{"venue":"ACM Trans. Graph.","id":"7ce118a7ac5efee404ff58cb0147200d8c0d6b1b","venue_1":"ACM Trans. Graph.","year":"2009","title":"The UnMousePad: an interpolating multi-touch force-sensing input pad","authors":"Ilya D. Rosenberg, Ken Perlin","author_ids":"1916506, 2307214","abstract":"Recently, there has been great interest in multi-touch interfaces. Such devices have taken the form of camera-based systems such as Microsoft Surface [de los Reyes et al. 2007] and Perceptive Pixel's FTIR Display [Han 2005] as well as hand-held devices using capacitive sensors such as the Apple iPhone [Jobs et al. 2008]. However, optical systems are inherently bulky while most capacitive systems are only practical in small form factors and are limited in their application since they respond only to human touch and are insensitive to variations in pressure [Westerman 1999].\n We have created the UnMousePad, a flexible and inexpensive multitouch input device based on a newly developed pressure-sensing principle called Interpolating Force Sensitive Resistance. IFSR sensors can acquire high-quality anti-aliased pressure images at high frame rates. They can be paper-thin, flexible, and transparent and can easily be scaled to fit on a portable device or to cover an entire table, floor or wall. The UnMousePad can sense three orders of magnitude of pressure variation, and can be used to distinguish multiple fingertip touches while simultaneously tracking pens and styli with a positional accuracy of 87 dpi, and can sense the pressure distributions of objects placed on its surface.\n In addition to supporting multi-touch interaction, IFSR is a general pressure imaging technology that can be incorporated into shoes, tennis racquets, hospital beds, factory assembly lines and many other applications. The ability to measure high-quality pressure images at low cost has the potential to dramatically improve the way that people interact with machines and the way that machines interact with the world.","cites":"67","conferencePercentile":"85.91160221"},{"venue":"ACM Trans. Graph.","id":"da8b554b2d00841557a9bc8adc06ac5cf421489f","venue_1":"ACM Trans. Graph.","year":"2016","title":"HexEx: robust hexahedral mesh extraction","authors":"Max Lyon, David Bommes, Leif Kobbelt","author_ids":"3430440, 1825432, 1763010","abstract":"State-of-the-art hex meshing algorithms consist of three steps: Frame-field design, parametrization generation, and mesh extraction. However, while the first two steps are usually discussed in detail, the last step is often not well studied. In this paper, we fully concentrate on reliable mesh extraction.\n Parametrization methods employ computationally expensive countermeasures to avoid mapping input tetrahedra to degenerate or flipped tetrahedra in the parameter domain because such a parametrization does not define a proper hexahedral mesh. Nevertheless, there is no known technique that can guarantee the complete absence of such artifacts.\n We tackle this problem from the other side by developing a mesh extraction algorithm which is extremely robust against typical imperfections in the parametrization. First, a sanitization process cleans up numerical inconsistencies of the parameter values caused by limited precision solvers and floating-point number representation. On the sanitized parametrization, we extract vertices and so-called darts based on intersections of the integer grid with the parametric image of the tetrahedral mesh. The darts are reliably interconnected by tracing within the parametrization and thus define the topology of the hexahedral mesh. In a postprocessing step, we let certain pairs of darts cancel each other, counteracting the effect of flipped regions of the parametrization. With this strategy, our algorithm is able to robustly extract hexahedral meshes from imperfect parametrizations which previously would have been considered defective. The algorithm will be published as an open source library [Lyon et al. 2016].","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"50b29cb08f459aad0b60fb86205cdfff66202d85","venue_1":"ACM Trans. Graph.","year":"2014","title":"Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks","authors":"Jonathan Tompson, Murphy Stein, Yann LeCun, Ken Perlin","author_ids":"2704494, 3172913, 8227491, 2307214","abstract":"We present a novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image. Our method consists of the following stages: a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real-time pose recovery. As one possible application of this pipeline, we show state-of-the-art results for real-time puppeteering of a skinned hand-model.","cites":"58","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"1e1410689de847485d7369040517deb588335629","venue_1":"ACM Trans. Graph.","year":"2013","title":"Real-time drawing assistance through crowdsourcing","authors":"Alex Limpaecher, Nicolas Feltman, Adrien Treuille, Michael F. Cohen","author_ids":"2168405, 2759973, 3064395, 1694613","abstract":"We propose a new method for the large-scale collection and analysis of drawings by using a mobile game specifically designed to collect such data. Analyzing this crowdsourced drawing database, we build a spatially varying model of artistic consensus at the stroke level. We then present a surprisingly simple stroke-correction method which uses our artistic consensus model to improve strokes in real-time. Importantly, our auto-corrections run interactively and appear nearly invisible to the user while seamlessly preserving artistic intent. Closing the loop, the game itself serves as a platform for large-scale evaluation of the effectiveness of our stroke correction algorithm.","cites":"18","conferencePercentile":"70.13574661"},{"venue":"ACM Trans. Graph.","id":"5aeb051667cd47681656b4b470da81b0bc7fa827","venue_1":"ACM Trans. Graph.","year":"2011","title":"ShadowDraw: real-time user guidance for freehand drawing","authors":"Yong Jae Lee, C. Lawrence Zitnick, Michael F. Cohen","author_ids":"3191790, 1699161, 1694613","abstract":"We present ShadowDraw, a system for guiding the freeform drawing of objects. As the user draws, ShadowDraw dynamically updates a <i>shadow image</i> underlying the user's strokes. The shadows are suggestive of object contours that guide the user as they continue drawing. This paradigm is similar to tracing, with two major differences. First, we do not provide a single image from which the user can trace; rather ShadowDraw automatically blends relevant images from a large database to construct the shadows. Second, the system dynamically adapts to the user's drawings in real-time and produces suggestions accordingly. ShadowDraw works by efficiently matching local edge patches between the query, constructed from the current drawing, and a database of images. A hashing technique enforces both local and global similarity and provides sufficient speed for interactive feedback. Shadows are created by aggregating the edge maps from the best database matches, spatially weighted by their match scores. We test our approach with human subjects and show comparisons between the drawings that were produced with and without the system. The results show that our system produces more realistically proportioned line drawings.","cites":"58","conferencePercentile":"87.89473684"},{"venue":"ACM Trans. Graph.","id":"434d09d8a94ce7ae3d717ffdc62bf244aaeec28f","venue_1":"ACM Trans. Graph.","year":"2016","title":"Non-linear shape optimization using local subspace projections","authors":"Przemyslaw Musialski, Christian Hafner, Florian Rist, Michael Birsak, Michael Wimmer, Leif Kobbelt","author_ids":"2338275, 8175489, 3072427, 2143447, 1690158, 1763010","abstract":"In this paper we present a novel method for non-linear shape optimization of 3d objects given by their surface representation. Our method takes advantage of the fact that various shape properties of interest give rise to underdetermined design spaces implying the existence of many good solutions. Our algorithm exploits this by performing iterative projections of the problem to local subspaces where it can be solved much more efficiently using standard numerical routines. We demonstrate how this approach can be utilized for various shape optimization tasks using different shape parameterizations. In particular, we show how to efficiently optimize natural frequencies, mass properties, as well as the structural yield strength of a solid body. Our method is flexible, easy to implement, and very fast.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"552af7740d60187bed12a918a5e98439776960e0","venue_1":"ACM Trans. Graph.","year":"2015","title":"Quantized global parametrization","authors":"Marcel Campen, David Bommes, Leif Kobbelt","author_ids":"2304114, 1825432, 1763010","abstract":"Global surface parametrization often requires the use of cuts or charts due to non-trivial topology. In recent years a focus has been on so-called <i>seamless</i> parametrizations, where the transition functions across the cuts are rigid transformations with a rotation about some multiple of 90&#176;. Of particular interest, e.g. for quadrilateral meshing, paneling, or texturing, are those instances where in addition the translational part of these transitions is integral (or more generally: quantized). We show that finding not even the optimal, but just an arbitrary valid quantization (one that does not imply parametric degeneracies), is a complex combinatorial problem. We present a novel method that allows us to solve it, i.e. to find valid as well as good quality quantizations. It is based on an original approach to quickly construct solutions to linear Diophantine equation systems, exploiting the specific geometric nature of the parametrization problem. We thereby largely outperform the state-of-the-art, sometimes by several orders of magnitude.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"08c82d0d34ca51800de3eb446671c9cfab0fbea6","venue_1":"ACM Trans. Graph.","year":"2015","title":"Reduced-order shape optimization using offset surfaces","authors":"Przemyslaw Musialski, Thomas Auzinger, Michael Birsak, Michael Wimmer, Leif Kobbelt","author_ids":"2338275, 3307595, 2143447, 1690158, 1763010","abstract":"Given the 2-manifold surface of a 3d object, we propose a novel method for the computation of an offset surface with varying thickness such that the solid volume between the surface and its offset satisfies a set of prescribed constraints and at the same time minimizes a given objective functional. Since the constraints as well as the objective functional can easily be adjusted to specific application requirements, our method provides a flexible and powerful tool for shape optimization. We use manifold harmonics to derive a reduced-order formulation of the optimization problem, which guarantees a smooth offset surface and speeds up the computation independently from the input mesh resolution without affecting the quality of the result. The constrained optimization problem can be solved in a numerically robust manner with commodity solvers. Furthermore, the method allows simultaneously optimizing an inner and an outer offset in order to increase the degrees of freedom. We demonstrate our method in a number of examples where we control the physical mass properties of rigid objects for the purpose of 3d printing.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"3b7d2bb7e985dd81e06d658760daa55c0668747e","venue_1":"ACM Trans. Graph.","year":"2008","title":"Spectral quadrangulation with orientation and alignment control","authors":"Jin Huang, Muyang Zhang, Jin Ma, Xinguo Liu, Leif Kobbelt, Hujun Bao","author_ids":"3579556, 1698890, 2254808, 3227032, 1763010, 1679542","abstract":"This paper presents a new quadrangulation algorithm, extending the spectral surface quadrangulation approach where the coarse quadrangular structure is derived from the Morse-Smale complex of an eigenfunction of the Laplacian operator on the input mesh. In contrast to the original scheme, we provide flexible explicit controls of the shape, size, orientation and feature alignment of the quadrangular faces. We achieve this by proper selection of the optimal eigenvalue (shape), by adaption of the area term in the Laplacian operator (size), and by adding special constraints to the Laplace eigenproblem (orientation and alignment). By solving a generalized eigen-problem we can generate a scalar field on the mesh whose Morse-Smale complex is of high quality and satisfies all the user requirements. The final quadrilateral mesh is generated from the Morse-Smale complex by computing a globally smooth parametrization. Here we additionally introduce edge constraints to preserve user specified feature lines accurately.","cites":"55","conferencePercentile":"65.43209877"},{"venue":"ACM Trans. Graph.","id":"06b413caf76c2c1ba17574790489731db3df0d46","venue_1":"ACM Trans. Graph.","year":"2011","title":"Progressive photon beams","authors":"Wojciech Jarosz, Derek Nowrouzezahrai, Robert Thomas, Peter-Pike J. Sloan, Matthias Zwicker","author_ids":"1953515, 1795014, 2373118, 2838682, 1796846","abstract":"We present progressive photon beams, a new algorithm for rendering complex lighting in participating media. Our technique is efficient, robust to complex light paths, and handles heterogeneous media and anisotropic scattering while provably converging to the correct solution using a bounded memory footprint. We achieve this by extending the recent photon beams variant of volumetric photon mapping. We show how to formulate a progressive radiance estimate using photon beams, providing the convergence guarantees and bounded memory usage of progressive photon mapping. Progressive photon beams can robustly handle situations that are difficult for most other algorithms, such as scenes containing participating media and specular interfaces, with realistic light sources completely enclosed by refractive and reflective materials. Our technique handles heterogeneous media and also trivially supports stochastic effects such as depth-of-field and glossy materials. Finally, we show how progressive photon beams can be implemented efficiently on the GPU as a splatting operation, making it applicable to interactive and real-time applications. These features make our technique scalable, providing the same physically-based algorithm for interactive feedback and reference-quality, unbiased solutions.","cites":"23","conferencePercentile":"50.52631579"},{"venue":"ACM Trans. Graph.","id":"91f18a1a819ff255833cc16077d38f09be4c991e","venue_1":"ACM Trans. Graph.","year":"2016","title":"Authoring directed gaze for full-body motion capture","authors":"Tomislav Pejsa, Daniel Rakita, Bilge Mutlu, Michael Gleicher","author_ids":"2633572, 2854479, 2662943, 1776507","abstract":"We present an approach for adding directed gaze movements to characters animated using full-body motion capture. Our approach provides a comprehensive authoring solution that automatically infers plausible directed gaze from the captured body motion, provides convenient controls for manual editing, and adds synthetic gaze movements onto the original motion. The foundation of the approach is an abstract representation of gaze behavior as a sequence of gaze shifts and fixations toward targets in the scene. We present methods for automatic inference of this representation by analyzing the head and torso kinematics and scene features. We introduce tools for convenient editing of the gaze sequence and target layout that allow an animator to adjust the gaze behavior without worrying about the details of pose and timing. A synthesis component translates the gaze sequence into coordinated movements of the eyes, head, and torso, and blends these with the original body motion. We evaluate the effectiveness of our inference methods, the efficiency of the authoring process, and the quality of the resulting animation.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"158131a46fb35f3eb19fabc7d734bbec49037a27","venue_1":"ACM Trans. Graph.","year":"2003","title":"View-dependent displacement mapping","authors":"Lifeng Wang, Xi Wang, Xin Tong, Stephen Lin, Shi-Min Hu, Baining Guo, Harry Shum","author_ids":"3016024, 6172485, 1743927, 1686911, 1686809, 2738456, 1698102","abstract":"Significant visual effects arise from surface mesostructure, such as fine-scale shadowing, occlusion and silhouettes. To efficiently render its detailed appearance, we introduce a technique called view-dependent displacement mapping (VDM) that models surface displacements along the viewing direction. Unlike traditional displacement mapping, VDM allows for efficient rendering of self-shadows, occlusions and silhouettes without increasing the complexity of the underlying surface mesh. VDM is based on per-pixel processing, and with hardware acceleration it can render mesostructure with rich visual appearance in real time.","cites":"106","conferencePercentile":"58.06451613"},{"venue":"ACM Trans. Graph.","id":"28fd9d0f48176439a51ad4bc5d789a70aed5b170","venue_1":"ACM Trans. Graph.","year":"2015","title":"Surface turbulence for particle-based liquid simulations","authors":"Olivier Mercier, Cynthia Beauchemin, Nils Thürey, Theodore Kim, Derek Nowrouzezahrai","author_ids":"1786188, 1784767, 1786445, 1782832, 1795014","abstract":"We present a method to increase the apparent resolution of particle-based liquid simulations. Our method first outputs a dense, temporally coherent, regularized point set from a coarse particle-based liquid simulation. We then apply a surface-only Lagrangian wave simulation to this high-resolution point set. We develop novel methods for seeding and simulating waves over surface points, and use them to generate high-resolution details. We avoid error-prone surface mesh processing, and robustly propagate waves without the need for explicit connectivity information. Our seeding strategy combines a robust curvature evaluation with multiple bands of seeding oscillators, injects waves with arbitrarily fine-scale structures, and properly handles obstacle boundaries. We generate detailed fluid surfaces from coarse simulations as an independent post-process that can be applied to most particle-based fluid solvers.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"aa62e4daf37576e02b2e57ce3f7cdad80bee5085","venue_1":"ACM Trans. Graph.","year":"2010","title":"Manifold bootstrapping for SVBRDF capture","authors":"Yue Dong, Jiaping Wang, Xin Tong, John Snyder, Yanxiang Lan, Moshe Ben-Ezra, Baining Guo","author_ids":"1744268, 4912907, 1743927, 6314473, 2794374, 1760076, 2738456","abstract":"Manifold bootstrapping is a new method for data-driven modeling of real-world, spatially-varying reflectance, based on the idea that reflectance over a given material sample forms a low-dimensional manifold. It provides a high-resolution result in both the spatial and angular domains by decomposing reflectance measurement into two lower-dimensional phases. The first acquires <i>representatives</i> of high angular dimension but sampled sparsely over the surface, while the second acquires <i>keys</i> of low angular dimension but sampled densely over the surface.\n We develop a hand-held, high-speed BRDF capturing device for phase one measurements. A condenser-based optical setup collects a dense hemisphere of rays emanating from a single point on the target sample as it is manually scanned over it, yielding 10 BRDF point measurements per second. Lighting directions from 6 LEDs are applied at each measurement; these are amplified to a full 4D BRDF using the general (NDF-tabulated) microfacet model. The second phase captures <i>N</i>=20-200 images of the entire sample from a fixed view and lit by a varying area source. We show that the resulting <i>N</i>-dimensional keys capture much of the distance information in the original BRDF space, so that they effectively discriminate among representatives, though they lack sufficient angular detail to reconstruct the SVBRDF by themselves. At each surface position, a local linear combination of a small number of neighboring representatives is computed to match each key, yielding a high-resolution SVBRDF. A quick capture session (10-20 minutes) on simple devices yields results showing sharp and anisotropic specularity and rich spatial detail.","cites":"36","conferencePercentile":"64.03508772"},{"venue":"ACM Trans. Graph.","id":"3f191a5bd42f23ff0201f30b1b70723d87ebf78a","venue_1":"ACM Trans. Graph.","year":"2012","title":"Synthesizing open worlds with constraints using locally annealed reversible jump MCMC","authors":"Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah D. Goodman, Pat Hanrahan","author_ids":"2136573, 2406176, 2621519, 1945655, 4982303","abstract":"We present a novel Markov chain Monte Carlo (MCMC) algorithm that generates samples from transdimensional distributions encoding complex constraints. We use factor graphs, a type of graphical model, to encode constraints as factors. Our proposed MCMC method, called locally annealed reversible jump MCMC, exploits knowledge of how dimension changes affect the structure of the factor graph. We employ a sequence of annealed distributions during the sampling process, allowing us to explore the state space across different dimensionalities more freely. This approach is motivated by the application of layout synthesis where relationships between objects are characterized as constraints. In particular, our method addresses the challenge of synthesizing <i>open world</i> layouts where the number of objects are not fixed and optimal configurations for different numbers of objects may be drastically different. We demonstrate the applicability of our approach on two open world layout synthesis problems: coffee shops and golf courses.","cites":"31","conferencePercentile":"78.28282828"},{"venue":"ACM Trans. Graph.","id":"5a6627f6d7a25e228477606a16a05f2151cec39a","venue_1":"ACM Trans. Graph.","year":"2014","title":"Unifying points, beams, and paths in volumetric light transport simulation","authors":"Jaroslav Krivánek, Iliyan Georgiev, Toshiya Hachisuka, Petr Vévoda, Martin Sik, Derek Nowrouzezahrai, Wojciech Jarosz","author_ids":"8507738, 1975355, 2439297, 2784301, 2586357, 1795014, 1953515","abstract":"Efficiently computing light transport in participating media in a manner that is robust to variations in media density, scattering albedo, and anisotropy is a difficult and important problem in realistic image synthesis. While many specialized rendering techniques can efficiently resolve subsets of transport in specific media, no single approach can robustly handle all types of effects. To address this problem we unify volumetric density estimation, using point and beam estimators, and Monte Carlo solutions to the path integral formulation of the rendering and radiative transport equations. We extend multiple importance sampling to correctly handle combinations of these fundamentally different classes of estimators. This, in turn, allows us to develop a single rendering algorithm that correctly combines the benefits and mediates the limitations of these powerful volume rendering techniques.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"47caee76c4f36328b27781828233d73249f1b791","venue_1":"ACM Trans. Graph.","year":"2014","title":"Hierarchical diffusion curves for accurate automatic image vectorization","authors":"Guofu Xie, Xin Sun, Xin Tong, Derek Nowrouzezahrai","author_ids":"3304920, 1788730, 1743927, 1795014","abstract":"Diffusion curve primitives are a compact and powerful representation for vector images. While several vector image <i>authoring</i> tools leverage these representations, automatically and accurately vectorizing <i>arbitrary</i> raster images using diffusion curves remains a difficult problem. We automatically generate sparse diffusion curve vectorizations of raster images by fitting curves in the Laplacian domain. Our approach is fast, combines <i>Laplacian</i> and <i>bilaplacian</i> diffusion curve representations, and generates a hierarchical representation that accurately reconstructs both vector art and natural images. The key idea of our method is to trace curves in the Laplacian domain, which captures both sharp and smooth image features, across scales, more robustly than previous image- and gradient-domain fitting strategies. The sparse set of curves generated by our method accurately reconstructs images and often closely matches tediously hand-authored curve data. Also, our hierarchical curves are readily usable in all existing editing frameworks. We validate our method on a broad class of images, including natural images, synthesized images with turbulent multi-scale details, and traditional vector-art, as well as illustrating simple multi-scale abstraction and color editing results.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"5499286ce9a45324e21aa7e406cee15179a337ac","venue_1":"ACM Trans. Graph.","year":"2013","title":"Image-based reconstruction and synthesis of dense foliage","authors":"Derek Bradley, Derek Nowrouzezahrai, Paul A. Beardsley","author_ids":"1745149, 1795014, 1777539","abstract":"Flora is an element in many computer-generated scenes. But trees, bushes and plants have complex geometry and appearance, and are difficult to model manually. One way to address this is to capture models directly from the real world. Existing techniques have focused on extracting macro structure such as the branching structure of trees, or the structure of broad-leaved plants with a relatively small number of surfaces. This paper presents a finer scale technique to demonstrate for the first time the processing of densely leaved foliage - computation of 3D structure, plus extraction of statistics for leaf shape and the configuration of neighboring leaves. Our method starts with a mesh of a single exemplar leaf of the target foliage. Using a small number of images, point cloud data is obtained from multi-view stereo, and the exemplar leaf mesh is fitted non-rigidly to the point cloud over several iterations. In addition, our method learns a statistical model of leaf shape and appearance during the reconstruction phase, and a model of the transformations between neighboring leaves. This information is useful in two ways - to augment and increase leaf density in reconstructions of captured foliage, and to synthesize new foliage that conforms to a user-specified layout and density. The result of our technique is a dense set of captured leaves with realistic appearance, and a method for leaf synthesis. Our approach excels at reconstructing plants and bushes that are primarily defined by dense leaves and is demonstrated with multiple examples.","cites":"9","conferencePercentile":"29.63800905"},{"venue":"ACM Trans. Graph.","id":"06a625f1d180365761ee3611e5a4708e3502abf3","venue_1":"ACM Trans. Graph.","year":"2009","title":"Exploratory modeling with collaborative design spaces","authors":"Jerry O. Talton, Daniel Gibson, Lingfeng Yang, Pat Hanrahan, Vladlen Koltun","author_ids":"2220493, 5634833, 2406176, 4982303, 1770944","abstract":"Enabling ordinary people to create high-quality 3D models is a long-standing problem in computer graphics. In this work, we draw from the literature on design and human cognition to better understand the design processes of novice and casual modelers, whose goals and motivations are often distinct from those of professional artists. The result is a method for creating <i>exploratory</i> modeling tools, which are appropriate for casual users who may lack rigidly-specified goals or operational knowledge of modeling techniques.\n Our method is based on parametric design spaces, which are often high dimensional and contain wide quality variations. Our system estimates the distribution of good models in a space by tracking the modeling activity of a distributed community of users. These estimates drive intuitive modeling tools, creating a self-reinforcing system that becomes easier to use as more people participate.\n We present empirical evidence that the tools developed with our method allow rapid creation of complex, high-quality 3D models by users with no specialized modeling skills or experience. We report analyses of usage patterns garnered throughout the year-long deployment of one such tool, and demonstrate the generality of the method by applying it to several design spaces.","cites":"39","conferencePercentile":"58.83977901"},{"venue":"ACM Trans. Graph.","id":"3be8c42495513372c8309803d72fb12da4c7eece","venue_1":"ACM Trans. Graph.","year":"2013","title":"Joint importance sampling of low-order volumetric scattering","authors":"Iliyan Georgiev, Jaroslav Krivánek, Toshiya Hachisuka, Derek Nowrouzezahrai, Wojciech Jarosz","author_ids":"1975355, 8507738, 2439297, 1795014, 1953515","abstract":"Central to all Monte Carlo-based rendering algorithms is the construction of light transport paths from the light sources to the eye. Existing rendering approaches sample path vertices <i>incrementally</i> when constructing these light transport paths. The resulting probability density is thus a product of the <i>conditional</i> densities of each local sampling step, constructed without explicit control over the form of the final <i>joint</i> distribution of the complete path. We analyze why current incremental construction schemes often lead to high variance in the presence of participating media, and reveal that such approaches are an unnecessary legacy inherited from traditional surface-based rendering algorithms. We devise <i>joint importance sampling</i> of path vertices in participating media to construct paths that explicitly account for the product of all scattering and geometry terms along a sequence of vertices instead of just locally at a single vertex. This leads to a number of practical importance sampling routines to explicitly construct single-and double-scattering subpaths in anisotropically-scattering media. We demonstrate the benefit of our new sampling techniques, integrating them into several path-based rendering algorithms such as path tracing, bidirectional path tracing, and many-light methods. We also use our sampling routines to generalize deterministic shadow connections to <i>connection subpaths</i> consisting of two or three random decisions, to efficiently simulate higher-order multiple scattering. Our algorithms significantly reduce noise and increase performance in renderings with both isotropic and highly anisotropic, low-order scattering.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"dacbda2bd1c9ebfab4123bf5c49aa096544689ca","venue_1":"ACM Trans. Graph.","year":"2015","title":"Efficient and Accurate Spherical Kernel Integrals Using Isotropic Decomposition","authors":"Cyril Soler, Mahdi M. Bagher, Derek Nowrouzezahrai","author_ids":"1866568, 1885201, 1795014","abstract":"Spherical filtering is fundamental to many problems in image synthesis, such as computing the reflected light over a surface or anti-aliasing mirror reflections over a pixel. This operation is challenging since the profile of spherical filters (e.g., the view-evaluated BRDF or the geometry-warped pixel footprint, mentioned before) typically exhibits both spatial and rotational variation at each pixel, precluding precomputed solutions. We accelerate complex spherical filtering tasks using <i>isotropic spherical decomposition</i> (ISD), decomposing spherical filters into a linear combination of simpler isotropic kernels. Our general ISD is flexible to the choice of the isotropic kernels, and we demonstrate practical realizations of ISD on several problems in rendering: shading and prefiltering with spatially varying BRDFs, anti-aliasing-environment-mapped mirror reflections, and filtering of noisy reflectance data. Compared to previous basis-space rendering solutions, our shading solution generates ground-truth-quality results at interactive rates, avoiding costly reconstruction and large approximation errors.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"321e644af095509d09a9c1f670878856109e9fe9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Simulating the structure and texture of solid wood","authors":"Steve Marschner","author_ids":"2593798","abstract":"Wood is an important decorative material prized for its unique appearance. It is commonly rendered using artistically authored 2D color and bump textures, which reproduces color patterns on flat surfaces well. But the dramatic anisotropic specular figure caused by wood fibers, common in curly maple and other species, is harder to achieve. While suitable BRDF models exist, the texture parameter maps for these wood BRDFs are difficult to author---good results have been shown with elaborate measurements for small flat samples, but these models are not much used in practice. Furthermore, mapping 2D image textures onto 3D objects leads to distortion and inconsistencies. Procedural volumetric textures solve these geometric problems, but existing methods produce much lower quality than image textures. This paper aims to bring the best of all these techniques together: we present a comprehensive volumetric simulation of wood appearance, including growth rings, color variation, pores, rays, and growth distortions. The fiber directions required for anisotropic specular figure follow naturally from the distortions. Our results rival the quality of textures based on photographs, but with the consistency and convenience of a volumetric model. Our model is modular, with components that are intuitive to control, fast to compute, and require minimal storage.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"078f6dd6590b5688f5d3c0a0901f198cedeb6bf2","venue_1":"ACM Trans. Graph.","year":"2005","title":"Image-based spatio-temporal modeling and view interpolation of dynamic events","authors":"Sundar Vedula, Simon Baker, Takeo Kanade","author_ids":"3073266, 1737297, 7642093","abstract":"We present an approach for modeling and rendering a dynamic, real-world event from an arbitrary viewpoint, and at any time, using images captured from multiple video cameras. The event is modeled as a nonrigidly varying dynamic scene, captured by many images from different viewpoints, at discrete times. First, the spatio-temporal geometric properties (shape and instantaneous motion) are computed. The view synthesis problem is then solved using a reverse mapping algorithm, ray-casting across space and time, to compute a novel image from any viewpoint in the 4D space of position and time. Results are shown on real-world events captured in the CMU 3D Room, by creating synthetic renderings of the event from novel, arbitrary positions in space and time. Multiple such recreated renderings can be put together to create retimed fly-by movies of the event, with the resulting visual experience richer than that of a regular video clip, or switching between images from multiple cameras.","cites":"85","conferencePercentile":"57.66129032"},{"venue":"ACM Trans. Graph.","id":"0dc00690a27d051549a45de873ffa762bd7d02bf","venue_1":"ACM Trans. Graph.","year":"2010","title":"A multi-layered display with water drops","authors":"Peter C. Barnum, Srinivasa G. Narasimhan, Takeo Kanade","author_ids":"2754357, 1779052, 7642093","abstract":"We present a multi-layered display that uses water drops as voxels. Water drops refract most incident light, making them excellent wide-angle lenses. Each 2D layer of our display can exhibit arbitrary visual content, creating a layered-depth (2.5D) display. Our system consists of a single projector-camera system and a set of linear drop generator manifolds that are tightly synchronized and controlled using a computer. Following the principles of fluid mechanics, we are able to accurately generate and control drops so that, at any time instant, no two drops occupy the same projector pixel's line-of-sight. This drop control is combined with an algorithm for space-time division of projector light rays. Our prototype system has up to four layers, with each layer consisting of an row of 50 drops that can be generated at up to 60 Hz. The effective resolution of the display is 50x <i>projector vertical-resolution x number of layers.</i> We show how this water drop display can be used for text, videos, and interactive games.","cites":"17","conferencePercentile":"26.02339181"},{"venue":"ACM Trans. Graph.","id":"9d4b8130ce381d2a66e967a2fb7a2bd49978ba6e","venue_1":"ACM Trans. Graph.","year":"2016","title":"Position-normal distributions for efficient rendering of specular microstructure","authors":"Ling-Qi Yan, Milos Hasan, Steve Marschner, Ravi Ramamoorthi","author_ids":"2162776, 2545444, 2593798, 1752236","abstract":"Specular BRDF rendering traditionally approximates surface microstructure using a smooth normal distribution, but this ignores glinty effects, easily observable in the real world. While modeling the actual surface microstructure is possible, the resulting rendering problem is prohibitively expensive. Recently, Yan et al. [2014] and Jakob et al. [2014] made progress on this problem, but their approaches are still expensive and lack full generality in their material and illumination support. We introduce an efficient and general method that can be easily integrated in a standard rendering system. We treat a specular surface as a four-dimensional <i>position-normal distribution</i>, and fit this distribution using millions of 4D Gaussians, which we call <i>elements.</i> This leads to closed-form solutions to the required BRDF evaluation and sampling queries, enabling the first practical solution to rendering specular microstructure.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"41a7699bf7d86e48d820020f0d795df14248aa99","venue_1":"ACM Trans. Graph.","year":"2012","title":"A theory of monte carlo visibility sampling","authors":"Ravi Ramamoorthi, John Anderson, Mark Meyer, Derek Nowrouzezahrai","author_ids":"1752236, 6868026, 8735841, 1795014","abstract":"Soft shadows from area lights are one of the most crucial effects in high-quality and production rendering, but Monte-Carlo sampling of visibility is often the main source of noise in rendered images. Indeed, it is common to use deterministic uniform sampling for the smoother shading effects in direct lighting, so that all of the Monte Carlo noise arises from visibility sampling alone. In this article, we analyze theoretically and empirically, using both statistical and Fourier methods, the effectiveness of different nonadaptive Monte Carlo sampling patterns for rendering soft shadows.\n We start with a single image scanline and a linear light source, and gradually consider more complex visibility functions at a pixel. We show analytically that the lowest expected variance is in fact achieved by uniform sampling (albeit at the cost of visual banding artifacts). Surprisingly, we show that for two or more discontinuities in the visibility function, a comparable error to uniform sampling is obtained by &#8220;uniform jitter&#8221; sampling, where a constant jitter is applied to all samples in a uniform pattern (as opposed to jittering each stratum as in standard stratified sampling). The variance can be reduced by up to a factor of two, compared to stratified or quasi-Monte Carlo techniques, without the banding in uniform sampling.\n We augment our statistical analysis with a novel 2D Fourier analysis across the pixel-light space. This allows us to characterize the banding frequencies in uniform sampling, and gives insights into the behavior of uniform jitter and stratified sampling. We next extend these results to planar area light sources. We show that the best sampling method can vary, depending on the type of light source (circular, Gaussian, or square/rectangular). The correlation of adjacent &#8220;light scanlines&#8221; in square light sources can reduce the effectiveness of uniform jitter sampling, while the smoother shape of circular and Gaussian-modulated sources preserves its benefits&#8212;these findings are also exposed through our frequency analysis. In practical terms, the theory in this article provides guidelines for selecting visibility sampling strategies, which can reduce the number of shadow samples by 20--40&percnt;, with simple modifications to existing rendering code.","cites":"7","conferencePercentile":"9.848484848"},{"venue":"ACM Trans. Graph.","id":"117e0f19f2b785440f7915f9b4de66894a636734","venue_1":"ACM Trans. Graph.","year":"2012","title":"Virtual ray lights for rendering scenes with participating media","authors":"Jan Novák, Derek Nowrouzezahrai, Carsten Dachsbacher, Wojciech Jarosz","author_ids":"2041282, 1795014, 1705803, 1953515","abstract":"We present an efficient many-light algorithm for simulating indirect illumination in, and from, participating media. Instead of creating discrete virtual point lights (VPLs) at vertices of random-walk paths, we present a continuous generalization that places <i>virtual ray lights</i> (VRLs) along each path segment in the medium. Furthermore, instead of evaluating the lighting independently at discrete points in the medium, we calculate the contribution of each VRL to entire camera rays through the medium using an efficient Monte Carlo product sampling technique. We prove that by spreading the energy of virtual lights along both light and camera rays, the singularities that typically plague VPL methods are significantly diminished. This greatly reduces the need to clamp energy contributions in the medium, leading to robust and <i>unbiased</i> volumetric lighting not possible with current many-light techniques. Furthermore, by acting as a form of final gather, we obtain higher-quality multiple-scattering than existing density estimation techniques like progressive photon beams.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"0c5b41fb577fd9b31c9ae23aa446211c5c187705","venue_1":"ACM Trans. Graph.","year":"2012","title":"Sparse zonal harmonic factorization for efficient SH rotation","authors":"Derek Nowrouzezahrai, Patricio D. Simari, Eugene Fiume","author_ids":"1795014, 2876838, 3018043","abstract":"We present a sparse analytic representation for spherical functions, including those expressed in a Spherical Harmonic (SH) expansion, that is amenable to fast and accurate rotation on the GPU. Exploiting the fact that each band-<i>l</i> SH basis function can be expressed as a weighted sum of 2<i>l</i> + 1 rotated band-<i>l</i> Zonal Harmonic (ZH) lobes, we develop a factorization that significantly reduces this number. We investigate approaches for promoting sparsity in the change-of-basis matrix, and also introduce <i>lobe sharing</i> to reduce the total number of unique lobe directions used for an order-<i>N</i> expansion from <i>N</i><sup>2</sup> to 2<i>N</i>-1. Our representation does not introduce approximation error, is suitable for any type of spherical function (e.g., lighting or transfer), and requires no offline fitting procedure; only a (sparse) matrix multiplication is required to map to/from SH. We provide code for our rotation algorithms, and apply them to several real-time rendering applications.","cites":"6","conferencePercentile":"7.323232323"},{"venue":"ACM Trans. Graph.","id":"2d1f41abe2090261729d4f45bea3b980881c5029","venue_1":"ACM Trans. Graph.","year":"2015","title":"Computing locally injective mappings by advanced MIPS","authors":"Xiao-Ming Fu, Yang Liu, Baining Guo","author_ids":"7780643, 1750084, 2738456","abstract":"Computing locally injective mappings with low distortion in an efficient way is a fundamental task in computer graphics. By revisiting the well-known MIPS (Most-Isometric ParameterizationS) method, we introduce an advanced MIPS method that inherits the local injectivity of MIPS, achieves as low as possible distortions compared to the state-of-the-art locally injective mapping techniques, and performs one to two orders of magnitude faster in computing a mesh-based mapping. The success of our method relies on two key components. The first one is an enhanced MIPS energy function that penalizes the maximal distortion significantly and distributes the distortion evenly over the domain for both mesh-based and meshless mappings. The second is a use of the inexact block coordinate descent method in mesh-based mapping in a way that efficiently minimizes the distortion with the capability not to be trapped early by the local minimum. We demonstrate the capability and superiority of our method in various applications including mesh parameterization, mesh-based and meshless deformation, and mesh improvement.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"062a2367faa0a8eac0e115618e9bddd209bd7d92","venue_1":"ACM Trans. Graph.","year":"2012","title":"The magic lens: refractive steganography","authors":"Marios Papas, Thomas Houit, Derek Nowrouzezahrai, Markus H. Gross, Wojciech Jarosz","author_ids":"3112836, 2362893, 1795014, 1743207, 1953515","abstract":"We present an automatic approach to design and manufacture passive display devices based on optical hidden image decoding. Motivated by classical steganography techniques we construct <i>Magic Lenses</i>, composed of refractive lenslet arrays, to reveal hidden images when placed over potentially unstructured printed or displayed source images. We determine the refractive geometry of these surfaces by formulating and efficiently solving an inverse light transport problem, taking into account additional constraints imposed by the physical manufacturing processes. We fabricate several variants on the basic magic lens idea including using a single source image to encode several hidden images which are only revealed when the lens is placed at prescribed orientations on the source image or viewed from different angles. We also present an important special case, the <i>universal lens</i>, that forms an injection mapping from the lens surface to the source image grid, allowing it to be used with arbitrary source images. We use this type of lens to generate hidden animation sequences. We validate our simulation results with many real-world manufactured magic lenses, and experiment with two separate manufacturing processes.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"84205878e0b1702b995a3186563cc996be18600e","venue_1":"ACM Trans. Graph.","year":"2013","title":"Path-space manipulation of physically-based light transport","authors":"Thorsten-Walther Schmidt, Jan Novák, Johannes Meng, Anton Kaplanyan, Tim Reiner, Derek Nowrouzezahrai, Carsten Dachsbacher","author_ids":"3225988, 2041282, 1945844, 2457406, 2580747, 1795014, 1705803","abstract":"Industry-quality content creation relies on tools for lighting artists to quickly prototype, iterate, and refine final renders. As industry-leading studios quickly adopt physically-based rendering (PBR) across their art generation pipelines, many existing tools have become unsuitable as they address only simple effects without considering underlying PBR concepts and constraints. We present a novel light transport manipulation technique that operates directly on path-space solutions of the rendering equation. We expose intuitive direct and indirect manipulation approaches to edit complex effects such as (multi-refracted) caustics, diffuse and glossy indirect bounces, and direct/indirect shadows. With our sketch- and object-space selection, all built atop a parameterized regular expression engine, artists can search and isolate shading effects to inspect and edit. We classify and filter paths on the fly and visualize the selected transport phenomena. We survey artists who used our tool to manipulate complex phenomena on both static and animated scenes.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"9f26d793744337d3c72d09e05b7f4d6526ef1af1","venue_1":"ACM Trans. Graph.","year":"2016","title":"A Non-Parametric Factor Microfacet Model for Isotropic BRDFs","authors":"Mahdi M. Bagher, John Snyder, Derek Nowrouzezahrai","author_ids":"1885201, 6314473, 1795014","abstract":"We investigate the expressiveness of the microfacet model for isotropic bidirectional reflectance distribution functions (BRDFs) measured from real materials by introducing a <i>non-parametric factor model</i> that represents the model&#8217;s functional structure but abandons restricted parametric formulations of its factors. We propose a new objective based on <i>compressive weighting</i> that controls rendering error in high-dynamic-range BRDF fits better than previous factorization approaches. We develop a simple numerical procedure to minimize this objective and handle dependencies that arise between microfacet factors. Our method faithfully captures a more comprehensive set of materials than previous state-of-the-art parametric approaches yet remains compact (3.2KB per BRDF). We experimentally validate the benefit of the microfacet model over a na&#239;ve orthogonal factorization and show that fidelity for diffuse materials is modestly improved by fitting an unrestricted shadowing/masking factor. We also compare against a recent data-driven factorization approach [Bilgili et al. 2011] and show that our microfacet-based representation improves rendering accuracy for most materials while reducing storage by more than 10 &#215;.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"42b15a747ec8dcc0afa9c5007caf06baa61b25b9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Computing inversion-free mappings by simplex assembly","authors":"Xiao-Ming Fu, Yang Liu","author_ids":"7780643, 1750084","abstract":"We present a novel method, called <i>Simplex Assembly</i>, to compute inversion-free mappings with low or bounded distortion on simplicial meshes. Our method involves two steps: simplex disassembly and simplex assembly. Given a simplicial mesh and its initial piecewise affine mapping, we project the affine transformation associated with each simplex into the inversion-free and distortion-bounded space. The projection disassembles the input mesh into disjoint simplices. The disjoint simplices are then assembled to recover the original connectivity by minimizing the mapping distortion and the difference of the disjoint vertices with respect to the piecewise affine transformations, while the piecewise affine mapping is restricted inside the feasible space. Due to the use of affine transformations as variables, our method explicitly guarantees that no inverted simplex occurs, and that the mapping distortion is below the bound during the optimization. Compared with existing methods, our method is robust to an initialization with many inverted elements and positional constraints. We demonstrate the efficiency and robustness of our method through a variety of geometric processing tasks.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"06080f54273af04b9e7b832c1e8d09c8c22d54c6","venue_1":"ACM Trans. Graph.","year":"2013","title":"Synthesis of tiled patterns using factor graphs","authors":"Yi-Ting Yeh, Katherine Breeden, Lingfeng Yang, Matthew Fisher, Pat Hanrahan","author_ids":"2136573, 2443137, 2406176, 2676553, 4982303","abstract":"Patterns with pleasing structure are common in art, video games, and virtual worlds. We describe a method for synthesizing new patterns of tiles on a regular grid that are similar in appearance to a set of example patterns. Exemplars are used both to specify valid tile arrangements and to emphasize multi-tile structures. We model a pattern as a probabilistic graphical model called a <i>factor graph</i>. Factors represent the hard logical constraints between tiles, the soft statistical relationships that determine style, and the local dependencies between tiles at neighboring sites. We describe a simple method for learning factor functions from a small exemplar. We then synthesize new patterns through a stochastic search method that is inspired by MC-SAT. Efficient synthesis is challenging because of the combination of hard and soft constraints. Our synthesis algorithm, called BlockSS, scales linearly with the number of tiles and the hardness of the problem. We use our technique to model building facades, cities, and decorative patterns.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"25bc76b314afac721a4a63e7b90728b6a764eac0","venue_1":"ACM Trans. Graph.","year":"2007","title":"Character animation from 2D pictures and 3D motion data","authors":"Alexander Sorkine-Hornung, Ellen Dekkers, Leif Kobbelt","author_ids":"2893744, 3272650, 1763010","abstract":"This article presents a new method to animate photos of 2D characters using 3D motion capture data. Given a single image of a person or essentially human-like subject, our method transfers the motion of a 3D skeleton onto the subject's 2D shape in image space, generating the impression of a realistic movement. We present robust solutions to reconstruct a projective camera model and a 3D model pose which matches best to the given 2D image. Depending on the reconstructed view, a 2D shape template is selected which enables the proper handling of occlusions. After fitting the template to the character in the input image, it is deformed as-rigid-as-possible by taking the projected 3D motion data into account. Unlike previous work, our method thereby correctly handles projective shape distortion. It works for images from arbitrary views and requires only a small amount of user interaction. We present animations of a diverse set of human (and nonhuman) characters with different types of motions, such as walking, jumping, or dancing.","cites":"22","conferencePercentile":"15.2"},{"venue":"ACM Trans. Graph.","id":"306a35183bac4874e4160aca6544d6610985c97d","venue_1":"ACM Trans. Graph.","year":"2006","title":"Point-based multiscale surface representation","authors":"Mark Pauly, Leif Kobbelt, Markus H. Gross","author_ids":"1741645, 1763010, 1743207","abstract":"In this article we present a new multiscale surface representation based on point samples. Given an unstructured point cloud as input, our method first computes a series of point-based surface approximations at successively higher levels of smoothness, that is, coarser scales of detail, using geometric low-pass filtering. These point clouds are then encoded relative to each other by expressing each level as a scalar displacement of its predecessor. Low-pass filtering and encoding are combined in an efficient multilevel projection operator using local weighted least squares fitting.Our representation is motivated by the need for higher-level editing semantics which allow surface modifications at different scales. The user would be able to edit the surface at different approximation levels to perform coarse-scale edits on the whole model as well as very localized modifications on the surface detail. Additionally, the multiscale representation provides a separation in geometric scale which can be understood as a spectral decomposition of the surface geometry. Based on this observation, advanced geometric filtering methods can be implemented that mimic the effects of Fourier filters to achieve effects such as smoothing, enhancement, or band-bass filtering.","cites":"45","conferencePercentile":"34.25925926"},{"venue":"ACM Trans. Graph.","id":"32c9709c7f22a9adeb7bfc2df7bca2525a7ff8aa","venue_1":"ACM Trans. Graph.","year":"2015","title":"Space-time sketching of character animation","authors":"Martin Guay, Rémi Ronfard, Michael Gleicher, Marie-Paule Cani","author_ids":"1766277, 2898850, 1776507, 1710314","abstract":"We present a space-time abstraction for the sketch-based design of character animation. It allows animators to draft a full coordinated motion using a single stroke called the <i>space-time curve</i> (STC). From the STC we compute a dynamic line of action (DLOA) that drives the motion of a 3D character through projective constraints. Our dynamic models for the line's motion are entirely geometric, require no pre-existing data, and allow full artistic control. The resulting DLOA can be refined by over-sketching strokes along the space-time curve, or by composing another DLOA on top leading to control over complex motions with few strokes. Additionally, the resulting dynamic line of action can be applied to arbitrary body parts or characters. To match a 3D character to the 2D line over time, we introduce a robust matching algorithm based on closed-form solutions, yielding a tight match while allowing squash and stretch of the character's skeleton. Our experiments show that space-time sketching has the potential of bringing animation design within the reach of beginners while saving time for skilled artists.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"208048c39a9a627bb3441cbb2b7681b0c4bddcd2","venue_1":"ACM Trans. Graph.","year":"2009","title":"Content-preserving warps for 3D video stabilization","authors":"Feng Liu, Michael Gleicher, Hailin Jin, Aseem Agarwala","author_ids":"1734409, 1776507, 1722322, 1696487","abstract":"We describe a technique that transforms a video from a hand-held video camera so that it appears as if it were taken with a directed camera motion. Our method adjusts the video to appear as if it were taken from nearby viewpoints, allowing 3D camera movements to be simulated. By aiming only for perceptual plausibility, rather than accurate reconstruction, we are able to develop algorithms that can effectively recreate dynamic scenes from a single source video. Our technique first recovers the original 3D camera motion and a sparse set of 3D, static scene points using an off-the-shelf structure-from-motion system. Then, a desired camera path is computed either automatically (e.g., by fitting a linear or quadratic path) or interactively. Finally, our technique performs a least-squares optimization that computes a spatially-varying warp from each input video frame into an output frame. The warp is computed to both follow the sparse displacements suggested by the recovered 3D structure, <i>and</i> avoid deforming the content in the video frame. Our experiments on stabilizing challenging videos of dynamic scenes demonstrate the effectiveness of our technique.","cites":"116","conferencePercentile":"95.5801105"},{"venue":"ACM Trans. Graph.","id":"6fd3b901efc8acf955c89ba8cfd3c8b1cd8f4252","venue_1":"ACM Trans. Graph.","year":"2004","title":"Automated extraction and parameterization of motions in large data sets","authors":"Lucas Kovar, Michael Gleicher","author_ids":"2336720, 1776507","abstract":"Large motion data sets often contain many variants of the same kind of motion, but without appropriate tools it is difficult to fully exploit this fact. This paper provides automated methods for identifying logically similar motions in a data set and using them to build a continuous and intuitively parameterized space of motions. To find logically similar motions that are numerically dissimilar, our search method employs a novel distance metric to find \"close\" motions and then uses them as intermediaries to find more distant motions. Search queries are answered at interactive speeds through a precomputation that compactly represents all possibly similar motion segments. Once a set of related motions has been extracted, we automatically register them and apply blending techniques to create a continuous space of motions. Given a function that defines relevant motion parameters, we present a method for extracting motions from this space that accurately possess new parameters requested by the user. Our algorithm extends previous work by explicitly constraining blend weights to reasonable values and having a run-time cost that is nearly independent of the number of example motions. We present experimental results on a test data set of 37,000 frames, or about ten minutes of motion sampled at 60 Hz.","cites":"270","conferencePercentile":"85.86956522"},{"venue":"ACM Trans. Graph.","id":"05ef61ffac6afe199325dd2e3d1f849d0c983065","venue_1":"ACM Trans. Graph.","year":"2005","title":"Automatic restoration of polygon models","authors":"Stephan Bischoff, Darko Pavic, Leif Kobbelt","author_ids":"7273435, 1809559, 1763010","abstract":"We present a fully automatic technique which converts an inconsistent input mesh into an output mesh that is guaranteed to be a clean and consistent mesh representing the closed manifold surface of a solid object. The algorithm removes all typical mesh artifacts such as degenerate triangles, incompatible face orientation, non-manifold vertices and edges, overlapping and penetrating polygons, internal redundant geometry, as well as gaps and holes up to a user-defined maximum size &rho;. Moreover, the output mesh always stays within a prescribed tolerance &epsiv; to the input mesh. Due to the effective use of a hierarchical octree data structure, the algorithm achieves high voxel resolution (up to 4096<sup>3</sup> on a 2GB PC) and processing times of just a few minutes for moderately complex objects. We demonstrate our technique on various architectural CAD models to show its robustness and reliability.","cites":"55","conferencePercentile":"33.06451613"},{"venue":"ACM Trans. Graph.","id":"73d1eccba0044f6dcb20d509cd6e2ad269543a06","venue_1":"ACM Trans. Graph.","year":"2003","title":"Building efficient, accurate character skins from examples","authors":"Alex Mohr, Michael Gleicher","author_ids":"1725101, 1776507","abstract":"Good character animation requires convincing skin deformations including subtleties and details like muscle bulges. Such effects are typically created in commercial animation packages which provide very general and powerful tools. While these systems are convenient and flexible for artists, the generality often leads to characters that are slow to compute or that require a substantial amount of memory and thus cannot be used in interactive systems. Instead, interactive systems restrict artists to a specific character deformation model which is fast and memory efficient but is notoriously difficult to author and can suffer from many deformation artifacts. This paper presents an automated framework that allows character artists to use the full complement of tools in high-end systems to create characters for interactive systems. Our method starts with an arbitrarily rigged character in an animation system. A set of examples is exported, consisting of skeleton configurations paired with the deformed geometry as static meshes. Using these examples, we fit the parameters of a deformation model that best approximates the original data yet remains fast to compute and compact in memory.","cites":"172","conferencePercentile":"75.80645161"},{"venue":"ACM Trans. Graph.","id":"c2d07325c6f6a5bae5a9fe56fdee9c2572d53d16","venue_1":"ACM Trans. Graph.","year":"2001","title":"Computer puppetry: An importance-based approach","authors":"Hyun Joon Shin, Jehee Lee, Sung Yong Shin, Michael Gleicher","author_ids":"8737687, 8152254, 2627994, 1776507","abstract":"Computer puppetry maps the movements of a performer to an animated character in real-time. In this article, we provide a comprehensive solution to the problem of transferring the observations of the motion capture sensors to an animated character whose size and proportion may be different from the performer's. Our goal is to map as many of the <i>important</i> aspects of the motion to the target character as possible, while meeting the online, real-time demands of computer puppetry. We adopt a Kalman filter scheme that addresses motion capture noise issues in this setting. We provide the notion of dynamic importance of an end-effector that allows us to determine what aspects of the performance must be kept in the resulting motion. We introduce a novel inverse kinematics solver that realizes these important aspects within tight real-time constraints. Our approach is demonstrated by its application to broadcast television performances.","cites":"147","conferencePercentile":"80"},{"venue":"ACM Trans. Graph.","id":"3ac946a4815f569dd1d4ea587e6259c0f53658ce","venue_1":"ACM Trans. Graph.","year":"2004","title":"An intuitive framework for real-time freeform modeling","authors":"Mario Botsch, Leif Kobbelt","author_ids":"1716234, 1763010","abstract":"We present a freeform modeling framework for unstructured triangle meshes which is based on constraint shape optimization. The goal is to simplify the user interaction even for quite complex freeform or multiresolution modifications. The user first sets various boundary constraints to define a custom tailored (abstract) basis function which is adjusted to a given design task. The actual modification is then controlled by moving one single 9-dof manipulator object. The technique can handle arbitrary support regions and piecewise boundary conditions with smoothness ranging continuously from <i>C</i><sup>0</sup> to <i>C</i><sup>2</sup>. To more naturally adapt the modification to the shape of the support region, the deformed surface can be tuned to bend with anisotropic stiffness. We are able to achieve real-time response in an interactive design session even for complex meshes by precomputing a set of scalar-valued basis functions that correspond to the degrees of freedom of the manipulator by which the user controls the modification.","cites":"133","conferencePercentile":"65.76086957"},{"venue":"ACM Trans. Graph.","id":"bac6ae5848fdf0ccf1630f88c1f1328d078a6f06","venue_1":"ACM Trans. Graph.","year":"2013","title":"3D Wikipedia: using online text to automatically label and navigate reconstructed geometry","authors":"Bryan C. Russell, Ricardo Martin-Brualla, Daniel J. Butler, Steven M. Seitz, Luke S. Zettlemoyer","author_ids":"2537592, 2328243, 1849598, 1679223, 1982950","abstract":"We introduce an approach for analyzing Wikipedia and other text, together with online photos, to produce <i>annotated</i> 3D models of famous tourist sites. The approach is completely automated, and leverages online text and photo co-occurrences via Google Image Search. It enables a number of new interactions, which we demonstrate in a new 3D visualization tool. Text can be selected to move the camera to the corresponding objects, 3D bounding boxes provide anchors back to the text describing them, and the overall narrative of the text provides a temporal guide for automatically flying through the scene to visualize the world as you read about it. We show compelling results on several major tourist sites.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"77af92fabdad1262f08ba15254529c452a2e5af6","venue_1":"ACM Trans. Graph.","year":"2004","title":"Shell texture functions","authors":"Yanyun Chen, Xin Tong, Jiaping Wang, Stephen Lin, Baining Guo, Harry Shum","author_ids":"7377177, 1743927, 4912907, 1686911, 2738456, 1698102","abstract":"We propose a texture function for realistic modeling and efficient rendering of materials that exhibit surface mesostructures, translucency and volumetric texture variations. The appearance of such complex materials for dynamic lighting and viewing directions is expensive to calculate and requires an impractical amount of storage to precompute. To handle this problem, our method models an object as a shell layer, formed by texture synthesis of a volumetric material sample, and a homogeneous inner core. To facilitate computation of surface radiance from the shell layer, we introduce the <i>shell texture function</i> (STF) which describes voxel irradiance fields based on precomputed fine-level light interactions such as shadowing by surface mesostructures and scattering of photons inside the object. Together with a diffusion approximation of homogeneous inner core radiance, the STF leads to fast and detailed raytraced renderings of complex materials.","cites":"41","conferencePercentile":"20.65217391"},{"venue":"ACM Trans. Graph.","id":"a83add4f075885cb9f31783cb5f53111df9d6c8a","venue_1":"ACM Trans. Graph.","year":"2005","title":"Modeling and rendering of quasi-homogeneous materials","authors":"Xin Tong, Jiaping Wang, Stephen Lin, Baining Guo, Harry Shum","author_ids":"1743927, 4912907, 1686911, 2738456, 1698102","abstract":"Many translucent materials consist of evenly-distributed heterogeneous elements which produce a complex appearance under different lighting and viewing directions. For these <i>quasi-homogeneous</i> materials, existing techniques do not address how to acquire their material representations from physical samples in a way that allows arbitrary geometry models to be rendered with these materials. We propose a model for such materials that can be readily acquired from physical samples. This material model can be applied to geometric models of arbitrary shapes, and the resulting objects can be efficiently rendered without expensive subsurface light transport simulation. In developing a material model with these attributes, we capitalize on a key observation about the subsurface scattering characteristics of quasi-homogeneous materials at different scales. Locally, the non-uniformity of these materials leads to inhomogeneous subsurface scattering. For subsurface scattering on a global scale, we show that a lengthy photon path through an even distribution of heterogeneous elements statistically resembles scattering in a homogeneous medium. This observation allows us to represent and measure the global light transport within quasi-homogeneous materials as well as the transfer of light into and out of a material volume through surface mesostructures. We demonstrate our technique with results for several challenging materials that exhibit sophisticated appearance features such as transmission of back illumination through surface mesostructures.","cites":"39","conferencePercentile":"16.93548387"},{"venue":"ACM Trans. Graph.","id":"0c17267531604ec9ed19e78b4e20cb6eeed653b8","venue_1":"ACM Trans. Graph.","year":"2008","title":"Modeling and rendering of heterogeneous translucent materials using the diffusion equation","authors":"Jiaping Wang, Shuang Zhao, Xin Tong, Stephen Lin, Zhouchen Lin, Yue Dong, Baining Guo, Harry Shum","author_ids":"4912907, 2373908, 1743927, 1686911, 1692743, 1744268, 2738456, 1698102","abstract":"In this article, we propose techniques for modeling and rendering of heterogeneous translucent materials that enable acquisition from measured samples, interactive editing of material attributes, and real-time rendering. The materials are assumed to be optically dense such that multiple scattering can be approximated by a diffusion process described by the diffusion equation. For modeling heterogeneous materials, we present the <i>inverse diffusion algorithm</i> for acquiring material properties from appearance measurements. This modeling algorithm incorporates a regularizer to handle the ill-conditioning of the inverse problem, an adjoint method to dramatically reduce the computational cost, and a hierarchical GPU implementation for further speedup. To render an object with known material properties, we present the <i>polygrid diffusion algorithm</i>, which solves the diffusion equation with a boundary condition defined by the given illumination environment. This rendering technique is based on representation of an object by a polygrid, a grid with regular connectivity and an irregular shape, which facilitates solution of the diffusion equation in arbitrary volumes. Because of the regular connectivity, our rendering algorithm can be implemented on the GPU for real-time performance. We demonstrate our techniques by capturing materials from physical samples and performing real-time rendering and editing with these materials.","cites":"34","conferencePercentile":"37.34567901"},{"venue":"ACM Trans. Graph.","id":"72b849452c24eb35e9140d3307b1ee65363e05ba","venue_1":"ACM Trans. Graph.","year":"2008","title":"Modeling anisotropic surface reflectance with example-based microfacet synthesis","authors":"Jiaping Wang, Shuang Zhao, Xin Tong, John Snyder, Baining Guo","author_ids":"4912907, 2373908, 1743927, 6314473, 2738456","abstract":"We present a new technique for the visual modeling of spatiallyvarying anisotropic reflectance using data captured from a single view. Reflectance is represented using a microfacet-based BRDF which tabulates the facets' normal distribution (NDF) as a function of surface location. Data from a single view provides a 2D slice of the 4D BRDF at each surface point from which we fit a partial NDF. The fitted NDF is partial because the single view direction coupled with the set of light directions covers only a portion of the \"half-angle\" hemisphere. We complete the NDF at each point by applying a novel variant of texture synthesis using similar, overlapping partial NDFs from other points. Our similarity measure allows azimuthal rotation of partial NDFs, under the assumption that reflectance is spatially redundant but the local frame may be arbitrarily oriented. Our system includes a simple acquisition device that collects images over a 2D set of light directions by scanning a linear array of LEDs over a flat sample. Results demonstrate that our approach preserves spatial and directional BRDF details and generates a visually compelling match to measured materials.","cites":"42","conferencePercentile":"52.77777778"},{"venue":"ACM Trans. Graph.","id":"8f1745ae6b5ac6ceea9716b87f0136372296651f","venue_1":"ACM Trans. Graph.","year":"2009","title":"Kernel Nyström method for light transport","authors":"Jiaping Wang, Yue Dong, Xin Tong, Zhouchen Lin, Baining Guo","author_ids":"4912907, 1744268, 1743927, 1692743, 2738456","abstract":"Figure 1: Relighting results using the light transport matrix reconstructed by our method. Complex light transport effects, including caustics (a), complex occlusions (b), and a mixture of caustics, complex occlusions, inter-reflections, and subsurface scattering (c) are all faithfully reproduced. Abstract We propose a kernel Nyström method for reconstructing the light transport matrix from a relatively small number of acquired images. Our work is based on the generalized Nyström method for low rank matrices. We introduce the light transport kernel and incorporate it into the Nyström method to exploit the nonlinear coherence of the light transport matrix. We also develop an adaptive scheme for efficiently capturing the sparsely sampled images from the scene. Our experiments indicate that the kernel Nyström method can achieve good reconstruction of the light transport matrix with a few hundred images and produce high quality relighting results. The kernel Nyström method is effective for modeling scenes with complex lighting effects and occlusions which have been challenging for existing techniques.","cites":"38","conferencePercentile":"57.73480663"},{"venue":"ACM Trans. Graph.","id":"6ece103d30f4ca182e3bbbcc08441d558a0330fb","venue_1":"ACM Trans. Graph.","year":"2003","title":"Shape modeling with point-sampled geometry","authors":"Mark Pauly, Richard Keiser, Leif Kobbelt, Markus H. Gross","author_ids":"1741645, 3161356, 1763010, 1743207","abstract":"We present a versatile and complete free-form shape modeling framework for point-sampled geometry. By combining unstructured point clouds with the implicit surface definition of the moving least squares approximation, we obtain a hybrid geometry representation that allows us to exploit the advantages of implicit and parametric surface models. Based on this representation we introduce a shape modeling system that enables the designer to perform large constrained deformations as well as boolean operations on arbitrarily shaped objects. Due to minimum consistency requirements, point-sampled surfaces can easily be re-structured on the fly to support extreme geometric deformations during interactive editing. In addition, we show that strict topology control is possible and sharp features can be generated and preserved on point-sampled objects. We demonstrate the effectiveness of our system on a large set of input models, including noisy range scans, irregular point clouds, and sparsely as well as densely sampled models.","cites":"272","conferencePercentile":"89.24731183"},{"venue":"ACM Trans. Graph.","id":"a98418090fe319d481072fbc5aacf4142ae42449","venue_1":"ACM Trans. Graph.","year":"1998","title":"A Multiresolution Framework for Variational Aubdivision","authors":"Leif Kobbelt, Peter Schröder","author_ids":"1763010, 1775659","abstract":"Subdivision is a powerful paradigm for the generaton of curves and surfaces. It is easy to implement, computationally efficient, and useful in a variety of applications because of its intimate connection with multiresolution analysis. An important task in computer graphics and geometric modeling is the construction of curves that interpolate a griven set of points and minimize a fairness functional (variational design). In the context of subdivision, fairing leads to special schemes requiring the solution of a banded linear system at every subdivision step. We present several examples of such schemes including one that reproduces nonuniform interpolating cubic splines. Expressing the construction in terms of certain elementary operations we are able to embed variational  subdivision in the lifting framework, a powerful technique to construct wavelet filter banks given a subdivision scheme. This allows us to extend the traditional lifting scheme for FIR filters to a certain class of IIR filters. Consquently, we how how to build variationally optimal curves <italic>and</italic> associated, stable wavelets in a straightforward fashion. The algorithms to perform the corresponding decomposition and reconstruction transformations are easy to implement and efficient enough for interactive applications.","cites":"41","conferencePercentile":"72.72727273"},{"venue":"ACM Trans. Graph.","id":"49e4c42ad00bffc40cc30474ae62cfe14a9b5f90","venue_1":"ACM Trans. Graph.","year":"2009","title":"SubEdit: a representation for editing measured heterogeneous subsurface scattering","authors":"Ying Song, Xin Tong, Fabio Pellacini, Pieter Peers","author_ids":"2888011, 1743927, 1757883, 1808270","abstract":"In this paper we present <i>SubEdit</i>, a representation for editing the BSSRDF of heterogeneous subsurface scattering acquired from real-world samples. Directly editing measured raw data is difficult due to the non-local impact of heterogeneous subsurface scattering on the appearance. Our <i>SubEdit</i> representation decouples these non-local effects into the product of two local scattering profiles defined at respectively the incident and outgoing surface locations. This allows users to directly manipulate the appearance of single surface locations and to robustly make selections. To further facilitate editing, we reparameterize the scattering profiles into the local appearance concepts of albedo, scattering range, and profile shape. Our method preserves the visual quality of the measured material after editing by maintaining the consistency of subsurface transport for all edits. <i>SubEdit</i> fits measured data well while remaining efficient enough to support interactive rendering and manipulation. We illustrate the suitability of <i>SubEdit</i> as a representation for editing by applying various complex modifications on a wide variety of measured heterogeneous subsurface scattering materials.","cites":"18","conferencePercentile":"23.48066298"},{"venue":"ACM Trans. Graph.","id":"42da75bd69e70a724ed2191a3d4eabe249c7cb88","venue_1":"ACM Trans. Graph.","year":"2005","title":"Lpics: a hybrid hardware-accelerated relighting engine for computer cinematography","authors":"Fabio Pellacini, Kiril Vidimce, Aaron E. Lefohn, Alex Mohr, Mark Leone, John Warren","author_ids":"1757883, 2891782, 3329508, 1725101, 1925390, 2200039","abstract":"In computer cinematography, the process of lighting design involves placing and configuring lights to define the visual appearance of environments and to enhance story elements. This process is labor intensive and time consuming, primarily because lighting artists receive poor feedback from existing tools: interactive previews have very poor quality, while final-quality images often take hours to render.This paper presents an interactive cinematic lighting system used in the production of computer-animated feature films containing environments of very high complexity, in which surface and light appearances are described using procedural RenderMan shaders. Our system provides lighting artists with high-quality previews at interactive framerates with only small approximations compared to the final rendered images. This is accomplished by combining numerical estimation of surface response, image-space caching, deferred shading, and the computational power of modern graphics hardware.Our system has been successfully used in the production of two feature-length animated films, dramatically accelerating lighting tasks. In our experience interactivity fundamentally changes an artist's workflow, improving both productivity and artistic expressiveness.","cites":"46","conferencePercentile":"20.96774194"},{"venue":"ACM Trans. Graph.","id":"36f253befcb68a55b28ed71bd6a3fbf135c751e6","venue_1":"ACM Trans. Graph.","year":"2010","title":"Accelerating spatially varying Gaussian filters","authors":"Jongmin Baek, David E. Jacobs","author_ids":"2155413, 3049679","abstract":"High-dimensional Gaussian filters, most notably the bilateral filter, are important tools for many computer graphics and vision tasks. In recent years, a number of techniques for accelerating their evaluation have been developed by exploiting the separability of these Gaussians. However, these techniques do not apply to the more general class of <i>spatially varying</i> Gaussian filters, as they cannot be expressed as convolutions. These filters are useful because the underlying data---e.g. images, range data, meshes or light fields---often exhibit strong local anisotropy and scale. We propose an acceleration method for approximating spatially varying Gaussian filters using a set of spatially invariant Gaussian filters each of which is applied to a segment of some non-disjoint partitioning of the dataset. We then demonstrate that the resulting ability to locally tilt, rotate or scale the kernel improves filtering performance in various applications over traditional spatially invariant Gaussian filters, without incurring a significant penalty in computational expense.","cites":"11","conferencePercentile":"14.03508772"},{"venue":"ACM Trans. Graph.","id":"87f1d964ed94c9f71fa42d18541ef6616f399ac2","venue_1":"ACM Trans. Graph.","year":"2005","title":"A frequency analysis of light transport","authors":"Frédo Durand, Nicolas Holzschuch, Cyril Soler, Eric Chan, François X. Sillion","author_ids":"1728125, 3174773, 1866568, 2948172, 1708617","abstract":"We present a signal-processing framework for light transport. We study the frequency content of radiance and how it is altered by phenomena such as shading, occlusion, and transport. This extends previous work that considered either spatial or angular dimensions, and it offers a comprehensive treatment of both space and angle.We show that occlusion, a multiplication in the primal, amounts in the Fourier domain to a convolution by the spectrum of the blocker. Propagation corresponds to a shear in the space-angle frequency domain, while reflection on curved objects performs a different shear along the angular frequency axis. As shown by previous work, reflection is a convolution in the primal and therefore a multiplication in the Fourier domain. Our work shows how the spatial components of lighting are affected by this angular convolution.Our framework predicts the characteristics of interactions such as caustics and the disappearance of the shadows of small features. Predictions on the frequency content can then be used to control sampling rates for rendering. Other potential applications include precomputed radiance transfer and inverse rendering.","cites":"140","conferencePercentile":"75"},{"venue":"ACM Trans. Graph.","id":"2e7c50bd23412d7f184b31e34f0b73d278e806d0","venue_1":"ACM Trans. Graph.","year":"2011","title":"Perceptual models of viewpoint preference","authors":"Adrian Secord, Jingwan Lu, Adam Finkelstein, Manish Singh, Andrew Nealen","author_ids":"2821128, 2054975, 1707541, 4459183, 1729677","abstract":"The question of what are good views of a 3D object has been addressed by numerous researchers in perception, computer vision, and computer graphics. This has led to a large variety of measures for the goodness of views as well as some special-case viewpoint selection algorithms. In this article, we leverage the results of a large user study to optimize the parameters of a general model for viewpoint goodness, such that the fitted model can predict people's preferred views for a broad range of objects. Our model is represented as a combination of attributes known to be important for view selection, such as projected model area and silhouette length. Moreover, this framework can easily incorporate new attributes in the future, based on the data from our existing study. We demonstrate our combined goodness measure in a number of applications, such as automatically selecting a good set of representative views, optimizing camera orbits to pass through good views and avoid bad views, and trackball controls that gently guide the viewer towards better views.","cites":"36","conferencePercentile":"74.21052632"},{"venue":"ACM Trans. Graph.","id":"089abb525f9dff8b9bb78b139f235169391c3185","venue_1":"ACM Trans. Graph.","year":"2015","title":"Subspace condensation: full space adaptivity for subspace deformations","authors":"Yun Teng, Mark Meyer, Tony DeRose, Theodore Kim","author_ids":"2848713, 8735841, 1792251, 1782832","abstract":"Subspace deformable body simulations can be very fast, but can behave unrealistically when behaviors outside the prescribed subspace such as novel external collisions, are encountered. We address this limitation by presenting a fast, flexible new method that allows full space computation to be activated in the neighborhood of novel events while the rest of the body still computes in a subspace. We achieve this using a method we call <i>subspace condensation</i>, a variant on the classic <i>static condensation</i> precomputation. However, instead of a precomputation, we use the speed of subspace methods to perform the condensation at <i>every frame.</i> This approach allows the full space regions to be specified arbitrarily at runtime, and forms a natural two-way coupling with the subspace regions. While condensation is usually only applicable to linear materials, the speed of our technique enables its application to non-linear materials as well. We show the effectiveness of our approach by applying it to a variety of articulated character scenarios.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"6325205e623aa253f9cd02d6bf7d1d662a656ac1","venue_1":"ACM Trans. Graph.","year":"2014","title":"High-quality capture of eyes","authors":"Pascal Bérard, Derek Bradley, Maurizio Nitti, Thabo Beeler, Markus H. Gross","author_ids":"3104571, 1745149, 2103369, 2486770, 1743207","abstract":"Even though the human eye is one of the central features of individual appearance, its shape has so far been mostly approximated in our community with gross simplifications. In this paper we demonstrate that there is a lot of individuality to every eye, a fact that common practices for 3D eye generation do not consider. To faithfully reproduce all the intricacies of the human eye we propose a novel capture system that is capable of accurately reconstructing all the visible parts of the eye: the white <i>sclera</i>, the transparent <i>cornea</i> and the non-rigidly deforming colored <i>iris</i>. These components exhibit very different appearance properties and thus we propose a hybrid reconstruction method that addresses them individually, resulting in a complete model of both spatio-temporal shape and texture at an unprecedented level of detail, enabling the creation of more believable digital humans. Finally, we believe that the findings of this paper will alter our community's current assumptions regarding human eyes, and our work has the potential to significantly impact the way that eyes will be modelled in the future.","cites":"10","conferencePercentile":"61.72839506"},{"venue":"ACM Trans. Graph.","id":"1054042a573b88a9f0602f6c11b5c59d6132b136","venue_1":"ACM Trans. Graph.","year":"2005","title":"Compressing and companding high dynamic range images with subband architectures","authors":"Yuanzhen Li, Lavanya Sharan, Edward H. Adelson","author_ids":"2549090, 2240195, 1788148","abstract":"High dynamic range (HDR) imaging is an area of increasing importance, but most display devices still have limited dynamic range (LDR). Various techniques have been proposed for compressing the dynamic range while retaining important visual information. Multi-scale image processing techniques, which are widely used for many image processing tasks, have a reputation of causing halo artifacts when used for range compression. However, we demonstrate that they can work when properly implemented. We use a symmetrical analysis-synthesis filter bank, and apply local gain control to the subbands. We also show that the technique can be adapted for the related problem of \"companding\", in which an HDR image is converted to an LDR image, and later expanded back to high dynamic range.","cites":"104","conferencePercentile":"62.90322581"},{"venue":"ACM Trans. Graph.","id":"d61482cf88eeb223a235e9d441617bf1fadfac71","venue_1":"ACM Trans. Graph.","year":"2015","title":"Perceptually based downscaling of images","authors":"A. Cengiz Öztireli, Markus H. Gross","author_ids":"1787433, 1743207","abstract":"We propose a perceptually based method for downscaling images that provides a better apparent depiction of the input image. We formulate image downscaling as an optimization problem where the difference between the input and output images is measured using a widely adopted perceptual image quality metric. The downscaled images retain perceptually important features and details, resulting in an accurate and spatio-temporally consistent representation of the high resolution input. We derive the solution of the optimization problem in closed-form, which leads to a simple, efficient and parallelizable implementation with sums and convolutions. The algorithm has running times similar to linear filtering and is orders of magnitude faster than the state-of-the-art for image downscaling. We validate the effectiveness of the technique with extensive tests on many images, video, and by performing a user study, which indicates a clear preference for the results of the new algorithm.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"086f8f45ca867928ea5b0ea73f8c5c4575de38e8","venue_1":"ACM Trans. Graph.","year":"2015","title":"Data-driven fluid simulations using regression forests","authors":"Lubor Ladicky, SoHyeon Jeong, Barbara Solenthaler, Marc Pollefeys, Markus H. Gross","author_ids":"1728641, 2625401, 1789549, 1742208, 1743207","abstract":"Traditional fluid simulations require large computational resources even for an average sized scene with the main bottleneck being a very small time step size, required to guarantee the stability of the solution. Despite a large progress in parallel computing and efficient algorithms for pressure computation in the recent years, realtime fluid simulations have been possible only under very restricted conditions. In this paper we propose a novel machine learning based approach, that formulates physics-based fluid simulation as a regression problem, estimating the acceleration of every particle for each frame. We designed a feature vector, directly modelling individual forces and constraints from the Navier-Stokes equations, giving the method strong generalization properties to reliably predict positions and velocities of particles in a large time step setting on yet unseen test videos. We used a regression forest to approximate the behaviour of particles observed in the large training set of simulations obtained using a traditional solver. Our GPU implementation led to a speed-up of one to three orders of magnitude compared to the state-of-the-art position-based fluid solver and runs in real-time for systems with up to 2 million particles.","cites":"8","conferencePercentile":"85.30612245"},{"venue":"ACM Trans. Graph.","id":"c776c2c09218c3f2e6a5302d4387409293ba420c","venue_1":"ACM Trans. Graph.","year":"2016","title":"Adaptive polynomial rendering","authors":"Bochang Moon, Steven McDonagh, Kenny Mitchell, Markus H. Gross","author_ids":"3159549, 1714426, 3315742, 1743207","abstract":"In this paper, we propose a new adaptive rendering method to improve the performance of Monte Carlo ray tracing, by reducing noise contained in rendered images while preserving high-frequency edges. Our method locally approximates an image with polynomial functions and the optimal order of each polynomial function is estimated so that our reconstruction error can be minimized. To robustly estimate the optimal order, we propose a multi-stage error estimation process that iteratively estimates our reconstruction error. In addition, we present an energy-preserving outlier removal technique to remove spike noise without causing noticeable energy loss in our reconstruction result. Also, we adaptively allocate additional ray samples to high error regions guided by our error estimation. We demonstrate that our approach outperforms state-of-the-art methods by controlling the tradeoff between reconstruction bias and variance through locally defining our polynomial order, even without need for filtering bandwidth optimization, the common approach of other recent methods.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"2a83a201ddfad1b58638a5a2c0568c571aeb1b06","venue_1":"ACM Trans. Graph.","year":"2003","title":"Continuous capture of skin deformation","authors":"Peter Sand, Leonard McMillan, Jovan Popovic","author_ids":"2259071, 1748115, 1731389","abstract":"We describe a method for the acquisition of deformable human geometry from silhouettes. Our technique uses a commercial tracking system to determine the motion of the skeleton, then estimates geometry for each bone using constraints provided by the silhouettes from one or more cameras. These silhouettes do not give a complete characterization of the geometry for a particular point in time, but when the subject moves, many observations of the same local geometries allow the construction of a complete model. Our reconstruction algorithm provides a simple mechanism for solving the problems of view aggregation, occlusion handling, hole filling, noise removal, and deformation modeling. The resulting model is parameterized to synthesize geometry for new poses of the skeleton. We demonstrate this capability by rendering the geometry for motion sequences that were not included in the original datasets.","cites":"88","conferencePercentile":"48.38709677"},{"venue":"ACM Trans. Graph.","id":"2bc8b1c38d180cdf11277001d3b3f2f9822a800f","venue_1":"ACM Trans. Graph.","year":"2016","title":"Lightweight eye capture using a parametric model","authors":"Pascal Bérard, Derek Bradley, Markus H. Gross, Thabo Beeler","author_ids":"3104571, 1745149, 1743207, 2486770","abstract":"Facial scanning has become ubiquitous in digital media, but so far most efforts have focused on reconstructing the skin. Eye reconstruction, on the other hand, has received only little attention, and the current state-of-the-art method is cumbersome for the actor, time-consuming, and requires carefully setup and calibrated hardware. These constraints currently make eye capture impractical for general use. We present the first approach for high-quality <i>lightweight</i> eye capture, which leverages a database of pre-captured eyes to guide the reconstruction of new eyes from much less constrained inputs, such as traditional single-shot face scanners or even a single photo from the internet. This is accomplished with a new parametric model of the eye built from the database, and a novel image-based model fitting algorithm. Our method provides both automatic reconstructions of real eyes, as well as artistic control over the parameters to generate user-specific eyes.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"aa29b5d0e0a72123f1389130becde52bc733a17b","venue_1":"ACM Trans. Graph.","year":"2016","title":"Efficient rendering of heterogeneous polydisperse granular media","authors":"Thomas Müller, Marios Papas, Markus H. Gross, Wojciech Jarosz, Jan Novák","author_ids":"2231082, 3112836, 1743207, 1953515, 2041282","abstract":"We address the challenge of efficiently rendering massive assemblies of grains within a forward path-tracing framework. Previous approaches exist for accelerating high-order scattering for a limited, and static, set of granular materials, often requiring scene-dependent precomputation. We significantly expand the admissible regime of granular materials by considering <i>heterogeneous</i> and <i>dynamic</i> granular mixtures with spatially varying grain concentrations, pack rates, and sizes. Our method supports both procedurally generated grain assemblies and dynamic assemblies authored in off-the-shelf particle simulation tools. The key to our speedup lies in two complementary aggregate scattering approximations which we introduced to jointly accelerate construction of short and long light paths. For low-order scattering, we accelerate path construction using novel <i>grain scattering distribution functions</i> (GSDF) which aggregate intra-grain light transport while retaining important grain-level structure. For high-order scattering, we extend prior work on <i>shell transport functions</i> (STF) to support dynamic, heterogeneous mixtures of grains with varying sizes. We do this without a scene-dependent precomputation and show how this can also be used to accelerate light transport in arbitrary continuous heterogeneous media. Our multi-scale rendering automatically minimizes the usage of explicit path tracing to only the first grain along a light path, or can avoid it completely, when appropriate, by switching to our aggregate transport approximations. We demonstrate our technique on animated scenes containing heterogeneous mixtures of various types of grains that could not previously be rendered efficiently. We also compare to previous work on a simpler class of granular assemblies, reporting significant computation savings, often yielding higher accuracy results.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"e691f7f15fb1f0653da8aa15c4ef80c1aaa07733","venue_1":"ACM Trans. Graph.","year":"2006","title":"Removing camera shake from a single photograph","authors":"Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T. Roweis, William T. Freeman","author_ids":"2276554, 3245232, 1747779, 1767440, 1768236","abstract":"Camera shake during exposure leads to objectionable image blur and ruins many photographs. Conventional blind deconvolution methods typically assume frequency-domain constraints on images, or overly simplified parametric forms for the motion path during camera shake. Real camera motions can follow convoluted paths, and a spatial domain prior can better maintain visually salient image characteristics. We introduce a method to remove the effects of camera shake from seriously blurred images. The method assumes a uniform camera blur over the image and negligible in-plane camera rotation. In order to estimate the blur from the camera shake, the user must specify an image region without saturation effects. We show results for a variety of digital photographs taken from personal photo collections.","cites":"507","conferencePercentile":"99.07407407"},{"venue":"ACM Trans. Graph.","id":"51934b43c251c5fb1456d2aad246adbca6231f6c","venue_1":"ACM Trans. Graph.","year":"2016","title":"An anatomically-constrained local deformation model for monocular face capture","authors":"Chenglei Wu, Derek Bradley, Markus H. Gross, Thabo Beeler","author_ids":"1682672, 1745149, 1743207, 2486770","abstract":"We present a new anatomically-constrained local face model and fitting approach for tracking 3D faces from 2D motion data in very high quality. In contrast to traditional global face models, often built from a large set of blendshapes, we propose a local deformation model composed of many small subspaces spatially distributed over the face. Our local model offers far more flexibility and expressiveness than global blendshape models, even with a much smaller model size. This flexibility would typically come at the cost of reduced robustness, in particular during the under-constrained task of monocular reconstruction. However, a key contribution of this work is that we consider the face anatomy and introduce subspace skin thickness constraints into our model, which constrain the face to only valid expressions and helps counteract depth ambiguities in monocular tracking. Given our new model, we present a novel fitting optimization that allows 3D facial performance reconstruction from a single view at extremely high quality, far beyond previous fitting approaches. Our model is flexible, and can be applied also when only sparse motion data is available, for example with marker-based motion capture or even face posing from artistic sketches. Furthermore, by incorporating anatomical constraints we can automatically estimate the rigid motion of the skull, obtaining a rigid stabilization of the performance for free. We demonstrate our model and single-view fitting method on a number of examples, including, for the first time, extreme local skin deformation caused by external forces such as wind, captured from a single high-speed camera.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"21d53b8cc639f8b6bd63781f0c954ee9f9f33c08","venue_1":"ACM Trans. Graph.","year":"2010","title":"Precomputed wave simulation for real-time sound propagation of dynamic sources in complex scenes","authors":"Nikunj Raghuvanshi, John Snyder, Ravish Mehra, Ming C. Lin, Naga K. Govindaraju","author_ids":"3032886, 6314473, 2040242, 1709625, 3050319","abstract":"We present a method for real-time sound propagation that captures all wave effects, including diffraction and reverberation, for multiple moving sources and a moving listener in a complex, static 3D scene. It performs an offline numerical simulation over the scene and then applies a novel technique to extract and compactly encode the perceptually salient information in the resulting acoustic responses. Each response is automatically broken into two phases: early reflections (ER) and late reverberation (LR), via a threshold on the temporal density of arriving wavefronts. The LR is simulated and stored in the frequency domain, once per room in the scene. The ER accounts for more detailed spatial variation, by recording a set of peak delays/amplitudes in the time domain and a residual frequency response sampled in octave frequency bands, at each source/receiver point pair in a 5D grid. An efficient run-time uses this precomputed representation to perform binaural sound rendering based on frequency-domain convolution. Our system demonstrates realistic, wave-based acoustic effects in real time, including diffraction low-passing behind obstructions, sound focusing, hollow reverberation in empty rooms, sound diffusion in fully-furnished rooms, and realistic late reverberation.","cites":"29","conferencePercentile":"50.87719298"},{"venue":"ACM Trans. Graph.","id":"a7ba316d9b0a7cc59d96f7435f33a793b670dc73","venue_1":"ACM Trans. Graph.","year":"2003","title":"Motion sketching for control of rigid-body simulations","authors":"Jovan Popovic, Steven M. Seitz, Michael A. Erdmann","author_ids":"1731389, 1679223, 1771473","abstract":"Motion sketching is an approach for creating realistic rigid-body motion. In this approach, an animator sketches how objects should move and the system computes a physically plausible motion that best fits the sketch. The sketch is specified with a mouse-based interface or with hand-gestures, which move instrumented objects in the real world to act out the desired behaviors. The sketches may be imprecise, may be physically infeasible, or may have incorrect timing. A multiple-shooting optimization estimates the parameters of a rigid-body simulation needed to simulate an animation that matches the sketch with physically plausible timing and motion. This technique applies to physical simulations of multiple colliding rigid bodies possibly connected with joints in a tree (open-loop) topology.","cites":"55","conferencePercentile":"27.41935484"},{"venue":"ACM Trans. Graph.","id":"6ff3660934b22e35647c6dc4fe7d6a07d75c5bb0","venue_1":"ACM Trans. Graph.","year":"2013","title":"Wave-based sound propagation in large open scenes using an equivalent source formulation","authors":"Ravish Mehra, Nikunj Raghuvanshi, Lakulish Antani, Anish Chandak, Sean Curtis, Dinesh Manocha","author_ids":"2040242, 3032886, 2747880, 2890393, 2801220, 1699159","abstract":"We present a novel approach for wave-based sound propagation suitable for large, open spaces spanning hundreds of meters, with a small memory footprint. The scene is decomposed into disjoint rigid objects. The free-field acoustic behavior of each object is captured by a compact per-object transfer function relating the amplitudes of a set of incoming equivalent sources to outgoing equivalent sources. Pairwise acoustic interactions between objects are computed analytically to yield compact inter-object transfer functions. The global sound field accounting for all orders of interaction is computed using these transfer functions. The runtime system uses fast summation over the outgoing equivalent source amplitudes for all objects to auralize the sound field for a moving listener in real time. We demonstrate realistic acoustic effects such as diffraction, low-passed sound behind obstructions, focusing, scattering, high-order reflections, and echoes on a variety of scenes.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"05a59341f3484c4f85b0f6fc8caf02bd9255d003","venue_1":"ACM Trans. Graph.","year":"2004","title":"Deformation transfer for triangle meshes","authors":"Robert W. Sumner, Jovan Popovic","author_ids":"1693475, 1731389","abstract":"Deformation transfer applies the deformation exhibited by a source triangle mesh onto a different target triangle mesh. Our approach is general and does not require the source and target to share the same number of vertices or triangles, or to have identical connectivity. The user builds a correspondence map between the triangles of the source and those of the target by specifying a small set of vertex markers. Deformation transfer computes the set of transformations induced by the deformation of the source mesh, maps the transformations through the correspondence from the source to the target, and solves an optimization problem to consistently apply the transformations to the target shape. The resulting system of linear equations can be factored once, after which transferring a new deformation to the target mesh requires only a backsubstitution step. Global properties such as foot placement can be achieved by constraining vertex positions. We demonstrate our method by retargeting full body key poses, applying scanned facial deformations onto a digital character, and remapping rigid and non-rigid animation sequences from one mesh onto another.","cites":"407","conferencePercentile":"95.65217391"},{"venue":"ACM Trans. Graph.","id":"628082ea8d77343c5874a272848a79a756c68108","venue_1":"ACM Trans. Graph.","year":"2005","title":"Face transfer with multilinear models","authors":"Daniel Vlasic, Matthew Brand, Hanspeter Pfister, Jovan Popovic","author_ids":"1880628, 1740065, 1701371, 1731389","abstract":"Face Transfer is a method for mapping videorecorded performances of one individual to facial animations of another. It extracts visemes (speech-related mouth articulations), expressions, and three-dimensional (3D) pose from monocular video or film footage. These parameters are then used to generate and drive a detailed 3D textured face mesh for a target identity, which can be seamlessly rendered back into target footage. The underlying face model automatically adjusts for how the target performs facial expressions and visemes. The performance data can be easily edited to change the visemes, expressions, pose, or even the identity of the target---the attributes are separably controllable. This supports a wide variety of video rewrite and puppetry applications.Face Transfer is based on a multilinear model of 3D face meshes that separably parameterizes the space of geometric variations due to different attributes (e.g., identity, expression, and viseme). Separability means that each of these attributes can be independently varied. A multilinear model can be estimated from a Cartesian product of examples (identities x expressions x visemes) with techniques from statistical analysis, but only after careful preprocessing of the geometric data set to secure one-to-one correspondence, to minimize cross-coupling artifacts, and to fill in any missing examples. Face Transfer offers new solutions to these problems and links the estimated model with a face-tracking algorithm to extract pose, expression, and viseme parameters.","cites":"211","conferencePercentile":"92.74193548"},{"venue":"ACM Trans. Graph.","id":"8ede8105458168c9d11eca130dbfc29eb3b058f6","venue_1":"ACM Trans. Graph.","year":"2005","title":"Mesh-based inverse kinematics","authors":"Robert W. Sumner, Matthias Zwicker, Craig Gotsman, Jovan Popovic","author_ids":"1693475, 1796846, 1724072, 1731389","abstract":"The ability to position a small subset of mesh vertices and produce a <i>meaningful</i> overall deformation of the entire mesh is a fundamental task in mesh editing and animation. However, the class of meaningful deformations varies from mesh to mesh and depends on mesh kinematics, which prescribes valid mesh configurations, and a selection mechanism for choosing among them. Drawing an analogy to the traditional use of skeleton-based inverse kinematics for posing skeletons. we define <i>mesh-based inverse kinematics</i> as the problem of finding meaningful mesh deformations that meet specified vertex constraints.Our solution relies on example meshes to indicate the class of meaningful deformations. Each example is represented with a feature vector of deformation gradients that capture the affine transformations which individual triangles undergo relative to a reference pose. To pose a mesh, our algorithm efficiently searches among all meshes with specified vertex positions to find the one that is closest to some pose in a nonlinear span of the example feature vectors. Since the search is not restricted to the span of example shapes, this produces compelling deformations even when the constraints require poses that are different from those observed in the examples. Furthermore, because the span is formed by a nonlinear blend of the example feature vectors, the blending component of our system may also be used independently to pose meshes by specifying blending weights or to compute multi-way morph sequences.","cites":"122","conferencePercentile":"69.35483871"},{"venue":"ACM Trans. Graph.","id":"335493866cc6a5f035c31d948222b7697a12480a","venue_1":"ACM Trans. Graph.","year":"2014","title":"High-order diffraction and diffuse reflections for interactive sound propagation in large environments","authors":"Carl Schissler, Ravish Mehra, Dinesh Manocha","author_ids":"2368282, 2040242, 1699159","abstract":"We present novel algorithms for modeling interactive diffuse reflections and higher-order diffraction in large-scale virtual environments. Our formulation is based on ray-based sound propagation and is directly applicable to complex geometric datasets. We use an incremental approach that combines radiosity and path tracing techniques to iteratively compute diffuse reflections. We also present algorithms for wavelength-dependent simplification and visibility graph computation to accelerate higher-order diffraction at runtime. The overall system can generate plausible sound effects at interactive rates in large, dynamic scenes that have multiple sound sources. We highlight the performance in complex indoor and outdoor environments and observe an order of magnitude performance improvement over previous methods.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"0399e4998454068e210373c88c99f34a27612d5f","venue_1":"ACM Trans. Graph.","year":"2005","title":"Style translation for human motion","authors":"Eugene Hsu, Kari Pulli, Jovan Popovic","author_ids":"2647034, 1704409, 1731389","abstract":"Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.","cites":"137","conferencePercentile":"74.19354839"},{"venue":"ACM Trans. Graph.","id":"3b629056adb2e447256a5954a392f18c16b5caf8","venue_1":"ACM Trans. Graph.","year":"2009","title":"Abstraction of man-made shapes","authors":"Ravish Mehra, Qingnan Zhou, Jeremy Long, Alla Sheffer, Amy Ashurst Gooch, Niloy J. Mitra","author_ids":"2040242, 2760811, 2931862, 3354923, 2766360, 1710455","abstract":"Man-made objects are ubiquitous in the real world and in virtual environments. While such objects can be very detailed, capturing every small feature, they are often identified and characterized by a small set of defining curves. Compact, abstracted shape descriptions based on such curves are often visually more appealing than the original models, which can appear to be visually cluttered. We introduce a novel algorithm for abstracting three-dimensional geometric models using characteristic curves or contours as building blocks for the abstraction. Our method robustly handles models with poor connectivity, including the extreme cases of polygon soups, common in models of man-made objects taken from online repositories. In our algorithm, we use a two-step procedure that first approximates the input model using a manifold, closed <i>envelope</i> surface and then extracts from it a hierarchical abstraction curve network along with suitable normal information. The constructed curve networks form a compact, yet powerful, representation for the input shapes, retaining their key shape characteristics while discarding minor details and irregularities.","cites":"40","conferencePercentile":"60.22099448"},{"venue":"ACM Trans. Graph.","id":"08e858436f0752e755984650167d4ee69ef9bbdc","venue_1":"ACM Trans. Graph.","year":"2010","title":"Feature-based locomotion controllers","authors":"Martin de Lasa, Igor Mordatch, Aaron Hertzmann","author_ids":"2178721, 2080746, 1747779","abstract":"This paper introduces an approach to control of physics-based characters based on high-level features of movement, such as center-of-mass, angular momentum, and end-effectors. Objective terms are used to control each feature, and are combined by a prioritization algorithm. We show how locomotion can be expressed in terms of a small number of features that control balance and end-effectors. This approach is used to build controllers for human balancing, standing jump, and walking. These controllers provide numerous benefits: human-like qualities such as arm-swing, heel-off, and hip-shoulder counter-rotation emerge automatically during walking; controllers are robust to changes in body parameters; control parameters and goals may be modified at run-time; control parameters apply to intuitive properties such as center-of-mass height; and controllers may be mapped onto entirely new bipeds with different topology and mass distribution, without modifications to the controller itself. No motion capture or off-line optimization process is used.","cites":"75","conferencePercentile":"94.15204678"},{"venue":"ACM Trans. Graph.","id":"59eed7d2cc2bd94053d400ddb57c72ccb0588559","venue_1":"ACM Trans. Graph.","year":"2010","title":"Robust physics-based locomotion using low-dimensional planning","authors":"Igor Mordatch, Martin de Lasa, Aaron Hertzmann","author_ids":"2080746, 2178721, 1747779","abstract":"This paper presents a physics-based locomotion controller based on online planning. At each time-step, a planner optimizes locomotion over multiple phases of gait. Stance dynamics are modeled using a simplified Spring-Load Inverted (SLIP) model, while flight dynamics are modeled using projectile motion equations. Full-body control at each instant is optimized to match the instantaneous plan values, while also maintaining balance. Different types of gaits, including walking, running, and jumping, emerge automatically, as do transitions between different gaits. The controllers can traverse challenging terrain and withstand large external disturbances, while following high-level user commands at interactive rates.","cites":"56","conferencePercentile":"89.18128655"},{"venue":"ACM Trans. Graph.","id":"94e701a97a63b8c604ed925351c4598ac7d51506","venue_1":"ACM Trans. Graph.","year":"2014","title":"Computing smooth surface contours with accurate topology","authors":"Pierre Bénard, Aaron Hertzmann, Michael Kass","author_ids":"1710670, 1747779, 1749075","abstract":"This article introduces a method for accurately computing the visible contours of a smooth 3D surface for stylization. This is a surprisingly difficult problem, and previous methods are prone to topological errors, such as gaps in the outline. Our approach is to generate, for each viewpoint, a new triangle mesh with contours that are topologically equivalent and geometrically close to those of the original smooth surface. The contours of the mesh can then be rendered with exact visibility. The core of the approach is <i>Contour Consistency,</i> a way to prove topological equivalence between the contours of two surfaces. Producing a surface tessellation that satisfies this property is itself challenging; to this end, we introduce a type of triangle that ensures consistency at the contour. We then introduce an iterative mesh generation procedure, based on these ideas. This procedure does not fully guarantee consistency, but errors are not noticeable in our experiments. Our algorithm can operate on any smooth input surface representation; we use Catmull-Clark subdivision surfaces in our implementation. We demonstrate results computing contours of complex 3D objects, on which our method eliminates the contour artifacts of other methods.","cites":"2","conferencePercentile":"8.436213992"},{"venue":"ACM Trans. Graph.","id":"0ee139bbcd81d38ae152109413b9fe4ffed5b69a","venue_1":"ACM Trans. Graph.","year":"2011","title":"Example-based elastic materials","authors":"Sebastian Martin, Bernhard Thomaszewski, Eitan Grinspun, Markus H. Gross","author_ids":"3341690, 1784345, 7522998, 1743207","abstract":"We propose an example-based approach for simulating complex elastic material behavior. Supplied with a few poses that characterize a given object, our system starts by constructing a space of prefered deformations by means of interpolation. During simulation, this example manifold then acts as an additional elastic attractor that guides the object towards its space of prefered shapes. Added on top of existing solid simulation codes, this example potential effectively allows us to implement inhomogeneous and anisotropic materials in a direct and intuitive way. Due to its example-based interface, our method promotes an art-directed approach to solid simulation, which we exemplify on a set of practical examples.","cites":"51","conferencePercentile":"85.78947368"},{"venue":"ACM Trans. Graph.","id":"2573f761c6ee2dc19573d627b2e97faabe48c054","venue_1":"ACM Trans. Graph.","year":"2012","title":"Analysis and synthesis of point distributions based on pair correlation","authors":"A. Cengiz Öztireli, Markus H. Gross","author_ids":"1787433, 1743207","abstract":"Analyzing and synthesizing point distributions are of central importance for a wide range of problems in computer graphics. Existing synthesis algorithms can only generate white or blue-noise distributions with characteristics dictated by the underlying processes used, and analysis tools have not been focused on exploring relations among distributions. We propose a unified analysis and general synthesis algorithms for point distributions. We employ the pair correlation function as the basis of our methods and design synthesis algorithms that can generate distributions with given target characteristics, possibly extracted from an example point set, and introduce a unified characterization of distributions by mapping them to a space implied by pair correlations. The algorithms accept example and output point sets of different sizes and dimensions, are applicable to multi-class distributions and non-Euclidean domains, simple to implement and run in <i>O</i>(<i>n</i>) time. We illustrate applications of our method to real world distributions.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"26ca6a8e97bbcec21a3e788eaab860d5133b3b7e","venue_1":"ACM Trans. Graph.","year":"2012","title":"Gaze correction for home video conferencing","authors":"Claudia Kuster, Tiberiu Popa, Jean Charles Bazin, Craig Gotsman, Markus H. Gross","author_ids":"3346309, 2822563, 1745931, 1724072, 1743207","abstract":"Effective communication using current video conferencing systems is severely hindered by the lack of eye contact caused by the disparity between the locations of the subject and the camera. While this problem has been partially solved for high-end expensive video conferencing systems, it has not been convincingly solved for consumer-level setups. We present a gaze correction approach based on a single Kinect sensor that preserves both the integrity and expressiveness of the face as well as the fidelity of the scene as a whole, producing nearly artifact-free imagery. Our method is suitable for mainstream home video conferencing: it uses inexpensive consumer hardware, achieves real-time performance and requires just a simple and short setup. Our approach is based on the observation that for our application it is sufficient to synthesize only the corrected face. Thus we render a gaze-corrected 3D model of the scene and, with the aid of a face tracker, transfer the gaze-corrected facial portion in a seamless manner onto the original image.","cites":"15","conferencePercentile":"42.67676768"},{"venue":"ACM Trans. Graph.","id":"e5ffca98ffad5c43f5c9cb0f75dd820969754511","venue_1":"ACM Trans. Graph.","year":"2009","title":"Multi-operator media retargeting","authors":"Michael Rubinstein, Ariel Shamir, Shai Avidan","author_ids":"1836449, 2947946, 2740179","abstract":"Content aware resizing gained popularity lately and users can now choose from a battery of methods to retarget their media. However, no single retargeting operator performs well on all images and all target sizes. In a user study we conducted, we found that users prefer to combine seam carving with cropping and scaling to produce results they are satisfied with. This inspires us to propose an algorithm that combines different operators in an optimal manner. We define a <i>resizing space</i> as a conceptual multi-dimensional space combining several resizing operators, and show how a path in this space defines a sequence of operations to retarget media. We define a new image similarity measure, which we term Bi-Directional Warping (BDW), and use it with a dynamic programming algorithm to find an optimal path in the resizing space. In addition, we show a simple and intuitive user interface allowing users to explore the resizing space of various image sizes interactively. Using key-frames and interpolation we also extend our technique to retarget video, providing the flexibility to use the best combination of operators at different times in the sequence.","cites":"104","conferencePercentile":"95.02762431"},{"venue":"ACM Trans. Graph.","id":"37cd51ec8a594456d354f1ecb263a9731823fdae","venue_1":"ACM Trans. Graph.","year":"2012","title":"Lagrangian vortex sheets for animating fluids","authors":"Tobias Pfaff, Nils Thürey, Markus H. Gross","author_ids":"2801835, 1786445, 1743207","abstract":"Buoyant turbulent smoke plumes with a sharp smoke-air interface, such as volcanic plumes, are notoriously hard to simulate. The surface clearly shows small-scale turbulent structures which are costly to resolve. In addition, the turbulence onset is directly visible at the interface, and is not captured by commonly used turbulence models. We present a novel approach that employs a triangle mesh as a high-resolution surface representation combined with a coarse Eulerian solver. On the mesh, we solve the interfacial vortex sheet equations, which allows us to accurately simulate buoyancy induced turbulence. For complex boundary conditions we propose an orthogonal turbulence model that handles vortices caused by obstacle interaction. In addition, we demonstrate a re-sampling scheme to remove surfaces that are hidden inside the bulk volume. In this way we are able to achieve highly detailed simulations of turbulent plumes efficiently.","cites":"23","conferencePercentile":"68.18181818"},{"venue":"ACM Trans. Graph.","id":"923a2a63ee027c5ab6dbc0af6848022dc867be03","venue_1":"ACM Trans. Graph.","year":"2012","title":"Practical temporal consistency for image-based graphics applications","authors":"Manuel Lang, Oliver Wang, Tunç Ozan Aydin, Aljoscha Smolic, Markus H. Gross","author_ids":"2951367, 1958703, 2769987, 1741139, 1743207","abstract":"We present an efficient and simple method for introducing temporal consistency to a large class of optimization driven image-based computer graphics problems. Our method extends recent work in edge-aware filtering, approximating costly global regularization with a fast iterative joint filtering operation. Using this representation, we can achieve tremendous efficiency gains both in terms of memory requirements and running time. This enables us to process entire shots at once, taking advantage of supporting information that exists across far away frames, something that is difficult with existing approaches due to the computational burden of video data. Our method is able to filter along motion paths using an iterative approach that simultaneously uses and estimates per-pixel optical flow vectors. We demonstrate its utility by creating temporally consistent results for a number of applications including optical flow, disparity estimation, colorization, scribble propagation, sparse data up-sampling, and visual saliency computation.","cites":"41","conferencePercentile":"86.11111111"},{"venue":"ACM Trans. Graph.","id":"4fc36e89c0e385d01513fd2c7d21f14920c4bc7a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Scene reconstruction from high spatio-angular resolution light fields","authors":"Changil Kim, Henning Zimmer, Yael Pritch, Alexander Sorkine-Hornung, Markus H. Gross","author_ids":"2692728, 1788459, 1782328, 2893744, 1743207","abstract":"This paper describes a method for scene reconstruction of complex, detailed environments from 3D light fields. Densely sampled light fields in the order of 10<sup>9</sup> light rays allow us to capture the real world in unparalleled detail, but efficiently processing this amount of data to generate an equally detailed reconstruction represents a significant challenge to existing algorithms. We propose an algorithm that leverages coherence in massive light fields by breaking with a number of established practices in image-based reconstruction. Our algorithm first computes reliable depth estimates specifically around object boundaries instead of interior regions, by operating on <i>individual light rays</i> instead of image patches. More homogeneous interior regions are then processed in a <i>fine-to-coarse</i> procedure rather than the standard coarse-to-fine approaches. At no point in our method is any form of global optimization performed. This allows our algorithm to retain precise object contours while still ensuring smooth reconstructions in less detailed areas. While the core reconstruction method handles general unstructured input, we also introduce a <i>sparse representation</i> and a <i>propagation scheme</i> for reliable depth estimates which make our algorithm particularly effective for 3D input, enabling fast and memory efficient processing of \"Gigaray light fields\" on a standard GPU. We show dense 3D reconstructions of highly detailed scenes, enabling applications such as automatic segmentation and image-based rendering, and provide an extensive evaluation and comparison to existing image-based reconstruction techniques.","cites":"68","conferencePercentile":"98.64253394"},{"venue":"ACM Trans. Graph.","id":"0b40a2f3181e34a67486b62fc4de065a4f3609e3","venue_1":"ACM Trans. Graph.","year":"2008","title":"Improved seam carving for video retargeting","authors":"Michael Rubinstein, Ariel Shamir, Shai Avidan","author_ids":"1836449, 2947946, 2740179","abstract":"Video, like images, should support content aware resizing. We present video retargeting using an improved seam carving operator. Instead of removing 1D seams from 2D images we remove 2D seam manifolds from 3D space-time volumes. To achieve this we replace the dynamic programming method of seam carving with graph cuts that are suitable for 3D volumes. In the new formulation, a seam is given by a minimal cut in the graph and we show how to construct a graph such that the resulting cut is a valid seam. That is, the cut is monotonic and connected. In addition, we present a novel energy criterion that improves the visual quality of the retargeted images and videos. The original seam carving operator is focused on removing seams with the least amount of energy, ignoring energy that is introduced into the images and video by applying the operator. To counter this, the new criterion is looking forward in time - removing seams that introduce the least amount of energy into the retargeted result. We show how to encode the improved criterion into graph cuts (for images and video) as well as dynamic programming (for images). We apply our technique to images and videos and present results of various applications.","cites":"253","conferencePercentile":"99.38271605"},{"venue":"ACM Trans. Graph.","id":"79a0cd5a75870ec8b9418069cd0fb77c4321304b","venue_1":"ACM Trans. Graph.","year":"2015","title":"Adaptive color display via perceptually-driven factored spectral projection","authors":"Isaac Kauvar, Samuel J. Yang, Liang Shi, Ian McDowall, Gordon Wetzstein","author_ids":"1841549, 3224268, 1761924, 3153437, 1731170","abstract":"Fundamental display characteristics are constantly being improved, especially resolution, dynamic range, and color reproduction. However, whereas high resolution and high-dynamic range displays have matured as a technology, it remains largely unclear how to extend the color gamut of a display without either sacrificing light throughput or making other tradeoffs. In this paper, we advocate for adaptive color display; with hardware implementations that allow for color primaries to be dynamically chosen, an optimal gamut and corresponding pixel states can be computed in a content-adaptive and user-centric manner. We build a flexible gamut projector and develop a perceptually-driven optimization framework that robustly factors a wide color gamut target image into a set of time-multiplexed primaries and corresponding pixel values. We demonstrate that adaptive primary selection has many benefits over fixed gamut selection and show that our algorithm for joint primary selection and gamut mapping performs better than existing methods. Finally, we evaluate the proposed computational display system extensively in simulation and, via photographs and user experiments, with a prototype adaptive color projector.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"2f7452476910a7dbf6231b6b27aed67d9ed455d3","venue_1":"ACM Trans. Graph.","year":"2007","title":"Seam carving for content-aware image resizing","authors":"Shai Avidan, Ariel Shamir","author_ids":"2740179, 2947946","abstract":"Effective resizing of images should not only use geometric constraints, but consider the image content as well. We present a simple image operator called <i>seam carving</i> that supports content-aware image resizing for both reduction and expansion. A seam is an optimal 8-connected path of pixels on a <i>single</i> image from top to bottom, or left to right, where optimality is defined by an image energy function. By repeatedly carving out or inserting seams in one direction we can change the aspect ratio of an image. By applying these operators in both directions we can retarget the image to a new size. The selection and order of seams protect the content of the image, as defined by the energy function. Seam carving can also be used for image content enhancement and object removal. We support various visual saliency measures for defining the energy of an image, and can also include user input to guide the process. By storing the order of seams in an image we create <i>multi-size</i> images, that are able to continuously change in real time to fit a given size.","cites":"538","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"40213ebcc1e03c25ba97f4110c0b2030fd2e79b6","venue_1":"ACM Trans. Graph.","year":"2016","title":"Computational imaging with multi-camera time-of-flight systems","authors":"Shikhar Shrestha, Felix Heide, Wolfgang Heidrich, Gordon Wetzstein","author_ids":"2184369, 1736720, 1752192, 1731170","abstract":"Depth cameras are a ubiquitous technology used in a wide range of applications, including robotic and machine vision, human-computer interaction, autonomous vehicles as well as augmented and virtual reality. In this paper, we explore the design and applications of phased multi-camera time-of-flight (ToF) systems. We develop a reproducible hardware system that allows for the exposure times and waveforms of up to three cameras to be synchronized. Using this system, we analyze waveform interference between multiple light sources in ToF applications and propose simple solutions to this problem. Building on the concept of orthogonal frequency design, we demonstrate state-of-the-art results for instantaneous radial velocity capture via Doppler time-of-flight imaging and we explore new directions for optically probing global illumination, for example by de-scattering dynamic scenes and by non-line-of-sight motion detection via frequency gating.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"e2774865a86b0ea20f7e5186ce7ba62ed444a84f","venue_1":"ACM Trans. Graph.","year":"2016","title":"ProxImaL: efficient image optimization using proximal algorithms","authors":"Felix Heide, Steven Diamond, Matthias Nießner, Jonathan Ragan-Kelley, Wolfgang Heidrich, Gordon Wetzstein","author_ids":"1736720, 2213823, 2209612, 2488277, 1752192, 1731170","abstract":"Computational photography systems are becoming increasingly diverse, while computational resources---for example on mobile platforms---are rapidly increasing. As diverse as these camera systems may be, slightly different variants of the underlying image processing tasks, such as demosaicking, deconvolution, denoising, inpainting, image fusion, and alignment, are shared between all of these systems. Formal optimization methods have recently been demonstrated to achieve state-of-the-art quality for many of these applications. Unfortunately, different combinations of natural image priors and optimization algorithms may be optimal for different problems, and implementing and testing each combination is currently a time-consuming and error-prone process. ProxImaL is a domain-specific language and compiler for image optimization problems that makes it easy to experiment with different problem formulations and algorithm choices. The language uses proximal operators as the fundamental building blocks of a variety of linear and nonlinear image formation models and cost functions, advanced image priors, and noise models. The compiler intelligently chooses the best way to translate a problem formulation and choice of optimization algorithm into an efficient solver implementation. In applications to the image processing pipeline, deconvolution in the presence of Poisson-distributed shot noise, and burst denoising, we show that a few lines of ProxImaL code can generate highly efficient solvers that achieve state-of-the-art results. We also show applications to the nonlinear and nonconvex problem of phase retrieval.","cites":"3","conferencePercentile":"91.7721519"},{"venue":"ACM Trans. Graph.","id":"f6d0c56b39362748ee10130aa31f93ef22cc77fc","venue_1":"ACM Trans. Graph.","year":"2013","title":"Authoring and animating painterly characters","authors":"Katie Bassett, Ilya Baran, Johannes Schmid, Markus H. Gross, Robert W. Sumner","author_ids":"3298025, 1789898, 5770941, 1743207, 1693475","abstract":"Artists explore the visual style of animated characters through 2D concept art, since it affords them a nearly unlimited degree of creative freedom. Realizing the desired visual style, however, within the 3D character animation pipeline is often impossible, since artists must work within the technical limitations of the pipeline toolset. In order to expand the range of possible visual styles for digital characters, our research aims to incorporate the expressiveness afforded by 2D concept painting into the computer animation pipeline as a core component of character authoring and animation. While prior 3D painting methods focus on static geometry or simple animations, we develop tools for the more difficult task of character animation. Our system shows how 3D stroke-based paintings can be deformed using standard rigging tools. We also propose a configuration-space keyframing algorithm for authoring stroke effects that depend on scene variables such as character pose or light position. During animation, our system supports stroke-based temporal keyframing for one-off effects. Our primary technical contribution is a novel interpolation scheme for configuration-space keyframing that ensures smooth, controllable results. We demonstrate several characters authored with our system that exhibit painted effects difficult to achieve with traditional animation tools.","cites":"3","conferencePercentile":"5.656108597"},{"venue":"ACM Trans. Graph.","id":"c6078acd39ff9ca99bcef83c896307def00b31cc","venue_1":"ACM Trans. Graph.","year":"2013","title":"Topology-driven vectorization of clean line drawings","authors":"Gioacchino Noris, Alexander Sorkine-Hornung, Robert W. Sumner, Maryann Simmons, Markus H. Gross","author_ids":"3206366, 2893744, 1693475, 1943539, 1743207","abstract":"Vectorization provides a link between raster scans of pencil-and-paper drawings and modern digital processing algorithms that require accurate vector representations. Even when input drawings are comprised of clean, crisp lines, inherent ambiguities near junctions make vectorization deceptively difficult. As a consequence, current vectorization approaches often fail to faithfully capture the junctions of drawn strokes. We propose a vectorization algorithm specialized for clean line drawings that analyzes the drawing's topology in order to overcome junction ambiguities. A gradient-based pixel clustering technique facilitates topology computation. This topological information is exploited during centerline extraction by a new &#8220;reverse drawing&#8221; procedure that reconstructs all possible drawing states prior to the creation of a junction and then selects the most likely stroke configuration. For cases where the automatic result does not match the artist's interpretation, our drawing analysis enables an efficient user interface to easily adjust the junction location. We demonstrate results on professional examples and evaluate the vectorization quality with quantitative comparison to hand-traced centerlines as well as the results of leading commercial algorithms.","cites":"20","conferencePercentile":"73.98190045"},{"venue":"ACM Trans. Graph.","id":"e5b7a3af8227a5fa3e07c53799a1033dd046c817","venue_1":"ACM Trans. Graph.","year":"2015","title":"3D motion sensing of any object without prior knowledge","authors":"Leo Miyashita, Ryota Yonezawa, Yoshihiro Watanabe, Masatoshi Ishikawa","author_ids":"1710657, 2634101, 2279584, 1734807","abstract":"We propose a novel three-dimensional motion sensing method using lasers. Recently, object motion information is being used in various applications, and the types of targets that can be sensed continue to diversify. Nevertheless, conventional motion sensing systems have low universality because they require some devices to be mounted on the target, such as accelerometers and gyro sensors, or because they are based on cameras, which limits the types of targets that can be detected. Our method solves this problem and enables noncontact, high-speed, deterministic measurement of the velocity of a moving target without any prior knowledge about the target shape and texture, and can be applied to any unconstrained, unspecified target. These distinctive features are achieved by using a system consisting of a laser range finder, a laser Doppler velocimeter, and a beam controller, in addition to a robust 3D motion calculation method. The motion of the target is recovered from fragmentary physical information, such as the distance and speed of the target at the laser irradiation points. From the acquired laser information, our method can provide a numerically stable solution based on the generalized weighted Tikhonov regularization. Using this technique and a prototype system that we developed, we also demonstrated a number of applications, including motion capture, video game control, and 3D shape integration with everyday objects.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"219fcc484a835b4d8f9b331a44141901f5cf2c0b","venue_1":"ACM Trans. Graph.","year":"2014","title":"VideoSnapping: interactive synchronization of multiple videos","authors":"Oliver Wang, Christopher Schroers, Henning Zimmer, Markus H. Gross, Alexander Sorkine-Hornung","author_ids":"1958703, 2604867, 1788459, 1743207, 2893744","abstract":"Aligning video is a fundamental task in computer graphics and vision, required for a wide range of applications. We present an <i>interactive</i> method for computing optimal nonlinear temporal video alignments of an arbitrary number of videos. We first derive a robust approximation of alignment quality between pairs of clips, computed as a weighted histogram of feature matches. We then find optimal temporal mappings (constituting frame correspondences) using a graph-based approach that allows for very efficient evaluation with artist constraints. This enables an enhancement to the \"snapping\" interface in video editing tools, where videos in a time-line are now able snap to one another when dragged by an artist based on their <i>content</i>, rather than simply start-and-end times. The pairwise snapping is then generalized to multiple clips, achieving a globally optimal temporal synchronization that automatically arranges a series of clips filmed at different times into a single consistent time frame. When followed by a simple spatial registration, we achieve high quality spatiotemporal video alignments at a fraction of the computational complexity compared to previous methods. Assisted temporal alignment is a degree of freedom that has been largely unexplored, but is an important task in video editing. Our approach is simple to implement, highly efficient, and very robust to differences in video content, allowing for <i>interactive</i> exploration of the temporal alignment space for multiple real world HD videos.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"4814dd03d0d4258c9609a9a242181d650035175d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Temporally coherent local tone mapping of HDR video","authors":"Tunç Ozan Aydin, Nikolce Stefanoski, Simone Croci, Markus H. Gross, Aljoscha Smolic","author_ids":"2769987, 3177518, 3213616, 1743207, 1741139","abstract":"Recent subjective studies showed that current tone mapping operators either produce disturbing temporal artifacts, or are limited in their local contrast reproduction capability. We address both of these issues and present an HDR video tone mapping operator that can greatly reduce the input dynamic range, while at the same time preserving scene details without causing significant visual artifacts. To achieve this, we revisit the commonly used <i>spatial</i> base-detail layer decomposition and extend it to the <i>temporal domain</i>. We achieve high quality spatiotemporal edge-aware filtering efficiently by using a mathematically justified iterative approach that approximates a global solution. Comparison with the state-of-the-art, both qualitatively, and quantitatively through a controlled subjective experiment, clearly shows our method's advantages over previous work. We present local tone mapping results on challenging high resolution scenes with complex motion and varying illumination. We also demonstrate our method's capability of preserving scene details at user adjustable scales, and its advantages for low light video sequences with significant camera noise.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"26ce8ffb526474b80471c1c3e9a876ec9dcaf4f5","venue_1":"ACM Trans. Graph.","year":"2003","title":"High dynamic range video","authors":"Sing Bing Kang, Matthew Uyttendaele, Simon A. J. Winder, Richard Szeliski","author_ids":"1738740, 2262291, 2818882, 1717841","abstract":"Typical video footage captured using an off-the-shelf camcorder suffers from limited dynamic range. This paper describes our approach to generate high dynamic range (HDR) video from an image sequence of a dynamic scene captured while rapidly varying the exposure of each frame. Our approach consists of three parts: automatic exposure control during capture, HDR stitching across neighboring frames, and tonemapping for viewing. HDR stitching requires accurately registering neighboring frames and choosing appropriate pixels for computing the radiance map. We show examples for a variety of dynamic scenes. We also show how we can compensate for scene and camera movement when creating an HDR still from a series of bracketed still photographs.","cites":"214","conferencePercentile":"80.64516129"},{"venue":"ACM Trans. Graph.","id":"478937a6b0e95f53977fa1ae421bf6e87d11803e","venue_1":"ACM Trans. Graph.","year":"2004","title":"High-quality video view interpolation using a layered representation","authors":"C. Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon A. J. Winder, Richard Szeliski","author_ids":"1699161, 1738740, 2262291, 2818882, 1717841","abstract":"The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.","cites":"609","conferencePercentile":"97.82608696"},{"venue":"ACM Trans. Graph.","id":"04f7148222be2ef2ec413fb76a85816411de20d7","venue_1":"ACM Trans. Graph.","year":"2005","title":"Adaptation of performed ballistic motion","authors":"Adnan Sulejmanpasic, Jovan Popovic","author_ids":"2882520, 1731389","abstract":"Adaptation of ballistic motion demands a technique that can make required adjustments in anticipation of flight periods when only some physically consistent changes are possible. This article describes a numerical procedure that adjusts a physically consistent motion to fulfill new adaptation requirements expressed in kinematic and dynamic constraints. This iterative procedure refines the original motion with a sequence of minimal adjustments, implicitly favoring motions that are similar to the original performance, and transforming any input motion, including those that are difficult to characterize with an objective function. In total, over twenty adaptations were generated from two recorded performances, a run and a jump, by varying foot placement, restricting muscle use, adding new environment constraints, and changing the length and mass of specific limbs.","cites":"24","conferencePercentile":"12.09677419"},{"venue":"ACM Trans. Graph.","id":"0c7b6e688c46e3f7a86e0a6ed40326d5d00f1cd3","venue_1":"ACM Trans. Graph.","year":"2011","title":"OverCoat: an implicit canvas for 3D painting","authors":"Johannes Schmid, Martin Sebastian Senn, Markus H. Gross, Robert W. Sumner","author_ids":"5770941, 1972306, 1743207, 1693475","abstract":"We present a technique to generalize the 2D painting metaphor to 3D that allows the artist to treat the full 3D space as a canvas. Strokes painted in the 2D viewport window must be embedded in 3D space in a way that gives creative freedom to the artist while maintaining an acceptable level of controllability. We address this challenge by proposing a canvas concept defined implicitly by a 3D scalar field. The artist shapes the implicit canvas by creating approximate 3D proxy geometry. An optimization procedure is then used to embed painted strokes in space by satisfying different objective criteria defined on the scalar field. This functionality allows us to implement tools for painting along level set surfaces or across different level sets. Our method gives the power of fine-tuning the implicit canvas to the artist using a unified painting/sculpting metaphor. A sculpting tool can be used to paint into the implicit canvas. Rather than adding color, this tool creates a local change in the scalar field that results in outward or inward protrusions along the field's gradient direction. We address a visibility ambiguity inherent in 3D stroke rendering with a depth offsetting method that is well suited for hardware acceleration. We demonstrate results with a number of 3D paintings that exhibit effects difficult to realize with existing systems.","cites":"18","conferencePercentile":"41.31578947"},{"venue":"ACM Trans. Graph.","id":"2ed060ea15ce363a442094e05872e4f62bc0a4a8","venue_1":"ACM Trans. Graph.","year":"2006","title":"Inverse kinematics for reduced deformable models","authors":"Kevin G. Der, Robert W. Sumner, Jovan Popovic","author_ids":"2476571, 1693475, 1731389","abstract":"Articulated shapes are aptly described by reduced deformable models that express required shape deformations using a compact set of control parameters. Although sufficient to describe most shape deformations, these control parameters can be ill-suited for animation tasks, particularly when reduced deformable models are inferred automatically from example shapes. Our algorithm provides intuitive and direct control of reduced deformable models similar to a conventional inverse-kinematics algorithm for jointed rigid structures. We present a fully automated pipeline that transforms a set of unarticulated example shapes into a controllable, articulated model. With only a few manipulations, an animator can automatically and interactively pose detailed shapes at rates independent of their geometric complexity.","cites":"74","conferencePercentile":"56.48148148"},{"venue":"ACM Trans. Graph.","id":"ba4705a656937f808f537ea128294fe26e261350","venue_1":"ACM Trans. Graph.","year":"2014","title":"Designing inflatable structures","authors":"Mélina Skouras, Bernhard Thomaszewski, Peter Kaufmann, Akash Garg, Bernd Bickel, Eitan Grinspun, Markus H. Gross","author_ids":"2529055, 1784345, 2027140, 2004417, 3083909, 7522998, 1743207","abstract":"We propose an interactive, optimization-in-the-loop tool for designing inflatable structures. Given a target shape, the user draws a network of seams defining desired segment boundaries in 3D. Our method computes optimally-shaped flat panels for the segments, such that the inflated structure is as close as possible to the target while satisfying the desired seam positions. Our approach is underpinned by physics-based pattern optimization, accurate coarse-scale simulation using tension field theory, and a specialized constraint-optimization method. Our system is fast enough to warrant interactive exploration of different seam layouts, including internal connections, and their effects on the inflated shape. We demonstrate the resulting design process on a varied set of simulation examples, some of which we have fabricated, demonstrating excellent agreement with the design intent.","cites":"14","conferencePercentile":"79.218107"},{"venue":"ACM Trans. Graph.","id":"7ff57d17126277e724c3d0eac53f6396eb120220","venue_1":"ACM Trans. Graph.","year":"2007","title":"Constraint-based motion optimization using a statistical dynamic model","authors":"Jinxiang Chai, Jessica K. Hodgins","author_ids":"1759700, 1788773","abstract":"In this paper, we present a technique for generating animation from a variety of user-defined constraints. We pose constraint-based motion synthesis as a <i>maximum a posterior</i> (MAP) problem and develop an optimization framework that generates natural motion satisfying user constraints. The system automatically learns a statistical dynamic model from motion capture data and then enforces it as a motion prior. This motion prior, together with user-defined constraints, comprises a trajectory optimization problem. Solving this problem in the low-dimensional space yields optimal natural motion that achieves the goals specified by the user. We demonstrate the effectiveness of this approach by generating whole-body and facial motion from a variety of spatial-temporal constraints.","cites":"55","conferencePercentile":"56.4"},{"venue":"ACM Trans. Graph.","id":"0ae51a9ac89e363097bcd675a56901b9444fd739","venue_1":"ACM Trans. Graph.","year":"2007","title":"Construction and optimal search of interpolated motion graphs","authors":"Alla Safonova, Jessica K. Hodgins","author_ids":"2808399, 1788773","abstract":"Many compelling applications would become feasible if novice users had the ability to synthesize high quality human motion based only on a simple sketch and a few easily specified constraints. We approach this problem by representing the desired motion as an interpolation of two time-scaled paths through a motion graph. The graph is constructed to support interpolation and pruned for efficient search. We use an anytime version of <i>A*</i> search to find a globally optimal solution in this graph that satisfies the user's specification. Our approach retains the natural transitions of motion graphs and the ability to synthesize physically realistic variations provided by interpolation. We demonstrate the power of this approach by synthesizing optimal or near optimal motions that include a variety of behaviors in a single motion.","cites":"118","conferencePercentile":"87.2"},{"venue":"ACM Trans. Graph.","id":"4751430f6bfb2a8a8809869d3cc68a7c5d1e565d","venue_1":"ACM Trans. Graph.","year":"1994","title":"User interface specification using an enhanced spreadsheet model","authors":"Scott E. Hudson","author_ids":"1749296","abstract":"This paper describes a new interactive environment for user interface specification which is based on an enhanced spreadsheet model of computation. This environment allows sophisticated graphical user interfaces with dynamic feedback to be implemented with little or no explicit programming. Its goal is to support user interface specification by nonprogramming experts in human factors, visual design, or the application domain. In addition, the system is designed to allow sophisticated end-users to modify and customize their own interfaces. The system is based on a data flow model of computation. This model is presented to the interface designer in the form of a spreadsheet enhanced with new constructs for easier programming and reuse. These constructs include an improved interactive programming environment, a prototype-instance-based inheritance system, support for composition, abstraction, and customization using indirect references, the addition of support for graphical inputs and outputs, and support for the encapsulation of application data structures and routines within system objects.","cites":"34","conferencePercentile":"50"},{"venue":"ACM Trans. Graph.","id":"bbb4d1e25c9685455cc6bff3d0b2f7e9c7410b10","venue_1":"ACM Trans. Graph.","year":"2008","title":"Data-driven modeling of skin and muscle deformation","authors":"Sang Il Park, Jessica K. Hodgins","author_ids":"2459192, 1788773","abstract":"In this paper, we present a data-driven technique for synthesizing skin deformation from skeletal motion. We first create a database of dynamic skin deformations by recording the motion of the surface of the skin with a very large set of motion capture markers. We then build a statistical model of the deformations by dividing them into two parts: static and dynamic. Static deformations are modeled as a function of pose. Dynamic deformations are caused by the actions of the muscles as they move the joints and the inertia of muscles and fat. We approximate these effects by fitting a set of dynamic equations to the pre-recorded data. We demonstrate the viability of this approach by generating skin deformations from the skeletal motion of an actor. We compare the generated animation both to synchronized video of the actor and to ground truth animation created directly from the large marker set.","cites":"36","conferencePercentile":"41.66666667"},{"venue":"ACM Trans. Graph.","id":"0aade95359a3009cb7c327b93cd67d67aae0d975","venue_1":"ACM Trans. Graph.","year":"2010","title":"Stable spaces for real-time clothing","authors":"Edilson de Aguiar, Leonid Sigal, Adrien Treuille, Jessica K. Hodgins","author_ids":"2049341, 2956921, 3064395, 1788773","abstract":"We present a technique for learning clothing models that enables the simultaneous animation of thousands of detailed garments in real-time. This surprisingly simple conditional model learns and preserves the key dynamic properties of a cloth motion along with folding details. Our approach requires no <i>a priori</i> physical model, but rather treats training data as a \"black box.\" We show that the models learned with our method are stable over large time-steps and can approximately resolve cloth-body collisions. We also show that within a class of methods, no simpler model covers the full range of cloth dynamics captured by ours. Our method bridges the current gap between skinning and physical simulation, combining benefits of speed from the former with dynamic effects from the latter. We demonstrate our approach on a variety of apparel worn by male and female human characters performing a varied set of motions typically used in video games (<i>e.g.</i>, walking, running, jumping, <i>etc.</i>).","cites":"28","conferencePercentile":"48.83040936"},{"venue":"ACM Trans. Graph.","id":"6fb968ea35fee24ecad658a9c08e234b80d5bd6c","venue_1":"ACM Trans. Graph.","year":"2012","title":"Fabricating articulated characters from skinned meshes","authors":"Moritz Bächer, Bernd Bickel, Doug L. James, Hanspeter Pfister","author_ids":"8021864, 3083909, 1739671, 1701371","abstract":"Articulated deformable characters are widespread in computer animation. Unfortunately, we lack methods for their automatic fabrication using modern additive manufacturing (AM) technologies. We propose a method that takes a skinned mesh as input, then estimates a fabricatable single-material model that approximates the 3D kinematics of the corresponding virtual articulated character in a piecewise linear manner. We first extract a set of potential joint locations. From this set, together with optional, user-specified range constraints, we then estimate mechanical friction joints that satisfy inter-joint non-penetration and other fabrication constraints. To avoid brittle joint designs, we place joint centers on an approximate medial axis representation of the input geometry, and maximize each joint's minimal cross-sectional area. We provide several demonstrations, manufactured as single, assembled pieces using 3D printers.","cites":"49","conferencePercentile":"89.64646465"},{"venue":"ACM Trans. Graph.","id":"314a508686906f48d55567694fdf3bff50a4604d","venue_1":"ACM Trans. Graph.","year":"2013","title":"Example-based video color grading","authors":"Nicolas Bonneel, Kalyan Sunkavalli, Sylvain Paris, Hanspeter Pfister","author_ids":"1722900, 2454127, 1720990, 1701371","abstract":"In most professional cinema productions, the color palette of the movie is painstakingly adjusted by a team of skilled colorists -- through a process referred to as <i>color grading</i> -- to achieve a certain visual look. The time and expertise required to grade a video makes it difficult for amateurs to manipulate the colors of their own video clips. In this work, we present a method that allows a user to transfer the color palette of a model video clip to their own video sequence. We estimate a per-frame color transform that maps the color distributions in the input video sequence to that of the model video clip. Applying this transformation naively leads to artifacts such as bleeding and flickering. Instead, we propose a novel differential-geometry-based scheme that interpolates these transformations in a manner that minimizes their curvature, similarly to curvature flows. In addition, we automatically determine a set of keyframes that best represent this interpolated transformation curve, and can be used subsequently, to manually refine the color grade. We show how our method can successfully transfer color palettes between videos for a range of visual styles and a number of input video clips.","cites":"12","conferencePercentile":"45.47511312"},{"venue":"ACM Trans. Graph.","id":"114dacbdfe2b666705215ab3ce2c7058eacff5c4","venue_1":"ACM Trans. Graph.","year":"2014","title":"Interactive intrinsic video editing","authors":"Hanspeter Pfister","author_ids":"1701371","abstract":"Separating a photograph into its reflectance and illumination intrinsic images is a fundamentally ambiguous problem, and state-of-the-art algorithms combine sophisticated reflectance and illumination priors with user annotations to create plausible results. However, these algorithms cannot be easily extended to videos for two reasons: first, n&#228;ively applying algorithms designed for single images to videos produce results that are temporally incoherent; second, effectively specifying user annotations for a video requires interactive feedback, and current approaches are orders of magnitudes too slow to support this. We introduce a fast and temporally consistent algorithm to decompose video sequences into their reflectance and illumination components. Our algorithm uses a hybrid <i>&ell;</i><sub>2</sub><i>&ell;</i><sub>p</sub> formulation that separates image gradients into smooth illumination and sparse reflectance gradients using look-up tables. We use a multi-scale parallelized solver to reconstruct the reflectance and illumination from these gradients while enforcing spatial and temporal reflectance constraints and user annotations. We demonstrate that our algorithm automatically produces reasonable results, that can be interactively refined by users, at rates that are two orders of magnitude faster than existing tools, to produce high-quality decompositions for challenging real-world video sequences. We also show how these decompositions can be used for a number of video editing applications including recoloring, retexturing, illumination editing, and lighting-aware compositing.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"1e06308bacd633ae2a71f11ec38b1dfb6d9f35c9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Artist-directed dynamics for 2D animation","authors":"Yunfei Bai, Danny M. Kaufman, C. Karen Liu, Jovan Popovic","author_ids":"1709141, 2972719, 1688533, 1731389","abstract":"Animation artists enjoy the benefits of simulation but do not want to be held back by its constraints. Artist-directed dynamics seeks to resolve this need with a unified method that combines simulation with classical keyframing techniques. The combination of these approaches improves upon both extremes: simulation becomes more customizable and keyframing becomes more automatic. Examining our system in the context of the twelve fundamental animation principles reveals that it stands out for its treatment of exaggeration and appeal. Our system accommodates abrupt jumps, large plastic deformations, and makes it easy to reuse carefully crafted animations.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"247d8d94500b9f22125504794169468e1b4a639f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Facial performance enhancement using dynamic shape space analysis","authors":"Amit Bermano, Derek Bradley, Thabo Beeler, Fabio Zünd, Derek Nowrouzezahrai, Ilya Baran, Olga Sorkine-Hornung, Hanspeter Pfister, Robert W. Sumner, Bernd Bickel, Markus H. Gross","author_ids":"1755628, 1745149, 2486770, 3071164, 1795014, 1789898, 2250001, 1701371, 1693475, 3083909, 1743207","abstract":"The facial performance of an individual is inherently rich in subtle deformation and timing details. Although these subtleties make the performance realistic and compelling, they often elude both motion capture and hand animation. We present a technique for adding fine-scale details and expressiveness to low-resolution art-directed facial performances, such as those created manually using a rig, via marker-based capture, by fitting a morphable model to a video, or through Kinect reconstruction using recent <i>faceshift</i> technology. We employ a high-resolution facial performance capture system to acquire a representative performance of an individual in which he or she explores the full range of facial expressiveness. From the captured data, our system extracts an expressiveness model that encodes subtle spatial and temporal deformation details specific to that particular individual. Once this model has been built, these details can be transferred to low-resolution art-directed performances. We demonstrate results on various forms of input; after our enhancement, the resulting animations exhibit the same nuances and fine spatial details as the captured performance, with optional temporal enhancement to match the dynamics of the actor. Finally, we show that our technique outperforms the current state-of-the-art in example-based facial animation.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"169308a0f79bd15f7f33b8223c371695c72002bd","venue_1":"ACM Trans. Graph.","year":"2015","title":"Generalizing wave gestures from sparse examples for real-time character control","authors":"Helge Rhodin, James Tompkin, Kwang In Kim, Edilson de Aguiar, Hanspeter Pfister, Hans-Peter Seidel, Christian Theobalt","author_ids":"2933543, 1854493, 1808255, 2049341, 1701371, 1746884, 1680185","abstract":"Motion-tracked real-time character control is important for games and VR, but current solutions are limited: retargeting is hard for non-human characters, with locomotion bound to the sensing volume; and pose mappings are ambiguous with difficult dynamic motion control. We robustly estimate wave properties ---<i>amplitude, frequency, and phase</i>---for a set of interactively-defined gestures by mapping user motions to a low-dimensional independent representation. The mapping separates simultaneous or intersecting gestures, and extrapolates gesture variations from single training examples. For animations such as locomotion, wave properties map naturally to stride length, step frequency, and progression, and allow smooth transitions from standing, to walking, to running. Interpolating out-of-phase locomotions is hard, e.g., quadruped legs between walks and runs switch phase, so we introduce a new time-interpolation scheme to reduce artifacts. These improvements to real-time motion-tracked character control are important for common cyclic animations. We validate this in a user study, and show versatility to apply to part- and full-body motions across a variety of sensors.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"0019c4058470829d04520a3f80dc879495cddfc9","venue_1":"ACM Trans. Graph.","year":"2012","title":"Data-driven finger motion synthesis for gesturing characters","authors":"Sophie Jörg, Jessica K. Hodgins, Alla Safonova","author_ids":"1907778, 1788773, 2808399","abstract":"Capturing the body movements of actors to create animations for movies, games, and VR applications has become standard practice, but finger motions are usually added manually as a tedious post-processing step. In this paper, we present a surprisingly simple method to automate this step for gesturing and conversing characters. In a controlled environment, we carefully captured and post-processed finger and body motions from multiple actors. To augment the body motions of virtual characters with plausible and detailed finger movements, our method selects finger motion segments from the resulting database taking into account the similarity of the arm motions and the smoothness of consecutive finger motions. We investigate which parts of the arm motion best discriminate gestures with leave-one-out cross-validation and use the result as a metric to select appropriate finger motions. Our approach provides good results for a number of examples with different gesture types and is validated in a perceptual experiment.","cites":"7","conferencePercentile":"9.848484848"},{"venue":"ACM Trans. Graph.","id":"035b6de367a68470bb5a8914b80b4fd431402c9f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Video-based 3D motion capture through biped control","authors":"Marek Vondrak, Leonid Sigal, Jessica K. Hodgins, Odest Chadwicke Jenkins","author_ids":"3330331, 2956921, 1788773, 1792217","abstract":"Marker-less motion capture is a challenging problem, particularly when only monocular video is available. We estimate human motion from monocular video by recovering three-dimensional controllers capable of implicitly simulating the observed human behavior and replaying this behavior in other environments and under physical perturbations. Our approach employs a state-space biped controller with a balance feedback mechanism that encodes control as a sequence of simple control tasks. Transitions among these tasks are triggered on time and on proprioceptive events (<i>e.g</i>., contact). Inference takes the form of optimal control where we optimize a high-dimensional vector of control parameters and the structure of the controller based on an objective function that compares the resulting simulated motion with input observations. We illustrate our approach by automatically estimating controllers for a variety of motions directly from monocular video. We show that the estimation of controller structure through incremental optimization and refinement leads to controllers that are more stable and that better approximate the reference motion. We demonstrate our approach by capturing sequences of walking, jumping, and gymnastics.","cites":"13","conferencePercentile":"35.1010101"},{"venue":"ACM Trans. Graph.","id":"030fe897ece426ee02a89aedc7378431a7cd6049","venue_1":"ACM Trans. Graph.","year":"2013","title":"Evaluating the distinctiveness and attractiveness of human motions on realistic virtual bodies","authors":"Ludovic Hoyet, Kenneth Ryall, Katja Zibrek, Hwangpil Park, Jehee Lee, Jessica K. Hodgins, Carol O'Sullivan","author_ids":"1869571, 2789969, 1710384, 2683463, 8152254, 1788773, 7711112","abstract":"Recent advances in rendering and data-driven animation have enabled the creation of compelling characters with impressive levels of realism. While data-driven techniques can produce animations that are extremely faithful to the original motion, many challenging problems remain because of the high complexity of human motion. A better understanding of the factors that make human motion recognizable and appealing would be of great value in industries where creating a variety of appealing virtual characters with realistic motion is required. To investigate these issues, we captured thirty actors walking, jogging and dancing, and applied their motions to the same virtual character (one each for the males and females). We then conducted a series of perceptual experiments to explore the distinctiveness and attractiveness of these human motions, and whether characteristic motion features transfer across an individual's different gaits. Average faces are perceived to be less distinctive but more attractive, so we explored whether this was also true for body motion. We found that dancing motions were most easily recognized and that distinctiveness in one gait does not predict how recognizable the same actor is when performing a different motion. As hypothesized, average motions were always amongst the least distinctive and most attractive. Furthermore, as 50% of participants in the experiment were Caucasian European and 50% were Asian Korean, we found that the latter were as good as or better at recognizing the motions of the Caucasian actors than their European counterparts, in particular for dancing males, whom they also rated more highly for attractiveness.","cites":"5","conferencePercentile":"12.66968326"},{"venue":"ACM Trans. Graph.","id":"08616dcbdc0e10c54e2bb052eb8e1142bc531ae0","venue_1":"ACM Trans. Graph.","year":"2006","title":"Interactive decal compositing with discrete exponential maps","authors":"Ryan Schmidt, Cindy Grimm, Brian Wyvill","author_ids":"2291899, 1805157, 8541842","abstract":"A method is described for texturing surfaces using <i>decals</i>, images placed on the surface using local parameterizations. Decal parameterizations are generated with a novel <i>O</i>(<i>N</i> log <i>N</i>) discrete approximation to the exponential map which requires only a single additional step in Dijkstra's graph-distance algorithm. Decals are dynamically composited in an interface that addresses many limitations of previous work. Tools for image processing, deformation/feature-matching, and vector graphics are implemented using direct surface interaction. Exponential map decals can contain holes and can also be combined with conformal parameterization to reduce distortion. The exponential map approximation can be computed on any point set, including meshes and sampled implicit surfaces, and is relatively stable under resampling. The decals stick to the surface as it is interactively deformed, allowing the texture to be preserved even if the surface changes topology. These properties make exponential map decals a suitable approach for texturing animated implicit surfaces.","cites":"47","conferencePercentile":"37.5"},{"venue":"ACM Trans. Graph.","id":"0ea1c5eec8647d4e804d044ae09e16af5f508eef","venue_1":"ACM Trans. Graph.","year":"2014","title":"Automatic editing of footage from multiple social cameras","authors":"Ido Arev, Hyun Soo Park, Yaser Sheikh, Jessica K. Hodgins, Ariel Shamir","author_ids":"2025562, 1806522, 1774867, 1788773, 2947946","abstract":"We present an approach that takes multiple videos captured by <i>social</i> cameras---cameras that are carried or worn by members of the group involved in an activity---and produces a coherent \"cut\" video of the activity. Footage from social cameras contains an intimate, personalized view that reflects the part of an event that was of importance to the camera operator (or wearer). We leverage the insight that social cameras share the focus of attention of the people carrying them. We use this insight to determine where the important \"content\" in a scene is taking place, and use it in conjunction with cinematographic guidelines to select which cameras to cut to and to determine the timing of those cuts. A trellis graph representation is used to optimize an objective function that maximizes coverage of the important content in the scene, while respecting cinematographic guidelines such as the 180-degree rule and avoiding jump cuts. We demonstrate cuts of the videos in various styles and lengths for a number of scenarios, including sports games, street performances, family activities, and social get-togethers. We evaluate our results through an in-depth analysis of the cuts in the resulting videos and through comparison with videos produced by a professional editor and existing commercial solutions.","cites":"29","conferencePercentile":"96.09053498"},{"venue":"ACM Trans. Graph.","id":"493ca0c9fca160775397bf7eccad8cdb590cf344","venue_1":"ACM Trans. Graph.","year":"2015","title":"Autoscanning for coupled scene reconstruction and proactive object analysis","authors":"Kai Xu, Hui Huang, Yifei Shi, Hao Li, Pinxin Long, Jianong Caichen, Wei Sun, Baoquan Chen","author_ids":"1723225, 1927737, 8064820, 1706574, 3089602, 3029379, 1712625, 1748939","abstract":"Detailed scanning of indoor scenes is tedious for humans. We propose autonomous scene scanning by a robot to relieve humans from such a laborious task. In an autonomous setting, detailed scene acquisition is inevitably <i>coupled</i> with scene analysis at the required level of detail. We develop a framework for object-level scene reconstruction coupled with object-centric scene analysis. As a result, the autoscanning and reconstruction will be <i>object-aware</i>, guided by the object analysis. The analysis is, in turn, gradually improved with progressively increased object-wise data fidelity. In realizing such a framework, we drive the robot to execute an iterative <i>analyze-and-validate</i> algorithm which interleaves between object analysis and guided validations.\n The object analysis incorporates online learning into a robust graph-cut based segmentation framework, achieving a global update of object-level segmentation based on the knowledge gained from robot-operated local validation. Based on the current analysis, the robot performs <i>proactive</i> validation over the scene with physical push and scan refinement, aiming at reducing the uncertainty of both object-level segmentation and object-wise reconstruction. We propose a joint entropy to measure such uncertainty based on segmentation confidence and reconstruction quality, and formulate the selection of validation actions as a maximum information gain problem. The output of our system is a reconstructed scene with both object extraction and object-wise geometry fidelity.","cites":"12","conferencePercentile":"93.67346939"},{"venue":"ACM Trans. Graph.","id":"157d16650992d6e552fae6acce467a6617d17064","venue_1":"ACM Trans. Graph.","year":"2014","title":"Generating and ranking diverse multi-character interactions","authors":"Jungdam Won, Kyungho Lee, Carol O'Sullivan, Jessica K. Hodgins, Jehee Lee","author_ids":"2497902, 2802474, 7711112, 1788773, 8152254","abstract":"In many application areas, such as animation for pre-visualizing movie sequences and choreography for dance or other types of performance, only a high-level description of the desired scene is provided as input, either written or verbal. Such sparsity, however, lends itself well to the creative process, as the choreographer, animator or director can be given more choice and control of the final scene. Animating scenes with multi-character interactions can be a particularly complex process, as there are many different constraints to enforce and actions to synchronize. Our novel 'generate-and-rank' approach rapidly and semi-automatically generates data-driven multi-character interaction scenes from high-level graphical descriptions composed of simple clauses and phrases. From a database of captured motions, we generate a multitude of plausible candidate scenes. We then efficiently and intelligently rank these scenes in order to recommend a small but high-quality and diverse selection to the user. This set can then be refined by re-ranking or by generating alternatives to specific interactions. While our approach is applicable to any scenes that depict multi-character interactions, we demonstrate its efficacy for choreographing fighting scenes and evaluate it in terms of performance and the diversity and coverage of the results.","cites":"0","conferencePercentile":"2.057613169"},{"venue":"ACM Trans. Graph.","id":"5cf676a6227d3b1388802991ef4f44ffaf98bec1","venue_1":"ACM Trans. Graph.","year":"2015","title":"Blind video temporal consistency","authors":"Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, Hanspeter Pfister","author_ids":"1722900, 1854493, 2454127, 3232265, 1720990, 1701371","abstract":"Extending image processing techniques to videos is a non-trivial task; applying processing independently to each video frame often leads to temporal inconsistencies, and explicitly encoding temporal consistency requires algorithmic changes. We describe a more general approach to temporal consistency. We propose a gradient-domain technique that is blind to the particular image processing algorithm. Our technique takes a series of processed frames that suffers from flickering and generates a temporally-consistent video sequence. The core of our solution is to infer the temporal regularity from the original unprocessed video, and use it as a temporal consistency guide to stabilize the processed sequence. We formally characterize the frequency properties of our technique, and demonstrate, in practice, its ability to stabilize a wide range of popular image processing techniques including enhancement and stylization of color and tone, intrinsic images, and depth estimation.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"11757f6404965892e02c8c49c5665e2b25492bec","venue_1":"ACM Trans. Graph.","year":"2006","title":"Capturing and animating skin deformation in human motion","authors":"Sang Il Park, Jessica K. Hodgins","author_ids":"2459192, 1788773","abstract":"During dynamic activities, the surface of the human body moves in many subtle but visually significant ways: bending, bulging, jiggling, and stretching. We present a technique for capturing and animating those motions using a commercial motion capture system and approximately 350 markers. Although the number of markers is significantly larger than that used in conventional motion capture, it is only a sparse representation of the true shape of the body. We supplement this sparse sample with a detailed, actor-specific surface model. The motion of the skin can then be computed by segmenting the markers into the motion of a set of rigid parts and a residual deformation (approximated first as a quadratic transformation and then with radial basis functions). We demonstrate the power of this approach by capturing flexing muscles, high frequency motions, and abrupt decelerations on several actors. We compare these results both to conventional motion capture and skinning and to synchronized video of the actors.","cites":"101","conferencePercentile":"75.46296296"},{"venue":"ACM Trans. Graph.","id":"598969b8612f436247522af9524bd4d5b64837a8","venue_1":"ACM Trans. Graph.","year":"2005","title":"Performance animation from low-dimensional control signals","authors":"Jinxiang Chai, Jessica K. Hodgins","author_ids":"1759700, 1788773","abstract":"This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.","cites":"170","conferencePercentile":"88.30645161"},{"venue":"ACM Trans. Graph.","id":"070cd6cfeecb31a024ea643b69babf8716e00436","venue_1":"ACM Trans. Graph.","year":"2004","title":"Synthesizing animations of human manipulation tasks","authors":"Katsu Yamane, James J. Kuffner, Jessica K. Hodgins","author_ids":"1707684, 1743428, 1788773","abstract":"Even such simple tasks as placing a box on a shelf are difficult to animate, because the animator must carefully position the character to satisfy geometric and balance constraints while creating motion to perform the task with a natural-looking style. In this paper, we explore an approach for animating characters manipulating objects that combines the power of path planning with the domain knowledge inherent in data-driven, constraint-based inverse kinematics. A path planner is used to find a motion for the object such that the corresponding poses of the character satisfy geometric, kinematic, and posture constraints. The inverse kinematics computation of the character's pose resolves redundancy by biasing the solution toward natural-looking poses extracted from a database of captured motions. Having this database greatly helps to increase the quality of the output motion. The computed path is converted to a motion trajectory using a model of the velocity profile. We demonstrate the effectiveness of the algorithm by generating animations across a wide range of scenarios that cover variations in the geometric, kinematic, and dynamic models of the character, the manipulated object, and obstacles in the scene.","cites":"140","conferencePercentile":"70.65217391"},{"venue":"ACM Trans. Graph.","id":"5388d77ffd6e350ca87c646e6cfa811046c732cd","venue_1":"ACM Trans. Graph.","year":"2008","title":"Real-time control of physically based simulations using gentle forces","authors":"Jernej Barbic, Jovan Popovic","author_ids":"1740055, 1731389","abstract":"Recent advances have brought real-time physically based simulation within reach, but simulations are still difficult to control in real time. We present interactive simulations of passive systems such as deformable solids or fluids that are not only fast, but also directable: they follow given input trajectories while simultaneously reacting to user input and other unexpected disturbances. We achieve such directability using a real-time controller that runs in tandem with a real-time physically based simulation. To avoid stiff and over-controlled systems where the natural dynamics are overpowered, the injection of control forces has to be minimized. This search for gentle forces can be made tractable in real-time by linearizing the system dynamics around the input trajectory, and then using a time-varying linear quadratic regulator to build the controller. We show examples of controlled complex deformable solids and fluids, demonstrating that our approach generates a requested fixed outcome for reasonable user inputs, while simultaneously providing runtime motion variety.","cites":"37","conferencePercentile":"42.90123457"},{"venue":"ACM Trans. Graph.","id":"3a211418c2718ccb063b8da5e694f95100a97b1d","venue_1":"ACM Trans. Graph.","year":"2008","title":"Interactive simulation of stylized human locomotion","authors":"Marco da Silva, Yeuhi Abe, Jovan Popovic","author_ids":"2370848, 2087969, 1731389","abstract":"Animating natural human motion in dynamic environments is difficult because of complex geometric and physical interactions. Simulation provides an automatic solution to parts of this problem, but it needs control systems to produce lifelike motions. This paper describes the systematic computation of controllers that can reproduce a range of locomotion styles in interactive simulations. Given a reference motion that describes the desired style, a derived control system can reproduce that style in simulation and in new environments. Because it produces high-quality motions that are both geometrically and physically consistent with simulated surroundings, interactive animation systems could begin to use this approach along with more established kinematic methods.","cites":"69","conferencePercentile":"75.30864198"},{"venue":"ACM Trans. Graph.","id":"6b9aadccee811de0a47d94c6bedfd66899e614c4","venue_1":"ACM Trans. Graph.","year":"2009","title":"Deformable object animation using reduced optimal control","authors":"Jernej Barbic, Marco da Silva, Jovan Popovic","author_ids":"1740055, 2370848, 1731389","abstract":"Keyframe animation is a common technique to generate animations of deformable characters and other soft bodies. With spline interpolation, however, it can be difficult to achieve secondary motion effects such as plausible dynamics when there are thousands of degrees of freedom to animate. Physical methods can provide more realism with less user effort, but it is challenging to apply them to quickly create <i>specific</i> animations that closely follow prescribed animator goals. We present a fast space-time optimization method to author physically based deformable object simulations that conform to animator-specified keyframes. We demonstrate our method with FEM deformable objects and mass-spring systems.\n Our method minimizes an objective function that penalizes the sum of keyframe deviations plus the deviation of the trajectory from physics. With existing methods, such minimizations operate in high dimensions, are slow, memory consuming, and prone to local minima. We demonstrate that significant computational speedups and robustness improvements can be achieved if the optimization problem is properly solved in a low-dimensional space. Selecting a low-dimensional space so that the intent of the animator is accommodated, and that at the same time space-time optimization is convergent and fast, is difficult. We present a method that generates a quality low-dimensional space using the given keyframes. It is then possible to find quality solutions to difficult space-time optimization problems robustly and in a manner of minutes.","cites":"47","conferencePercentile":"69.06077348"},{"venue":"ACM Trans. Graph.","id":"0caf294cad1bf737ec777ff6645f636070c24c09","venue_1":"ACM Trans. Graph.","year":"2009","title":"Semantic deformation transfer","authors":"Ilya Baran, Daniel Vlasic, Eitan Grinspun, Jovan Popovic","author_ids":"1789898, 1880628, 7522998, 1731389","abstract":"Transferring existing mesh deformation from one character to another is a simple way to accelerate the laborious process of mesh animation. In many cases, it is useful to preserve the semantic characteristics of the motion instead of its literal deformation. For example, when applying the walking motion of a human to a flamingo, the knees should bend in the opposite direction. Semantic deformation transfer accomplishes this task with a shape space that enables interpolation and projection with standard linear algebra. Given several example mesh pairs, semantic deformation transfer infers a correspondence between the shape spaces of the two characters. This enables automatic transfer of new poses and animations.","cites":"31","conferencePercentile":"46.68508287"},{"venue":"ACM Trans. Graph.","id":"1d1b928a6affb5758d8f3be7aade7d49b4c97e78","venue_1":"ACM Trans. Graph.","year":"2015","title":"Microstructures to control elasticity in 3D printing","authors":"Christian Schumacher, Bernd Bickel, Jan Rys, Steve Marschner, Chiara Daraio, Markus H. Gross","author_ids":"2635933, 3083909, 2190543, 2593798, 2566438, 1743207","abstract":"We propose a method for fabricating deformable objects with spatially varying elasticity using 3D printing. Using a single, relatively stiff printer material, our method designs an assembly of small-scale microstructures that have the effect of a softer material at the object scale, with properties depending on the microstructure used in each part of the object. We build on work in the area of metamaterials, using numerical optimization to design tiled microstructures with desired properties, but with the key difference that our method designs families of related structures that can be interpolated to smoothly vary the material properties over a wide range. To create an object with spatially varying elastic properties, we tile the object's interior with microstructures drawn from these families, generating a different microstructure for each cell using an efficient algorithm to select compatible structures for neighboring cells. We show results computed for both 2D and 3D objects, validating several 2D and 3D printed structures using standard material tests as well as demonstrating various example applications.","cites":"15","conferencePercentile":"97.34693878"},{"venue":"ACM Trans. Graph.","id":"546f53ef6c8e195d30b29efcf53d0590d153dff6","venue_1":"ACM Trans. Graph.","year":"2015","title":"Detailed spatio-temporal reconstruction of eyelids","authors":"Amit Bermano, Thabo Beeler, Yeara Kozlov, Derek Bradley, Bernd Bickel, Markus H. Gross","author_ids":"1755628, 2486770, 2541901, 1745149, 3083909, 1743207","abstract":"In recent years we have seen numerous improvements on 3D scanning and tracking of human faces, greatly advancing the creation of digital doubles for film and video games. However, despite the high-resolution quality of the reconstruction approaches available, current methods are unable to capture one of the most important regions of the face - the eye region. In this work we present the first method for detailed spatio-temporal reconstruction of eyelids. Tracking and reconstructing eyelids is extremely challenging, as this region exhibits very complex and unique skin deformation where skin is folded under while opening the eye. Furthermore, eyelids are often only partially visible and obstructed due to self-occlusion and eyelashes. Our approach is to combine a geometric deformation model with image data, leveraging multi-view stereo, optical flow, contour tracking and wrinkle detection from local skin appearance. Our deformation model serves as a prior that enables reconstruction of eyelids even under strong self-occlusions caused by rolling and folding skin as the eye opens and closes. The output is a person-specific, time-varying eyelid reconstruction with anatomically plausible deformations. Our high-resolution detailed eyelids couple naturally with current facial performance capture approaches. As a result, our method can largely increase the fidelity of facial capture and the creation of digital doubles.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"0a3b14a0e7cad17ceef71d353cb4751c9c71b03a","venue_1":"ACM Trans. Graph.","year":"2009","title":"Linear Bellman combination for control of character animation","authors":"Marco da Silva, Frédo Durand, Jovan Popovic","author_ids":"2370848, 1728125, 1731389","abstract":"Controllers are necessary for physically-based synthesis of character animation. However, creating controllers requires either manual tuning or expensive computer optimization. We introduce linear Bellman combination as a method for reusing existing controllers. Given a set of controllers for related tasks, this combination creates a controller that performs a new task. It naturally weights the contribution of each component controller by its relevance to the current state and goal of the system. We demonstrate that linear Bellman combination outperforms naive combination often succeeding where naive combination fails. Furthermore, this combination is provably optimal for a new task if the component controllers are also optimal for related tasks. We demonstrate the applicability of linear Bellman combination to interactive character control of stepping motions and acrobatic maneuvers.","cites":"32","conferencePercentile":"48.06629834"},{"venue":"ACM Trans. Graph.","id":"03d775a024ad0f570b52a29b92aae6bc3a9cbc6a","venue_1":"ACM Trans. Graph.","year":"2011","title":"Bounded biharmonic weights for real-time deformation","authors":"Alec Jacobson, Ilya Baran, Jovan Popovic, Olga Sorkine-Hornung","author_ids":"2574283, 1789898, 1731389, 2250001","abstract":"Object deformation with linear blending dominates practical use as the fastest approach for transforming raster images, vector graphics, geometric models and animated characters. Unfortunately, linear blending schemes for skeletons or cages are not always easy to use because they may require manual weight painting or modeling closed polyhedral envelopes around objects. Our goal is to make the design and control of deformations simpler by allowing the user to work freely with the most convenient combination of handle types. We develop linear blending weights that produce smooth and intuitive deformations for points, bones and cages of arbitrary topology. Our weights, called bounded biharmonic weights, minimize the Laplacian energy subject to bound constraints. Doing so spreads the influences of the controls in a shape-aware and localized manner, even for objects with complex and concave boundaries. The variational weight optimization also makes it possible to customize the weights so that they preserve the shape of specified essential object features. We demonstrate successful use of our blending weights for real-time deformation of 2D and 3D shapes.","cites":"112","conferencePercentile":"97.89473684"},{"venue":"ACM Trans. Graph.","id":"c7b34086fafb08360ffd2adba4f78ec8473972c2","venue_1":"ACM Trans. Graph.","year":"2016","title":"Computational design of stable planar-rod structures","authors":"Eder Miguel, Mathias Lepoutre, Bernd Bickel","author_ids":"1885083, 3430111, 3083909","abstract":"We present a computational method for designing wire sculptures consisting of interlocking wires. Our method allows the computation of aesthetically pleasing structures that are structurally stable, efficiently fabricatable with a 2D wire bending machine, and assemblable without the need of additional connectors. Starting from a set of planar contours provided by the user, our method automatically tests for the feasibility of a design, determines a discrete ordering of wires at intersection points, and optimizes for the rest shape of the individual wires to maximize structural stability under frictional contact. In addition to their application to art, wire sculptures present an extremely efficient and fast alternative for low-fidelity rapid prototyping because manufacturing time and required material linearly scales with the physical size of objects. We demonstrate the effectiveness of our approach on a varied set of examples, all of which we fabricated.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"97375d0e4141173534f8e10d6e13e9503cfc1a79","venue_1":"ACM Trans. Graph.","year":"2015","title":"Topology-based catalogue exploration framework for identifying view-enhanced tower designs","authors":"Harish Doraiswamy, Nivan Ferreira, Marcos Lage, Huy T. Vo, Luc Wilson, Heidi Werner, Muchan Park, Cláudio T. Silva","author_ids":"2163458, 3254269, 2637259, 1688854, 2242066, 2567782, 3143987, 1719203","abstract":"There is a growing expectation for high performance design in architecture which negotiates between the requirements of the client and the physical constraints of a building site. Clients for building projects often challenge architects to maximize view quality since it can significantly increase real estate value. To pursue this challenge, architects typically move through several design revision cycles to identify a set of design options which satisfy these view quality expectations in coordination with other goals of the project. However, reviewing a large quantity of design options within the practical time constraints is challenging due to the limitations of existing tools for view performance evaluation. These challenges include flexibility in the definition of view quality and the ability to handle the expensive computation involved in assessing both the view quality and the exploration of a large number of possible design options. To address these challenges, we propose a catalogue-based framework that enables the interactive exploration of conceptual building design options based on adjustable view preferences. We achieve this by integrating a flexible mechanism to combine different view measures with an indexing scheme for view computation that achieves high performance and precision. Furthermore, the combined view measures are then used to model the building design space as a high dimensional scalar function. The topological features of this function are then used as candidate building designs. Finally, we propose an interactive design catalogue for the exploration of potential building designs based on the given view preferences. We demonstrate the effectiveness of our approach through two use case scenarios to assess view potential and explore conceptual building designs on sites with high development likelihood in Manhattan, New York City.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"42a07d1cc33c2042817fe4bac831b9a30aa7a8f0","venue_1":"ACM Trans. Graph.","year":"1999","title":"Two Methods for Display of High Contrast Images","authors":"Jack Tumblin, Jessica K. Hodgins, Brian K. Guenter","author_ids":"2914934, 1788773, 1842844","abstract":"High contrast images are common in night scenes and other scenes that  include dark shadows and bright light sources. These scenes are difficult to display because their contrasts greatly exceed the range of most display devices for images. As a result, the image constrasts are compressed or truncated, obscuring subtle textures and details. Humans view and understand high contrast scenes easily, &#8220;adapting&#8221; their visual response to avoid compression or truncation with no apparent loss of detail. By imitating some of these visual adaptation processes, we developed methods for the improved display of high-contrast images. The first builds a display image from several layers of lighting and surface properties. Only the lighting layers are compressed, drastically reducing  contrast while preserving much of the image detail. This method is practical only for synthetic images where the layers can be retained from the rendering process. The second method interactively adjusts the displayed image to preserve local contrasts in a small &#8220;foveal&#8221; neighborhood. Unlike the first method, this technique is usable on any image and includes a new tone reproduction operator. Both methods use a sigmoid function for contrast compression. This function has no effect when applied to small signals but compresses large signals to fit within an asymptotic limit. We demonstrate the effectiveness of these approaches by comparing processed and unprocessed images.","cites":"110","conferencePercentile":"92.85714286"},{"venue":"ACM Trans. Graph.","id":"ba80c76249bed29426c3d6de0458d2637d93e103","venue_1":"ACM Trans. Graph.","year":"2016","title":"FlexMolds: automatic design of flexible shells for molding","authors":"Luigi Malomo, Nico Pietroni, Bernd Bickel, Paolo Cignoni","author_ids":"2908942, 3283805, 3083909, 1738697","abstract":"We present FlexMolds, a novel computational approach to automatically design flexible, reusable molds that, once 3D printed, allow us to physically fabricate, by means of liquid casting, multiple copies of complex shapes with rich surface details and complex topology. The approach to design such flexible molds is based on a greedy bottom-up search of possible cuts over an object, evaluating for each possible cut the feasibility of the resulting mold. We use a dynamic simulation approach to evaluate candidate molds, providing a heuristic to generate forces that are able to open, detach, and remove a complex mold from the object it surrounds. We have tested the approach with a number of objects with nontrivial shapes and topologies.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"0a3fb6b2e6908ac790bcb95a555940fcee9a99c6","venue_1":"ACM Trans. Graph.","year":"2012","title":"Fast automatic skinning transformations","authors":"Alec Jacobson, Ilya Baran, Ladislav Kavan, Jovan Popovic, Olga Sorkine-Hornung","author_ids":"2574283, 1789898, 1771758, 1731389, 2250001","abstract":"Skinning transformations are a popular way to articulate shapes and characters. However, traditional animation interfaces require all of the skinning transformations to be specified explicitly, typically using a control structure (a rig). We propose a system where the user specifies only a subset of the degrees of freedom and the rest are automatically inferred using nonlinear, rigidity energies. By utilizing a low-order model and reformulating our energy functions accordingly, our algorithm runs orders of magnitude faster than previous methods without compromising quality. In addition to the immediate boosts in performance for existing modeling and real time animation tools, our approach also opens the door to new modes of control: disconnected skeletons combined with shape-aware inverse kinematics. With automatically generated skinning weights, our method can also be used for fast variational shape modeling.","cites":"40","conferencePercentile":"84.34343434"},{"venue":"ACM Trans. Graph.","id":"03cc82d34c31db07e7d0858f3fe2d65ccbad4006","venue_1":"ACM Trans. Graph.","year":"2012","title":"Temporally coherent completion of dynamic shapes","authors":"Hao Li, Linjie Luo, Daniel Vlasic, Pieter Peers, Jovan Popovic, Mark Pauly, Szymon Rusinkiewicz","author_ids":"1706574, 1702459, 1880628, 1808270, 1731389, 1741645, 7723706","abstract":"We present a novel shape completion technique for creating temporally coherent watertight surfaces from real-time captured dynamic performances. Because of occlusions and low surface albedo, scanned mesh sequences typically exhibit large holes that persist over extended periods of time. Most conventional dynamic shape reconstruction techniques rely on template models or assume slow deformations in the input data. Our framework sidesteps these requirements and directly initializes shape completion with topology derived from the visual hull. To seal the holes with patches that are consistent with the subject's motion, we first minimize surface bending energies in each frame to ensure smooth transitions across hole boundaries. Temporally coherent dynamics of surface patches are obtained by unwarping all frames within a time window using accurate interframe correspondences. Aggregated surface samples are then filtered with a temporal visibility kernel that maximizes the use of nonoccluded surfaces. A key benefit of our shape completion strategy is that it does not rely on long-range correspondences or a template model. Consequently, our method does not suffer error accumulation typically introduced by noise, large deformations, and drastic topological changes. We illustrate the effectiveness of our method on several high-resolution scans of human performances captured with a state-of-the-art multiview 3D acquisition system.","cites":"42","conferencePercentile":"87.37373737"},{"venue":"ACM Trans. Graph.","id":"0a6721ec02d828ee16c902b3f7711f541681f9e9","venue_1":"ACM Trans. Graph.","year":"2002","title":"Creating models of truss structures with optimization","authors":"Jeffrey Smith, Jessica K. Hodgins, Irving J. Oppenheim, Andrew P. Witkin","author_ids":"4088961, 1788773, 2892725, 1809905","abstract":"We present a method for designing truss structures, a common and complex category of buildings, using non-linear optimization. Truss structures are ubiquitous in the industrialized world, appearing as bridges, towers, roof supports and building exoskeletons, yet are complex enough that modeling them by hand is time consuming and tedious. We represent trusses as a set of rigid bars connected by pin joints, which may change location during optimization. By including the location of the joints as well as the strength of individual beams in our design variables, we can simultaneously optimize the geometry and the mass of structures. We present the details of our technique together with examples illustrating its use, including comparisons with real structures.","cites":"30","conferencePercentile":"20"},{"venue":"ACM Trans. Graph.","id":"212a6148b6d8a48e3e4d935df98f65e51bd534de","venue_1":"ACM Trans. Graph.","year":"2002","title":"Interactive control of avatars animated with human motion data","authors":"Jehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, Nancy S. Pollard","author_ids":"8152254, 1759700, 2550962, 1788773, 1735665","abstract":"Real-time control of three-dimensional avatars is an important problem in the context of computer games and virtual environments. Avatar animation and control is difficult, however, because a large repertoire of avatar behaviors must be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a motion database can be preprocessed for flexibility in behavior and efficient search and exploited for real-time avatar control. Flexibility is created by identifying plausible transitions between motion segments, and efficient search through the resulting graph structure is obtained through clustering. Three interface techniques are demonstrated for controlling avatar motion using this data structure: the user selects from a set of available choices, sketches a path through an environment, or acts out a desired motion in front of a video camera. We demonstrate the flexibility of the approach through four different applications and compare the avatar motion to directly recorded human motion.","cites":"514","conferencePercentile":"96"},{"venue":"ACM Trans. Graph.","id":"31a54d485f81b5b9109bf02c34e6709dc1e644d7","venue_1":"ACM Trans. Graph.","year":"2000","title":"Theory and application of specular path perturbation","authors":"Min Chen, James Arvo","author_ids":"1711628, 1739434","abstract":"In this paper we apply perturbation methods to the problem of computing specular reflections in curved surfaces. The key idea is to generate families of closely related optical paths by expanding a given path into a high-dimensional Taylor series. Our path perturbation method is based on closed-form expressions for linear and higher-order approximations of ray paths, which are derived using Fermat's Variation Principle and the Implicit Function Theorem (IFT). The perturbation formula presented here holds for general multiple-bounce reflection paths and provides a mathematical foundation for exploiting path coherence in ray tracing acceleration techniques and incremental rendering. To illustrate its use, we describe an algorithm for fast approximation of specular reflections on curved surfaces; the resulting images are highly accurate and nearly indistinguishable from ray traced images.","cites":"40","conferencePercentile":"72.72727273"},{"venue":"ACM Trans. Graph.","id":"101ecc4be6d1a0d886546a7276e2dc9aef7cc21d","venue_1":"ACM Trans. Graph.","year":"2004","title":"Flow-based video synthesis and editing","authors":"Kiran S. Bhat, Steven M. Seitz, Jessica K. Hodgins, Pradeep K. Khosla","author_ids":"2553644, 1679223, 1788773, 1708808","abstract":"This paper presents a novel algorithm for synthesizing and editing video of natural phenomena that exhibit continuous flow patterns. The algorithm analyzes the motion of textured particles in the input video along user-specified flow lines, and synthesizes seamless video of arbitrary length by enforcing temporal continuity along a second set of user-specified flow lines. The algorithm is simple to implement and use. We used this technique to edit video of water-falls, rivers, flames, and smoke.","cites":"46","conferencePercentile":"23.91304348"},{"venue":"ACM Trans. Graph.","id":"43b32d0184cf214d58eaae4c93002bd774e3d5f4","venue_1":"ACM Trans. Graph.","year":"2004","title":"Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces","authors":"Alla Safonova, Jessica K. Hodgins, Nancy S. Pollard","author_ids":"2808399, 1788773, 1735665","abstract":"Optimization is an appealing way to compute the motion of an animated character because it allows the user to specify the desired motion in a sparse, intuitive way. The difficulty of solving this problem for complex characters such as humans is due in part to the high dimensionality of the search space. The dimensionality is an artifact of the problem representation because most dynamic human behaviors are intrinsically low dimensional with, for example, legs and arms operating in a coordinated way. We describe a method that exploits this observation to create an optimization problem that is easier to solve. Our method utilizes an existing motion capture database to find a low-dimensional space that captures the properties of the desired behavior. We show that when the optimization problem is solved within this low-dimensional subspace, a sparse sketch can be used as an initial guess and full physics constraints can be enabled. We demonstrate the power of our approach with examples of forward, vertical, and turning jumps; with running and walking; and with several acrobatic flips.","cites":"257","conferencePercentile":"84.7826087"},{"venue":"ACM Trans. Graph.","id":"ab21f64f0ffde7c8d13e90a69cce14c2aac8752b","venue_1":"ACM Trans. Graph.","year":"2007","title":"Automatic rigging and animation of 3D characters","authors":"Ilya Baran, Jovan Popovic","author_ids":"1789898, 1731389","abstract":"Animating an articulated 3D character currently requires manual rigging to specify its internal skeletal structure and to define how the input motion deforms its surface. We present a method for animating characters automatically. Given a static character mesh and a generic skeleton, our method adapts the skeleton to the character and attaches it to the surface, allowing skeletal motion data to animate the character. Because a single skeleton can be used with a wide range of characters, our method, in conjunction with a library of motions for a few skeletons, enables a user-friendly animation system for novices and children. Our prototype implementation, called Pinocchio, typically takes under a minute to rig a character on a modern midrange PC.","cites":"230","conferencePercentile":"96.8"},{"venue":"ACM Trans. Graph.","id":"884d5ae69c3ec530e46d71633cedfed96062cea8","venue_1":"ACM Trans. Graph.","year":"2016","title":"Subdivision exterior calculus for geometry processing","authors":"Fernando de Goes, Mathieu Desbrun, Mark Meyer, Tony DeRose","author_ids":"2322290, 1716096, 8735841, 1792251","abstract":"This paper introduces a new computational method to solve differential equations on subdivision surfaces. Our approach adapts the numerical framework of Discrete Exterior Calculus (DEC) from the polygonal to the subdivision setting by exploiting the refin-ability of subdivision basis functions. The resulting <i>Subdivision Exterior Calculus</i> (SEC) provides significant improvements in accuracy compared to existing polygonal techniques, while offering exact finite-dimensional analogs of continuum structural identities such as Stokes' theorem and Helmholtz-Hodge decomposition. We demonstrate the versatility and efficiency of SEC on common geometry processing tasks including parameterization, geodesic distance computation, and vector field design.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"038205969f19e8bee9ced0f0f5f69458590d7fef","venue_1":"ACM Trans. Graph.","year":"2011","title":"Mixed-order compositing for 3D paintings","authors":"Ilya Baran, Johannes Schmid, Thomas Siegrist, Markus H. Gross, Robert W. Sumner","author_ids":"1789898, 5770941, 2702932, 1743207, 1693475","abstract":"We present a method for rendering 3D paintings by compositing brush strokes embedded in space. The challenge in compositing 3D brush strokes is reconciling conflicts between their <i>z</i>-order in 3D and the order in which the strokes were painted, while maintaining temporal and spatial coherence. Our algorithm smoothly transitions between compositing closer strokes over those farther away and compositing strokes painted later over those painted earlier. It is efficient, running in <i>O</i>(<i>n</i> log <i>n</i>) time, and simple to implement. We demonstrate its effectiveness on a variety of 3D paintings.","cites":"8","conferencePercentile":"12.10526316"},{"venue":"ACM Trans. Graph.","id":"65c7b26ca7d56b42b0fd7d08f5b9735a9066f78a","venue_1":"ACM Trans. Graph.","year":"2011","title":"OSCAM - optimized stereoscopic camera control for interactive 3D","authors":"Thomas Oskam, Alexander Sorkine-Hornung, Huw Bowles, Kenny Mitchell, Markus H. Gross","author_ids":"3032690, 2893744, 2195121, 3315742, 1743207","abstract":"This paper presents a controller for camera convergence and interaxial separation that specifically addresses challenges in <i>interactive</i> stereoscopic applications like games. In such applications, unpredictable viewer- or object-motion often compromises stereopsis due to excessive binocular disparities. We derive constraints on the camera separation and convergence that enable our controller to automatically adapt to any given viewing situation and 3D scene, providing an exact mapping of the virtual content into a comfortable depth range around the display. Moreover, we introduce an interpolation function that linearizes the transformation of stereoscopic depth over time, minimizing nonlinear visual distortions. We describe how to implement the complete control mechanism on the GPU to achieve running times below 0.2ms for full HD. This provides a practical solution even for demanding real-time applications. Results of a user study show a significant increase of stereoscopic comfort, without compromising perceived realism. Our controller enables 'fail-safe' stereopsis, provides intuitive control to accommodate to personal preferences, and allows to properly display stereoscopic content on differently sized output devices.","cites":"23","conferencePercentile":"50.52631579"},{"venue":"ACM Trans. Graph.","id":"730229d3412a63a74b60d95e50c96fc6569a953f","venue_1":"ACM Trans. Graph.","year":"2005","title":"Wavelet noise","authors":"Robert L. Cook, Tony DeRose","author_ids":"3127802, 1792251","abstract":"Noise functions are an essential building block for writing procedural shaders in 3D computer graphics. The original noise function introduced by Ken Perlin is still the most popular because it is simple and fast, and many spectacular images have been made with it. Nevertheless, it is prone to problems with aliasing and detail loss. In this paper we analyze these problems and show that they are particularly severe when 3D noise is used to texture a 2D surface. We use the theory of wavelets to create a new class of simple and fast noise functions that avoid these problems.","cites":"30","conferencePercentile":"13.70967742"},{"venue":"ACM Trans. Graph.","id":"ca8739de82b3cfcd81468f8bf50333dfed0894b9","venue_1":"ACM Trans. Graph.","year":"2014","title":"Automatic acquisition of high-fidelity facial performances using monocular videos","authors":"Fuhao Shi, Hsiang-Tao Wu, Xin Tong, Jinxiang Chai","author_ids":"2298089, 1775992, 1743927, 1759700","abstract":"This paper presents a facial performance capture system that automatically captures high-fidelity facial performances using uncontrolled monocular videos (<i>e.g</i>., Internet videos). We start the process by detecting and tracking important facial features such as the nose tip and mouth corners across the entire sequence and then use the detected facial features along with multilinear facial models to reconstruct 3D head poses and large-scale facial deformation of the subject at each frame. We utilize per-pixel shading cues to add fine-scale surface details such as emerging or disappearing wrinkles and folds into large-scale facial deformation. At a final step, we iterate our reconstruction procedure on large-scale facial geometry and fine-scale facial details to further improve the accuracy of facial reconstruction. We have tested our system on monocular videos downloaded from the Internet, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant shape differences across individuals. We show our system advances the state of the art in facial performance capture by comparing against alternative methods.","cites":"26","conferencePercentile":"95.0617284"},{"venue":"ACM Trans. Graph.","id":"004834230bbd43881e510a1ad83fc0a6f10b908b","venue_1":"ACM Trans. Graph.","year":"1997","title":"Multiresolution Analysis for Surfaces of Arbitrary Topological Type","authors":"Michael Lounsbery, Tony DeRose, Joe D. Warren","author_ids":"2013255, 1792251, 1754322","abstract":"Multiresolution analysis and wavelets provide useful and efficient tools for representing functions at multiple levels of detail. Wavelet representations have been used in a broad range of applications, including image compression, physical simulation, and numerical analysis. In this article, we present a new class of wavelets, based on subdivision surfaces, that radically extends the class of representable functions. Whereas previous two-dimensional methods were restricted to functions difined on <bold>R</bold><supscrpt>2</supscrpt>, the subdivision wavelets developed here may be applied to functions defined on compact surfaces of arbitrary topological type. We envision many applications of this work, including continuous level-of-detail control for graphics rendering, compression of geometric models, and acceleration of global illumination algorithms. Level-of-detail control for spherical domains is illustrated using two examples: shape approximation of a polyhedral model, and color approximation of global terrain data.","cites":"412","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"27c80580f22dce2eb64bb6112a46bd7e0668975b","venue_1":"ACM Trans. Graph.","year":"2013","title":"Globally optimal direction fields","authors":"Felix Knöppel, Keenan Crane, Ulrich Pinkall, Peter Schröder","author_ids":"2226189, 3284915, 1781946, 1775659","abstract":"We present a method for constructing smooth <i>n</i>-direction fields (line fields, cross fields, <i>etc</i>.) on surfaces that is an order of magnitude faster than state-of-the-art methods, while still producing fields of equal or better quality. Fields produced by the method are globally optimal in the sense that they minimize a simple, well-defined quadratic smoothness energy over all possible configurations of singularities (number, location, and index). The method is fully automatic and can optionally produce fields aligned with a given guidance field such as principal curvature directions. Computationally the smoothest field is found via a sparse eigenvalue problem involving a matrix similar to the cotan-Laplacian. When a guidance field is present, finding the optimal field amounts to solving a single linear system.","cites":"31","conferencePercentile":"86.65158371"},{"venue":"ACM Trans. Graph.","id":"18453b83d213dc863c94d77cce24afc822c0734b","venue_1":"ACM Trans. Graph.","year":"2014","title":"Dynamic hair capture using spacetime optimization","authors":"Zexiang Xu, Hsiang-Tao Wu, Lvdi Wang, Changxi Zheng, Xin Tong, Yue Qi","author_ids":"2615346, 1775992, 2903269, 1797875, 1743927, 6821737","abstract":"Dynamic hair strands have complex structures and experience intricate collisions and occlusion, posing significant challenges for high-quality reconstruction of their motions. We present a comprehensive dynamic hair capture system for reconstructing realistic hair motions from multiple synchronized video sequences. To recover hair strands' temporal correspondence, we propose a motion-path analysis algorithm that can robustly track local hair motions in input videos. To ensure the spatial and temporal coherence of the dynamic capture, we formulate the global hair reconstruction as a spacetime optimization problem solved iteratively. Demonstrated using a range of real-world hairstyles driven by different wind conditions and head motions, our approach is able to reconstruct complex hair dynamics matching closely with video recordings both in terms of geometry and motion details.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"73bff8ab4421af7e4dba6645f0587a47d45744a5","venue_1":"ACM Trans. Graph.","year":"2008","title":"Shading-based surface editing","authors":"Yotam I. Gingold, Denis Zorin","author_ids":"1697606, 1798055","abstract":"We present a system for free-form surface modeling that allows a user to modify a shape by changing its rendered, shaded image using stroke-based drawing tools. User input is translated into a set of tangent and positional constraints on the surface. A new shape, whose rendered image closely approximates user input, is computed using an efficient and stable surface optimization procedure. We demonstrate how several types of free-form surface edits which may be difficult to cast in terms of standard deformation approaches can be easily performed using our system.","cites":"16","conferencePercentile":"12.65432099"},{"venue":"ACM Trans. Graph.","id":"b0dfd52520abe882b1126c740f0fd8f65454d9b3","venue_1":"ACM Trans. Graph.","year":"2014","title":"Feature Matching with Bounded Distortion","authors":"Yaron Lipman, Stav Yagev, Roi Poranne, David W. Jacobs, Ronen Basri","author_ids":"3232072, 2072457, 1721193, 1771485, 1760994","abstract":"We consider the problem of finding a geometrically consistent set of point matches between two images. We assume that local descriptors have provided a set of candidate matches, which may include many outliers. We then seek the largest subset of these correspondences that can be aligned perfectly using a nonrigid deformation that exerts a bounded distortion. We formulate this as a constrained optimization problem and solve it using a constrained, iterative reweighted least-squares algorithm. In each iteration of this algorithm we solve a convex quadratic program obtaining a globally optimal match over a subset of the bounded distortion transformations. We further prove that a sequence of such iterations converges monotonically to a critical point of our objective function. We show experimentally that this algorithm produces excellent results on a number of test sets, in comparison to several state-of-the-art approaches.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"3d3eb34d3b43d078c6db910c75de3ae134949ef2","venue_1":"ACM Trans. Graph.","year":"2005","title":"Mesh saliency","authors":"Chang Ha Lee, Amitabh Varshney, David W. Jacobs","author_ids":"7496720, 1804783, 1771485","abstract":"Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of <i>mesh saliency</i> as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes. compared to using a purely geometric measure of shape. such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.","cites":"162","conferencePercentile":"82.66129032"},{"venue":"ACM Trans. Graph.","id":"6fef08f6905bc5fd28fb0a86ddaa74962d25d464","venue_1":"ACM Trans. Graph.","year":"2007","title":"Harmonic coordinates for character articulation","authors":"Pushkar Joshi, Mark Meyer, Tony DeRose, Brian Green, Tom Sanocki","author_ids":"1725489, 8735841, 1792251, 7162915, 2130760","abstract":"In this paper we consider the problem of creating and controlling volume deformations used to articulate characters for use in high-end applications such as computer generated feature films. We introduce a method we call harmonic coordinates that significantly improves upon existing volume deformation techniques. Our deformations are controlled using a topologically flexible structure, called a cage, that consists of a closed three dimensional mesh. The cage can optionally be augmented with additional interior vertices, edges, and faces to more precisely control the interior behavior of the deformation. We show that harmonic coordinates are generalized barycentric coordinates that can be extended to any dimension. Moreover, they are the first system of generalized barycentric coordinates that are non-negative even in strongly concave situations, and their magnitude falls off with distance as measured within the cage.","cites":"177","conferencePercentile":"93.2"},{"venue":"ACM Trans. Graph.","id":"03778e148ea28b8778c7136cb765d72f5fded3a1","venue_1":"ACM Trans. Graph.","year":"2014","title":"Appearance-from-motion: recovering spatially varying surface reflectance under unknown lighting","authors":"Yue Dong, Guojun Chen, Pieter Peers, Jiawan Zhang, Xin Tong","author_ids":"1744268, 8018112, 1808270, 8214269, 1743927","abstract":"We present \"appearance-from-motion\", a novel method for recovering the spatially varying isotropic surface reflectance from a video of a rotating subject, with known geometry, under unknown natural illumination. We formulate the appearance recovery as an iterative process that alternates between estimating surface reflectance and estimating incident lighting. We characterize the surface reflectance by a data-driven microfacet model, and recover the microfacet normal distribution for each surface point separately from temporal changes in the observed radiance. To regularize the recovery of the incident lighting, we rely on the observation that natural lighting is sparse in the gradient domain. Furthermore, we exploit the sparsity of strong edges in the incident lighting to improve the robustness of the surface reflectance estimation. We demonstrate robust recovery of spatially varying isotropic reflectance from captured video as well as an internet video sequence for a wide variety of materials and natural lighting conditions.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"55fb449647e8ed210bf6b4793d22056ac1b23cfa","venue_1":"ACM Trans. Graph.","year":"2011","title":"Spin transformations of discrete surfaces","authors":"Keenan Crane, Ulrich Pinkall, Peter Schröder","author_ids":"3284915, 1781946, 1775659","abstract":"We introduce a new method for computing conformal transformations of triangle meshes in R<sup>3</sup>. Conformal maps are desirable in digital geometry processing because they do not exhibit <i>shear</i>, and therefore preserve texture fidelity as well as the quality of the mesh itself. Traditional discretizations consider maps into the complex plane, which are useful only for problems such as surface parameterization and planar shape deformation where the target surface is flat. We instead consider maps into the <i>quaternions</i> H, which allows us to work directly with surfaces sitting in R<sup>3</sup>. In particular, we introduce a <i>quaternionic Dirac operator</i> and use it to develop a novel integrability condition on conformal deformations. Our discretization of this condition results in a sparse linear system that is simple to build and can be used to efficiently edit surfaces by manipulating curvature and boundary data, as demonstrated via several mesh processing applications.","cites":"21","conferencePercentile":"46.05263158"},{"venue":"ACM Trans. Graph.","id":"0645562bd12350ed8abd32d6d0d4a8c254a49281","venue_1":"ACM Trans. Graph.","year":"2010","title":"A simple geometric model for elastic deformations","authors":"Isaac Chao, Ulrich Pinkall, Patrick Sanan, Peter Schröder","author_ids":"1762662, 1781946, 1785538, 1775659","abstract":"We advocate a simple geometric model for elasticity: <i>distance between the differential of a deformation and the rotation group</i>. It comes with rigorous differential geometric underpinnings, both smooth and discrete, and is computationally almost as simple and efficient as linear elasticity. Owing to its geometric non-linearity, though, it does not suffer from the usual linearization artifacts. A material model with standard elastic moduli (Lam&#233; parameters) falls out naturally, and a minimizer for static problems is easily augmented to construct a fully variational 2<sup>nd</sup> order time integrator. It has excellent conservation properties even for very coarse simulations, making it very robust.\n Our analysis was motivated by a number of heuristic, physics-like algorithms from geometry processing (editing, morphing, parameterization, and simulation). Starting with a continuous energy formulation and taking the underlying geometry into account, we simplify and accelerate these algorithms while avoiding common pitfalls. Through the connection with the Biot strain of mechanics, the intuition of previous work that these ideas are \"like\" elasticity is shown to be spot on.","cites":"76","conferencePercentile":"94.73684211"},{"venue":"ACM Trans. Graph.","id":"7890f1f60b2dbf79f06073dd99ea0cb0919a28fd","venue_1":"ACM Trans. Graph.","year":"2015","title":"Sampling based scene-space video processing","authors":"Felix Klose, Oliver Wang, Jean Charles Bazin, Marcus A. Magnor, Alexander Sorkine-Hornung","author_ids":"2547248, 1958703, 1745931, 1686739, 2893744","abstract":"Many compelling video processing effects can be achieved if per-pixel depth information and 3D camera calibrations are known. However, the success of such methods is highly dependent on the accuracy of this \"scene-space\" information. We present a novel, sampling-based framework for processing video that enables high-quality scene-space video effects in the presence of inevitable errors in depth and camera pose estimation. Instead of trying to improve the explicit 3D scene representation, the key idea of our method is to exploit the high redundancy of approximate scene information that arises due to most scene points being visible multiple times across many frames of video. Based on this observation, we propose a novel pixel gathering and filtering approach. The gathering step is general and collects pixel samples in scene-space, while the filtering step is application-specific and computes a desired output video from the gathered sample sets. Our approach is easily parallelizable and has been implemented on GPU, allowing us to take full advantage of large volumes of video data and facilitating practical runtimes on HD video using a standard desktop computer. Our generic scene-space formulation is able to comprehensively describe a multitude of video processing applications such as denoising, deblurring, super resolution, object removal, computational shutter functions, and other scene-space camera effects. We present results for various casually captured, hand-held, moving, compressed, monocular videos depicting challenging scenes recorded in uncontrolled environments.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"65dd819bd6578ee1b3f903ffbc79afcf3833d50f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Garment Replacement in Monocular Video Sequences","authors":"Lorenz Rogge, Felix Klose, Michael Stengel, Martin Eisemann, Marcus A. Magnor","author_ids":"2079851, 2547248, 2267017, 1701306, 1686739","abstract":"We present a semi-automatic approach to exchange the clothes of an actor for arbitrary virtual garments in conventional monocular video footage as a postprocess. We reconstruct the actor's body shape and motion from the input video using a parameterized body model. The reconstructed dynamic 3D geometry of the actor serves as an animated mannequin for simulating the virtual garment. It also aids in scene illumination estimation, necessary to realistically light the virtual garment. An image-based warping technique ensures realistic compositing of the rendered virtual garment and the original video. We present results for eight real-world video sequences featuring complex test cases to evaluate performance for different types of motion, camera settings, and illumination conditions.","cites":"0","conferencePercentile":"2.057613169"},{"venue":"ACM Trans. Graph.","id":"1b549c04a50def19e5a503fd329b9b3b8216ec0e","venue_1":"ACM Trans. Graph.","year":"2013","title":"Sparse localized deformation components","authors":"Thomas Neumann, Kiran Varanasi, Stephan Wenger, Markus Wacker, Marcus A. Magnor, Christian Theobalt","author_ids":"3268200, 1715245, 1833738, 2283945, 1686739, 1680185","abstract":"We propose a method that extracts sparse and spatially localized deformation modes from an animated mesh sequence. To this end, we propose a new way to extend the theory of sparse matrix decompositions to 3D mesh sequence processing, and further contribute with an automatic way to ensure spatial locality of the decomposition in a new optimization framework. The extracted dimensions often have an intuitive and clear interpretable meaning. Our method optionally accepts user-constraints to guide the process of discovering the underlying latent deformation space. The capabilities of our efficient, versatile, and easy-to-implement method are extensively demonstrated on a variety of data sets and application contexts. We demonstrate its power for user friendly intuitive editing of captured mesh animations, such as faces, full body motion, cloth animations, and muscle deformations. We further show its benefit for statistical geometry processing and biomechanically meaningful animation editing. It is further shown qualitatively and quantitatively that our method outperforms other unsupervised decomposition methods and other animation parameterization approaches in the above use cases.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"0b285502cd2af0738c8f137701116360016b6b5e","venue_1":"ACM Trans. Graph.","year":"2008","title":"Time-resolved 3d capture of non-stationary gas flows","authors":"Bradley Atcheson, Ivo Ihrke, Wolfgang Heidrich, Art Tevs, Derek Bradley, Marcus A. Magnor, Hans-Peter Seidel","author_ids":"2353092, 1749103, 1752192, 3320218, 1745149, 1686739, 1746884","abstract":"Fluid simulation is one of the most active research areas in computer graphics. However, it remains difficult to obtain measurements of real fluid flows for validation of the simulated data.\n In this paper, we take a step in the direction of capturing flow data for such purposes. Specifically, we present the first time-resolved Schlieren tomography system for capturing full 3D, non-stationary gas flows on a dense volumetric grid. Schlieren tomography uses 2D ray deflection measurements to reconstruct a time-varying grid of 3D refractive index values, which directly correspond to physical properties of the flow. We derive a new solution for this reconstruction problem that lends itself to efficient algorithms that robustly work with relatively small numbers of cameras. Our physical system is easy to set up, and consists of an array of relatively low cost rolling-shutter camcorders that are synchronized with a new approach. We demonstrate our method with real measurements, and analyze precision with synthetic data for which ground truth information is available.","cites":"59","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"30226489c632528605a24cfd5b2d73dcc2a836bb","venue_1":"ACM Trans. Graph.","year":"2007","title":"Eikonal rendering: efficient light transport in refractive objects","authors":"Ivo Ihrke, Gernot Ziegler, Art Tevs, Christian Theobalt, Marcus A. Magnor, Hans-Peter Seidel","author_ids":"1749103, 1824589, 3320218, 1680185, 1686739, 1746884","abstract":"We present a new method for real-time rendering of sophisticated lighting effects in and around refractive objects. It enables us to realistically display refractive objects with complex material properties, such as arbitrarily varying refractive index, inhomogeneous attenuation, as well as spatially-varying anisotropic scattering and reflectance properties. User-controlled changes of lighting positions only require a few seconds of update time. Our method is based on a set of ordinary differential equations derived from the eikonal equation, the main postulate of geometric optics. This set of equations allows for fast casting of bent light rays with the complexity of a particle tracer. Based on this concept, we also propose an efficient light propagation technique using adaptive wavefront tracing. Efficient GPU implementations for our algorithmic concepts enable us to render a combination of visual effects that were previously not reproducible in real-time.","cites":"47","conferencePercentile":"46.8"},{"venue":"ACM Trans. Graph.","id":"49eaab260c8dfbc33d572706576f2fcf276f4267","venue_1":"ACM Trans. Graph.","year":"2005","title":"Physically-based simulation of twilight phenomena","authors":"Jörg Haber, Marcus A. Magnor, Hans-Peter Seidel","author_ids":"1722980, 1686739, 1746884","abstract":"We present a physically-based approach to compute the colors of the sky during the twilight period before sunrise and after sunset. The simulation is based on the theory of light scattering by small particles. A realistic atmosphere model is assumed, consisting of air molecules, aerosols, and water. Air density, aerosols, and relative humidity vary with altitude. In addition, the aerosol component varies in composition and particle-size distribution. This allows us to realistically simulate twilight phenomena for a wide range of different climate conditions. Besides considering multiple Rayleigh and Mie scattering, we take into account wavelength-dependent refraction of direct sunlight as well as the shadow of the Earth. Incorporating several optimizations into the radiative transfer simulation, a photo-realistic hemispherical twilight sky is computed in less than two hours on a conventional PC. The resulting radiometric data is useful, for instance, for high-dynamic range environment mapping, outdoor global illumination calculations, mesopic vision research and optical aerosol load probing.","cites":"7","conferencePercentile":"2.016129032"},{"venue":"ACM Trans. Graph.","id":"6efd3abb2044c90e78dcde7d4e8f0b696c1a931d","venue_1":"ACM Trans. Graph.","year":"2013","title":"Robust fairing via conformal curvature flow","authors":"Keenan Crane, Ulrich Pinkall, Peter Schröder","author_ids":"3284915, 1781946, 1775659","abstract":"We present a formulation of Willmore flow for triangulated surfaces that permits extraordinarily large time steps and naturally preserves the quality of the input mesh. The main insight is that Willmore flow becomes remarkably stable when expressed in curvature space -- we develop the precise conditions under which curvature is allowed to evolve. The practical outcome is a highly efficient algorithm that naturally preserves texture and does not require remeshing during the flow. We apply this algorithm to surface fairing, geometric modeling, and construction of constant mean curvature (CMC) surfaces. We also present a new algorithm for length-preserving flow on planar curves, which provides a valuable analogy for the surface case.","cites":"16","conferencePercentile":"64.02714932"},{"venue":"ACM Trans. Graph.","id":"93d017c74b727d94a3f1673c359b366bfc8b66e6","venue_1":"ACM Trans. Graph.","year":"2014","title":"Smoke rings from smoke","authors":"Steffen Weißmann, Ulrich Pinkall, Peter Schröder","author_ids":"1787516, 1781946, 1775659","abstract":"We give an algorithm which extracts vortex filaments (\"smoke rings\") from a given 3D velocity field. Given a filament strength <i>h</i> &gt; 0, an optimal number of vortex filaments, together with their extent and placement, is given by the zero set of a complex valued function over the domain. This function is the global minimizer of a quadratic energy based on a Schr&#246;dinger operator. Computationally this amounts to finding the eigenvector belonging to the smallest eigenvalue of a Laplacian type sparse matrix.\n Turning traditional vector field representations of flows, for example, on a regular grid, into a corresponding set of vortex filaments is useful for visualization, analysis of measured flows, hybrid simulation methods, and sparse representations. To demonstrate our method we give examples from each of these.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"03111d14ae4b7f9a7018afc26622b9237f972117","venue_1":"ACM Trans. Graph.","year":"2009","title":"Structured annotations for 2D-to-3D modeling","authors":"Yotam I. Gingold, Takeo Igarashi, Denis Zorin","author_ids":"1697606, 1717356, 1798055","abstract":"We present a system for 3D modeling of free-form surfaces from 2D sketches. Our system frees users to create 2D sketches from arbitrary angles using their preferred tool, which may include pencil and paper. A 3D model is created by placing primitives and annotations on the 2D image. Our primitives are based on commonly used sketching conventions and allow users to maintain a single view of the model. This eliminates the frequent view changes inherent to existing 3D modeling tools, both traditional and sketch-based, and enables users to match input to the 2D guide image. Our annotations---same-lengths and angles, alignment, mirror symmetry, and connection curves---allow the user to communicate higher-level semantic information; through them our system builds a consistent model even in cases where the original image is inconsistent. We present the results of a user study comparing our approach to a conventional \"sketch-rotate-sketch\" workflow.","cites":"41","conferencePercentile":"61.60220994"},{"venue":"ACM Trans. Graph.","id":"1323eb4aba0c774d8e145a2532ea78ab13c26841","venue_1":"ACM Trans. Graph.","year":"2012","title":"RigMesh: automatic rigging for part-based shape modeling and deformation","authors":"Péter Borosán, Ming Jin, Douglas DeCarlo, Yotam I. Gingold, Andrew Nealen","author_ids":"3109479, 2935038, 2476753, 1697606, 1729677","abstract":"The creation of a 3D model is only the first stage of the 3D character animation pipeline. Once a model has been created, and before it can be animated, it must be <i>rigged</i>. Manual rigging is laborious, and automatic rigging approaches are far from real-time and do not allow for incremental updates. This is a hindrance in the real world, where the shape of a model is often revised after rigging has been performed. In this paper, we introduce algorithms and a user-interface for sketch-based 3D modeling that unify the modeling and rigging stages of the 3D character animation pipeline. Our algorithms create a rig for each sketched part in real-time, and update the rig as parts are merged or cut. As a result, users can freely pose and animate their shapes and characters while rapidly iterating on the base shape. The rigs are compatible with the state-of-the-art character animation pipeline; they consist of a low-dimensional skeleton along with skin weights identifying the surface with bones of the skeleton.","cites":"16","conferencePercentile":"46.46464646"},{"venue":"ACM Trans. Graph.","id":"13c9c8890962b643cc0890b08cba213ff844845b","venue_1":"ACM Trans. Graph.","year":"2011","title":"Leveraging motion capture and 3D scanning for high-fidelity facial performance acquisition","authors":"Hao-Da Huang, Jinxiang Chai, Xin Tong, Hsiang-Tao Wu","author_ids":"1777480, 1759700, 1743927, 1775992","abstract":"This paper introduces a new approach for acquiring high-fidelity 3D facial performances with realistic dynamic wrinkles and fine-scale facial details. Our approach leverages state-of-the-art motion capture technology and advanced 3D scanning technology for facial performance acquisition. We start the process by recording 3D facial performances of an actor using a marker-based motion capture system and perform facial analysis on the captured data, thereby determining a minimal set of face scans required for accurate facial reconstruction. We introduce a two-step registration process to efficiently build dense consistent surface correspondences across all the face scans. We reconstruct high-fidelity 3D facial performances by combining motion capture data with the minimal set of face scans in the blendshape interpolation framework. We have evaluated the performance of our system on both real and synthetic data. Our results show that the system can capture facial performances that match both the spatial resolution of static face scans and the acquisition speed of motion capture systems.","cites":"36","conferencePercentile":"74.21052632"},{"venue":"ACM Trans. Graph.","id":"325910da6d5540f3b90149572c79a3b8ac2c40e9","venue_1":"ACM Trans. Graph.","year":"2011","title":"AppGen: interactive material modeling from a single image","authors":"Yue Dong, Xin Tong, Fabio Pellacini, Baining Guo","author_ids":"1744268, 1743927, 1757883, 2738456","abstract":"We present <i>AppGen</i>, an interactive system for modeling materials from a single image. Given a texture image of a nearly planar surface lit with directional lighting, our system models the detailed spatially-varying reflectance properties (diffuse, specular and roughness) and surface normal variations with minimal user interaction. We ask users to indicate global shading and reflectance information by roughly marking the image with a few user strokes, while our system assigns reflectance properties and normals to each pixel. We first interactively decompose the input image into the product of a diffuse albedo map and a shading map. A two-scale normal reconstruction algorithm is then introduced to recover the normal variations from the shading map and preserve the geometric features at different scales. We finally assign the specular parameters to each pixel guided by user strokes and the diffuse albedo. Our system generates convincing results within minutes of interaction and works well for a variety of material types that exhibit different reflectance and normal variations, including natural surfaces and man-made ones.","cites":"13","conferencePercentile":"30.26315789"},{"venue":"ACM Trans. Graph.","id":"4fb8b7e952ff1434fe3e86b5dea92c863434c894","venue_1":"ACM Trans. Graph.","year":"2011","title":"Temporal light field reconstruction for rendering distribution effects","authors":"Jaakko Lehtinen, Timo Aila, Jiawen Chen, Samuli Laine, Frédo Durand","author_ids":"1780788, 1761103, 1967685, 2365390, 1728125","abstract":"Traditionally, effects that require evaluating multidimensional integrals for each pixel, such as motion blur, depth of field, and soft shadows, suffer from noise due to the variance of the high-dimensional integrand. In this paper, we describe a general reconstruction technique that exploits the anisotropy in the temporal light field and permits efficient reuse of samples between pixels, multiplying the effective sampling rate by a large factor. We show that our technique can be applied in situations that are challenging or impossible for previous anisotropic reconstruction methods, and that it can yield good results with very sparse inputs. We demonstrate our method for simultaneous motion blur, depth of field, and soft shadows.","cites":"50","conferencePercentile":"85.26315789"},{"venue":"ACM Trans. Graph.","id":"d291591afede09b1049be57e0d1e394017c3aecb","venue_1":"ACM Trans. Graph.","year":"2011","title":"Decoupled sampling for graphics pipelines","authors":"Jonathan Ragan-Kelley, Jaakko Lehtinen, Jiawen Chen, Michael C. Doggett, Frédo Durand","author_ids":"2488277, 1780788, 1967685, 2551074, 1728125","abstract":"We propose a generalized approach to decoupling shading from visibility sampling in graphics pipelines, which we call <i>decoupled sampling</i>. Decoupled sampling enables stochastic supersampling of motion and defocus blur at reduced shading cost, as well as controllable or adaptive shading rates which trade off shading quality for performance. It can be thought of as a generalization of multisample antialiasing (MSAA) to support complex and dynamic mappings from visibility to shading samples, as introduced by motion and defocus blur and adaptive shading. It works by defining a many-to-one hash from visibility to shading samples, and using a buffer to memoize shading samples and exploit reuse across visibility samples. Decoupled sampling is inspired by the Reyes rendering architecture, but like traditional graphics pipelines, it shades <i>fragments</i> rather than micropolygon vertices, decoupling shading from the geometry sampling rate. Also unlike Reyes, decoupled sampling only shades fragments after precise computation of visibility, reducing overshading.\n We present extensions of two modern graphics pipelines to support decoupled sampling: a GPU-style sort-last fragment architecture, and a Larrabee-style sort-middle pipeline. We study the architectural implications of decoupled sampling and blur, and derive end-to-end performance estimates on real applications through an instrumented functional simulator. We demonstrate high-quality motion and defocus blur, as well as variable and adaptive shading rates.","cites":"35","conferencePercentile":"72.10526316"},{"venue":"ACM Trans. Graph.","id":"829fb5a8c6e1c7435b85a4aa2671cf483fac272d","venue_1":"ACM Trans. Graph.","year":"2015","title":"Efficient construction and simplification of Delaunay meshes","authors":"Yong-Jin Liu, Chunxu Xu, Dian Fan, Ying He","author_ids":"1715826, 2250021, 3244211, 1734129","abstract":"Delaunay meshes (DM) are a special type of triangle mesh where the local Delaunay condition holds everywhere. We present an efficient algorithm to convert an arbitrary manifold triangle mesh <i>M</i> into a Delaunay mesh. We show that the constructed DM has <i>O</i>(<i>Kn</i>) vertices, where <i>n</i> is the number of vertices in <i>M</i> and <i>K</i> is a model-dependent constant. We also develop a novel algorithm to simplify Delaunay meshes, allowing a smooth choice of detail levels. Our methods are conceptually simple, theoretically sound and easy to implement. The DM construction algorithm also scales well due to its <i>O</i>(<i>nK</i> log <i>K</i>) time complexity.\n Delaunay meshes have many favorable geometric and numerical properties. For example, a DM has exactly the same geometry as the input mesh, and it can be encoded by any mesh data structure. Moreover, the empty geodesic circumcircle property implies that the commonly used cotangent Laplace-Beltrami operator has non-negative weights. Therefore, the existing digital geometry processing algorithms can benefit the numerical stability of DM without changing any codes. We observe that DMs can improve the accuracy of the heat method for computing geodesic distances. Also, popular parameterization techniques, such as discrete harmonic mapping, produce more stable results on the DMs than on the input meshes.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"29136b1cceebe63540f72f7e6aa355e816b1f496","venue_1":"ACM Trans. Graph.","year":"2015","title":"Intuitive and efficient camera control with the toric space","authors":"Christophe Lino, Marc Christie","author_ids":"2869929, 1701717","abstract":"A large range of computer graphics applications such as data visualization or virtual movie production require users to position and move viewpoints in 3D scenes to effectively convey visual information or tell stories. The desired viewpoints and camera paths are required to satisfy a number of visual properties (<i>e.g.</i> size, vantage angle, visibility, and on-screen position of targets). Yet, existing camera manipulation tools only provide limited interaction methods and automated techniques remain computationally expensive.\n In this work, we introduce the <i>Toric space</i>, a novel and compact representation for intuitive and efficient virtual camera control. We first show how visual properties are expressed in this Toric space and propose an efficient interval-based search technique for automated viewpoint computation. We then derive a novel screen-space manipulation technique that provides intuitive and real-time control of visual properties. Finally, we propose an effective viewpoint interpolation technique which ensures the continuity of visual properties along the generated paths. The proposed approach (i) performs better than existing automated viewpoint computation techniques in terms of speed and precision, (ii) provides a screen-space manipulation tool that is more efficient than classical manipulators and easier to use for beginners, and (iii) enables the creation of complex camera motions such as long takes in a very short time and in a controllable way. As a result, the approach should quickly find its place in a number of applications that require interactive or automated camera control such as 3D modelers, navigation tools or 3D games.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"174ab69c116eea117252d3597298c7025f044e80","venue_1":"ACM Trans. Graph.","year":"2014","title":"Parallel chen-han (PCH) algorithm for discrete geodesics","authors":"Xiang Ying, Shi-Qing Xin, Ying He","author_ids":"7255947, 2012376, 1734129","abstract":"In many graphics applications, the computation of exact geodesic distance is very important. However, the high computational cost of existing geodesic algorithms means that they are not practical for large-scale models or time-critical applications. To tackle this challenge, we propose the Parallel Chen-Han (or PCH) algorithm, which extends the classic Chen-Han (CH) discrete geodesic algorithm to the parallel setting. The original CH algorithm and its variant both lack a parallel solution because the windows (a key data structure that carries the shortest distance in the wavefront propagation) are maintained in a strict order or a tightly coupled manner, which means that only one window is processed at a time. We propose dividing the CH's sequential algorithm into four phases, window selection, window propagation, data organization, and events processing so that there is no data dependence or conflicts in each phase and the operations within each phase can be carried out in parallel. The proposed PCH algorithm is able to propagate a large number of windows simultaneously and independently. We also adopt a simple yet effective strategy to control the total number of windows. We implement the PCH algorithm on modern GPUs (such as Nvidia GTX 580) and analyze the performance in detail. The performance improvement (compared to the sequential algorithms) is highly consistent with GPU double-precision performance (GFLOPS). Extensive experiments on real-world models demonstrate an order of magnitude improvement in execution time compared to the state-of-the-art.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"fad5c33fc93bbdc35520980e471c6b221fed97ab","venue_1":"ACM Trans. Graph.","year":"2013","title":"Saddle vertex graph (SVG): a novel solution to the discrete geodesic problem","authors":"Xiang Ying, Xiaoning Wang, Ying He","author_ids":"7255947, 6052854, 1734129","abstract":"This paper presents the Saddle Vertex Graph (SVG), a novel solution to the discrete geodesic problem. The SVG is a sparse undirected graph that encodes complete geodesic distance information: a geodesic path on the mesh is equivalent to a shortest path on the SVG, which can be solved efficiently using the shortest path algorithm (e.g., Dijkstra algorithm). The SVG method solves the discrete geodesic problem from a local perspective. We have observed that the polyhedral surface has some interesting and unique properties, such as the fact that the discrete geodesic exhibits a strong local structure, which is not available on the smooth surfaces. The richer the details and complicated geometry of the mesh, the stronger such local structure will be. Taking advantage of the local nature, the SVG algorithm breaks down the discrete geodesic problem into significantly smaller sub-problems, and elegantly enables information reuse. It does not require any numerical solver, and is numerically stable and insensitive to the mesh resolution and tessellation. Users can intuitively specify a model-independent parameter <i>K</i>, which effectively balances the SVG complexity and the accuracy of the computed geodesic distance. More importantly, the computed distance is guaranteed to be a metric. The experimental results on real-world models demonstrate significant improvement to the existing approximate geodesic methods in terms of both performance and accuracy.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"19678273bae2a0006b43228bbb9d8a57248e1b5a","venue_1":"ACM Trans. Graph.","year":"2015","title":"Learning to Remove Soft Shadows","authors":"Maciej Gryka, Michael Terry, Gabriel J. Brostow","author_ids":"2871336, 2893550, 3309893","abstract":"Manipulated images lose believability if the user's edits fail to account for shadows. We propose a method that makes removal and editing of soft shadows easy. Soft shadows are ubiquitous, but remain notoriously difficult to extract and manipulate. We posit that soft shadows can be segmented, and therefore edited, by learning a mapping function for image patches that generates shadow mattes. We validate this premise by removing soft shadows from photographs with only a small amount of user input.\n Given only broad user brush strokes that indicate the region to be processed, our new supervised regression algorithm automatically unshadows an image, removing the umbra and penumbra. The resulting lit image is frequently perceived as a believable shadow-free version of the scene. We tested the approach on a large set of soft shadow images, and performed a user study that compared our method to the state-of-the-art and to real lit scenes. Our results are more difficult to identify as being altered and are perceived as preferable compared to prior work.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"9eb75541ff84a99e271836c217103dda93c999a9","venue_1":"ACM Trans. Graph.","year":"2011","title":"Making burr puzzles from 3D models","authors":"Shi-Qing Xin, Chi-Fu Lai, Chi-Wing Fu, Tien-Tsin Wong, Ying He, Daniel Cohen-Or","author_ids":"2012376, 1897738, 1699457, 1720633, 1734129, 1701009","abstract":"A 3D burr puzzle is a 3D model that consists of interlocking pieces with a single-key property. That is, when the puzzle is assembled, all the pieces are notched except one single key component which remains mobile. The intriguing property of the assembled burr puzzle is that it is stable, perfectly interlocked, without glue or screws, etc. Moreover, a burr puzzle consisting of a small number of pieces is still rather difficult to solve since the assembly must follow certain orders while the combinatorial complexity of the puzzle's piece arrangements is extremely high.\n In this paper, we generalize the 6-piece orthogonal burr puzzle (a knot) to design and model burr puzzles from 3D models. Given a 3D input model, we first interactively embed a network of knots into the 3D shape. Our method automatically optimizes and arranges the orientation of each knot, and modifies pieces of adjacent knots with an appropriate connection type. Then, following the geometry of the embedded pieces, the entire 3D model is partitioned by splitting the solid while respecting the assembly motion of embedded pieces. The main technical challenge is to enforce the single-key property and ensure the assembly/disassembly remains feasible, as the puzzle pieces in a network of knots are highly interlocked. Lastly, we also present an automated approach to generate the visualizations of the puzzle assembly process.","cites":"28","conferencePercentile":"61.05263158"},{"venue":"ACM Trans. Graph.","id":"aa0125471100bd50345db8e01a7227860873bf62","venue_1":"ACM Trans. Graph.","year":"2010","title":"K-set tilable surfaces","authors":"Chi-Wing Fu, Chi-Fu Lai, Ying He, Daniel Cohen-Or","author_ids":"1699457, 1897738, 1734129, 1701009","abstract":"This paper introduces a method for optimizing the tiles of a quad-mesh. Given a quad-based surface, the goal is to generate a set of <i>K</i> quads whose instances can produce a tiled surface that approximates the input surface. A solution to the problem is a K-set tilable surface, which can lead to an effective cost reduction in the physical construction of the given surface. Rather than molding lots of different building blocks, a K-set tilable surface requires the construction of <i>K</i> prefabricated components only. To realize the K-set tilable surface, we use a cluster-optimize approach. First, we iteratively cluster and analyze: clusters of similar shapes are merged, while edge connections between the <i>K</i> quads on the target surface are analyzed to learn the induced flexibility of the K-set tilable surface. Then, we apply a non-linear optimization model with constraints that maintain the <i>K</i> quads connections and shapes, and show how quad-based surfaces are optimized into K-set tilable surfaces. Our algorithm is demonstrated on various surfaces, including some that mimic the exteriors of certain renowned building landmarks.","cites":"19","conferencePercentile":"30.99415205"},{"venue":"ACM Trans. Graph.","id":"823621cf0af4cad0f280b9a2f1838c97a5a1462b","venue_1":"ACM Trans. Graph.","year":"2015","title":"AniMesh: interleaved animation, modeling, and editing","authors":"Ming Jin, Daniel Gopstein, Yotam I. Gingold, Andrew Nealen","author_ids":"2935038, 3416125, 1697606, 1729677","abstract":"We introduce AniMesh, a system that supports interleaved modeling and animation creation and editing. AniMesh is suitable for rapid prototyping and easily accessible to non-experts. Source animations can be obtained from commodity motion capture devices or by adapting canned motion sequences. We propose skeleton abstraction and motion retargeting algorithms for finding correspondences and transferring motion between skeletons, or portions of skeletons, with varied topology. Motion can be copied-and-pasted between kinematic chains with different skeletal topologies, and entire model parts can be cut and reattached, while always retaining plausible, composite animations.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"76d1573e3f56b34754a1c18460cfd0d375a1a5db","venue_1":"ACM Trans. Graph.","year":"2016","title":"Schrödinger's smoke","authors":"Albert Chern, Felix Knöppel, Ulrich Pinkall, Peter Schröder, Steffen Weißmann","author_ids":"2825600, 2226189, 1781946, 1775659, 1787516","abstract":"We describe a new approach for the purely Eulerian simulation of incompressible fluids. In it, the fluid state is represented by a C<sup>2</sup>-valued wave function evolving under the Schr&#246;dinger equation subject to incompressibility constraints. The underlying dynamical system is Hamiltonian and governed by the kinetic energy of the fluid together with an energy of Landau-Lifshitz type. The latter ensures that dynamics due to thin vortical structures, all important for visual simulation, are faithfully reproduced. This enables robust simulation of intricate phenomena such as vortical wakes and interacting vortex filaments, even on modestly sized grids. Our implementation uses a simple splitting method for time integration, employing the FFT for Schr&#246;dinger evolution as well as constraint projection. Using a standard penalty method we also allow arbitrary obstacles. The resulting algorithm is simple, unconditionally stable, and efficient. In particular it does not require any Lagrangian techniques for advection or to counteract the loss of vorticity. We demonstrate its use in a variety of scenarios, compare it with experiments, and evaluate it against benchmark tests. A full implementation is included in the ancillary materials.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"e7a735ab8af288189d2b7de283dd3bcb657f2048","venue_1":"ACM Trans. Graph.","year":"2015","title":"Stripe patterns on surfaces","authors":"Felix Knöppel, Keenan Crane, Ulrich Pinkall, Peter Schröder","author_ids":"2226189, 3284915, 1781946, 1775659","abstract":"<i>Stripe patterns</i> are ubiquitous in nature, describing macroscopic phenomena such as stripes on plants and animals, down to material impurities on the atomic scale. We propose a method for synthesizing stripe patterns on triangulated surfaces, where singularities are automatically inserted in order to achieve user-specified orientation and line spacing. Patterns are characterized as global minimizers of a convex-quadratic energy which is well-defined in the smooth setting. Computation amounts to finding the principal eigenvector of a symmetric positive-definite matrix with the same sparsity as the standard graph Laplacian. The resulting patterns are globally continuous, and can be applied to a variety of tasks in design and texture synthesis.","cites":"7","conferencePercentile":"81.2244898"},{"venue":"ACM Trans. Graph.","id":"18a46b617d21e7686e0ab7477eafed25fc2a0e58","venue_1":"ACM Trans. Graph.","year":"2011","title":"AppWarp: retargeting measured materials by appearance-space warping","authors":"Xiaobo An, Xin Tong, Jonathan D. Denning, Fabio Pellacini","author_ids":"2370616, 1743927, 2219227, 1757883","abstract":"We propose a method for retargeting measured materials, where a source measured material is edited by applying the reflectance functions of a template measured dataset. The resulting dataset is a material that maintains the spatial patterns of the source dataset, while exhibiting the reflectance behaviors of the template. Compared to editing materials by subsequent selections and modifications, retargeting shortens the time required to achieve a desired look by directly using template data, just as color transfer does for editing images. With our method, users have to just mark corresponding regions of source and template with rough strokes, with no need for further input.\n This paper introduces <i>AppWarp</i>, an algorithm that achieves retargeting as a user-constrained, appearance-space warping operation, that executes in tens of seconds. Our algorithm is independent of the measured material representation and supports retargeting of analytic and tabulated BRDFs as well as BSSRDFs. In addition, our method makes no assumption of the data distribution in appearance-space nor on the underlying correspondence between source and target. These characteristics make <i>AppWarp</i> the first general formulation for appearance retargeting. We validate our method on several types of materials, including leaves, metals, waxes, woods and greeting cards. Furthermore, we demonstrate how retargeting can be used to enhance diffuse texture with high quality reflectance.","cites":"3","conferencePercentile":"3.947368421"},{"venue":"ACM Trans. Graph.","id":"1142fdf6e05028eef8faef7249ad5f36a6f4efe4","venue_1":"ACM Trans. Graph.","year":"2012","title":"Printing spatially-varying reflectance for reproducing HDR images","authors":"Yue Dong, Xin Tong, Fabio Pellacini, Baining Guo","author_ids":"1744268, 1743927, 1757883, 2738456","abstract":"We present a solution for viewing high dynamic range (HDR) images with spatially-varying distributions of glossy materials printed on reflective media. Our method exploits appearance variations of the glossy materials in the angular domain to display the input HDR image at different exposures. As viewers change the print orientation or lighting directions, the print gradually varies its appearance to display the image content from the darkest to the brightest levels. Our solution is based on a commercially available printing system and is fully automatic. Given the input HDR image and the BRDFs of a set of available inks, our method computes the optimal exposures of the HDR image for all viewing conditions and the optimal ink combinations for all pixels by minimizing the difference of their appearances under all viewing conditions. We demonstrate the effectiveness of our method with print samples generated from different inputs and visualized under different viewing and lighting conditions.","cites":"3","conferencePercentile":"1.767676768"},{"venue":"ACM Trans. Graph.","id":"5943291fe1f67c061b849499fcb0c0d9eb81318e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Decomposing time-lapse paintings into layers","authors":"Jianchao Tan, Marek Dvoroznák, Daniel Sýkora, Yotam I. Gingold","author_ids":"1997964, 2052469, 7997286, 1697606","abstract":"The creation of a painting, in the physical world or digitally, is a process that occurs over time. Later strokes cover earlier strokes, and strokes painted at a similar time are likely to be part of the same object. In the final painting, this temporal history is lost, and a static arrangement of color is all that remains. The rich literature for interacting with image editing history cannot be used. To enable these interactions, we present a set of techniques to decompose a time lapse video of a painting (defined generally to include pencils, markers, etc.) into a sequence of translucent \"stroke\" images. We present translucency-maximizing solutions for recovering physical (Kubelka and Munk layering) or digital (Porter and Duff \"over\" blending operation) paint parameters from before/after image pairs. We also present a pipeline for processing real-world videos of paintings capable of handling long-term occlusions, such as the painter's hand and its shadow, color shifts, and noise.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"9ac44ae0261a3a5872be8c479ac137f165a4a3eb","venue_1":"ACM Trans. Graph.","year":"2015","title":"Close-to-conformal deformations of volumes","authors":"Albert Chern, Ulrich Pinkall, Peter Schröder","author_ids":"2825600, 1781946, 1775659","abstract":"Conformal deformations are infinitesimal scale-rotations, which can be parameterized by quaternions. The condition that such a quaternion field gives rise to a conformal deformation is nonlinear and in any case only admits M&#246;bius transformations as solutions. We propose a particular decoupling of scaling and rotation which allows us to find near to conformal deformations as minimizers of a quadratic, convex Dirichlet energy. Applied to tetrahedral meshes we find deformations with low quasiconformal distortion as the principal eigenvector of a (quaternionic) Laplace matrix. The resulting algorithms can be implemented with highly optimized standard linear algebra libraries and yield deformations comparable in quality to far more expensive approaches.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"0132920b4487fe31fc96d3f5054ebf9411c7c762","venue_1":"ACM Trans. Graph.","year":"2012","title":"Micro perceptual human computation for visual tasks","authors":"Yotam I. Gingold, Ariel Shamir, Daniel Cohen-Or","author_ids":"1697606, 2947946, 1701009","abstract":"Human Computation (HC) utilizes humans to solve problems or carry out tasks that are hard for pure computational algorithms. Many graphics and vision problems have such tasks. Previous HC approaches mainly focus on generating data in batch, to gather benchmarks, or perform surveys demanding nontrivial interactions. We advocate a tighter integration of human computation into online, interactive algorithms. We aim to distill the differences between humans and computers and maximize the advantages of both in one algorithm. Our key idea is to decompose such a problem into a massive number of very simple, carefully designed, human <i>micro-tasks</i> that are based on <i>perception</i>, and whose answers can be combined algorithmically to solve the original problem. Our approach is inspired by previous work on micro-tasks and perception experiments. We present three specific examples for the design of micro perceptual human computation algorithms to extract depth layers and image normals from a single photograph, and to augment an image with high-level semantic information such as symmetry.","cites":"19","conferencePercentile":"56.81818182"},{"venue":"ACM Trans. Graph.","id":"7f2ae1b0d30e46c0e086cf66623203d58b6b9fee","venue_1":"ACM Trans. Graph.","year":"2004","title":"Pitching a baseball: tracking high-speed motion with multi-exposure images","authors":"Christian Theobalt, Irene Albrecht, Jörg Haber, Marcus A. Magnor, Hans-Peter Seidel","author_ids":"1680185, 1737620, 1722980, 1686739, 1746884","abstract":"Athletes and coaches in most professional sports make use of high-tech equipment to analyze and, subsequently, improve the athlete's performance. High-speed video cameras are employed, for instance, to record the swing of a golf club or a tennis racket, the movement of the feet while running, and the body motion in apparatus gymnastics. High-tech and high-speed equipment, however, usually implies high-cost as well. In this paper, we present a passive optical approach to capture high-speed motion using multi-exposure images obtained with low-cost commodity still cameras and a stroboscope. The recorded motion remains completely undisturbed by the motion capture process. We apply our approach to capture the motion of hand and ball for a variety of baseball pitches and present algorithms to automatically track the position, velocity, rotation axis, and spin of the ball along its trajectory. To demonstrate the validity of our setup and algorithms, we analyze the consistency of our measurements with a physically based model that predicts the trajectory of a spinning baseball. Our approach can be applied to capture a wide variety of other high-speed objects and activities such as golfing, bowling, or tennis for visualization as well as analysis purposes.","cites":"20","conferencePercentile":"7.065217391"},{"venue":"ACM Trans. Graph.","id":"04bf572b5cb18c2cf55e93d34f8c06a113558e78","venue_1":"ACM Trans. Graph.","year":"2011","title":"Two-scale particle simulation","authors":"Barbara Solenthaler, Markus H. Gross","author_ids":"1789549, 1743207","abstract":"We propose a two-scale method for particle-based fluids that allocates computing resources to regions of the fluid where complex flow behavior emerges. Our method uses a low- and a high-resolution simulation that run at the same time. While in the coarse simulation the whole fluid is represented by large particles, the fine level simulates only a subset of the fluid with small particles. The subset can be arbitrarily defined and also dynamically change over time to capture complex flows and small-scale surface details. The low- and high-resolution simulations are coupled by including feedback forces and defining appropriate boundary conditions. Our method offers the benefit that particles are of the same size within each simulation level. This avoids particle splitting and merging processes, and allows the simulation of very large resolution differences without any stability problems. The model is easy to implement, and we show how it can be integrated into a standard SPH simulation as well as into the incompressible PCISPH solver. Compared to the single-resolution simulation, our method produces similar surface details while improving the efficiency linearly to the achieved reduction rate of the particle number.","cites":"39","conferencePercentile":"78.15789474"},{"venue":"ACM Trans. Graph.","id":"89a7dd58e31b4ea69f59351fef7dfd4fa1cc6273","venue_1":"ACM Trans. Graph.","year":"2003","title":"Free-viewpoint video of human actors","authors":"Joel Carranza, Christian Theobalt, Marcus A. Magnor, Hans-Peter Seidel","author_ids":"3062478, 1680185, 1686739, 1746884","abstract":"In free-viewpoint video, the viewer can interactively choose his viewpoint in 3-D space to observe the action of a dynamic real-world scene from arbitrary perspectives. The human body and its motion plays a central role in most visual media and its structure can be exploited for robust motion estimation and efficient visualization. This paper describes a system that uses multi-view synchronized video footage of an actor's performance to estimate motion parameters and to interactively re-render the actor's appearance from any viewpoint.The actor's silhouettes are extracted from synchronized video frames via background segmentation and then used to determine a sequence of poses for a 3D human body model. By employing multi-view texturing during rendering, time-dependent changes in the body surface are reproduced in high detail. The motion capture subsystem runs offline, is non-intrusive, yields robust motion parameter estimates, and can cope with a broad range of motion. The rendering subsystem runs at real-time frame rates using ubiquous graphics hardware, yielding a highly naturalistic impression of the actor. The actor can be placed in virtual environments to create composite dynamic scenes. Free-viewpoint video allows the creation of camera fly-throughs or viewing the action interactively from arbitrary perspectives.","cites":"297","conferencePercentile":"91.39784946"},{"venue":"ACM Trans. Graph.","id":"170976415e6fbec4197845f9448516e1ac349f2f","venue_1":"ACM Trans. Graph.","year":"2007","title":"Design of tangent vector fields","authors":"Matthew Fisher, Peter Schröder, Mathieu Desbrun, Hugues Hoppe","author_ids":"2676553, 1775659, 1716096, 1688461","abstract":"Tangent vector fields are an essential ingredient in controlling surface appearance for applications ranging from anisotropic shading to texture synthesis and non-photorealistic rendering. To achieve a desired effect one is typically interested in smoothly varying fields that satisfy a sparse set of user-provided constraints. Using tools from Discrete Exterior Calculus, we present a simple and efficient algorithm for designing such fields over arbitrary triangle meshes. By representing the field as scalars over mesh edges (<i>i.e.</i>, discrete 1-forms), we obtain an intrinsic, coordinate-free formulation in which field smoothness is enforced through discrete Laplace operators. Unlike previous methods, such a formulation leads to a linear system whose sparsity permits efficient pre-factorization. Constraints are incorporated through weighted least squares and can be updated rapidly enough to enable interactive design, as we demonstrate in the context of anisotropic texture synthesis.","cites":"84","conferencePercentile":"72"},{"venue":"ACM Trans. Graph.","id":"3d4cd95ea9c79a201e5f6b18c99de293494a81ee","venue_1":"ACM Trans. Graph.","year":"2009","title":"Efficient affinity-based edit propagation using K-D tree","authors":"Kun Xu, Yong Li, Tao Ju, Shi-Min Hu, Tian-Qiang Liu","author_ids":"1750039, 1689181, 1787371, 1686809, 3326351","abstract":"Image/video editing by strokes has become increasingly popular due to the ease of interaction. Propagating the user inputs to the rest of the image/video, however, is often time and memory consuming especially for large data. We propose here an efficient scheme that allows affinity-based edit propagation to be computed on data containing tens of millions of pixels at interactive rate (in matter of seconds). The key in our scheme is a novel means for approximately solving the optimization problem involved in edit propagation, using adaptive clustering in a high-dimensional, affinity space. Our approximation significantly reduces the cost of existing affinity-based propagation methods while maintaining visual fidelity, and enables interactive stroke-based editing even on high resolution images and long video sequences using commodity computers.","cites":"50","conferencePercentile":"72.37569061"},{"venue":"ACM Trans. Graph.","id":"4c23baab8913cc034d99ac4b830db517dd08586d","venue_1":"ACM Trans. Graph.","year":"2010","title":"Dynamic video narratives","authors":"Carlos D. Correa, Kwan-Liu Ma","author_ids":"1696379, 1707383","abstract":"This paper presents a system for generating dynamic narratives from videos. These narratives are characterized for being compact, coherent and interactive, as inspired by principles of sequential art. Narratives depict the motion of one or several actors over time. Creating compact narratives is challenging as it is desired to combine the video frames in a way that reuses redundant backgrounds and depicts the stages of a motion. In addition, previous approaches focus on the generation of static summaries and can afford expensive image composition techniques. A dynamic narrative, on the other hand, must be played and skimmed in real-time, which imposes certain cost limitations in the video processing. In this paper, we define a novel process to compose foreground and background regions of video frames in a single interactive image using a series of spatio-temporal masks. These masks are created to improve the output of automatic video processing techniques such as image stitching and foreground segmentation. Unlike hand-drawn narratives, often limited to static representations, the proposed system allows users to explore the narrative dynamically and produce different representations of motion. We have built an authoring system that incorporates these methods and demonstrated successful results on a number of video clips. The authoring system can be used to create interactive posters of video clips, browse video in a compact manner or highlight a motion sequence in a movie.","cites":"33","conferencePercentile":"58.47953216"},{"venue":"ACM Trans. Graph.","id":"0aa8bb2891a35e7f45e6a446dee29362f22baba2","venue_1":"ACM Trans. Graph.","year":"2011","title":"A rendering framework for multiscale views of 3D models","authors":"Wei-Hsien Hsu, Kwan-Liu Ma, Carlos D. Correa","author_ids":"2181693, 1707383, 1696379","abstract":"Images that seamlessly combine views at different levels of detail are appealing. However, creating such multiscale images is not a trivial task, and most such illustrations are handcrafted by skilled artists. This paper presents a framework for direct multiscale rendering of geometric and volumetric models. The basis of our approach is a set of non-linearly bent camera rays that smoothly cast through multiple scales. We show that by properly setting up a sequence of conventional pinhole cameras to capture features of interest at different scales, along with image masks specifying the regions of interest for each scale on the projection plane, our rendering framework can generate non-linear sampling rays that smoothly project objects in a scene at multiple levels of detail onto a single image. We address two important issues with non-linear camera projection. First, our streamline-based ray generation algorithm avoids undesired camera ray intersections, which often result in unexpected images. Second, in order to maintain camera ray coherence and preserve aesthetic quality, we create an interpolated 3D field that defines the contribution of each pinhole camera for determining ray orientations. The resulting multiscale camera has three main applications: (1) presenting hierarchical structure in a compact and continuous manner, (2) achieving focus+context visualization, and (3) creating fascinating and artistic images.","cites":"12","conferencePercentile":"26.05263158"},{"venue":"ACM Trans. Graph.","id":"1a789ddb30abe008a3a62995f78c1b56b3e29109","venue_1":"ACM Trans. Graph.","year":"2012","title":"Automatic stylistic manga layout","authors":"Ying Cao, Antoni B. Chan, Rynson W. H. Lau","author_ids":"4525102, 7751933, 1726262","abstract":"Manga layout is a core component in manga production, characterized by its unique styles. However, stylistic manga layouts are difficult for novices to produce as it requires hands-on experience and domain knowledge. In this paper, we propose an approach to automatically generate a stylistic manga layout from a set of input artworks with user-specified semantics, thus allowing less-experienced users to create high-quality manga layouts with minimal efforts. We first introduce three parametric style models that encode the unique stylistic aspects of manga layouts, including layout structure, panel importance, and panel shape. Next, we propose a two-stage approach to generate a manga layout: 1) an initial layout is created that best fits the input artworks and layout structure model, according to a generative probabilistic framework; 2) the layout and artwork geometries are jointly refined using an efficient optimization procedure, resulting in a professional-looking manga layout. Through a user study, we demonstrate that our approach enables novice users to easily and quickly produce higher-quality layouts that exhibit realistic manga styles, when compared to a commercially-available manual layout tool.","cites":"13","conferencePercentile":"35.1010101"},{"venue":"ACM Trans. Graph.","id":"038e6cc89a9dcdb7bd682ad3cdb414426571ac37","venue_1":"ACM Trans. Graph.","year":"2013","title":"Spatio-temporal extrapolation for fluid animation","authors":"Yubo Zhang, Kwan-Liu Ma","author_ids":"2388708, 1707383","abstract":"We introduce a novel spatio-temporal extrapolation technique for fluid simulation designed to improve the results without using higher resolution simulation grids. In general, there are rigid demands associated with pushing fluid animations to higher resolutions given limited computational capabilities. This results in tradeoffs between implementing high-order numerical methods and increasing the resolution of the simulation in space and time. For 3D problems, such challenges rapidly become cost-ineffective. The extrapolation method we present improves the flow features without using higher resolution simulation grids. In this paper, we show that simulation results from our extrapolation are comparable to those from higher resolution simulations. In addition, our method differs from high-order numerical methods because it does not depend on the equation or specific solver. We demonstrate that it is easy to implement and can significantly improve the fluid animation results.","cites":"1","conferencePercentile":"3.167420814"},{"venue":"ACM Trans. Graph.","id":"00640fb3fb1efce64421baa95a6d3543c9d6ce31","venue_1":"ACM Trans. Graph.","year":"2005","title":"Meshless deformations based on shape matching","authors":"Matthias Müller, Bruno Heidelberger, Matthias Teschner, Markus H. Gross","author_ids":"6295817, 2119419, 1759233, 1743207","abstract":"We present a new approach for simulating deformable objects. The underlying model is geometrically motivated. It handles pointbased objects and does not need connectivity information. The approach does not require any pre-processing, is simple to compute, and provides unconditionally stable dynamic simulations.The main idea of our deformable model is to replace energies by geometric constraints and forces by distances of current positions to goal positions. These goal positions are determined via a generalized shape matching of an undeformed rest state with the current deformed state of the point cloud. Since points are always drawn towards well-defined locations, the overshooting problem of explicit integration schemes is eliminated. The versatility of the approach in terms of object representations that can be handled, the efficiency in terms of memory and computational complexity, and the unconditional stability of the dynamic simulation make the approach particularly interesting for games.","cites":"240","conferencePercentile":"95.16129032"},{"venue":"ACM Trans. Graph.","id":"143a8e040e43d41f724951875c00b4dfd2a2f683","venue_1":"ACM Trans. Graph.","year":"2007","title":"Algebraic point set surfaces","authors":"Gaël Guennebaud, Markus H. Gross","author_ids":"3051989, 1743207","abstract":"In this paper we present a new Point Set Surface (PSS) definition based on moving least squares (MLS) fitting of algebraic spheres. Our surface representation can be expressed by either a projection procedure or in implicit form. The central advantages of our approach compared to existing planar MLS include significantly improved stability of the projection under low sampling rates and in the presence of high curvature. The method can approximate or interpolate the input point set and naturally handles planar point clouds. In addition, our approach provides a reliable estimate of the mean curvature of the surface at no additional cost and allows for the robust handling of sharp features and boundaries. It processes a simple point set as input, but can also take significant advantage of surface normals to improve robustness, quality and performance. We also present an novel normal estimation procedure which exploits the properties of the spherical fit for both direction estimation and orientation propagation. Very efficient computational procedures enable us to compute the algebraic sphere fitting with up to 40 million points per second on latest generation GPUs.","cites":"115","conferencePercentile":"86"},{"venue":"ACM Trans. Graph.","id":"c2876df90cdab2ab04448b01c157c45208b3eabe","venue_1":"ACM Trans. Graph.","year":"2012","title":"A statistical similarity measure for aggregate crowd dynamics","authors":"Stephen J. Guy, Jur P. van den Berg, Wenxi Liu, Rynson W. H. Lau, Ming C. Lin, Dinesh Manocha","author_ids":"1794602, 1764300, 2819961, 1726262, 1709625, 1699159","abstract":"We present an information-theoretic method to measure the similarity between a given set of observed, real-world data and visual simulation technique for aggregate crowd motions of a complex system consisting of many individual agents. This metric uses a two-step process to quantify a simulator's ability to reproduce the collective behaviors of the whole system, as observed in the recorded real-world data. First, Bayesian inference is used to estimate the simulation states which best correspond to the observed data, then a maximum likelihood estimator is used to approximate the prediction errors. This process is iterated using the EM-algorithm to produce a robust, statistical estimate of the magnitude of the prediction error as measured by its entropy (smaller is better). This metric serves as a simulator-to-data similarity measurement. We evaluated the metric in terms of robustness to sensor noise, consistency across different datasets and simulation methods, and correlation to perceptual metrics.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"099822052bc20ddeb9686534458b65a457dd5cdd","venue_1":"ACM Trans. Graph.","year":"2013","title":"Bi-scale appearance fabrication","authors":"Yanxiang Lan, Yue Dong, Fabio Pellacini, Xin Tong","author_ids":"2794374, 1744268, 1757883, 1743927","abstract":"Surfaces in the real world exhibit complex appearance due to spatial variations in both their reflectance and local shading frames (i.e. the local coordinate system defined by the normal and tangent direction). For opaque surfaces, existing fabrication solutions can reproduce well only the spatial variations of isotropic reflectance. In this paper, we present a system for fabricating surfaces with desired spatially-varying reflectance, including anisotropic ones, and local shading frames. We approximate each input reflectance, rotated by its local frame, as a small patch of oriented facets coated with isotropic glossy inks. By assigning different ink combinations to facets with different orientations, this bi-scale material can reproduce a wider variety of reflectance than the printer gamut, including anisotropic materials. By orienting the facets appropriately, we control the local shading frame. We propose an algorithm to automatically determine the optimal facets orientations and ink combinations that best approximate a given input appearance, while obeying manufacturing constraints on both geometry and ink gamut. We fabricate the resulting surface with commercially available hardware, a 3D printer to fabricate the facets and a flatbed UV printer to coat them with inks. We validate our method by fabricating a variety of isotropic and anisotropic materials with rich variations in normals and tangents.","cites":"9","conferencePercentile":"29.63800905"},{"venue":"ACM Trans. Graph.","id":"ae49b330407a7d73ef2efcf9234c3d4b08124fc8","venue_1":"ACM Trans. Graph.","year":"2007","title":"A hardware architecture for surface splatting","authors":"Tim Weyrich, Simon Heinzle, Timo Aila, Daniel Bernhard Fasnacht, Stephan Oetiker, Mario Botsch, Cyril Flaig, Simon Mall, Kaspar Rohrer, Norbert Felber, Hubert Kaeslin, Markus H. Gross","author_ids":"1784306, 1782613, 1761103, 1804991, 1776641, 1716234, 1753060, 1774827, 1754807, 1799478, 1681972, 1743207","abstract":"We present a novel architecture for hardware-accelerated rendering of point primitives. Our pipeline implements a refined version of EWA splatting, a high quality method for antialiased rendering of point sampled representations. A central feature of our design is the seamless integration of the architecture into conventional, OpenGL-like graphics pipelines so as to complement triangle-based rendering. The specific properties of the EWA algorithm required a variety of novel design concepts including a ternary depth test and using an on-chip pipelined heap data structure for making the memory accesses of splat primitives more coherent. In addition, we developed a computationally stable evaluation scheme for perspectively corrected splats. We implemented our architecture both on reconfigurable FPGA boards and as an ASIC prototype, and we integrated it into an OpenGL-like software implementation. Our evaluation comprises a detailed performance analysis using scenes of varying complexity.","cites":"14","conferencePercentile":"3.6"},{"venue":"ACM Trans. Graph.","id":"6209274d7fd95c15627495d643758f67ef6a4a4c","venue_1":"ACM Trans. Graph.","year":"2009","title":"Synthetic turbulence using artificial boundary layers","authors":"Tobias Pfaff, Nils Thürey, Andrew Selle, Markus H. Gross","author_ids":"2801835, 1786445, 1714992, 1743207","abstract":"Turbulent vortices in fluid flows are crucial for a visually interesting appearance. Although there has been a significant amount of work on turbulence in graphics recently, these algorithms rely on the underlying simulation to resolve the flow around objects. We build upon work from classical fluid mechanics to design an algorithm that allows us to accurately precompute the turbulence being generated around an object immersed in a flow. This is made possible by modeling turbulence formation based on an averaged flow field, and relying on universal laws describing the flow near a wall. We precompute the confined vorticity in the boundary layer around an object, and simulate the boundary layer separation during a fluid simulation. Then, a turbulence model is used to identify areas where this separated layer will transition into actual turbulence. We sample these regions with vortex particles, and simulate the further dynamics of the vortices based on these particles. We will show how our method complements previous work on synthetic turbulence, and yields physically plausible results. In addition, we demonstrate that our method can efficiently compute turbulent flows around a variety of objects including cars, whisks, as well as boulders in a river flow. We can even apply our model to precomputed static flow fields, yielding turbulent dynamics without a costly simulation.","cites":"34","conferencePercentile":"50.82872928"},{"venue":"ACM Trans. Graph.","id":"012c9ef63c0910eaf411b343a44377456aaff4d2","venue_1":"ACM Trans. Graph.","year":"2012","title":"Diffusion curve textures for resolution independent texture mapping","authors":"Xin Sun, Guofu Xie, Yue Dong, Stephen Lin, Weiwei Xu, Wencheng Wang, Xin Tong, Baining Guo","author_ids":"1788730, 3304920, 1744268, 1686911, 6953977, 5358275, 1743927, 2738456","abstract":"We introduce a vector representation called <i>diffusion curve textures</i> for mapping diffusion curve images (DCI) onto arbitrary surfaces. In contrast to the original <i>implicit</i> representation of DCIs [Orzan et al. 2008], where determining a single texture value requires iterative computation of the entire DCI via the Poisson equation, diffusion curve textures provide an <i>explicit</i> representation from which the texture value at any point can be solved directly, while preserving the compactness and resolution independence of diffusion curves. This is achieved through a formulation of the DCI diffusion process in terms of Green's functions. This formulation furthermore allows the texture value of any rectangular region (e.g. pixel area) to be solved in closed form, which facilitates anti-aliasing. We develop a GPU algorithm that renders anti-aliased diffusion curve textures in real time, and demonstrate the effectiveness of this method through high quality renderings with detailed control curves and color variations.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"31b9f4190050fe078200369223f1aacd2af6b72c","venue_1":"ACM Trans. Graph.","year":"2009","title":"Enrichment textures for detailed cutting of shells","authors":"Peter Kaufmann, Sebastian Martin, Mario Botsch, Eitan Grinspun, Markus H. Gross","author_ids":"2027140, 3341690, 1716234, 7522998, 1743207","abstract":"We present a method for simulating highly detailed cutting and fracturing of thin shells using low-resolution simulation meshes. Instead of refining or remeshing the underlying simulation domain to resolve complex cut paths, we adapt the extended finite element method (XFEM) and enrich our approximation by customdesigned basis functions, while keeping the simulation mesh unchanged. The enrichment functions are stored in <i>enrichment textures</i>, which allows for fracture and cutting discontinuities at a resolution much finer than the underlying mesh, similar to image textures for increased visual resolution. Furthermore, we propose <i>harmonic enrichment functions</i> to handle multiple, intersecting, arbitrarily shaped, progressive cuts per element in a simple and unified framework. Our underlying shell simulation is based on discontinuous Galerkin (DG) FEM, which relaxes the restrictive requirement of <i>C</i><sup>1</sup> continuous basis functions and thus allows for simpler, <i>C</i><sup>0</sup> continuous XFEM enrichment functions.","cites":"24","conferencePercentile":"35.35911602"},{"venue":"ACM Trans. Graph.","id":"84f0f0d564e690bba312582eb41a42b2729c6c42","venue_1":"ACM Trans. Graph.","year":"2009","title":"A system for retargeting of streaming video","authors":"Philipp Krähenbühl, Manuel Lang, Alexander Sorkine-Hornung, Markus H. Gross","author_ids":"2562966, 2951367, 2893744, 1743207","abstract":"We present a novel, integrated system for content-aware video retargeting. A simple and interactive framework combines key frame based constraint editing with numerous automatic algorithms for video analysis. This combination gives content producers high level control of the retargeting process. The central component of our framework is a non-uniform, pixel-accurate warp to the target resolution which considers automatic as well as interactively defined features. Automatic features comprise video saliency, edge preservation at the pixel resolution, and scene cut detection to enforce bilateral temporal coherence. Additional high level constraints can be added by the producer to guarantee a consistent scene composition across arbitrary output formats. For high quality video display we adopted a 2D version of EWA splatting eliminating aliasing artifacts known from previous work. Our method seamlessly integrates into postproduction and computes the reformatting in real-time. This allows us to retarget annotated video streams at a high quality to arbitary aspect ratios while retaining the intended cinematographic scene composition. For evaluation we conducted a user study which revealed a strong viewer preference for our method.","cites":"0","conferencePercentile":"1.104972376"},{"venue":"ACM Trans. Graph.","id":"84db401dcc4b7c918b05800a440ea959f559b503","venue_1":"ACM Trans. Graph.","year":"2010","title":"Programmable motion effects","authors":"Johannes Schmid, Robert W. Sumner, Huw Bowles, Markus H. Gross","author_ids":"5770941, 1693475, 2195121, 1743207","abstract":"Although animation is one of the most compelling aspects of computer graphics, the possibilities for depicting the movement that make dynamic scenes so exciting remain limited for both still images and animations. In our work, we experiment with motion depiction as a first-class entity within the rendering process. We extend the concept of a surface shader, which is evaluated on an infinitesimal portion of an object's surface at one instant in time, to that of a programmable motion effect, which is evaluated with global knowledge about all portions of an object's surface that pass in front of a pixel during an arbitrary long sequence of time. With this added information, our programmable motion effects can decide to color pixels long after (or long before) an object has passed in front of them. In order to compute the input required by the motion effects, we propose a 4D data structure that aggregates an object's movement into a single geometric representation by sampling an object's position at different time instances and connecting corresponding edges in two adjacent samples with a bilinear patch. We present example motion effects for various styles of speed lines, multiple stroboscopic images, temporal offsetting, and photorealistic and stylized blurring on both simple and production examples.","cites":"14","conferencePercentile":"20.1754386"},{"venue":"ACM Trans. Graph.","id":"604b1e5e2799d66b0c3a8e4f7045a8ecd1a7c8e6","venue_1":"ACM Trans. Graph.","year":"2010","title":"Nonlinear disparity mapping for stereoscopic 3D","authors":"Manuel Lang, Alexander Sorkine-Hornung, Oliver Wang, Steven Poulakos, Aljoscha Smolic, Markus H. Gross","author_ids":"2951367, 2893744, 1958703, 2634383, 1741139, 1743207","abstract":"This paper addresses the problem of remapping the disparity range of stereoscopic images and video. Such operations are highly important for a variety of issues arising from the production, live broadcast, and consumption of 3D content. Our work is motivated by the observation that the displayed depth and the resulting 3D viewing experience are dictated by a complex combination of perceptual, technological, and artistic constraints. We first discuss the most important perceptual aspects of stereo vision and their implications for stereoscopic content creation. We then formalize these insights into a set of basic <i>disparity mapping operators.</i> These operators enable us to control and retarget the depth of a stereoscopic scene in a nonlinear and locally adaptive fashion. To implement our operators, we propose a new strategy based on <i>stereoscopic warping</i> of the input video streams. From a sparse set of stereo correspondences, our algorithm computes disparity and image-based saliency estimates, and uses them to compute a deformation of the input views so as to meet the target disparities. Our approach represents a practical solution for actual stereo production and display that does not require camera calibration, accurate dense depth maps, occlusion handling, or inpainting. We demonstrate the performance and versatility of our method using examples from live action post-production, 3D display size adaptation, and live broadcast. An additional user study and ground truth comparison further provide evidence for the quality and practical relevance of the presented work.","cites":"117","conferencePercentile":"98.24561404"},{"venue":"ACM Trans. Graph.","id":"0a77c26a3923a0edb491d23d6dac82e76bd586ab","venue_1":"ACM Trans. Graph.","year":"2010","title":"Unified simulation of elastic rods, shells, and solids","authors":"Sebastian Martin, Peter Kaufmann, Mario Botsch, Eitan Grinspun, Markus H. Gross","author_ids":"3341690, 2027140, 1716234, 7522998, 1743207","abstract":"We develop an accurate, unified treatment of elastica. Following the method of resultant-based formulation to its logical extreme, we derive a higher-order integration rule, or <i>elaston</i>, measuring stretching, shearing, bending, and twisting along any axis. The theory and accompanying implementation do not distinguish between forms of different dimension (solids, shells, rods), nor between manifold regions and non-manifold junctions. Consequently, a single code accurately models a diverse range of elastoplastic behaviors, including buckling, writhing, cutting and merging. Emphasis on convergence to the continuum sets us apart from early unification efforts.","cites":"32","conferencePercentile":"55.84795322"},{"venue":"ACM Trans. Graph.","id":"609c4b0debdac7bce2b01e902816e1996a0fa8af","venue_1":"ACM Trans. Graph.","year":"2010","title":"Scalable fluid simulation using anisotropic turbulence particles","authors":"Tobias Pfaff, Nils Thürey, Jonathan M. Cohen, Sarah Tariq, Markus H. Gross","author_ids":"2801835, 1786445, 2147469, 3158964, 1743207","abstract":"It is usually difficult to resolve the fine details of turbulent flows, especially when targeting real-time applications. We present a novel, scalable turbulence method that uses a realistic energy model and an efficient particle representation that allows for the accurate and robust simulation of small-scale detail. We compute transport of turbulent energy using a complete two-equation <i>k</i>-&#949; model with accurate production terms that allows us to capture anisotropic turbulence effects, which integrate smoothly into the base flow. We only require a very low grid resolution to resolve the underlying base flow. As we offload complexity from the fluid solver to the particle system, we can control the detail of the simulation easily by adjusting the number of particles, without changing the large scale behavior. In addition, no computations are wasted on areas that are not visible. We demonstrate that due to the design of our algorithm it is highly suitable for massively parallel architectures, and is able to generate detailed turbulent simulations with millions of particles at high framerates.","cites":"25","conferencePercentile":"43.85964912"},{"venue":"ACM Trans. Graph.","id":"8bf7fa3580c6452ead85009fad6816faef892cbd","venue_1":"ACM Trans. Graph.","year":"2008","title":"Wavelet turbulence for fluid simulation","authors":"Theodore Kim, Nils Thürey, Doug L. James, Markus H. Gross","author_ids":"1782832, 1786445, 1739671, 1743207","abstract":"We present a novel wavelet method for the simulation of fluids at high spatial resolution. The algorithm enables large- and small-scale detail to be edited separately, allowing high-resolution detail to be added as a post-processing step. Instead of solving the Navier-Stokes equations over a highly refined mesh, we use the wavelet decomposition of a low-resolution simulation to determine the location and energy characteristics of missing high-frequency components. We then synthesize these missing components using a novel incompressible turbulence function, and provide a method to maintain the temporal coherence of the resulting structures. There is no linear system to solve, so the method parallelizes trivially and requires only a few auxiliary arrays. The method guarantees that the new frequencies will not interfere with existing frequencies, allowing animators to set up a low resolution simulation quickly and later add details without changing the overall fluid motion.","cites":"78","conferencePercentile":"82.71604938"},{"venue":"ACM Trans. Graph.","id":"d0f93c7c32c433f246f20ed4c15d1c9de718938d","venue_1":"ACM Trans. Graph.","year":"2009","title":"Generating photo manipulation tutorials by demonstration","authors":"Floraine Grabler, Maneesh Agrawala, Wilmot Li, Mira Dontcheva, Takeo Igarashi","author_ids":"2224711, 1820412, 2812691, 2875493, 1717356","abstract":"We present a demonstration-based system for automatically generating succinct step-by-step visual tutorials of photo manipulations. An author first demonstrates the manipulation using an instrumented version of GIMP that records all changes in interface and application state. From the example recording, our system automatically generates tutorials that illustrate the manipulation using images, text, and annotations. It leverages automated image labeling (recognition of facial features and outdoor scene structures in our implementation) to generate more precise text descriptions of many of the steps in the tutorials. A user study comparing our automatically generated tutorials to hand-designed tutorials and screen-capture video recordings finds that users are 20--44% faster and make 60--95% fewer errors using our tutorials. While our system focuses on tutorial generation, we also present some initial work on generating content-dependent macros that use image recognition to automatically transfer selection operations from the example image used in the demonstration to new target images. While our macros are limited to transferring selection operations we demonstrate automatic transfer of several common retouching techniques including eye recoloring, whitening teeth and sunset enhancement.","cites":"72","conferencePercentile":"87.56906077"},{"venue":"ACM Trans. Graph.","id":"6b7c77fa9ebd7c48610891348026b22bfdaef660","venue_1":"ACM Trans. Graph.","year":"2011","title":"Microgeometry capture using an elastomeric sensor","authors":"Micah K. Johnson, Forrester Cole, Alvin Raj, Edward H. Adelson","author_ids":"1744375, 1805136, 2205640, 1788148","abstract":"We describe a system for capturing microscopic surface geometry. The system extends the retrographic sensor [Johnson and Adelson 2009] to the microscopic domain, demonstrating spatial resolution as small as 2 microns. In contrast to existing microgeometry capture techniques, the system is not affected by the optical characteristics of the surface being measured---it captures the same geometry whether the object is matte, glossy, or transparent. In addition, the hardware design allows for a variety of form factors, including a hand-held device that can be used to capture high-resolution surface geometry in the field. We achieve these results with a combination of improved sensor materials, illumination design, and reconstruction algorithm, as compared to the original sensor of Johnson and Adelson [2009].","cites":"36","conferencePercentile":"74.21052632"},{"venue":"ACM Trans. Graph.","id":"1c41734a25697375b21ec39112c704f582f0bd71","venue_1":"ACM Trans. Graph.","year":"2011","title":"Exploration of continuous variability in collections of 3D shapes","authors":"Maks Ovsjanikov, Wilmot Li, Leonidas J. Guibas, Niloy J. Mitra","author_ids":"1690177, 2812691, 1744254, 1710455","abstract":"As large public repositories of 3D shapes continue to grow, the amount of shape variability in such collections also increases, both in terms of the number of different classes of shapes, as well as the geometric variability of shapes within each class. While this gives users more choice for shape selection, it can be difficult to explore large collections and understand the range of variations amongst the shapes. Exploration is particularly challenging for public shape repositories, which are often only loosely tagged and contain neither point-based nor part-based correspondences. In this paper, we present a method for discovering and exploring continuous variability in a collection of 3D shapes without correspondences. Our method is based on a novel navigation interface that allows users to explore a collection of related shapes by deforming a base template shape through a set of intuitive deformation controls. We also help the user to select the most meaningful deformations using a novel technique for learning shape variability in terms of deformations of the template. Our technique assumes that the set of shapes lies near a low-dimensional manifold in a certain descriptor space, which allows us to avoid establishing correspondences between shapes, while being rotation and scaling invariant. We present results on several shape collections taken directly from public repositories.","cites":"47","conferencePercentile":"83.42105263"},{"venue":"ACM Trans. Graph.","id":"da77845b9a18e7f8a3616782471af51041f71227","venue_1":"ACM Trans. Graph.","year":"2006","title":"Edge subdivision schemes and the construction of smooth vector fields","authors":"Ke Wang, Weiwei Yang, Yiying Tong, Mathieu Desbrun, Peter Schröder","author_ids":"1751643, 1772706, 3225345, 1716096, 1775659","abstract":"Vertex- and face-based subdivision schemes are now routinely used in geometric modeling and computational science, and their primal/dual relationships are well studied. In this paper, we interpret these schemes as defining bases for <i>discrete differential 0- resp. 2-forms</i>, and complete the picture by introducing <i>edge-based</i> subdivision schemes to construct the missing bases for discrete differential 1-forms. Such subdivision schemes map scalar coefficients on edges from the coarse to the refined mesh and are <i>intrinsic</i> to the surface. Our construction is based on treating vertex-, edge-, and face-based subdivision schemes as a <i>joint triple</i> and enforcing that subdivision commutes with the topological exterior derivative. We demonstrate our construction for the case of arbitrary topology triangle meshes. Using Loop's scheme for 0-forms and generalized half-box splines for 2-forms results in a <i>unique</i> generalized spline scheme for 1-forms, easily incorporated into standard subdivision surface codes. We also provide corresponding boundary stencils. Once a metric is supplied, the scalar 1-form coefficients define a smooth tangent vector field on the underlying subdivision surface. Design of tangent vector fields is made particularly easy with this machinery as we demonstrate.","cites":"25","conferencePercentile":"17.12962963"},{"venue":"ACM Trans. Graph.","id":"00c49c3fbd7334a770f857f28de5df366c74bc4a","venue_1":"ACM Trans. Graph.","year":"2005","title":"Discrete conformal mappings via circle patterns","authors":"Liliya Kharevych, Boris Springborn, Peter Schröder","author_ids":"2928650, 1768362, 1775659","abstract":"We introduce a novel method for the construction of discrete conformal mappings from surface meshes of arbitrary topology to the plane. Our approach is based on <i>circle patterns</i>, that is, arrangements of circles---one for each face---with prescribed intersection angles. Given these angles, the circle radii follow as the unique minimizer of a convex energy. The method supports very flexible boundary conditions ranging from free boundaries to control of the boundary shape via prescribed curvatures. Closed meshes of genus zero can be parameterized over the sphere. To parameterize higher genus meshes, we introduce cone singularities at designated vertices. The parameter domain is then a piecewise Euclidean surface. Cone singularities can also help to reduce the often very large area distortion of global conformal maps to moderate levels. Our method involves two optimization problems: a quadratic program and the unconstrained minimization of the circle pattern energy. The latter is a convex function of logarithmic radius variables with simple explicit expressions for gradient and Hessian. We demonstrate the versatility and performance of our algorithm with a variety of examples.","cites":"115","conferencePercentile":"66.93548387"},{"venue":"ACM Trans. Graph.","id":"8c45fb075296efdeda4ab148e5c4b89617ecdbad","venue_1":"ACM Trans. Graph.","year":"2013","title":"Dynamic element textures","authors":"Chongyang Ma, Li-Yi Wei, Sylvain Lefebvre, Xin Tong","author_ids":"1797422, 2420851, 2757631, 1743927","abstract":"Many natural phenomena consist of geometric elements with dynamic motions characterized by small scale repetitions over large scale structures, such as particles, herds, threads, and sheets. Due to their ubiquity, controlling the appearance and behavior of such phenomena is important for a variety of graphics applications. However, such control is often challenging; the repetitive elements are often too numerous for manual edit, while their overall structures are often too versatile for fully automatic computation.\n We propose a method that facilitates easy and intuitive controls at both scales: high-level structures through spatial-temporal output constraints (e.g. overall shape and motion of the output domain), and low-level details through small input exemplars (e.g. element arrangements and movements). These controls are suitable for manual specification, while the corresponding geometric and dynamic repetitions are suitable for automatic computation. Our system takes such user controls as inputs, and generates as outputs the corresponding repetitions satisfying the controls.\n Our method, which we call <i>dynamic element textures</i>, aims to produce such controllable repetitions through a combination of constrained optimization (satisfying controls) and data driven computation (synthesizing details). We use spatial-temporal samples as the core representation for dynamic geometric elements. We propose analysis algorithms for decomposing small scale repetitions from large scale themes, as well as synthesis algorithms for generating outputs satisfying user controls. Our method is general, producing a range of artistic effects that previously required disparate and specialized techniques.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"08f497d7c586e5815d8d850b612573b60e165d8c","venue_1":"ACM Trans. Graph.","year":"2004","title":"Removing excess topology from isosurfaces","authors":"Zoë J. Wood, Hugues Hoppe, Mathieu Desbrun, Peter Schröder","author_ids":"3048844, 1688461, 1716096, 1775659","abstract":"Many high-resolution surfaces are created through isosurface extraction from volumetric representations, obtained by 3D photography, CT, or MRI. Noise inherent in the acquisition process can lead to geometrical and <i>topological</i> errors. Reducing geometrical errors during reconstruction is well studied. However, isosurfaces often contain many topological errors in the form of tiny handles. These nearly invisible artifacts hinder subsequent operations like mesh simplification, remeshing, and parametrization. In this article we present a practical method for removing handles in an isosurface. Our algorithm makes an axis-aligned sweep through the volume to locate handles, compute their sizes, and selectively remove them. The algorithm is designed to facilitate out-of-core execution. It finds the handles by incrementally constructing and analyzing a Reeb graph. The size of a handle is measured by a short nonseparating cycle. Handles are removed robustly by modifying the volume rather than attempting \"mesh surgery.\" Finally, the volumetric modifications are spatially localized to preserve geometrical detail. We demonstrate topology simplification on several complex models, and show its benefits for subsequent surface processing.","cites":"129","conferencePercentile":"61.95652174"},{"venue":"ACM Trans. Graph.","id":"28753a0094a4a27e958207ff7c864bc17e6cc776","venue_1":"ACM Trans. Graph.","year":"2004","title":"Variational normal meshes","authors":"Ilja Friedel, Peter Schröder, Andrei Khodakovsky","author_ids":"1793634, 1775659, 1678711","abstract":"Hierarchical representations of surfaces have many advantages for digital geometry processing applications. &#60;i>Normal meshes&#60;/i> are particularly attractive since their level-to-level displacements are in the local normal direction only. Consequently, they only require scalar coefficients to specify. In this article, we propose a novel method to approximate a given mesh with a normal mesh. Instead of building an associated parameterization on the fly, we assume a globally smooth parameterization at the beginning and cast the problem as one of perturbing this parameterization. Controlling the magnitude of this perturbation gives us explicit control over the range between fully constrained (only scalar coefficients) and unconstrained (3-vector coefficients) approximations. With the unconstrained problem giving the lowest approximation error, we can thus characterize the error cost of normal meshes as a function of the number of nonnormal offsets---we find a significant gain for little (error) cost. Because the normal mesh construction creates a &#60;i>geometry driven&#60;/i> approximation, we can replace the difficult geometric distance minimization problem with a much simpler least squares problem. This variational approach reduces magnitude &#60;i>and&#60;/i> structure (aliasing) of the error further. Our method separates the parameterization construction into an initial setup followed only by subsequent perturbations, giving us an algorithm which is far simpler to implement, more robust, and significantly faster.","cites":"11","conferencePercentile":"3.260869565"},{"venue":"ACM Trans. Graph.","id":"7ecaa2f5a153e4572dd62fb0435e021fbea4670a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Cost-effective printing of 3D objects with skin-frame structures","authors":"Weiming Wang, Tuanfeng Y. Wang, Zhouwang Yang, Ligang Liu, Xin Tong, Weihua Tong, Jiansong Deng, Falai Chen, Xiuping Liu","author_ids":"3043553, 2098438, 2016529, 1724542, 1743927, 2855773, 1735199, 1785918, 4934421","abstract":"3D printers have become popular in recent years and enable fabrication of custom objects for home users. However, the cost of the material used in printing remains high. In this paper, we present an automatic solution to design a skin-frame structure for the purpose of reducing the material cost in printing a given 3D object. The frame structure is designed by an optimization scheme which significantly reduces material volume and is guaranteed to be physically stable, geometrically approximate, and printable. Furthermore, the number of struts is minimized by solving an <i>l</i><sub>0</sub> sparsity optimization. We formulate it as a multi-objective programming problem and an iterative extension of the preemptive algorithm is developed to find a compromise solution. We demonstrate the applicability and practicability of our solution by printing various objects using both powder-type and extrusion-type 3D printers. Our method is shown to be more cost-effective than previous works.","cites":"43","conferencePercentile":"94.11764706"},{"venue":"ACM Trans. Graph.","id":"6756733ed1d996e903ddb000969b609b9778f6a2","venue_1":"ACM Trans. Graph.","year":"2003","title":"Progressive encoding of complex isosurfaces","authors":"Haeyoung Lee, Mathieu Desbrun, Peter Schröder","author_ids":"2850114, 1716096, 1775659","abstract":"We present a progressive encoding technique specifically designed for complex isosurfaces. It achieves better rate distortion performance than all standard mesh coders, and even improves on all previous single rate isosurface coders. Our novel algorithm handles isosurfaces with or without sharp features, and deals gracefully with high topologic and geometric complexity. The inside/outside function of the volume data is progressively transmitted through the use of an adaptive octree, while a local frame based encoding is used for the fine level placement of surface samples. Local patterns in topology and local smoothness in geometry are exploited by context-based arithmetic encoding, allowing us to achieve an average of 6.10 bits per vertex (b/v) at very low distortion. Of this rate only 0.65 b/v are dedicated to connectivity data: this improves by 24% over the best previous single rate isosurface encoder.","cites":"36","conferencePercentile":"18.8172043"},{"venue":"ACM Trans. Graph.","id":"0ea154c1ea82bb368895c88ac68f4a277f3f3a0c","venue_1":"ACM Trans. Graph.","year":"2003","title":"Globally smooth parameterizations with low distortion","authors":"Andrei Khodakovsky, Nathan Litke, Peter Schröder","author_ids":"1678711, 3183686, 1775659","abstract":"Good parameterizations are of central importance in many digital geometry processing tasks. Typically the behavior of such processing algorithms is related to the smoothness of the parameterization and how much distortion it contains. Since a parameterization maps a bounded region of the plane to the surface, a parameterization for a surface which is not homeomorphic to a disc must be made up of multiple pieces. We present a novel parameterization algorithm for arbitrary topology surface meshes which computes a <i>globally</i> smooth parameterization with low distortion. We optimize the patch layout subject to criteria such as shape quality and metric distortion, which are used to steer a mesh simplification approach for base complex construction. Global smoothness is achieved through simultaneous relaxation over all patches, with suitable transition functions between patches incorporated into the relaxation procedure. We demonstrate the quality of our parameterizations through numerical evaluation of distortion measures and the excellent rate distortion performance of semi-regular remeshes produced with these parameterizations. The numerical algorithms required to compute the parameterizations are robust and run on the order of minutes even for large meshes.","cites":"132","conferencePercentile":"69.89247312"},{"venue":"ACM Trans. Graph.","id":"ef750dc8144dad67b58bfe7837e703940e71234a","venue_1":"ACM Trans. Graph.","year":"2016","title":"PATEX: exploring pattern variations","authors":"Paul Guerrero, Gilbert Bernstein, Wilmot Li, Niloy J. Mitra","author_ids":"2871337, 6408441, 2812691, 1710455","abstract":"Patterns play a central role in 2D graphic design. A critical step in the design of patterns is evaluating multiple design alternatives. Exploring these alternatives with existing tools is challenging because most tools force users to work with a single fixed representation of the pattern that encodes a specific set of geometric relationships between pattern elements. However, for most patterns, there are many different interpretations of its regularity that correspond to different design variations. The exponential nature of this variation space makes the problem of finding all variations intractable. We present a method called PATEX to characterize and efficiently identify distinct and valid pattern variations, allowing users to directly navigate the variation space. Technically, we propose a novel linear approximation to handle the complexity of the problem and efficiently enumerate suitable pattern variations under proposed element movements. We also present two pattern editing interfaces that expose the detected pattern variations as suggested edits to the user. We show a diverse collection of pattern edits and variations created with PATEX. The results from our user study indicate that our suggested variations can be useful and inspirational for typical pattern editing tasks.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"1ec857a920570167394136e457e5bb9ae8618c85","venue_1":"ACM Trans. Graph.","year":"2015","title":"Style compatibility for 3D furniture models","authors":"Tianqiang Liu, Aaron Hertzmann, Wilmot Li, Thomas A. Funkhouser","author_ids":"2414948, 1747779, 2812691, 1807080","abstract":"This paper presents a method for learning to predict the stylistic compatibility between 3D furniture models from different object classes: e.g., how well does this chair go with that table? To do this, we collect relative assessments of style compatibility using crowdsourcing. We then compute geometric features for each 3D model and learn a mapping of them into a space where Euclidean distances represent style incompatibility. Motivated by the geometric subtleties of style, we introduce part-aware geometric feature vectors that characterize the shapes of different parts of an object separately. Motivated by the need to compute style compatibility between different object classes, we introduce a method to learn object class-specific mappings from geometric features to a shared feature space. During experiments with these methods, we find that they are effective at predicting style compatibility agreed upon by people. We find in user studies that the learned compatibility metric is useful for novel interactive tools that: 1) retrieve stylistically compatible models for a query, 2) suggest a piece of furniture for an existing scene, and 3) help guide an interactive 3D modeler towards scenes with compatible furniture.","cites":"13","conferencePercentile":"95.10204082"},{"venue":"ACM Trans. Graph.","id":"33383d3017bfbdfe32cea2365468e8c16c9b2fdd","venue_1":"ACM Trans. Graph.","year":"2013","title":"Global illumination with radiance regression functions","authors":"Peiran Ren, Jiaping Wang, Minmin Gong, Stephen Lin, Xin Tong, Baining Guo","author_ids":"3246404, 4912907, 2742377, 1686911, 1743927, 2738456","abstract":"We present radiance regression functions for fast rendering of global illumination in scenes with dynamic local light sources. A radiance regression function (RRF) represents a non-linear mapping from local and contextual attributes of surface points, such as position, viewing direction, and lighting condition, to their indirect illumination values. The RRF is obtained from precomputed shading samples through regression analysis, which determines a function that best fits the shading data. For a given scene, the shading samples are precomputed by an offline renderer.\n The key idea behind our approach is to exploit the nonlinear coherence of the indirect illumination data to make the RRF both compact and fast to evaluate. We model the RRF as a multilayer acyclic feed-forward neural network, which provides a close functional approximation of the indirect illumination and can be efficiently evaluated at run time. To effectively model scenes with spatially variant material properties, we utilize an augmented set of attributes as input to the neural network RRF to reduce the amount of inference that the network needs to perform. To handle scenes with greater geometric complexity, we partition the input space of the RRF model and represent the subspaces with separate, smaller RRFs that can be evaluated more rapidly. As a result, the RRF model scales well to increasingly complex scene geometry and material variation. Because of its compactness and ease of evaluation, the RRF model enables real-time rendering with full global illumination effects, including changing caustics and multiple-bounce high-frequency glossy interreflections.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"bfa6b7fea4fd0b664cc9bcced59b24ee0b7e9e49","venue_1":"ACM Trans. Graph.","year":"2016","title":"Directing user attention via visual flow on web designs","authors":"Xufang Pang, Ying Cao, Rynson W. H. Lau, Antoni B. Chan","author_ids":"2293733, 4525102, 1726262, 7751933","abstract":"We present a novel approach that allows web designers to easily direct user attention via visual flow on web designs. By collecting and analyzing users' eye gaze data on real-world webpages under the task-driven condition, we build two user attention models that characterize user attention patterns between a pair of page components. These models enable a novel web design interaction for designers to easily create a visual flow to guide users' eyes (i.e., direct user attention along a given path) through a web design with minimal effort. In particular, given an existing web design as well as a designer-specified path over a subset of page components, our approach automatically optimizes the web design so that the resulting design can direct users' attention to move along the input path. We have tested our approach on various web designs of different categories. Results show that our approach can effectively guide user attention through the web design according to the designer's high-level specification.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"dc9574281a84388e6e7e3b2163dab2724365719a","venue_1":"ACM Trans. Graph.","year":"2015","title":"WrapIt: computer-assisted crafting of wire wrapped jewelry","authors":"Emmanuel Iarussi, Wilmot Li, Adrien Bousseau","author_ids":"1859777, 2812691, 2149814","abstract":"Wire wrapping is a traditional form of handmade jewelry that involves bending metal wire to create intricate shapes. The technique appeals to novices and casual crafters because of its low cost, accessibility and unique aesthetic. We present a computational design tool that addresses the two main challenges of creating 2D wire-wrapped jewelry: decomposing an input drawing into a set of wires, and bending the wires to give them shape. Our main contribution is an automatic wire decomposition algorithm that segments a drawing into a small number of wires based on aesthetic and fabrication principles. We formulate the task as a constrained graph labeling problem and present a stochastic optimization approach that produces good results for a variety of inputs.\n Given a decomposition, our system generates a 3D-printed custom support structure, or <i>jig</i>, that helps users bend the wire into the appropriate shape. We validated our wire decomposition algorithm against existing wire-wrapped designs, and used our end-to-end system to create new jewelry from clipart drawings. We also evaluated our approach with novice users, who were able to create various pieces of jewelry in less than half an hour.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"4b1bde7007a72b8abb2b8c3c9a41a438e85d5528","venue_1":"ACM Trans. Graph.","year":"2014","title":"Look over here: attention-directing composition of manga elements","authors":"Ying Cao, Rynson W. H. Lau, Antoni B. Chan","author_ids":"4525102, 1726262, 7751933","abstract":"Picture subjects and text balloons are basic elements in comics, working together to propel the story forward. Japanese comics artists often leverage a carefully designed composition of subjects and balloons (generally referred to as <i>panel elements</i>) to provide a continuous and fluid reading experience. However, such a composition is hard to produce for people without the required experience and knowledge. In this paper, we propose an approach for novices to synthesize a composition of panel elements that can effectively guide the reader's attention to convey the story. Our primary contribution is a probabilistic graphical model that describes the relationships among the artist's guiding path, the panel elements, and the viewer attention, which can be effectively learned from a small set of existing manga pages. We show that the proposed approach can measurably improve the readability, visual appeal, and communication of the story of the resulting pages, as compared to an existing method. We also demonstrate that the proposed approach enables novice users to create higher-quality compositions with less time, compared with commercially available programs.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"3fd5a6df1182ffad6229a3b47f0011e3686f2b4e","venue_1":"ACM Trans. Graph.","year":"2014","title":"Creating works-like prototypes of mechanical objects","authors":"Wilmot Li, Maneesh Agrawala, Niloy J. Mitra","author_ids":"2812691, 1820412, 1710455","abstract":"Designers often create physical <i>works-like</i> prototypes early in the product development cycle to explore possible mechanical architectures for a design. Yet, creating functional prototypes requires time and expertise, which discourages rapid design iterations. Designers must carefully specify part and joint parameters to ensure that parts move and fit and together in the intended manner. We present an interactive system that streamlines the process by allowing users to annotate rough 3D models with high-level functional relationships (e.g., part <i>A</i> fits inside part <i>B</i>). Based on these relationships, our system optimizes the model geometry to produce a working design. We demonstrate the versatility of our system by using it to design a variety of works-like prototypes.","cites":"14","conferencePercentile":"79.218107"},{"venue":"ACM Trans. Graph.","id":"935d8f5a6a9155ad6319ba149a36638c0e8b2669","venue_1":"ACM Trans. Graph.","year":"2013","title":"Interpreting concept sketches","authors":"Tianjia Shao, Wilmot Li, Kun Zhou, Weiwei Xu, Baining Guo, Niloy J. Mitra","author_ids":"2252400, 2812691, 6671887, 6953977, 2738456, 1710455","abstract":"Concept sketches are popularly used by designers to convey pose and function of products. Understanding such sketches, however, requires special skills to form a mental 3D representation of the product geometry by linking parts across the different sketches and imagining the intermediate object configurations. Hence, the sketches can remain inaccessible to many, especially non-designers. We present a system to facilitate easy interpretation and exploration of concept sketches. Starting from crudely specified incomplete geometry, often inconsistent across the different views, we propose a globally-coupled analysis to extract part correspondence and inter-part junction information that best explain the different sketch views. The user can then interactively explore the abstracted object to gain better understanding of the product functions. Our key technical contribution is performing shape analysis without access to any coherent 3D geometric model by reasoning in the space of inter-part relations. We evaluate our system on various concept sketches obtained from popular product design books and websites.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"671d3c0938c84ba4f0688eada07bcd4244340afc","venue_1":"ACM Trans. Graph.","year":"2013","title":"Designing and fabricating mechanical automata from mocap sequences","authors":"Duygu Ceylan, Wilmot Li, Niloy J. Mitra, Maneesh Agrawala, Mark Pauly","author_ids":"5350989, 2812691, 1710455, 1820412, 1741645","abstract":"Mechanical figures that mimic human motions continue to entertain us and capture our imagination. Creating such automata requires expertise in motion planning, knowledge of mechanism design, and familiarity with fabrication constraints. Thus, automaton design remains restricted to only a handful of experts. We propose an automatic algorithm that takes a motion sequence of a humanoid character and generates the design for a mechanical figure that approximates the input motion when driven with a single input crank. Our approach has two stages. The <i>motion approximation</i> stage computes a motion that approximates the input sequence as closely as possible while remaining compatible with the geometric and motion constraints of the mechanical parts in our design. Then, in the <i>layout</i> stage, we solve for the sizing parameters and spatial layout of all the elements, while respecting all fabrication and assembly constraints. We apply our algorithm on a range of input motions taken from motion capture databases. We also fabricate two of our designs to demonstrate the viability of our approach.","cites":"27","conferencePercentile":"81.67420814"},{"venue":"ACM Trans. Graph.","id":"42f6fa66fe16d0ee6a98713898bf9be8e03c66f1","venue_1":"ACM Trans. Graph.","year":"2013","title":"Learning part-based templates from large collections of 3D shapes","authors":"Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha Chaudhuri, Stephen DiVerdi, Thomas A. Funkhouser","author_ids":"3082383, 2812691, 1710455, 7218171, 3004014, 1807080","abstract":"As large repositories of 3D shape collections continue to grow, understanding the data, especially encoding the inter-model similarity and their variations, is of central importance. For example, many data-driven approaches now rely on access to semantic segmentation information, accurate inter-model point-to-point correspondence, and deformation models that characterize the model collections. Existing approaches, however, are either supervised requiring manual labeling; or employ super-linear matching algorithms and thus are unsuited for analyzing large collections spanning many thousands of models. We propose an automatic algorithm that starts with an initial template model and then jointly optimizes for part segmentation, point-to-point surface correspondence, and a compact deformation model to best explain the input model collection. As output, the algorithm produces a set of probabilistic part-based templates that groups the original models into clusters of models capturing their styles and variations. We evaluate our algorithm on several standard datasets and demonstrate its scalability by analyzing much larger collections of up to thousands of shapes.","cites":"41","conferencePercentile":"92.08144796"},{"venue":"ACM Trans. Graph.","id":"0f1debdff4b698b53a85c6c1b390fd201bc7f369","venue_1":"ACM Trans. Graph.","year":"2012","title":"Exploring collections of 3D models using fuzzy correspondences","authors":"Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, Thomas A. Funkhouser","author_ids":"3082383, 2812691, 1710455, 3004014, 1807080","abstract":"Large collections of 3D models from the same object class (e.g., chairs, cars, animals) are now commonly available via many public repositories, but exploring the range of shape variations across such collections remains a challenging task. In this work, we present a new exploration interface that allows users to browse collections based on similarities and differences between shapes in user-specified regions of interest (ROIs). To support this interactive system, we introduce a novel analysis method for computing similarity relationships between points on 3D shapes across a collection. We encode the inherent ambiguity in these relationships using fuzzy point correspondences and propose a robust and efficient computational framework that estimates fuzzy correspondences using only a sparse set of pairwise model alignments. We evaluate our analysis method on a range of correspondence benchmarks and report substantial improvements in both speed and accuracy over existing alternatives. In addition, we demonstrate how fuzzy correspondences enable key features in our exploration tool, such as automated view alignment, ROI-based similarity search, and faceted browsing.","cites":"59","conferencePercentile":"94.94949495"},{"venue":"ACM Trans. Graph.","id":"080fe94a52cd5242fee1a1e9bd6f0ceeb0d1bbce","venue_1":"ACM Trans. Graph.","year":"2008","title":"Conformal equivalence of triangle meshes","authors":"Boris Springborn, Peter Schröder, Ulrich Pinkall","author_ids":"1768362, 1775659, 1781946","abstract":"We present a new algorithm for conformal mesh parameterization. It is based on a precise notion of <i>discrete conformal equivalence</i> for triangle meshes which mimics the notion of conformal equivalence for smooth surfaces. The problem of finding a flat mesh that is discretely conformally equivalent to a given mesh can be solved efficiently by minimizing a convex energy function, whose Hessian turns out to be the well known cot-Laplace operator. This method can also be used to map a surface mesh to a parameter domain which is flat except for isolated cone singularities, and we show how these can be placed automatically in order to reduce the distortion of the parameterization. We present the salient features of the theory and elaborate the algorithms with a number of examples.","cites":"98","conferencePercentile":"89.81481481"},{"venue":"ACM Trans. Graph.","id":"2c10c18cbb2842c76d55bd5283cd21df42b6f1d3","venue_1":"ACM Trans. Graph.","year":"2007","title":"Image and depth from a conventional camera with a coded aperture","authors":"Anat Levin, Rob Fergus, Frédo Durand, William T. Freeman","author_ids":"1801055, 2276554, 1728125, 1768236","abstract":"A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image.\n Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with the modified camera. A layered depth map is then extracted, requiring user-drawn strokes to clarify layer assignments in some cases. The resulting sharp image and layered depth map can be combined for various photographic applications, including automatic scene segmentation, post-exposure refocusing, or re-rendering of the scene from an alternate viewpoint.","cites":"449","conferencePercentile":"99.2"},{"venue":"ACM Trans. Graph.","id":"ab6d6d16ed290baa47683ec7790db221fc47587c","venue_1":"ACM Trans. Graph.","year":"2016","title":"Why New Programming Languages for Simulation?","authors":"Gilbert Louis Bernstein, Fredrik Kjolstad","author_ids":"2541837, 2290932","abstract":"Writing highly performant simulations requires a lot of human effort to optimize for an increasingly diverse set of hardware platforms, such as multi-core CPUs, GPUs, and distributed machines. Since these optimizations cut across both the design of geometric data structures and numerical linear algebra, code reusability and portability is frequently sacrificed for performance.\n We believe the key to make simulation programmers more productive at developing portable and performant code is to introduce new linguistic abstractions, as in rendering and image processing. In this perspective, we distill the core ideas from our two languages, Ebb and Simit, that are published in this journal.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"034ff107aed33f3cbb0be061259dc1a138274b46","venue_1":"ACM Trans. Graph.","year":"2007","title":"The lightspeed automatic interactive lighting preview system","authors":"Jonathan Ragan-Kelley, Charlie Kilpatrick, Brian W. Smith, Doug Epps, Paul Green, Christophe Hery, Frédo Durand","author_ids":"2488277, 2063087, 3211977, 2711699, 2535850, 1953024, 1728125","abstract":"We present an automated approach for high-quality preview of feature-film rendering during lighting design. Similar to previous work, we use a deep-framebuffer shaded on the GPU to achieve interactive performance. Our first contribution is to generate the deep-framebuffer and corresponding shaders automatically through data-flow analysis and compilation of the original scene. Cache compression reduces automatically-generated deep-framebuffers to reasonable size for complex production scenes and shaders. We also propose a new structure, the <i>indirect framebuffer</i>, that decouples shading samples from final pixels and allows a deep-framebuffer to handle antialiasing, motion blur and transparency efficiently. Progressive refinement enables fast feedback at coarser resolution. We demonstrate our approach in real-world production.","cites":"26","conferencePercentile":"20.8"},{"venue":"ACM Trans. Graph.","id":"19838ddbb0cac09a193db2f59d306e4e891945cd","venue_1":"ACM Trans. Graph.","year":"2003","title":"Motion synthesis from annotations","authors":"Okan Arikan, David A. Forsyth, James F. O'Brien","author_ids":"2846625, 1744452, 1692280","abstract":"This paper describes a framework that allows a user to synthesize human motion while retaining control of its qualitative properties. The user paints a timeline with annotations --- like walk, run or jump --- from a vocabulary which is freely chosen by the user. The system then assembles frames from a motion database so that the final motion performs the specified actions at specified times. The motion can also be forced to pass through particular configurations at particular times, and to go to a particular position and orientation. Annotations can be painted positively (for example, <i>must</i> run), negatively (for example, <i>may not</i> run backwards) or as a <i>don't-care</i>. The system uses a novel search method, based around dynamic programming at several scales, to obtain a solution efficiently so that authoring is interactive. Our results demonstrate that the method can generate smooth, natural-looking motion.The annotation vocabulary can be chosen to fit the application, and allows specification of composite motions (run and jump simultaneously, for example). The process requires a collection of motion data that has been annotated with the chosen vocabulary. This paper also describes an effective tool, based around repeated use of support vector machines, that allows a user to annotate a large collection of motions quickly and easily so that they may be used with the synthesis algorithm.","cites":"242","conferencePercentile":"87.09677419"},{"venue":"ACM Trans. Graph.","id":"1eb11de3f319ff7c06252082dc65e47e832567d6","venue_1":"ACM Trans. Graph.","year":"2005","title":"Fast and detailed approximate global illumination by irradiance decomposition","authors":"Okan Arikan, David A. Forsyth, James F. O'Brien","author_ids":"2846625, 1744452, 1692280","abstract":"In this paper we present an approximate method for accelerated computation of the final gathering step in a global illumination algorithm. Our method operates by decomposing the radiance field close to surfaces into separate far- and near-field components that can be approximated individually. By computing surface shading using these approximations, instead of directly querying the global illumination solution, we have been able to obtain rendering time speed ups on the order of 10x compared to previous acceleration methods. Our approximation schemes rely mainly on the assumptions that radiance due to distant objects will exhibit low spatial and angular variation, and that the visibility between a surface and nearby surfaces can be reasonably predicted by simple location and orientation-based heuristics. Motivated by these assumptions, our far-field scheme uses scattered-data interpolation with spherical harmonics to represent spatial and angular variation, and our near-field scheme employs an aggressively simple visibility heuristic. For our test scenes, the errors introduced when our assumptions fail do not result in visually objectionable artifacts or easily noticeable deviation from a ground-truth solution. We also discuss how our near-field approximation can be used with standard local illumination algorithms to produce significantly improved images at only negligible additional cost.","cites":"48","conferencePercentile":"23.79032258"},{"venue":"ACM Trans. Graph.","id":"0e21b744e6314321570ef34ec6be4d33bf512faa","venue_1":"ACM Trans. Graph.","year":"2007","title":"Capturing and animating occluded cloth","authors":"Ryan White, Keenan Crane, David A. Forsyth","author_ids":"6714943, 3284915, 1744452","abstract":"We capture the shape of moving cloth using a custom set of color markers printed on the surface of the cloth. The output is a sequence of triangle meshes with static connectivity and with detail at the scale of individual markers in both smooth and folded regions. We compute markers' coordinates in space using correspondence across multiple synchronized video cameras. Correspondence is determined from color information in small neighborhoods and refined using a novel strain pruning process. Final correspondence does not require neighborhood information. We use a novel data driven hole-filling technique to fill occluded regions. Our results include several challenging examples: a wrinkled shirt sleeve, a dancing pair of pants, and a rag tossed onto a cup. Finally, we demonstrate that cloth capture is reusable by animating a pair of pants using human motion capture data.","cites":"81","conferencePercentile":"70.4"},{"venue":"ACM Trans. Graph.","id":"240acfe14b608c81f68045e0d0851a5dbbecd33b","venue_1":"ACM Trans. Graph.","year":"2009","title":"Generalizing motion edits with Gaussian processes","authors":"Leslie Ikemoto, Okan Arikan, David A. Forsyth","author_ids":"3015207, 2846625, 1744452","abstract":"One way that artists create compelling character animations is by manipulating details of a character's motion. This process is expensive and repetitive. We show that we can make such motion editing more efficient by generalizing the edits an animator makes on short sequences of motion to other sequences. Our method predicts frames for the motion using Gaussian process models of kinematics and dynamics. These estimates are combined with probabilistic inference. Our method can be used to propagate edits from examples to an entire sequence for an existing character, and it can also be used to map a motion from a control character to a very different target character. The technique shows good generalization. For example, we show that an estimator, learned from a few seconds of edited example animation using our methods, generalizes well enough to edit minutes of character animation in a high-quality fashion. Learning is interactive: An animator who wants to improve the output can provide small, correcting examples and the system will produce improved estimates of motion. We make this interactive learning process efficient and natural with a fast, full-body IK system with novel features. Finally, we present data from interviews with professional character animators that indicate that generalizing and propagating animator edits can save artists significant time and work.","cites":"26","conferencePercentile":"38.95027624"},{"venue":"ACM Trans. Graph.","id":"4f8d647ddc09473e9310c461e0a5f5adf908a169","venue_1":"ACM Trans. Graph.","year":"2012","title":"Lightweight binocular facial performance capture under uncontrolled lighting","authors":"Levi Valgaerts, Chenglei Wu, Andrés Bruhn, Hans-Peter Seidel, Christian Theobalt","author_ids":"1797649, 1682672, 1808969, 1746884, 1680185","abstract":"Recent progress in passive facial performance capture has shown impressively detailed results on highly articulated motion. However, most methods rely on complex multi-camera set-ups, controlled lighting or fiducial markers. This prevents them from being used in general environments, outdoor scenes, during live action on a film set, or by freelance animators and everyday users who want to capture their digital selves. In this paper, we therefore propose a lightweight passive facial performance capture approach that is able to reconstruct high-quality dynamic facial geometry from only a single pair of stereo cameras. Our method succeeds under uncontrolled and time-varying lighting, and also in outdoor scenes. Our approach builds upon and extends recent image-based scene flow computation, lighting estimation and shading-based refinement algorithms. It integrates them into a pipeline that is specifically tailored towards facial performance reconstruction from challenging binocular footage under uncontrolled lighting. In an experimental evaluation, the strong capabilities of our method become explicit: We achieve detailed and spatio-temporally coherent results for expressive facial motion in both indoor and outdoor scenes -- even from low quality input images recorded with a hand-held consumer stereo camera. We believe that our approach is the first to capture facial performances of such high quality from a single stereo rig and we demonstrate that it brings facial performance capture out of the studio, into the wild, and within the reach of everybody.","cites":"48","conferencePercentile":"88.88888889"},{"venue":"ACM Trans. Graph.","id":"66bb1604482741a16f3abb1bfb1c531790c66a57","venue_1":"ACM Trans. Graph.","year":"2011","title":"Rendering synthetic objects into legacy photographs","authors":"Kevin Karsch, Varsha Hedau, David A. Forsyth, Derek Hoiem","author_ids":"2732061, 3236027, 1744452, 2433269","abstract":"We propose a method to realistically insert synthetic objects into existing photographs without requiring access to the scene or any additional scene measurements. With a single image and a small amount of annotation, our method creates a physical model of the scene that is suitable for realistically rendering synthetic objects with diffuse, specular, and even glowing materials while accounting for lighting interactions between the objects and the scene. We demonstrate in a user study that synthetic images produced by our method are confusable with real scenes, even for people who believe they are good at telling the difference. Further, our study shows that our method is competitive with other insertion methods while requiring less scene information. We also collected new illumination and reflectance datasets; renderings produced by our system compare well to ground truth. Our system has applications in the movie and gaming industry, as well as home decorating and user content creation, among others.","cites":"62","conferencePercentile":"88.42105263"},{"venue":"ACM Trans. Graph.","id":"6e7fdf88d053bb8e5506167ffb62b7dfea54c5a3","venue_1":"ACM Trans. Graph.","year":"2012","title":"Videoscapes: exploring sparse, unstructured video collections","authors":"James Tompkin, Kwang In Kim, Jan Kautz, Christian Theobalt","author_ids":"1854493, 1808255, 1690538, 1680185","abstract":"The abundance of mobile devices and digital cameras with video capture makes it easy to obtain large collections of video clips that contain the same location, environment, or event. However, such an unstructured collection is difficult to comprehend and explore. We propose a system that analyzes collections of unstructured but related video data to create a <i>Videoscape</i>: a data structure that enables interactive exploration of video collections by visually navigating -- spatially and/or temporally -- between different clips. We automatically identify transition opportunities, or <i>portals</i>. From these portals, we construct the Videoscape, a graph whose edges are video clips and whose nodes are portals between clips. Now structured, the videos can be interactively explored by walking the graph or by geographic map. Given this system, we gauge preference for different video transition styles in a user study, and generate heuristics that automatically choose an appropriate transition style. We evaluate our system using three further user studies, which allows us to conclude that Videoscapes provides significant benefits over related methods. Our system leads to previously unseen ways of interactive spatio-temporal exploration of casually captured videos, and we demonstrate this on several video collections.","cites":"17","conferencePercentile":"49.24242424"},{"venue":"ACM Trans. Graph.","id":"c9b38dbce8a42da9b41fd001c3167842d83616c8","venue_1":"ACM Trans. Graph.","year":"2013","title":"Reconstructing detailed dynamic face geometry from monocular video","authors":"Pablo Garrido, Levi Valgaerts, Chenglei Wu, Christian Theobalt","author_ids":"3148360, 1797649, 1682672, 1680185","abstract":"Detailed facial performance geometry can be reconstructed using dense camera and light setups in controlled studios. However, a wide range of important applications cannot employ these approaches, including all movie productions shot from a single principal camera. For post-production, these require dynamic monocular face capture for appearance modification. We present a new method for capturing face geometry from monocular video. Our approach captures detailed, dynamic, spatio-temporally coherent 3D face geometry without the need for markers. It works under uncontrolled lighting, and it successfully reconstructs expressive motion including high-frequency face detail such as folds and laugh lines. After simple manual initialization, the capturing process is fully automatic, which makes it versatile, lightweight and easy-to-deploy. Our approach tracks accurate sparse 2D features between automatically selected key frames to animate a parametric blend shape model, which is further refined in pose, expression and shape by temporally coherent optical flow and photometric stereo. We demonstrate performance capture results for long and complex face sequences captured indoors and outdoors, and we exemplify the relevance of our approach as an enabling technology for model-based face editing in movies and video, such as adding new facial textures, as well as a step towards enabling everyone to do facial performance capture with a single affordable camera.","cites":"48","conferencePercentile":"96.3800905"},{"venue":"ACM Trans. Graph.","id":"8ad807c83c2f930f028906ef4b690adc421f2d1f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Automatic noise modeling for ghost-free HDR reconstruction","authors":"Miguel Granados, Kwang In Kim, James Tompkin, Christian Theobalt","author_ids":"2644804, 1808255, 1854493, 1680185","abstract":"High dynamic range reconstruction of dynamic scenes requires careful handling of dynamic objects to prevent ghosting. However, in a recent review, Srikantha et al. [2012] conclude that \"there is no single best method and the selection of an approach depends on the user's goal\". We attempt to solve this problem with a novel approach that models the noise distribution of color values. We estimate the likelihood that a pair of colors in different images are observations of the same irradiance, and we use a Markov random field prior to reconstruct irradiance from pixels that are likely to correspond to the same static scene object. Dynamic content is handled by selecting a single low dynamic range source image and hand-held capture is supported through homography-based image alignment. Our noise-based reconstruction method achieves better ghost detection and removal than state-of-the-art methods for cluttered scenes with large object displacements. As such, our method is broadly applicable and helps move the field towards a single method for dynamic scene HDR reconstruction.","cites":"9","conferencePercentile":"29.63800905"},{"venue":"ACM Trans. Graph.","id":"21b2651d3760b72b53f05257752a8fa4e080353c","venue_1":"ACM Trans. Graph.","year":"2014","title":"ConstructAide: analyzing and visualizing construction sites through photographs and building models","authors":"David A. Forsyth","author_ids":"1744452","abstract":"We describe a set of tools for analyzing, visualizing, and assessing architectural/construction progress with unordered photo collections and 3D building models. With our interface, a user guides the registration of the model in one of the images, and our system automatically computes the alignment for the rest of the photos using a novel Structure-from-Motion (SfM) technique; images with nearby viewpoints are also brought into alignment with each other. After aligning the photo(s) and model(s), our system allows a user, such as a project manager or facility owner, to explore the construction site seamlessly in time, monitor the progress of construction, assess errors and deviations, and create photorealistic architectural visualizations. These interactions are facilitated by automatic reasoning performed by our system: static and dynamic occlusions are removed automatically, rendering information is collected, and semantic selection tools help guide user input. We also demonstrate that our user-assisted SfM method outperforms existing techniques on both real-world construction data and established multi-view datasets.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"1f831be798282fbded930766742c126920648912","venue_1":"ACM Trans. Graph.","year":"2014","title":"Automatic Scene Inference for 3D Object Compositing","authors":"Kevin Karsch, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, Hailin Jin, Rafael Fonte, Michael Sittig, David A. Forsyth","author_ids":"2732061, 2454127, 1733732, 2211959, 1722322, 2444765, 2827845, 1744452","abstract":"We present a user-friendly image editing system that supports a drag-and-drop object insertion (where the user merely drags objects into the image, and the system automatically places them in 3D and relights them appropriately), postprocess illumination editing, and depth-of-field manipulation. Underlying our system is a fully automatic technique for recovering a comprehensive 3D scene model (geometry, illumination, diffuse albedo, and camera parameters) from a single, low dynamic range photograph. This is made possible by two novel contributions: an illumination inference algorithm that recovers a full lighting model of the scene (including light sources that are not directly visible in the photograph), and a depth estimation algorithm that combines data-driven depth transfer with geometric reasoning about the scene layout. A user study shows that our system produces perceptually convincing results, and achieves the same level of realism as techniques that require significant user interaction.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"6e96d2c3f41c24ab4a5b2ceb7737515278610d13","venue_1":"ACM Trans. Graph.","year":"2013","title":"On-set performance capture of multiple actors with a stereo camera","authors":"Chenglei Wu, Carsten Stoll, Levi Valgaerts, Christian Theobalt","author_ids":"1682672, 1835746, 1797649, 1680185","abstract":"State-of-the-art marker-less performance capture algorithms reconstruct detailed human skeletal motion and space-time coherent surface geometry. Despite being a big improvement over marker-based motion capture methods, they are still rarely applied in practical VFX productions as they require ten or more cameras and a studio with controlled lighting or a green screen background. If one was able to capture performances directly on a general set using only the primary stereo camera used for principal photography, many possibilities would open up in virtual production and previsualization, the creation of virtual actors, and video editing during post-production. We describe a new algorithm which works towards this goal. It is able to track skeletal motion and detailed surface geometry of one or more actors from footage recorded with a stereo rig that is allowed to move. It succeeds in general sets with uncontrolled background and uncontrolled illumination, and scenes in which actors strike non-frontal poses. It is one of the first performance capture methods to exploit detailed BRDF information and scene illumination for accurate pose tracking and surface refinement in general scenes. It also relies on a new foreground segmentation approach that combines appearance, stereo, and pose tracking results to segment out actors from the background. Appearance, segmentation, and motion cues are combined in a new pose optimization framework that is robust under uncontrolled lighting, uncontrolled background and very sparse camera views.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"0a15bb26b01a495eace20eb4f219ecb3ee6e6bcf","venue_1":"ACM Trans. Graph.","year":"2003","title":"Layered acting for character animation","authors":"Mira Dontcheva, Gary D. Yngve, Zoran Popovic","author_ids":"2875493, 2265618, 1696595","abstract":"We introduce an acting-based animation system for creating and editing character animation at interactive speeds. Our system requires minimal training, typically under an hour, and is well suited for rapidly prototyping and creating expressive motion. A real-time motion-capture framework records the user's motions for simultaneous analysis and playback on a large screen. The animator's real-world, expressive motions are mapped into the character's virtual world. Visual feedback maintains a tight coupling between the animator and character. Complex motion is created by layering multiple passes of acting. We also introduce a novel motion-editing technique, which derives implicit relationships between the animator and character. The animator mimics some aspect of the character motion, and the system infers the association between features of the animator's motion and those of the character. The animator modifies the mimic by acting again, and the system maps the changes onto the character. We demonstrate our system with several examples and present the results from informal user studies with expert and novice animators.","cites":"108","conferencePercentile":"60.21505376"},{"venue":"ACM Trans. Graph.","id":"3172e731f7d5a0b15d7f3535222de75fbaf7c98d","venue_1":"ACM Trans. Graph.","year":"2007","title":"Responsive characters from motion fragments","authors":"James McCann, Nancy S. Pollard","author_ids":"2400939, 1735665","abstract":"In game environments, animated character motion must rapidly adapt to changes in player input - for example, if a directional signal from the player's gamepad is not incorporated into the character's trajectory immediately, the character may blithely run off a ledge. Traditional schemes for data-driven character animation lack the split-second reactivity required for this direct control; while they can be made to work, motion artifacts will result. We describe an on-line character animation controller that assembles a motion stream from short motion fragments, choosing each fragment based on current player input and the previous fragment. By adding a simple model of player behavior we are able to improve an existing reinforcement learning method for precalculating good fragment choices. We demonstrate the efficacy of our model by comparing the animation selected by our new controller to that selected by existing methods and to the optimal selection, given knowledge of the entire path. This comparison is performed over real-world data collected from a game prototype. Finally, we provide results indicating that occasional low-quality transitions between motion segments are crucial to high-quality on-line motion generation; this is an important result for others crafting animation systems for directly-controlled characters, as it argues against the common practice of transition thresholding.","cites":"72","conferencePercentile":"66"},{"venue":"ACM Trans. Graph.","id":"6a7d0a1ff422d10d4e0431413f5de6379b862dbc","venue_1":"ACM Trans. Graph.","year":"2003","title":"Realistic modeling of bird flight animations","authors":"Jia-chi Wu, Zoran Popovic","author_ids":"2811720, 1696595","abstract":"In this paper we describe a physics-based method for synthesis of bird flight animations. Our method computes a realistic set of wingbeats that enables a bird to follow the specified trajectory. We model the bird as an articulated skeleton with elastically deformable feathers. The bird motion is created by applying joint torques and aerodynamic forces over time in a forward dynamics simulation. We solve for each wingbeat motion separately by optimizing for wingbeat parameters that create the most natural motion. The final animation is constructed by concatenating a series of optimal wingbeats. This detailed bird flight model enables us to produce flight motions of different birds performing a variety of maneuvers including taking off, cruising, rapidly descending, turning, and landing.","cites":"49","conferencePercentile":"25.2688172"},{"venue":"ACM Trans. Graph.","id":"6afb8792a123b169618a9143bb3a85910ed6ce16","venue_1":"ACM Trans. Graph.","year":"2008","title":"Real-time gradient-domain painting","authors":"James McCann, Nancy S. Pollard","author_ids":"2400939, 1735665","abstract":"We present an image editing program which allows artists to paint in the gradient domain with real-time feedback on megapixel-sized images. Along with a pedestrian, though powerful, gradient-painting brush and gradient-clone tool, we introduce an <i>edge brush</i> designed for edge selection and replay. These brushes, coupled with special blending modes, allow users to accomplish global lighting and contrast adjustments using only local image manipulations --- e.g. strengthening a given edge or removing a shadow boundary. Such operations would be tedious in a conventional intensity-based paint program and hard for users to get right in the gradient domain without real-time feedback. The core of our paint program is a simple-to-implement GPU multigrid method which allows integration of megapixel-sized full-color gradient fields at over 20 frames per second on modest hardware. By way of evaluation, we present example images produced with our program and characterize the iteration time and convergence rate of our integration method.","cites":"42","conferencePercentile":"52.77777778"},{"venue":"ACM Trans. Graph.","id":"4cd9d0f4d66b5cf256cf85decb7b1a9bcf5d979e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Synthesis of Complex Image Appearance from Limited Exemplars","authors":"Olga Diamanti, Connelly Barnes, Sylvain Paris, Eli Shechtman, Olga Sorkine-Hornung","author_ids":"1868022, 1794537, 1720990, 2177801, 2250001","abstract":"Editing materials in photos opens up numerous opportunities like turning an unappealing dirt ground into luscious grass and creating a comfortable wool sweater in place of a cheap t-shirt. However, such edits are challenging. Approaches such as 3D rendering and BTF rendering can represent virtually everything, but they are also data intensive and computationally expensive, which makes user interaction difficult. Leaner methods such as texture synthesis are more easily controllable by artists, but also more limited in the range of materials that they handle, for example, grass and wool are typically problematic because of their non-Lambertian reflectance and numerous self-occlusions. We propose a new approach for editing of complex materials in photographs. We extend the texture-by-numbers approach with ideas from texture interpolation. The inputs to our method are coarse user annotation maps that specify the desired output, such as the local scale of the material and the illumination direction. Our algorithm then synthesizes the output from a discrete set of annotated exemplars. A key component of our method is that it can cope with missing data, interpolating information from the available exemplars when needed. This enables production of satisfying results involving materials with complex appearance variations such as foliage, carpet, and fabric from only one or a couple of exemplar photographs.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"793dd497cb0a95ce6f7ef6cdc9ea06a0ab69f58a","venue_1":"ACM Trans. Graph.","year":"2002","title":"Articulated body deformation from range scan data","authors":"Brett Allen, Brian Curless, Zoran Popovic","author_ids":"8353687, 1810052, 1696595","abstract":"This paper presents an example-based method for calculating skeleton-driven body deformations. Our example data consists of range scans of a human body in a variety of poses. Using markers captured during range scanning, we construct a kinematic skeleton and identify the pose of each scan. We then construct a mutually consistent parameterization of all the scans using a posable subdivision surface template. The detail deformations are represented as displacements from this surface, and holes are filled smoothly within the displacement maps. Finally, we combine the range scans using <i>k</i>-nearest neighbor interpolation in pose space. We demonstrate results for a human upper body with controllable pose, kinematics, and underlying surface shape.","cites":"179","conferencePercentile":"66"},{"venue":"ACM Trans. Graph.","id":"33850e715dfbf05109d6eb8c1745b96b8d5c0115","venue_1":"ACM Trans. Graph.","year":"2011","title":"Video-based characters: creating new human performances from a multi-view video database","authors":"Feng Xu, Yebin Liu, Carsten Stoll, James Tompkin, Gaurav Bharaj, Qionghai Dai, Hans-Peter Seidel, Jan Kautz, Christian Theobalt","author_ids":"1770633, 1680777, 1835746, 1854493, 3090007, 1713158, 1746884, 1690538, 1680185","abstract":"We present a method to synthesize plausible video sequences of humans according to user-defined body motions and viewpoints. We first capture a small database of multi-view video sequences of an actor performing various basic motions. This database needs to be captured only once and serves as the input to our synthesis algorithm. We then apply a marker-less model-based performance capture approach to the entire database to obtain pose and geometry of the actor in each database frame. To create novel video sequences of the actor from the database, a user animates a 3D human skeleton with novel motion and viewpoints. Our technique then synthesizes a realistic video sequence of the actor performing the specified motion based only on the initial database. The first key component of our approach is a new efficient retrieval strategy to find appropriate spatio-temporally coherent database frames from which to synthesize target video frames. The second key component is a warping-based texture synthesis approach that uses the retrieved most-similar database frames to synthesize spatio-temporally coherent target video frames. For instance, this enables us to easily create video sequences of actors performing dangerous stunts without them being placed in harm's way. We show through a variety of result videos and a user study that we can synthesize realistic videos of people, even if the target motions and camera views are different from the database content.","cites":"30","conferencePercentile":"65.52631579"},{"venue":"ACM Trans. Graph.","id":"8920de51f383451d850542fcea3fd590286371ec","venue_1":"ACM Trans. Graph.","year":"2010","title":"Video-based reconstruction of animatable human characters","authors":"Carsten Stoll, Juergen Gall, Edilson de Aguiar, Sebastian Thrun, Christian Theobalt","author_ids":"1835746, 3116266, 2049341, 1738444, 1680185","abstract":"We present a new performance capture approach that incorporates a physically-based cloth model to reconstruct a rigged fully-animatable virtual double of a real person in loose apparel from multi-view video recordings. Our algorithm only requires a minimum of manual interaction. Without the use of optical markers in the scene, our algorithm first reconstructs skeleton motion and detailed time-varying surface geometry of a real person from a reference video sequence. These captured reference performance data are then analyzed to automatically identify non-rigidly deforming pieces of apparel on the animated geometry. For each piece of apparel, parameters of a physically-based real-time cloth simulation model are estimated, and surface geometry of occluded body regions is approximated. The reconstructed character model comprises a skeleton-based representation for the actual body parts and a physically-based simulation model for the apparel. In contrast to previous performance capture methods, we can now also create new real-time animations of actors captured in general apparel.","cites":"34","conferencePercentile":"60.81871345"},{"venue":"ACM Trans. Graph.","id":"6ddd3fcf525ed3c767cddb0985cd1a5f6dff2e55","venue_1":"ACM Trans. Graph.","year":"2010","title":"MovieReshape: tracking and reshaping of humans in videos","authors":"Arjun Jain, Thorsten Thormählen, Hans-Peter Seidel, Christian Theobalt","author_ids":"3255075, 2543070, 1746884, 1680185","abstract":"We present a system for quick and easy manipulation of the body shape and proportions of a human actor in arbitrary video footage. The approach is based on a morphable model of 3D human shape and pose that was learned from laser scans of real people. The algorithm commences by spatio-temporally fitting the pose and shape of this model to the actor in either single-view or multi-view video footage. Once the model has been fitted, semantically meaningful attributes of body shape, such as height, weight or waist girth, can be interactively modified by the user. The changed proportions of the virtual human model are then applied to the actor in all video frames by performing an image-based warping. By this means, we can now conveniently perform spatio-temporal reshaping of human actors in video footage which we show on a variety of video sequences.","cites":"41","conferencePercentile":"70.76023392"},{"venue":"ACM Trans. Graph.","id":"51bee390eeccb5fce23599e2d3ab332608461840","venue_1":"ACM Trans. Graph.","year":"2005","title":"Feature-based surface parameterization and texture mapping","authors":"Eugene Zhang, Konstantin Mischaikow, Greg Turk","author_ids":"1785634, 3077222, 1713189","abstract":"Surface parameterization is necessary for many graphics tasks: texture-preserving simplification, remeshing, surface painting, and precomputation of solid textures. The stretch caused by a given parameterization determines the sampling rate on the surface. In this article, we present an automatic parameterization method for segmenting a surface into patches that are then flattened with little stretch.\n Many objects consist of regions of relatively simple shapes, each of which has a natural parameterization. Based on this observation, we describe a three-stage feature-based patch creation method for manifold surfaces. The first two stages, genus reduction and feature identification, are performed with the help of distance-based surface functions. In the last stage, we create one or two patches for each feature region based on a covariance matrix of the feature's surface points.\n To reduce stretch during patch unfolding, we notice that stretch is a 2 &#215; 2 tensor, which in ideal situations is the identity. Therefore, we use the &#60;i>Green-Lagrange tensor&#60;/i> to measure and to guide the optimization process. Furthermore, we allow the boundary vertices of a patch to be optimized by adding &#60;i>scaffold triangles&#60;/i>. We demonstrate our feature-based patch creation and patch unfolding methods for several textured models.\n Finally, to evaluate the quality of a given parameterization, we describe an image-based error measure that takes into account stretch, seams, smoothness, packing efficiency, and surface visibility.","cites":"114","conferencePercentile":"66.12903226"},{"venue":"ACM Trans. Graph.","id":"03cf95e9195379eda2657561977e888ff01f4210","venue_1":"ACM Trans. Graph.","year":"2005","title":"Water drops on surfaces","authors":"Huamin Wang, Peter J. Mucha, Greg Turk","author_ids":"6798421, 1794748, 1713189","abstract":"We present a physically-based method to enforce contact angles at the intersection of fluid free surfaces and solid objects, allowing us to simulate a variety of small-scale fluid phenomena including water drops on surfaces. The heart of this technique is a <i>virtual surface</i> method, which modifies the level set distance field representing the fluid surface in order to maintain an appropriate contact angle. The surface tension that is calculated on the contact line between the solid surface and liquid surface can then capture all interfacial tensions, including liquid-solid, liquid-air and solid-air tensions. We use a simple dynamic contact angle model to select contact angles according to the solid material property, water history, and the fluid front's motion. Our algorithm robustly and accurately treats various drop shape deformations, and handles both flat and curved solid surfaces. Our results show that our algorithm is capable of realistically simulating several small-scale liquid phenomena such as beading and flattened drops, stretched and separating drops, suspended drops on curved surfaces, and capillary action.","cites":"61","conferencePercentile":"41.93548387"},{"venue":"ACM Trans. Graph.","id":"1cac46b16a2a2e5506c8d8c34605631485fb752f","venue_1":"ACM Trans. Graph.","year":"2004","title":"Rigid fluid: animating the interplay between rigid bodies and fluid","authors":"Mark Carlson, Peter J. Mucha, Greg Turk","author_ids":"4435932, 1794748, 1713189","abstract":"We present the <i>Rigid Fluid</i> method, a technique for animating the interplay between rigid bodies and viscous incompressible fluid with free surfaces. We use distributed Lagrange multipliers to ensure two-way coupling that generates realistic motion for both the solid objects and the fluid as they interact with one another. We call our method the <i>rigid fluid</i> method because the simulator treats the rigid objects as if they were made of fluid. The rigidity of such an object is maintained by identifying the region of the velocity field that is inside the object and constraining those velocities to be rigid body motion. The rigid fluid method is straightforward to implement, incurs very little computational overhead, and can be added as a bridge between current fluid simulators and rigid body solvers. Many solid objects of different densities (<i>e.g.</i>, wood or lead) can be combined in the same animation.","cites":"148","conferencePercentile":"72.82608696"},{"venue":"ACM Trans. Graph.","id":"19da890ee500592eb0534b01f0b2130dd15f6cde","venue_1":"ACM Trans. Graph.","year":"2002","title":"Modelling with implicit surfaces that interpolate","authors":"Greg Turk, James F. O'Brien","author_ids":"1713189, 1692280","abstract":"We introduce new techniques for modelling with <i>interpolating implicit surfaces</i>. This form of implicit surface was first used for problems of surface reconstruction and shape transformation, but the emphasis of our work is on model creation. These implicit surfaces are described by specifying locations in 3D through which the surface should pass, and also identifying locations that are interior or exterior to the surface. A 3D implicit function is created from these constraints using a variational scattered data interpolation approach, and the iso-surface of this function describes a surface. Like other implicit surface descriptions, these surfaces can be used for CSG and interference detection, may be interactively manipulated, are readily approximated by polygonal tilings, and are easy to ray trace. A key strength for model creation is that interpolating implicit surfaces allow the direct specification of both the location of points on the surface and the surface normals. These are two important manipulation techniques that are difficult to achieve using other implicit surface representations such as sums of spherical or ellipsoidal Gaussian functions (\"blobbies\"). We show that these properties make this form of implicit surface particularly attractive for interactive sculpting using the particle sampling technique introduced by Witkin and Heckbert. Our formulation also yields a simple method for converting a polygonal model to a smooth implicit model, as well as a new way to form blends between objects.","cites":"191","conferencePercentile":"69"},{"venue":"ACM Trans. Graph.","id":"c2adb138674a7f75ade139d0e56d7a8e83f0803e","venue_1":"ACM Trans. Graph.","year":"2013","title":"Real-time 3D reconstruction at scale using voxel hashing","authors":"Matthias Nießner, Michael Zollhöfer, Shahram Izadi, Marc Stamminger","author_ids":"2209612, 1699058, 1699068, 1712970","abstract":"Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.","cites":"81","conferencePercentile":"99.54751131"},{"venue":"ACM Trans. Graph.","id":"c7f0747c78fd7306fae4116aed45cbb5e121fd86","venue_1":"ACM Trans. Graph.","year":"2013","title":"Scalable real-time volumetric surface reconstruction","authors":"Jiawen Chen, Dennis Bautembach, Shahram Izadi","author_ids":"1967685, 3171096, 1699068","abstract":"We address the fundamental challenge of <i>scalability</i> for real-time volumetric surface reconstruction methods. We design a memory efficient, hierarchical data structure for commodity graphics hardware, which supports live reconstruction of large-scale scenes with fine geometric details. Our sparse data structure fuses overlapping depth maps from a moving depth camera into a single volumetric representation, from which detailed surface models are extracted. Our hierarchy losslessly streams data bidirectionally between GPU and host, allowing for unbounded reconstructions. Our pipeline, comprised of depth map post-processing, camera pose estimation, volumetric fusion, surface extraction, and streaming, runs entirely in real-time. We experimentally demonstrate that a shallow hierarchy with relatively large branching factors yields the best memory/speed tradeoff, consuming an order of magnitude less memory than a regular grid. We compare an implementation of our data structure to existing methods and demonstrate higher-quality reconstructions on a variety of large-scale scenes, all captured in real-time.","cites":"48","conferencePercentile":"96.3800905"},{"venue":"ACM Trans. Graph.","id":"58c96811a12193f5340c8f8bbb1944b327f7bef9","venue_1":"ACM Trans. Graph.","year":"2014","title":"Learning to be a depth camera for close-range human capture and interaction","authors":"Sean Ryan Fanello, Cem Keskin, Shahram Izadi, Pushmeet Kohli, David Kim, David Sweeney, Antonio Criminisi, Jamie Shotton, Sing Bing Kang, Tim Paek","author_ids":"2219646, 7206941, 1699068, 1685185, 6308833, 2774999, 1716777, 1751781, 1738740, 3049377","abstract":"We present a machine learning technique for estimating absolute, per-pixel depth using any conventional monocular 2D camera, with minor hardware modifications. Our approach targets close-range human capture and interaction where dense 3D estimation of hands and faces is desired. We use hybrid classification-regression forests to <i>learn</i> how to map from near infrared intensity images to <i>absolute</i>, metric depth in real-time. We demonstrate a variety of human-computer interaction and capture scenarios. Experiments show an accuracy that outperforms a conventional light fall-off baseline, and is comparable to high-quality consumer depth cameras, but with a dramatically reduced cost, power consumption, and form-factor.","cites":"19","conferencePercentile":"88.06584362"},{"venue":"ACM Trans. Graph.","id":"97d99b17c1191e9d6f0931ab35f5da216df6b05d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Real-time shading-based refinement for consumer depth cameras","authors":"Shahram Izadi","author_ids":"1699068","abstract":"We present the first real-time method for refinement of depth data using shape-from-shading in general uncontrolled scenes. Per frame, our real-time algorithm takes raw noisy depth data and an aligned RGB image as input, and approximates the time-varying incident lighting, which is then used for geometry refinement. This leads to dramatically enhanced depth maps at 30Hz. Our algorithm makes few scene assumptions, handling arbitrary scene objects even under motion. To enable this type of real-time depth map enhancement, we contribute a new highly parallel algorithm that reformulates the inverse rendering optimization problem in prior work, allowing us to estimate lighting and shape in a temporally coherent way at video frame-rates. Our optimization problem is minimized using a new regular grid Gauss-Newton solver implemented fully on the GPU. We demonstrate results showing enhanced depth maps, which are comparable to offline methods but are computed orders of magnitude faster, as well as baseline comparisons with online filtering-based methods. We conclude with applications of our higher quality depth maps for improved real-time surface reconstruction and performance capture.","cites":"20","conferencePercentile":"88.88888889"},{"venue":"ACM Trans. Graph.","id":"3228631e256eef60067f646567723863dbb89d5b","venue_1":"ACM Trans. Graph.","year":"2009","title":"Local layering","authors":"James McCann, Nancy S. Pollard","author_ids":"2400939, 1735665","abstract":"In a conventional 2d painting or compositing program, graphical objects are stacked in a user-specified global order, as if each were printed on an image-sized sheet of transparent film. In this paper we show how to relax this restriction so that users can make stacking decisions on a per-overlap basis, as if the layers were pictures cut from a magazine. This allows for complex and visually exciting overlapping patterns, without painstaking layer-splitting, depth-value painting, region coloring, or mask-drawing. Instead, users are presented with a layers dialog which acts locally. Behind the scenes, we divide the image into overlap regions and track the ordering of layers in each region. We formalize this structure as a graph of stacking lists, define the set of orderings where layers do not interpenetrate as consistent, and prove that our local stacking operators are both correct and sufficient to reach any consistent stacking. We also provide a method for updating the local stacking when objects change shape or position due to user editing - this scheme prevents layer updates from producing undesired intersections. Our method extends trivially to both animation compositing and local visibility adjustment in depth-peeled 3d scenes; the latter of which allows for the creation of impossible figures which can be viewed and manipulated in real-time.","cites":"8","conferencePercentile":"11.32596685"},{"venue":"ACM Trans. Graph.","id":"924005aa69a813b2bcf0a0a1831b9a5055c2b8db","venue_1":"ACM Trans. Graph.","year":"2015","title":"SemanticPaint: Interactive 3D Labeling and Learning at your Fingertips","authors":"Julien P. C. Valentin, Vibhav Vineet, Ming-Ming Cheng, David Kim, Jamie Shotton, Pushmeet Kohli, Matthias Nießner, Antonio Criminisi, Shahram Izadi, Philip H. S. Torr","author_ids":"2898574, 1683102, 3184935, 6308833, 1751781, 1685185, 2209612, 1716777, 1699068, 7194119","abstract":"We present a new interactive and online approach to 3D scene understanding. Our system, <i>SemanticPaint</i>, allows users to simultaneously scan their environment whilst interactively segmenting the scene simply by reaching out and touching any desired object or surface. Our system continuously learns from these segmentations, and labels new unseen parts of the environment. Unlike offline systems where capture, labeling, and batch learning often take hours or even days to perform, our approach is fully online. This provides users with continuous live feedback of the recognition during capture, allowing to immediately correct errors in the segmentation and/or learning&#8212;a feature that has so far been unavailable to batch and offline methods. This leads to models that are tailored or <i>personalized</i> specifically to the user's environments and object classes of interest, opening up the potential for new applications in augmented reality, interior design, and human/robot navigation. It also provides the ability to capture substantial labeled 3D datasets for training large-scale visual recognition systems.","cites":"11","conferencePercentile":"91.63265306"},{"venue":"ACM Trans. Graph.","id":"36dfe6c7c19904e2740bbba7bb0792a5411c5315","venue_1":"ACM Trans. Graph.","year":"1992","title":"Tree Visualization with Tree-Maps: 2-d Space-Filling Approach","authors":"Ben Shneiderman","author_ids":"1740403","abstract":"Introduction The traditional approach to representing tree structures is as a rooted, directed graph with the root node at the top of the page and children nodes below the parent node with lines connecting them (Figure 1). Knuth (1968, p. 305-313) has a long discussion about this standard representation, especially why the root is at the top and he offers several alternatives including brief mention of a space-filling approach. However, the remainder of his presentation and most other discussions of trees focus on various node and edge representations. By contrast, this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy & Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a-1-2-d rectangular drawing, and hB-trees (Lomet and Salzberg, 1990) which are a more advanced multi-attribute indexing method that has a useful 2-d representation. None of these projects sought to provide human visualization aids for viewing large tree structures. Tree-maps are a representation designed for human visualization of complex traditional tree structures: arbitary trees are shown with a 2-d space-filling representation. The original motivation for this work was to gain a better representation of the utilization of storage space","cites":"540","conferencePercentile":"94.73684211"},{"venue":"ACM Trans. Graph.","id":"26115eed3661e6870b1a136403394795232f8a5d","venue_1":"ACM Trans. Graph.","year":"2015","title":"Real-time pixel luminance optimization for dynamic multi-projection mapping","authors":"Christian Siegl, Matteo Colaianni, Lucas Thies, Justus Thies, Michael Zollhöfer, Shahram Izadi, Marc Stamminger, Frank Bauer","author_ids":"2394458, 1746112, 3079262, 1725037, 1699058, 1699068, 1712970, 8329426","abstract":"Using projection mapping enables us to bring virtual worlds into shared physical spaces. In this paper, we present a novel, adaptable and real-time projection mapping system, which supports multiple projectors and high quality rendering of dynamic content on surfaces of complex geometrical shape. Our system allows for smooth blending across multiple projectors using a new optimization framework that simulates the diffuse direct light transport of the physical world to continuously adapt the color output of each projector pixel. We present a real-time solution to this optimization problem using off-the-shelf graphics hardware, depth cameras and projectors. Our approach enables us to move projectors, depth camera or objects while maintaining the correct illumination, in realtime, without the need for markers on the object. It also allows for projectors to be removed or dynamically added, and provides compelling results with only commodity hardware.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"2ac3def0440c6a50263133e76306494146537e40","venue_1":"ACM Trans. Graph.","year":"2016","title":"Interactively controlled quad remeshing of high resolution 3D models","authors":"Hans-Christian Ebke, Patrick Schmidt, Marcel Campen, Leif Kobbelt","author_ids":"2889614, 2256967, 2304114, 1763010","abstract":"Parametrization based methods have recently become very popular for the generation of high quality quad meshes. In contrast to previous approaches, they allow for intuitive user control in order to accommodate all kinds of application driven constraints and design intentions. A major obstacle in practice, however, are the relatively long computations that lead to response times of several minutes already for input models of moderate complexity. In this paper we introduce a novel strategy to handle highly complex input meshes with up to several millions of triangles such that quad meshes can still be created and edited within an interactive workflow. Our method is based on representing the input model on different levels of resolution with a mechanism to propagate parametrizations from coarser to finer levels. The major challenge is to guarantee consistent parametrizations even in the presence of charts, transition functions, and singularities. Moreover, the remaining degrees of freedom on coarser levels of resolution have to be chosen carefully in order to still achieve low distortion parametrizations. We demonstrate a prototypic system where the user can interactively edit quad meshes with powerful high-level operations such as guiding constraints, singularity repositioning, and singularity connections.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"076be4b491c2051d848fd3c7fd4504d8255bffe5","venue_1":"ACM Trans. Graph.","year":"2010","title":"Street slide: browsing street level imagery","authors":"Johannes Kopf, Billy Chen, Richard Szeliski, Michael F. Cohen","author_ids":"2891193, 8567761, 1717841, 1694613","abstract":"Systems such as Google Street View and Bing Maps Streetside enable users to virtually visit cities by navigating between immersive 360&#176; panoramas, or <i>bubbles.</i> The discrete moves from bubble to bubble enabled in these systems do not provide a good visual sense of a larger aggregate such as a whole city block. Multi-perspective \"strip\" panoramas can provide a visual summary of a city street but lack the full realism of immersive panoramas.\n We present Street Slide, which combines the best aspects of the immersive nature of bubbles with the overview provided by multi-perspective strip panoramas. We demonstrate a seamless transition between bubbles and multi-perspective panoramas. We also present a dynamic construction of the panoramas which overcomes many of the limitations of previous systems. As the user slides sideways, the multi-perspective panorama is constructed and rendered dynamically to simulate either a perspective or <i>hyper-perspective</i> view. This provides a strong sense of parallax, which adds to the immersion. We call this form of sliding sideways while looking at a street fa&#231;ade a <i>street slide.</i> Finally we integrate annotations and a mini-map within the user interface to provide geographic information as well additional affordances for navigation. We demonstrate our Street Slide system on a series of intersecting streets in an urban setting. We report the results of a user study, which shows that visual searching is greatly enhanced with the Street Slide interface over existing systems from Google and Bing.","cites":"36","conferencePercentile":"64.03508772"},{"venue":"ACM Trans. Graph.","id":"2b7dd86e46c1bb369718528b59ac4efc2a422d94","venue_1":"ACM Trans. Graph.","year":"2005","title":"Dual photography","authors":"Pradeep Sen, Billy Chen, Gaurav Garg, Steve Marschner, Mark Horowitz, Marc Levoy, Hendrik P. A. Lensch","author_ids":"2258791, 8567761, 1798782, 2593798, 1764167, 1801789, 1809190","abstract":"We present a novel photographic technique called dual photography, which exploits Helmholtz reciprocity to interchange the lights and cameras in a scene. With a video projector providing structured illumination, reciprocity permits us to generate pictures from the viewpoint of the projector, even though no camera was present at that location. The technique is completely image-based, requiring no knowledge of scene geometry or surface properties, and by its nature automatically includes all transport paths, including shadows, inter-reflections and caustics. In its simplest form, the technique can be used to take photographs without a camera; we demonstrate this by capturing a photograph using a projector and a photo-resistor. If the photo-resistor is replaced by a camera, we can produce a 4D dataset that allows for relighting with 2D incident illumination. Using an array of cameras we can produce a 6D slice of the 8D reflectance field that allows for relighting with arbitrary light fields. Since an array of cameras can operate in parallel without interference, whereas an array of light sources cannot, dual photography is fundamentally a more efficient way to capture such a 6D dataset than a system based on multiple projectors and one camera. As an example, we show how dual photography can be used to capture and relight scenes.","cites":"68","conferencePercentile":"46.37096774"},{"venue":"ACM Trans. Graph.","id":"0b421759f9bbffd05da19b5a58f24dc56a8fc788","venue_1":"ACM Trans. Graph.","year":"2004","title":"Synthetic aperture confocal imaging","authors":"Marc Levoy, Billy Chen, Vaibhav Vaish, Mark Horowitz, Ian McDowall, Mark T. Bolas","author_ids":"1801789, 8567761, 2146731, 1764167, 3153437, 1767669","abstract":"Confocal microscopy is a family of imaging techniques that employ focused patterned illumination and synchronized imaging to create cross-sectional views of 3D biological specimens. In this paper, we adapt confocal imaging to large-scale scenes by replacing the optical apertures used in microscopy with arrays of real or virtual video projectors and cameras. Our prototype implementation uses a video projector, a camera, and an array of mirrors. Using this implementation, we explore confocal imaging of partially occluded environments, such as foliage, and weakly scattering environments, such as murky water. We demonstrate the ability to selectively image any plane in a partially occluded environment, and to see further through murky water than is otherwise possible. By thresholding the confocal images, we extract mattes that can be used to selectively illuminate any plane in the scene.","cites":"119","conferencePercentile":"57.60869565"},{"venue":"ACM Trans. Graph.","id":"4f2a42ee353a74028af4bcd4acf4d1b1989c3fc7","venue_1":"ACM Trans. Graph.","year":"2008","title":"Performance capture from sparse multi-view video","authors":"Edilson de Aguiar, Carsten Stoll, Christian Theobalt, Naveed Ahmed, Hans-Peter Seidel, Sebastian Thrun","author_ids":"2049341, 1835746, 1680185, 8203054, 1746884, 1738444","abstract":"This paper proposes a new marker-less approach to capturing human performances from multi-view video. Our algorithm can jointly reconstruct spatio-temporally coherent geometry, motion and textural surface appearance of actors that perform complex and rapid moves. Furthermore, since our algorithm is purely meshbased and makes as few as possible prior assumptions about the type of subject being tracked, it can even capture performances of people wearing wide apparel, such as a dancer wearing a skirt. To serve this purpose our method efficiently and effectively combines the power of surface- and volume-based shape deformation techniques with a new mesh-based analysis-through-synthesis framework. This framework extracts motion constraints from video and makes the laser-scan of the tracked subject mimic the recorded performance. Also small-scale time-varying shape detail is recovered by applying model-guided multi-view stereo to refine the model surface. Our method delivers captured performance data at high level of detail, is highly versatile, and is applicable to many complex types of scenes that could not be handled by alternative marker-based or marker-free recording techniques.","cites":"238","conferencePercentile":"98.7654321"},{"venue":"ACM Trans. Graph.","id":"0add71f63a196ab1823139eff511641b288a3da5","venue_1":"ACM Trans. Graph.","year":"2009","title":"Real-time prosody-driven synthesis of body language","authors":"Sergey Levine, Christian Theobalt, Vladlen Koltun","author_ids":"1736651, 1680185, 1770944","abstract":"Human communication involves not only speech, but also a wide variety of gestures and body motions. Interactions in virtual environments often lack this multi-modal aspect of communication. We present a method for automatically synthesizing body language animations directly from the participants' speech signals, without the need for additional input. Our system generates appropriate body language animations by selecting segments from motion capture data of real people in conversation. The synthesis can be performed progressively, with no advance knowledge of the utterance, making the system suitable for animating characters from live human speech. The selection is driven by a hidden Markov model and uses prosody-based features extracted from speech. The training phase is fully automatic and does not require hand-labeling of input data, and the synthesis phase is efficient enough to run in real time on live microphone input. User studies confirm that our method is able to produce realistic and compelling body language.","cites":"40","conferencePercentile":"60.22099448"},{"venue":"ACM Trans. Graph.","id":"9a72d3203b93a9d00ed043e03c554710388bfd9f","venue_1":"ACM Trans. Graph.","year":"2000","title":"Image-driven simplification","authors":"Peter Lindstrom, Greg Turk","author_ids":"1682771, 1713189","abstract":"We introduce the notion of <italic>image-driven simplification</italic>, a framework that uses images to decide which portions of a model to simplify. This is a departure from approaches that make polygonal simplification decisions based on geometry. As with many methods, we use the edge collapse operator to make incremental changes to a model. Unique to our approach, however, is the use at comparisons between images of the original model against those of a simplified model to determine the cost of an ease collapse. We use common graphics rendering hardware to accelerate the creation of the required images. As expected, this method produces models that are close to the original model according to image differences. Perhaps more surprising, however, is that the method yields models that have high geometric fidelity as well. Our approach also solves the quandary of how to weight the geometric distance versus appearance properties such as normals, color, and texture. All of these trade-offs are balanced by  the image metric. Benefits of this approach include high fidelity silhouettes, extreme simplification  of hidden portions of a model, attention to shading interpolation effects, and simplification that is sensitive to the content of a texture. In order to better preserve the appearance of textured models, we introduce a novel technique for assigning texture coordinates to the new vertices of the mesh. This method is based on a geometric heuristic that can be integrated with any edge collapse algorithm to produce high quality textured surfaces.","cites":"111","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"2dd2f96ca6f8416883c9683e80ca51b20439ac04","venue_1":"ACM Trans. Graph.","year":"2005","title":"Texture transfer during shape transformation","authors":"Huong Quynh Dinh, Anthony J. Yezzi, Greg Turk","author_ids":"2720069, 1745521, 1713189","abstract":"Mappings between surfaces have a variety of uses, including texture transfer, multi-way morphing, and surface analysis. Given a 4D implicit function that defines a morph between two implicit surfaces, this article presents a method of calculating a mapping between the two surfaces. We create such a mapping by solving two PDEs over a tetrahedralized hypersurface that connects the two surfaces in 4D. Solving the first PDE yields a vector field that indicates how points on one surface flow to the other. Solving the second PDE propagates position labels along this vector field so that the second surface is tagged with a unique position on the first surface. One strength of this method is that it produces correspondences between surfaces even when they have different topologies. Even if the surfaces split apart or holes appear, the method still produces a mapping entirely automatically. We demonstrate the use of this approach to transfer texture between two surfaces that may have differing topologies.","cites":"10","conferencePercentile":"4.838709677"},{"venue":"ACM Trans. Graph.","id":"46d39b0e21ae956e4bcb7a789f92be480d45ee12","venue_1":"ACM Trans. Graph.","year":"2003","title":"The space of human body shapes: reconstruction and parameterization from range scans","authors":"Brett Allen, Brian Curless, Zoran Popovic","author_ids":"8353687, 1810052, 1696595","abstract":"We develop a novel method for fitting high-resolution template meshes to detailed human body range scans with sparse 3D markers. We formulate an optimization problem in which the degrees of freedom are an affine transformation at each template vertex. The objective function is a weighted combination of three measures: proximity of transformed vertices to the range data, similarity between neighboring transformations, and proximity of sparse markers at corresponding locations on the template and target surface. We solve for the transformations with a non-linear optimizer, run at two resolutions to speed convergence. We demonstrate reconstruction and consistent parameterization of 250 human body models. With this parameterized set, we explore a variety of applications for human body modeling, including: morphing, texture transfer, statistical analysis of shape, model fitting from sparse markers, feature analysis to modify multiple correlated parameters (such as the weight and height of an individual), and transfer of surface detail and animation controls from a template to fitted models.","cites":"377","conferencePercentile":"94.62365591"},{"venue":"ACM Trans. Graph.","id":"47ae44f066a4e98a9f59c7e7eb94babb011bd5a8","venue_1":"ACM Trans. Graph.","year":"2016","title":"Automatic Photo Adjustment Using Deep Neural Networks","authors":"Zhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris, Yizhou Yu","author_ids":"3305169, 1694161, 2450889, 1720990, 7877246","abstract":"Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work, but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep learning has shown unique abilities to address hard problems. This motivated us to explore the use of deep neural networks (DNNs) in the context of photo editing. In this article, we formulate automatic photo adjustment in a manner suitable for this approach. We also introduce an image descriptor accounting for the local semantics of an image. Our experiments demonstrate that training DNNs using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on image semantics. We show that this yields results that are qualitatively and quantitatively better than previous work.","cites":"9","conferencePercentile":"99.57805907"},{"venue":"ACM Trans. Graph.","id":"f5f470993c9028cd2f4dffd914c74085e5c8d0f9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Corrective 3D reconstruction of lips from monocular video","authors":"Pablo Garrido, Michael Zollhöfer, Chenglei Wu, Derek Bradley, Patrick Pérez, Thabo Beeler, Christian Theobalt","author_ids":"3148360, 1699058, 1682672, 1745149, 1799777, 2486770, 1680185","abstract":"In facial animation, the accurate shape and motion of the lips of virtual humans is of paramount importance, since subtle nuances in mouth expression strongly influence the interpretation of speech and the conveyed emotion. Unfortunately, passive photometric reconstruction of expressive lip motions, such as a kiss or rolling lips, is fundamentally hard even with multi-view methods in controlled studios. To alleviate this problem, we present a novel approach for fully automatic reconstruction of detailed and expressive lip shapes along with the dense geometry of the entire face, from just monocular RGB video. To this end, we learn the difference between inaccurate lip shapes found by a state-of-the-art monocular facial performance capture approach, and the true 3D lip shapes reconstructed using a high-quality multi-view system in combination with applied lip tattoos that are easy to track. A robust gradient domain regressor is trained to infer accurate lip shapes from coarse monocular reconstructions, with the additional help of automatically extracted inner and outer 2D lip contours. We quantitatively and qualitatively show that our monocular approach reconstructs higher quality lip shapes, even for complex shapes like a kiss or lip rolling, than previous monocular approaches. Furthermore, we compare the performance of person-specific and multi-person generic regression strategies and show that our approach generalizes to new individuals and general scenes, enabling high-fidelity reconstruction even from commodity video footage.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"0cff6406c1e3290f8b24075b573d4f5082de8229","venue_1":"ACM Trans. Graph.","year":"2013","title":"PatchNet: a patch-based image representation for interactive library-driven image editing","authors":"Shi-Min Hu, Fang-Lue Zhang, Miao Wang, Ralph R. Martin, Jue Wang","author_ids":"1686809, 3326435, 1693285, 4326042, 1718812","abstract":"We introduce <i>PatchNets</i>, a compact, hierarchical representation describing structural and appearance characteristics of image regions, for use in image editing. In a PatchNet, an image region with coherent appearance is summarized by a graph node, associated with a single representative patch, while geometric relationships between different regions are encoded by labelled graph edges giving contextual information. The hierarchical structure of a PatchNet allows a coarse-to-fine description of the image. We show how this PatchNet representation can be used as a basis for interactive, library-driven, image editing. The user draws rough sketches to quickly specify editing constraints for the target image. The system then automatically queries an image library to find semantically-compatible candidate regions to meet the editing goal. Contextual image matching is performed using the PatchNet representation, allowing suitable regions to be found and applied in a few seconds, even from a library containing thousands of images.","cites":"12","conferencePercentile":"45.47511312"},{"venue":"ACM Trans. Graph.","id":"527425e4e4c036368a9c33e616f834fccf19540c","venue_1":"ACM Trans. Graph.","year":"2013","title":"Inverse image editing: recovering a semantic editing history from a before-and-after image pair","authors":"Shi-Min Hu, Kun Xu, Li-Qian Ma, Bin Liu, Bi-Ye Jiang, Jue Wang","author_ids":"1686809, 1750039, 1678872, 1702756, 2270274, 1718812","abstract":"We study the problem of <i>inverse image editing</i>, which recovers a semantically-meaningful editing history from a source image and an edited copy. Our approach supports a wide range of commonly-used editing operations such as cropping, object insertion and removal, linear and non-linear color transformations, and spatially-varying adjustment brushes. Given an input image pair, we first apply a dense correspondence method between them to match edited image regions with their sources. For each edited region, we determine geometric and semantic appearance operations that have been applied. Finally, we compute an optimal editing path from the region-level editing operations, based on predefined semantic constraints. The recovered history can be used in various applications such as image re-editing, edit transfer, and image revision control. A user study suggests that the editing histories generated from our system are semantically comparable to the ones generated by artists.","cites":"7","conferencePercentile":"21.26696833"},{"venue":"ACM Trans. Graph.","id":"2bb286576fab7ea82ac2b8e91243e5f7d6f2b0ce","venue_1":"ACM Trans. Graph.","year":"2014","title":"EZ-sketching: three-level optimization for error-tolerant image tracing","authors":"Qingkun Su, Wing Ho Andy Li, Jue Wang, Hongbo Fu","author_ids":"1842335, 1726018, 1718812, 1691065","abstract":"We present a new image-guided drawing interface called <i>EZ-Sketching</i>, which uses a tracing paradigm and automatically corrects sketch lines roughly traced over an image by analyzing and utilizing the image features being traced. While previous edge snapping methods aim at optimizing individual strokes, we show that a co-analysis of multiple roughly placed nearby strokes better captures the user's intent. We formulate automatic sketch improvement as a three-level optimization problem and present an efficient solution to it. EZ-Sketching can tolerate errors from various sources such as indirect control and inherently inaccurate input, and works well for sketching on touch devices with small screens using fingers. Our user study confirms that the drawings our approach helped generate show closer resemblance to the traced images, and are often aesthetically more pleasing.","cites":"3","conferencePercentile":"13.78600823"},{"venue":"ACM Trans. Graph.","id":"0774a972aee4c780ba97c319bc530d429552d5a9","venue_1":"ACM Trans. Graph.","year":"2007","title":"Near-optimal character animation with continuous control","authors":"Adrien Treuille, Yongjoon Lee, Zoran Popovic","author_ids":"3064395, 1770155, 1696595","abstract":"We present a new approach to realtime character animation with interactive control. Given a corpus of motion capture data and a desired task, we automatically compute near-optimal controllers using a low-dimensional basis representation. We show that these controllers produce motion that fluidly responds to several dimensions of user control and environmental constraints in realtime. Our results indicate that very few basis functions are required to create high-fidelity character controllers which permit complex user navigation and obstacle-avoidance tasks.","cites":"109","conferencePercentile":"81.6"},{"venue":"ACM Trans. Graph.","id":"27e6044889e6ff30395490b38d7e0346ef883ba4","venue_1":"ACM Trans. Graph.","year":"2014","title":"TrackCam: 3D-aware tracking shots from consumer video","authors":"Shuaicheng Liu, Jue Wang, Sunghyun Cho, Ping Tan","author_ids":"2202149, 1718812, 7942592, 1911264","abstract":"Panning and tracking shots are popular photography techniques in which the camera tracks a moving object and keeps it at the same position, resulting in an image where the moving foreground is sharp but the background is blurred accordingly, creating an artistic illustration of the foreground motion. Such shots however are hard to capture even for professionals, especially when the foreground motion is complex (e.g., non-linear motion trajectories).\n In this work we propose a system to generate realistic, 3D-aware tracking shots from consumer videos. We show how computer vision techniques such as segmentation and structure-from-motion can be used to lower the barrier and help novice users create high quality tracking shots that are physically plausible. We also introduce a pseudo 3D approach for relative depth estimation to avoid expensive 3D reconstruction for improved robustness and a wider application range. We validate our system through extensive quantitative and qualitative evaluations.","cites":"1","conferencePercentile":"4.938271605"},{"venue":"ACM Trans. Graph.","id":"b027a53dc5daf36153d3623ff92de15a14688c18","venue_1":"ACM Trans. Graph.","year":"2016","title":"Robust background identification for dynamic video editing","authors":"Fang-Lue Zhang, Xian Wu, Hao-Tian Zhang, Jue Wang, Shi-Min Hu","author_ids":"3326435, 1738906, 2194444, 1718812, 1686809","abstract":"Extracting background features for estimating the camera path is a key step in many video editing and enhancement applications. Existing approaches often fail on highly dynamic videos that are shot by moving cameras and contain severe foreground occlusion. Based on existing theories, we present a new, practical method that can reliably identify background features in complex video, leading to accurate camera path estimation and background layering. Our approach contains a local motion analysis step and a global optimization step. We first divide the input video into overlapping temporal windows, and extract local motion clusters in each window. We form a directed graph from these local clusters, and identify background ones by finding a minimal path through the graph using optimization. We show that our method significantly outperforms other alternatives, and can be directly used to improve common video editing applications such as stabilization, compositing and background reconstruction.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"e7d7761ba516340457488955f3dd5959761bbfd0","venue_1":"ACM Trans. Graph.","year":"2016","title":"Automatic triage for a photo series","authors":"Huiwen Chang, Fisher Yu, Jue Wang, Douglas Ashley, Adam Finkelstein","author_ids":"2914394, 1807197, 1718812, 3439595, 1707541","abstract":"People often take a series of nearly redundant pictures to capture a moment or scene. However, selecting photos to keep or share from a large collection is a painful chore. To address this problem, we seek a relative quality measure within a series of photos taken of the same scene, which can be used for automatic photo triage. Towards this end, we gather a large dataset comprised of photo series distilled from personal photo albums. The dataset contains 15, 545 unedited photos organized in 5,953 series. By augmenting this dataset with ground truth human preferences among photos within each series, we establish a benchmark for measuring the effectiveness of algorithmic models of how people select photos. We introduce several new approaches for modeling human preference based on machine learning. We also describe applications for the dataset and predictor, including a smart album viewer, automatic photo enhancement, and providing overviews of video clips.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"6bd21bce5378cdecdb59c91ccc0f3e140b2ed26d","venue_1":"ACM Trans. Graph.","year":"2007","title":"Implicit visibility and antiradiance for interactive global illumination","authors":"Carsten Dachsbacher, Marc Stamminger, George Drettakis, Frédo Durand","author_ids":"1705803, 1712970, 1721779, 1728125","abstract":"We reformulate the rendering equation to alleviate the need for explicit visibility computation, thus enabling interactive global illumination on graphics hardware. This is achieved by treating visibility <i>implicitly</i> and propagating an additional quantity, called <i>antiradiance</i>, to compensate for light transmitted extraneously. Our new algorithm shifts visibility computation to simple local iterations by maintaining additional directional antiradiance information with samples in the scene. It is easy to parallelize on a GPU. By correctly treating discretization and filtering, we can compute indirect illumination in scenes with dynamic objects much faster than traditional methods. Our results show interactive update of indirect illumination with moving characters and lights.","cites":"63","conferencePercentile":"60.4"},{"venue":"ACM Trans. Graph.","id":"4488813572e96dd9059a3bf153dd2b9ad2d19bc4","venue_1":"ACM Trans. Graph.","year":"2009","title":"Compact character controllers","authors":"Yongjoon Lee, Seong Jae Lee, Zoran Popovic","author_ids":"1770155, 2461469, 1696595","abstract":"We present methods for creating compact and efficient data-driven character controllers. Our first method identifies the essential motion data examples tailored for a given task. It enables complex yet efficient high-dimensional controllers, as well as automatically generated connecting controllers that merge a set of independent controllers into a much larger aggregate one without modifying existing ones. Our second method iteratively refines basis functions to enable highly complex value functions. We show that our methods dramatically reduce the computation and storage requirement of controllers and enable very complex behaviors.","cites":"21","conferencePercentile":"29.83425414"},{"venue":"ACM Trans. Graph.","id":"147ae2e23e74e80cd09bf60314945141db64de6e","venue_1":"ACM Trans. Graph.","year":"2009","title":"Optimal gait and form for animal locomotion","authors":"Kevin Wampler, Zoran Popovic","author_ids":"3013193, 1696595","abstract":"We present a fully automatic method for generating gaits and morphologies for legged animal locomotion. Given a specific animal's shape we can determine an efficient gait with which it can move. Similarly, we can also adapt the animal's morphology to be optimal for a specific locomotion task. We show that determining such gaits is possible without the need to specify a good initial motion, and without manually restricting the allowed gaits of each animal. Our approach is based on a hybrid optimization method which combines an efficient derivative-aware spacetime constraints optimization with a derivative-free approach able to find non-local solutions in high-dimensional discontinuous spaces. We demonstrate the effectiveness of this approach by synthesizing dynamic locomotions of bipeds, a quadruped, and an imaginary five-legged creature.","cites":"60","conferencePercentile":"79.83425414"},{"venue":"ACM Trans. Graph.","id":"5fd1ecd5c0f67fad45d72e67f9c4c1eeaea7415f","venue_1":"ACM Trans. Graph.","year":"2009","title":"Contact-aware nonlinear control of dynamic characters","authors":"Uldarico Muico, Yongjoon Lee, Jovan Popovic, Zoran Popovic","author_ids":"1787011, 1770155, 1731389, 1696595","abstract":"Dynamically simulated characters are difficult to control because they are underactuated---they have no direct control over their global position and orientation. In order to succeed, control policies must look ahead to determine stabilizing actions, but such planning is complicated by frequent ground contacts that produce a discontinuous search space. This paper introduces a locomotion system that generates high-quality animation of agile movements using nonlinear controllers that plan through such contact changes. We demonstrate the general applicability of this approach by emulating walking and running motions in rigid-body simulations. Then we consolidate these controllers under a higher-level planner that interactively controls the character's direction.","cites":"78","conferencePercentile":"89.77900552"},{"venue":"ACM Trans. Graph.","id":"3e944c147bdaa3d5fa0e99fb1843df319da5e266","venue_1":"ACM Trans. Graph.","year":"2010","title":"Terrain-adaptive bipedal locomotion control","authors":"Jia-chi Wu, Zoran Popovic","author_ids":"2811720, 1696595","abstract":"We describe a framework for the automatic synthesis of biped locomotion controllers that adapt to uneven terrain at run-time. The framework consists of two components: a per-footstep end-effector path planner and a per-timestep generalized-force solver. At the start of each footstep, the planner performs short-term planning in the space of end-effector trajectories. These trajectories adapt to the interactive task goals and the features of the surrounding uneven terrain at run-time. We solve for the parameters of the planner for different tasks in offline optimizations. Using the per-footstep plan, the generalized-force solver takes ground contacts into consideration and solves a quadratic program at each simulation timestep to obtain joint torques that drive the biped. We demonstrate the capabilities of the controllers in complex navigation tasks where they perform gradual or sharp turns and transition between moving forwards, backwards, and sideways on uneven terrain (including hurdles and stairs) according to the interactive task goals. We also show that the resulting controllers are capable of handling morphology changes to the character.","cites":"40","conferencePercentile":"69.29824561"},{"venue":"ACM Trans. Graph.","id":"aab9ee829c871af99ca5c91dbf594ad9e0674a79","venue_1":"ACM Trans. Graph.","year":"2010","title":"Character animation in two-player adversarial games","authors":"Kevin Wampler, Erik Andersen, Evan Herbst, Yongjoon Lee, Zoran Popovic","author_ids":"3013193, 3007073, 6376655, 1770155, 1696595","abstract":"The incorporation of randomness is critical for the believability and effectiveness of controllers for characters in competitive games. We present a fully automatic method for generating intelligent real-time controllers for characters in such a game. Our approach uses game theory to deal with the ramifications of the characters acting simultaneously, and generates controllers which employ both long-term planning and an intelligent use of randomness. Our results exhibit nuanced strategies based on unpredictability, such as feints and misdirection moves, which take into account and exploit the possible strategies of an adversary. The controllers are generated by examining the interaction between the rules of the game and the motions generated from a parametric motion graph. This involves solving a large-scale planning problem, so we also describe a new technique for scaling this process to higher dimensions.","cites":"11","conferencePercentile":"14.03508772"},{"venue":"ACM Trans. Graph.","id":"09c292960575b4c15d54331db34c4e7592cb9235","venue_1":"ACM Trans. Graph.","year":"2010","title":"Learning behavior styles with inverse reinforcement learning","authors":"Seong Jae Lee, Zoran Popovic","author_ids":"2461469, 1696595","abstract":"We present a method for inferring the behavior styles of character controllers from a small set of examples. We show that a rich set of behavior variations can be captured by determining the appropriate reward function in the reinforcement learning framework, and show that the discovered reward function can be applied to different environments and scenarios. We also introduce a new algorithm to recover the unknown reward function that improves over the original apprenticeship learning algorithm. We show that the reward function representing a behavior style can be applied to a variety of different tasks, while still preserving the key features of the style present in the given examples. We describe an adaptive process where an author can, with just a few additional examples, refine the behavior so that it has better generalization properties.","cites":"10","conferencePercentile":"11.40350877"},{"venue":"ACM Trans. Graph.","id":"1b07de1c252e3a17cbd4f471562b76d11ef60248","venue_1":"ACM Trans. Graph.","year":"2012","title":"Design-driven quadrangulation of closed 3D curves","authors":"Mikhail Bessmeltsev, Caoyu Wang, Alla Sheffer, Karan Singh","author_ids":"2455246, 2845236, 3354923, 1682205","abstract":"We propose a novel, design-driven, approach to quadrangulation of closed 3D curves created by sketch-based or other curve modeling systems. Unlike the multitude of approaches for quad-remeshing of existing surfaces, we rely solely on the input curves to both conceive and construct the quad-mesh of an artist imagined surface bounded by them. We observe that viewers complete the intended shape by envisioning a dense network of smooth, gradually changing, <i>flow-lines</i> that interpolates the input curves. Components of the network bridge pairs of input curve segments with similar orientation and shape. Our algorithm mimics this behavior. It first segments the input closed curves into pairs of <i>matching</i> segments, defining dominant flow line sequences across the surface. It then interpolates the input curves by a network of quadrilateral cycles whose iso-lines define the desired flow line network. We proceed to interpolate these networks with all-quad meshes that convey designer intent. We evaluate our results by showing convincing quadrangulations of complex and diverse curve networks with concave, non-planar cycles, and validate our approach by comparing our results to artist generated interpolating meshes.","cites":"19","conferencePercentile":"56.81818182"},{"venue":"ACM Trans. Graph.","id":"431688ee595f07f7127265634c255a2c93e1b8f9","venue_1":"ACM Trans. Graph.","year":"2011","title":"Space-time planning with parameterized locomotion controllers","authors":"Sergey Levine, Yongjoon Lee, Vladlen Koltun, Zoran Popovic","author_ids":"1736651, 1770155, 1770944, 1696595","abstract":"We present a technique for efficiently synthesizing animations for characters traversing complex dynamic environments. Our method uses parameterized locomotion controllers that correspond to specific motion skills, such as jumping or obstacle avoidance. The controllers are created from motion capture data with reinforcement learning. A space-time planner determines the sequence in which controllers must be executed to reach a goal location, and admits a variety of cost functions to produce paths that exhibit different behaviors. By planning in space and time, the planner can discover paths through dynamically changing environments, even if no path exists in any static snapshot. By using parameterized controllers able to handle navigational tasks, the planner can operate efficiently at a high level, leading to interactive replanning rates.","cites":"15","conferencePercentile":"34.47368421"},{"venue":"ACM Trans. Graph.","id":"bc787830e2a4337fe97f424a8d7e6e9a92598175","venue_1":"ACM Trans. Graph.","year":"2016","title":"Live intrinsic video","authors":"Abhimitra Meka, Michael Zollhöfer, Christian Richardt, Christian Theobalt","author_ids":"1953101, 1699058, 7240032, 1680185","abstract":"Intrinsic video decomposition refers to the fundamentally ambiguous task of separating a video stream into its constituent layers, in particular reflectance and shading layers. Such a decomposition is the basis for a variety of video manipulation applications, such as realistic recoloring or retexturing of objects. We present a novel variational approach to tackle this underconstrained inverse problem at real-time frame rates, which enables on-line processing of live video footage. The problem of finding the intrinsic decomposition is formulated as a mixed variational <i>&ell;</i><sub>2</sub>-<i>&ell;</i><sub><i>p</i></sub>-optimization problem based on an objective function that is specifically tailored for fast optimization. To this end, we propose a novel combination of sophisticated local spatial and global spatio-temporal priors resulting in temporally coherent decompositions at real-time frame rates without the need for explicit correspondence search. We tackle the resulting high-dimensional, non-convex optimization problem via a novel data-parallel iteratively reweighted least squares solver that runs on commodity graphics hardware. Real-time performance is obtained by combining a local-global solution strategy with hierarchical coarse-to-fine optimization. Compelling real-time augmented reality applications, such as recoloring, material editing and retexturing, are demonstrated in a live setup. Our qualitative and quantitative evaluation shows that we obtain high-quality real-time decompositions even for challenging sequences. Our method is able to outperform state-of-the-art approaches in terms of runtime <i>and</i> result quality -- even without user guidance such as scribbles.","cites":"6","conferencePercentile":"98.52320675"},{"venue":"ACM Trans. Graph.","id":"59564bc937ebdfc7c599d20bb05a9b4511d8f2ea","venue_1":"ACM Trans. Graph.","year":"2013","title":"A no-reference metric for evaluating the quality of motion deblurring","authors":"Yiming Liu, Jue Wang, Sunghyun Cho, Adam Finkelstein, Szymon Rusinkiewicz","author_ids":"2055258, 1718812, 7942592, 1707541, 7723706","abstract":"Methods to undo the effects of motion blur are the subject of intense research, but evaluating and tuning these algorithms has traditionally required either user input or the availability of ground-truth images. We instead develop a metric for automatically predicting the perceptual quality of images produced by state-of-the-art deblurring algorithms. The metric is learned based on a massive user study, incorporates features that capture common deblurring artifacts, and does not require access to the original images (i.e., is \"noreference\"). We show that it better matches user-supplied rankings than previous approaches to measuring quality, and that in most cases it outperforms conventional full-reference image-similarity measures. We demonstrate applications of this metric to automatic selection of optimal algorithms and parameters, and to generation of fused images that combine multiple deblurring results.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"12e241823670b825167bae8994874aa0db816b5f","venue_1":"ACM Trans. Graph.","year":"2009","title":"Interactive reflection editing","authors":"Tobias Ritschel, Makoto Okabe, Thorsten Thormählen, Hans-Peter Seidel","author_ids":"1759347, 3102663, 2543070, 1746884","abstract":"Effective digital content creation tools must be both efficient in the interactions they provide but also allow full user control. There may be occasions, when art direction requires changes that contradict physical laws. In particular, it is known that physical correctness of reflections for the human observer is hard to assess. For many centuries, traditional artists have exploited this fact to depict reflections that lie outside the realm of physical possibility. However, a system that gives explicit control of this effect to digital artists has not yet been described. This paper introduces a system that transforms physically correct reflections into art-directed reflections, as specified by <i>reflection constraints</i>. The system introduces a taxonomy of reflection editing operations, using an intuitive user interface, that works directly on the reflecting surfaces with real-time visual feedback using a GPU. A user study shows how such a system can allow users to quickly manipulate reflections according to an art direction task.","cites":"8","conferencePercentile":"11.32596685"},{"venue":"ACM Trans. Graph.","id":"062a2de4a7a129026076be867b20785e4ffe4da3","venue_1":"ACM Trans. Graph.","year":"2004","title":"Near-regular texture analysis and manipulation","authors":"Yanxi Liu, Wen-Chieh Lin, James Hays","author_ids":"1689241, 2019904, 2151506","abstract":"A near-regular texture deviates geometrically and photometrically from a regular congruent tiling. Although near-regular textures are ubiquitous in the man-made and natural world, they present computational challenges for state of the art texture analysis and synthesis algorithms. Using regular tiling as our anchor point, and with user-assisted lattice extraction, we can explicitly model the deformation of a near-regular texture with respect to geometry, lighting and color. We treat a deformation field both as a function that acts on a texture and as a texture that is acted upon, and develop a multi-modal framework where each deformation field is subject to analysis, synthesis and manipulation. Using this formalization, we are able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity.","cites":"151","conferencePercentile":"73.91304348"},{"venue":"ACM Trans. Graph.","id":"532c4a3c8bb15db9e72802c92f6694aa29b659b7","venue_1":"ACM Trans. Graph.","year":"2015","title":"Shading-based refinement on volumetric signed distance functions","authors":"Michael Zollhöfer, Angela Dai, Matthias Innmann, Chenglei Wu, Marc Stamminger, Christian Theobalt, Matthias Nießner","author_ids":"1699058, 2208531, 2873438, 1682672, 1712970, 1680185, 2209612","abstract":"We present a novel method to obtain fine-scale detail in 3D reconstructions generated with low-budget RGB-D cameras or other commodity scanning devices. As the depth data of these sensors is noisy, truncated signed distance fields are typically used to regularize out the noise, which unfortunately leads to over-smoothed results. In our approach, we leverage RGB data to refine these reconstructions through shading cues, as color input is typically of much higher resolution than the depth data. As a result, we obtain reconstructions with high geometric detail, far beyond the depth resolution of the camera itself. Our core contribution is shading-based refinement directly on the implicit surface representation, which is generated from globally-aligned RGB-D images. We formulate the inverse shading problem on the volumetric distance field, and present a novel objective function which jointly optimizes for fine-scale surface geometry and spatially-varying surface reflectance. In order to enable the efficient reconstruction of sub-millimeter detail, we store and process our surface using a sparse voxel hashing scheme which we augment by introducing a grid hierarchy. A tailored GPU-based Gauss-Newton solver enables us to refine large shape models to previously unseen resolution within only a few seconds.","cites":"11","conferencePercentile":"91.63265306"},{"venue":"ACM Trans. Graph.","id":"5908671e46e1fc256c45cc37f5c03673503d26d7","venue_1":"ACM Trans. Graph.","year":"2005","title":"Learning physics-based motion style with nonlinear inverse optimization","authors":"C. Karen Liu, Aaron Hertzmann, Zoran Popovic","author_ids":"1688533, 1747779, 1696595","abstract":"This paper presents a novel physics-based representation of realistic character motion. The dynamical model incorporates several factors of locomotion derived from the biomechanical literature, including relative preferences for using some muscles more than others. elastic mechanisms at joints due to the mechanical properties of tendons, ligaments, and muscles, and variable stiffness at joints depending on the task. When used in a spacetime optimization framework, the parameters of this model define a wide range of styles of natural human movement.Due to the complexity of biological motion, these style parameters are too difficult to design by hand. To address this, we introduce Nonlinear Inverse Optimization, a novel algorithm for estimating optimization parameters from motion capture data. Our method can extract the physical parameters from a single short motion sequence. Once captured, this representation of style is extremely flexible: motions can be generated in the same style but performing different tasks, and styles may be edited to change the physical properties of the body.","cites":"170","conferencePercentile":"88.30645161"},{"venue":"ACM Trans. Graph.","id":"00e79a637dbcd436f72102187b3dee473dc49ab4","venue_1":"ACM Trans. Graph.","year":"2005","title":"Interactive video cutout","authors":"Jue Wang, Pravin Bhat, Alex Colburn, Maneesh Agrawala, Michael F. Cohen","author_ids":"1718812, 2300778, 5945803, 1820412, 1694613","abstract":"We present an interactive system for efficiently extracting foreground objects from a video. We extend previous min-cut based image segmentation techniques to the domain of video with four new contributions. We provide a novel painting-based user interface that allows users to easily indicate the foreground object across space and time. We introduce a hierarchical mean-shift preprocess in order to minimize the number of nodes that min-cut must operate on. Within the min-cut we also define new local cost functions to augment the global costs defined in earlier work. Finally, we extend 2D alpha matting methods designed for images to work with 3D video volumes. We demonstrate that our matting approach preserves smoothness across both space and time. Our interactive video cutout system allows users to quickly extract foreground objects from video sequences for use in a variety of applications including compositing onto new backgrounds and NPR cartoon style rendering.","cites":"205","conferencePercentile":"91.53225806"},{"venue":"ACM Trans. Graph.","id":"1ea5f15b81087f5d5e7fc8f06caa108978af3ed3","venue_1":"ACM Trans. Graph.","year":"2004","title":"Multi-finger gestural interaction with 3D volumetric displays","authors":"Tovi Grossman, Daniel J. Wigdor, Ravin Balakrishnan","author_ids":"3313809, 1961958, 1748870","abstract":"Volumetric displays provide interesting opportunities and challenges for 3D interaction and visualization, particularly when used in a highly interactive manner. We explore this area through the design and implementation of techniques for interactive direct manipulation of objects with a 3D volumetric display. Motion tracking of the user's fingers provides for direct gestural interaction with the virtual objects, through manipulations on and around the display's hemispheric enclosure. Our techniques leverage the unique features of volumetric displays, including a 360&#176; viewing volume that enables manipulation from any viewpoint around the display, as well as natural and accurate perception of true depth information in the displayed 3D scene. We demonstrate our techniques within a prototype 3D geometric model building application.","cites":"106","conferencePercentile":"53.80434783"},{"venue":"ACM Trans. Graph.","id":"4b21c238d8424956d4e12a3ff0f56be79e61597c","venue_1":"ACM Trans. Graph.","year":"2015","title":"Real-time expression transfer for facial reenactment","authors":"Justus Thies, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, Christian Theobalt","author_ids":"1725037, 1699058, 2209612, 1797649, 1712970, 1680185","abstract":"We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor. The novelty of our approach lies in the transfer and photorealistic re-rendering of facial deformations and detail into the target video in a way that the newly-synthesized expressions are virtually indistinguishable from a real video. To achieve this, we accurately capture the facial performances of the source and target subjects in real-time using a commodity RGB-D sensor. For each frame, we jointly fit a parametric model for identity, expression, and skin reflectance to the input color and depth data, and also reconstruct the scene lighting. For expression transfer, we compute the difference between the source and target expressions in parameter space, and modify the target parameters to match the source expressions. A major challenge is the convincing re-rendering of the synthesized target face into the corresponding video stream. This requires a careful consideration of the lighting and shading design, which both must correspond to the real-world environment. We demonstrate our method in a live setup, where we modify a video conference feed such that the facial expressions of a different person (e.g., translator) are matched in real-time.","cites":"14","conferencePercentile":"96.32653061"},{"venue":"ACM Trans. Graph.","id":"8045fe0187d2c229f74496c876ec82714ccfc49f","venue_1":"ACM Trans. Graph.","year":"2006","title":"Model reduction for real-time fluids","authors":"Adrien Treuille, Andrew Lewis, Zoran Popovic","author_ids":"3064395, 4899890, 1696595","abstract":"We present a new model reduction approach to fluid simulation, enabling large, real-time, detailed flows with continuous user interaction. Our reduced model can also handle moving obstacles immersed in the flow. We create separate models for the velocity field and for each moving boundary, and show that the coupling forces may be reduced as well. Our results indicate that surprisingly few basis functions are needed to resolve small but visually important features such as spinning vortices.","cites":"91","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"2f783b30a90c8a3743b0f4f6ae43ce1f083eecc0","venue_1":"ACM Trans. Graph.","year":"2006","title":"The cartoon animation filter","authors":"Jue Wang, Steven M. Drucker, Maneesh Agrawala, Michael F. Cohen","author_ids":"1718812, 2311676, 1820412, 1694613","abstract":"We present the \"Cartoon Animation Filter\", a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more \"alive\" or \"animated\". The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.","cites":"64","conferencePercentile":"51.85185185"},{"venue":"ACM Trans. Graph.","id":"20f372eb99f4e38b4ed80e293a5766a691032561","venue_1":"ACM Trans. Graph.","year":"2007","title":"Soft scissors: an interactive tool for realtime high quality matting","authors":"Jue Wang, Maneesh Agrawala, Michael F. Cohen","author_ids":"1718812, 1820412, 1694613","abstract":"We present <i>Soft Scissors</i>, an interactive tool for extracting alpha mattes of foreground objects in realtime. We recently proposed a novel offline matting algorithm capable of extracting high-quality mattes for complex foreground objects such as furry animals [Wang and Cohen 2007]. In this paper we both improve the quality of our offline algorithm and give it the ability to incrementally update the matte in an online interactive setting. Our realtime system efficiently estimates foreground color thereby allowing both the matte and the final composite to be revealed instantly as the user roughly paints along the edge of the foreground object. In addition, our system can dynamically adjust the width and boundary conditions of the scissoring paint brush to approximately capture the boundary of the foreground object that lies ahead on the scissor's path. These advantages in both speed and accuracy create the first interactive tool for high quality image matting and compositing.","cites":"76","conferencePercentile":"68"},{"venue":"ACM Trans. Graph.","id":"69ab9e293ae3b54f7b004ac89d789716e0ea5aa4","venue_1":"ACM Trans. Graph.","year":"2004","title":"Style-based inverse kinematics","authors":"Keith Grochow, Steven L. Martin, Aaron Hertzmann, Zoran Popovic","author_ids":"1734756, 2594036, 1747779, 1696595","abstract":"This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.","cites":"300","conferencePercentile":"88.04347826"},{"venue":"ACM Trans. Graph.","id":"1f054e5f08a5ab00e6766eab5b80bad0e437fe77","venue_1":"ACM Trans. Graph.","year":"2006","title":"Continuum crowds","authors":"Adrien Treuille, Seth Cooper, Zoran Popovic","author_ids":"3064395, 1740775, 1696595","abstract":"We present a real-time crowd model based on continuum dynamics. In our model, a dynamic potential field simultaneously integrates global navigation with moving obstacles such as other people, efficiently solving for the motion of large crowds without the need for explicit collision avoidance. Simulations created with our system run at interactive rates, demonstrate smooth flow under a variety of conditions, and naturally exhibit emergent phenomena that have been observed in real crowds.","cites":"61","conferencePercentile":"50"},{"venue":"ACM Trans. Graph.","id":"b7fa6b26a6dde98b6e16ac4676e241c6453dd1ed","venue_1":"ACM Trans. Graph.","year":"2016","title":"Reconstruction of Personalized 3D Face Rigs from Monocular Video","authors":"Pablo Garrido, Michael Zollhöfer, Dan Casas, Levi Valgaerts, Kiran Varanasi, Patrick Pérez, Christian Theobalt","author_ids":"3148360, 1699058, 8188478, 1797649, 1715245, 1799777, 1680185","abstract":"We present a novel approach for the automatic creation of a personalized high-quality 3D face rig of an actor from just monocular video data (e.g., vintage movies). Our rig is based on three distinct layers that allow us to model the actor&#8217;s facial shape as well as capture his person-specific expression characteristics at high fidelity, ranging from coarse-scale geometry to fine-scale static and transient detail on the scale of folds and wrinkles. At the heart of our approach is a parametric shape prior that encodes the plausible subspace of facial identity and expression variations. Based on this prior, a coarse-scale reconstruction is obtained by means of a novel variational fitting approach. We represent person-specific idiosyncrasies, which cannot be represented in the restricted shape and expression space, by learning a set of medium-scale corrective shapes. Fine-scale skin detail, such as wrinkles, are captured from video via shading-based refinement, and a generative detail formation model is learned. Both the medium- and fine-scale detail layers are coupled with the parametric prior by means of a novel sparse linear regression formulation. Once reconstructed, all layers of the face rig can be conveniently controlled by a low number of blendshape expression parameters, as widely used by animation artists. We show captured face rigs and their motions for several actors filmed in different monocular video formats, including legacy footage from YouTube, and demonstrate how they can be used for 3D animation and 2D video editing. Finally, we evaluate our approach qualitatively and quantitatively and compare to related state-of-the-art methods.","cites":"4","conferencePercentile":"95.35864979"},{"venue":"ACM Trans. Graph.","id":"044222b56b790047ef70cedaba938e0063de8971","venue_1":"ACM Trans. Graph.","year":"2009","title":"Noise brush: interactive high quality image-noise separation","authors":"Jia Chen, Chi-Keung Tang, Jue Wang","author_ids":"5562274, 2546217, 1718812","abstract":"This paper proposes an <i>interactive</i> approach using <i>joint image-noise filtering</i> for achieving high quality image-noise separation. The core of the system is our novel joint image-noise filter which operates in both image and noise domain, and can effectively separate noise from both high and low frequency image structures. A novel user interface is introduced, which allows the user to interact with both the image and the noise layer, and apply the filter adaptively and locally to achieve optimal results. A comprehensive and quantitative evaluation shows that our interactive system can significantly improve the initial image-noise separation results. Our system can also be deployed in various noise-consistent image editing tasks, where preserving the noise characteristics inherent in the input image is a desired feature.","cites":"5","conferencePercentile":"7.458563536"},{"venue":"ACM Trans. Graph.","id":"644f08a4d68cc12fccee1e2c063ec5db36920231","venue_1":"ACM Trans. Graph.","year":"2016","title":"Model-based teeth reconstruction","authors":"Chenglei Wu, Derek Bradley, Pablo Garrido, Michael Zollhöfer, Christian Theobalt, Markus H. Gross, Thabo Beeler","author_ids":"1682672, 1745149, 3148360, 1699058, 1680185, 1743207, 2486770","abstract":"In recent years, sophisticated image-based reconstruction methods for the human face have been developed. These methods capture highly detailed static and dynamic geometry of the whole face, or specific models of face regions, such as hair, eyes or eye lids. Unfortunately, image-based methods to capture the mouth cavity in general, and the teeth in particular, have received very little attention. The accurate rendering of teeth, however, is crucial for the realistic display of facial expressions, and currently high quality face animations resort to tooth row models created by tedious manual work. In dentistry, special intra-oral scanners for teeth were developed, but they are invasive, expensive, cumbersome to use, and not readily available. In this paper, we therefore present the first approach for non-invasive reconstruction of an entire person-specific tooth row from just a sparse set of photographs of the mouth region. The basis of our approach is a new parametric tooth row prior learned from high quality dental scans. A new model-based reconstruction approach fits teeth to the photographs such that visible teeth are accurately matched and occluded teeth plausibly synthesized. Our approach seamlessly integrates into photogrammetric multi-camera reconstruction setups for entire faces, but also enables high quality teeth modeling from normal uncalibrated photographs and even short videos captured with a mobile phone.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"73734ea5d5ab4dcbb3567bd76f50656ed61368ba","venue_1":"ACM Trans. Graph.","year":"2009","title":"Video SnapCut: robust video object cutout using localized classifiers","authors":"Xue Bai, Jue Wang, David Simons, Guillermo Sapiro","author_ids":"5141744, 1718812, 7589754, 1699339","abstract":"Although tremendous success has been achieved for interactive object cutout in still images, accurately extracting dynamic objects in video remains a very challenging problem. Previous video cutout systems present two major limitations: (1) reliance on global statistics, thus lacking the ability to deal with complex and diverse scenes; and (2) treating segmentation as a global optimization, thus lacking a practical workflow that can guarantee the convergence of the systems to the desired results.\n We present <i>Video SnapCut</i>, a robust video object cutout system that significantly advances the state-of-the-art. In our system segmentation is achieved by the collaboration of a set of local classifiers, each adaptively integrating multiple local image features. We show how this segmentation paradigm naturally supports local user editing and propagates them across time. The object cutout system is completed with a novel coherent video matting technique. A comprehensive evaluation and comparison is presented, demonstrating the effectiveness of the proposed system at achieving high quality results, as well as the robustness of the system against various types of inputs.","cites":"177","conferencePercentile":"97.79005525"},{"venue":"ACM Trans. Graph.","id":"8de905ace761ed5cb8c1e256c773ff2cb1e1ee07","venue_1":"ACM Trans. Graph.","year":"2011","title":"Subspace video stabilization","authors":"Feng Liu, Michael Gleicher, Jue Wang, Hailin Jin, Aseem Agarwala","author_ids":"1734409, 1776507, 1718812, 1722322, 1696487","abstract":"We present a robust and efficient approach to video stabilization that achieves high-quality camera motion for a wide range of videos. In this article, we focus on the problem of transforming a set of input 2D motion trajectories so that they are both smooth and resemble visually plausible views of the imaged scene; our key insight is that we can achieve this goal by enforcing <i>subspace constraints</i> on feature trajectories while smoothing them. Our approach assembles tracked features in the video into a trajectory matrix, factors it into two low-rank matrices, and performs filtering or curve fitting in a low-dimensional linear space. In order to process long videos, we propose a moving factorization that is both efficient and streamable. Our experiments confirm that our approach can efficiently provide stabilization results comparable with prior 3D methods in cases where those methods succeed, but also provides smooth camera motions in cases where such approaches often fail, such as videos that lack parallax. The presented approach offers the first method that both achieves high-quality video stabilization and is practical enough for consumer applications.","cites":"66","conferencePercentile":"90.52631579"},{"venue":"ACM Trans. Graph.","id":"302e52822b04b2c30470581de11f6668cf729950","venue_1":"ACM Trans. Graph.","year":"2011","title":"Expression flow for 3D-aware face component transfer","authors":"Fei Yang, Jue Wang, Eli Shechtman, Lubomir D. Bourdev, Dimitris N. Metaxas","author_ids":"1684164, 1718812, 2177801, 1769383, 1711560","abstract":"We address the problem of correcting an undesirable expression on a face photo by transferring local facial components, such as a smiling mouth, from another face photo of the same person which has the desired expression. Direct copying and blending using existing compositing tools results in semantically unnatural composites, since expression is a global effect and the local component in one expression is often incompatible with the shape and other components of the face in another expression. To solve this problem we present Expression Flow, a 2D flow field which can warp the target face globally in a natural way, so that the warped face is compatible with the new facial component to be copied over. To do this, starting with the two input face photos, we jointly construct a pair of 3D face shapes with the same identity but different expressions. The expression flow is computed by projecting the difference between the two 3D shapes back to 2D. It describes how to warp the target face photo to match the expression of the reference photo. User studies suggest that our system is able to generate face composites with much higher fidelity than existing methods.","cites":"28","conferencePercentile":"61.05263158"},{"venue":"ACM Trans. Graph.","id":"c5b93a746a96c4f9c66eab5a49648d41a11c7b04","venue_1":"ACM Trans. Graph.","year":"2012","title":"Video deblurring for hand-held cameras using patch-based synthesis","authors":"Sunghyun Cho, Jue Wang, Seungyong Lee","author_ids":"7942592, 1718812, 6518393","abstract":"Videos captured by hand-held Cameras often contain significant camera shake, causing many frames to be blurry. Restoring shaky videos not only requires smoothing the camera motion and stabilizing the content, but also demands removing blur from video frames. However, video blur is hard to remove using existing single or multiple image deblurring techniques, as the blur kernel is both spatially and temporally varying. This paper presents a video deblurring method that can effectively restore sharp frames from blurry ones caused by camera shake. Our method is built upon the observation that due to the nature of camera shake, not all video frames are equally blurry. The same object may appear sharp on some frames while blurry on others. Our method detects sharp regions in the video, and uses them to restore blurry regions of the same content in nearby frames. Our method also ensures that the deblurred frames are both spatially and temporally coherent using patch-based synthesis. Experimental results show that our method can effectively remove complex video blur under the presence of moving objects and other outliers, which cannot be achieved using previous deconvolution-based approaches.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"03e378305fe34f7ac359fa81b4227795c6b7020a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Radial view based culling for continuous self-collision detection of skeletal models","authors":"Sai-Keung Wong, Wen-Chieh Lin, Chun-Hung Hung, Yi-Jheng Huang, Shing-Yeu Lii","author_ids":"3264728, 2019904, 3022464, 1905734, 3029335","abstract":"We present a novel radial-view-based culling method for continuous self-collision detection (CSCD) of skeletal models. Our method targets closed triangular meshes used to represent the surface of a model. It can be easily integrated with bounding volume hierarchies (BVHs) and used as the first stage for culling non-colliding triangle pairs. A mesh is decomposed into clusters with respect to a set of observer primitives (i.e., observer points and line segments) on the skeleton of the mesh so that each cluster is associated with an observer primitive. One BVH is then built for each cluster. At the runtime stage, a radial view test is performed from the observer primitive of each cluster to check its collision state. Every pair of clusters is also checked for collisions. We evaluated our method on various models and compared its performance with prior methods. Experimental results show that our method reduces the number of the bounding volume overlapping tests and the number of potentially colliding triangle pairs, thereby improving the overall process of CSCD.","cites":"8","conferencePercentile":"25.56561086"},{"venue":"ACM Trans. Graph.","id":"3cafd0f53d52062e92e8e4efe3474bbda17b7379","venue_1":"ACM Trans. Graph.","year":"2007","title":"Active learning for real-time motion controllers","authors":"Seth Cooper, Aaron Hertzmann, Zoran Popovic","author_ids":"1740775, 1747779, 1696595","abstract":"This paper describes an approach to building real-time highly-controllable characters. A kinematic character controller is built on-the-fly during a capture session, and updated after each new motion clip is acquired. Active learning is used to identify which motion sequence the user should perform next, in order to improve the quality and responsiveness of the controller. Because motion clips are selected adaptively, we avoid the difficulty of manually determining which ones to capture, and can build complex controllers from scratch while significantly reducing the number of necessary motion samples.","cites":"27","conferencePercentile":"22.4"},{"venue":"ACM Trans. Graph.","id":"04a37325c5f7476f4e28bc59bb5be5d5ca5bb7d8","venue_1":"ACM Trans. Graph.","year":"2006","title":"Vector field design on surfaces","authors":"Eugene Zhang, Konstantin Mischaikow, Greg Turk","author_ids":"1785634, 3077222, 1713189","abstract":"Vector field design on surfaces is necessary for many graphics applications: example-based texture synthesis, nonphotorealistic rendering, and fluid simulation. For these applications, singularities contained in the input vector field often cause visual artifacts. In this article, we present a vector field design system that allows the user to create a wide variety of vector fields with control over vector field topology, such as the number and location of singularities. Our system combines basis vector fields to make an initial vector field that meets user specifications.The initial vector field often contains unwanted singularities. Such singularities cannot always be eliminated due to the Poincar&#233;-Hopf index theorem. To reduce the visual artifacts caused by these singularities, our system allows the user to move a singularity to a more favorable location or to cancel a pair of singularities. These operations offer topological guarantees for the vector field in that they only affect user-specified singularities. We develop efficient implementations of these operations based on <i>Conley index theory</i>. Our system also provides other editing operations so that the user may change the topological and geometric characteristics of the vector field.To create continuous vector fields on curved surfaces represented as meshes, we make use of the ideas of <i>geodesic polar maps</i> and <i>parallel transport</i> to interpolate vector values defined at the vertices of the mesh. We also use geodesic polar maps and parallel transport to create basis vector fields on surfaces that meet the user specifications. These techniques enable our vector field design system to work for both planar domains and curved surfaces.We demonstrate our vector field design system for several applications: example-based texture synthesis, painterly rendering of images, and pencil sketch illustrations of smooth surfaces.","cites":"100","conferencePercentile":"73.61111111"},{"venue":"ACM Trans. Graph.","id":"d25affa4b22db3fbe15d1437058fa38b93c0b970","venue_1":"ACM Trans. Graph.","year":"2007","title":"A finite element method for animating large viscoplastic flow","authors":"Adam W. Bargteil, Christopher Wojtan, Jessica K. Hodgins, Greg Turk","author_ids":"1720867, 2106076, 1788773, 1713189","abstract":"We present an extension to Lagrangian finite element methods to allow for large plastic deformations of solid materials. These behaviors are seen in such everyday materials as shampoo, dough, and clay as well as in fantastic gooey and blobby creatures in special effects scenes. To account for plastic deformation, we explicitly update the linear basis functions defined over the finite elements during each simulation step. When these updates cause the basis functions to become ill-conditioned, we remesh the simulation domain to produce a new high-quality finite-element mesh, taking care to preserve the original boundary. We also introduce an enhanced plasticity model that preserves volume and includes creep and work hardening/softening. We demonstrate our approach with simulations of synthetic objects that squish, dent, and flow. To validate our methods, we compare simulation results to videos of real materials.","cites":"72","conferencePercentile":"66"},{"venue":"ACM Trans. Graph.","id":"2756904b8c0dee83808e0e5913600d2dd63ab062","venue_1":"ACM Trans. Graph.","year":"2012","title":"Perspective-aware warping for seamless stereoscopic image cloning","authors":"Sheng-Jie Luo, I-Chao Shen, Bing-Yu Chen, Wen-Huang Cheng, Yung-Yu Chuang","author_ids":"2571040, 1715728, 1733344, 1711298, 3032320","abstract":"This paper presents a novel technique for seamless stereoscopic image cloning, which performs both shape adjustment and color blending such that the stereoscopic composite is seamless in both the perceived depth and color appearance. The core of the proposed method is an iterative disparity adaptation process which alternates between two steps: disparity estimation, which re-estimates the disparities in the gradient domain so that the disparities are continuous across the boundary of the cloned region; and perspective-aware warping, which locally re-adjusts the shape and size of the cloned region according to the estimated disparities. This process guarantees not only depth continuity across the boundary but also models local perspective projection in accordance with the disparities, leading to more natural stereoscopic composites. The proposed method allows for easy cloning of objects with intricate silhouettes and vague boundaries because it does not require precise segmentation of the objects. Several challenging cases are demonstrated to show that our method generates more compelling results compared to methods with only global shape adjustment.","cites":"11","conferencePercentile":"25.25252525"},{"venue":"ACM Trans. Graph.","id":"06513a465ba7ae419bb8de8600f03f0c6835a01f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Interactive spacetime control of deformable objects","authors":"Klaus Hildebrandt, Christian Schulz, Christoph von Tycowicz, Konrad Polthier","author_ids":"2167599, 3022743, 3165005, 2265027","abstract":"Creating motions of objects or characters that are physically plausible and follow an animator's intent is a key task in computer animation. The <i>spacetime constraints</i> paradigm is a valuable approach to this problem, but it suffers from high computational costs. Based on spacetime constraints, we propose a framework for controlling the motion of deformable objects that offers interactive response times. This is achieved by a model reduction of the underlying variational problem, which combines dimension reduction, multipoint linearization, and decoupling of ODEs. After a preprocess, the cost for creating or editing a motion is reduced to solving a number of one-dimensional spacetime problems, whose solutions are the <i>wiggly splines</i> introduced by Kass and Anderson [2008]. We achieve interactive response times through a new fast and robust numerical scheme for solving the one-dimensional problems that is based on a closed-form representation of the wiggly splines.","cites":"15","conferencePercentile":"42.67676768"},{"venue":"ACM Trans. Graph.","id":"037352b3f93cda91152f2651deed7ae5e1c92884","venue_1":"ACM Trans. Graph.","year":"2012","title":"Plastic trees: interactive self-adapting botanical tree models","authors":"Sören Pirk, Ondrej Stava, Julian Kratt, Michel Abdul-Massih, Boris Neubert, Radomír Mech, Bedrich Benes, Oliver Deussen","author_ids":"2604835, 2239460, 2832454, 1803072, 2466324, 2008027, 7590480, 1850438","abstract":"We present a dynamic tree modeling and representation technique that allows complex tree models to interact with their environment. Our method uses changes in the light distribution and proximity to solid obstacles and other trees as approximations of biologically motivated transformations on a skeletal representation of the tree's main branches and its procedurally generated foliage. Parts of the tree are transformed only when required, thus our approach is much faster than common algorithms such as Open L-Systems or space colonization methods. Input is a skeleton-based tree geometry that can be computed from common tree production systems or from reconstructed laser scanning models. Our approach enables content creators to directly interact with trees and to create visually convincing ecosystems interactively. We present different interaction types and evaluate our method by comparing our transformations to biologically based growth simulation techniques.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"0e6e94055a6c0533b1edd47f3367f80afcb3894a","venue_1":"ACM Trans. Graph.","year":"2011","title":"Color compatibility from large datasets","authors":"Peter O'Donovan, Aseem Agarwala, Aaron Hertzmann","author_ids":"6873550, 1696487, 1747779","abstract":"This paper studies color compatibility theories using large datasets, and develops new tools for choosing colors. There are three parts to this work. First, using on-line datasets, we test new and existing theories of human color preferences. For example, we test whether certain hues or hue templates may be preferred by viewers. Second, we learn quantitative models that score the quality of a five-color set of colors, called a <i>color theme</i>. Such models can be used to rate the quality of a new color theme. Third, we demonstrate simple proto-types that apply a learned model to tasks in color design, including improving existing themes and extracting themes from images.","cites":"34","conferencePercentile":"70.26315789"},{"venue":"ACM Trans. Graph.","id":"77c416a109ad40b8993c1a870b42f81645e4948b","venue_1":"ACM Trans. Graph.","year":"2008","title":"Deep photo: model-based photograph enhancement and viewing","authors":"Johannes Kopf, Boris Neubert, Billy Chen, Michael F. Cohen, Daniel Cohen-Or, Oliver Deussen, Matthew Uyttendaele, Dani Lischinski","author_ids":"2891193, 2466324, 8567761, 1694613, 1701009, 1850438, 2262291, 1684384","abstract":"In this paper, we introduce a novel system for browsing, enhancing, and manipulating casual outdoor photographs by combining them with already existing georeferenced digital terrain and urban models. A simple interactive registration process is used to align a photograph with such a model. Once the photograph and the model have been registered, an abundance of information, such as depth, texture, and GIS data, becomes immediately available to our system. This information, in turn, enables a variety of operations, ranging from dehazing and relighting the photograph, to novel view synthesis, and overlaying with geographic information. We describe the implementation of a number of these applications and discuss possible extensions. Our results show that augmenting photographs with already available 3D models of the world supports a wide variety of new ways for us to experience and interact with our everyday snapshots.","cites":"109","conferencePercentile":"92.59259259"},{"venue":"ACM Trans. Graph.","id":"3c1d14120b4a2b8177d1c0cd69467b13e361f449","venue_1":"ACM Trans. Graph.","year":"2007","title":"Approximate image-based tree-modeling using particle flows","authors":"Boris Neubert, Thomas Franken, Oliver Deussen","author_ids":"2466324, 3171709, 1850438","abstract":"We present a method for producing 3D tree models from input photographs with only limited user intervention. An approximate voxel-based tree volume is estimated using image information. The density values of the voxels are used to produce initial positions for a set of particles. Performing a 3D flow simulation, the particles are traced downwards to the tree basis and are combined to form twigs and branches. If possible, the trunk and the first-order branches are determined in the input photographs and are used as attractors for particle simulation. The geometry of the tree skeleton is produced using botanical rules for branch thicknesses and branching angles. Finally, leaves are added. Different initial seeds for particle simulation lead to a variety, yet similar-looking branching structures for a single set of photographs.","cites":"58","conferencePercentile":"58"},{"venue":"ACM Trans. Graph.","id":"91c997385516829b824143ebe9b64274bd120d3e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Two-shot SVBRDF capture for stationary materials","authors":"Miika Aittala, Tim Weyrich, Jaakko Lehtinen","author_ids":"1907688, 1784306, 1780788","abstract":"Material appearance acquisition usually makes a trade-off between acquisition effort and richness of reflectance representation. In this paper, we instead aim for both a light-weight acquisition procedure and a rich reflectance representation simultaneously, by restricting ourselves to one, but very important, class of appearance phenomena: texture-like materials. While such materials' reflectance is generally spatially varying, they exhibit self-similarity in the sense that for any point on the texture there exist many others with similar reflectance properties. We show that the texturedness assumption allows reflectance capture using only two images of a planar sample, taken with and without a headlight flash. Our reconstruction pipeline starts with redistributing reflectance observations across the image, followed by a regularized texture statistics transfer and a non-linear optimization to fit a spatially-varying BRDF (SVBRDF) to the resulting data. The final result describes the material as spatially-varying, diffuse and specular, anisotropic reflectance over a detailed normal map. We validate the method by side-by-side and novel-view comparisons to photographs, comparing normal map resolution to sub-micron ground truth scans, as well as simulated results. Our method is robust enough to use handheld, JPEG-compressed photographs taken with a mobile phone camera and built-in flash.","cites":"11","conferencePercentile":"91.63265306"},{"venue":"ACM Trans. Graph.","id":"26a40a3e47c19cbba0042ab989ec675393e950a7","venue_1":"ACM Trans. Graph.","year":"2013","title":"Practical SVBRDF capture in the frequency domain","authors":"Miika Aittala, Tim Weyrich, Jaakko Lehtinen","author_ids":"1907688, 1784306, 1780788","abstract":"Spatially-varying reflectance and small geometric variations play a vital role in the appearance of real-world surfaces. Consequently, robust, automatic capture of such models is highly desirable; however, current systems require either specialized hardware, long capture times, user intervention, or rely heavily on heuristics. We describe an acquisition setup that utilizes only portable commodity hardware (an LCD display, an SLR camera) and contains no moving parts. In particular, a laptop screen can be used for illumination. Our setup, aided by a carefully constructed image formation model, automatically produces realistic spatially-varying reflectance parameters over a wide range of materials from diffuse to almost mirror-like specular surfaces, while requiring relatively few photographs. We believe our system is the first to offer such generality, while requiring only standard office equipment and no user intervention or parameter tuning. Our results exhibit a good qualitative match to photographs taken under novel viewing and lighting conditions for a range of materials.","cites":"16","conferencePercentile":"64.02714932"},{"venue":"ACM Trans. Graph.","id":"8d049872718b3266d8eaedca6f2c8e5b1f0901c8","venue_1":"ACM Trans. Graph.","year":"2010","title":"A practical appearance model for dynamic facial color","authors":"Jorge Jimenez, Timothy Scully, Nuno Barbosa, Craig Donner, Xenxo Alvarez, Teresa Vieira, Paul Matts, Verónica Orvalho, Diego Gutierrez, Tim Weyrich","author_ids":"3217761, 1721441, 2412164, 2446336, 2448472, 2841870, 2197019, 2087332, 1723695, 1784306","abstract":"Facial appearance depends on both the physical and physiological state of the skin. As people move, talk, undergo stress, and change expression, skin appearance is in constant flux. One of the key indicators of these changes is the <i>color</i> of skin. Skin color is determined by scattering and absorption of light within the skin layers, caused mostly by concentrations of two chromophores, melanin and hemoglobin. In this paper we present a real-time dynamic appearance model of skin built from <i>in vivo</i> measurements of melanin and hemoglobin concentrations. We demonstrate an efficient implementation of our method, and show that it adds negligible overhead to existing animation and rendering pipelines. Additionally, we develop a realistic, intuitive, and automatic control for skin color, which we term a <i>skin appearance rig.</i> This rig can easily be coupled with a traditional geometric facial animation rig. We demonstrate our method by augmenting digital facial performance with realistic appearance changes.","cites":"26","conferencePercentile":"46.19883041"},{"venue":"ACM Trans. Graph.","id":"b02a640177ddf4d4dd8a2290c2789e32dafc4313","venue_1":"ACM Trans. Graph.","year":"2008","title":"A layered, heterogeneous reflectance model for acquiring and rendering human skin","authors":"Craig Donner, Tim Weyrich, Eugene d'Eon, Ravi Ramamoorthi, Szymon Rusinkiewicz","author_ids":"2446336, 1784306, 1712604, 1752236, 7723706","abstract":"We introduce a layered, heterogeneous spectral reflectance model for human skin. The model captures the inter-scattering of light among layers, each of which may have an independent set of spatially-varying absorption and scattering parameters. For greater physical accuracy and control, we introduce an infinitesimally thin absorbing layer between scattering layers. To obtain parameters for our model, we use a novel acquisition method that begins with multi-spectral photographs. By using an inverse rendering technique, along with known chromophore spectra, we optimize for the best set of parameters for each pixel of a patch. Our method finds close matches to a wide variety of inputs with low residual error.\n We apply our model to faithfully reproduce the complex variations in skin pigmentation. This is in contrast to most previous work, which assumes that skin is homogeneous or composed of homogeneous layers. We demonstrate the accuracy and flexibility of our model by creating complex skin visual effects such as veins, tattoos, rashes, and freckles, which would be difficult to author using only albedo textures at the skin's outer surface. Also, by varying the parameters to our model, we simulate effects from external forces, such as visible changes in blood flow within the skin due to external pressure.","cites":"52","conferencePercentile":"64.19753086"},{"venue":"ACM Trans. Graph.","id":"08c0956e8c94cd693c9df128cf13f1252cd3d19e","venue_1":"ACM Trans. Graph.","year":"2008","title":"A system for high-volume acquisition and matching of fresco fragments: reassembling Theran wall paintings","authors":"Benedict J. Brown, Corey Toler-Franklin, Diego F. Nehab, Michael Burns, David P. Dobkin, Andreas Vlachopoulos, Christos Doumas, Szymon Rusinkiewicz, Tim Weyrich","author_ids":"2142291, 2966788, 1764421, 2018621, 1794954, 3355362, 1686783, 7723706, 1784306","abstract":"Although mature technologies exist for acquiring images, geometry, and normals of small objects, they remain cumbersome and time-consuming for non-experts to employ on a large scale. In an archaeological setting, a practical acquisition system for routine use on <i>every</i> artifact and fragment would open new possibilities for archiving, analysis, and dissemination. We present an inexpensive system for acquiring all three types of information, and associated metadata, for small objects such as fragments of wall paintings. The acquisition system requires minimal supervision, so that a single, non-expert user can scan at least 10 fragments per hour. To achieve this performance, we introduce new algorithms to robustly and automatically align range scans, register 2-D scans to 3-D geometry, and compute normals from 2-D scans. As an illustrative application, we present a novel 3-D matching algorithm that efficiently searches for matching fragments using the scanned geometry.","cites":"42","conferencePercentile":"52.77777778"},{"venue":"ACM Trans. Graph.","id":"040fd05ae4fdf22ecfb1f2d9a33dc56b54b442c3","venue_1":"ACM Trans. Graph.","year":"2011","title":"Interactive surface modeling using modal analysis","authors":"Klaus Hildebrandt, Christian Schulz, Christoph von Tycowicz, Konrad Polthier","author_ids":"2167599, 3022743, 3165005, 2265027","abstract":"We propose a framework for deformation-based surface modeling that is interactive, robust, and intuitive to use. The deformations are described by a nonlinear optimization problem that models static states of elastic shapes under external forces which implement the user input. Interactive response is achieved by a combination of model reduction, a robust energy approximation, and an efficient quasi-Newton solver. Motivated by the observation that a typical modeling session requires only a fraction of the full shape space of the underlying model, we use second and third derivatives of a deformation energy to construct a low-dimensional shape space that forms the feasible set for the optimization. Based on mesh coarsening, we propose an energy approximation scheme with adjustable approximation quality. The quasi-Newton solver guarantees superlinear convergence without the need of costly Hessian evaluations during modeling. We demonstrate the effectiveness of the approach on different examples including the test suite introduced in Sorkine [2008].","cites":"18","conferencePercentile":"41.31578947"},{"venue":"ACM Trans. Graph.","id":"bffa32adf7d02d1bdf0757ef9417fbdd54864429","venue_1":"ACM Trans. Graph.","year":"1991","title":"Using Multivariate Resultants to Find the Intersection of Three Quadric Surfaces","authors":"Eng-Wee Chionh, Ron Goldman, James R. Miller","author_ids":"2117575, 1705063, 5145046","abstract":"Macaulay's concise but explicit expression for nmltivariate resultants has many potential applications in computer-aided geometric design. Here we describe its use in solid modeling for finding the intersections of three implicit quadric surfaces. By B6zout's theorem, three quadric surfaces have either at most eight or intlnitely many intersections. Our method finds the intersections, when there are finitely many, by generating a polynomial of degree at most eight whose roots are the intersection coordinates along an appropriate axis. Only addition, subtraction , and multiplication are required to find the polynomial. But when there are pmsibilities of extraneous roots, division and greatest common divisor computations are necessary to identify and remove them.","cites":"16","conferencePercentile":"66.66666667"},{"venue":"ACM Trans. Graph.","id":"24694d417f1b758d6419dd330c00fc941e56f44a","venue_1":"ACM Trans. Graph.","year":"2004","title":"Capture of hair geometry from multiple images","authors":"Sylvain Paris, Héctor M. Briceño, François X. Sillion","author_ids":"1720990, 2007034, 1708617","abstract":"Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.","cites":"70","conferencePercentile":"32.60869565"},{"venue":"ACM Trans. Graph.","id":"33a23e37be8402b51a1d74b2254b2723741223dd","venue_1":"ACM Trans. Graph.","year":"2006","title":"Two-scale tone management for photographic look","authors":"Soonmin Bae, Sylvain Paris, Frédo Durand","author_ids":"3107120, 1720990, 1728125","abstract":"We introduce a new approach to tone management for photographs. Whereas traditional tone-mapping operators target a neutral and faithful rendition of the input image, we explore pictorial looks by controlling visual qualities such as the tonal balance and the amount of detail. Our method is based on a two-scale non-linear decomposition of an image. We modify the different layers based on their histograms and introduce a technique that controls the spatial variation of detail. We introduce a Poisson correction that prevents potential gradient reversal and preserves detail. In addition to directly controlling the parameters, the user can transfer the look of a model photograph to the picture being edited.","cites":"134","conferencePercentile":"83.33333333"},{"venue":"ACM Trans. Graph.","id":"5001e018eb0dd1e0212702a7a3ec2263fc1b55f1","venue_1":"ACM Trans. Graph.","year":"2004","title":"Speaking with hands: creating animated conversational characters from recordings of human performance","authors":"Matthew Stone, Douglas DeCarlo, Insuk Oh, Christian Rodriguez, Adrian Stere, Alyssa Lees, Christoph Bregler","author_ids":"4109189, 2476753, 2471279, 2915751, 2924284, 2655444, 2428034","abstract":"We describe a method for using a database of recorded speech and captured motion to create an animated conversational character. People's utterances are composed of short, clearly-delimited phrases; in each phrase, gesture and speech go together meaningfully and synchronize at a common point of maximum emphasis. We develop tools for collecting and managing performance data that exploit this structure. The tools help create scripts for performers, help annotate and segment performance data, and structure specific messages for characters to use within application contexts. Our animations then reproduce this structure. They recombine motion samples with new speech samples to recreate coherent phrases, and blend segments of speech and motion together phrase-by-phrase into extended utterances. By framing problems for utterance generation and synthesis so that they can draw closely on a talented performance, our techniques support the rapid construction of animated characters with rich and appropriate expression.","cites":"95","conferencePercentile":"44.56521739"},{"venue":"ACM Trans. Graph.","id":"386fe28c84b9db36fd5d75ca2a230651ac35ca2a","venue_1":"ACM Trans. Graph.","year":"2008","title":"Light mixture estimation for spatially varying white balance","authors":"Eugene Hsu, Tom Mertens, Sylvain Paris, Shai Avidan, Frédo Durand","author_ids":"2647034, 1736035, 1720990, 2740179, 1728125","abstract":"White balance is a crucial step in the photographic pipeline. It ensures the proper rendition of images by eliminating color casts due to differing illuminants. Digital cameras and editing programs provide white balance tools that assume a single type of light per image, such as daylight. However, many photos are taken under mixed lighting. We propose a white balance technique for scenes with two light types that are specified by the user. This covers many typical situations involving indoor/outdoor or flash/ambient light mixtures. Since we work from a single image, the problem is highly underconstrained. Our method recovers a set of dominant material colors which allows us to estimate the local intensity mixture of the two light types. Using this mixture, we can neutralize the light colors and render visually pleasing images. Our method can also be used to achieve post-exposure relighting effects.","cites":"58","conferencePercentile":"66.66666667"},{"venue":"ACM Trans. Graph.","id":"3a35aa67b06428b7687147df643a5e381865684d","venue_1":"ACM Trans. Graph.","year":"2005","title":"Line drawings from volume data","authors":"Michael Burns, Janek Klawe, Szymon Rusinkiewicz, Adam Finkelstein, Douglas DeCarlo","author_ids":"2018621, 2762881, 7723706, 1707541, 2476753","abstract":"Renderings of volumetric data have become an important data analysis tool for applications ranging from medicine to scientific simulation. We propose a volumetric drawing system that directly extracts sparse linear features, such as silhouettes and suggestive contours, using a temporally coherent seed-and-traverse framework. In contrast to previous methods based on isosurfaces or nonrefractive transparency, producing these drawings requires examining an asymptotically smaller subset of the data, leading to efficiency on large data sets. In addition, the resulting imagery is often more comprehensible than standard rendering styles, since it focuses attention on important features in the data. We test our algorithms on datasets up to 512<sup>3</sup>, demonstrating interactive extraction and rendering of line drawings in a variety of drawing styles.","cites":"54","conferencePercentile":"31.85483871"},{"venue":"ACM Trans. Graph.","id":"3b916406d6950efe8cca9dc212a329f8ab7f70c5","venue_1":"ACM Trans. Graph.","year":"2013","title":"An efficient construction of reduced deformable objects","authors":"Christoph von Tycowicz, Christian Schulz, Hans-Peter Seidel, Klaus Hildebrandt","author_ids":"3165005, 3022743, 1746884, 2167599","abstract":"Many efficient computational methods for physical simulation are based on model reduction. We propose new model reduction techniques for the <i>approximation of reduced forces</i> and for the <i>construction of reduced shape spaces of deformable objects</i> that accelerate the construction of a reduced dynamical system, increase the accuracy of the approximation, and simplify the implementation of model reduction. Based on the techniques, we introduce schemes for real-time simulation of deformable objects and interactive deformation-based editing of triangle or tet meshes. We demonstrate the effectiveness of the new techniques in different experiments with elastic solids and shells and compare them to alternative approaches.","cites":"14","conferencePercentile":"55.20361991"},{"venue":"ACM Trans. Graph.","id":"5850647aee364ab3bbc717e1c6e31a06d1f569ea","venue_1":"ACM Trans. Graph.","year":"2014","title":"Exploratory font selection using crowdsourced attributes","authors":"Peter O'Donovan, Janis Libeks, Aseem Agarwala, Aaron Hertzmann","author_ids":"6873550, 2569322, 1696487, 1747779","abstract":"This paper presents interfaces for exploring large collections of fonts for design tasks. Existing interfaces typically list fonts in a long, alphabetically-sorted menu that can be challenging and frustrating to explore. We instead propose three interfaces for font selection. First, we organize fonts using high-level descriptive attributes, such as \"dramatic\" or \"legible.\" Second, we organize fonts in a tree-based hierarchical menu based on perceptual similarity. Third, we display fonts that are most similar to a user's currently-selected font. These tools are complementary; a user may search for \"graceful\" fonts, select a reasonable one, and then refine the results from a list of fonts similar to the selection. To enable these tools, we use crowdsourcing to gather font attribute data, and then train models to predict attribute values for new fonts. We use attributes to help learn a font similarity metric using crowdsourced comparisons. We evaluate the interfaces against a conventional list interface and find that our interfaces are preferred to the baseline. Our interfaces also produce better results in two real-world tasks: finding the nearest match to a target font, and font selection for graphic designs.","cites":"23","conferencePercentile":"93.00411523"},{"venue":"ACM Trans. Graph.","id":"63deed41cd5c7df373bed2ee741683c2996ba47b","venue_1":"ACM Trans. Graph.","year":"2012","title":"Capturing and animating the morphogenesis of polygonal tree models","authors":"Sören Pirk, Till Niese, Oliver Deussen, Boris Neubert","author_ids":"2604835, 1844922, 1850438, 2466324","abstract":"Given a static tree model we present a method to compute developmental stages that approximate the tree's natural growth. The tree model is analyzed and a graph-based description its skeleton is determined. Based on structural similarity, branches are added where pruning has been applied or branches have died off over time. Botanic growth models and allometric rules enable us to produce convincing animations from a young tree that converge to the given model. Furthermore, the user can explore all intermediate stages. By selectively applying the process to parts of the tree even complex models can be edited easily. This form of reverse engineering enables users to create rich natural scenes from a small number of static tree models.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"36c31db023db39d8ba3ddaa42e23acfdff5c7530","venue_1":"ACM Trans. Graph.","year":"2015","title":"Interactive design of probability density functions for shape grammars","authors":"Minh Dang, Stefan Lienhard, Duygu Ceylan, Boris Neubert, Peter Wonka, Mark Pauly","author_ids":"1936455, 2496454, 5350989, 2466324, 1798011, 1741645","abstract":"A shape grammar defines a procedural <i>shape space</i> containing a variety of models of the same class, e.g. buildings, trees, furniture, airplanes, bikes, etc. We present a framework that enables a user to interactively design a probability density function (pdf) over such a shape space and to sample models according to the designed pdf. First, we propose a user interface that enables a user to quickly provide preference scores for selected shapes and suggest sampling strategies to decide which models to present to the user to evaluate. Second, we propose a novel kernel function to encode the similarity between two procedural models. Third, we propose a framework to interpolate user preference scores by combining multiple techniques: function factorization, Gaussian process regression, autorelevance detection, and <i>l</i><sub>1</sub> regularization. Fourth, we modify the original grammars to generate models with a pdf proportional to the user preference scores. Finally, we provide evaluations of our user interface and framework parameters and a comparison to other exploratory modeling techniques using modeling tasks in five example shape spaces: furniture, low-rise buildings, skyscrapers, airplanes, and vegetation.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"81fd1a1f72963ce16ddebacea82e71dab3d6992a","venue_1":"ACM Trans. Graph.","year":"2015","title":"Interactive surface design with interlocking elements","authors":"Mélina Skouras, Stelian Coros, Eitan Grinspun, Bernhard Thomaszewski","author_ids":"2529055, 1783776, 7522998, 1784345","abstract":"We present an interactive tool for designing physical surfaces made from flexible interlocking quadrilateral elements of a single size and shape. With the element shape fixed, the design task becomes one of finding a discrete structure---i.e., element connectivity and binary orientations---that leads to a desired geometry. In order to address this challenging problem of combinatorial geometry, we propose a forward modeling tool that allows the user to interactively explore the space of feasible designs. Paralleling principles from conventional modeling software, our approach leverages a library of base shapes that can be instantiated, combined, and extended using two fundamental operations: merging and extrusion. In order to assist the user in building the designs, we furthermore propose a method to automatically generate assembly instructions. We demonstrate the versatility of our method by creating a diverse set of digital and physical examples that can serve as personalized lamps or decorative items.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"3dbf26bbc6c6fc3ac579351b8fc00fd1ecf01d07","venue_1":"ACM Trans. Graph.","year":"2015","title":"Design and fabrication of flexible rod meshes","authors":"Jesús Pérez, Bernhard Thomaszewski, Stelian Coros, Bernd Bickel, José A. Canabal, Robert W. Sumner, Miguel A. Otaduy","author_ids":"1805419, 1784345, 1783776, 3083909, 2086633, 1693475, 1704342","abstract":"We present a computational tool for fabrication-oriented design of flexible rod meshes. Given a deformable surface and a set of deformed poses as input, our method automatically computes a printable rod mesh that, once manufactured, closely matches the input poses under the same boundary conditions. The core of our method is formed by an optimization scheme that adjusts the cross-sectional profiles of the rods and their rest centerline in order to best approximate the target deformations. This approach allows us to locally control the bending and stretching resistance of the surface with a single material, yielding high design flexibility and low fabrication cost.","cites":"7","conferencePercentile":"81.2244898"},{"venue":"ACM Trans. Graph.","id":"1cb112f99e6fb2fdcb1543d2cbd4bac824df0d5f","venue_1":"ACM Trans. Graph.","year":"1999","title":"Analyzing bounding boxes for object intersection","authors":"Subhash Suri, Philip M. Hubbard, John F. Hughes","author_ids":"8031820, 1946656, 2057964","abstract":"Heuristics that exploit bouning boxes are common in algorithms for rendering, modeling, and animation. While experience has shown that bounding boxes improve the performance of these algorithms in practice, the previous theoretical analysis has concluded that bounding boxes perform poorly in the worst case. This paper reconciles this discrepancy by analyzing intersections among <italic>n</italic> geometric objects in terms of two parameters: &#945; an upper bound on the <italic>aspect ratio</italic> or elongatedness of each object; and &sgr; an upper bound on the <italic>scale factor</italic> or size disparity between the largest and smallest  objects. Letting <italic>K<subscrpt>o</subscrpt></italic> and <italic>K<subscrpt>b</subscrpt></italic> be the number of intersecting object       pairs and bounding box pairs, respectively, we analyze a ratio measure of the   bounding boxes'  efficiency, <inline-equation> <f> <g>r</g>=K<inf>b</inf>/<fen lp=\"par\">n+K<inf>o</inf><rp post=\"par\"></fen> </f> </inline-equation>. The analysis proves that <inline-equation> <f> <g>r</g>=O<fen lp=\"par\"><g>a</g><rad><rcd><g>s</g></rcd></rad> <lim align=\"r\"><op><rf>log</rf></op><ul>2</ul></lim><g>s</g><rp post=\"par\"></fen> </f> </inline-equation> and <inline-equation> <f> <g>r</g>=<g>W</g><fen lp=\"par\"><g>a</g><rad><rcd><g>s</g></rcd></rad> <rp post=\"par\"></fen></f> </inline-equation>.\n One important consequence is that if &#945; and &sgr; are small constants (as is often the case in practice), then<italic>K<subscrpt>b</subscrpt></italic>= <italic>O</italic>(<italic>K<subscrpt>o</subscrpt></italic>)+<italic>O</italic>(<italic>n</italic>, so an algorithm that uses bounding boxes has time complexity proportional to the number of actual object intersections. This theoretical result validates the efficiency that bounding boxes have demonstrated in practice. Another consequence of our analysis is a proof of the output-sensitivity of an algorithm for reporting all intersecting pairs in a set of <italic>n</italic> convex polyhedra with constant &#945; and &sgr;. The algorithm takes time <italic>O</italic>(<italic>n</italic>log<supscrpt><italic>d</italic>&minus;1</supscrpt><italic>n</italic>+<italic>K<subscrpt>o</subscrpt></italic>log<supscrpt><italic>d</italic>&minus;1</supscrpt><italic>n</italic>) for dimension <italic>d</italic> = 2, 3. This running time improves on the performance of previous algorithms, which make no assumptions about &#945; and &sgr;.","cites":"12","conferencePercentile":"28.57142857"},{"venue":"ACM Trans. Graph.","id":"358f33b73340628c5232ed4c305f61e25fe85368","venue_1":"ACM Trans. Graph.","year":"2015","title":"LinkEdit: interactive linkage editing using symbolic kinematics","authors":"Moritz Bächer, Stelian Coros, Bernhard Thomaszewski","author_ids":"8021864, 1783776, 1784345","abstract":"We present a method for interactive editing of planar linkages. Given a working linkage as input, the user can make targeted edits to the shape or motion of selected parts while preserving other, e.g., functionally-important aspects. In order to make this process intuitive and efficient, we provide a number of editing tools at different levels of abstraction. For instance, the user can directly change the structure of a linkage by displacing joints, edit the motion of selected points on the linkage, or impose limits on the size of its enclosure. Our method safeguards against degenerate configurations during these edits, thus ensuring the correct functioning of the mechanism at all times. Linkage editing poses strict requirements on performance that standard approaches fail to provide. In order to enable interactive and robust editing, we build on a symbolic kinematics approach that uses closed-form expressions instead of numerical methods to compute the motion of a linkage and its derivatives. We demonstrate our system on a diverse set of examples, illustrating the potential to adapt and personalize the structure and motion of existing linkages. To validate the feasibility of our edited designs, we fabricated two physical prototypes.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"c2beb8d6a4c84b950f51eb4c9934da6af0a2ca73","venue_1":"ACM Trans. Graph.","year":"2015","title":"Interactive design of 3D-printable robotic creatures","authors":"Vittorio Megaro, Bernhard Thomaszewski, Maurizio Nitti, Otmar Hilliges, Markus H. Gross, Stelian Coros","author_ids":"2097417, 1784345, 2103369, 2531379, 1743207, 1783776","abstract":"We present an interactive design system that allows casual users to quickly create 3D-printable robotic creatures. Our approach automates the tedious parts of the design process while providing ample room for customization of morphology, proportions, gait and motion style. The technical core of our framework is an efficient optimization-based solution that generates stable motions for legged robots of arbitrary designs. An intuitive set of editing tools allows the user to interactively explore the space of feasible designs and to study the relationship between morphological features and the resulting motions. Fabrication blueprints are generated automatically such that the robot designs can be manufactured using 3D-printing and off-the-shelf servo motors. We demonstrate the effectiveness of our solution by designing six robotic creatures with a variety of morphological features: two, four or five legs, point or area feet, actuated spines and different proportions. We validate the feasibility of the designs generated with our system through physics simulations and physically-fabricated prototypes.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"45f72b2812212f8b4e0b9fe185af022da992cd6f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Computational design of linkage-based characters","authors":"Bernhard Thomaszewski, Stelian Coros, Damien Gauge, Vittorio Megaro, Eitan Grinspun, Markus H. Gross","author_ids":"1784345, 1783776, 2854612, 2097417, 7522998, 1743207","abstract":"We present a design system for linkage-based characters, combining form and function in an aesthetically-pleasing manner. Linkage-based character design exhibits a mix of discrete and continuous problems, making for a highly unintuitive design space that is difficult to navigate without assistance. Our system significantly simplifies this task by allowing users to interactively browse different topology options, thus guiding the discrete set of choices that need to be made. A subsequent continuous optimization step improves motion quality and, crucially, safeguards against singularities. We demonstrate the flexibility of our method on a diverse set of character designs, and then realize our designs by physically fabricating prototypes.","cites":"22","conferencePercentile":"91.56378601"},{"venue":"ACM Trans. Graph.","id":"01040a9b6e05300f9d1f83737d2f2769ae4278dd","venue_1":"ACM Trans. Graph.","year":"2014","title":"Subspace clothing simulation using adaptive bases","authors":"Fabian Hahn, Bernhard Thomaszewski, Stelian Coros, Robert W. Sumner, Forrester Cole, Mark Meyer, Tony DeRose, Markus H. Gross","author_ids":"1757129, 1784345, 1783776, 1693475, 1805136, 8735841, 1792251, 1743207","abstract":"We present a new approach to clothing simulation using low-dimensional linear subspaces with temporally adaptive bases. Our method exploits full-space simulation training data in order to construct a pool of low-dimensional bases distributed across pose space. For this purpose, we interpret the simulation data as offsets from a kinematic deformation model that captures the global shape of clothing due to body pose. During subspace simulation, we select low-dimensional sets of basis vectors according to the current pose of the character and the state of its clothing. Thanks to this adaptive basis selection scheme, our method is able to reproduce diverse and detailed folding patterns with only a few basis vectors. Our experiments demonstrate the feasibility of subspace clothing simulation and indicate its potential in terms of quality and computational efficiency.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"38a35c31d5cafb31099ab9361f310070eb75557f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Computational design of actuated deformable characters","authors":"Mélina Skouras, Bernhard Thomaszewski, Stelian Coros, Bernd Bickel, Markus H. Gross","author_ids":"2529055, 1784345, 1783776, 3083909, 1743207","abstract":"We present a method for fabrication-oriented design of actuated deformable characters that allows a user to automatically create physical replicas of digitally designed characters using rapid manufacturing technologies. Given a deformable character and a set of target poses as input, our method computes a small set of actuators along with their locations on the surface and optimizes the internal material distribution such that the resulting character exhibits the desired deformation behavior. We approach this problem with a dedicated algorithm that combines finite-element analysis, sparse regularization, and constrained optimization. We validate our pipeline on a set of two- and three-dimensional example characters and present results in simulation and physically-fabricated prototypes.","cites":"39","conferencePercentile":"90.49773756"},{"venue":"ACM Trans. Graph.","id":"310aa766be965b82f873b132e6e424519583af2a","venue_1":"ACM Trans. Graph.","year":"2006","title":"Exaggerated shading for depicting shape and detail","authors":"Szymon Rusinkiewicz, Michael Burns, Douglas DeCarlo","author_ids":"7723706, 2018621, 2476753","abstract":"In fields ranging from technical illustration to mapmaking, artists have developed distinctive visual styles designed to convey both detail and overall shape as clearly as possible. We investigate a non-photorealistic shading model, inspired by techniques for carto-graphic terrain relief, based on dynamically adjusting the effective light position for different areas of the surface. It reveals detail regardless of surface orientation and, by operating at multiple scales, is designed to convey detail at all frequencies simultaneously.","cites":"83","conferencePercentile":"62.03703704"},{"venue":"ACM Trans. Graph.","id":"4b2ff80d7e64f036cf749a8a5ce21dcf9569b79e","venue_1":"ACM Trans. Graph.","year":"2012","title":"Deformable objects alive!","authors":"Stelian Coros, Sebastian Martin, Bernhard Thomaszewski, Christian Schumacher, Robert W. Sumner, Markus H. Gross","author_ids":"1783776, 3341690, 1784345, 2635933, 1693475, 1743207","abstract":"We present a method for controlling the motions of active deformable characters. As an underlying principle, we require that all motions be driven by <i>internal</i> deformations. We achieve this by dynamically adapting rest shapes in order to induce deformations that, together with environment interactions, result in purposeful and physically-plausible motions. Rest shape adaptation is a powerful concept and we show that by restricting shapes to suitable subspaces, it is possible to explicitly control the motion styles of deformable characters. Our formulation is general and can be combined with arbitrary elastic models and locomotion controllers. We demonstrate the efficiency of our method by animating curve, shell, and solid-based characters whose motion repertoires range from simple hopping to complex walking behaviors.","cites":"20","conferencePercentile":"59.84848485"},{"venue":"ACM Trans. Graph.","id":"3f63c9d1ea25ed09c8e61a688e4407e3081d3dce","venue_1":"ACM Trans. Graph.","year":"2011","title":"Locomotion skills for simulated quadrupeds","authors":"Stelian Coros, Andrej Karpathy, Ben Jones, Lionel Revéret, Michiel van de Panne","author_ids":"1783776, 2354728, 5214198, 2118256, 1745029","abstract":"We develop an integrated set of gaits and skills for a physics-based simulation of a quadruped. The motion repertoire for our simulated dog includes walk, trot, pace, canter, transverse gallop, rotary gallop, leaps capable of jumping on-and-off platforms and over obstacles, sitting, lying down, standing up, and getting up from a fall. The controllers use a representation based on gait graphs, a dual leg frame model, a flexible spine model, and the extensive use of internal virtual forces applied via the Jacobian transpose. Optimizations are applied to these control abstractions in order to achieve robust gaits and leaps with desired motion styles. The resulting gaits are evaluated for robustness with respect to push disturbances and the traversal of variable terrain. The simulated motions are also compared to motion data captured from a filmed dog.","cites":"47","conferencePercentile":"83.42105263"},{"venue":"ACM Trans. Graph.","id":"111507f37e92627c78567333620d0dc1b1ff5107","venue_1":"ACM Trans. Graph.","year":"2010","title":"Generalized biped walking control","authors":"Stelian Coros, Philippe Beaudoin, Michiel van de Panne","author_ids":"1783776, 2895922, 1745029","abstract":"We present a control strategy for physically-simulated walking motions that generalizes well across gait parameters, motion styles, character proportions, and a variety of skills. The control is realtime, requires no character-specific or motion-specific tuning, is robust to disturbances, and is simple to compute. The method works by integrating tracking, using proportional-derivative control; foot placement, using an inverted pendulum model; and adjustments for gravity and velocity errors, using Jacobian transpose control. High-level gait parameters allow for forwards-and-backwards walking, various walking speeds, turns, walk-to-stop, idling, and stop-to-walk behaviors. Character proportions and motion styles can be authored interactively, with edits resulting in the instant realization of a suitable controller. The control is further shown to generalize across a variety of walking-related skills, including picking up objects placed at any height, lifting and walking with heavy crates, pushing and pulling crates, stepping over obstacles, ducking under obstacles, and climbing steps.","cites":"62","conferencePercentile":"91.8128655"},{"venue":"ACM Trans. Graph.","id":"21603ea83e7506abe9ad8ae1da0b85764c08228d","venue_1":"ACM Trans. Graph.","year":"2009","title":"Robust task-based control policies for physics-based characters","authors":"Stelian Coros, Philippe Beaudoin, Michiel van de Panne","author_ids":"1783776, 2895922, 1745029","abstract":"We present a method for precomputing robust task-based control policies for physically simulated characters. This allows for characters that can demonstrate skill and purpose in completing a given task, such as walking to a target location, while physically interacting with the environment in significant ways. As input, the method assumes an abstract action vocabulary consisting of balance-aware, step-based controllers. A novel constrained state exploration phase is first used to define a character dynamics model as well as a finite volume of character states over which the control policy will be defined. An optimized control policy is then computed using reinforcement learning. The final policy spans the cross-product of the character state and task state, and is more robust than the conrollers it is constructed from. We demonstrate real-time results for six locomotion-based tasks and on three highly-varied bipedal characters. We further provide a game-scenario demonstration.","cites":"48","conferencePercentile":"69.88950276"},{"venue":"ACM Trans. Graph.","id":"ba8eee378db670a4d2b11fe4856bd144743ae4e2","venue_1":"ACM Trans. Graph.","year":"2008","title":"Synthesis of constrained walking skills","authors":"Stelian Coros, Philippe Beaudoin, KangKang Yin, Michiel van de Panne","author_ids":"1783776, 2895922, 2362176, 1745029","abstract":"Simulated characters in simulated worlds require simulated skills. We develop control strategies that enable physically-simulated characters to dynamically navigate environments with significant stepping constraints, such as sequences of gaps. We present a synthesis-analysis-synthesis framework for this type of problem. First, an offline optimization method is applied in order to compute example control solutions for randomly-generated example problems from the given task domain. Second, the example motions and their underlying control patterns are analyzed to build a low-dimensional step-to-step model of the dynamics. Third, this model is exploited by a planner to solve new instances of the task at interactive rates. We demonstrate real-time navigation across constrained terrain for physics-based simulations of 2D and 3D characters. Because the framework sythesizes its own example data, it can be applied to bipedal characters for which no motion data is available.","cites":"30","conferencePercentile":"30.86419753"},{"venue":"ACM Trans. Graph.","id":"14ec785e185b30d8ba759a6cf7a28119f3f50efe","venue_1":"ACM Trans. Graph.","year":"2008","title":"Continuation methods for adapting simulated skills","authors":"KangKang Yin, Stelian Coros, Philippe Beaudoin, Michiel van de Panne","author_ids":"2362176, 1783776, 2895922, 1745029","abstract":"Modeling the large space of possible human motions requires scalable techniques. Generalizing from example motions or example controllers is one way to provide the required scalability. We present techniques for generalizing a controller for physics-based walking to significantly different tasks, such as climbing a large step up, or pushing a heavy object. Continuation methods solve such problems using a progressive sequence of problems that trace a path from an existing solved problem to the final desired-but-unsolved problem. Each step in the continuation sequence makes progress towards the target problem while further adapting the solution. We describe and evaluate a number of choices in applying continuation methods to adapting walking gaits for tasks involving interaction with the environment. The methods have been successfully applied to automatically adapt a regular cyclic walk to climbing a 65<i>cm</i> step, stepping over a 55<i>cm</i> sill, pushing heavy furniture, walking up steep inclines, and walking on ice. The continuation path further provides parameterized solutions to these problems.","cites":"49","conferencePercentile":"60.18518519"},{"venue":"ACM Trans. Graph.","id":"3bb3006a7c793ae89f2862fa2221036144241b98","venue_1":"ACM Trans. Graph.","year":"2013","title":"3D+2DTV: 3D displays with no ghosting for viewers without glasses","authors":"Steven Scher, Jing Liu, Rajan Vaish, Prabath Gunawardane, James Davis","author_ids":"3298212, 5661757, 1927646, 1731153, 3050723","abstract":"3D displays are increasingly popular in consumer and commercial applications. Many such displays show 3D images to viewers wearing special glasses, while showing an incomprehensible double image to viewers without glasses. We demonstrate a simple method that provides those with glasses a 3D experience, while viewers without glasses see a 2D image without artifacts.\n In addition to separate left and right images in each frame, we add a third image, invisible to those with glasses. In the combined view seen by those without glasses, this cancels the right image, leaving only the left.\n If the left and right images are of equal brightness, this approach results in low contrast for viewers without glasses. Allowing differential brightness between the left and right images improves 2D contrast. We observe experimentally that: (1) viewers without glasses prefer our 3D&plus;2DTV to a standard 3DTV, (2) viewers with glasses maintain a strong 3D percept, even when one eye is significantly darker than the other, and (3) sequential-stereo display viewers with glasses experience a depth illusion caused by the Pulfrich effect, but it is small and innocuous.\n Our technique is applicable to displays using either active shutter glasses or passive glasses. Our prototype uses active shutter glasses and a polarizer.","cites":"7","conferencePercentile":"21.26696833"},{"venue":"ACM Trans. Graph.","id":"af0078586ef861be0bb7c6aa9b8f546c3a3d9aaa","venue_1":"ACM Trans. Graph.","year":"2015","title":"Real-Time Nonlinear Shape Interpolation","authors":"Christoph von Tycowicz, Christian Schulz, Hans-Peter Seidel, Klaus Hildebrandt","author_ids":"3165005, 3022743, 1746884, 2167599","abstract":"We introduce a scheme for real-time nonlinear interpolation of a set of shapes. The scheme exploits the structure of the shape interpolation problem, in particular the fact that the set of all possible interpolated shapes is a low-dimensional object in a high-dimensional shape space. The interpolated shapes are defined as the minimizers of a nonlinear objective functional on the shape space. Our approach is to construct a reduced optimization problem that approximates its unreduced counterpart and can be solved in milliseconds. To achieve this, we restrict the optimization to a low-dimensional subspace that is specifically designed for the shape interpolation problem. The construction of the subspace is based on two components: a formula for the calculation of derivatives of the interpolated shapes and a Krylov-type sequence that combines the derivatives and the Hessian of the objective functional. To make the computational cost for solving the reduced optimization problem independent of the resolution of the example shapes, we combine the dimensional reduction with schemes for the efficient approximation of the reduced nonlinear objective functional and its gradient. In our experiments, we obtain rates of 20--100 interpolated shapes per second, even for the largest examples which have 500k vertices per example shape.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"b6044c44bfb62fa2dbd00544cfc89abca7615675","venue_1":"ACM Trans. Graph.","year":"2014","title":"Animating deformable objects using sparse spacetime constraints","authors":"Christian Schulz, Christoph von Tycowicz, Hans-Peter Seidel, Klaus Hildebrandt","author_ids":"3022743, 3165005, 1746884, 2167599","abstract":"We propose a scheme for animating deformable objects based on spacetime optimization. The main feature is that it robustly and within a few seconds generates interesting motion from a sparse set of spacetime constraints. Providing only partial (as opposed to full) keyframes for positions and velocities is sufficient. The computed motion satisfies the constraints and the remaining degrees of freedom are determined by physical principles using elasticity and the spacetime constraints paradigm. Our modeling of the spacetime optimization problem combines dimensional reduction, modal coordinates, wiggly splines, and rotation strain warping. Our solver is based on a theorem that characterizes the solutions of the optimization problem and allows us to restrict the optimization to low-dimensional search spaces. This treatment of the optimization problem avoids a time discretization and the resulting method can robustly deal with sparse input and wiggly motion.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"353654b196ca2e806d0562722df6d11ad685aefe","venue_1":"ACM Trans. Graph.","year":"2012","title":"Rig-space physics","authors":"Fabian Hahn, Sebastian Martin, Bernhard Thomaszewski, Robert W. Sumner, Stelian Coros, Markus H. Gross","author_ids":"1757129, 3341690, 1784345, 1693475, 1783776, 1743207","abstract":"We present a method that brings the benefits of physics-based simulations to traditional animation pipelines. We formulate the equations of motions in the subspace of deformations defined by an animator's rig. Our framework fits seamlessly into the workflow typically employed by artists, as our output consists of animation curves that are identical in nature to the result of manual keyframing. Artists can therefore explore the full spectrum between handcrafted animation and unrestricted physical simulation. To enhance the artist's control, we provide a method that transforms stiffness values defined on rig parameters to a non-homogeneous distribution of material parameters for the underlying FEM model. In addition, we use automatically extracted high-level rig parameters to intuitively edit the results of our simulations, and also to speed up computation. To demonstrate the effectiveness of our method, we create compelling results by adding rich physical motions to coarse input animations. In the absence of artist input, we create realistic passive motion directly in rig space.","cites":"24","conferencePercentile":"69.6969697"},{"venue":"ACM Trans. Graph.","id":"45967c959accd06b29f80eb04be5ce3f01efdc44","venue_1":"ACM Trans. Graph.","year":"2007","title":"Digital bas-relief from 3D scenes","authors":"Tim Weyrich, Jia Deng, Connelly Barnes, Szymon Rusinkiewicz, Adam Finkelstein","author_ids":"1784306, 2169243, 1794537, 7723706, 1707541","abstract":"We present a system for semi-automatic creation of bas-relief sculpture. As an artistic medium, relief spans the continuum between 2D drawing or painting and full 3D sculpture. Bas-relief (or low relief) presents the unique challenge of squeezing shapes into a nearly-flat surface while maintaining as much as possible the perception of the full 3D scene. Our solution to this problem adapts methods from the tone-mapping literature, which addresses the similar problem of squeezing a high dynamic range image into the (low) dynamic range available on typical display devices. However, the bas-relief medium imposes its own unique set of requirements, such as maintaining small, fixed-size depth discontinuities. Given a 3D model, camera, and a few parameters describing the relative attenuation of different frequencies in the shape, our system creates a relief that gives the illusion of the 3D shape from a given vantage point while conforming to a greatly compressed height.","cites":"50","conferencePercentile":"52"},{"venue":"ACM Trans. Graph.","id":"31e3cf9e0bde9914c92223eda02ffeda3425c4fe","venue_1":"ACM Trans. Graph.","year":"1993","title":"Inferring Constraints from Multiple Snapshots","authors":"David Kurlander, Steven K. Feiner","author_ids":"8309818, 1809403","abstract":"Many graphic tasks, such as the manipulation of graphical objects and the construction of user-interface widgets, can be facilitated by geometric constraints. However, the difficulty of specifying constraints by traditional methods forms a barrier to their widespread use. In order to make constraints easier to declare, we have developed a method of specifying constraints implicitly, through multiple examples. Snapshots are taken of an initial scene configuration, and one or more additional snapshots are taken after the scene has been edited into other valid configurations. The constraints that are satisfied in all of the snapshots are then applied to the scene objects. We discuss an efficient algorithm for inferring constraints from multiple snapshots. The algorithm has been incorporated into the Chimera editor, and several examples of its use are discussed.","cites":"50","conferencePercentile":"78.57142857"},{"venue":"ACM Trans. Graph.","id":"052173659e9c800819c872bc9af8c2e08c367de5","venue_1":"ACM Trans. Graph.","year":"2009","title":"A visibility algorithm for converting 3D meshes into editable 2D vector graphics","authors":"Elmar Eisemann, Sylvain Paris, Frédo Durand","author_ids":"1737690, 1720990, 1728125","abstract":"Artists often need to import and embellish 3D models coming from CAD-CAM into 2D vector graphics software to produce, e.g., brochures or manuals. Current automatic solutions tend to result, at best, in a 2D triangle soup and artists often have to trace over 3D renderings. We describe a method to convert 3D models into 2D layered vector illustrations that respect visibility and facilitate further editing. Our core contribution is a visibility method that can partition a mesh into large components that can be layered according to visibility. Because self-occluding objects and objects forming occlusion cycles cannot be represented by layers without being cut, we introduce a new cut algorithm that uses a graph representation of the mesh and curvature-aware geodesic distances.","cites":"13","conferencePercentile":"19.33701657"},{"venue":"ACM Trans. Graph.","id":"09d03b792923695deb0492d8fc3582a50e5f1a1e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Band-Sifting Decomposition for Image-Based Material Editing","authors":"Ivaylo Boyadzhiev, Kavita Bala, Sylvain Paris, Edward H. Adelson","author_ids":"2162534, 8261370, 1720990, 1788148","abstract":"Photographers often &#8220;prep&#8221; their subjects to achieve various effects; for example, toning down overly shiny skin, covering blotches, etc. Making such adjustments digitally after a shoot is possible, but difficult without good tools and good skills. Making such adjustments to video footage is harder still. We describe and study a set of 2D image operations, based on multiscale image analysis, that are easy and straightforward and that can consistently modify perceived material properties. These operators first build a subband decomposition of the image and then selectively modify the coefficients within the subbands. We call this selection process <i>band sifting</i>.\n We show that different siftings of the coefficients can be used to modify the appearance of properties such as gloss, smoothness, pigmentation, or weathering. The band-sifting operators have particularly striking effects when applied to faces; they can provide &#8220;knobs&#8221; to make a face look wetter or drier, younger or older, and with heavy or light variation in pigmentation. Through user studies, we identify a set of operators that yield consistent subjective effects for a variety of materials and scenes. We demonstrate that these operators are also useful for processing video sequences.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"0d5ec7e8ae4ebd24ec7891775d7e70436a066983","venue_1":"ACM Trans. Graph.","year":"2010","title":"A wave-based anisotropic quadrangulation method","authors":"Muyang Zhang, Jin Huang, Xinguo Liu, Hujun Bao","author_ids":"1698890, 3579556, 3227032, 1679542","abstract":"This paper proposes a new method for remeshing a surface into anisotropically sized quads. The basic idea is to construct a special standing wave on the surface to generate the global quadrilateral structure. This wave based quadrangulation method is capable of controlling the quad size in two directions and precisely aligning the quads with feature lines. Similar to the previous methods, we augment the input surface with a vector field to guide the quad orientation. The anisotropic size control is achieved by using two size fields on the surface. In order to reduce singularity points, the size fields are optimized by a new curl minimization method. The experimental results show that the proposed method can successfully handle various quadrangulation requirements and complex shapes, which is difficult for the existing state-of-the-art methods.","cites":"36","conferencePercentile":"64.03508772"},{"venue":"ACM Trans. Graph.","id":"2759a8fc8e71d6c52786ba68cc0a272a3f050ae7","venue_1":"ACM Trans. Graph.","year":"2006","title":"Subspace gradient domain mesh deformation","authors":"Jin Huang, Xiaohan Shi, Xinguo Liu, Kun Zhou, Li-Yi Wei, Shang-Hua Teng, Hujun Bao, Baining Guo, Harry Shum","author_ids":"3579556, 3132594, 3227032, 6671887, 2420851, 1714796, 1679542, 2738456, 1698102","abstract":"In this paper we present a general framework for performing constrained mesh deformation tasks with gradient domain techniques. We present a gradient domain technique that works well with a wide variety of linear and nonlinear constraints. The constraints we introduce include the nonlinear volume constraint for volume preservation, the nonlinear skeleton constraint for maintaining the rigidity of limb segments of articulated figures, and the projection constraint for easy manipulation of the mesh without having to frequently switch between multiple viewpoints. To handle nonlinear constraints, we cast mesh deformation as a nonlinear energy minimization problem and solve the problem using an iterative algorithm. The main challenges in solving this nonlinear problem are the slow convergence and numerical instability of the iterative solver. To address these issues, we develop a subspace technique that builds a coarse control mesh around the original mesh and projects the deformation energy and constraints onto the control mesh vertices using the mean value interpolation. The energy minimization is then carried out in the subspace formed by the control mesh vertices. Running in this subspace, our energy minimization solver is both fast and stable and it provides interactive responses. We demonstrate our deformation constraints and subspace deformation technique with a variety of constrained deformation examples.","cites":"141","conferencePercentile":"87.5"},{"venue":"ACM Trans. Graph.","id":"04a9c4ad10e57b6815e632d1d4190a3fcde2bfa2","venue_1":"ACM Trans. Graph.","year":"2005","title":"Large mesh deformation using the volumetric graph Laplacian","authors":"Kun Zhou, Jin Huang, John Snyder, Xinguo Liu, Hujun Bao, Baining Guo, Harry Shum","author_ids":"6671887, 3579556, 6314473, 3227032, 1679542, 2738456, 1698102","abstract":"We present a novel technique for large deformations on 3D meshes using the volumetric graph Laplacian. We first construct a graph representing the volume inside the input mesh. The graph need not form a solid meshing of the input mesh's interior; its edges simply connect nearby points in the volume. This graph's Laplacian encodes volumetric details as the difference between each point in the graph and the average of its neighbors. Preserving these volumetric details during deformation imposes a volumetric constraint that prevents unnatural changes in volume. We also include in the graph points a short distance outside the mesh to avoid local self-intersections. Volumetric detail preservation is represented by a quadric energy function. Minimizing it preserves details in a least-squares sense, distributing error uniformly over the whole deformed mesh. It can also be combined with conventional constraints involving surface positions, details or smoothness, and efficiently minimized by solving a sparse linear system.We apply this technique in a 2D curve-based deformation system allowing novice users to create pleasing deformations with little effort. A novel application of this system is to apply nonrigid and exaggerated deformations of 2D cartoon characters to 3D meshes. We demonstrate our system's potential with several examples.","cites":"166","conferencePercentile":"85.48387097"},{"venue":"ACM Trans. Graph.","id":"6883a910d49afa95e7e21078bb8e6aa37ea4ef63","venue_1":"ACM Trans. Graph.","year":"2014","title":"Blending liquids","authors":"Karthik Raveendran, Christopher Wojtan, Nils Thürey, Greg Turk","author_ids":"7772786, 2106076, 1786445, 1713189","abstract":"We present a method for smoothly blending between existing liquid animations. We introduce a semi-automatic method for matching two existing liquid animations, which we use to create new fluid motion that plausibly interpolates the input. Our contributions include a new space-time non-rigid iterative closest point algorithm that incorporates user guidance, a subsampling technique for efficient registration of meshes with millions of vertices, and a fast surface extraction algorithm that produces 3D triangle meshes from a 4D space-time surface. Our technique can be used to instantly create hundreds of new simulations, or to interactively explore complex parameter spaces. Our method is guaranteed to produce output that does not deviate from the input animations, and it generalizes to multiple dimensions. Because our method runs at interactive rates after the initial precomputation step, it has potential applications in games and training simulations.","cites":"0","conferencePercentile":"2.057613169"},{"venue":"ACM Trans. Graph.","id":"2b3a374f6b968a546724726bc4283da0dd1553e5","venue_1":"ACM Trans. Graph.","year":"2012","title":"Soft body locomotion","authors":"Jie Tan, Greg Turk, C. Karen Liu","author_ids":"2753365, 1713189, 1688533","abstract":"We present a physically-based system to simulate and control the locomotion of soft body characters without skeletons. We use the finite element method to simulate the deformation of the soft body, and we instrument a character with muscle fibers to allow it to actively control its shape. To perform locomotion, we use a variety of intuitive controls such as moving a point on the character, specifying the center of mass or the angular momentum, and maintaining balance. These controllers yield an objective function that is passed to our optimization solver, which handles convex quadratic program with linear complementarity constraints. This solver determines the new muscle fiber lengths, and moreover it determines whether each point of contact should remain static, slide, or lift away from the floor. Our system can automatically find an appropriate combination of muscle contractions that enables a soft character to fulfill various locomotion tasks, including walking, jumping, crawling, rolling and balancing.","cites":"7","conferencePercentile":"9.848484848"},{"venue":"ACM Trans. Graph.","id":"7c8468d03f812f01d8a2399d0ff8da022fb8e619","venue_1":"ACM Trans. Graph.","year":"2015","title":"Transform recipes for efficient cloud photo enhancement","authors":"Michaël Gharbi, Yi-Chang Shih, Gaurav Chaurasia, Jonathan Ragan-Kelley, Sylvain Paris, Frédo Durand","author_ids":"3282136, 1989579, 2585067, 2488277, 1720990, 1728125","abstract":"Cloud image processing is often proposed as a solution to the limited computing power and battery life of mobile devices: it allows complex algorithms to run on powerful servers with virtually unlimited energy supply. Unfortunately, this overlooks the time and energy cost of uploading the input and downloading the output images. When transfer overhead is accounted for, processing images on a remote server becomes less attractive and many applications do not benefit from cloud offloading. We aim to change this in the case of image enhancements that preserve the overall content of an image. Our key insight is that, in this case, the server can compute and transmit a description of the <i>transformation</i> from input to output, which we call a <i>transform recipe.</i> At equivalent quality, our recipes are much more compact than JPEG images: this reduces the client's download. Furthermore, recipes can be computed from highly compressed inputs which significantly reduces the data uploaded to the server. The client reconstructs a high-fidelity approximation of the output by applying the recipe to its local high-quality input. We demonstrate our results on 168 images and 10 image processing applications, showing that our recipes form a compact representation for a diverse set of image filters. With an equivalent transmission budget, they provide higher-quality results than JPEG-compressed input/output images, with a gain of the order of 10 dB in many cases. We demonstrate the utility of recipes on a mobile phone by profiling the energy consumption and latency for both local and cloud computation: a transform recipe-based pipeline runs 2--4x faster and uses 2--7x less energy than local or naive cloud computation.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"42cddee329f0b11f5c6d6dd63dd3226810b38e9d","venue_1":"ACM Trans. Graph.","year":"2011","title":"Articulated swimming creatures","authors":"Jie Tan, Yuting Gu, Greg Turk, C. Karen Liu","author_ids":"2753365, 7733760, 1713189, 1688533","abstract":"We present a general approach to creating realistic swimming behavior for a given articulated creature body. The two main components of our method are creature/fluid simulation and the optimization of the creature motion parameters. We simulate two-way coupling between the fluid and the articulated body by solving a linear system that matches acceleration at fluid/solid boundaries and that also enforces fluid incompressibility. The swimming motion of a given creature is described as a set of periodic functions, one for each joint degree of freedom. We optimize over the space of these functions in order to find a motion that causes the creature to swim straight and stay within a given energy budget. Our creatures can perform path following by first training appropriate turning maneuvers through offline optimization and then selecting between these motions to track the given path. We present results for a clownfish, an eel, a sea turtle, a manta ray and a frog, and in each case the resulting motion is a good match to the real-world animals. We also demonstrate a plausible swimming gait for a fictional creature that has no real-world counterpart.","cites":"23","conferencePercentile":"50.52631579"},{"venue":"ACM Trans. Graph.","id":"2dab236b4503075423c929eefc786f8870d8ecac","venue_1":"ACM Trans. Graph.","year":"2011","title":"Boundary aligned smooth 3D cross-frame field","authors":"Jin Huang, Yiying Tong, Hongyu Wei, Hujun Bao","author_ids":"3579556, 3225345, 3343652, 1679542","abstract":"In this paper, we present a method for constructing a 3D <i>cross-frame field</i>, a 3D extension of the 2D cross-frame field as applied to surfaces in applications such as quadrangulation and texture synthesis. In contrast to the surface cross-frame field (equivalent to a 4-Way Rotational-Symmetry vector field), symmetry for 3D cross-frame fields cannot be formulated by simple one-parameter 2D rotations in the tangent planes. To address this critical issue, we represent the 3D frames by spherical harmonics, in a manner invariant to combinations of rotations around any axis by multiples of &pi;/2. With such a representation, we can formulate an efficient smoothness measure of the cross-frame field. Through minimization of this measure under certain boundary conditions, we can construct a smooth 3D cross-frame field that is aligned with the surface normal at the boundary. We visualize the resulting cross-frame field through restrictions to the boundary surface, streamline tracing in the volume, and singularities. We also demonstrate the application of the 3D cross-frame field to producing hexahedron-dominant meshes for given volumes, and discuss its potential in high-quality hexahedralization, much as its 2D counterpart has shown in quadrangulation.","cites":"24","conferencePercentile":"53.42105263"},{"venue":"ACM Trans. Graph.","id":"0b351e934210edc715e099c8258a989bad86ca5d","venue_1":"ACM Trans. Graph.","year":"2010","title":"Physics-inspired topology changes for thin fluid features","authors":"Christopher Wojtan, Nils Thürey, Markus H. Gross, Greg Turk","author_ids":"2106076, 1786445, 1743207, 1713189","abstract":"We propose a mesh-based surface tracking method for fluid animation that both preserves fine surface details and robustly adjusts the topology of the surface in the presence of arbitrarily thin features like sheets and strands. We replace traditional re-sampling methods with a convex hull method for connecting surface features during topological changes. This technique permits arbitrarily thin fluid features with minimal re-sampling errors by reusing points from the original surface. We further reduce re-sampling artifacts with a subdivision-based mesh-stitching algorithm, and we use a higher order interpolating subdivision scheme to determine the location of any newly-created vertices. The resulting algorithm efficiently produces detailed fluid surfaces with arbitrarily thin features while maintaining a consistent topology with the underlying fluid simulation.","cites":"38","conferencePercentile":"65.78947368"},{"venue":"ACM Trans. Graph.","id":"040d9ff7f28dd54a14717a03ca39033c87f44f62","venue_1":"ACM Trans. Graph.","year":"2010","title":"Optimizing walking controllers for uncertain inputs and environments","authors":"Jack M. Wang, David J. Fleet, Aaron Hertzmann","author_ids":"1876447, 1793739, 1747779","abstract":"We introduce methods for optimizing physics-based walking controllers for robustness to uncertainty. Many unknown factors, such as external forces, control torques, and user control inputs, cannot be known in advance and must be treated as uncertain. These variables are represented with probability distributions, and a return function scores the desirability of a single motion. Controller optimization entails maximizing the expected value of the return, which is computed by Monte Carlo methods. We demonstrate examples with different sources of uncertainty and task constraints. Optimizing control strategies under uncertainty increases robustness and produces natural variations in style.","cites":"47","conferencePercentile":"80.70175439"},{"venue":"ACM Trans. Graph.","id":"3b4a05be75c5a6e0fa3f12e99e1529b141d09024","venue_1":"ACM Trans. Graph.","year":"2010","title":"A multiscale approach to mesh-based surface tension flows","authors":"Nils Thürey, Christopher Wojtan, Markus H. Gross, Greg Turk","author_ids":"1786445, 2106076, 1743207, 1713189","abstract":"We present an approach to simulate flows driven by surface tension based on triangle meshes. Our method consists of two simulation layers: the first layer is an Eulerian method for simulating surface tension forces that is free from typical strict time step constraints. The second simulation layer is a Lagrangian finite element method that simulates sub-grid scale wave details on the fluid surface. The surface wave simulation employs an unconditionally stable, symplectic time integration method that allows for a high propagation speed due to strong surface tension. Our approach can naturally separate the grid- and sub-grid scales based on a volume-preserving mean curvature flow. As our model for the sub-grid dynamics enforces a local conservation of mass, it leads to realistic pinch off and merging effects. In addition to this method for simulating dynamic surface tension effects, we also present an efficient non-oscillatory approximation for capturing damped surface tension behavior. These approaches allow us to efficiently simulate complex phenomena associated with strong surface tension, such as Rayleigh-Plateau instabilities and crown splashes, in a short amount of time.","cites":"39","conferencePercentile":"67.54385965"},{"venue":"ACM Trans. Graph.","id":"384fce28045b5e23bd83305570c5a875287234ef","venue_1":"ACM Trans. Graph.","year":"2009","title":"Physically guided liquid surface modeling from videos","authors":"Huamin Wang, Miao Liao, Qing Zhang, Ruigang Yang, Greg Turk","author_ids":"6798421, 2928576, 1681771, 1682766, 1713189","abstract":"We present an image-based reconstruction framework to model real water scenes captured by stereoscopic video. In contrast to many image-based modeling techniques that rely on user interaction to obtain high-quality 3D models, we instead apply automatically calculated physically-based constraints to refine the initial model. The combination of image-based reconstruction with physically-based simulation allows us to model complex and dynamic objects such as fluid. Using a depth map sequence as initial conditions, we use a physically based approach that automatically fills in missing regions, removes outliers, and refines the geometric shape so that the final 3D model is consistent to both the input video data and the laws of physics. Physically-guided modeling also makes interpolation or extrapolation in the space-time domain possible, and even allows the fusion of depth maps that were taken at different times or viewpoints. We demonstrated the effectiveness of our framework with a number of real scenes, all captured using only a single pair of cameras.","cites":"20","conferencePercentile":"27.62430939"},{"venue":"ACM Trans. Graph.","id":"02febb2078bdc8bfe0913c45d3339ea3f62dfc4d","venue_1":"ACM Trans. Graph.","year":"2009","title":"Deforming meshes that split and merge","authors":"Christopher Wojtan, Nils Thürey, Markus H. Gross, Greg Turk","author_ids":"2106076, 1786445, 1743207, 1713189","abstract":"We present a method for accurately tracking the moving surface of deformable materials in a manner that gracefully handles topological changes. We employ a Lagrangian surface tracking method, and we use a triangle mesh for our surface representation so that fine features can be retained. We make topological changes to the mesh by first identifying merging or splitting events at a particular grid resolution, and then locally creating new pieces of the mesh in the affected cells using a standard isosurface creation method. We stitch the new, topologically simplified portion of the mesh to the rest of the mesh at the cell boundaries. Our method detects and treats topological events with an emphasis on the preservation of detailed features, while simultaneously simplifying those portions of the material that are not visible. Our surface tracker is not tied to a particular method for simulating deformable materials. In particular, we show results from two significantly different simulators: a Lagrangian FEM simulator with tetrahedral elements, and an Eulerian grid-based fluid simulator. Although our surface tracking method is generic, it is particularly well-suited for simulations that exhibit fine surface details and numerous topological events. Highlights of our results include merging of viscoplastic materials with complex geometry, a taffy-pulling animation with many fold and merge events, and stretching and slicing of stiff plastic material.","cites":"60","conferencePercentile":"79.83425414"},{"venue":"ACM Trans. Graph.","id":"63ff1ae4e09db8b9b3a6159e6c4d52647b6b0ddc","venue_1":"ACM Trans. Graph.","year":"2006","title":"SmoothSketch: 3D free-form shapes from complex sketches","authors":"Olga A. Karpenko, John F. Hughes","author_ids":"3194772, 2057964","abstract":"We introduce SmoothSketch---a system for inferring plausible 3D free-form shapes from visible-contour sketches. In our system, a user's sketch need not be a simple closed curve as in Igarashi's Teddy [1999], but may have cusps and T-junctions, i.e., endpoints of hidden parts of the contour. We follow a process suggested by Williams [1994] for inferring a smooth solid shape from its visible contours: completion of hidden contours, topological shape reconstruction, and smoothly embedding the shape via relaxation. Our main contribution is a practical method to go from a contour drawing to a fairly smooth surface with that drawing as its visible contour. In doing so, we make several technical contributions: &#8226; extending Williams' and Mumford's work [Mumford 1994] on figural completion of hidden contours containing T-junctions to contours containing cusps as well, &#8226; characterizing a class of visible-contour drawings for which inflation can be proved possible, &#8226; finding a topological embedding of the combinatorial surface that Williams creates from the figural completion, and &#8226; creating a fairly smooth solid shape by smoothing the topological embedding using a mass-spring system.We handle many kinds of drawings (including objects with holes), and the generated shapes are plausible interpretations of the sketches. The method can be incorporated into any sketch-based free-form modeling interface like Teddy.","cites":"136","conferencePercentile":"84.25925926"},{"venue":"ACM Trans. Graph.","id":"5ba850744ea30fb41c52339e70634a0f59acf165","venue_1":"ACM Trans. Graph.","year":"2009","title":"Optimizing walking controllers","authors":"Jack M. Wang, David J. Fleet, Aaron Hertzmann","author_ids":"1876447, 1793739, 1747779","abstract":"This paper describes a method for optimizing the parameters of a physics-based controller for full-body, 3D walking. A modified version of the SIMBICON controller [Yin et al. 2007] is optimized for characters of varying body shape, walking speed and step length. The objective function includes terms for power minimization, angular momentum minimization, and minimal head motion, among others. Together these terms produce a number of important features of natural walking, including active toe-off, near-passive knee swing, and leg extension during swing. We explain the specific form of our objective criteria, and show the importance of each term to walking style. We demonstrate optimized controllers for walking with different speeds, variation in body shape, and in ground slope.","cites":"64","conferencePercentile":"83.42541436"},{"venue":"ACM Trans. Graph.","id":"6a0587cf2c5c142f02c7753febf343bba0e262ec","venue_1":"ACM Trans. Graph.","year":"2007","title":"Line drawings via abstracted shading","authors":"Yunjin Lee, Lee Markosian, Seungyong Lee, John F. Hughes","author_ids":"7857167, 2898303, 6518393, 2057964","abstract":"We describe a GPU-based algorithm for rendering a 3D model as a line drawing, based on the insight that a line drawing can be understood as an abstraction of a shaded image. We thus render lines along tone boundaries or thin dark areas in the shaded image. We extend this notion to the dual: we render highlight lines along thin bright areas and tone boundaries. We combine the lines with toon shading to capture broad regions of tone.\n The resulting line drawings effectively convey both shape and material cues. The lines produced by the method can include silhouettes. creases, and ridges, along with a generalization of suggestive contours that responds to lighting as well as viewing changes. The method supports automatic level of abstraction, where the size of depicted shape features adjusts appropriately as the camera zooms in or out. Animated models can be rendered in real time because costly mesh curvature calculations are not needed.","cites":"50","conferencePercentile":"52"},{"venue":"ACM Trans. Graph.","id":"2e38feb413d8c0d4b621dba0386eea6a9604f75b","venue_1":"ACM Trans. Graph.","year":"2008","title":"Fast viscoelastic behavior with thin features","authors":"Christopher Wojtan, Greg Turk","author_ids":"2106076, 1713189","abstract":"We introduce a method for efficiently animating a wide range of deformable materials. We combine a high resolution surface mesh with a tetrahedral finite element simulator that makes use of frequent re-meshing. This combination allows for fast and detailed simulations of complex elastic and plastic behavior. We significantly expand the range of physical parameters that can be simulated with a single technique, and the results are free from common artifacts such as volume-loss, smoothing, popping, and the absence of thin features like strands and sheets. Our decision to couple a high resolution surface with low-resolution physics leads to efficient simulation and detailed surface features, and our approach to creating the tetrahedral mesh leads to an order-of-magnitude speedup over previous techniques in the time spent re-meshing. We compute masses, collisions, and surface tension forces on the scale of the fine mesh, which helps avoid visual artifacts due to the differing mesh resolutions. The result is a method that can simulate a large array of different material behaviors with high resolution features in a short amount of time.","cites":"74","conferencePercentile":"79.01234568"},{"venue":"ACM Trans. Graph.","id":"560984e3eea5d38f395ef67c0ade58058b1197ef","venue_1":"ACM Trans. Graph.","year":"2010","title":"Reconstructing surfaces of particle-based fluids using anisotropic kernels","authors":"Jihun Yu, Greg Turk","author_ids":"2977637, 1713189","abstract":"In this paper we present a novel surface reconstruction method for particle-based fluid simulators such as Smoothed Particle Hydrodynamics. In particle-based simulations, fluid surfaces are usually defined as a level set of an implicit function. We formulate the implicit function as a sum of <i>anisotropic</i> smoothing kernels, and the direction of anisotropy at a particle is determined by performing Principal Component Analysis (PCA) over the neighboring particles. In addition, we perform a smoothing step that re-positions the centers of these smoothing kernels. Since these anisotropic smoothing kernels capture the local particle distributions more accurately, our method has advantages over existing methods in representing smooth surfaces, thin streams and sharp features of fluids. Our method is fast, easy to implement, and our results demonstrate a significant improvement in the quality of reconstructed surfaces as compared to existing methods.","cites":"54","conferencePercentile":"88.01169591"},{"venue":"ACM Trans. Graph.","id":"c16f39237a7d5680322d4c0fb56ade41f089f3be","venue_1":"ACM Trans. Graph.","year":"2014","title":"Flow-complex-based shape reconstruction from 3D curves","authors":"Bardia Sadri, Karan Singh","author_ids":"2474781, 1682205","abstract":"We address the problem of shape reconstruction from a sparse unorganized collection of 3D curves, typically generated by increasingly popular 3D curve sketching applications. Experimentally, we observe that human understanding of shape from connected 3D curves is largely consistent, and informed by both topological connectivity and geometry of the curves. We thus employ the <i>flow complex</i>, a structure that captures aspects of input topology and geometry, in a novel algorithm to produce an intersection-free 3D triangulated shape that interpolates the input 3D curves. Our approach is able to triangulate highly nonplanar and concave curve cycles, providing a robust 3D mesh and parametric embedding for challenging 3D curve input. Our evaluation is fourfold: we show our algorithm to match designer-selected curve cycles for surfacing; we produce user-acceptable shapes for a wide range of curve inputs; we show our approach to be predictable and robust to curve addition and deletion; we compare our results to prior art.","cites":"2","conferencePercentile":"8.436213992"},{"venue":"ACM Trans. Graph.","id":"02ffd1f7e9cd429c0b85bdfd2e97c3eba397700f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Interactive localized liquid motion editing","authors":"Zherong Pan, Jin Huang, Yiying Tong, Changxi Zheng, Hujun Bao","author_ids":"1831485, 3579556, 3225345, 1797875, 1679542","abstract":"Animation techniques for controlling liquid simulation are challenging: they commonly require carefully setting initial and boundary conditions or performing a costly numerical optimization scheme against user-provided keyframes or animation sequences. Either way, the whole process is laborious and computationally expensive.\n We introduce a novel method to provide intuitive and interactive control of liquid simulation. Our method enables a user to locally edit selected keyframes and automatically propagates the editing in a nearby temporal region using geometric deformation. We formulate our local editing techniques as a small-scale nonlinear optimization problem which can be solved interactively. With this uniformed formulation, we propose three editing metaphors, including (i) sketching local fluid features using a few user strokes, (ii) dragging a local fluid region, and (iii) controlling a local shape with a small mesh patch. Finally, we use the edited liquid animation to guide an offline high-resolution simulation to recover more surface details. We demonstrate the intuitiveness and efficacy of our method in various practical scenarios.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"44dc0035ca5abd1fa54b172c24e3fea66305a2b1","venue_1":"ACM Trans. Graph.","year":"2014","title":"Fast Local Laplacian Filters: Theory and Applications","authors":"Mathieu Aubry, Sylvain Paris, Samuel W. Hasinoff, Jan Kautz, Frédo Durand","author_ids":"3124570, 1720990, 1979640, 1690538, 1728125","abstract":"Multiscale manipulations are central to image editing but also prone to halos. Achieving artifact-free results requires sophisticated edge-aware techniques and careful parameter tuning. These shortcomings were recently addressed by the local Laplacian filters, which can achieve a broad range of effects using standard Laplacian pyramids. However, these filters are slow to evaluate and their relationship to other approaches is unclear. In this article, we show that they are closely related to anisotropic diffusion and to bilateral filtering. Our study also leads to a variant of the bilateral filter that produces cleaner edges while retaining its speed. Building upon this result, we describe an acceleration scheme for local Laplacian filters on gray-scale images that yields speedups on the order of 50&#215;. Finally, we demonstrate how to use local Laplacian filters to alter the distribution of gradients in an image. We illustrate this property with a robust algorithm for photographic style transfer.","cites":"20","conferencePercentile":"88.88888889"},{"venue":"ACM Trans. Graph.","id":"019e13deb2b30e6e0a99b5b39b4ffde568a7cac5","venue_1":"ACM Trans. Graph.","year":"2015","title":"Augmented Airbrush for Computer Aided Painting (CAP)","authors":"Roy Shilkrot, Pattie Maes, Joseph A. Paradiso, Amit Zoran","author_ids":"2509354, 1701876, 4798651, 2866829","abstract":"We present an augmented airbrush that allows novices to experience the art of spray painting. Inspired by the thriving field of smart tools, our handheld device uses 6DOF tracking, augmentation of the airbrush trigger, and a specialized algorithm to restrict the application of paint to a preselected reference image. Our device acts both as a physical spraying device and as an intelligent assistive tool, providing simultaneous manual and computerized control. Unlike prior art, here the virtual simulation guides the physical rendering (<i>inverse rendering</i>), allowing for a new spray painting experience with singular physical results. We present our novel hardware design, control software, and a user study that verifies our research objectives.","cites":"9","conferencePercentile":"88.16326531"},{"venue":"ACM Trans. Graph.","id":"10c6a21e2f6cb5ca6785c0e858d5fd35ddaa949f","venue_1":"ACM Trans. Graph.","year":"2009","title":"User-assisted intrinsic images","authors":"Adrien Bousseau, Sylvain Paris, Frédo Durand","author_ids":"2149814, 1720990, 1728125","abstract":"For many computational photography applications, the lighting and materials in the scene are critical pieces of information. We seek to obtain <i>intrinsic images</i>, which decompose a photo into the product of an <i>illumination</i> component that represents lighting effects and a <i>reflectance</i> component that is the color of the observed material. This is an under-constrained problem and automatic methods are challenged by complex natural images. We describe a new approach that enables users to guide an optimization with simple indications such as regions of constant reflectance or illumination. Based on a simple assumption on local reflectance distributions, we derive a new propagation energy that enables a closed form solution using linear least-squares. We achieve fast performance by introducing a novel downsampling that preserves local color distributions. We demonstrate intrinsic image decomposition on a variety of images and show applications.","cites":"67","conferencePercentile":"85.91160221"},{"venue":"ACM Trans. Graph.","id":"7ac0a2cd0cc0ebf26474b88e1b0a38498ab1df3d","venue_1":"ACM Trans. Graph.","year":"2011","title":"Displacement interpolation using Lagrangian mass transport","authors":"Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, Wolfgang Heidrich","author_ids":"1722900, 1745029, 1720990, 1752192","abstract":"Interpolation between pairs of values, typically vectors, is a fundamental operation in many computer graphics applications. In some cases simple linear interpolation yields meaningful results without requiring domain knowledge. However, interpolation between pairs of distributions or pairs of functions often demands more care because features may exhibit translational motion between exemplars. This property is not captured by linear interpolation. This paper develops the use of <i>displacement interpolation</i> for this class of problem, which provides a generic method for interpolating between distributions or functions based on advection instead of blending. The functions can be non-uniformly sampled, high-dimensional, and defined on non-Euclidean manifolds, e.g., spheres and tori. Our method decomposes distributions or functions into sums of radial basis functions (RBFs). We solve a mass transport problem to pair the RBFs and apply partial transport to obtain the interpolated function. We describe practical methods for computing the RBF decomposition and solving the transport problem. We demonstrate the interpolation approach on synthetic examples, BRDFs, color distributions, environment maps, stipple patterns, and value functions.","cites":"36","conferencePercentile":"74.21052632"},{"venue":"ACM Trans. Graph.","id":"2b1e17eac9d05070f40a886c959cfc6b6f8f9ef1","venue_1":"ACM Trans. Graph.","year":"2009","title":"Direct trimming of NURBS surfaces on the GPU","authors":"Andre Schollmeyer, Bernd Fröhlich","author_ids":"2433019, 5399927","abstract":"This paper presents a highly efficient direct trimming technique for NURBS surfaces, which is applicable to tessellation-based rendering as well as ray tracing systems. The central idea is to split the trim curves into monotonic segments with respect to the two parameter dimensions of the surface patches. We use an optimized bisection method to classify a point with respect to each monotonic trim curve segment without performing an actual intersection test. Our hierarchical acceleration structure allows the use of a large number of such curve segments and performs the bisection method only for points contained in the bounding boxes of the curve segments.\n We have integrated our novel point classification scheme into a GPU-based NURBS ray casting system and implemented the entire trimmed NURBS rendering algorithm in a single OpenGL GLSL shader. The shader can handle surfaces and trim curves of arbitrary degrees, which allows the use of original CAD data without incorporating any approximations. Performance data confirms that our trimming approach can deal with hundreds of thousands of trim curves at interactive rates. Our point classification scheme can be applied to other application domains dealing with complex curved regions including flood fills, font rendering and vector graphics mapped on arbitrary surfaces.","cites":"3","conferencePercentile":"3.591160221"},{"venue":"ACM Trans. Graph.","id":"160eef2742634b6576c23ba6400a54448cf09ded","venue_1":"ACM Trans. Graph.","year":"2015","title":"Animating human dressing","authors":"Alexander Clegg, Jie Tan, Greg Turk, C. Karen Liu","author_ids":"2806106, 2753365, 1713189, 1688533","abstract":"Dressing is one of the most common activities in human society. Perfecting the skill of dressing can take an average child three to four years of daily practice. The challenge is primarily due to the combined difficulty of coordinating different body parts and manipulating soft and deformable objects (clothes). We present a technique to synthesize human dressing by controlling a human character to put on an article of simulated clothing. We identify a set of <i>primitive actions</i> which account for the vast majority of motions observed in human dressing. These primitive actions can be assembled into a variety of motion sequences for dressing different garments with different styles. Exploiting both feed-forward and feedback control mechanisms, we develop a dressing controller to handle each of the primitive actions. The controller plans a path to achieve the action goal while making constant adjustments locally based on the current state of the simulated cloth when necessary. We demonstrate that our framework is versatile and able to animate dressing with different clothing types including a jacket, a pair of shorts, a robe, and a vest. Our controller is also robust to different cloth mesh resolutions which can cause the cloth simulator to generate significantly different cloth motions. In addition, we show that the same controller can be extended to assistive dressing.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"b1fc5a70f8d8e3bbc704158a528b04005bcf1011","venue_1":"ACM Trans. Graph.","year":"2014","title":"Learning bicycle stunts","authors":"Jie Tan, Yuting Gu, C. Karen Liu, Greg Turk","author_ids":"2753365, 7733760, 1688533, 1713189","abstract":"We present a general approach for simulating and controlling a human character that is riding a bicycle. The two main components of our system are offline learning and online simulation. We simulate the bicycle and the rider as an articulated rigid body system. The rider is controlled by a policy that is optimized through offline learning. We apply policy search to learn the optimal policies, which are parameterized with splines or neural networks for different bicycle maneuvers. We use Neuroevolution of Augmenting Topology (NEAT) to optimize both the parametrization and the parameters of our policies. The learned controllers are robust enough to withstand large perturbations and allow interactive user control. The rider not only learns to steer and to balance in normal riding situations, but also learns to perform a wide variety of stunts, including wheelie, endo, bunny hop, front wheel pivot and back hop.","cites":"10","conferencePercentile":"61.72839506"},{"venue":"ACM Trans. Graph.","id":"dae57540710c2cc5ba6e43634f7357decfe1186f","venue_1":"ACM Trans. Graph.","year":"2016","title":"All-hex meshing using closed-form induced polycube","authors":"Xianzhong Fang, Weiwei Xu, Hujun Bao, Jin Huang","author_ids":"2602234, 6953977, 1679542, 3579556","abstract":"The polycube-based hexahedralization methods are robust to generate all-hex meshes without internal singularities. They avoid the difficulty to control the global singularity structure for a valid hexahedralization in frame-field based methods. To thoroughly utilize this advantage, we propose to use a frame field without internal singularities to guide the polycube construction. Theoretically, our method extends the vector fields associated with the polycube from exact forms to closed forms, which are curl free everywhere but may be not globally integrable. The closed forms give additional degrees of freedom to deal with the topological structure of high-genus models, and also provide better initial axis alignment for subsequent polycube generation. We demonstrate the advantages of our method on various models, ranging from genus-zero models to high-genus ones, and from single-boundary models to multiple-boundary ones.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"58dd58693d6264ab2105d374c5da0c4ce627b856","venue_1":"ACM Trans. Graph.","year":"2012","title":"User-guided white balance for mixed lighting conditions","authors":"Ivaylo Boyadzhiev, Kavita Bala, Sylvain Paris, Frédo Durand","author_ids":"2162534, 8261370, 1720990, 1728125","abstract":"Proper white balance is essential in photographs to eliminate color casts due to illumination. The single-light case is hard to solve automatically but relatively easy for humans. Unfortunately, many scenes contain multiple light sources such as an indoor scene with a window, or when a flash is used in a tungsten-lit room. The light color can then vary on a per-pixel basis and the problem becomes challenging at best, even with advanced image editing tools.\n We propose a solution to the ill-posed mixed light white balance problem, based on user guidance. Users scribble on a few regions that should have the same color, indicate one or more regions of neutral color, and select regions where the current color looks correct. We first expand the provided scribble groups to more regions using pixel similarity and a robust voting scheme. We formulate the spatially varying white balance problem as a sparse data interpolation problem in which the user scribbles and their extensions form constraints. We demonstrate that our approach can produce satisfying results on a variety of scenes with intuitive scribbles and without any knowledge about the lights.","cites":"12","conferencePercentile":"30.05050505"},{"venue":"ACM Trans. Graph.","id":"15e736c258c0d6e3ac25070889bbeb523be534bd","venue_1":"ACM Trans. Graph.","year":"2014","title":"Spectral Quadrangulation with Feature Curve Alignment and Element Size Control","authors":"Ruotian Ling, Jin Huang, Bert Jüttler, Feng Sun, Hujun Bao, Wenping Wang","author_ids":"2570737, 3579556, 1737026, 4803694, 1679542, 1698520","abstract":"Existing methods for surface quadrangulation cannot ensure accurate alignment with feature or boundary curves and tight control of local element size, which are important requirements in many numerical applications (e.g., FEA). Some methods rely on a prescribed direction field to guide quadrangulation for feature alignment, but such a direction field may conflict with a desired density field, thus making it difficult to control the element size. We propose a new spectral method that achieves both accurate feature curve alignment and tight control of local element size according to a given density field. Specifically, the following three technical contributions are made. First, to make the quadrangulation align accurately with feature curves or surface boundary curves, we introduce novel boundary conditions for wave-like functions that satisfy the Helmholtz equation approximately in the least squares sense. Such functions, called <i>quasi-eigenfunctions</i>, are computed efficiently as the solutions to a variational problem. Second, the mesh element size is effectively controlled by locally modulating the Laplace operator in the Helmholtz equation according to a given density field. Third, to improve robustness, we propose a novel scheme to minimize the vibration difference of the quasi-eigenfunction in two orthogonal directions. It is demonstrated by extensive experiments that our method outperforms previous methods in generating feature-aligned quadrilateral meshes with tight control of local elememt size. We further present some preliminary results to show that our method can be extended to generating hex-dominant volume meshes.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"767d4e892b406fd769aa49fc09a664069f108a5f","venue_1":"ACM Trans. Graph.","year":"2012","title":"Coherent intrinsic images from photo collections","authors":"Pierre-Yves Laffont, Adrien Bousseau, Sylvain Paris, Frédo Durand, George Drettakis","author_ids":"2615801, 2149814, 1720990, 1728125, 1721779","abstract":"An intrinsic image is a decomposition of a photo into an illumination layer and a reflectance layer, which enables powerful editing such as the alteration of an object's material independently of its illumination. However, decomposing a single photo is highly under-constrained and existing methods require user assistance or handle only simple scenes. In this paper, we compute intrinsic decompositions using several images of the same scene under different viewpoints and lighting conditions. We use multi-view stereo to automatically reconstruct 3D points and normals from which we derive relationships between reflectance values at different locations, across multiple views and consequently different lighting conditions. We use robust estimation to reliably identify reflectance ratios between pairs of points. From these, we infer constraints for our optimization and enforce a coherent solution across multiple views and illuminations. Our results demonstrate that this constrained optimization yields high-quality and coherent intrinsic decompositions of complex scenes. We illustrate how these decompositions can be used for image-based illumination transfer and transitions between views with consistent lighting.","cites":"37","conferencePercentile":"81.81818182"},{"venue":"ACM Trans. Graph.","id":"483165366f51369c602aed21ed2fd5d77af0493a","venue_1":"ACM Trans. Graph.","year":"2015","title":"Frame field generation through metric customization","authors":"Tengfei Jiang, Xianzhong Fang, Jin Huang, Hujun Bao, Yiying Tong, Mathieu Desbrun","author_ids":"7148874, 2602234, 3579556, 1679542, 3225345, 1716096","abstract":"This paper presents a new technique for frame field generation. As generic frame fields (with arbitrary anisotropy, orientation, and sizing) can be regarded as cross fields in a specific Riemannian metric, we tackle frame field design by first computing a discrete metric on the input surface that is compatible with a sparse or dense set of input constraints. The final frame field is then found by computing an optimal cross field in this customized metric. We propose frame field design constraints on alignment, size, and skewness at arbitrary locations on the mesh as well as along feature curves, offering much improved flexibility over previous approaches. We demonstrate the advantages of our frame field generation through the automatic quadrangulation of man-made and organic shapes with controllable anisotropy, robust handling of narrow surface strips, and precise feature alignment. We also extend our technique to the design of <i>n</i>-vector fields.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"ea98278a672a6ee6fbe43060a1ad181f4f8cc40c","venue_1":"ACM Trans. Graph.","year":"2015","title":"Subspace dynamic simulation using rotation-strain coordinates","authors":"Zherong Pan, Hujun Bao, Jin Huang","author_ids":"1831485, 1679542, 3579556","abstract":"In this paper, we propose a full featured and efficient subspace simulation method in the rotation-strain (RS) space for elastic objects. Sharply different from previous methods using the rotation-strain space, except for the ability to handle non-linear elastic materials and external forces, our method correctly formulates the kinetic energy, centrifugal and Coriolis forces which significantly reduces the dynamic artifacts. We show many techniques used in the Euclidean space methods, such as modal derivatives, polynomial and cubature approximation, can be adapted to our RS simulator. Carefully designed experiments show that the equation of motion in RS space has less non-linearity than its Euclidean counterpart, and as a consequence, our method has great advantages of lower dimension and computational complexity than state-of-the-art methods in the Euclidean space.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"39519d8858e66f06ee8eee2eccbe69a71c3fe522","venue_1":"ACM Trans. Graph.","year":"2015","title":"Power particles: an incompressible fluid solver based on power diagrams","authors":"Fernando de Goes, Corentin Wallez, Jin Huang, Dmitry Pavlov, Mathieu Desbrun","author_ids":"2322290, 2968625, 3579556, 8001522, 1716096","abstract":"This paper introduces a new particle-based approach to incompressible fluid simulation. We depart from previous Lagrangian methods by considering fluid particles no longer purely as material points, but also as volumetric parcels that partition the fluid domain. The fluid motion is described as a time series of well-shaped power diagrams (hence the name <i>power particles</i>), offering evenly spaced particles and accurate pressure computations. As a result, we circumvent the typical excess damping arising from kernel-based evaluations of internal forces or density without having recourse to auxiliary Eulerian grids. The versatility of our solver is demonstrated by the simulation of multiphase flows and free surfaces.","cites":"9","conferencePercentile":"88.16326531"},{"venue":"ACM Trans. Graph.","id":"63480b9a1a7e0f404e9a527c28666244ec67721b","venue_1":"ACM Trans. Graph.","year":"2013","title":"User-assisted image compositing for photographic lighting","authors":"Ivaylo Boyadzhiev, Sylvain Paris, Kavita Bala","author_ids":"2162534, 1720990, 8261370","abstract":"Good lighting is crucial in photography and can make the difference between a great picture and a discarded image. Traditionally, professional photographers work in a studio with many light sources carefully set up, with the goal of getting a near-final image at exposure time, with post-processing mostly focusing on aspects orthogonal to lighting. Recently, a new workflow has emerged for architectural and commercial photography, where photographers capture several photos from a fixed viewpoint with a moving light source. The objective is not to produce the final result immediately, but rather to capture useful data that are later processed, often significantly, in photo editing software to create the final well-lit image.\n This new workflow is flexible, requires less manual setup, and works well for time-constrained shots. But dealing with several tens of unorganized layers is painstaking, requiring hours to days of manual effort, as well as advanced photo editing skills. Our objective in this paper is to make the compositing step easier. We describe a set of optimizations to assemble the input images to create a few <i>basis lights</i> that correspond to common goals pursued by photographers, e.g., accentuating edges and curved regions. We also introduce <i>modifiers</i> that capture standard photographic tasks, e.g., to alter the lights to soften highlights and shadows, akin to umbrellas and soft boxes. Our experiments with novice and professional users show that our approach allows them to quickly create satisfying results, whereas working with unorganized images requires considerably more time. Casual users particularly benefit from our approach since coping with a large number of layers is daunting for them and requires significant experience.","cites":"6","conferencePercentile":"16.5158371"},{"venue":"ACM Trans. Graph.","id":"151248ca60c7a2cfebc353939ee9c19eca0eaff2","venue_1":"ACM Trans. Graph.","year":"2014","title":"Space-time editing of elastic motion through material optimization and reduction","authors":"Siwang Li, Jin Huang, Fernando de Goes, Xiaogang Jin, Hujun Bao, Mathieu Desbrun","author_ids":"2314606, 3579556, 2322290, 1735879, 1679542, 1716096","abstract":"We present a novel method for elastic animation editing with space-time constraints. In a sharp departure from previous approaches, we not only optimize control forces added to a linearized dynamic model, but also optimize material properties to better match user constraints and provide plausible and consistent motion. Our approach achieves efficiency and scalability by performing all computations in a reduced rotation-strain (RS) space constructed with both cubature and geometric reduction, leading to two orders of magnitude improvement over the original RS method. We demonstrate the utility and versatility of our method in various applications, including motion editing, pose interpolation, and estimation of material parameters from existing animation sequences.","cites":"17","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"5c08a91d6b9cfa18ffe800303a8e632577054d24","venue_1":"ACM Trans. Graph.","year":"2013","title":"Data-driven hallucination of different times of day from a single outdoor photo","authors":"Yi-Chang Shih, Sylvain Paris, Frédo Durand, William T. Freeman","author_ids":"1989579, 1720990, 1728125, 1768236","abstract":"We introduce \"time hallucination\": synthesizing a plausible image at a different time of day from an input image. This challenging task often requires dramatically altering the color appearance of the picture. In this paper, we introduce the first data-driven approach to automatically creating a plausible-looking photo that appears as though it were taken at a different time of day. The time of day is specified by a semantic time label, such as \"night\".\n Our approach relies on a database of time-lapse videos of various scenes. These videos provide rich information about the variations in color appearance of a scene throughout the day. Our method transfers the color appearance from videos with a similar scene as the input photo. We propose a <i>locally affine model</i> learned from the video for the transfer, allowing our model to synthesize new color data while retaining image details. We show that this model can hallucinate a wide range of different times of day. The model generates a large sparse linear system, which can be solved by off-the-shelf solvers. We validate our methods by synthesizing transforming photos of various outdoor scenes to four times of interest: daytime, the golden hour, the blue hour, and nighttime.","cites":"29","conferencePercentile":"84.84162896"},{"venue":"ACM Trans. Graph.","id":"06af4ac125c20b387da5d9f99b095999c0dd151b","venue_1":"ACM Trans. Graph.","year":"2014","title":"Style transfer for headshot portraits","authors":"Yi-Chang Shih, Sylvain Paris, Connelly Barnes, William T. Freeman, Frédo Durand","author_ids":"1989579, 1720990, 1794537, 1768236, 1728125","abstract":"Headshot portraits are a popular subject in photography but to achieve a compelling visual style requires advanced skills that a casual photographer will not have. Further, algorithms that automate or assist the stylization of generic photographs do not perform well on headshots due to the feature-specific, local retouching that a professional photographer typically applies to generate such portraits. We introduce a technique to transfer the style of an example headshot photo onto a new one. This can allow one to easily reproduce the look of renowned artists. At the core of our approach is a new multiscale technique to robustly transfer the local statistics of an example portrait onto a new one. This technique matches properties such as the local contrast and the overall lighting direction while being tolerant to the unavoidable differences between the faces of two different people. Additionally, because artists sometimes produce entire headshot collections in a common style, we show how to automatically find a good example to use as a reference for a given portrait, enabling style transfer without the user having to search for a suitable example for each input. We demonstrate our approach on data taken in a controlled environment as well as on a large set of photos downloaded from the Internet. We show that we can successfully handle styles by a variety of different artists.","cites":"22","conferencePercentile":"91.56378601"},{"venue":"ACM Trans. Graph.","id":"352461b920071caa552eaa13204de8f109061afa","venue_1":"ACM Trans. Graph.","year":"2014","title":"ℓ1-Based Construction of Polycube Maps from Complex Shapes","authors":"Jin Huang, Tengfei Jiang, Zeyun Shi, Yiying Tong, Hujun Bao, Mathieu Desbrun","author_ids":"3579556, 7148874, 3288711, 3225345, 1679542, 1716096","abstract":"Polycube maps of triangle meshes have proved useful in a wide range of applications, including texture mapping and hexahedral mesh generation. However, constructing either fully automatically or with limited user control a low-distortion polycube from a detailed surface remains challenging in practice. We propose a variational method for deforming an input triangle mesh into a polycube shape through minimization of the &ell;<sub>1</sub>-norm of the mesh normals, regularized via an as-rigid-as-possible volumetric distortion energy. Unlike previous work, our approach makes no assumption on the orientation, or on the presence of features in the input model. User-guided control over the resulting polycube map is also offered to increase design flexibility. We demonstrate the robustness, efficiency, and controllability of our method on a variety of examples, and explore applications in hexahedral remeshing and quadrangulation.","cites":"13","conferencePercentile":"76.54320988"},{"venue":"ACM Trans. Graph.","id":"17bd125578eac662eddb01e78c1dc08e4ef51cd3","venue_1":"ACM Trans. Graph.","year":"2012","title":"Decoupling algorithms from schedules for easy optimization of image processing pipelines","authors":"Jonathan Ragan-Kelley, Andrew Adams, Sylvain Paris, Marc Levoy, Saman P. Amarasinghe, Frédo Durand","author_ids":"2488277, 4030789, 1720990, 1801789, 1709150, 1728125","abstract":"Using existing programming tools, writing high-performance image processing code requires sacrificing readability, portability, and modularity. We argue that this is a consequence of conflating what computations define the <i>algorithm</i>, with decisions about <i>storage</i> and the <i>order</i> of computation. We refer to these latter two concerns as the <i>schedule</i>, including choices of tiling, fusion, recomputation vs. storage, vectorization, and parallelism.\n We propose a representation for feed-forward imaging pipelines that separates the algorithm from its schedule, enabling high-performance without sacrificing code clarity. This decoupling simplifies the algorithm specification: images and intermediate buffers become functions over an infinite integer domain, with no explicit storage or boundary conditions. Imaging pipelines are compositions of functions. Programmers separately specify scheduling strategies for the various functions composing the algorithm, which allows them to efficiently explore different optimizations without changing the algorithmic code.\n We demonstrate the power of this representation by expressing a range of recent image processing applications in an embedded domain specific language called Halide, and compiling them for ARM, x86, and GPUs. Our compiler targets SIMD units, multiple cores, and complex memory hierarchies. We demonstrate that it can handle algorithms such as a camera raw pipeline, the bilateral grid, fast local Laplacian filtering, and image segmentation. The algorithms expressed in our language are both shorter and faster than state-of-the-art implementations.","cites":"63","conferencePercentile":"96.96969697"},{"venue":"ACM Trans. Graph.","id":"bcf1fc23df2e4cc328d4791116d6d4f80eedbf83","venue_1":"ACM Trans. Graph.","year":"2014","title":"Interactive shape modeling using a skeleton-mesh co-representation","authors":"Jakob Andreas Bærentzen, Rinat Abdrashitov, Karan Singh","author_ids":"2505174, 2003680, 1682205","abstract":"We introduce the Polar-Annular Mesh representation (PAM). A PAM is a mesh-skeleton co-representation designed for the modeling of 3D organic, articulated shapes. A PAM represents a manifold mesh as a partition of polar (triangle fans) and annular (rings of quads) regions. The skeletal topology of a shape is uniquely embedded in the mesh connectivity of a PAM, enabling both surface and skeletal modeling operations, interchangeably and directly on the mesh itself. We develop an algorithm to convert arbitrary triangle meshes into PAMs as well as techniques to simplify PAMs and a method to convert a PAM to a quad-only mesh. We further present a PAM-based multi-touch sculpting application in order to demonstrate its utility as a shape representation for the interactive modeling of organic, articulated figures as well as for editing and posing of pre-existing models.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"46a0af2aac7aa034b2723b455e2bccc7d91c1b6b","venue_1":"ACM Trans. Graph.","year":"2015","title":"Gaze-Driven Video Re-Editing","authors":"Eakta Jain, Yaser Sheikh, Ariel Shamir, Jessica K. Hodgins","author_ids":"2617152, 1774867, 2947946, 1788773","abstract":"Given the current profusion of devices for viewing media, video content created at one aspect ratio is often viewed on displays with different aspect ratios. Many previous solutions address this problem by retargeting or resizing the video, but a more general solution would re-edit the video for the new display. Our method employs the three primary editing operations: pan, cut, and zoom. We let viewers implicitly reveal what is important in a video by tracking their gaze as they watch the video. We present an algorithm that optimizes the path of a cropping window based on the collected eyetracking data, finds places to cut, and computes the size of the cropping window. We present results on a variety of video clips, including close-up and distant shots, and stationary and moving cameras. We conduct two experiments to evaluate our results. First, we eyetrack viewers on the result videos generated by our algorithm, and second, we perform a subjective assessment of viewer preference. These experiments show that viewer gaze patterns are similar on our result videos and on the original video clips, and that viewers prefer our results to an optimized crop-and-warp algorithm.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"66d1991dfbfa6fd79bdacc798b36a48dc4476ee0","venue_1":"ACM Trans. Graph.","year":"2015","title":"Modeling Character Canvases from Cartoon Drawings","authors":"Mikhail Bessmeltsev, William Chang, Nicholas Vining, Alla Sheffer, Karan Singh","author_ids":"2455246, 2540201, 1983439, 3354923, 1682205","abstract":"We introduce a novel technique for the construction of a 3D character proxy, or <i>canvas</i>, directly from a 2D cartoon drawing and a user-provided correspondingly posed 3D skeleton. Our choice of input is motivated by the observation that traditional cartoon characters are well approximated by a union of generalized surface of revolution body parts, anchored by a skeletal structure. While typical 2D character contour drawings allow ambiguities in 3D interpretation, our use of a 3D skeleton eliminates such ambiguities and enables the construction of believable character canvases from complex drawings. Our canvases conform to the 2D contours of the input drawings, and are consistent with the perceptual principles of Gestalt continuity, simplicity, and contour persistence. We first segment the input 2D contours into individual body-part outlines corresponding to 3D skeletal bones using the Gestalt continuation principle to correctly resolve inter-part occlusions in the drawings. We then use this segmentation to compute the canvas geometry, generating 3D generalized surfaces of revolution around the skeletal bones that conform to the original outlines and balance simplicity against contour persistence. The combined method generates believable canvases for characters drawn in complex poses with numerous inter-part occlusions, variable contour depth, and significant foreshortening. Our canvases serve as 3D geometric proxies for cartoon characters, enabling unconstrained 3D viewing, articulation, and non-photorealistic rendering. We validate our algorithm via a range of user studies and comparisons to ground-truth 3D models and artist-drawn results. We further demonstrate a compelling gallery of 3D character canvases created from a diverse set of cartoon drawings with matching 3D skeletons.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"b62b7a65fc0f5112757d356bda88852b90397c7e","venue_1":"ACM Trans. Graph.","year":"2016","title":"Occluded Imaging with Time-of-Flight Sensors","authors":"Achuta Kadambi, Hang Zhao, Boxin Shi, Ramesh Raskar","author_ids":"7992331, 7574604, 2563079, 1717566","abstract":"We explore the question of whether phase-based time-of-flight (TOF) range cameras can be used for looking around corners and through scattering diffusers. By connecting TOF measurements with theory from array signal processing, we conclude that performance depends on two primary factors: camera modulation frequency and the width of the specular lobe (&#8220;shininess&#8221;) of the wall. For purely Lambertian walls, commodity TOF sensors achieve resolution on the order of meters between targets. For seemingly diffuse walls, such as posterboard, the resolution is drastically reduced, to the order of 10cm. In particular, we find that the relationship between reflectance and resolution is nonlinear&#8212;a slight amount of shininess can lead to a dramatic improvement in resolution. Since many realistic scenes exhibit a slight amount of shininess, we believe that off-the-shelf TOF cameras can look around corners.","cites":"4","conferencePercentile":"95.35864979"},{"venue":"ACM Trans. Graph.","id":"b869596b07cd1f65b182501d449c8261e4000e77","venue_1":"ACM Trans. Graph.","year":"2015","title":"An interactive tool for designing quadrotor camera shots","authors":"Niels Joubert, Mike Roberts, Anh Truong, Floraine Berthouzoz, Pat Hanrahan","author_ids":"2568050, 3311425, 6684681, 2842099, 4982303","abstract":"Cameras attached to small quadrotor aircraft are rapidly becoming a ubiquitous tool for cinematographers, enabling dynamic camera movements through 3D environments. Currently, professionals use these cameras by flying quadrotors manually, a process which requires much skill and dexterity. In this paper, we investigate the needs of quadrotor cinematographers, and build a tool to support video capture using quadrotor-based camera systems. We begin by conducting semi-structured interviews with professional photographers and videographers, from which we extract a set of design principles. We present a tool based on these principles for designing and autonomously executing quadrotor-based camera shots. Our tool enables users to: (1) specify shots visually using keyframes; (2) preview the resulting shots in a virtual environment; (3) precisely control the timing of shots using easing curves; and (4) capture the resulting shots in the real world with a single button click using commercially available quadrotors. We evaluate our tool in a user study with novice and expert cinematographers. We show that our tool makes it possible for novices and experts to design compelling and challenging shots, and capture them fully autonomously.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"e930c7e4d0bd6a76ad396eb5908f0a4665f573b3","venue_1":"ACM Trans. Graph.","year":"2015","title":"Visual transcripts: lecture notes from blackboard-style lecture videos","authors":"Hijung Shin, Floraine Berthouzoz, Wilmot Li, Frédo Durand","author_ids":"2596714, 2842099, 2812691, 1728125","abstract":"Blackboard-style lecture videos are popular, but learning using existing video player interfaces can be challenging. Viewers cannot consume the lecture material at their own pace, and the content is also difficult to search or skim. For these reasons, some people prefer lecture notes to videos. To address these limitations, we present <i>Visual Transcripts</i>, a readable representation of lecture videos that combines visual information with transcript text. To generate a Visual Transcript, we first segment the visual content of a lecture into discrete visual entities that correspond to equations, figures, or lines of text. Then, we analyze the temporal correspondence between the transcript and visuals to determine how sentences relate to visual entities. Finally, we arrange the text and visuals in a linear layout based on these relationships. We compare our result with a standard video player, and a state-of-the-art interface designed specifically for blackboard-style lecture videos. User evaluation suggests that users prefer our interface for learning and that our interface is effective in helping them browse or search through lecture videos.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"e0a051ad689963e33eaa854ed4bf849b91240f34","venue_1":"ACM Trans. Graph.","year":"2013","title":"Parsing sewing patterns into 3D garments","authors":"Floraine Berthouzoz, Akash Garg, Danny M. Kaufman, Eitan Grinspun, Maneesh Agrawala","author_ids":"2842099, 2004417, 2972719, 7522998, 1820412","abstract":"We present techniques for automatically parsing existing sewing patterns and converting them into 3D garment models. Our parser takes a sewing pattern in PDF format as input and starts by extracting the set of panels and styling elements (e.g. darts, pleats and hemlines) contained in the pattern. It then applies a combination of machine learning and integer programming to infer how the panels must be stitched together to form the garment. Our system includes an interactive garment simulator that takes the parsed result and generates the corresponding 3D model. Our fully automatic approach correctly parses 68% of the sewing patterns in our collection. Most of the remaining patterns contain only a few errors that can be quickly corrected within the garment simulator. Finally we present two applications that take advantage of our collection of parsed sewing patterns. Our garment hybrids application lets users smoothly interpolate multiple garments in the 2D space of patterns. Our sketch-based search application allows users to navigate the pattern collection by drawing the shape of panels.","cites":"10","conferencePercentile":"35.29411765"},{"venue":"ACM Trans. Graph.","id":"0c0e5f475af63350aac24fba5d5abb8f2cd92422","venue_1":"ACM Trans. Graph.","year":"2007","title":"Continuous collision detection for articulated models using Taylor models and temporal culling","authors":"Xinyu Zhang, Stéphane Redon, Minkyoung Lee, Young J. Kim","author_ids":"1775391, 3127641, 7793508, 1795892","abstract":"We present a fast continuous collision detection (CCD) algorithm for articulated models using Taylor models and temporal culling. Our algorithm is a generalization of conservative advancement (CA) from convex models [Mirtich 1996] to articulated models with non-convex links. Given the initial and final configurations of a moving articulated model, our algorithm creates a continuous motion with constant translational and rotational velocities for each link, and checks for interferences between the articulated model under continuous motion and other models in the environment and for self-collisions. If collisions occur, our algorithm reports the first time of contact (TOC) as well as <i>collision witness features</i>. We have implemented our CCD algorithm and applied it to several challenging scenarios including locomotion generation, articulated-body dynamics and character motion planning. Our algorithm can perform CCDs including self-collision detection for articulated models consisting of many links and tens of thousands of triangles in 1.22 ms on average running on a 3.6 GHz Pentium 4 PC. This is an improvement on the performance of prior algorithms of more than an order of magnitude.","cites":"52","conferencePercentile":"55.2"},{"venue":"ACM Trans. Graph.","id":"bab5a033437baf6be3f219ad65d295e8b7947155","venue_1":"ACM Trans. Graph.","year":"2013","title":"Efficient penetration depth approximation using active learning","authors":"Jia Pan, Xinyu Zhang, Dinesh Manocha","author_ids":"8701457, 1775391, 1699159","abstract":"(a) (b) (c) (d) (e) (f) (g) (h) Figure 1: Our algorithm computes a global penetration depth between overlapping non-convex and non-manifold objects. (a) Dynamic simulation of angry bird characters falling into a complex chute in Box2D physics engine; (b) rainfall of 1, 000 rings in Bullet physics engine; (c) a star and a spoon; (d) a spoon and a cup; (e) multiple contacts between upper and lower teeth (each has more than 40, 000 triangles). (f-h) benchmarks consisting of complex models (bunny, dragon and Buddha models have 70K, 230K and 1M triangles, respectively). Our PD algorithm takes less than 0.1 ∼ 2 milliseconds, with less than 2-3% relative error, for each pair of overlapping objects. Abstract We present a new method for efficiently approximating the global penetration depth between two rigid objects using machine learning techniques. Our approach consists of two phases: offline learning and performing run-time queries. In the learning phase, we precom-pute an approximation of the contact space of a pair of intersecting objects from a set of samples in the configuration space. We use active and incremental learning algorithms to accelerate the precomputation and improve the accuracy. During the run-time phase, our algorithm performs a nearest-neighbor query based on translational or rotational distance metrics. The run-time query has a small overhead and computes an approximation to global penetration depth in a few milliseconds. We use our algorithm for collision response computations in Box2D or Bullet game physics engines and complex 3D models and observe more than an order of magnitude improvement over prior PD computation techniques.","cites":"5","conferencePercentile":"12.66968326"},{"venue":"ACM Trans. Graph.","id":"03f10f844ba457af540b987d72d4157dedf71d77","venue_1":"ACM Trans. Graph.","year":"2012","title":"3D-printing of non-assembly, articulated models","authors":"Jacques Calì, Dan Andrei Calian, Cristina Amati, Rebecca Kleinberger, Anthony Steed, Jan Kautz, Tim Weyrich","author_ids":"1997807, 2792016, 2205405, 2418542, 1678604, 1690538, 1784306","abstract":"Additive manufacturing (3D printing) is commonly used to produce physical models for a wide variety of applications, from archaeology to design. While static models are directly supported, it is desirable to also be able to print models with functional articulations, such as a hand with joints and knuckles, without the need for manual assembly of joint components. Apart from having to address limitations inherent to the printing process, this poses a particular challenge for articulated models that should be posable: to allow the model to hold a pose, joints need to exhibit internal friction to withstand gravity, without their parts fusing during 3D printing. This has not been possible with previous printable joint designs. In this paper, we propose a method for converting 3D models into printable, functional, non-assembly models with internal friction. To this end, we have designed an intuitive work-flow that takes an appropriately rigged 3D model, automatically fits novel 3D-printable and posable joints, and provides an interface for specifying rotational constraints. We show a number of results for different articulated models, demonstrating the effectiveness of our method.","cites":"49","conferencePercentile":"89.64646465"},{"venue":"ACM Trans. Graph.","id":"83c8b8f40b539d49c8e047d24125ba1d66fd1b42","venue_1":"ACM Trans. Graph.","year":"2004","title":"Making papercraft toys from meshes using strip-based approximate unfolding","authors":"Jun Mitani, Hiromasa Suzuki","author_ids":"2618827, 1726356","abstract":"We propose a new method for producing unfolded papercraft patterns of rounded toy animal figures from triangulated meshes by means of strip-based approximation. Although in principle a triangulated model can be unfolded simply by retaining as much as possible of its connectivity while checking for intersecting triangles in the unfolded plane, creating a pattern with tens of thousands of triangles is unrealistic. Our approach is to approximate the mesh model by a set of continuous triangle strips with no internal vertices. Initially, we subdivide our mesh into parts corresponding to the features of the model. We segment each part into zonal regions, grouping triangles which are similar topological distances from the part boundary. We generate triangle strips by simplifying the mesh while retaining the borders of the zonal regions and additional cut-lines. The pattern is then created simply by unfolding the set of strips. The distinguishing feature of our method is that we approximate a mesh model by a set of continuous strips, not by other ruled surfaces such as parts of cones or cylinders. Thus, the approximated unfolded pattern can be generated using only mesh operations and a simple unfolding algorithm. Furthermore, a set of strips can be crafted just by bending the paper (without breaking edges) and can represent smooth features of the original mesh models.","cites":"103","conferencePercentile":"48.91304348"},{"venue":"ACM Trans. Graph.","id":"0db02a9518d2fe353274fb843d84720a1c23b830","venue_1":"ACM Trans. Graph.","year":"2016","title":"Reconciling Elastic and Equilibrium Methods for Static Analysis","authors":"Hijung Shin, Christopher F. Porst, Etienne Vouga, John Ochsendorf, Frédo Durand","author_ids":"2596714, 3411691, 1778579, 2052915, 1728125","abstract":"We examine two widely used classes of methods for static analysis of masonry buildings: linear elasticity analysis using finite elements and equilibrium methods. It is often claimed in the literature that finite element analysis is less accurate than equilibrium analysis when it comes to masonry analysis; we examine and qualify this claimed inaccuracy, provide a systematic explanation for the discrepancy observed between their results, and present a unified formulation of the two approaches to stability analysis. We prove that both approaches can be viewed as equivalent, dual methods for getting the same answer to the same problem. We validate our observations with simulations and physical tilt experiments of structures.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"3b284775d82f3b2914e2519fc3e98308acbfc0c9","venue_1":"ACM Trans. Graph.","year":"2016","title":"Downsampling scattering parameters for rendering anisotropic media","authors":"Shuang Zhao, Frédo Durand","author_ids":"2373908, 1728125","abstract":"Volumetric micro-appearance models have provided remarkably high-quality renderings, but are highly data intensive and usually require tens of gigabytes in storage. When an object is viewed from a distance, the highest level of detail offered by these models is usually unnecessary, but traditional linear downsampling weakens the object's intrinsic shadowing structures and can yield poor accuracy. We introduce a joint optimization of single-scattering albedos and phase functions to accurately downsample heterogeneous and anisotropic media. Our method is built upon <i>scaled phase functions</i>, a new representation combining abledos and (standard) phase functions. We also show that modularity can be exploited to greatly reduce the amortized optimization overhead by allowing multiple synthesized models to share one set of down-sampled parameters. Our optimized parameters generalize well to novel lighting and viewing configurations, and the resulting data sets offer several orders of magnitude storage savings.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"f3b1effc43a5ef536396e70c0cf90af2f9f5f2d5","venue_1":"ACM Trans. Graph.","year":"2016","title":"Temporal gradient-domain path tracing","authors":"Frédo Durand","author_ids":"1728125","abstract":"We present a novel approach to improve temporal coherence in Monte Carlo renderings of animation sequences. Unlike other approaches that exploit temporal coherence in a post-process, our technique does so already during sampling. Building on previous gradient-domain rendering techniques that sample finite differences over the image plane, we introduce temporal finite differences and formulate a corresponding 3D spatio-temporal screened Poisson reconstruction problem that is solved over windowed batches of several frames simultaneously. We further extend our approach to include second order, mixed spatio-temporal differences, an improved technique to compute temporal differences exploiting motion vectors, and adaptive sampling. Our algorithm can be built on a gradient-domain path tracer without large modifications. In particular, we do not require the ability to evaluate animation paths over multiple frames. We demonstrate that our approach effectively reduces temporal flickering in animation sequences, significantly improving the visual quality compared to both path tracing and gradient-domain rendering of individual frames.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"240dce423c6976ecc6ba5da7113ee7fc68f9c44c","venue_1":"ACM Trans. Graph.","year":"2016","title":"Computational bounce flash for indoor portraits","authors":"Frédo Durand","author_ids":"1728125","abstract":"Portraits taken with direct flash look harsh and unflattering because the light source comes from a small set of angles very close to the camera. Advanced photographers address this problem by using <i>bounce flash</i>, a technique where the flash is directed towards other surfaces in the room, creating a larger, virtual light source that can be cast from different directions to provide better shading variation for 3D modeling. However, finding the right direction to point a bounce flash requires skill and careful consideration of the available surfaces and subject configuration. Inspired by the impact of automation for exposure, focus and flash metering, we automate control of the flash direction for bounce illumination. We first identify criteria for evaluating flash directions, based on established photography literature, and relate these criteria to the color and geometry of a scene. We augment a camera with servomotors to rotate the flash head, and additional sensors (a fisheye and 3D sensors) to gather information about potential bounce surfaces. We present a simple numerical optimization criterion that finds directions for the flash that consistently yield compelling illumination and demonstrate the effectiveness of our various criteria in common photographic configurations.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"d003c167e9e3c8ee9f6ba6f96e298a5646e39b41","venue_1":"ACM Trans. Graph.","year":"2016","title":"Deep joint demosaicking and denoising","authors":"Frédo Durand","author_ids":"1728125","abstract":"Demosaicking and denoising are the key first stages of the digital imaging pipeline but they are also a severely ill-posed problem that infers three color values per pixel from a single noisy measurement. Earlier methods rely on hand-crafted filters or priors and still exhibit disturbing visual artifacts in hard cases such as moir&#233; or thin edges. We introduce a new data-driven approach for these challenges: we train a deep neural network on a large corpus of images instead of using hand-tuned filters. While deep learning has shown great success, its naive application using existing training datasets does not give satisfactory results for our problem because these datasets lack hard cases. To create a better training set, we present metrics to identify difficult patches and techniques for mining community photographs for such patches. Our experiments show that this network and training procedure outperform state-of-the-art both on noisy and noise-free data. Furthermore, our algorithm is an order of magnitude faster than the previous best performing techniques.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"db1e9065565af5bc58f5e89cd30eec6c644eaace","venue_1":"ACM Trans. Graph.","year":"2015","title":"Video diff: highlighting differences between similar actions in videos","authors":"Guha Balakrishnan, Frédo Durand, John V. Guttag","author_ids":"2094153, 1728125, 1724429","abstract":"When looking at videos of very similar actions with the naked eye, it is often difficult to notice subtle motion differences between them. In this paper we introduce video diffing, an algorithm that highlights the important differences between a pair of video recordings of similar actions. We overlay the edges of one video onto the frames of the second, and color the edges based on a measure of local dissimilarity between the videos. We measure dissimilarity by extracting spatiotemporal gradients from both videos and calculating how dissimilar histograms of these gradients are at varying spatial scales. We performed a user study with 54 people to compare the ease with which users could use our method to find differences. Users gave our method an average grade of 4.04 out of 5 for ease of use, compared to 3.48 and 2.08 for two baseline approaches. Anecdotal results also show that our overlays are useful in the specific use cases of professional golf instruction and analysis of animal locomotion simulations.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"4246df2d19fb57cc6eddcc857751e921b39a4a3a","venue_1":"ACM Trans. Graph.","year":"2015","title":"Image-space modal bases for plausible manipulation of objects in video","authors":"Abe Davis, Justin G. Chen, Frédo Durand","author_ids":"1825995, 2292318, 1728125","abstract":"We present algorithms for extracting an image-space representation of object structure from video and using it to synthesize physically plausible animations of objects responding to new, previously unseen forces. Our representation of structure is derived from an image-space analysis of modal object deformation: projections of an object's resonant modes are recovered from the temporal spectra of optical flow in a video, and used as a basis for the image-space simulation of object dynamics. We describe how to extract this basis from video, and show that it can be used to create physically-plausible animations of objects without any knowledge of scene geometry or material properties.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"b2a099cbfd44b67e3e298cc144ba756a107030d6","venue_1":"ACM Trans. Graph.","year":"2015","title":"Gradient-domain path tracing","authors":"Markus Kettunen, Marco Manzi, Miika Aittala, Jaakko Lehtinen, Frédo Durand, Matthias Zwicker","author_ids":"2093415, 2653287, 1907688, 1780788, 1728125, 1796846","abstract":"We introduce gradient-domain rendering for Monte Carlo image synthesis. While previous gradient-domain Metropolis Light Transport sought to distribute more samples in areas of high gradients, we show, in contrast, that estimating image gradients is also possible using standard (non-Metropolis) Monte Carlo algorithms, and furthermore, that even without changing the sample distribution, this often leads to significant error reduction. This broadens the applicability of gradient rendering considerably. To gain insight into the conditions under which gradient-domain sampling is beneficial, we present a frequency analysis that compares Monte Carlo sampling of gradients followed by Poisson reconstruction to traditional Monte Carlo sampling. Finally, we describe Gradient-Domain Path Tracing (G-PT), a relatively simple modification of the standard path tracing algorithm that can yield far superior results.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"460481a14524ca7c37b080f6d506bdaba972edbd","venue_1":"ACM Trans. Graph.","year":"2015","title":"Fast 4D Sheared Filtering for Interactive Rendering of Distribution Effects","authors":"Ling-Qi Yan, Soham Uday Mehta, Ravi Ramamoorthi, Frédo Durand","author_ids":"2162776, 2269369, 1752236, 1728125","abstract":"Soft shadows, depth of field, and diffuse global illumination are common distribution effects, usually rendered by Monte Carlo ray tracing. Physically correct, noise-free images can require hundreds or thousands of ray samples per pixel, and take a long time to compute. Recent approaches have exploited sparse sampling and filtering; the filtering is either fast (axis-aligned), but requires more input samples, or needs fewer input samples but is very slow (sheared). We present a new approach for fast sheared filtering on the GPU. Our algorithm factors the 4D sheared filter into four 1D filters. We derive complexity bounds for our method, showing that the per-pixel complexity is reduced from <i>O</i>(<i>n</i><sup>2</sup> <i>l</i><sup>2</sup>) to <i>O</i>(<i>nl</i>), where <i>n</i> is the linear filter width (filter size is <i>O</i>(<i>n</i><sup>2</sup>)) and <i>l</i> is the (usually very small) number of samples for each dimension of the light or lens per pixel (spp is <i>l</i><sup>2</sup>). We thus reduce sheared filtering overhead dramatically. We demonstrate rendering of depth of field, soft shadows and diffuse global illumination at interactive speeds. We reduce the number of samples needed by 5-8&#215;, compared to axis-aligned filtering, and framerates are 4&#215; faster for equal quality.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"320d0f5c43eaf6f2650f4f3e7c672c3ab312d2e2","venue_1":"ACM Trans. Graph.","year":"2015","title":"eyeSelfie: self directed eye alignment using reciprocal eye box imaging","authors":"Tristan Swedish, Karin Roesch, Ikhyun Lee, Krishna Rastogi, Shoshana Bernstein, Ramesh Raskar","author_ids":"2161276, 2213455, 2724597, 2638339, 2214049, 1717566","abstract":"Eye alignment to the optical system is very critical in many modern devices, such as for biometrics, gaze tracking, head mounted displays, and health. We show alignment in the context of the most difficult challenge: retinal imaging. Alignment in retinal imaging, even conducted by a physician, is very challenging due to precise alignment requirements and lack of direct user eye gaze control. Self-imaging of the retina is nearly impossible.\n We frame this problem as a user-interface (UI) challenge. We can create a better UI by controlling the eye box of a projected cue. Our key concept is to exploit the reciprocity, \"If you see me, I see you\", to develop near eye alignment displays. Two technical aspects are critical: a) tightness of the eye box and (b) the eye box discovery comfort. We demonstrate that previous pupil forming display architectures are not adequate to address alignment in depth. We then analyze two ray-based designs to determine efficacious fixation patterns. These ray based displays and a sequence of user steps allow lateral (x, y) and depth (z) wise alignment to deal with image centering and focus. We show a highly portable prototype and demonstrate the effectiveness through a user study.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"dfe9cc520769356e16030be2697d7b40db0cc996","venue_1":"ACM Trans. Graph.","year":"2014","title":"Eyeglasses-free display: towards correcting visual aberrations with computational light field displays","authors":"Fu-Chung Huang, Gordon Wetzstein, Brian A. Barsky, Ramesh Raskar","author_ids":"1798019, 1731170, 1700557, 1717566","abstract":"Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. By designing optics in concert with prefiltering algorithms, the proposed display architecture achieves significantly higher resolution and contrast than prior approaches to vision-correcting image display. We demonstrate that inexpensive light field displays driven by efficient implementations of 4D prefiltering algorithms can produce the desired vision-corrected imagery, even for higher-order aberrations that are difficult to be corrected with glasses. The proposed computational display architecture is evaluated in simulation and with a low-cost prototype device.","cites":"8","conferencePercentile":"48.35390947"},{"venue":"ACM Trans. Graph.","id":"06642b5909f298598ac614fe69121b6ca09ba015","venue_1":"ACM Trans. Graph.","year":"2016","title":"Physics-driven pattern adjustment for direct 3D garment editing","authors":"Aric Bartle, Alla Sheffer, Vladimir G. Kim, Danny M. Kaufman, Nicholas Vining, Floraine Berthouzoz","author_ids":"3017583, 3354923, 3082383, 2972719, 1983439, 2842099","abstract":"Designers frequently reuse existing designs as a starting point for creating new garments. In order to apply garment modifications, which the designer envisions in 3D, existing tools require meticulous manual editing of 2D patterns. These 2D edits need to account both for the envisioned geometric changes in the 3D shape, as well as for various physical factors that affect the look of the draped garment. We propose a new framework that allows designers to directly apply the changes they envision in 3D space; and creates the 2D patterns that replicate this envisioned target geometry when lifted into 3D via a physical draping simulation. Our framework removes the need for laborious and knowledge-intensive manual 2D edits and allows users to effortlessly mix existing garment designs as well as adjust for garment length and fit. Following each user specified editing operation we first compute a <i>target</i> 3D garment shape, one that maximally preserves the input garment's style-its proportions, fit and shape-subject to the modifications specified by the user. We then automatically compute 2D patterns that recreate the target garment shape when draped around the input mannequin within a user-selected simulation environment. To generate these patterns, we propose a fixed-point optimization scheme that compensates for the deformation due to the physical forces affecting the drape and is independent of the underlying simulation tool used. Our experiments show that this method quickly and reliably converges to patterns that, under simulation, form the desired target look, and works well with different black-box physical simulators. We demonstrate a range of edited and resimulated garments, and further validate our approach via expert and amateur critique, and comparisons to alternative solutions.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"cc24186435946473554c039a110c996a32b7326e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Measurement-based editing of diffuse albedo with consistent interreflections","authors":"Bo Dong, Yue Dong, Xin Tong, Pieter Peers","author_ids":"1714196, 1744268, 1743927, 1808270","abstract":"We present a novel measurement-based method for editing the albedo of diffuse surfaces with consistent interreflections in a photograph of a scene under natural lighting. Key to our method is a novel technique for decomposing a photograph of a scene in several images that encode how much of the observed radiance has interacted a specified number of times with the target diffuse surface. Altering the albedo of the target area is then simply a weighted sum of the decomposed components. We estimate the interaction components by recursively applying the light transport operator and formulate the resulting radiance in each recursion as a linear expression in terms of the relevant interaction components. Our method only requires a camera-projector pair, and the number of required measurements per scene is linearly proportional to the decomposition degree for a single target area. Our method does not impose restrictions on the lighting or on the material properties in the unaltered part of the scene. Furthermore, we extend our method to accommodate editing of the albedo in multiple target areas with consistent interreflections and we introduce a prediction model for reducing the acquisition cost. We demonstrate our method on a variety of scenes and validate the accuracy on both synthetic and real examples.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"e6eadaa39f5f39ac8e2a65516f950c2923f7f882","venue_1":"ACM Trans. Graph.","year":"2016","title":"Sparse-as-possible SVBRDF acquisition","authors":"Zhiming Zhou, Guojun Chen, Yue Dong, David P. Wipf, Yong Yu, John Snyder, Xin Tong","author_ids":"3973592, 8018112, 1744268, 2242717, 3578922, 6314473, 1743927","abstract":"We present a novel method for capturing real-world, spatially-varying surface reflectance from a small number of object views (<i>k</i>). Our key observation is that a specific target's reflectance can be represented by a small number of custom basis materials (<i>N</i>) convexly blended by an even smaller number of non-zero weights at each point (<i>n</i>). Based on this <i>sparse basis/sparser blend</i> model, we develop an SVBRDF reconstruction algorithm that jointly solves for <i>n</i>, <i>N</i>, the basis BRDFs, and their spatial blend weights with an alternating iterative optimization, each step of which solves a linearly-constrained quadratic programming problem. We develop a numerical tool that lets us estimate the number of views required and analyze the effect of lighting and geometry on reconstruction quality. We validate our method with images rendered from synthetic BRDFs, and demonstrate convincing results on real objects of pre-scanned shape and lit by uncontrolled natural illumination, from very few or even a single input image.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"35be6e0b529cac00078441683ee3f3af21f3d47d","venue_1":"ACM Trans. Graph.","year":"2016","title":"Fusion4D: real-time performance capture of challenging scenes","authors":"Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip L. Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts, Christoph Rhemann, David Kim, Jonathan Taylor, Pushmeet Kohli, Vladimir Tankovich, Shahram Izadi","author_ids":"3273714, 3144576, 2337994, 2362379, 2219646, 2371390, 7540558, 2086328, 6308833, 7356633, 1685185, 3440762, 1699068","abstract":"We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.","cites":"6","conferencePercentile":"98.52320675"},{"venue":"ACM Trans. Graph.","id":"0ee1a93a95253d26179a69c1a53e7e368b57e84a","venue_1":"ACM Trans. Graph.","year":"2011","title":"Switchable primaries using shiftable layers of color filter arrays","authors":"Behzad Sajadi, Aditi Majumder, Kazuhiro Hiwada, Atsuto Maki, Ramesh Raskar","author_ids":"1746530, 1709667, 1790977, 1801052, 1717566","abstract":"We present a camera with switchable primaries using shiftable layers of color filter arrays (CFAs). By layering a pair of CMY CFAs in this novel manner we can switch between multiple sets of color primaries (namely RGB, CMY and RGBCY) in the same camera. In contrast to fixed color primaries (e.g. RGB or CMY), which cannot provide optimal image quality for all scene conditions, our camera with switchable primaries provides optimal <i>color fidelity</i> and <i>signal to noise ratio</i> for multiple scene conditions.\n Next, we show that the same concept can be used to layer two RGB CFAs to design a camera with switchable low dynamic range (LDR) and high dynamic range (HDR) modes. Further, we show that such layering can be generalized as a constrained satisfaction problem (CSP) allowing to constrain a large number of parameters (e.g. different operational modes, amount and direction of the shifts, placement of the primaries in the CFA) to provide an optimal solution.\n We investigate practical design options for shiftable layering of the CFAs. We demonstrate these by building prototype cameras for both switchable primaries and switchable LDR/HDR modes.\n To the best of our knowledge, we present, for the first time, the concept of shiftable layers of CFAs that provides a new degree of freedom in photography where multiple operational modes are available to the user in a single camera for optimizing the picture quality based on the nature of the scene geometry, color and illumination.","cites":"5","conferencePercentile":"8.157894737"},{"venue":"ACM Trans. Graph.","id":"5106c9268d92994955bfda519f9bc38bc22658b8","venue_1":"ACM Trans. Graph.","year":"2012","title":"Correcting for optical aberrations using multilayer displays","authors":"Fu-Chung Huang, Douglas Lanman, Brian A. Barsky, Ramesh Raskar","author_ids":"1798019, 3235339, 1700557, 1717566","abstract":"Optical aberrations of the human eye are currently corrected using eyeglasses, contact lenses, or surgery. We describe a fourth option: modifying the composition of displayed content such that the perceived image appears in focus, after passing through an eye with known optical defects. Prior approaches synthesize pre-filtered images by deconvolving the content by the point spread function of the aberrated eye. Such methods have not led to practical applications, due to severely reduced contrast and ringing artifacts. We address these limitations by introducing multilayer pre-filtering, implemented using stacks of semi-transparent, light-emitting layers. By optimizing the layer positions and the partition of spatial frequencies between layers, contrast is improved and ringing artifacts are eliminated. We assess design constraints for multilayer displays; autostereoscopic light field displays are identified as a preferred, thin form factor architecture, allowing synthetic layers to be displaced in response to viewer movement and refractive errors. We assess the benefits of multilayer pre-filtering versus prior light field pre-distortion methods, showing pre-filtering works within the constraints of current display resolutions. We conclude by analyzing benefits and limitations using a prototype multilayer LCD.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"8c1b93a3d98c7844e4c733e6f7efb9490bc0bff7","venue_1":"ACM Trans. Graph.","year":"2015","title":"Rolling guidance normal filter for geometric processing","authors":"Peng-Shuai Wang, Xiao-Ming Fu, Yang Liu, Xin Tong, Shi-Lin Liu, Baining Guo","author_ids":"2666565, 7780643, 1750084, 1743927, 3142349, 2738456","abstract":"3D geometric features constitute rich details of polygonal meshes. Their analysis and editing can lead to vivid appearance of shapes and better understanding of the underlying geometry for shape processing and analysis. Traditional mesh smoothing techniques mainly focus on noise filtering and they cannot distinguish different scales of features well, even mixing them up. We present an efficient method to process different scale geometric features based on a novel rolling-guidance normal filter. Given a 3D mesh, our method iteratively applies a joint bilateral filter to face normals at a specified scale, which empirically smooths small-scale geometric features while preserving large-scale features. Our method recovers the mesh from the filtered face normals by a modified Poisson-based gradient deformation that yields better surface quality than existing methods. We demonstrate the effectiveness and superiority of our method on a series of geometry processing tasks, including geometry texture removal and enhancement, coating transfer, mesh segmentation and level-of-detail meshing.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"270a433eff7268c05613ecb9e6cb71aff5b09e60","venue_1":"ACM Trans. Graph.","year":"2012","title":"Tensor displays: compressive light field synthesis using multilayer displays with directional backlighting","authors":"Gordon Wetzstein, Douglas Lanman, Matthew Hirsch, Ramesh Raskar","author_ids":"1731170, 3235339, 2309851, 1717566","abstract":"We introduce tensor displays: a family of compressive light field displays comprising all architectures employing a stack of time-multiplexed, light-attenuating layers illuminated by uniform or directional backlighting (i.e., any low-resolution light field emitter). We show that the light field emitted by an <i>N</i>-layer, <i>M</i>-frame tensor display can be represented by an <i>N</i><sup>th</sup>-order, rank-<i>M</i> tensor. Using this representation we introduce a unified optimization framework, based on nonnegative tensor factorization (NTF), encompassing all tensor display architectures. This framework is the first to allow joint multilayer, multiframe light field decompositions, significantly reducing artifacts observed with prior multilayer-only and multiframe-only decompositions; it is also the first optimization method for designs combining multiple layers with directional backlighting. We verify the benefits and limitations of tensor displays by constructing a prototype using modified LCD panels and a custom integral imaging backlight. Our efficient, GPU-based NTF implementation enables interactive applications. Through simulations and experiments we show that tensor displays reveal practical architectures with greater depths of field, wider fields of view, and thinner form factors, compared to prior automultiscopic displays.","cites":"60","conferencePercentile":"95.95959596"},{"venue":"ACM Trans. Graph.","id":"b2f9ee11224e8a126d1841ca186b41da05357fd9","venue_1":"ACM Trans. Graph.","year":"2012","title":"Primal-dual coding to probe light transport","authors":"Matthew O'Toole, Ramesh Raskar, Kiriakos N. Kutulakos","author_ids":"2174564, 1717566, 1734027","abstract":"We present <i>primal-dual coding</i>, a photography technique that enables direct fine-grain control over which light paths contribute to a photo. We achieve this by projecting a sequence of patterns onto the scene while the sensor is exposed to light. At the same time, a second sequence of patterns, derived from the first and applied in lockstep, modulates the light received at individual sensor pixels. We show that photography in this regime is equivalent to a <i>matrix probing</i> operation in which the elements of the scene's transport matrix are individually re-scaled and then mapped to the photo. This makes it possible to directly acquire photos in which specific light transport paths have been blocked, attenuated or enhanced. We show captured photos for several scenes with challenging light transport effects, including specular inter-reflections, caustics, diffuse inter-reflections and volumetric scattering. A key feature of primal-dual coding is that it operates almost exclusively in the optical domain: our results consist of directly-acquired, unprocessed RAW photos or differences between them.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"55c5443c8efef0d15b70dd52cc78488bb1dc1433","venue_1":"ACM Trans. Graph.","year":"2012","title":"Reflectance model for diffraction","authors":"Tom Cuypers, Tom Haber, Philippe Bekaert, Se Baek Oh, Ramesh Raskar","author_ids":"1904527, 1806102, 7583073, 2628664, 1717566","abstract":"We present a novel method of simulating wave effects in graphics using ray-based renderers with a new function: the Wave BSDF (Bidirectional Scattering Distribution Function). Reflections from neighboring surface patches represented by local BSDFs are mutually independent. However, in many surfaces with wavelength-scale microstructures, interference and diffraction requires a joint analysis of reflected wavefronts from neighboring patches. We demonstrate a simple method to compute the BSDF for the entire microstructure, which can be used independently for each patch. This allows us to use traditional ray-based rendering pipelines to synthesize wave effects. We exploit the Wigner Distribution Function (WDF) to create transmissive, reflective, and emissive BSDFs for various diffraction phenomena in a physically accurate way. In contrast to previous methods for computing interference, we circumvent the need to explicitly keep track of the phase of the wave by using BSDFs that include positive as well as negative coefficients. We describe and compare the theory in relation to well-understood concepts in rendering and demonstrate a straightforward implementation. In conjunction with standard raytracers, such as PBRT, we demonstrate wave effects for a range of scenarios such as multibounce diffraction materials, holograms, and reflection of high-frequency surfaces.","cites":"6","conferencePercentile":"7.323232323"},{"venue":"ACM Trans. Graph.","id":"3545e7d88ad8026c22822d0c0b9e0b48c2c5c732","venue_1":"ACM Trans. Graph.","year":"2015","title":"Micron-scale light transport decomposition using interferometry","authors":"Ioannis Gkioulekas, Anat Levin, Frédo Durand, Todd E. Zickler","author_ids":"2407724, 1801055, 1728125, 1713451","abstract":"We present a computational imaging system, inspired by the optical coherence tomography (OCT) framework, that uses interferometry to produce decompositions of light transport in small scenes or volumes. The system decomposes transport according to various attributes of the paths that photons travel through the scene, including where on the source the paths originate, their pathlengths from source to camera through the scene, their wavelength, and their polarization. Since it uses interference, the system can achieve high pathlength resolutions, with the ability to distinguish paths whose lengths differ by as little as ten microns. We describe how to construct and optimize an optical assembly for this technique, and we build a prototype to measure and visualize three-dimensional shape, direct and indirect reflection components, and properties of scattering, refractive/dispersive, and birefringent materials.","cites":"8","conferencePercentile":"85.30612245"},{"venue":"ACM Trans. Graph.","id":"4977a04989ded5f17575c99b77697644a3320af3","venue_1":"ACM Trans. Graph.","year":"2012","title":"Tailored displays to compensate for visual aberrations","authors":"Vitor F. Pamplona, Manuel Menezes de Oliveira Neto, Daniel G. Aliaga, Ramesh Raskar","author_ids":"1742358, 2856168, 1698910, 1717566","abstract":"We introduce tailored displays that enhance visual acuity by decomposing virtual objects and placing the resulting anisotropic pieces into the subject's focal range. The goal is to free the viewer from needing wearable optical corrections when looking at displays. Our tailoring process uses aberration and scattering maps to account for refractive errors and cataracts. It splits an object's light field into multiple instances that are each in-focus for a given eye sub-aperture. Their integration onto the retina leads to a quality improvement of perceived images when observing the display with naked eyes. The use of multiple depths to render each point of focus on the retina creates multi-focus, multi-depth displays. User evaluations and validation with modified camera optics are performed. We propose tailored displays for daily tasks where using eyeglasses are unfeasible or inconvenient (<i>e.g</i>., on head-mounted displays, e-readers, as well as for games); when a multi-focus function is required but undoable (<i>e.g</i>., driving for farsighted individuals, checking a portable device while doing physical activities); or for correcting the visual distortions produced by high-order aberrations that eyeglasses are not able to.","cites":"18","conferencePercentile":"52.52525253"},{"venue":"ACM Trans. Graph.","id":"b8184f01c4c425d5992eb206ad4cfc39fa2ac1f9","venue_1":"ACM Trans. Graph.","year":"2013","title":"Adaptive image synthesis for compressive displays","authors":"Felix Heide, Gordon Wetzstein, Ramesh Raskar, Wolfgang Heidrich","author_ids":"1736720, 1731170, 1717566, 1752192","abstract":"Recent years have seen proposals for exciting new computational display technologies that are <i>compressive</i> in the sense that they generate high resolution images or light fields with relatively few display parameters. Image synthesis for these types of displays involves two major tasks: sampling and rendering high-dimensional target imagery, such as light fields or time-varying light fields, as well as optimizing the display parameters to provide a good approximation of the target content.\n In this paper, we introduce an adaptive optimization framework for compressive displays that generates high quality images and light fields using only a fraction of the total plenoptic samples. We demonstrate the framework for a large set of display technologies, including several types of auto-stereoscopic displays, high dynamic range displays, and high-resolution displays. We achieve significant performance gains, and in some cases are able to process data that would be infeasible with existing methods.","cites":"13","conferencePercentile":"49.77375566"},{"venue":"ACM Trans. Graph.","id":"9c0acb483a1fac011e8985b32dbd41ca7a8a23ab","venue_1":"ACM Trans. Graph.","year":"2013","title":"Femto-photography: capturing and visualizing the propagation of light","authors":"Andreas Velten, Di Wu, Adrian Jarabo, Belen Masia, Christopher Barsi, Chinmaya Joshi, Everett Lawson, Moungi Bawendi, Diego Gutierrez, Ramesh Raskar","author_ids":"2253695, 7463685, 2208378, 1775667, 3112458, 2843902, 3068263, 2189844, 1723695, 1717566","abstract":"We present <i>femto-photography</i>, a novel imaging technique to capture and visualize the propagation of light. With an effective exposure time of 1.85 picoseconds (ps) per frame, we reconstruct movies of ultrafast events at an equivalent resolution of about one half trillion frames per second. Because cameras with this shutter speed do not exist, we re-purpose modern imaging hardware to record an ensemble average of repeatable events that are synchronized to a streak sensor, in which the time of arrival of light from the scene is coded in one of the sensor's spatial dimensions. We introduce reconstruction methods that allow us to visualize the propagation of femtosecond light pulses through macroscopic scenes; at such fast resolution, we must consider the notion of <i>time-unwarping</i> between the camera's and the world's space-time coordinate systems to take into account effects associated with the finite speed of light. We apply our femto-photography technique to visualizations of very different scenes, which allow us to observe the rich dynamics of time-resolved light transport effects, including scattering, specular reflections, diffuse interreflections, diffraction, caustics, and subsurface scattering. Our work has potential applications in artistic, educational, and scientific visualizations; industrial imaging to analyze material properties; and medical imaging to reconstruct subsurface elements. In addition, our time-resolved technique may motivate new forms of computational photography.","cites":"25","conferencePercentile":"80.31674208"},{"venue":"ACM Trans. Graph.","id":"7a85011c02ae02ee20bd9107f76115f3c8e80b97","venue_1":"ACM Trans. Graph.","year":"2013","title":"Focus 3D: Compressive accommodation display","authors":"Andrew Maimone, Gordon Wetzstein, Matthew Hirsch, Douglas Lanman, Ramesh Raskar, Henry Fuchs","author_ids":"2483888, 1731170, 2309851, 3235339, 1717566, 1741038","abstract":"We present a glasses-free 3D display design with the potential to provide viewers with nearly correct accommodative depth cues, as well as motion parallax and binocular cues. Building on multilayer attenuator and directional backlight architectures, the proposed design achieves the high angular resolution needed for accommodation by placing spatial light modulators about a large lens: one conjugate to the viewer's eye, and one or more near the plane of the lens. Nonnegative tensor factorization is used to compress a high angular resolution light field into a set of masks that can be displayed on a pair of commodity LCD panels. By constraining the tensor factorization to preserve only those light rays seen by the viewer, we effectively steer narrow high-resolution viewing cones into the user's eyes, allowing binocular disparity, motion parallax, and the potential for nearly correct accommodation over a wide field of view. We verify the design experimentally by focusing a camera at different depths about a prototype display, establish formal upper bounds on the design's accommodation range and diffraction-limited performance, and discuss practical limitations that must be overcome to allow the device to be used with human observers.","cites":"15","conferencePercentile":"60.18099548"},{"venue":"ACM Trans. Graph.","id":"1db1338c999d2cfab7adcf5a4bd695f8896f259d","venue_1":"ACM Trans. Graph.","year":"2013","title":"Near-invariant blur for depth and 2D motion via time-varying light field analysis","authors":"Yosuke Bando, Henry Holtzman, Ramesh Raskar","author_ids":"1799420, 1681180, 1717566","abstract":"Recently, several camera designs have been proposed for either making defocus blur invariant to scene depth or making motion blur invariant to object motion. The benefit of such invariant capture is that no depth or motion estimation is required to remove the resultant spatially uniform blur. So far, the techniques have been studied separately for defocus and motion blur, and object motion has been assumed 1D (e.g., horizontal). This article explores a more general capture method that makes both defocus blur and motion blur nearly invariant to scene depth and in-plane 2D object motion. We formulate the problem as capturing a time-varying light field through a time-varying light field modulator at the lens aperture, and perform 5D (4D light field &plus; 1D time) analysis of all the existing computational cameras for defocus/motion-only deblurring and their hybrids. This leads to a surprising conclusion that focus sweep, previously known as a depth-invariant capture method that moves the plane of focus through a range of scene depth during exposure, is near-optimal both in terms of depth and 2D motion invariance and in terms of high-frequency preservation for certain combinations of depth and motion ranges. Using our prototype camera, we demonstrate joint defocus and motion deblurring for moving scenes with depth variation.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"ae4acbad6b9843ac334c185cc2b725a2afab7f72","venue_1":"ACM Trans. Graph.","year":"2011","title":"Composite control of physically simulated characters","authors":"Uldarico Muico, Jovan Popovic, Zoran Popovic","author_ids":"1787011, 1731389, 1696595","abstract":"A physics-based control system that tracks a single motion trajectory produces high-quality animations, but does not recover from large disturbances that require deviating from this tracked trajectory. In order to enhance the responsiveness of physically simulated characters, we introduce algorithms that construct composite controllers that track multiple trajectories in parallel instead of sequentially switching from one control to the other. The composite controllers can blend or transition between different path controllers at arbitrary times according to the current system state. As a result, a composite control system generates both high-quality animations and natural responses to certain disturbances. We demonstrate its potential for improving robustness in performing several locomotion tasks. Then we consolidate these controllers into graphs that allow us to direct the character in real time.","cites":"15","conferencePercentile":"34.47368421"},{"venue":"ACM Trans. Graph.","id":"03efbce52ab8a22bcd704b4f4feae48e0f69127a","venue_1":"ACM Trans. Graph.","year":"2014","title":"A compressive light field projection system","authors":"Matthew Hirsch, Gordon Wetzstein, Ramesh Raskar","author_ids":"2309851, 1731170, 1717566","abstract":"Researchers and experimentalists have strived to bring glasses-free 3D to the big screen. Though light field projection systems are now commercially available, unfortunately they employ dozens of devices. They are costly, energy inefficient, and bulky. We present a compressive approach to light field synthesis for projection devices. We propose a novel, passive screen design inspired by angle-expanding Keplerian telescopes. Combined with high-speed light field projection and nonnegative light field factorization, we demonstrate compressive light field projection with a single device. Our prototype can alternatively display super-resolved and high dynamic range 2D images on a conventional screen.","cites":"18","conferencePercentile":"87.24279835"},{"venue":"ACM Trans. Graph.","id":"2393356fbac12ad45acff8be171bced88187e34d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Toward BxDF display using multilayer diffraction","authors":"Ramesh Raskar","author_ids":"1717566","abstract":"With a wide range of applications in product design and optical watermarking, computational BxDF display has become an emerging trend in the graphics community. In this paper, we analyze the design space of BxDF displays and show that existing approaches cannot reproduce arbitrary BxDFs. In particular, existing surface-based fabrication techniques are often limited to generating only specific angular frequencies, angle-shift-invariant radiance distributions, and sometimes only symmetric BxDFs. To overcome these limitations, we propose diffractive multilayer BxDF displays. We derive forward and inverse methods to synthesize patterns that are printed on stacked, high-resolution transparencies and reproduce prescribed BxDFs with unprecedented degrees of freedom within the limits of available fabrication techniques.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"e0ff65c1d6e8ba731265675f7a39727291a6b3c3","venue_1":"ACM Trans. Graph.","year":"2015","title":"Image based relighting using neural networks","authors":"Peiran Ren, Yue Dong, Stephen Lin, Xin Tong, Baining Guo","author_ids":"3246404, 1744268, 1686911, 1743927, 2738456","abstract":"We present a neural network regression method for relighting realworld scenes from a small number of images. The relighting in this work is formulated as the product of the scene's light transport matrix and new lighting vectors, with the light transport matrix reconstructed from the input images. Based on the observation that there should exist non-linear local coherence in the light transport matrix, our method approximates matrix segments using neural networks that model light transport as a non-linear function of light source position and pixel coordinates. Central to this approach is a proposed neural network design which incorporates various elements that facilitate modeling of light transport from a small image set. In contrast to most image based relighting techniques, this regression-based approach allows input images to be captured under arbitrary illumination conditions, including light sources moved freely by hand. We validate our method with light transport data of real scenes containing complex lighting effects, and demonstrate that fewer input images are required in comparison to related techniques.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"4bd9f6097a0da6e33d37e7b1c8e2c6cd56dcf84a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Compressive light field photography using overcomplete dictionaries and optimized projections","authors":"Kshitij Marwah, Gordon Wetzstein, Yosuke Bando, Ramesh Raskar","author_ids":"3337517, 1731170, 1799420, 1717566","abstract":"Light field photography has gained a significant research interest in the last two decades; today, commercial light field cameras are widely available. Nevertheless, most existing acquisition approaches either multiplex a low-resolution light field into a single 2D sensor image or require multiple photographs to be taken for acquiring a high-resolution light field. We propose a compressive light field camera architecture that allows for higher-resolution light fields to be recovered than previously possible from a single image. The proposed architecture comprises three key components: light field atoms as a sparse representation of natural light fields, an optical design that allows for capturing optimized 2D light field projections, and robust sparse reconstruction methods to recover a 4D light field from a single coded 2D projection. In addition, we demonstrate a variety of other applications for light field atoms and sparse coding, including 4D light field compression and denoising.","cites":"43","conferencePercentile":"94.11764706"},{"venue":"ACM Trans. Graph.","id":"6bd86bbbe1d3c644038f0248541f92f2061c608e","venue_1":"ACM Trans. Graph.","year":"2011","title":"Layered 3D: tomographic image synthesis for attenuation-based light field and high dynamic range displays","authors":"Gordon Wetzstein, Douglas Lanman, Wolfgang Heidrich, Ramesh Raskar","author_ids":"1731170, 3235339, 1752192, 1717566","abstract":"We develop tomographic techniques for image synthesis on displays composed of compact volumes of light-attenuating material. Such volumetric attenuators recreate a 4D light field or high-contrast 2D image when illuminated by a uniform backlight. Since arbitrary oblique views may be inconsistent with any single attenuator, iterative tomographic reconstruction minimizes the difference between the emitted and target light fields, subject to physical constraints on attenuation. As multi-layer generalizations of conventional parallax barriers, such displays are shown, both by theory and experiment, to exceed the performance of existing dual-layer architectures. For 3D display, spatial resolution, depth of field, and brightness are increased, compared to parallax barriers. For a plane at a fixed depth, our optimization also allows optimal construction of high dynamic range displays, confirming existing heuristics and providing the first extension to multiple, disjoint layers. We conclude by demonstrating the benefits and limitations of attenuation-based light field displays using an inexpensive fabrication method: separating multiple printed transparencies with acrylic sheets.","cites":"67","conferencePercentile":"91.57894737"},{"venue":"ACM Trans. Graph.","id":"0477c32e686df344dde1ccab358b843770e2600e","venue_1":"ACM Trans. Graph.","year":"2015","title":"Deviation magnification: revealing departures from ideal geometries","authors":"Neal Wadhwa, Tali Dekel, Donglai Wei, Frédo Durand, William T. Freeman","author_ids":"2103475, 2112779, 1766333, 1728125, 1768236","abstract":"Structures and objects are often supposed to have idealized geometries such as straight lines or circles. Although not always visible to the naked eye, in reality, these objects deviate from their idealized models. Our goal is to reveal and visualize such subtle geometric deviations, which can contain useful, surprising information about our world. Our framework, termed <i>Deviation Magnification</i>, takes a still image as input, fits parametric models to objects of interest, computes the geometric deviations, and renders an output image in which the departures from ideal geometries are exaggerated. We demonstrate the correctness and usefulness of our method through quantitative evaluation on a synthetic dataset and by application to challenging natural images.","cites":"1","conferencePercentile":"18.97959184"},{"venue":"ACM Trans. Graph.","id":"35d75c9cadc5dfb4f991a83bd8a44d76084765a5","venue_1":"ACM Trans. Graph.","year":"2012","title":"Staggered meshless solid-fluid coupling","authors":"Xiaowei He, Ning Liu, Guoping Wang, Fengjun Zhang, Sheng Li, Songdong Shao, Hongan Wang","author_ids":"2433407, 1680152, 2896119, 3280032, 3451996, 3070879, 7643981","abstract":"Simulating solid-fluid coupling with the classical meshless methods is an difficult issue due to the lack of the Kronecker delta property of the shape functions when enforcing the essential boundary conditions. In this work, we present a novel staggered meshless method to overcome this problem. We create a set of staggered particles from the original particles in each time step by mapping the mass and momentum onto these staggered particles, aiming to stagger the velocity field from the pressure field. Based on this arrangement, an new approximate projection method is proposed to enforce divergence-free on the fluid velocity with compatible boundary conditions. In the simulations, the method handles the fluid and solid in a unified meshless manner and generalizes the formulations for computing the viscous and pressure forces. To enhance the robustness of the algorithm, we further propose a new framework to handle the degeneration case in the solid-fluid coupling, which guarantees stability of the simulation. The proposed method offers the benefit that various slip boundary conditions can be easily implemented. Besides, explicit collision handling for the fluid and solid is avoided. The method is easy to implement and can be extended from the standard SPH algorithm in a straightforward manner. The paper also illustrates both one-way and two-way couplings of the fluids and rigid bodies using several test cases in two and three dimensions.","cites":"5","conferencePercentile":"5.555555556"},{"venue":"ACM Trans. Graph.","id":"2b2d03f8b96aa1e306fb941e0318d403efbde4be","venue_1":"ACM Trans. Graph.","year":"2015","title":"Capturing the human figure through a wall","authors":"Fadel Adib, Chen-Yu Hsu, Hongzi Mao, Dina Katabi, Frédo Durand","author_ids":"1761544, 2502450, 2512621, 1785714, 1728125","abstract":"We present RF-Capture, a system that captures the human figure -- i.e., a coarse skeleton -- through a wall. RF-Capture tracks the 3D positions of a person's limbs and body parts even when the person is fully occluded from its sensor, and does so without placing any markers on the subject's body. In designing RF-Capture, we built on recent advances in wireless research, which have shown that certain radio frequency (RF) signals can traverse walls and reflect off the human body, allowing for the detection of human motion through walls. In contrast to these past systems which abstract the entire human body as a single point and find the overall location of that point through walls, we show how we can reconstruct various human body parts and stitch them together to capture the human figure. We built a prototype of RF-Capture and tested it on 15 subjects. Our results show that the system can capture a representative human figure through walls and use it to distinguish between various users.","cites":"9","conferencePercentile":"88.16326531"},{"venue":"ACM Trans. Graph.","id":"4eae6c015937b6808b2e84a125b1af7704b34c9c","venue_1":"ACM Trans. Graph.","year":"2009","title":"Procedural modeling of structurally-sound masonry buildings","authors":"Emily Whiting, John Ochsendorf, Frédo Durand","author_ids":"2778710, 2052915, 1728125","abstract":"We introduce structural feasibility into procedural modeling of buildings. This allows for more realistic structural models that can be interacted with in physical simulations. While existing structural analysis tools focus heavily on providing an analysis of the stress state, our proposed method automatically tunes a set of designated free parameters to obtain forms that are structurally sound.","cites":"45","conferencePercentile":"67.67955801"},{"venue":"ACM Trans. Graph.","id":"1ce3a91214c94ed05f15343490981ec7cc810016","venue_1":"ACM Trans. Graph.","year":"2011","title":"Exploring photobios","authors":"Ira Kemelmacher-Shlizerman, Eli Shechtman, Rahul Garg, Steven M. Seitz","author_ids":"2419955, 2177801, 1779656, 1679223","abstract":"We present an approach for generating face animations from large image collections of the same person. Such collections, which we call <i>photobios</i>, sample the appearance of a person over changes in pose, facial expression, hairstyle, age, and other variations. By optimizing the order in which images are displayed and cross-dissolving between them, we control the motion through face space and create compelling animations (e.g., render a smooth transition from frowning to smiling). Used in this context, the <i>cross dissolve</i> produces a very strong motion effect; a key contribution of the paper is to explain this effect and analyze its operating range. The approach operates by creating a graph with faces as nodes, and similarities as edges, and solving for walks and shortest paths on this graph. The processing pipeline involves face detection, locating fiducials (eyes/nose/mouth), solving for pose, warping to frontal views, and image comparison based on Local Binary Patterns. We demonstrate results on a variety of datasets including time-lapse photography, personal photo collections, and images of celebrities downloaded from the Internet. Our approach is the basis for the Face Movies feature in Google's Picasa.","cites":"30","conferencePercentile":"65.52631579"},{"venue":"ACM Trans. Graph.","id":"21e7158676eb204dc748b7fc694718c79241cedc","venue_1":"ACM Trans. Graph.","year":"2009","title":"4D frequency analysis of computational cameras for depth of field extension","authors":"Anat Levin, Samuel W. Hasinoff, Paul Green, Frédo Durand, William T. Freeman","author_ids":"1801055, 1979640, 2535850, 1728125, 1768236","abstract":"Depth of field (DOF), the range of scene depths that appear sharp in a photograph, poses a fundamental tradeoff in photography---wide apertures are important to reduce imaging noise, but they also increase defocus blur. Recent advances in computational imaging modify the acquisition process to extend the DOF through deconvolution. Because deconvolution quality is a tight function of the frequency power spectrum of the defocus kernel, designs with high spectra are desirable. In this paper we study how to design effective extended-DOF systems, and show an upper bound on the maximal power spectrum that can be achieved. We analyze defocus kernels in the 4D light field space and show that in the frequency domain, only a low-dimensional 3D manifold contributes to focus. Thus, to maximize the defocus spectrum, imaging systems should concentrate their limited energy on this manifold. We review several computational imaging systems and show either that they spend energy outside the focal manifold or do not achieve a high spectrum over the DOF. Guided by this analysis we introduce the lattice-focal lens, which concentrates energy at the low-dimensional focal manifold and achieves a higher power spectrum than previous designs. We have built a prototype lattice-focal lens and present extended depth of field results.","cites":"67","conferencePercentile":"85.91160221"},{"venue":"ACM Trans. Graph.","id":"4ef33634701397674553a501f47cb34603fede70","venue_1":"ACM Trans. Graph.","year":"2009","title":"Edge-preserving multiscale image decomposition based on local extrema","authors":"Kartic Subr, Cyril Soler, Frédo Durand","author_ids":"2740413, 1866568, 1728125","abstract":"We propose a new model for detail that inherently captures <i>oscillations</i>, a key property that distinguishes textures from individual edges. Inspired by techniques in empirical data analysis and morphological image analysis, we use the local extrema of the input image to extract information about oscillations: We define detail as oscillations between local minima and maxima. Building on the key observation that the spatial scale of oscillations are characterized by the density of local extrema, we develop an algorithm for decomposing images into multiple scales of superposed oscillations.\n Current edge-preserving image decompositions assume image detail to be low contrast variation. Consequently they apply filters that extract features with increasing contrast as successive layers of detail. As a result, they are unable to distinguish between high-contrast, fine-scale features and edges of similar contrast that are to be preserved. We compare our results with existing edge-preserving image decomposition algorithms and demonstrate exciting applications that are made possible by our new notion of detail.","cites":"66","conferencePercentile":"84.53038674"},{"venue":"ACM Trans. Graph.","id":"79e252f84eccd3890645d05fe7c3f72e1ccfa09e","venue_1":"ACM Trans. Graph.","year":"2009","title":"Frequency analysis and sheared reconstruction for rendering motion blur","authors":"Kevin Egan, Yu-Ting Tseng, Nicolas Holzschuch, Frédo Durand, Ravi Ramamoorthi","author_ids":"3078722, 2056696, 3174773, 1728125, 1752236","abstract":"Motion blur is crucial for high-quality rendering, but is also very expensive. Our first contribution is a frequency analysis of motion-blurred scenes, including moving objects, specular reflections, and shadows. We show that motion induces a shear in the frequency domain, and that the spectrum of moving scenes can be approximated by a wedge. This allows us to compute adaptive space-time sampling rates, to accelerate rendering. For uniform velocities and standard axis-aligned reconstruction, we show that the product of spatial and temporal bandlimits or sampling rates is constant, independent of velocity. Our second contribution is a novel sheared reconstruction filter that is aligned to the first-order direction of motion and enables even lower sampling rates. We present a rendering algorithm that computes a sheared reconstruction filter per pixel, without any intermediate Fourier representation. This often permits synthesis of motion-blurred images with far fewer rendering samples than standard techniques require.","cites":"72","conferencePercentile":"87.56906077"},{"venue":"ACM Trans. Graph.","id":"05dd9a2648d94f724c795b689bd431dbd4818226","venue_1":"ACM Trans. Graph.","year":"2009","title":"Fourier depth of field","authors":"Cyril Soler, Kartic Subr, Frédo Durand, Nicolas Holzschuch, François X. Sillion","author_ids":"1866568, 2740413, 1728125, 3174773, 1708617","abstract":"Optical systems used in photography and cinema produce depth-of-field effects, that is, variations of focus with depth. These effects are simulated in image synthesis by integrating incoming radiance at each pixel over the lense aperture. Unfortunately, aperture integration is extremely costly for defocused areas where the incoming radiance has high variance, since many samples are then required for a noise-free Monte Carlo integration. On the other hand, using many aperture samples is wasteful in focused areas where the integrand varies little. Similarly, image sampling in defocused areas should be adapted to the very smooth appearance variations due to blurring. This article introduces an analysis of focusing and depth-of-field in the frequency domain, allowing a practical characterization of a light field's frequency content both for image and aperture sampling. Based on this analysis we propose an adaptive depth-of-field rendering algorithm which optimizes sampling in two important ways. First, image sampling is based on conservative bandwidth prediction and a splatting reconstruction technique ensures correct image reconstruction. Second, at each pixel the variance in the radiance over the aperture is estimated and used to govern sampling. This technique is easily integrated in any sampling-based renderer, and vastly improves performance.","cites":"50","conferencePercentile":"72.37569061"},{"venue":"ACM Trans. Graph.","id":"225ebfce8cae8c82e314c558d69d15c8308905b4","venue_1":"ACM Trans. Graph.","year":"2008","title":"Motion-invariant photography","authors":"Anat Levin, Peter Sand, Taeg Sang Cho, Frédo Durand, William T. Freeman","author_ids":"1801055, 2259071, 2002310, 1728125, 1768236","abstract":"Object motion during camera exposure often leads to noticeable blurring artifacts. Proper elimination of this blur is challenging because the blur kernel is unknown, varies over the image as a function of object velocity, and destroys high frequencies. In the case of motions along a 1D direction (e.g. horizontal) we show that these challenges can be addressed using a camera that moves during the exposure. Through the analysis of motion blur as space-time integration, we show that a parabolic integration (corresponding to constant sensor acceleration) leads to motion blur that is invariant to object velocity. Thus, a single deconvolution kernel can be used to remove blur and create sharp images of scenes with objects moving at different speeds, without requiring any segmentation and without knowledge of the object speeds. Apart from motion invariance, we prove that the derived parabolic motion preserves image frequency content nearly optimally. That is, while static objects are degraded relative to their image from a static camera, a reliable reconstruction of all moving objects within a given velocities range is made possible. We have built a prototype camera and present successful deblurring results over a wide variety of human motions.","cites":"84","conferencePercentile":"85.18518519"},{"venue":"ACM Trans. Graph.","id":"2cdd5b50a67e4615cb0892beaac12664ec53b81f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Mirror mirror: crowdsourcing better portraits","authors":"Alexei A. Efros, Eli Shechtman","author_ids":"1763086, 2177801","abstract":"We describe a method for providing feedback on portrait expressions, and for selecting the most attractive expressions from large video/photo collections. We capture a video of a subject's face while they are engaged in a task designed to elicit a range of positive emotions. We then use crowdsourcing to score the captured expressions for their attractiveness. We use these scores to train a model that can automatically predict attractiveness of different expressions of a given person. We also train a cross-subject model that evaluates portrait attractiveness of novel subjects and show how it can be used to automatically mine attractive photos from personal photo collections. Furthermore, we show how, with a little bit ($5-worth) of extra crowdsourcing, we can substantially improve the cross-subject model by \"fine-tuning\" it to a new individual using active learning. Finally, we demonstrate a training app that helps people learn how to mimic their best expressions.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"f838788faf2cdc54f4f3b9212fdc3df53313452b","venue_1":"ACM Trans. Graph.","year":"2016","title":"Globally optimal toon tracking","authors":"Haichao Zhu, Xueting Liu, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"2387872, 8016236, 1720633, 1714602","abstract":"The ability to identify objects or region correspondences between consecutive frames of a given hand-drawn animation sequence is an indispensable tool for automating animation modification tasks such as sequence-wide recoloring or shape-editing of a specific animated character. Existing correspondence identification methods heavily rely on appearance features, but these features alone are insufficient to reliably identify region correspondences when there exist occlusions or when two or more objects share similar appearances. To resolve the above problems, manual assistance is often required. In this paper, we propose a new correspondence identification method which considers both appearance features and motions of regions in a global manner. We formulate correspondence likelihoods between temporal region pairs as a network flow graph problem which can be solved by a well-established optimization algorithm. We have evaluated our method with various animation sequences and results show that our method consistently outperforms the state-of-the-art methods without any user guidance.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"549e14748029240b4b6d013e636abe479499f427","venue_1":"ACM Trans. Graph.","year":"2015","title":"Closure-aware sketch simplification","authors":"Xueting Liu, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"8016236, 1720633, 1714602","abstract":"In this paper, we propose a novel approach to simplify sketch drawings. The core problem is how to group sketchy strokes meaningfully, and this depends on how humans understand the sketches. The existing methods mainly rely on thresholding low-level geometric properties among the strokes, such as proximity, continuity and parallelism. However, it is not uncommon to have strokes with equal geometric properties but different semantics. The lack of semantic analysis will lead to the inability in differentiating the above semantically different scenarios. In this paper, we point out that, due to the gestalt phenomenon of <i>closure</i>, the grouping of strokes is actually highly influenced by the interpretation of regions. On the other hand, the interpretation of regions is also influenced by the interpretation of strokes since regions are formed and depicted by strokes. This is actually a chicken-or-the-egg dilemma and we solve it by an iterative cyclic refinement approach. Once the formed stroke groups are stabilized, we can simplify the sketchy strokes by replacing each stroke group with a smooth curve. We evaluate our method on a wide range of different sketch styles and semantically meaningful simplification results can be obtained in all test cases.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"9e548dc79eff7483e282c6e36d9e45ca13dc9a19","venue_1":"ACM Trans. Graph.","year":"2012","title":"Binocular tone mapping","authors":"Xuan S. Yang, Linling Zhang, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"1764231, 2592987, 1720633, 1714602","abstract":"By extending from monocular displays to binocular displays, one additional image domain is introduced. Existing binocular display systems only utilize this additional image domain for stereopsis. Our human vision is not only able to fuse two displaced images, but also two images with difference in detail, contrast and luminance, up to a certain limit. This phenomenon is known as <i>binocular single vision</i>. Humans can perceive more visual content via binocular fusion than just a linear blending of two views. In this paper, we make a first attempt in computer graphics to utilize this human vision phenomenon, and propose a binocular tone mapping framework. The proposed framework generates a binocular low-dynamic range (LDR) image pair that preserves more human-perceivable visual content than a single LDR image using the additional image domain. Given a tone-mapped LDR image (left, without loss of generality), our framework optimally synthesizes its counterpart (right) in the image pair from the same source HDR image. The two LDR images are different, so that they can aggregately present <i>more human-perceivable visual richness</i> than a single arbitrary LDR image, <i>without triggering visual discomfort</i>. To achieve this goal, a novel <i>binocular viewing comfort predictor</i> (BVCP) is also proposed to prevent such visual discomfort. The design of BVCP is based on the findings in vision science. Through our user studies, we demonstrate the increase of human-perceivable visual richness and the effectiveness of the proposed BVCP in conservatively predicting the visual discomfort threshold of human observers.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"5e7b2ff5ffda38ca115f9c6b570ba2945bc3a5f7","venue_1":"ACM Trans. Graph.","year":"2010","title":"Resizing by symmetry-summarization","authors":"Huisi Wu, Yu-Shuen Wang, Kun-Chuan Feng, Tien-Tsin Wong, Tong-Yee Lee, Pheng-Ann Heng","author_ids":"2207538, 3133234, 2563745, 1720633, 1724980, 1714602","abstract":"Image resizing can be achieved more effectively if we have a better understanding of the image semantics. In this paper, we analyze the <i>translational symmetry</i>, which exists in many real-world images. By detecting the symmetric lattice in an image, we can <i>summarize</i>, instead of only distorting or cropping, the image content. This opens a new space for image resizing that allows us to manipulate, not only image pixels, but also the semantic <i>cells</i> in the lattice. As a general image contains both symmetry &amp; non-symmetry regions and their natures are different, we propose to resize symmetry regions by summarization and non-symmetry region by warping. The difference in resizing strategy induces discontinuity at their shared boundary. We demonstrate how to reduce the artifact. To achieve practical resizing applications for general images, we developed a fast symmetry detection method that can detect multiple disjoint symmetry regions, even when the lattices are curved and perspectively viewed. Comparisons to state-of-the-art resizing techniques and a user study were conducted to validate the proposed method. Convincing visual results are shown to demonstrate its effectiveness.","cites":"36","conferencePercentile":"64.03508772"},{"venue":"ACM Trans. Graph.","id":"dd131f236b2bf19333548fdde7698dfc52b9757a","venue_1":"ACM Trans. Graph.","year":"2008","title":"Richness-preserving manga screening","authors":"Yingge Qu, Wai-Man Pang, Tien-Tsin Wong, Pheng-Ann Heng","author_ids":"2549934, 1729624, 1720633, 1714602","abstract":"Due to the tediousness and labor intensive cost, some manga artists have already employed computer-assisted methods for converting color photographs to manga backgrounds. However, existing bitonal image generation methods usually produce unsatisfactory uniform screening results that are not consistent with traditional mangas, in which the artist employs a rich set of screens. In this paper, we propose a novel method for generating bitonal manga backgrounds from color photographs. Our goal is to preserve the visual richness in the original photograph by utilizing not only screen density, but also the variety of screen patterns. To achieve the goal, we select screens for different regions in order to preserve the tone similarity, texture similarity, and chromaticity distinguishability. The multi-dimensional scaling technique is employed in such a color-to-pattern matching for maintaining pattern dissimilarity of the screens. Users can control the mapping by a few parameters and interactively fine-tune the result. Several results are presented to demonstrate the effectiveness and convenience of the proposed method.","cites":"16","conferencePercentile":"12.65432099"},{"venue":"ACM Trans. Graph.","id":"05dda1b3eb192ba5a1ca2f918087bb78c80f2fdf","venue_1":"ACM Trans. Graph.","year":"2008","title":"A precomputed polynomial representation for interactive BRDF editing with global illumination","authors":"Aner Ben-Artzi, Kevin Egan, Ravi Ramamoorthi, Frédo Durand","author_ids":"2822153, 3078722, 1752236, 1728125","abstract":"The ability to interactively edit BRDFs in their final placement within a computer graphics scene is vital to making informed choices for material properties. We significantly extend previous work on BRDF editing for static scenes (with fixed lighting and view) by developing a precomputed polynomial representation that enables interactive BRDF editing with global illumination. Unlike previous precomputation-based rendering techniques, the image is not linear in the BRDF when considering interreflections. We introduce a framework for precomputing a multibounce tensor of polynomial coefficients that encapsulates the nonlinear nature of the task. Significant reductions in complexity are achieved by leveraging the low-frequency nature of indirect light. We use a high-quality representation for the BRDFs at the first bounce from the eye and lower-frequency (often diffuse) versions for further bounces. This approximation correctly captures the general global illumination in a scene, including color-bleeding, near-field object reflections, and even caustics. We adapt Monte Carlo path tracing for precomputing the tensor of coefficients for BRDF basis functions. At runtime, the high-dimensional tensors can be reduced to a simple dot product at each pixel for rendering. We present a number of examples of editing BRDFs in complex scenes with interactive feedback rendered with global illumination.","cites":"27","conferencePercentile":"28.08641975"},{"venue":"ACM Trans. Graph.","id":"348dd3bc17f3ce9275f907ff505231876983c1b6","venue_1":"ACM Trans. Graph.","year":"2008","title":"Anisotropic noise","authors":"Alexander Goldberg, Matthias Zwicker, Frédo Durand","author_ids":"4308796, 1796846, 1728125","abstract":"Programmable graphics hardware makes it possible to generate procedural noise textures on the fly for interactive rendering. However, filtering and antialiasing procedural noise involves a tradeoff between aliasing artifacts and loss of detail. In this paper we present a technique, targeted at interactive applications, that provides high-quality anisotropic filtering for noise textures. We generate noise tiles directly in the frequency domain by partitioning the frequency domain into oriented subbands. We then compute weighted sums of the subband textures to accurately approximate noise with a desired spectrum. This allows us to achieve high-quality anisotropic filtering. Our approach is based solely on 2D textures, avoiding the memory overhead of techniques based on 3D noise tiles. We devise a technique to compensate for texture distortions to generate uniform noise on arbitrary meshes. We develop a GPU-based implementation of our technique that achieves similar rendering performance as state-of-the-art algorithms for procedural noise. In addition, it provides anisotropic filtering and achieves superior image quality.","cites":"10","conferencePercentile":"5.24691358"},{"venue":"ACM Trans. Graph.","id":"5b4f6ee11ca96b9e253263b824e476d729d1535f","venue_1":"ACM Trans. Graph.","year":"2008","title":"A meshless hierarchical representation for light transport","authors":"Jaakko Lehtinen, Matthias Zwicker, Emmanuel Turquin, Janne Kontkanen, Frédo Durand, François X. Sillion, Timo Aila","author_ids":"1780788, 1796846, 1765808, 1682553, 1728125, 1708617, 1761103","abstract":"We introduce a meshless hierarchical representation for solving light transport problems. Precomputed radiance transfer (PRT) and finite elements require a discrete representation of illumination over the scene. Non-hierarchical approaches such as per-vertex values are simple to implement, but lead to long precomputation. Hierarchical bases like wavelets lead to dramatic acceleration, but in their basic form they work well only on flat or smooth surfaces. We introduce a hierarchical function basis induced by scattered data approximation. It is decoupled from the geometric representation, allowing the hierarchical representation of illumination on complex objects. We present simple data structures and algorithms for constructing and evaluating the basis functions. Due to its hierarchical nature, our representation adapts to the complexity of the illumination, and can be queried at different scales. We demonstrate the power of the new basis in a novel precomputed direct-to-indirect light transport algorithm that greatly increases the complexity of scenes that can be handled by PRT approaches.","cites":"47","conferencePercentile":"58.95061728"},{"venue":"ACM Trans. Graph.","id":"095de867a0b846333f6fa48f468661d4e1f38497","venue_1":"ACM Trans. Graph.","year":"2014","title":"Light Field Reconstruction Using Sparsity in the Continuous Fourier Domain","authors":"Lixin Shi, Haitham Hassanieh, Abe Davis, Dina Katabi, Frédo Durand","author_ids":"1850632, 2349340, 1825995, 1785714, 1728125","abstract":"Sparsity in the Fourier domain is an important property that enables the dense reconstruction of signals, such as 4D light fields, from a small set of samples. The sparsity of natural spectra is often derived from continuous arguments, but reconstruction algorithms typically work in the discrete Fourier domain. These algorithms usually assume that sparsity derived from continuous principles will hold under discrete sampling. This article makes the critical observation that sparsity is much greater in the <i>continuous</i> Fourier spectrum than in the <i>discrete</i> spectrum. This difference is caused by a windowing effect. When we sample a signal over a finite window, we convolve its spectrum by an infinite sinc, which destroys much of the sparsity that was in the continuous domain. Based on this observation, we propose an approach to reconstruction that optimizes for sparsity in the continuous Fourier spectrum. We describe the theory behind our approach and discuss how it can be used to reduce sampling requirements and improve reconstruction quality. Finally, we demonstrate the power of our approach by showing how it can be applied to the task of recovering non-Lambertian light fields from a small number of 1D viewpoint trajectories.","cites":"6","conferencePercentile":"35.59670782"},{"venue":"ACM Trans. Graph.","id":"0feedfde298fa5732cf270a6ab51dc3a88ffa924","venue_1":"ACM Trans. Graph.","year":"2010","title":"Geodesic image and video editing","authors":"Antonio Criminisi, Toby Sharp, Carsten Rother, Patrick Pérez","author_ids":"1716777, 2732209, 7699610, 1799777","abstract":"This article presents a new, unified technique to perform general edge-sensitive editing operations on n-dimensional images and videos efficiently.\n The first contribution of the article is the introduction of a Generalized Geodesic Distance Transform (GGDT), based on soft masks. This provides a <i>unified</i> framework to address several edge-aware editing operations. Diverse tasks such as denoising and nonphotorealistic rendering are all dealt with fundamentally the same, fast algorithm. Second, a new Geodesic Symmetric Filter (GSF) is presented which imposes contrast-sensitive spatial smoothness into segmentation and segmentation-based editing tasks (cutout, object highlighting, colorization, panorama stitching). The effect of the filter is controlled by two intuitive, geometric parameters. In contrast to existing techniques, the GSF filter is applied to real-valued pixel likelihoods (soft masks), thanks to GGDTs and it can be used for both interactive and automatic editing. Complex object topologies are dealt with effortlessly. Finally, the parallelism of GGDTs enables us to exploit modern multicore CPU architectures as well as powerful new GPUs, thus providing great flexibility of implementation and deployment. Our technique operates on both images and videos, and generalizes naturally to n-dimensional data.\n The proposed algorithm is validated via quantitative and qualitative comparisons with existing, state-of-the-art approaches. Numerous results on a variety of image and video editing tasks further demonstrate the effectiveness of our method.","cites":"49","conferencePercentile":"84.21052632"},{"venue":"ACM Trans. Graph.","id":"5fdf3f35220a42902c5466848c790808171789b6","venue_1":"ACM Trans. Graph.","year":"2010","title":"Programmable rendering of line drawing from 3D scenes","authors":"Stéphane Grabli, Emmanuel Turquin, Frédo Durand, François X. Sillion","author_ids":"1804400, 1765808, 1728125, 1708617","abstract":"This article introduces a programmable approach to nonphotorealistic line drawings from 3D models, inspired by programmable shaders in traditional rendering. This approach relies on the assumption generally made in NPR that style attributes (color, thickness, etc.) are chosen depending on generic properties of the scene such as line characteristics or depth discontinuities, etc. We propose a new image creation model where all operations are controlled through user-defined procedures in which the relations between style attributes and scene properties are specified. A <i>view map</i> describing all relevant support lines in the drawing and their topological arrangement is first created from the 3D model so as to ensure the continuity of all scene properties along its edges; a number of style modules operate on this map, by procedurally selecting, chaining, or splitting lines, before creating strokes and assigning drawing attributes. Consistent access to properties of the scene is provided from the different elements of the map that are manipulated throughout the whole process. The resulting drawing system permits flexible control of all elements of drawing style: First, different style modules can be applied to different types of lines in a view; second, the topology and geometry of strokes are entirely controlled from the programmable modules; and third, stroke attributes are assigned procedurally and can be correlated at will with various scene or view properties. We illustrate the components of our system and show how style modules successfully encode stylized visual characteristics that can be applied across a wide range of models.","cites":"18","conferencePercentile":"28.3625731"},{"venue":"ACM Trans. Graph.","id":"3162092e2a8b752ae85838982e7388dae5952f40","venue_1":"ACM Trans. Graph.","year":"2016","title":"Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences","authors":"Jonathan Taylor, Lucas Bordeaux, Thomas J. Cashman, Bob Corish, Cem Keskin, Toby Sharp, Eduardo Soto, David Sweeney, Julien P. C. Valentin, Benjamin Luff, Arran Topalian, Erroll Wood, Sameh Khamis, Pushmeet Kohli, Shahram Izadi, Richard Banks, Andrew W. Fitzgibbon, Jamie Shotton","author_ids":"7356633, 3023847, 1796780, 3439575, 7206941, 2732209, 6285575, 2774999, 2898574, 3440690, 3440027, 2128504, 3144576, 1685185, 1699068, 1698225, 1708974, 1751781","abstract":"Fully articulated hand tracking promises to enable fundamentally new interactions with virtual and augmented worlds, but the limited accuracy and efficiency of current systems has prevented widespread adoption. Today's dominant paradigm uses machine learning for initialization and recovery followed by iterative model-fitting optimization to achieve a detailed pose fit. We follow this paradigm, but make several changes to the model-fitting, namely using: (1) a more discriminative objective function; (2) a smooth-surface model that provides gradients for non-linear optimization; and (3) joint optimization over both the model pose and the correspondences between observed data points and the model surface. While each of these changes may actually <i>increase</i> the cost per fitting iteration, we find a compensating decrease in the number of iterations. Further, the wide basin of convergence means that fewer starting points are needed for successful model fitting. Our system runs in real-time on CPU only, which frees up the commonly over-burdened GPU for experience designers. The hand tracker is efficient enough to run on low-power devices such as tablets. We can track up to several meters from the camera to provide a large working volume for interaction, even using the noisy data from current-generation depth cameras. Quantitative assessments on standard datasets show that the new approach exceeds the state of the art in accuracy. Qualitative results take the form of live recordings of a range of interactive experiences enabled by this new approach.","cites":"5","conferencePercentile":"97.46835443"},{"venue":"ACM Trans. Graph.","id":"1b95f1580cad83af8cba8f29681251e4513b00fe","venue_1":"ACM Trans. Graph.","year":"2005","title":"Energy redistribution path tracing","authors":"David Cline, Justin Talbot, Parris K. Egbert","author_ids":"3072268, 3094694, 1796197","abstract":"We present Energy Redistribution (ER) sampling as an unbiased method to solve correlated integral problems. ER sampling is a hybrid algorithm that uses Metropolis sampling-like mutation strategies in a standard Monte Carlo integration setting, rather than resorting to an intermediate probability distribution step. In the context of global illumination, we present Energy Redistribution Path Tracing (ERPT). Beginning with an inital set of light samples taken from a path tracer, ERPT uses path mutations to redistribute the energy of the samples over the image plane to reduce variance. The result is a global illumination algorithm that is conceptually simpler than Metropolis Light Transport (MLT) while retaining its most powerful feature, path mutation. We compare images generated with the new technique to standard path tracing and MLT.","cites":"48","conferencePercentile":"23.79032258"},{"venue":"ACM Trans. Graph.","id":"de5b55ea99c6e80344757b9b9e1585cd091ce6e6","venue_1":"ACM Trans. Graph.","year":"2014","title":"Factored axis-aligned filtering for rendering multiple distribution effects","authors":"Soham Uday Mehta, JiaXian Yao, Ravi Ramamoorthi, Frédo Durand","author_ids":"2269369, 2630145, 1752236, 1728125","abstract":"Monte Carlo (MC) ray-tracing for photo-realistic rendering often requires hours to render a single image due to the large sampling rates needed for convergence. Previous methods have attempted to filter sparsely sampled MC renders but these methods have high reconstruction overheads. Recent work has shown fast performance for individual effects, like soft shadows and indirect illumination, using axis-aligned filtering. While some components of light transport such as indirect or area illumination are smooth, they are often multiplied by high-frequency components such as texture, which prevents their sparse sampling and reconstruction.\n We propose an approach to adaptively sample and filter for simultaneously rendering primary (defocus blur) and secondary (soft shadows and indirect illumination) distribution effects, based on a multi-dimensional frequency analysis of the direct and indirect illumination light fields. We describe a novel approach of factoring texture and irradiance in the presence of defocus blur, which allows for pre-filtering noisy irradiance when the texture is not noisy. Our approach naturally allows for different sampling rates for primary and secondary effects, further reducing the overall ray count. While the theory considers only Lambertian surfaces, we obtain promising results for moderately glossy surfaces. We demonstrate 30x sampling rate reduction compared to equal quality noise-free MC. Combined with a GPU implementation and low filtering over-head, we can render scenes with complex geometry and diffuse and glossy BRDFs in a few seconds.","cites":"11","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"1f78591fc40cf97636599cd4f92ab345f519c42c","venue_1":"ACM Trans. Graph.","year":"2012","title":"CrossShade: shading concept sketches using cross-section curves","authors":"Cloud Shao, Adrien Bousseau, Alla Sheffer, Karan Singh","author_ids":"2767415, 2149814, 3354923, 1682205","abstract":"We facilitate the creation of 3D-looking shaded production drawings from concept sketches. The key to our approach is a class of commonly used construction curves known as <i>cross-sections</i>, that function as an aid to both sketch creation and viewer understanding of the depicted 3D shape. In particular, intersections of these curves, or <i>cross-hairs</i>, convey valuable 3D information, that viewers compose into a mental model of the overall sketch. We use the artist-drawn cross-sections to automatically infer the 3D normals across the sketch, enabling 3D-like rendering.\n The technical contribution of our work is twofold. First, we distill artistic guidelines for drawing cross-sections and insights from perception literature to introduce an explicit mathematical formulation of the relationships between cross-section curves and the geometry they aim to convey. We then use these relationships to develop an algorithm for estimating a normal field from cross-section curve networks and other curves present in concept sketches. We validate our formulation and algorithm through a user study and a ground truth normal comparison. As demonstrated by the examples throughout the paper, these contributions enable us to shade a wide range of concept sketches with a variety of rendering styles.","cites":"13","conferencePercentile":"35.1010101"},{"venue":"ACM Trans. Graph.","id":"6bbc3fffea381e82d2507800c7e5763dff00766f","venue_1":"ACM Trans. Graph.","year":"2014","title":"How do people edit light fields?","authors":"Adrian Jarabo, Belen Masia, Adrien Bousseau, Fabio Pellacini, Diego Gutierrez","author_ids":"2208378, 1775667, 2149814, 1757883, 1723695","abstract":"We present a thorough study to evaluate different light field editing interfaces, tools and workflows from a user perspective. This is of special relevance given the multidimensional nature of light fields, which may make common image editing tasks become complex in light field space. We additionally investigate the potential benefits of using depth information when editing, and the limitations imposed by imperfect depth reconstruction using current techniques. We perform two different experiments, collecting both objective and subjective data from a varied number of editing tasks of increasing complexity based on local point-and-click tools. In the first experiment, we rely on perfect depth from synthetic light fields, and focus on simple edits. This allows us to gain basic insight on light field editing, and to design a more advanced editing interface. This is then used in the second experiment, employing real light fields with imperfect reconstructed depth, and covering more advanced editing tasks. Our study shows that users can edit light fields with our tested interface and tools, even in the presence of imperfect depth. They follow different workflows depending on the task at hand, mostly relying on a combination of different depth cues. Last, we confirm our findings by asking a set of artists to freely edit both real and synthetic light fields.","cites":"9","conferencePercentile":"54.32098765"},{"venue":"ACM Trans. Graph.","id":"99f176345f720432ff94df1aabb1dd2bbd5fcde8","venue_1":"ACM Trans. Graph.","year":"2015","title":"Multiview Intrinsic Images of Outdoors Scenes with an Application to Relighting","authors":"Sylvain Duchêne, Clément Riant, Gaurav Chaurasia, Jorge Lopez-Moreno, Pierre-Yves Laffont, Stefan Popov, Adrien Bousseau, George Drettakis","author_ids":"3052859, 3200745, 2585067, 1752874, 2615801, 1721543, 2149814, 1721779","abstract":"We introduce a method to compute intrinsic images for a multiview set of outdoor photos with cast shadows, taken under the same lighting. We use an automatic 3D reconstruction from these photos and the sun direction as input and decompose each image into reflectance and shading layers, despite the inaccuracies and missing data of the 3D model. Our approach is based on two key ideas. First, we progressively improve the accuracy of the parameters of our image formation model by performing iterative estimation and combining 3D lighting simulation with 2D image optimization methods. Second, we use the image formation model to express reflectance as a function of discrete visibility values for shadow and light, which allows to introduce a robust visibility classifier for pairs of points in a scene. This classifier is used for shadow labeling, allowing to compute high-quality reflectance and shading layers. Our multiview intrinsic decomposition is of sufficient quality to allow relighting of the input images. We create shadow-caster geometry which preserves shadow silhouettes and, using the intrinsic layers, we can perform multiview relighting with moving cast shadows. We present results on several multiview datasets, and show how it is now possible to perform image-based rendering with changing illumination conditions.","cites":"6","conferencePercentile":"73.87755102"},{"venue":"ACM Trans. Graph.","id":"fd01f552f902871aa508e730e4b5c210ba2c3e87","venue_1":"ACM Trans. Graph.","year":"2016","title":"Fidelity vs. simplicity: a global approach to line drawing vectorization","authors":"Jean-Dominique Favreau, Florent Lafarge, Adrien Bousseau","author_ids":"1928333, 3176947, 2149814","abstract":"Vector drawing is a popular representation in graphic design because of the precision, compactness and editability offered by parametric curves. However, prior work on line drawing vectorization focused solely on faithfully capturing input bitmaps, and largely overlooked the problem of producing a compact and editable curve network. As a result, existing algorithms tend to produce overly-complex drawings composed of many short curves and control points, especially in the presence of thick or sketchy lines that yield spurious curves at junctions. We propose the first vectorization algorithm that explicitly balances fidelity to the input bitmap with simplicity of the output, as measured by the number of curves and their degree. By casting this trade-off as a global optimization, our algorithm generates few yet accurate curves, and also disambiguates curve topology at junctions by favoring the simplest interpretations overall. We demonstrate the robustness of our algorithm on a variety of drawings, sketchy cartoons and rough design sketches.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"43554798990c250fcef7054307907170dba079ae","venue_1":"ACM Trans. Graph.","year":"2016","title":"Interactive sketching of urban procedural models","authors":"Gen Nishida, Ignacio Garcia-Dorado, Daniel G. Aliaga, Bedrich Benes, Adrien Bousseau","author_ids":"3061837, 1826251, 1698910, 7590480, 2149814","abstract":"3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use non-photorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments. A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.","cites":"2","conferencePercentile":"86.28691983"},{"venue":"ACM Trans. Graph.","id":"b1455373fbbdb1c0aab8d71200fdea6bc187380f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Axis-aligned filtering for interactive physically-based diffuse indirect lighting","authors":"Soham Uday Mehta, Brandon Wang, Ravi Ramamoorthi, Frédo Durand","author_ids":"2269369, 2983976, 1752236, 1728125","abstract":"We introduce an algorithm for interactive rendering of physically-based global illumination, based on a novel frequency analysis of indirect lighting. Our method combines adaptive sampling by Monte Carlo ray or path tracing, using a standard GPU-accelerated raytracer, with real-time reconstruction of the resulting noisy images. Our theoretical analysis assumes diffuse indirect lighting, with general Lambertian and specular receivers. In practice, we demonstrate accurate interactive global illumination with diffuse and moderately glossy objects, at 1-3 fps. We show mathematically that indirect illumination is a structured signal in the Fourier domain, with inherent band-limiting due to the BRDF and geometry terms. We extend previous work on sheared and axis-aligned filtering for motion blur and shadows, to develop an image-space filtering method for interreflections. Our method enables 5--8X reduced sampling rates and wall clock times, and converges to ground truth as more samples are added. To develop our theory, we overcome important technical challenges---unlike previous work, there is no light source to serve as a band-limit in indirect lighting, and we also consider non-parallel geometry of receiver and reflecting surfaces, without first-order approximations.","cites":"17","conferencePercentile":"67.19457014"},{"venue":"ACM Trans. Graph.","id":"bdb183259888fb0e7672f0cb817691e6d0101683","venue_1":"ACM Trans. Graph.","year":"2015","title":"Anisotropic Gaussian mutations for metropolis light transport through Hessian-Hamiltonian dynamics","authors":"Tzu-Mao Li, Jaakko Lehtinen, Ravi Ramamoorthi, Wenzel Jakob, Frédo Durand","author_ids":"1775024, 1780788, 1752236, 1780369, 1728125","abstract":"The simulation of light transport in the presence of multi-bounce glossy effects and motion is challenging because the integrand is high dimensional and areas of high-contribution tend to be narrow and hard to sample. We present a Markov Chain Monte Carlo (MCMC) rendering algorithm that extends Metropolis Light Transport by automatically and explicitly adapting to the local shape of the integrand, thereby increasing the acceptance rate. Our algorithm characterizes the local behavior of throughput in path space using its gradient as well as its Hessian. In particular, the Hessian is able to capture the strong anisotropy of the integrand. We obtain the derivatives using automatic differentiation, which makes our solution general and easy to extend to additional sampling dimensions such as time.\n However, the resulting second order Taylor expansion is not a proper distribution and cannot be used directly for importance sampling. Instead, we use ideas from Hamiltonian Monte-Carlo and simulate the Hamiltonian dynamics in a flipped version of the Taylor expansion where gravity pulls particles towards the high-contribution region. Whereas such methods usually require numerical integration, we show that our quadratic landscape leads to a closed-form anisotropic Gaussian distribution for the final particle positions, and it results in a standard Metropolis-Hastings algorithm. Our method excels at rendering glossy-to-glossy reflections on small and highly curved surfaces. Furthermore, unlike previous work that derives sampling anisotropy with pen and paper and only considers specific effects such as specular BSDFs, we characterize the local shape of throughput through automatic differentiation. This makes our approach very general. In particular, our method is the first MCMC rendering algorithm that is able to resolve the anisotropy in the time dimension and render difficult moving caustics.","cites":"2","conferencePercentile":"31.63265306"},{"venue":"ACM Trans. Graph.","id":"ccb8c78634fe0c92a96801c1d543b6ac5e5704ac","venue_1":"ACM Trans. Graph.","year":"2013","title":"Gradient-domain metropolis light transport","authors":"Jaakko Lehtinen, Tero Karras, Samuli Laine, Miika Aittala, Frédo Durand, Timo Aila","author_ids":"1780788, 2976930, 2365390, 1907688, 1728125, 1761103","abstract":"We introduce a novel Metropolis rendering algorithm that directly computes image gradients, and reconstructs the final image from the gradients by solving a Poisson equation. The reconstruction is aided by a low-fidelity approximation of the image computed during gradient sampling. As an extension of path-space Metropolis light transport, our algorithm is well suited for difficult transport scenarios. We demonstrate that our method outperforms the state-of-the-art in several well-known test scenes. Additionally, we analyze the spectral properties of gradient-domain sampling, and compare it to the traditional image-domain sampling.","cites":"12","conferencePercentile":"45.47511312"},{"venue":"ACM Trans. Graph.","id":"5d47720d61b88293ff229260605d22972349dc0f","venue_1":"ACM Trans. Graph.","year":"2013","title":"Phase-based video motion processing","authors":"Neal Wadhwa, Michael Rubinstein, Frédo Durand, William T. Freeman","author_ids":"2103475, 1836449, 1728125, 1768236","abstract":"We introduce a technique to manipulate small movements in videos based on an analysis of motion in complex-valued image pyramids. Phase variations of the coefficients of a complex-valued steerable pyramid over time correspond to motion, and can be temporally processed and amplified to reveal imperceptible motions, or attenuated to remove distracting changes. This processing does not involve the computation of optical flow, and in comparison to the previous Eulerian Video Magnification method it supports larger amplification factors and is significantly less sensitive to noise. These improved capabilities broaden the set of applications for motion processing in videos. We demonstrate the advantages of this approach on synthetic and natural video sequences, and explore applications in scientific analysis, visualization and video enhancement.","cites":"36","conferencePercentile":"89.36651584"},{"venue":"ACM Trans. Graph.","id":"77ec194c93d77dabeb7f31a84afa1e137b7dde68","venue_1":"ACM Trans. Graph.","year":"2012","title":"Eulerian video magnification for revealing subtle changes in the world","authors":"Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John V. Guttag, Frédo Durand, William T. Freeman","author_ids":"2886966, 1836449, 2832119, 1724429, 1728125, 1768236","abstract":"Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at the temporal frequencies selected by the user.","cites":"115","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"994efaea3ba4bf2e0bf599f0b65aafd79b15e4ab","venue_1":"ACM Trans. Graph.","year":"2012","title":"Sculpting by numbers","authors":"Alec R. Rivers, Andrew Adams, Frédo Durand","author_ids":"1913819, 4030789, 1728125","abstract":"We propose a method that allows an unskilled user to create an accurate physical replica of a digital 3D model. We use a projector/camera pair to scan a work in progress, and project multiple forms of guidance onto the object itself that indicate which areas need more material, which need less, and where any ridges, valleys or depth discontinuities are. The user adjusts the model using the guidance and iterates, making the shape of the physical object approach that of the target 3D model over time. We show how this approach can be used to create a duplicate of an existing object, by scanning the object and using that scan as the target shape. The user is free to make the reproduction at a different scale and out of different materials: we turn a toy car into cake. We extend the technique to support replicating a sequence of models to create stop-motion video. We demonstrate an end-to-end system in which real-world performance capture data is retargeted to claymation. Our approach allows users to easily and accurately create complex shapes, and naturally supports a large range of materials and model sizes.","cites":"24","conferencePercentile":"69.6969697"},{"venue":"ACM Trans. Graph.","id":"299c00a527d106d906026f4fef8b322312fb491c","venue_1":"ACM Trans. Graph.","year":"2012","title":"Reconstructing the indirect light field for global illumination","authors":"Jaakko Lehtinen, Timo Aila, Samuli Laine, Frédo Durand","author_ids":"1780788, 1761103, 2365390, 1728125","abstract":"Stochastic techniques for rendering indirect illumination suffer from noise due to the variance in the integrand. In this paper, we describe a general reconstruction technique that exploits anisotropy in the light field and permits efficient reuse of input samples between pixels or world-space locations, multiplying the effective sampling rate by a large factor. Our technique introduces visibility-aware anisotropic reconstruction to indirect illumination, ambient occlusion and glossy reflections. It operates on point samples without knowledge of the scene, and can thus be seen as an advanced image filter. Our results show dramatic improvement in image quality while using very sparse input samplings.","cites":"22","conferencePercentile":"65.4040404"},{"venue":"ACM Trans. Graph.","id":"517ac6c483ebcb1a0676180d1cbbeed0fde82029","venue_1":"ACM Trans. Graph.","year":"2012","title":"Position-correcting tools for 2D digital fabrication","authors":"Alec R. Rivers, Ilan E. Moyer, Frédo Durand","author_ids":"1913819, 2735190, 1728125","abstract":"Many kinds of digital fabrication are accomplished by precisely moving a tool along a digitally-specified path. This precise motion is typically accomplished fully automatically using a computer-controlled multi-axis stage. With that approach, one can only create objects smaller than the positioning stage, and large stages can be quite expensive. We propose a new approach to precise positioning of a tool that combines manual and automatic positioning: in our approach, the user coarsely positions a frame containing the tool in an approximation of the desired path, while the device tracks the frame's location and adjusts the position of the tool within the frame to correct the user's positioning error in real time. Because the automatic positioning need only cover the range of the human's positioning error, this frame can be small and inexpensive, and because the human has unlimited range, such a frame can be used to precisely position tools over an unlimited range.","cites":"21","conferencePercentile":"61.86868687"},{"venue":"ACM Trans. Graph.","id":"7648ec5f4b6503e0e7cf1663fa45362e0965d939","venue_1":"ACM Trans. Graph.","year":"2012","title":"Structural optimization of 3D masonry buildings","authors":"Emily Whiting, Hijung Shin, Robert Wang, John Ochsendorf, Frédo Durand","author_ids":"2778710, 2596714, 3771277, 2052915, 1728125","abstract":"In the design of buildings, structural analysis is traditionally performed after the aesthetic design has been determined and has little influence on the overall form. In contrast, this paper presents an approach to guide the form towards a shape that is more structurally sound. Our work is centered on the study of how variations of the geometry might improve structural stability. We define a new measure of structural soundness for masonry buildings as well as cables, and derive its closed-form derivative with respect to the displacement of all the vertices describing the geometry. We start with a gradient descent tool which displaces each vertex along the gradient. We then introduce displacement operators, imposing constraints such as the preservation of orientation or thickness; or setting additional objectives such as volume minimization.","cites":"25","conferencePercentile":"72.47474747"},{"venue":"ACM Trans. Graph.","id":"fd33d1fc2e119194c6d6ea290f38934ec5484fd5","venue_1":"ACM Trans. Graph.","year":"2011","title":"Frequency analysis and sheared filtering for shadow light fields of complex occluders","authors":"Kevin Egan, Florian Hecht, Frédo Durand, Ravi Ramamoorthi","author_ids":"3078722, 2986696, 1728125, 1752236","abstract":"Monte Carlo ray tracing of soft shadows produced by area lighting and intricate geometries, such as the shadows through plant leaves or arrays of blockers, is a critical challenge. The final image often has relatively smooth shadow patterns, since it integrates over the light source. However, Monte Carlo rendering exhibits considerable noise even at high sample counts because of the large variance of the integrand due to the intricate shadow function. This article develops an efficient diffuse soft shadow technique for mid to far occluders that relies on a new 4D cache and sheared reconstruction filter. For this, we first derive a frequency analysis of shadows for planar area lights and complex occluders. Our analysis subsumes convolution soft shadows for parallel planes as a special case. It allows us to derive 4D sheared filters that enable lower sampling rates for soft shadows. While previous sheared-reconstruction techniques were able primarily to index samples according to screen position, we need to perform reconstruction at surface receiver points that integrate over vastly different shapes in the reconstruction domain. This is why we develop a new light-field-like 4D data structure to store shadowing values and depth information. Any ray tracing system that shoots shadow rays can easily incorporate our method to greatly reduce sampling rates for diffuse soft shadows.","cites":"48","conferencePercentile":"84.47368421"},{"venue":"ACM Trans. Graph.","id":"49ee61386fe52ff09541c97655cdc0dffafd3973","venue_1":"ACM Trans. Graph.","year":"2011","title":"Practical filtering for efficient ray-traced directional occlusion","authors":"Kevin Egan, Frédo Durand, Ravi Ramamoorthi","author_ids":"3078722, 1728125, 1752236","abstract":"Ambient occlusion and directional (spherical harmonic) occlusion have become a staple of production rendering because they capture many visually important qualities of global illumination while being reusable across multiple artistic lighting iterations. However, ray-traced solutions for hemispherical occlusion require many rays per shading point (typically 256-1024) due to the full hemispherical angular domain. Moreover, each ray can be expensive in scenes with moderate to high geometric complexity. However, many nearby rays sample similar areas, and the final occlusion result is often low frequency. We give a frequency analysis of shadow light fields using distant illumination with a general BRDF and normal mapping, allowing us to share ray information even among complex receivers. We also present a new rotationally-invariant filter that easily handles samples spread over a large angular domain. Our method can deliver 4x speed up for scenes that are computationally bound by ray tracing costs.","cites":"26","conferencePercentile":"57.36842105"},{"venue":"ACM Trans. Graph.","id":"335027acc27abd992632adf40ee2adf7ec2c91c4","venue_1":"ACM Trans. Graph.","year":"2013","title":"5D Covariance tracing for efficient defocus and motion blur","authors":"Laurent Belcour, Cyril Soler, Kartic Subr, Nicolas Holzschuch, Frédo Durand","author_ids":"2514769, 1866568, 2740413, 3174773, 1728125","abstract":"The rendering of effects such as motion blur and depth-of-field requires costly 5D integrals. We accelerate their computation through adaptive sampling and reconstruction based on the prediction of the anisotropy and bandwidth of the integrand. For this, we develop a new frequency analysis of the 5D temporal light-field, and show that first-order motion can be handled through simple changes of coordinates in 5D. We further introduce a compact representation of the spectrum using the covariance matrix and Gaussian approximations. We derive update equations for the 5 &#215; 5 covariance matrices for each atomic light transport event, such as transport, occlusion, BRDF, texture, lens, and motion. The focus on atomic operations makes our work general, and removes the need for special-case formulas. We present a new rendering algorithm that computes 5D covariance matrices on the image plane by tracing paths through the scene, focusing on the single-bounce case. This allows us to reduce sampling rates when appropriate and perform reconstruction of images with complex depth-of-field and motion blur effects.","cites":"24","conferencePercentile":"78.73303167"},{"venue":"ACM Trans. Graph.","id":"0dde9b7b1e3aab2dcef6456dde1d2237fbbdc575","venue_1":"ACM Trans. Graph.","year":"2014","title":"Parametric wave field coding for precomputed sound propagation","authors":"Nikunj Raghuvanshi, John Snyder","author_ids":"3032886, 6314473","abstract":"The acoustic wave field in a complex scene is a chaotic 7D function of time and the positions of source and listener, making it difficult to compress and interpolate. This hampers precomputed approaches which tabulate impulse responses (IRs) to allow immersive, real-time sound propagation in static scenes. We code the field of time-varying IRs in terms of a few perceptual parameters derived from the IR's energy decay. The resulting parameter fields are spatially smooth and compressed using a lossless scheme similar to PNG. We show that this encoding removes two of the seven dimensions, making it possible to handle large scenes such as entire game maps within 100MB of memory. Run-time decoding is fast, taking 100<i>&mu;</i>s per source. We introduce an efficient and scalable method for convolutionally rendering acoustic parameters that generates artifact-free audio even for fast motion and sudden changes in reverberance. We demonstrate convincing spatially-varying effects in complex scenes including occlusion/obstruction and reverberation, in our system integrated with Unreal Engine 3<sup>&#8482;</sup>.","cites":"7","conferencePercentile":"42.18106996"},{"venue":"ACM Trans. Graph.","id":"ba7a2b650b42272611baa72c2d398af7ac07221a","venue_1":"ACM Trans. Graph.","year":"2013","title":"Coded time of flight cameras: sparse deconvolution to address multipath interference and recover time profiles","authors":"Achuta Kadambi, Refael Whyte, Ayush Bhandari, Lee V. Streeter, Christopher Barsi, Adrian A. Dorrington, Ramesh Raskar","author_ids":"7992331, 2150459, 3276312, 2687142, 3112458, 1807151, 1717566","abstract":"Time of flight cameras produce real-time range maps at a relatively low cost using continuous wave amplitude modulation and demodulation. However, they are geared to measure range (or phase) for a single reflected bounce of light and suffer from systematic errors due to multipath interference.\n We re-purpose the conventional time of flight device for a new goal: to recover per-pixel sparse time profiles expressed as a sequence of impulses. With this modification, we show that we can not only address multipath interference but also enable new applications such as recovering depth of near-transparent surfaces, looking through diffusers and creating time-profile movies of sweeping light.\n Our key idea is to formulate the forward amplitude modulated light propagation as a convolution with custom codes, record samples by introducing a simple sequence of electronic time delays, and perform sparse deconvolution to recover sequences of Diracs that correspond to multipath returns. Applications to computer vision include ranging of near-transparent objects and subsurface imaging through diffusers. Our low cost prototype may lead to new insights regarding forward and inverse problems in light transport.","cites":"42","conferencePercentile":"92.98642534"},{"venue":"ACM Trans. Graph.","id":"059582bee125512b127296364e7700ebd9f80436","venue_1":"ACM Trans. Graph.","year":"2016","title":"Action-driven 3D indoor scene evolution","authors":"Rui Ma, Honghua Li, Changqing Zou, Zicheng Liao, Xin Tong, Hao Zhang","author_ids":"5888590, 1829406, 2876552, 2928799, 1743927, 1694161","abstract":"We introduce a framework for <i>action-driven evolution</i> of 3D indoor scenes, where the goal is to simulate how scenes are altered by human actions, and specifically, by object placements necessitated by the actions. To this end, we develop an <i>action model</i> with each type of action combining information about one or more human poses, one or more object categories, and spatial configurations of objects belonging to these categories which summarize the object-object and object-human relations for the action. Importantly, all these pieces of information are learned from annotated <i>photos.</i> Correlations between the learned actions are analyzed to guide the construction of an <i>action graph.</i> Starting with an initial 3D scene, we probabilistically sample a sequence of actions from the action graph to drive progressive scene evolution. Each action triggers appropriate object placements, based on object co-occurrences and spatial configurations learned for the action model. We show results of our scene evolution that lead to realistic and messy 3D scenes, as well as quantitative evaluations by user studies which compare our method to manual scene creation and state-of-the-art, data-driven methods, in terms of scene plausibility and naturalness.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"86dbdb1b6613ea4576bf801b91d20f33b4b24078","venue_1":"ACM Trans. Graph.","year":"2007","title":"Distinctive regions of 3D surfaces","authors":"Philip Shilane, Thomas A. Funkhouser","author_ids":"1816098, 1807080","abstract":"Selecting the most important regions of a surface is useful for shape matching and a variety of applications in computer graphics and geometric modeling. While previous research has analyzed geometric properties of meshes in isolation, we select regions that distinguish a shape from objects of a different type. Our approach to analyzing distinctive regions is based on performing a shape-based search using each region as a query into a database. Distinctive regions of a surface have shape consistent with objects of the same type and different from objects of other types. We demonstrate the utility of detecting distinctive surface regions for shape matching and other graphics applications including mesh visualization, icon generation, and mesh simplification.","cites":"68","conferencePercentile":"63.6"},{"venue":"ACM Trans. Graph.","id":"33cd48444a31399605a8dc0397f1a93d2329138a","venue_1":"ACM Trans. Graph.","year":"2012","title":"Stress relief: improving structural strength of 3D printable objects","authors":"Ondrej Stava, Juraj Vanek, Bedrich Benes, Nathan A. Carr, Radomír Mech","author_ids":"2239460, 2913424, 7590480, 1682513, 2008027","abstract":"The use of 3D printing has rapidly expanded in the past couple of years. It is now possible to produce 3D-printed objects with exceptionally high fidelity and precision. However, although the quality of 3D printing has improved, both the time to print and the material costs have remained high. Moreover, there is no guarantee that a printed model is structurally sound. The printed product often does not survive cleaning, transportation, or handling, or it may even collapse under its own weight. We present a system that addresses this issue by providing automatic detection and correction of the problematic cases. The structural problems are detected by combining a lightweight structural analysis solver with 3D medial axis approximations. After areas with high structural stress are found, the model is corrected by combining three approaches: hollowing, thickening, and strut insertion. Both detection and correction steps are repeated until the problems have been eliminated. Our process is designed to create a model that is visually similar to the original model but possessing greater structural integrity.","cites":"75","conferencePercentile":"98.48484848"},{"venue":"ACM Trans. Graph.","id":"50bff7d5f4d8f3f230b6f446e667dd22f12567dc","venue_1":"ACM Trans. Graph.","year":"2004","title":"High dynamic range display systems","authors":"Helge Seetzen, Wolfgang Heidrich, Wolfgang Stuerzlinger, Greg Ward, Lorne Whitehead, Matthew Trentacoste, Abhijeet Ghosh, Andrejs Vorozcovs","author_ids":"1710304, 1752192, 3342964, 1689242, 1740256, 1736851, 1740273, 1690878","abstract":"The dynamic range of many real-world environments exceeds the capabilities of current display technology by several orders of magnitude. In this paper we discuss the design of two different display systems that are capable of displaying images with a dynamic range much more similar to that encountered in the real world. The first display system is based on a combination of an LCD panel and a DLP projector, and can be built from off-the-shelf components. While this design is feasible in a lab setting, the second display system, which relies on a custom-built LED panel instead of the projector, is more suitable for usual office workspaces and commercial applications. We describe the design of both systems as well as the software issues that arise. We also discuss the advantages and disadvantages of the two designs and potential applications for both systems.","cites":"197","conferencePercentile":"78.26086957"},{"venue":"ACM Trans. Graph.","id":"3417609ef90dfb9ee5745453c24833ed380f1efb","venue_1":"ACM Trans. Graph.","year":"2009","title":"Motion field texture synthesis","authors":"Chongyang Ma, Li-Yi Wei, Baining Guo, Kun Zhou","author_ids":"1797422, 2420851, 2738456, 6671887","abstract":"A variety of animation effects such as herds and fluids contain detailed motion fields characterized by repetitive structures. Such detailed motion fields are often visually important, but tedious to specify manually or expensive to simulate computationally. Due to the repetitive nature, some of these motion fields (e.g. turbulence in fluids) could be synthesized by procedural texturing, but procedural texturing is known for its limited generality.\n We apply example-based texture synthesis for motion fields. Our technique is general and can take on a variety of user inputs, including captured data, manual art, and physical/procedural simulation. This data-driven approach enables artistic effects that are difficult to achieve via previous methods, such as heart shaped swirls in fluid animation. Due to the use of texture synthesis, our method is able to populate a large output field from a small input exemplar, imposing minimum user workload. Our algorithm also allows the synthesis of output motion fields not only with the same dimension as the input (e.g. 2D to 2D) but also of higher dimension, such as 3D volumetric outputs from 2D planar inputs. This cross-dimension capability supports a convenient usage scenario, i.e. the user could simply supply 2D images and our method produces a 3D motion field with similar characteristics. The motion fields produced by our method are generic, and could be combined with a variety of large-scale low-resolution motions that are easy to specify either manually or computationally but lack the repetitive structures to be characterized as textures. We apply our technique to a variety of animation phenomena, including smoke, liquid, and group motion.","cites":"19","conferencePercentile":"25.69060773"},{"venue":"ACM Trans. Graph.","id":"4067a5de7f9670481986546914ba14fefdb4b5ab","venue_1":"ACM Trans. Graph.","year":"2006","title":"A planar-reflective symmetry transform for 3D shapes","authors":"Joshua Podolak, Philip Shilane, Aleksey Golovinskiy, Szymon Rusinkiewicz, Thomas A. Funkhouser","author_ids":"2590255, 1816098, 3139470, 7723706, 1807080","abstract":"Symmetry is an important cue for many applications, including object alignment, recognition, and segmentation. In this paper, we describe a planar reflective symmetry transform (PRST) that captures a continuous measure of the reflectional symmetry of a shape with respect to all possible planes. This transform combines and extends previous work that has focused on global symmetries with respect to the center of mass in 3D meshes and local symmetries with respect to points in 2D images. We provide an efficient Monte Carlo sampling algorithm for computing the transform for surfaces and show that it is stable under common transformations. We also provide an iterative refinement algorithm to find local maxima of the transform precisely. We use the transform to define two new geometric properties, center of symmetry and principal symmetry axes, and show that they are useful for aligning objects in a canonical coordinate system. Finally, we demonstrate that the symmetry transform is useful for several applications in computer graphics, including shape matching, segmentation of meshes into parts, and automatic viewpoint selection.","cites":"140","conferencePercentile":"86.11111111"},{"venue":"ACM Trans. Graph.","id":"55d2222c902a29edb75fa8e1b4c1a0fb96ad4595","venue_1":"ACM Trans. Graph.","year":"2009","title":"Invertible motion blur in video","authors":"Amit K. Agrawal, Yi Xu, Ramesh Raskar","author_ids":"1985085, 1734114, 1717566","abstract":"We show that motion blur in successive video frames is invertible even if the point-spread function (PSF) due to motion smear in a single photo is non-invertible. Blurred photos exhibit nulls (zeros) in the frequency transform of the PSF, leading to an ill-posed deconvolution. Hardware solutions to avoid this require specialized devices such as the coded exposure camera or accelerating sensor motion. We employ ordinary video cameras and introduce the notion of null-filling along with joint-invertibility of multiple blur-functions. The key idea is to record the same object with varying PSFs, so that the nulls in the frequency component of one frame can be filled by other frames. The combined frequency transform becomes null-free, making deblurring well-posed. We achieve jointly-invertible blur simply by changing the exposure time of successive frames. We address the problem of automatic deblurring of objects moving with constant velocity by solving the four critical components: preservation of all spatial frequencies, segmentation of moving parts, motion estimation of moving parts, and non-degradation of the static parts of the scene. We demonstrate several challenging cases of object motion blur including textured backgrounds and partial occluders.","cites":"34","conferencePercentile":"50.82872928"},{"venue":"ACM Trans. Graph.","id":"3186f82000e19c76c114dd4a3f764389705bb5c7","venue_1":"ACM Trans. Graph.","year":"2012","title":"Discovery of complex behaviors through contact-invariant optimization","authors":"Igor Mordatch, Emanuel Todorov, Zoran Popovic","author_ids":"2080746, 7419989, 1696595","abstract":"We present a motion synthesis framework capable of producing a wide variety of important human behaviors that have rarely been studied, including getting up from the ground, crawling, climbing, moving heavy objects, acrobatics (hand-stands in particular), and various cooperative actions involving two characters and their manipulation of the environment. Our framework is not specific to humans, but applies to characters of arbitrary morphology and limb configuration. The approach is fully automatic and does not require domain knowledge specific to each behavior. It also does not require pre-existing examples or motion capture data.\n At the core of our framework is the contact-invariant optimization (CIO) method we introduce here. It enables simultaneous optimization of contact and behavior. This is done by augmenting the search space with scalar variables that indicate whether a potential contact should be active in a given phase of the movement. These auxiliary variables affect not only the cost function but also the dynamics (by enabling and disabling contact forces), and are optimized together with the movement trajectory. Additional innovations include a continuation scheme allowing helper forces at the potential contacts rather than the torso, as well as a feature-based model of physics which is particularly well-suited to the CIO framework. We expect that CIO can also be used with a full physics model, but leave that extension for future work.","cites":"59","conferencePercentile":"94.94949495"},{"venue":"ACM Trans. Graph.","id":"0dbf8c96a548fe65c0632305ce1869084cbe103e","venue_1":"ACM Trans. Graph.","year":"2012","title":"Point sampling with general noise spectrum","authors":"Yahan Zhou, Haibin Huang, Li-Yi Wei, Rui Wang","author_ids":"2753846, 3119608, 2420851, 1699697","abstract":"Point samples with different spectral noise properties (often defined using color names such as white, blue, green, and red) are important for many science and engineering disciplines including computer graphics. While existing techniques can easily produce white and blue noise samples, relatively little is known for generating other noise patterns. In particular, no single algorithm is available to generate different noise patterns according to user-defined spectra.\n In this paper, we describe an algorithm for generating point samples that match a user-defined Fourier spectrum function. Such a spectrum function can be either obtained from a known sampling method, or completely constructed by the user. Our key idea is to convert the Fourier spectrum function into a <i>differential distribution function</i> that describes the samples' local spatial statistics; we then use a gradient descent solver to iteratively compute a sample set that matches the target differential distribution function. Our algorithm can be easily modified to achieve adaptive sampling, and we provide a GPU-based implementation. Finally, we present a variety of different sample patterns obtained using our algorithm, and demonstrate suitable applications.","cites":"19","conferencePercentile":"56.81818182"},{"venue":"ACM Trans. Graph.","id":"b754251ac66da195799f28f91358e257323c6e6c","venue_1":"ACM Trans. Graph.","year":"2004","title":"Modeling by example","authors":"Thomas A. Funkhouser, Michael M. Kazhdan, Philip Shilane, Patrick Min, William Kiefer, Ayellet Tal, Szymon Rusinkiewicz, David P. Dobkin","author_ids":"1807080, 1690653, 1816098, 2161228, 8695375, 3226509, 7723706, 1794954","abstract":"In this paper, we investigate a data-driven synthesis approach to constructing 3D geometric surface models. We provide methods with which a user can search a large database of 3D meshes to find parts of interest, cut the desired parts out of the meshes with intelligent scissoring, and composite them together in different ways to form new objects. The main benefit of this approach is that it is both easy to learn and able to produce highly detailed geometric models -- the conceptual design for new models comes from the user, while the geometric details come from examples in the database. The focus of the paper is on the main research issues motivated by the proposed approach: (1) interactive segmentation of 3D surfaces, (2) shape-based search to find 3D models with parts matching a query, and (3) composition of parts to form new models. We provide new research contributions on all three topics and incorporate them into a prototype modeling system. Experience with our prototype system indicates that it allows untrained users to create interesting and detailed 3D models.","cites":"207","conferencePercentile":"80.43478261"},{"venue":"ACM Trans. Graph.","id":"bdec8618e3a0b203e2c77d66ea3f0816ddac64d2","venue_1":"ACM Trans. Graph.","year":"2009","title":"Bokode: imperceptible visual tags for camera based interaction from a distance","authors":"Ankit Mohan, Grace Woo, Shinsaku Hiura, Quinn Smithwick, Ramesh Raskar","author_ids":"1705528, 8671435, 1915982, 8231167, 1717566","abstract":"We show a new camera based interaction solution where an ordinary camera can detect small optical tags from a relatively large distance. Current optical tags, such as barcodes, must be read within a short range and the codes occupy valuable physical space on products. We present a new low-cost optical design so that the tags can be shrunk to <i>3mm</i> visible diameter, and unmodified ordinary cameras several meters away can be set up to decode the identity plus the relative distance and angle. The design exploits the bokeh effect of ordinary cameras lenses, which maps rays exiting from an out of focus scene point into a disk like blur on the camera sensor. This bokeh-code or <i>Bokode</i> is a barcode design with a simple lenslet over the pattern. We show that a code with 15<i>&mu;m</i> features can be read using an off-the-shelf camera from distances of up to 2 meters. We use intelligent binary coding to estimate the relative distance and angle to the camera, and show potential for applications in augmented reality and motion capture. We analyze the constraints and performance of the optical system, and discuss several plausible application scenarios.","cites":"56","conferencePercentile":"76.24309392"},{"venue":"ACM Trans. Graph.","id":"3de418cadb6cb9dfd77f41ff8b27110d2c4e180b","venue_1":"ACM Trans. Graph.","year":"2004","title":"Shape matching and anisotropy","authors":"Michael M. Kazhdan, Thomas A. Funkhouser, Szymon Rusinkiewicz","author_ids":"1690653, 1807080, 7723706","abstract":"With recent improvements in methods for the acquisition and rendering of 3D models, the need for retrieval of models has gained prominence in the graphics and vision communities. A variety of methods have been proposed that enable the efficient querying of model repositories for a desired 3D shape. Many of these methods use a 3D model as a query and attempt to retrieve models from the database that have a similar shape.In this paper we consider the implications of anisotropy on the shape matching paradigm. In particular, we propose a novel method for matching 3D models that factors the shape matching equation as the disjoint outer product of anisotropy and geometric comparisons. We provide a general method for computing the factored similarity metric and show how this approach can be applied to improve the matching performance of many existing shape matching methods.","cites":"31","conferencePercentile":"15.2173913"},{"venue":"ACM Trans. Graph.","id":"a5cfa4fca27026fd828a54941487fb45558239b8","venue_1":"ACM Trans. Graph.","year":"2016","title":"Recovering shape and spatially-varying surface reflectance under unknown illumination","authors":"Rui Xia, Yue Dong, Pieter Peers, Xin Tong","author_ids":"3854934, 1744268, 1808270, 1743927","abstract":"We present a novel integrated approach for estimating both spatially-varying surface reflectance and detailed geometry from a video of a rotating object under unknown static illumination. Key to our method is the decoupling of the recovery of normal and surface reflectance from the estimation of surface geometry. We define an apparent normal field with corresponding reflectance for each point (including those not on the object's surface) that best explain the observations. We observe that the object's surface goes through points where the apparent normal field and corresponding reflectance exhibit a high degree of consistency with the observations. However, estimating the apparent normal field requires knowledge of the unknown incident lighting. We therefore formulate the recovery of shape, surface reflectance, and incident lighting, as an iterative process that alternates between estimating shape and lighting, and simultaneously recovers surface reflectance at each step. To recover the shape, we first form an initial surface that passes through locations with consistent apparent temporal traces, followed by a refinement that maximizes the consistency of the surface normals with the underlying apparent normal field. To recover the lighting, we rely on appearance-from-motion using the recovered geometry from the previous step. We demonstrate our integrated framework on a variety of synthetic and real test cases exhibiting a wide variety of materials and shape.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"42a990e548eab8985b053dc076240f6f581d434f","venue_1":"ACM Trans. Graph.","year":"2014","title":"Generalizing locomotion style to new animals with inverse optimal regression","authors":"Kevin Wampler, Zoran Popovic, Jovan Popovic","author_ids":"3013193, 1696595, 1731389","abstract":"We present a technique for analyzing a set of animal gaits to predict the gait of a new animal from its shape alone. This method works on a wide range of bipeds and quadrupeds, and adapts the motion style to the size and shape of the animal. We achieve this by combining inverse optimization with sparse data interpolation. Starting with a set of reference walking gaits extracted from sagittal plane video footage, we first use inverse optimization to learn physically motivated parameters describing the style of each of these gaits. Given a new animal, we estimate the parameters describing its gait with sparse data interpolation, then solve a forward optimization problem to synthesize the final gait. To improve the realism of the results, we introduce a novel algorithm called <i>joint inverse optimization</i> which learns coherent patterns in motion style from a database of example animal-gait pairs. We quantify the predictive performance of our model by comparing its synthesized gaits to ground truth walking motions for a range of different animals. We also apply our method to the prediction of gaits for dinosaurs and other extinct creatures.","cites":"4","conferencePercentile":"21.39917695"},{"venue":"ACM Trans. Graph.","id":"37d52ec85b35f8433d886fb2df07749c137ccac1","venue_1":"ACM Trans. Graph.","year":"2002","title":"Boom chameleon: simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display","authors":"Michael Tsang, George W. Fitzmaurice, Gordon Kurtenbach, Azam Khan, William Buxton","author_ids":"2497114, 1703735, 1708940, 1715535, 6037251","abstract":"We review the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical armature. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We created a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Results of an informal user study show that the Boom Chameleon annotation facilities have the potential to be an effective, and intuitive system for reviewing 3D designs.","cites":"74","conferencePercentile":"42"},{"venue":"ACM Trans. Graph.","id":"548e69e8e572744c80271e18918fbc5f27589af8","venue_1":"ACM Trans. Graph.","year":"2008","title":"Glare aware photography: 4D ray sampling for reducing glare effects of camera lenses","authors":"Ramesh Raskar, Amit K. Agrawal, Cyrus A. Wilson, Ashok Veeraraghavan","author_ids":"1717566, 1985085, 2812406, 1785066","abstract":"Glare arises due to multiple scattering of light inside the camera's body and lens optics and reduces image contrast. While previous approaches have analyzed glare in 2D image space, we show that glare is inherently a 4D ray-space phenomenon. By statistically analyzing the ray-space inside a camera, we can classify and remove glare artifacts. In ray-space, glare behaves as high frequency noise and can be reduced by outlier rejection. While such analysis can be performed by capturing the light field inside the camera, it results in the loss of spatial resolution. Unlike light field cameras, we do not need to reversibly encode the spatial structure of the ray-space, leading to simpler designs. We explore masks for uniform and non-uniform ray sampling and show a practical solution to analyze the 4D statistics without significantly compromising image resolution. Although diffuse scattering of the lens introduces 4D low-frequency glare, we can produce useful solutions in a variety of common scenarios. Our approach handles photography looking into the sun and photos taken without a hood, removes the effect of lens smudges and reduces loss of contrast due to camera body reflections. We show various applications in contrast enhancement and glare manipulation.","cites":"26","conferencePercentile":"27.16049383"},{"venue":"ACM Trans. Graph.","id":"107a2c807e8aacbcd9afcdeb2ddc9222ac25b15b","venue_1":"ACM Trans. Graph.","year":"2003","title":"A search engine for 3D models","authors":"Thomas A. Funkhouser, Patrick Min, Michael M. Kazhdan, Joyce Chen, J. Alex Halderman, David P. Dobkin, David Pokrass Jacobs","author_ids":"1807080, 2161228, 1690653, 5296970, 2349976, 1794954, 1946402","abstract":"As the number of 3D models available on the Web grows, there is an increasing need for a search engine to help people find them. Unfortunately, traditional text-based search techniques are not always effective for 3D data. In this article, we investigate new shape-based search methods. The key challenges are to develop query methods simple enough for novice users and matching algorithms robust enough to work for arbitrary polygonal models. We present a Web-based search engine system that supports queries based on 3D sketches, 2D sketches, 3D models, and/or text keywords. For the shape-based queries, we have developed a new matching algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations. It provides 46 to 245&percnt; better performance than related shape-matching methods during precision--recall experiments, and it is fast enough to return query results from a repository of 20,000 models in under a second. The net result is a growing interactive index of 3D models available on the Web (i.e., a Google for 3D models).","cites":"421","conferencePercentile":"97.84946237"},{"venue":"ACM Trans. Graph.","id":"251546c38552e82593aa24ac807b3c37fa6f8134","venue_1":"ACM Trans. Graph.","year":"2008","title":"Inverse texture synthesis","authors":"Li-Yi Wei, Jianwei Han, Kun Zhou, Hujun Bao, Baining Guo, Harry Shum","author_ids":"2420851, 1753976, 6671887, 1679542, 2738456, 1698102","abstract":"The quality and speed of most texture synthesis algorithms depend on a 2D input sample that is small and contains enough texture variations. However, little research exists on how to acquire such sample. For homogeneous patterns this can be achieved via manual cropping, but no adequate solution exists for inhomogeneous or <i>globally varying</i> textures, i.e. patterns that are local but not stationary, such as rusting over an iron statue with appearance conditioned on varying moisture levels.\n We present <i>inverse texture synthesis</i> to address this issue. Our inverse synthesis runs in the opposite direction with respect to traditional forward synthesis: given a large globally varying texture, our algorithm automatically produces a small texture compaction that best summarizes the original. This small compaction can be used to reconstruct the original texture or to re-synthesize new textures under user-supplied controls. More important, our technique allows real-time synthesis of globally varying textures on a GPU, where the texture memory is usually too small for large textures. We propose an optimization framework for inverse texture synthesis, ensuring that each input region is properly encoded in the output compaction. Our optimization process also automatically computes orientation fields for anisotropic textures containing both low- and high-frequency regions, a situation difficult to handle via existing techniques.","cites":"59","conferencePercentile":"68.51851852"},{"venue":"ACM Trans. Graph.","id":"60a9788c75b0a5eb4502a0a4bad4854985a58085","venue_1":"ACM Trans. Graph.","year":"2002","title":"Shape distributions","authors":"Robert Osada, Thomas A. Funkhouser, Bernard Chazelle, David P. Dobkin","author_ids":"1866481, 1807080, 1730838, 1794954","abstract":"Measuring the similarity between 3D shapes is a fundamental problem, with applications in computer graphics, computer vision, molecular biology, and a variety of other fields. A challenging aspect of this problem is to find a suitable shape signature that can be constructed and compared quickly, while still discriminating between similar and dissimilar shapes.In this paper, we propose and analyze a method for computing shape signatures for arbitrary (possibly degenerate) 3D polygonal models. The key idea is to represent the signature of an object as a <i>shape distribution</i> sampled from a <i>shape function</i> measuring global geometric properties of an object. The primary motivation for this approach is to reduce the shape matching problem to the comparison of probability distributions, which is simpler than traditional shape matching methods that require pose registration, feature correspondence, or model fitting.We find that the dissimilarities between sampled distributions of simple shape functions (e.g., the distance between two random points on a surface) provide a robust method for discriminating between classes of objects (e.g., cars versus airplanes) in a moderately sized database, despite the presence of arbitrary translations, rotations, scales, mirrors, tessellations, simplifications, and model degeneracies. They can be evaluated quickly, and thus the proposed method could be applied as a pre-classifier in a complete shape-based retrieval or analysis system concerned with finding similar whole objects. The paper describes our early experiences using shape distributions for object classification and for interactive web-based retrieval of 3D models.","cites":"457","conferencePercentile":"92"},{"venue":"ACM Trans. Graph.","id":"40350e34f07a4fe39ecd757bcde7d41a8a77d6b5","venue_1":"ACM Trans. Graph.","year":"2008","title":"Towards passive 6D reflectance field displays","authors":"Martin Fuchs, Ramesh Raskar, Hans-Peter Seidel, Hendrik P. A. Lensch","author_ids":"3100139, 1717566, 1746884, 1809190","abstract":"Traditional flat screen displays present 2D images. 3D and 4D displays have been proposed making use of lenslet arrays to shape a fixed outgoing light field for horizontal or bidirectional parallax. In this article, we present different designs of multi-dimensional displays which passively react to the light of the environment behind. The prototypes physically implement a reflectance field and generate different light fields depending on the incident illumination, for example light falling through a window. We discretize the incident light field using an optical system, and modulate it with a 2D pattern, creating a flat display which is view <i>and</i> illumination-dependent. It is free from electronic components. For distant light and a fixed observer position, we demonstrate a passive optical configuration which directly renders a 4D reflectance field in the real-world illumination behind it. We further propose an optical setup that allows for projecting out different angular distributions depending on the incident light direction. Combining multiple of these devices we build a display that renders a 6D experience, where the incident 2D illumination influences the outgoing light field, both in the spatial and in the angular domain. Possible applications of this technology are time-dependent displays driven by sunlight, object virtualization and programmable light benders / ray blockers without moving parts.","cites":"29","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"4c5ce525024abec160206c2ce49041898a57ee28","venue_1":"ACM Trans. Graph.","year":"2007","title":"Dappled photography: mask enhanced cameras for heterodyned light fields and coded aperture refocusing","authors":"Ashok Veeraraghavan, Ramesh Raskar, Amit K. Agrawal, Ankit Mohan, Jack Tumblin","author_ids":"1785066, 1717566, 1985085, 1705528, 2914934","abstract":"We describe a theoretical framework for reversibly modulating 4D light fields using an attenuating mask in the optical path of a lens based camera. Based on this framework, we present a novel design to reconstruct the 4D light field from a 2D camera image without any additional refractive elements as required by previous light field cameras. The patterned mask attenuates light rays inside the camera instead of bending them, and the attenuation recoverably encodes the rays on the 2D sensor. Our mask-equipped camera focuses just as a traditional camera to capture conventional 2D photos at full sensor resolution, but the raw pixel values also hold a modulated 4D light field. The light field can be recovered by rearranging the tiles of the 2D Fourier transform of sensor values into 4D planes, and computing the inverse Fourier transform. In addition, one can also recover the full resolution image information for the in-focus parts of the scene.\n We also show how a broadband mask placed at the lens enables us to compute refocused images at full sensor resolution for layered Lambertian scenes. This partial encoding of 4D ray-space data enables editing of image contents by depth, yet does not require computational recovery of the complete 4D light field.","cites":"241","conferencePercentile":"97.6"},{"venue":"ACM Trans. Graph.","id":"77f98942c9f78b760f5eeaa4503f662d4fb3942d","venue_1":"ACM Trans. Graph.","year":"2016","title":"Mapping virtual and physical reality","authors":"Qi Sun, Li-Yi Wei, Arie E. Kaufman","author_ids":"1718296, 2420851, 1716343","abstract":"Real walking offers higher immersive presence for virtual reality (VR) applications than alternative locomotive means such as walking-in-place and external control gadgets, but needs to take into consideration different room sizes, wall shapes, and surrounding objects in the virtual and real worlds. Despite perceptual study of impossible spaces and redirected walking, there are no general methods to match a given pair of virtual and real scenes.\n We propose a system to match a given pair of virtual and physical worlds for immersive VR navigation. We first compute a planar map between the virtual and physical floor plans that minimizes angular and distal distortions while conforming to the virtual environment goals and physical environment constraints. Our key idea is to design maps that are globally surjective to allow proper folding of large virtual scenes into smaller real scenes but locally injective to avoid locomotion ambiguity and intersecting virtual objects. From these maps we derive altered rendering to guide user navigation within the physical environment while retaining visual fidelity to the virtual environment. Our key idea is to properly warp the virtual world appearance into real world geometry with sufficient quality and performance. We evaluate our method through a formative user study, and demonstrate applications in gaming, architecture walkthrough, and medical imaging.","cites":"1","conferencePercentile":"71.72995781"},{"venue":"ACM Trans. Graph.","id":"86f1d921b4728183e1fec87e6e9581172d568766","venue_1":"ACM Trans. Graph.","year":"2015","title":"Improving light field camera sample design with irregularity and aberration","authors":"Li-Yi Wei, Chia-Kai Liang, Graham Myhre, Colvin Pitts, Kurt Akeley","author_ids":"2420851, 1753650, 3014525, 3276358, 3221156","abstract":"Conventional camera designs usually shun sample irregularities and lens aberrations. We demonstrate that such irregularities and aberrations, when properly applied, can improve the quality and usability of light field cameras. Examples include spherical aberrations for the mainlens, and misaligned sampling patterns for the microlens and photosensor elements. These observations are a natural consequence of a key difference between conventional and light field cameras: optimizing for a single captured 2D image versus a range of reprojected 2D images from a captured 4D light field. We propose designs in mainlens aberrations and microlens/photosensor sample patterns, and evaluate them through simulated measurements and captured results with our hardware prototype.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"e47c52e005fb84a2eb4633ae31dc835a2e61f0a3","venue_1":"ACM Trans. Graph.","year":"2015","title":"Vector Regression Functions for Texture Compression","authors":"Ying Song, Jiaping Wang, Li-Yi Wei, Wencheng Wang","author_ids":"2888011, 4912907, 2420851, 5358275","abstract":"Raster images are the standard format for texture mapping, but they suffer from limited resolution. Vector graphics are resolution-independent but are less general and more difficult to implement on a GPU. We propose a hybrid representation called vector regression functions (VRFs), which compactly approximate any point-sampled image and support GPU texture mapping, including random access and filtering operations. Unlike standard GPU texture compression, (VRFs) provide a variable-rate encoding in which piecewise smooth regions compress to the square root of the original size. Our key idea is to represent images using the <i>multilayer perceptron</i>, allowing general encoding via regression and efficient decoding via a simple GPU pixel shader. We also propose a content-aware spatial partitioning scheme to reduce the complexity of the neural network model. We demonstrate benefits of our method including its quality, size, and runtime speed.","cites":"0","conferencePercentile":"6.530612245"},{"venue":"ACM Trans. Graph.","id":"0cbcfbe4ae48547ec68760a796b025dc51134699","venue_1":"ACM Trans. Graph.","year":"2006","title":"Coded exposure photography: motion deblurring using fluttered shutter","authors":"Ramesh Raskar, Amit K. Agrawal, Jack Tumblin","author_ids":"1717566, 1985085, 2914934","abstract":"In a conventional single-exposure photograph, moving objects or moving cameras cause motion blur. The exposure time defines a temporal box filter that smears the moving object across the image by convolution. This box filter destroys important high-frequency spatial details so that deblurring via deconvolution becomes an ill-posed problem.Rather than leaving the shutter open for the entire exposure duration, we \"flutter\" the camera's shutter open and closed during the chosen exposure time with a binary pseudo-random sequence. The flutter changes the box filter to a broad-band filter that preserves high-frequency spatial details in the blurred image and the corresponding deconvolution becomes a well-posed problem. We demonstrate that manually-specified point spread functions are sufficient for several challenging cases of motion-blur removal including extremely large motions, textured backgrounds and partial occluders.","cites":"218","conferencePercentile":"97.22222222"},{"venue":"ACM Trans. Graph.","id":"4ddc794383125705dbc02581704d11c552c459e1","venue_1":"ACM Trans. Graph.","year":"2015","title":"Structure and appearance optimization for controllable shape design","authors":"Jonàs Martínez, Jérémie Dumas, Sylvain Lefebvre, Li-Yi Wei","author_ids":"2772264, 2062459, 2757631, 2420851","abstract":"The field of topology optimization seeks to optimize shapes under structural objectives, such as achieving the most rigid shape using a given quantity of material. Besides optimal shape design, these methods are increasingly popular as design tools, since they automatically produce structures having desirable physical properties, a task hard to perform by hand even for skilled designers. However, there is no simple way to control the appearance of the generated objects.\n In this paper, we propose to optimize shapes for <i>both</i> their structural properties <i>and</i> their appearance, the latter being controlled by a user-provided pattern example. These two objectives are challenging to combine, as optimal structural properties fully define the shape, leaving no degrees of freedom for appearance. We propose a new formulation where appearance is optimized as an objective while structural properties serve as <i>constraints.</i> This produces shapes with sufficient rigidity while allowing enough freedom for the appearance of the final structure to resemble the input exemplar.\n Our approach generates rigid shapes using a specified quantity of material while observing optional constraints such as voids, fills, attachment points, and external forces. The appearance is defined by examples, making our technique accessible to casual users. We demonstrate its use in the context of fabrication using a laser cutter to manufacture real objects from optimized shapes.","cites":"4","conferencePercentile":"54.28571429"},{"venue":"ACM Trans. Graph.","id":"bb0a3bef10530f370f4b554fb3fa74466708c18d","venue_1":"ACM Trans. Graph.","year":"2014","title":"Capturing braided hairstyles","authors":"Liwen Hu, Chongyang Ma, Linjie Luo, Li-Yi Wei, Hao Li","author_ids":"1808579, 1797422, 1702459, 2420851, 1706574","abstract":"From fishtail to princess braids, these intricately woven structures define an important and popular class of hairstyle, frequently used for digital characters in computer graphics. In addition to the challenges created by the infinite range of styles, existing modeling and capture techniques are particularly constrained by the geometric and topological complexities. We propose a data-driven method to automatically reconstruct braided hairstyles from input data obtained from a single consumer RGB-D camera. Our approach covers the large variation of repetitive braid structures using a family of compact procedural braid models. From these models, we produce a database of braid patches and use a robust random sampling approach for data fitting. We then recover the input braid structures using a multi-label optimization algorithm and synthesize the intertwining hair strands of the braids. We demonstrate that a minimal capture equipment is sufficient to effectively capture a wide range of complex braids with distinct shapes and structures.","cites":"5","conferencePercentile":"29.62962963"},{"venue":"ACM Trans. Graph.","id":"c332ee278a637bcebe97856ecc1f70e092c8dd87","venue_1":"ACM Trans. Graph.","year":"2015","title":"Aerophones in flatland: interactive wave simulation of wind instruments","authors":"Andrew Allen, Nikunj Raghuvanshi","author_ids":"6897346, 3032886","abstract":"We present the first real-time technique to synthesize full-bandwidth sounds for 2D virtual wind instruments. A novel interactive wave solver is proposed that synthesizes audio at 128,000Hz on commodity graphics cards. Simulating the wave equation captures the resonant and radiative properties of the instrument body automatically. We show that a variety of existing non-linear excitation mechanisms such as reed or lips can be successfully coupled to the instrument's 2D wave field. Virtual musical performances can be created by mapping user inputs to control geometric features of the instrument body, such as tone holes, and modifying parameters of the excitation model, such as blowing pressure. Field visualizations are also produced. Our technique promotes experimentation by providing instant audio-visual feedback from interactive virtual designs. To allow artifact-free audio despite dynamic geometric modification, we present a novel time-varying Perfectly Matched Layer formulation that yields smooth, natural-sounding transitions between notes. We find that visco-thermal wall losses are crucial for musical sound in 2D simulations and propose a practical approximation. Weak non-linearity at high amplitudes is incorporated to improve the sound quality of brass instruments.","cites":"3","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"5a0f8f5f104c165e60fc8971540c29ac060cb6a2","venue_1":"ACM Trans. Graph.","year":"2013","title":"Bilateral blue noise sampling","authors":"Jiating Chen, Xiaoyin Ge, Li-Yi Wei, Bin Wang, Yusu Wang, Huamin Wang, Yun Fei, Kang-Lai Qian, Jun-Hai Yong, Wenping Wang","author_ids":"1993869, 1873651, 2420851, 1726439, 1768281, 6798421, 2547088, 2460420, 1724669, 1698520","abstract":"Blue noise sampling is an important component in many graphics applications, but existing techniques consider mainly the spatial positions of samples, making them less effective when handling problems with non-spatial features. Examples include biological distribution in which plant spacing is influenced by non-positional factors such as tree type and size, photon mapping in which photon flux and direction are not a direct function of the attached surface, and point cloud sampling in which the underlying surface is unknown a priori. These scenarios can benefit from blue noise sample distributions, but cannot be adequately handled by prior art.\n Inspired by bilateral filtering, we propose a bilateral blue noise sampling strategy. Our key idea is a general formulation to modulate the traditional sample distance measures, which are determined by sample position in spatial domain, with a similarity measure that considers arbitrary per sample attributes. This modulation leads to the notion of <i>bilateral</i> blue noise whose properties are influenced by not only the uniformity of the sample positions but also the similarity of the sample attributes. We describe how to incorporate our modulation into various sample analysis and synthesis methods, and demonstrate applications in object distribution, photon density estimation, and point cloud sub-sampling.","cites":"11","conferencePercentile":"41.40271493"},{"venue":"ACM Trans. Graph.","id":"0b73203177ac06b61970836f2209003694caea8c","venue_1":"ACM Trans. Graph.","year":"2008","title":"Shield fields: modeling and capturing 3D occluders","authors":"Douglas Lanman, Ramesh Raskar, Amit K. Agrawal, Gabriel Taubin","author_ids":"3235339, 1717566, 1985085, 1690237","abstract":"We describe a unified representation of occluders in light transport and photography using shield fields: the 4D attenuation function which acts on any light field incident on an occluder. Our key theoretical result is that shield fields can be used to decouple the effects of occluders and incident illumination. We first describe the properties of shield fields in the frequency-domain and briefly analyze the \"forward\" problem of efficiently computing cast shadows. Afterwards, we apply the shield field signal-processing framework to make several new observations regarding the \"inverse\" problem of reconstructing 3D occluders from cast shadows -- extending previous work on shape-from-silhouette and visual hull methods. From this analysis we develop the first single-camera, single-shot approach to capture visual hulls without requiring moving or programmable illumination. We analyze several competing camera designs, ultimately leading to the development of a new large-format, mask-based light field camera that exploits optimal tiled-broadband codes for light-efficient shield field capture. We conclude by presenting a detailed experimental analysis of shield field capture and 3D occluder reconstruction.","cites":"46","conferencePercentile":"57.40740741"},{"venue":"ACM Trans. Graph.","id":"3a50a701ac879836c834130a173435e42ca30b56","venue_1":"ACM Trans. Graph.","year":"2010","title":"Multi-class blue noise sampling","authors":"Li-Yi Wei","author_ids":"2420851","abstract":"Sampling is a core process for a variety of graphics applications. Among existing sampling methods, blue noise sampling remains popular thanks to its spatial uniformity and absence of aliasing artifacts. However, research so far has been mainly focused on blue noise sampling with a single class of samples. This could be insufficient for common natural as well as man-made phenomena requiring multiple classes of samples, such as object placement, imaging sensors, and stippling patterns.\n We extend blue noise sampling to multiple classes where each individual class as well as their unions exhibit blue noise characteristics. We propose two flavors of algorithms to generate such multi-class blue noise samples, one extended from traditional Poisson <i>hard</i> disk sampling for explicit control of sample spacing, and another based on our <i>soft</i> disk sampling for explicit control of sample count. Our algorithms support uniform and adaptive sampling, and are applicable to both discrete and continuous sample space in arbitrary dimensions. We study characteristics of samples generated by our methods, and demonstrate applications in object placement, sensor layout, and color stippling.","cites":"34","conferencePercentile":"60.81871345"},{"venue":"ACM Trans. Graph.","id":"b07bf1741fa5ad0e8952097491ae39c7f7ab9f75","venue_1":"ACM Trans. Graph.","year":"2011","title":"Differential domain analysis for non-uniform sampling","authors":"Li-Yi Wei, Rui Wang","author_ids":"2420851, 1699697","abstract":"Sampling is a core component for many graphics applications including rendering, imaging, animation, and geometry processing. The efficacy of these applications often crucially depends upon the distribution quality of the underlying samples. While uniform sampling can be analyzed by using existing spatial and spectral methods, these cannot be easily extended to general non-uniform settings, such as adaptive, anisotropic, or non-Euclidean domains.\n We present new methods for analyzing non-uniform sample distributions. Our key insight is that standard Fourier analysis, which depends on samples' spatial locations, can be reformulated into an equivalent form that depends only on the distribution of their location <i>differentials</i>. We call this differential domain analysis. The main benefit of this reformulation is that it bridges the fundamental connection between the samples' spatial statistics and their spectral properties. In addition, it allows us to generalize our method with different computation kernels and differential measurements. Using this analysis, we can quantitatively measure the spatial and spectral properties of various non-uniform sample distributions, including adaptive, anisotropic, and non-Euclidean domains.","cites":"31","conferencePercentile":"67.10526316"},{"venue":"ACM Trans. Graph.","id":"16321d4fba1dca79dcc79d535e2ca984c437776a","venue_1":"ACM Trans. Graph.","year":"2010","title":"Biharmonic distance","authors":"Yaron Lipman, Raif M. Rustamov, Thomas A. Funkhouser","author_ids":"3232072, 2559604, 1807080","abstract":"Measuring distances between pairs of points on a 3D surface is a fundamental problem in computer graphics and geometric processing. For most applications, the important properties of a distance are that it is a metric, smooth, locally isotropic, globally &#8220;shape-aware,&#8221; isometry-invariant, insensitive to noise and small topology changes, parameter-free, and practical to compute on a discrete mesh. However, the basic methods currently popular in computer graphics (e.g., geodesic and diffusion distances) do not have these basic properties. In this article, we propose a new distance measure based on the biharmonic differential operator that has all the desired properties. This new surface distance is related to the diffusion and commute-time distances, but applies different (inverse squared) weighting to the eigenvalues of the Laplace-Beltrami operator, which provides a nice trade-off between nearly geodesic distances for small distances and global shape-awareness for large distances. The article provides theoretical and empirical analysis for a large number of meshes.","cites":"33","conferencePercentile":"58.47953216"},{"venue":"ACM Trans. Graph.","id":"2cbca66da4e5ff06c4ace2f5b60ad82c3dcc080e","venue_1":"ACM Trans. Graph.","year":"2015","title":"SecondSkin: sketch-based construction of layered 3D models","authors":"Chris De Paoli, Karan Singh","author_ids":"2193668, 1682205","abstract":"<i>SecondSkin</i> is a sketch-based modeling system focused on the creation of structures comprised of layered, shape interdependent 3D volumes. Our approach is built on three novel insights gleaned from an analysis of representative artist sketches. First, we observe that a closed loop of strokes typically define surface patches that bound volumes in conjunction with underlying surfaces. Second, a significant majority of these strokes map to a small set of curve-types, that describe the 3D geometric relationship between the stroke and underlying layer geometry. Third, we find that a few simple geometric features allow us to consistently classify 2D strokes to our proposed set of 3D curve-types. Our algorithm thus processes strokes as they are drawn, identifies their curve-type, and interprets them as 3D curves <i>on</i> and <i>around</i> underlying 3D geometry, using other connected 3D curves for context. Curve loops are automatically surfaced and turned into volumes bound to the underlying layer, creating additional curves and surfaces as necessary. Stroke classification by 15 viewers on a suite of ground truth sketches validates our curve-types and classification algorithm. We evaluate SecondSkin via a compelling gallery of layered 3D models that would be tedious to produce using current sketch modelers.","cites":"5","conferencePercentile":"65.10204082"},{"venue":"ACM Trans. Graph.","id":"4406a7730f960494761c0e20158e3681e3c388bd","venue_1":"ACM Trans. Graph.","year":"2016","title":"Mesh denoising via cascaded normal regression","authors":"Peng-Shuai Wang, Yang Liu, Xin Tong","author_ids":"2666565, 1750084, 1743927","abstract":"We present a data-driven approach for mesh denoising. Our key idea is to formulate the denoising process with cascaded non-linear regression functions and learn them from a set of noisy meshes and their ground-truth counterparts. Each regression function infers the normal of a denoised output mesh facet from geometry features extracted from its neighborhood facets on the input mesh and sends the result as the input of the next regression function. Specifically, we develop a <i>filtered facet normal descriptor (FND)</i> for modeling the geometry features around each facet on the noisy mesh and model a regression function with neural networks for mapping the FNDs to the facet normals of the denoised mesh. To handle meshes with different geometry features and reduce the training difficulty, we cluster the input mesh facets according to their FNDs and train neural networks for each cluster separately in an offline learning stage. At runtime, our method applies the learned cascaded regression functions to a noisy input mesh and reconstructs the denoised mesh from the output facet normals.\n Our method learns the non-linear denoising process from the training data and makes no specific assumptions about the noise distribution and geometry features in the input. The runtime denoising process is fully automatic for different input meshes. Our method can be easily adapted to meshes with arbitrary noise patterns by training a dedicated regression scheme with mesh data and the particular noise pattern. We evaluate our method on meshes with both synthetic and real scanned noise, and compare it to other mesh denoising algorithms. Results demonstrate that our method outperforms the state-of-the-art mesh denoising methods and successfully removes different kinds of noise for meshes with various geometry features.","cites":"0","conferencePercentile":"30.37974684"},{"venue":"ACM Trans. Graph.","id":"3fa2d2d40c917646b3502452a76924d46cc1f86e","venue_1":"ACM Trans. Graph.","year":"2012","title":"Schelling points on 3D surface meshes","authors":"Xiaobai Chen, Abulhair Saparov, Bill Pang, Thomas A. Funkhouser","author_ids":"1943104, 2407368, 3180415, 1807080","abstract":"This paper investigates \"Schelling points\" on 3D meshes, feature points selected by people in a pure coordination game due to their salience. To collect data for this investigation, we designed an online experiment that asked people to select points on 3D surfaces that they expect will be selected by other people. We then analyzed properties of the selected points, finding that: 1) Schelling point sets are usually highly symmetric, and 2) local curvature properties (e.g., Gauss curvature) are most helpful for identifying obvious Schelling points (tips of protrusions), but 3) global properties (e.g., segment centeredness, proximity to a symmetry axis, etc.) are required to explain more subtle features. Based on these observations, we use regression analysis to combine multiple properties into an analytical model that predicts where Schelling points are likely to be on new meshes. We find that this model benefits from a variety of surface properties, particularly when training data comes from examples in the same object class.","cites":"30","conferencePercentile":"77.27272727"},{"venue":"ACM Trans. Graph.","id":"3db75d30d6388067c21af0f7b90bee55fe16c862","venue_1":"ACM Trans. Graph.","year":"2012","title":"Simple formulas for quasiconformal plane deformations","authors":"Yaron Lipman, Vladimir G. Kim, Thomas A. Funkhouser","author_ids":"3232072, 3082383, 1807080","abstract":"We introduce a simple formula for 4-point planar warping that produces provably good 2D deformations. In contrast to previous work, the new deformations minimize the <i>maximum</i> conformal distortion and spread the distortion equally across the domain. We derive closed-form formulas for computing the 4-point interpolant and analyze its properties. We further explore applications to 2D shape deformations by building local deformation operators that use thin-plate splines to further deform the 4-point interpolant to satisfy certain boundary conditions. Although this modification no longer has any theoretical guarantees, we demonstrate that, practically, these local operators can be used to create compound deformations with fewer control points and smaller worst-case distortions in comparisons to the state-of-the-art.","cites":"11","conferencePercentile":"25.25252525"},{"venue":"ACM Trans. Graph.","id":"4be05fcdd13f0b62454c1aa3f1141e83ba980ca7","venue_1":"ACM Trans. Graph.","year":"2012","title":"Symmetry-guided texture synthesis and manipulation","authors":"Vladimir G. Kim, Yaron Lipman, Thomas A. Funkhouser","author_ids":"3082383, 3232072, 1807080","abstract":"This article presents a framework for symmetry-guided texture synthesis and processing. It is motivated by the long-standing problem of how to optimize, transfer, and control the spatial patterns in textures. The key idea is that symmetry representations that measure autocorrelations with respect to all transformations of a group are a natural way to describe spatial patterns in many real-world textures. To leverage this idea, we provide methods to transfer symmetry representations from one texture to another, process the symmetries of a texture, and optimize textures with respect to properties of their symmetry representations. These methods are automatic and robust, as they don't require explicit detection of discrete symmetries. Applications are investigated for optimizing, processing, and transferring symmetries and textures.","cites":"10","conferencePercentile":"19.94949495"},{"venue":"ACM Trans. Graph.","id":"3d80513f60ea7bf8679208befb16ed375f8040f6","venue_1":"ACM Trans. Graph.","year":"1995","title":"Further Experience with Controller-Based Automatic Motion Synthesis for Articulated Figures","authors":"Joel Auslander, Alex S. Fukunaga, Hadi Partovi, Jon Christensen, Lloyd Hsu, Peter Reiss, Andrew Shuman, Joe Marks, J. Thomas Ngo","author_ids":"2809263, 3129677, 1775979, 3048778, 2748313, 7993382, 7776938, 1822613, 1738039","abstract":"We extend an earlier automatic motion-synthesis algorithm for physically realistic articulated  figures in several ways. First, we summarize several incremental improvements to the original algorithm that improve its efficiency significantly and provide the user with some ability to influence what motions are generated. These techniques can be used by an animator to achieve a desired movement style, or they can be used to guarantee variety in the motions synthesized over several runs of the algorithm. Second, we report on new mechanisms that support the concatenation of existing, automatically generated motion controllers to produce complex, composite movement. Finally, we describe initial work on generalizing the techniques from 2D to 3D articulated figures. Taken together, these   results illustrate the promise and challenges afforded by the controller-based approach to automatic motion synthesis for computer animation.","cites":"31","conferencePercentile":"42.85714286"},{"venue":"ACM Trans. Graph.","id":"0877253582d6a6f8d6279528a8fc04c2e83f4713","venue_1":"ACM Trans. Graph.","year":"2011","title":"Blended intrinsic maps","authors":"Vladimir G. Kim, Yaron Lipman, Thomas A. Funkhouser","author_ids":"3082383, 3232072, 1807080","abstract":"This paper describes a fully automatic pipeline for finding an intrinsic map between two non-isometric, genus zero surfaces. Our approach is based on the observation that efficient methods exist to search for nearly isometric maps (e.g., M&#246;bius Voting or Heat Kernel Maps), but no single solution found with these methods provides low-distortion everywhere for pairs of surfaces differing by large deformations. To address this problem, we suggest using a weighted combination of these maps to produce a \"blended map.\" This approach enables algorithms that leverage efficient search procedures, yet can provide the flexibility to handle large deformations.\n The main challenges of this approach lie in finding a set of candidate maps {<i>m</i><sub><i>i</i></sub>} and their associated blending weights {<i>b</i><sub><i>i</i></sub>(<i>p</i>)} for every point <i>p</i> on the surface. We address these challenges specifically for conformal maps by making the following contributions. First, we provide a way to blend maps, defining the image of <i>p</i> as the weighted geodesic centroid of <i>m</i><sub><i>i</i></sub>(<i>p</i>). Second, we provide a definition for smooth blending weights at every point <i>p</i> that are proportional to the area preservation of <i>m</i><sub><i>i</i></sub> at <i>p</i>. Third, we solve a global optimization problem that selects candidate maps based both on their area preservation and consistency with other selected maps. During experiments with these methods, we find that our algorithm produces blended maps that align semantic features better than alternative approaches over a variety of data sets.","cites":"107","conferencePercentile":"97.36842105"},{"venue":"ACM Trans. Graph.","id":"0fb0b6dbf2a5271b60f7eb837797617b13438801","venue_1":"ACM Trans. Graph.","year":"1995","title":"An Empirical Study of Algorithms for Point-Feature Label Placement","authors":"Jon Christensen, Joe Marks, Stuart M. Shieber","author_ids":"3048778, 1822613, 1692491","abstract":"A major factor affecting the clarity of graphical displays that include text labels is the degree to which labels obscure display features (including other labels) as a result of spatial overlap. Point-feature label placement (PFLP) is the problem of placing text labels adjacent to point features on a map or diagram so as to maximize legibility. This problem occurs frequently in the production of many types of informational graphics, though it arises most often in automated cartography. In this paper we present a comprehensive treatment of the PFLP problem, viewed as a type of combinatorial optimization problem. Complexity analysis reveals that the basic PFLP problem and most interesting variants of it are NP-hard. These negative results help inform a survey of previously reported algorithms for PFLP; not surprisingly, all such algorithms either have exponential time complexity or are incomplete. To solve the PFLP problem in practice, then, we must rely on good heuristic methods. We propose two new methods, one based on a discrete form of gradient descent, the other on simulated annealing, and report on a series of empirical tests comparing these and the other known algorithms for the problem. Based on this study, the first to be conducted, we identify the best approaches as a function of available computation time.","cites":"154","conferencePercentile":"100"},{"venue":"ACM Trans. Graph.","id":"03225b9d7ecaee02c6953d841dbad113406c0af0","venue_1":"ACM Trans. Graph.","year":"2011","title":"Polarization fields: dynamic light field display using multi-layer LCDs","authors":"Douglas Lanman, Gordon Wetzstein, Matthew Hirsch, Wolfgang Heidrich, Ramesh Raskar","author_ids":"3235339, 1731170, 2309851, 1752192, 1717566","abstract":"We introduce polarization field displays as an optically-efficient design for dynamic light field display using multi-layered LCDs. Such displays consist of a stacked set of liquid crystal panels with a single pair of crossed linear polarizers. Each layer is modeled as a spatially-controllable polarization rotator, as opposed to a conventional spatial light modulator that directly attenuates light. Color display is achieved using field sequential color illumination with monochromatic LCDs, mitigating severe attenuation and moir&#233; occurring with layered color filter arrays. We demonstrate such displays can be controlled, at interactive refresh rates, by adopting the SART algorithm to tomographically solve for the optimal spatially-varying polarization state rotations applied by each layer. We validate our design by constructing a prototype using modified off-the-shelf panels. We demonstrate interactive display using a GPU-based SART implementation supporting both polarization-based and attenuation-based architectures. Experiments characterize the accuracy of our image formation model, verifying polarization field displays achieve increased brightness, higher resolution, and extended depth of field, as compared to existing automultiscopic display methods for dual-layer and multi-layer LCDs.","cites":"46","conferencePercentile":"82.36842105"},{"venue":"ACM Trans. Graph.","id":"58c01e6425f4068ab875994451dd39a2d37baab5","venue_1":"ACM Trans. Graph.","year":"2010","title":"Symmetry factored embedding and distance","authors":"Yaron Lipman, Xiaobai Chen, Ingrid Daubechies, Thomas A. Funkhouser","author_ids":"3232072, 1943104, 1737063, 1807080","abstract":"We introduce the Symmetry Factored Embedding (SFE) and the Symmetry Factored Distance (SFD) as new tools to analyze and represent symmetries in a point set. The SFE provides new coordinates in which symmetry is \"factored out,\" and the SFD is the Euclidean distance in that space. These constructions characterize the space of symmetric correspondences between points -- i.e., orbits. A key observation is that a set of points in the same orbit appears as a clique in a correspondence graph induced by pairwise similarities. As a result, the problem of finding approximate and partial symmetries in a point set reduces to the problem of measuring connectedness in the correspondence graph, a well-studied problem for which spectral methods provide a robust solution. We provide methods for computing the SFE and SFD for extrinsic global symmetries and then extend them to consider partial extrinsic and intrinsic cases. During experiments with difficult examples, we find that the proposed methods can characterize symmetries in inputs with noise, missing data, non-rigid deformations, and complex symmetries, without a priori knowledge of the symmetry group. As such, we believe that it provides a useful tool for automatic shape analysis in applications such as segmentation and stationary point detection.","cites":"40","conferencePercentile":"69.29824561"},{"venue":"ACM Trans. Graph.","id":"5b818b2867856ebce3fd03baa6c7c6c9cba97af5","venue_1":"ACM Trans. Graph.","year":"2011","title":"CATRA: interactive measuring and modeling of cataracts","authors":"Vitor F. Pamplona, Erick Baptista Passos, Jan Zizka, Manuel Menezes de Oliveira Neto, Everett Lawson, Esteban Walter Gonzalez Clua, Ramesh Raskar","author_ids":"1742358, 2491342, 2973652, 2856168, 3068263, 2214579, 1717566","abstract":"We introduce an interactive method to assess cataracts in the human eye by crafting an optical solution that measures the perceptual impact of forward scattering on the foveal region. Current solutions rely on highly-trained clinicians to check the back scattering in the crystallin lens and test their predictions on visual acuity tests. Close-range parallax barriers create collimated beams of light to scan through sub-apertures, scattering light as it strikes a cataract. User feedback generates maps for opacity, attenuation, contrast and sub-aperture point-spread functions. The goal is to allow a general audience to operate a portable high-contrast light-field display to gain a meaningful understanding of their own visual conditions. User evaluations and validation with modified camera optics are performed. Compiled data is used to reconstruct the individual's cataract-affected view, offering a novel approach for capturing information for screening, diagnostic, and clinical analysis.","cites":"9","conferencePercentile":"15.78947368"},{"venue":"ACM Trans. Graph.","id":"abae30eaa0378a4cdaabc04b0c9ab6d522da7ff7","venue_1":"ACM Trans. Graph.","year":"2008","title":"A perceptually validated model for surface depth hallucination","authors":"Mashhuda Glencross, Gregory J. Ward, Francho Melendez, Caroline Jay, Jun Liu, Roger J. Hubbold","author_ids":"2502700, 1876626, 1832537, 1728418, 1843598, 2878103","abstract":"Capturing detailed surface geometry currently requires specialized equipment such as laser range scanners, which despite their high accuracy, leave gaps in the surfaces that must be reconciled with photographic capture for relighting applications. Using only a standard digital camera and a single view, we present a method for recovering models of predominantly diffuse textured surfaces that can be plausibly relit and viewed from any angle under any illumination. Our multiscale shape-from-shading technique uses diffuse-lit/flash-lit image pairs to produce an albedo map and textured height field. Using two lighting conditions enables us to subtract one from the other to estimate albedo. In the absence of a flash-lit image of a surface for which we already have a similar exemplar pair, we approximate both albedo and diffuse shading images using histogram matching. Our depth estimation is based on local visibility. Unlike other depth-from-shading approaches, all operations are performed on the diffuse shading image in image space, and we impose no constant albedo restrictions. An experimental validation shows our method works for a broad range of textured surfaces, and viewers are frequently unable to identify our results as synthetic in a randomized presentation. Furthermore, in side-by-side comparisons, subjects found a rendering of our depth map equally plausible to one generated from a laser range scan. We see this method as a significant advance in acquiring surface detail for texturing using a standard digital camera, with applications in architecture, archaeological reconstruction, games and special effects.","cites":"17","conferencePercentile":"15.43209877"},{"venue":"ACM Trans. Graph.","id":"4ab134629c2ea7d706fef22844e871897e60e7dd","venue_1":"ACM Trans. Graph.","year":"2010","title":"Parallel Poisson disk sampling with spectrum analysis on surfaces","authors":"John C. Bowers, Rui Wang, Li-Yi Wei, David Maletz","author_ids":"2959665, 1699697, 2420851, 3296658","abstract":"The ability to place surface samples with Poisson disk distribution can benefit a variety of graphics applications. Such a distribution satisfies the blue noise property, i.e. lack of low frequency noise and structural bias in the Fourier power spectrum. While many techniques are available for sampling the plane, challenges remain for sampling arbitrary surfaces. In this paper, we present new methods for Poisson disk sampling with spectrum analysis on arbitrary manifold surfaces. Our first contribution is a parallel dart throwing algorithm that generates high-quality surface samples at interactive rates. It is flexible and can be extended to adaptive sampling given a user-specified radius field. Our second contribution is a new method for analyzing the spectral quality of surface samples. Using the spectral mesh basis derived from the discrete mesh Laplacian operator, we extend standard concepts in power spectrum analysis such as radial means and anisotropy to arbitrary manifold surfaces. This provides a way to directly evaluate the spectral distribution quality of surface samples without requiring mesh parameterization. Finally, we implement our Poisson disk sampling algorithm on the GPU, and demonstrate practical applications involving interactive sampling and texturing on arbitrary surfaces.","cites":"48","conferencePercentile":"82.16374269"},{"venue":"ACM Trans. Graph.","id":"71e8a222208ef47c32fccba2dfe26049bb2e7122","venue_1":"ACM Trans. Graph.","year":"2010","title":"Multi-feature matching of fresco fragments","authors":"Corey Toler-Franklin, Benedict J. Brown, Tim Weyrich, Thomas A. Funkhouser, Szymon Rusinkiewicz","author_ids":"2966788, 2142291, 1784306, 1807080, 7723706","abstract":"We present a multiple-feature approach for determining matches between small fragments of archaeological artifacts such as Bronze-Age and Roman frescoes. In contrast with traditional 2D and 3D shape matching approaches, we introduce a set of feature descriptors that are based on not only color and shape, but also <i>normal maps.</i> These are easy to acquire and combine high data quality with discriminability and robustness to some types of deterioration. Our feature descriptors range from general-purpose to domain-specific, and are quick to compute and match. We have tested our system on three datasets of fresco fragments, demonstrating that multi-cue matching using different subsets of features leads to different tradeoffs between efficiency and effectiveness. In particular, we show that normal-based features are more effective than color-based ones at similar computational complexity, and that 3D features are more discriminative than ones based on 2D or normals, but at higher computational cost. We also demonstrate how machine learning techniques can be used to effectively combine our new features with traditional ones. Our results show good retrieval performance, significantly improving upon the match prediction rate of state-of-the-art 3D matching algorithms, and are expected to extend to general matching problems in applications such as texture synthesis and forensics.","cites":"19","conferencePercentile":"30.99415205"},{"venue":"ACM Trans. Graph.","id":"6c1af1be4972551840f7c5787817c40e75ecbd60","venue_1":"ACM Trans. Graph.","year":"2008","title":"Parallel Poisson disk sampling","authors":"Li-Yi Wei","author_ids":"2420851","abstract":"Sampling is important for a variety of graphics applications include rendering, imaging, and geometry processing. However, producing sample sets with desired efficiency and blue noise statistics has been a major challenge, as existing methods are either sequential with limited speed, or are parallel but only through pre-computed datasets and thus fall short in producing samples with blue noise statistics. We present a Poisson disk sampling algorithm that runs in parallel and produces all samples on the fly with desired blue noise properties. Our main idea is to subdivide the sample domain into grid cells and we draw samples concurrently from multiple cells that are sufficiently far apart so that their samples cannot conflict one another. We present a parallel implementation of our algorithm running on a GPU with constant cost per sample and constant number of computation passes for a target number of samples. Our algorithm also works in arbitrary dimension, and allows adaptive sampling from a user-specified importance field. Furthermore, our algorithm is simple and easy to implement, and runs faster than existing techniques.","cites":"61","conferencePercentile":"70.98765432"},{"venue":"ACM Trans. Graph.","id":"0658d4494e7c72b927d5f07f3f484235703ea68f","venue_1":"ACM Trans. Graph.","year":"2009","title":"Analytic drawing of 3D scaffolds","authors":"Ryan Schmidt, Azam Khan, Karan Singh, Gordon Kurtenbach","author_ids":"2291899, 1715535, 1682205, 1708940","abstract":"We describe a novel approach to inferring 3D curves from perspective drawings in an interactive design tool. Our methods are based on a traditional design drawing style known as <i>analytic drawing</i>, which supports precise image-space construction of a linear 3D scaffold. This scaffold in turn acts as a set of visual constraints for sketching 3D curves. We implement analytic drawing techniques in a pure-inference sketching interface which supports both single-and multi-view incremental construction of complex scaffolds and curve networks. A new representation of 3D drawings is proposed, and useful interactive drawing aids are described. Novel techniques are presented for deriving constraints from single-view sketches drawn relative to the current 3D scaffold, and then inferring 3D line and curve geometry which satisfies these constraints. The resulting analytic drawing tool allows 3D drawings to be constructed using exactly the same strokes as one would make on paper.","cites":"50","conferencePercentile":"72.37569061"},{"venue":"ACM Trans. Graph.","id":"131af2a87749aa08bfabc7efd5d8c53e9945e4d5","venue_1":"ACM Trans. Graph.","year":"2010","title":"NETRA: interactive display for estimating refractive errors and focal range","authors":"Vitor F. Pamplona, Ankit Mohan, Manuel Menezes de Oliveira Neto, Ramesh Raskar","author_ids":"1742358, 1705528, 2856168, 1717566","abstract":"We introduce an interactive, portable, and inexpensive solution for estimating refractive errors in the human eye. While expensive optical devices for automatic estimation of refractive correction exist, our goal is to greatly simplify the mechanism by putting the human subject in the loop. Our solution is based on a high-resolution programmable display and combines inexpensive optical elements, interactive GUI, and computational reconstruction. The key idea is to interface a lenticular view-dependent display with the human eye in <i>close range</i> - a few millimeters apart. Via this platform, we create a new range of interactivity that is extremely sensitive to parameters of the human eye, like refractive errors, focal range, focusing speed, lens opacity, etc. We propose several simple optical setups, verify their accuracy, precision, and validate them in a user study.","cites":"22","conferencePercentile":"38.59649123"},{"venue":"ACM Trans. Graph.","id":"6b99f024c1ccceac0c74e428a9d2028c12ac6e07","venue_1":"ACM Trans. Graph.","year":"2008","title":"Magnets in motion","authors":"Bernhard Thomaszewski, Andreas Gumann, Simon Pabst, Wolfgang Straßer","author_ids":"1784345, 1822669, 2074759, 1697038","abstract":"We introduce magnetic interaction for rigid body simulation. Our approach is based on an equivalent dipole method and as such it is discrete from the ground up. Our approach is symmetric as we base both field and force computations on dipole interactions. Enriching rigid body simulation with magnetism allows for many new and interesting possibilities in computer animation and special effects. Our method also allows the accurate computation of magnetic fields for arbitrarily shaped objects, which is especially interesting for pedagogy as it allows the user to visually discover properties of magnetism which would otherwise be difficult to grasp. We demonstrate our method on a variety of problems and our results reflect intuitive as well as surprising effects. Our method is fast and can be coupled with any rigid body solver to simulate dozens of magnetic objects at interactive rates.","cites":"5","conferencePercentile":"2.469135802"},{"venue":"ACM Trans. Graph.","id":"addd38b5023ebbb353edbb6ccae3bc0f291a597b","venue_1":"ACM Trans. Graph.","year":"2009","title":"Self-organizing tree models for image synthesis","authors":"Wojciech Palubicki, Kipp Horel, Steven Longay, Adam Runions, Brendan Lane, Radomír Mech, Przemyslaw Prusinkiewicz","author_ids":"2776595, 3019425, 2233615, 1754049, 1783476, 2008027, 1725594","abstract":"We present a method for generating realistic models of temperate-climate trees and shrubs. This method is based on the biological hypothesis that the form of a developing tree emerges from a self-organizing process dominated by the competition of buds and branches for light or space, and regulated by internal signaling mechanisms. Simulations of this process robustly generate a wide range of realistic trees and bushes. The generated forms can be controlled with a variety of interactive techniques, including procedural brushes, sketching, and editing operations such as pruning and bending of branches. We illustrate the usefulness and versatility of the proposed method with diverse tree models, forest scenes, animations of tree development, and examples of combined interactive-procedural tree modeling.","cites":"38","conferencePercentile":"57.73480663"},{"venue":"ACM Trans. Graph.","id":"32d165c84f94a49a0fb54e08b374ae116a428c63","venue_1":"ACM Trans. Graph.","year":"2008","title":"Randomized cuts for 3D mesh analysis","authors":"Aleksey Golovinskiy, Thomas A. Funkhouser","author_ids":"3139470, 1807080","abstract":"The goal of this paper is to investigate a new shape analysis method based on randomized cuts of 3D surface meshes. The general strategy is to generate a random set of mesh segmentations and then to measure how often each edge of the mesh lies on a segmentation boundary in the randomized set. The resulting \"partition function\" defined on edges provides a continuous measure of where natural part boundaries occur in a mesh, and the set of \"most consistent cuts\" provides a stable list of global shape features. The paper describes methods for generating random distributions of mesh segmentations, studies sensitivity of the resulting partition functions to noise, tessellation, pose, and intra-class shape variations, and investigates applications in mesh visualization, segmentation, deformation, and registration.","cites":"76","conferencePercentile":"79.9382716"},{"venue":"ACM Trans. Graph.","id":"452b22ca4a391bd551d69ed68b56c1184609b9a1","venue_1":"ACM Trans. Graph.","year":"2010","title":"Axial-cones: modeling spherical catadioptric cameras for wide-angle light field rendering","authors":"Yuichi Taguchi, Amit K. Agrawal, Ashok Veeraraghavan, Srikumar Ramalingam, Ramesh Raskar","author_ids":"2246577, 1985085, 1785066, 1699414, 1717566","abstract":"Catadioptric imaging systems are commonly used for wide-angle imaging, but lead to multi-perspective images which do not allow algorithms designed for perspective cameras to be used. Efficient use of such systems requires accurate geometric ray modeling as well as fast algorithms. We present accurate geometric modeling of the multi-perspective photo captured with a spherical catadioptric imaging system using <i>axial-cone cameras:</i> multiple perspective cameras lying on an axis each with a different viewpoint and a different cone of rays. This modeling avoids geometric approximations and allows several algorithms developed for perspective cameras to be applied to multi-perspective catadioptric cameras.\n We demonstrate axial-cone modeling in the context of rendering wide-angle light fields, captured using a spherical mirror array. We present several applications such as spherical distortion correction, digital refocusing for artistic depth of field effects in wide-angle scenes, and wide-angle dense depth estimation. Our GPU implementation using axial-cone modeling achieves up to three orders of magnitude speed up over ray tracing for these applications.","cites":"21","conferencePercentile":"36.5497076"},{"venue":"ACM Trans. Graph.","id":"3837ea73f19f3c9792101ec1a30cf95670e1708e","venue_1":"ACM Trans. Graph.","year":"2010","title":"Content-adaptive parallax barriers: optimizing dual-layer 3D displays using low-rank light field factorization","authors":"Douglas Lanman, Matthew Hirsch, Yunhee Kim, Ramesh Raskar","author_ids":"3235339, 2309851, 7652498, 1717566","abstract":"We optimize automultiscopic displays built by stacking a pair of modified LCD panels. To date, such dual-stacked LCDs have used heuristic parallax barriers for view-dependent imagery: the front LCD shows a fixed array of slits or pinholes, independent of the multi-view content. While prior works adapt the spacing between slits or pinholes, depending on viewer position, we show both layers can also be adapted to the multi-view content, increasing brightness and refresh rate. Unlike conventional barriers, both masks are allowed to exhibit non-binary opacities. It is shown that any 4D light field emitted by a dual-stacked LCD is the tensor product of two 2D masks. Thus, any pair of 1D masks only achieves a rank-1 approximation of a 2D light field. Temporal multiplexing of masks is shown to achieve higher-rank approximations. Non-negative matrix factorization (NMF) minimizes the weighted Euclidean distance between a target light field and that emitted by the display. Simulations and experiments characterize the resulting <i>content-adaptive parallax barriers</i> for low-rank light field approximation.","cites":"49","conferencePercentile":"84.21052632"},{"venue":"ACM Trans. Graph.","id":"74491c03f0385b21e5e16b8f2639ac9b558448aa","venue_1":"ACM Trans. Graph.","year":"2009","title":"How well do line drawings depict shape?","authors":"Forrester Cole, Kevin Sanik, Douglas DeCarlo, Adam Finkelstein, Thomas A. Funkhouser, Szymon Rusinkiewicz, Manish Singh","author_ids":"1805136, 2808416, 2476753, 1707541, 1807080, 7723706, 4459183","abstract":"This paper investigates the ability of sparse line drawings to depict 3D shape. We perform a study in which people are shown an image of one of twelve 3D objects depicted with one of six styles and asked to orient a gauge to coincide with the surface normal at many positions on the object's surface. The normal estimates are compared with each other and with ground truth data provided by a registered 3D surface model to analyze accuracy and precision. The paper describes the design decisions made in collecting a large data set (275,000 gauge measurements) and provides analysis to answer questions about how well people interpret shapes from drawings. Our findings suggest that people interpret certain shapes almost as well from a line drawing as from a shaded image, that current computer graphics line drawing techniques can effectively depict shape and even match the effectiveness of artist's drawings, and that errors in depiction are often localized and can be traced to particular properties of the lines used. The data collected for this study will become a publicly available resource for further studies of this type.","cites":"78","conferencePercentile":"89.77900552"},{"venue":"ACM Trans. Graph.","id":"07ac55614b0fd552f214511680ef4dbcbff12e4b","venue_1":"ACM Trans. Graph.","year":"2011","title":"Highlighted depth-of-field photography: Shining light on focus","authors":"Jaewon Kim, Roarke Horstmeyer, Ig-Jae Kim, Ramesh Raskar","author_ids":"5815624, 3223699, 1740140, 1717566","abstract":"We present a photographic method to enhance intensity differences between objects at varying distances from the focal plane. By combining a unique capture procedure with simple image processing techniques, the detected brightness of an object is decreased proportional to its degree of defocus. A camera-projector system casts distinct grid patterns onto a scene to generate a spatial distribution of point reflections. These point reflections relay a relative measure of defocus that is utilized in postprocessing to generate a highlighted DOF photograph. Trade-offs between three different projector-processing pairs are analyzed, and a model is developed to help describe a new intensity-dependent depth of field that is controlled by the pattern of illumination. Results are presented for a primary single snapshot design as well as a scanning method and a comparison method. As an application, automatic matting results are presented.","cites":"1","conferencePercentile":"1.578947368"},{"venue":"ACM Trans. Graph.","id":"0722077581460d8f060a96956a79c782d4fca57c","venue_1":"ACM Trans. Graph.","year":"2012","title":"Continuous character control with low-dimensional embeddings","authors":"Sergey Levine, Jack M. Wang, Alexis Haraux, Zoran Popovic, Vladlen Koltun","author_ids":"1736651, 1876447, 2065726, 1696595, 1770944","abstract":"Interactive, task-guided character controllers must be agile and responsive to user input, while retaining the flexibility to be readily authored and modified by the designer. Central to a method's ease of use is its capacity to synthesize character motion for novel situations without requiring excessive data or programming effort. In this work, we present a technique that animates characters performing user-specified tasks by using a probabilistic motion model, which is trained on a small number of artist-provided animation clips. The method uses a low-dimensional space learned from the example motions to continuously control the character's pose to accomplish the desired task. By controlling the character through a reduced space, our method can discover new transitions, tractably precompute a control policy, and avoid low quality poses.","cites":"25","conferencePercentile":"72.47474747"},{"venue":"ACM Trans. Graph.","id":"c8f97df0005417bbe9a972fb559c107f062938ef","venue_1":"ACM Trans. Graph.","year":"2011","title":"Single view reflectance capture using multiplexed scattering and time-of-flight imaging","authors":"Nikhil Naik, Shuang Zhao, Andreas Velten, Ramesh Raskar, Kavita Bala","author_ids":"3285462, 2373908, 2253695, 1717566, 8261370","abstract":"This paper introduces the concept of time-of-flight reflectance estimation, and demonstrates a new technique that allows a camera to rapidly acquire reflectance properties of objects from a single view-point, over relatively long distances and without encircling equipment. We measure material properties by indirectly illuminating an object by a laser source, and observing its reflected light indirectly using a time-of-flight camera. The configuration collectively acquires dense angular, but low spatial sampling, within a limited solid angle range - all from a single viewpoint. Our ultra-fast imaging approach captures space-time \"streak images\" that can separate out different bounces of light based on path length. Entanglements arise in the streak images mixing signals from multiple paths if they have the same total path length. We show how reflectances can be recovered by solving for a linear system of equations and assuming parametric material models; fitting to lower dimensional reflectance models enables us to disentangle measurements.\n We demonstrate proof-of-concept results of parametric reflectance models for homogeneous and discretized heterogeneous patches, both using simulation and experimental hardware. As compared to lengthy or highly calibrated BRDF acquisition techniques, we demonstrate a device that can rapidly, on the order of seconds, capture meaningful reflectance information. We expect hardware advances to improve the portability and speed of this device.","cites":"37","conferencePercentile":"76.31578947"},{"venue":"ACM Trans. Graph.","id":"0cf2fa36135344d1be1549a2e1502b005d44c920","venue_1":"ACM Trans. Graph.","year":"2009","title":"A benchmark for 3D mesh segmentation","authors":"Xiaobai Chen, Aleksey Golovinskiy, Thomas A. Funkhouser","author_ids":"1943104, 3139470, 1807080","abstract":"This paper describes a benchmark for evaluation of 3D mesh segmentation salgorithms. The benchmark comprises a data set with 4,300 manually generated segmentations for 380 surface meshes of 19 different object categories, and it includes software for analyzing 11 geometric properties of segmentations and producing 4 quantitative metrics for comparison of segmentations. The paper investigates the design decisions made in building the benchmark, analyzes properties of human-generated and computer-generated segmentations, and provides quantitative comparisons of 7 recently published mesh segmentation algorithms. Our results suggest that people are remarkably consistent in the way that they segment most 3D surface meshes, that no one automatic segmentation algorithm is better than the others for all types of objects, and that algorithms based on non-local shape features seem to produce segmentations that most closely resemble ones made by humans.","cites":"201","conferencePercentile":"98.34254144"},{"venue":"ACM Trans. Graph.","id":"9f120b102f3c5c286fab60da73230c118e4d2fcd","venue_1":"ACM Trans. Graph.","year":"2009","title":"Möbius voting for surface correspondence","authors":"Yaron Lipman, Thomas A. Funkhouser","author_ids":"3232072, 1807080","abstract":"The goal of our work is to develop an efficient, automatic algorithm for discovering point correspondences between surfaces that are approximately and/or partially isometric. Our approach is based on three observations. First, isometries are a subset of the Möbius group, which has low-dimensionality – six degrees of freedom for topological spheres, and three for topolog-ical discs. Second, computing the Möbius transformation that interpolates any three points can be computed in closed-form after a mid-edge flattening to the complex plane. Third, deviations from isometry can be modeled by a transportation-type distance between corresponding points in that plane. Motivated by these observations, we have developed a Möbius Voting algorithm that iteratively: 1) samples a triplet of three random points from each of two point sets, 2) uses the Möbius transformations defined by those triplets to map both point sets into a canoni-cal coordinate frame on the complex plane, and 3) produces \" votes \" for predicted correspondences between the mutually closest points with magnitude representing their estimated deviation from isom-etry. The result of this process is a fuzzy correspondence matrix, which is converted to a permutation matrix with simple matrix operations and output as a discrete set of point correspondences with confidence values. The main advantage of this algorithm is that it can find intrinsic point correspondences in cases of extreme deformation. During experiments with a variety of data sets, we find that it is able to find dozens of point correspondences between different object types in different poses fully automatically.","cites":"155","conferencePercentile":"97.23756906"}]}