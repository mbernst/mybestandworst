{"SC_Companion.csv":[{"venue":"SC Companion","id":"34bfa8053c0e108c51d96b2aa1cd0c1cee60ffa9","venue_1":"SC Companion","year":"2012","title":"Abstract: Bringing Task and Data Parallelism to Analysis of Climate Model Output","authors":"Robert L. Jacob, Jayesh Krishna, Xiabing Xu, Sheri A. Mickelson, Timothy J. Tautges, Mike Wilde, Robert Latham, Ian T. Foster, Robert B. Ross, Mark Hereld, Jay Walter Larson, Pavel B. Bochev, Kara Peterson, Mark A. Taylor, Karen Schuchardt, Jain Yin, Don Middleton, Mary Haley, David Brown, Wei Huang, Dennis G. Shea, Richard Brownrigg, Mariana Vertenstein, Kwan-Liu Ma, Jingrong Xie","author_ids":"1995804, 2620320, 2704761, 1790074, 3233175, 1756158, 1692762, 1698701, 1737737, 3102192, 2168672, 7632780, 2186528, 1760905, 3170113, 2049325, 2247697, 8565095, 5414537, 1730584, 2595446, 2376935, 1804879, 1707383, 2743846","abstract":"— Climate models are both outputting larger and larger amounts of data and are doing it on more sophisticated numerical grids. The tools climate scientists have used to analyze climate output, an essential component of climate modeling, are single threaded and assume rectangular structured grids in their analysis algorithms. We are bringing both task-and data-parallelism to the analysis of climate model output. We have created a new data-parallel library, the Parallel Gridded Analysis Library (ParGAL) which can read in data using parallel I/O, store the data on a compete representation of the structured or unstructured mesh and perform sophisticated analysis on the data in parallel. ParGAL has been used to create a parallel version of a script-based analysis and visualization package. Finally, we have also taken current workflows and employed task-based parallelism to decrease the total execution time. I. INTRODUCTION As climate model's reach the petascale, they are outputting more and more data on structured and unstructured grids [1]. Analysis of this output is essential to determining what the model is saying about the climate system. The programs currently used to perform this analysis are often not nearly as flexible or high-performing as the primary applications. They either break or require workarounds for the ultra-large unstructured-grid data that is becoming the norm in climate science [2]. Ultra-large data sets also present a memory challenge because often these programs cannot read all the data in to memory. Programs such as Parallel-R [3] provide data-parallel versions of some of its statistical analysis functions. However it does not support operations on a grid. Tools such as GLEAN [4] or DIY [5] provide facilities for data staging and movement in an HPC environment but not the discretization-aware data model we need. In this poster, we describe how to speed up climate data post processing by using a new parallel gridded analysis library and a parallel scripting language.","cites":"0","conferencePercentile":"12.63157895"},{"venue":"SC Companion","id":"7e27a89d857d9063377b01afe92483d2fbc479dd","venue_1":"SC Companion","year":"2012","title":"Abstract: Scalable Fast Multipole Methods for Vortex Element Methods","authors":"Qi Hu, Nail A. Gumerov, Rio Yokota, Lorena A. Barba, Ramani Duraiswami","author_ids":"8163916, 1697880, 2274654, 1731975, 1719541","abstract":"We use a particle-based method to simulate incompressible flows, where the Fast Multipole Method (FMM) is used to accelerate the calculation of particle interactions. The most time-consuming kernels—the Biot-Savart equation and stretching term of the vorticity equation—are mathematically reformulated so that only two Laplace scalar potentials are used instead of six, while automatically ensuring divergence-free far-field computation. Based on this formulation, and on our previous work for a scalar heterogeneous FMM algorithm, we develop a new FMM-based vortex method capable of simulating general flows including turbulence on heterogeneous architectures. Our work for this poster focuses on the computation perspective and our implementation can perform one time step of the velocity+stretching for one billion particles on 32 nodes in 55.9 seconds, which yields 49.12 Tflop/s. I. PROBLEM FORMULATION According to Helmholtz's laws the vortex elements move with the local velocity of the fluid.","cites":"0","conferencePercentile":"12.63157895"},{"venue":"SC Companion","id":"11f04533943d56e6e8e4235839650c10758d8afd","venue_1":"SC Companion","year":"2012","title":"An Approach to Protect the Privacy of Cloud Data from Data Mining Based Attacks","authors":"Himel Dev, Tanmoy Sen, Madhusudan Basak, Mohammed Eunus Ali","author_ids":"1890113, 3086078, 3267192, 2532195","abstract":"—Cloud computing has revolutionized the way computing and software services are delivered to the clients on demand. It offers users the ability to connect to computing resources and access IT managed services with a previously unknown level of ease. Due to this greater level of flexibility, the cloud has become the breeding ground of a new generation of products and services. However, the flexibility of cloud-based services comes with the risk of the security and privacy of users' data. Thus, security concerns among users of the cloud have become a major barrier to the widespread growth of cloud computing. One of the security concerns of cloud is data mining based privacy attacks that involve analyzing data over a long period to extract valuable information. In particular, in current cloud architecture a client entrusts a single cloud provider with his data. It gives the provider and outside attackers having unauthorized access to cloud, an opportunity of analyzing client data over a long period to extract sensitive information that causes privacy violation of clients. This is a big concern for many clients of cloud. In this paper, we first identify the data mining based privacy risks on cloud data and propose a distributed architecture to eliminate the risks.","cites":"5","conferencePercentile":"76.31578947"},{"venue":"SC Companion","id":"ee9fd193fa1d086e1ef7c7bc052acf620e2bc2c6","venue_1":"SC Companion","year":"2012","title":"Enabling In Situ Pre- and Post-processing for Exascale Hemodynamic Simulations - A Co-design Study with the Sparse Geometry Lattice-Boltzmann Code HemeLB","authors":"Fang Chen, Markus Flatken, Achim Basermann, Andreas Gerndt, James Hetherington, Timm Krüger, Gregor Matura, Rupert W. Nash","author_ids":"3991227, 3061647, 2244755, 1840256, 8071003, 2203057, 3484551, 1956915","abstract":"—Today's fluid simulations deal with complex geome-tries and numerical data on an extreme scale. As computation approaches the exascale, it will no longer be possible to write and store the full-sized data set. In situ data analysis and scientific visualisation provide feasible solutions to the analysis of complex large scaled CFD simulations. To bring pre-and post-processing to the exascale we must consider modifications to data structure and memory layout, and address latency and error resiliency. In this respect, a particular challenge is the exascale data processing for the sparse geometry lattice-Boltzmann code HemeLB, intended for hemodynamic simulations. In this paper, we assess the needs and challenges of HemeLB users and sketch a co-design infrastructure and system architecture for pre-and post-processing the simulation data. To enable in situ data visualisation and analysis during a running simulation, post-processing needs to work on a reduced subset of the original data. Particular choices of data structure and visualisation techniques need to be co-designed with the application scientists in order to achieve efficient and interactive data processing and analysis. In this work, we focus on the hierarchical data structure and suitable visualisation techniques which provide possible solutions to interactive in situ data processing at exascale. Architectural challenges and road-maps will be presented as the major focus of this paper. We sketch a software architecture which integrates pre-and post-processing techniques that can provide in situ analysis and ultimately computational steering to HemeLB.","cites":"1","conferencePercentile":"34.73684211"},{"venue":"SC Companion","id":"627650c2929371a208f94ac7b1a782ba818b4414","venue_1":"SC Companion","year":"2012","title":"FRIEDA: Flexible Robust Intelligent Elastic Data Management in Cloud Environments","authors":"Devarshi Ghoshal, Lavanya Ramakrishnan","author_ids":"2176038, 1792683","abstract":"—Scientific applications are increasingly using cloud resources for their data analysis workflows. However, managing data effectively and efficiently over these cloud resources is challenging due to the myriad storage choices with different performance, cost trade-offs, complex application choices and complexity associated with elasticity, failure rates in these environments. The explosion in scientific data coupled with unique characteristics of cloud environments require a more flexible and robust distributed data management solution than the ones currently in existence. This paper describes the design and implementation of FRIEDA-a Flexible Robust Intelligent Elastic Data Management framework. FRIEDA coordinates data in a transient cloud environment taking into account specific application characteristics. Additionally, we describe a range of data management strategies and show the benefit of flexible data management approaches in cloud environments. We study two distinct scientific applications from bioinformatics and light source image analysis to understand the effectiveness of such a framework.","cites":"6","conferencePercentile":"82.10526316"},{"venue":"SC Companion","id":"c968ab668b672bd84fcb3c9a994f45779ccd4b44","venue_1":"SC Companion","year":"2012","title":"Flexible Analysis Software for Emerging Architectures","authors":"Kenneth Moreland, Brad King, Robert Maynard, Kwan-Liu Ma","author_ids":"1715133, 1770509, 3321994, 1707383","abstract":"—We are on the threshold of a transformative change in the basic architecture of high-performance computing. The use of accelerator processors, characterized by large core counts, shared but asymmetrical memory, and heavy thread loading, is quickly becoming the norm in high performance computing. These accelerators represent significant challenges in updating our existing base of software. An intrinsic problem with this transition is a fundamental programming shift from message passing processes to much more fine thread scheduling with memory sharing. Another problem is the lack of stability in accelerator implementation; processor and compiler technology is currently changing rapidly. In this paper we describe our approach to address these two immediate problems with respect to scientific analysis and visualization algorithms. Our approach to accelerator programming forms the basis of the Dax toolkit, a framework to build data analysis and visualization algorithms applicable to exascale computing.","cites":"2","conferencePercentile":"51.05263158"},{"venue":"SC Companion","id":"e03420ca622e7b4022a09281834a353b4cc17fcf","venue_1":"SC Companion","year":"2011","title":"Electronic poster: visualizing multiscale simulation data","authors":"Joseph A. Insley, Leopold Grinberg, Michael E. Papka, George E. Karniadakis","author_ids":"8651217, 2422405, 1857295, 1720124","abstract":"Accurately modeling many physical and biological systems requires simulating at multiple scales. This results in large and heterogeneous data sets on vastly differing scales, both physical and temporal. Here we look specifically at blood flow in a patient-specific cerebrovasculature with a brain aneurysm, and analyze the interaction of platelets with the arterial walls that lead to thrombus formation.","cites":"0","conferencePercentile":"32.63888889"},{"venue":"SC Companion","id":"2c052cb28826e775509b02ed281c0815a798631b","venue_1":"SC Companion","year":"2011","title":"Blood flow: multi-scale modeling and visualization","authors":"Joseph A. Insley, Leopold Grinberg, Dmitry A. Fedosov, Vitali A. Morozov, Bruce Caswell, Michael E. Papka, George E. Karniadakis","author_ids":"8651217, 2422405, 1879122, 2120088, 3009226, 1857295, 1720124","abstract":"Multi-scale modeling of arterial blood flow can shed light on the interaction between events happening at micro- and meso-scales (i.e., adhesion of red blood cells to the arterial wall, clot formation) and at macro-scales (i.e., change in flow patterns due to the clot). Coupled numerical simulations of such multi-scale flow require state-of-the-art computers and algorithms, along with techniques for multi-scale visualizations.\n This animation presents results of two studies used in the development of a multi-scale visualization methodology. The first illustrates a flow of healthy (red) and diseased (blue) blood cells with a Dissipative Particle Dynamics (DPD) method. Each blood cell is represented by a mesh made of 500 DPD-particles, and small spheres show a sub-set of the DPD particles representing the blood plasma, while instantaneous streamlines and slices represent the ensemble average velocity. In the second we investigate the process of thrombus (blood clot) formation, which may be responsible for the rupture of aneurysms, by concentrating on the platelet blood cells, observing as they aggregate on the wall of an aneurysm. The ability to use a single integrated tool for the visualization of this multi-scale simulation data is important to understanding the effects of the large-scale flow patterns on the detailed particle behavior.","cites":"0","conferencePercentile":"32.63888889"},{"venue":"SC Companion","id":"3f7d1e5dca45d0f2b181c8a524f78072c79d35d0","venue_1":"SC Companion","year":"2011","title":"Modeling early galaxies using radiation hydrodynamics","authors":"Joseph A. Insley, Rick Wagner, Robert Harkness, Daniel R. Reynolds, Michael L. Norman, Mark Hereld, Eric C. Olson, Michael E. Papka, Venkatram Vishwanath","author_ids":"8651217, 2428852, 3063657, 2472023, 7868127, 3102192, 3166804, 1857295, 3348747","abstract":"This simulation uses a flux-limited diffusion solver to explore the radiation hydrodynamics of early galaxies, in particular, the ionizing radiation created by Population III stars. At the time of this rendering, the simulation has evolved to a redshift of 3.5. The simulation volume is 11.2 comoving megaparsecs, and has a uniform grid of 1024<sup>3</sup> cells, with over 1 billion dark matter and star particles.\n This animation shows a combined view of the baryon density, dark matter density, radiation energy and emissivity from this simulation. The multi-variate rendering is particularly useful because is shows both the baryonic matter (\"normal\") and dark matter, and the pressure and temperature variables are properties of only the baryonic matter. Visible in the gas density are \"bubbles\", or shells, created by the radiation feedback from young stars. Seeing the bubbles from feedback provides confirmation of the physics model implemented. Features such as these are difficult to identify algorithmically, but easily found when viewing the visualization","cites":"0","conferencePercentile":"32.63888889"},{"venue":"SC Companion","id":"196a92d8d1ac59ad1bfd0f8bd3c0fb9ae715289e","venue_1":"SC Companion","year":"2011","title":"Poster: scalable infrastructure to support supercomputer resiliency-aware applications and load balancing","authors":"Yoav Tock, Benjamin Mandler, José E. Moreira, Terry Jones","author_ids":"3220004, 2800036, 1689481, 5085925","abstract":"High performance computing systems display increasing complexity and component counts. This trend exposes weaknesses in the underlying clustering infrastructure needed for continuous availability, maximizing utilization, and efficient administration of such systems. To mitigate the problem, we present a highly scalable clustering infrastructure, based on peer-to-peer technologies, for supporting resiliency-aware applications as well as efficient monitoring and load balancing. Supported services include Membership, Publish-subscribe messaging, Convergecast, Attribute replication and a DHT. We present a preliminary evaluation taken from an IBM BlueGene/P, demonstrating scalability up to ~ 256K nodes.","cites":"2","conferencePercentile":"90.27777778"},{"venue":"SC Companion","id":"67331c5847fe4a0459f76f46627eb558ad8c67ca","venue_1":"SC Companion","year":"2011","title":"Electronic poster: co-visualization of full data and in situ data extracts from unstructured grid cfd at 160k cores","authors":"Michel E. Rasquin, Patrick Marion, Venkatram Vishwanath, Benjamin A. Matthews, Mark Hereld, Kenneth E. Jansen, Raymond M. Loy, Andrew C. Bauer, Min Zhou, Onkar Sahni, Jing Fu, Ning Liu, Christopher D. Carothers, Mark S. Shephard, Michael E. Papka, Kalyan Kumaran, Berk Geveci","author_ids":"2582996, 3249940, 3348747, 2399408, 3102192, 1774599, 2780659, 2310096, 3227818, 1799893, 8505807, 1680152, 1759102, 1741564, 1857295, 3084082, 2104667","abstract":"Scalability and time-to-solution studies have historically been focused on the size of the problem and run time. We consider a more strict definition of \"solution\" whereby a live data analysis (co-visualization of either the full data or in situ data extracts) provides continuous and reconfigurable insight into massively parallel simulations. Specifically, we used the Argonne Leadership Class Facility's (ALCF) BlueGene/P machine with 163,840 cores tightly linked through a high-speed network to 100 visualization nodes that share 800 cores and 200 GPUs. Three meshes with respectively 52M, 416M and 3.3B elements discretize the flow over a full swept wing with an unsteady synthetic jet to evaluate time-to-solution plus insight. On the full machine, the 416M element mesh takes about 2 seconds per flow solve step including the extraction and rendering of a slice or a contour, slowing currently the simulation by only 10 and 15% respectively. The 3.3B element case proved scalable at about 15 seconds per time step, whereas PHASTA's strong scaling could compress the time-to-solution for the 52M element case enough to allow the rendering of one frame (slice or contour) every 0.7 second, paving the way for interactive simulation and simulation steering on massively parallel systems<sup>1</sup>.","cites":"4","conferencePercentile":"97.22222222"},{"venue":"SC Companion","id":"26d60704581467029431819dda9a0bee79251e2f","venue_1":"SC Companion","year":"2012","title":"Community Accessible Datastore of High-Throughput Calculations: Experiences from the Materials Project","authors":"Dan Gunter, Shreyas Cholia, Anubhav Jain, Michael Kocher, Kristin A. Persson, Lavanya Ramakrishnan, Shyue Ping Ong, Gerbrand Ceder","author_ids":"8716222, 1918932, 6061444, 3264056, 2867828, 1792683, 2381325, 2807088","abstract":"—Efforts such as the Human Genome Project provided a dramatic example of opening scientific datasets to the community. Making high quality scientific data accessible through an online database allows scientists around the world to multiply the value of that data through scientific innovations. Similarly, the goal of the Materials Project is to calculate physical properties of all known inorganic materials and make this data freely available, with the goal of accelerating to invention of better materials. However, the complexity of scientific data, and the complexity of the simulations needed to generate and analyze it, pose challenges to current software ecosystem. In this paper, we describe the approach we used in the Materials Project to overcome these challenges and create and disseminate a high quality database of materials properties computed by solving the basic laws of physics. Our infrastructure requires a novel combination of high-throughput approaches with broadly applicable and scalable approaches to data storage and dissemination. I. INTRODUCTION Materials discovery and development is a key innovation driver for new technologies and markets, and an essential part of the drive to a renewable energy future. Yet, historically, novel materials exploration has been slow and expensive, taking on average 18 years from concept to commercializa-tion. [8] To address this challenge, the US government has created the US Materials Genome Initiative (MGI) [18], which aims to \" double the speed with which we discover, develop, and manufacture new materials \". The central component of the MGI approach is using our ability to accurately model nature through computer simulations at unprecedented scale. It is now well established, through several demonstrated examples [11], that many materials properties can be predicted by computing accurate approximate solutions to the basic laws of physics, and that this virtual testing of materials can be used to design and optimize materials in silico. By applying the power of many-task computing [26] on increasingly powerful computational platforms, materials designers, both theorists and experimen-talists, can scan through thousands of possible new materials across a wide range of chemistries. The Materials Project (MP) [19], part of and in fact a progenitor of the MGI, is providing a community accessible","cites":"3","conferencePercentile":"62.10526316"},{"venue":"SC Companion","id":"978684305cb452fb355bf5d69a89f920d490c273","venue_1":"SC Companion","year":"2011","title":"Poster: FLAMBES: evolving fast performance models","authors":"Adam Crume, Carlos Maltzahn, Jason Cope, Sam Lang, Robert B. Ross, Philip H. Carns, Christopher D. Carothers, Ning Liu, Curtis Janssen, John Bent, Stephan Eidenbenz, Meghan McClelland","author_ids":"2581298, 3198700, 2318401, 3079452, 1737737, 2797656, 1759102, 1680152, 2798440, 1747597, 1937685, 3133195","abstract":"Large clusters and supercomputers are simulated to aid in design. Many devices, such as hard drives, are slow to simulate. Our approach is to use a genetic algorithm to fit parameters for an analytical model of a device. Fitting focuses on aggregate accuracy rather than request-level accuracy since individual request times are irrelevant in large simulations. The model is fitted to traces from a physical device or a known device-accurate model. This is done once, offline, before running the simulation. Execution of the model is fast, since it only requires a modest amount of floating point math and no event queueing. Only a few floating point numbers are needed for state. Compared to an event-driven model, this trades a little accuracy for a large gain in performance.","cites":"0","conferencePercentile":"32.63888889"},{"venue":"SC Companion","id":"6fbd6ecc601e303de6ef7b1272bbb337863ef834","venue_1":"SC Companion","year":"2012","title":"Scalable Visual Queries for Data Exploration on Large, High-Resolution 3D Displays","authors":"Khairi Reda, Andrew E. Johnson, Victor A. Mateevitsi, Catherine Offord, Jason Leigh","author_ids":"1800051, 4415091, 1908213, 2278104, 1792379","abstract":"—As the scale and complexity of data continue to grow at unprecedented rates, scientists are increasingly relying on Large, High-Resolution Displays to visualize and analyze scientific datasets. Recent studies have demonstrated the effectiveness of these displays in supporting cognitively demanding data analysis and sensemaking tasks. While there has been an abundance of research on rendering algorithms for large, high-resolution displays, far less effort has gone into designing interactive visual analytic interfaces to effectively leverage these displays in visual exploration and sensemaking scenarios involving large collections of data. In this paper, we present an interactive visual analytics application for the exploration of large trajectory datasets. Our application utilizes large, high-resolution 3D display environments to simultaneously visualize and juxtapose a large number of trajectories. It also integrates a scalable visual query technique, which can be used to quickly formulate and verify hypotheses, encouraging scientists to contemplate multiple competing theories before drawing conclusions. We evaluate our design within the context of a behavioral ecology case study. We also share our observations from a pilot user study to provide insights on how scientists might utilize large display environments in visual exploration and sensemaking scenarios.","cites":"6","conferencePercentile":"82.10526316"},{"venue":"SC Companion","id":"a73a85f46ea6bce0cf40046df466a9716a7f64d2","venue_1":"SC Companion","year":"2011","title":"Magnetic field outflows from active galactic nuclei","authors":"David Pugmire, Paul Sutter, Paul Ricker, Hsiang-Yi Yang, George Foreman","author_ids":"2193258, 2815716, 3938631, 2966679, 2797359","abstract":"We examine several models of injecting magnetic fields into clusters of galaxies from active galactic nuclei, which are the powerful outflows associated with supermassive black holes in the centers of clusters. Shown are magnetic field lines after six billion years of evolution.","cites":"0","conferencePercentile":"32.63888889"},{"venue":"SC Companion","id":"b62e8d2d76cb40483c443e9e0ba90a149182c196","venue_1":"SC Companion","year":"2011","title":"Poster: fast GPU read alignment with burrows wheeler transform based index","authors":"Aleksandr Drozd, Naoya Maruyama, Satoshi Matsuoka","author_ids":"2817839, 3264280, 1696166","abstract":"We address the problem of performing faster read alignment on GPU devices. The task of DNA sequence processing is extremely computationally intensive as constant progress in sequencing technology leads to ever-increasing amounts of sequence data[6]. One of possible solutions for this problem is to use the extreme parallel capacities of modern GPU devices[5]. However, performance characteristics and programming models for GPU differ from those of traditional architectures and require new approaches.\n Most importantly, host memory and I/O systems are not directly accessible from a GPU device and GPU memory is usually an order of magnitude smaller than memory on a host. Considering the size of read alignment data, the memory limit becomes a real problem: when reference sequence index does not fit into memory it has to be split into chunks that will be processed individually. In most cases the complexity of the algorithm does not depend on the index size, so such index splitting increases computation time tremendously. Analysis of existing solutions for read alignment on GPU showed that memory limit is the chief performance issue. One of the attempts to reduce memory consumption consisted in replacing commonly used suffix tree, which allows for better theoretical performance of the algorithm [4], with suffix array, which is less efficient in terms of pure computational complexity but more compact. By doing this, authors of MummerGPU++ achieved several times better performance[3].\n We suggest using Burrows-Wheeler Transform[1] for both index and the corresponding search algorithm to achieve much smaller memory footprint. This transform is used mainly in compression algorithms such as bzip2 as it replaces reoccurring patterns in the string by continuous runs of a single symbol, but it can be also used for pattern matching[2]. At the same time we continue using more traditional suffix array on host side to benefit from computational characteristics of both GPU and CPU. We reduced index size 12 times and just by doing this achieved 3-4 time performance improvement compared to suffix-array based solution MummerGPU++.\n Since even with this compressed index workload size can exceed available device memory we developed a performance model to analyze how overall execution time is affected by proportions and succession in memory is allocated for chunks of index and query set. This model allowed us to find best balance of memory allocation and double performance compared to naive approach when we allocate equal shares of memory for index and queries. The model is then applied to show that using multiple GPUs is a way not only to speed up application, but also to overcome some single-GPU performance issues and have super-linear scaling at least on number of GPUs typically available on one host.","cites":"1","conferencePercentile":"75"}]}