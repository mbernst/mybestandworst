{"SIGDIAL_Workshop.csv":[{"venue":"SIGDIAL Workshop","id":"9c3585c3bc28e5dd40d6dc6e50ea3b4aae782d34","venue_1":"SIGDIAL Workshop","year":"2008","title":"A Framework for Building Conversational Agents Based on a Multi-Expert Model","authors":"Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hiroshi Tsujino","author_ids":"1734868, 1747395, 1975185, 2206851","abstract":"This paper presents a novel framework for building symbol-level control modules of animated agents and robots having a spoken dialogue interface. It features distributed modules called experts each of which is specialized to perform certain kinds of tasks. A common interface that all experts must support is specified, and any kind of expert can be incorporated if it has the interface. Several modules running in parallel coordinate the experts by accessing them through the interface, so that the whole system can achieve flexible control, such as interruption handling and parallel task execution.","cites":"11","conferencePercentile":"66.66666667"},{"venue":"SIGDIAL Workshop","id":"213eae5c32346ef518fdacfa04a596ce229482ff","venue_1":"SIGDIAL Workshop","year":"2002","title":"Synchronization in an Asynchronous Agent-based architecture for Dialogue Systems","authors":"Nate Blaylock, James F. Allen, George Ferguson","author_ids":"1748576, 1749025, 1752556","abstract":"Most dialogue architectures are either pipelined or, if agent-based, are restricted to a pipelined flow-of-information. The TRIPS dialogue architecture is agent-based and asynchronous, with several layers of information flow. We present this architecture and the synchronization issues we encountered in building a truly distributed, agent-based dialogue architecture.","cites":"17","conferencePercentile":"84.09090909"},{"venue":"SIGDIAL Workshop","id":"143c89043402241b9db0d37b79632823f2fa70ee","venue_1":"SIGDIAL Workshop","year":"2000","title":"WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogu Systems","authors":"Mikio Nakano, Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama, Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa","author_ids":"1734868, 3096715, 2715365, 2899298, 2312961, 1751116, 1732656","abstract":"This paper describes WIT, a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and real-time responses. WIT's ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than it is from scratch.","cites":"9","conferencePercentile":"75"},{"venue":"SIGDIAL Workshop","id":"83cc4d0d3e0db3e1819b4cf3b2ea01b7aaf16e22","venue_1":"SIGDIAL Workshop","year":"2004","title":"Unifying Annotated Discourse Hierarchies to Create a Gold Standard","authors":"Marco Carbone, Ya'akov Gal, Stuart M. Shieber, Barbara J. Grosz","author_ids":"1721025, 1721094, 1692491, 1692242","abstract":"Human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement. This paper presents several techniques for unifying multiple discourse annotations into a single hierarchy , deemed a \" gold standard \" — the segmen-tation that best captures the underlying linguistic structure of the discourse. It proposes and analyzes methods that consider the level of em-beddedness of a segmentation as well as methods that do not. A corpus containing annotated hierarchical discourses, the Boston Directions Corpus, was used to evaluate the \" goodness \" of each technique, by comparing the similarity of the segmentation it derives to the original annotations in the corpus. Several metrics of similarity between hierarchical segmentations are computed: precision/recall of matching utterances , pairwise inter-reliability scores (), and non-crossing-brackets. A novel method for unification that minimizes conflicts among an-notators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among anno-tators.","cites":"4","conferencePercentile":"37.03703704"},{"venue":"SIGDIAL Workshop","id":"08cfaf9f1a08979c026313a1fdca6fc700afaa89","venue_1":"SIGDIAL Workshop","year":"2008","title":"Reactive Redundancy and Listener Comprehension in Direction-Giving","authors":"Rachel Baker, Alastair Gill, Justine Cassell","author_ids":"2966193, 4704705, 1741405","abstract":"We explore the role of redundancy, both in anticipation of and in response to listener confusion, in task-oriented dialogue. We find that direction-givers provide redundant utterances in response to both verbal and non-verbal signals of listener confusion. We also examine the effects of prior acquaintance and visibility upon redundancy. As expected, givers use more redundant utterances overall, and more redundant utterances in response to listener questions, when communicating with strangers. We discuss our findings in relation to theories of redundancy, the balance of speaker and listener effort, and potential applications.","cites":"6","conferencePercentile":"46.66666667"},{"venue":"SIGDIAL Workshop","id":"81ed50efc7a2198014f4b7374f86edabad42457e","venue_1":"SIGDIAL Workshop","year":"2001","title":"Labeling Corrections and Aware Sites in Spoken Dialogue Systems","authors":"Julia Hirschberg, Marc Swerts, Diane J. Litman","author_ids":"1784850, 1730911, 1737616","abstract":"This paper deals with user corrections and aware sites of system errors in the TOOT spoken dialogue system. We rst describe our corpus , and give details on our procedure to label corrections and aware sites. Then, we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from`normal' utterances. It appears that some correction types, such as simple repeats, are more likely to be correctly recognized than other types, such a s p a r a-phrases. We also present evidence that system dialogue strategy aects users' choice of correction type, suggesting that strategy-speciic methods of detecting or coaching users on corrections may be useful. Aware sites tend to be shorter than other utterances, and are also more dif-cult to recognize correctly for the ASR system.","cites":"11","conferencePercentile":"63.15789474"},{"venue":"SIGDIAL Workshop","id":"97ae9964dfcd3a1421ebf5a9be6c363ac5f31432","venue_1":"SIGDIAL Workshop","year":"2008","title":"Rapidly Deploying Grammar-Based Speech Applications with Active Learning and Back-off Grammars","authors":"Tim Paek, Sudeep Gandhe, David Maxwell Chickering","author_ids":"3049377, 2243729, 1724065","abstract":"A bstract Grammar-based approaches to spoken language understanding are utilized to a great extent in industry, particularly when developers are confronted with data sparsity. In order to ensure wide grammar coverage, developers typically modify their grammars in an iterative process of deploying the application, collecting and transcribing user utterances, and adjusting the grammar. In this paper, we explore enhancing this iterative process by leve-raging active learning with back-off grammars. Because the back-off grammars expand coverage of user utterances, developers have a safety net for deploying applications earlier. Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription. In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions.","cites":"0","conferencePercentile":"6.666666667"},{"venue":"SIGDIAL Workshop","id":"c3d9629f080c7bbdd686f356a55ef7278827cbe6","venue_1":"SIGDIAL Workshop","year":"2004","title":"Dialogue Systems that Can Handle Face-to-Face Joint Reference to Actions in Space","authors":"Justine Cassell","author_ids":"1741405","abstract":"This talk introduces new research that works towards an overarching model of natural face-to-face conversation about spatially-located actions in the world, and then uses that model to implement a trustworthy embodied conversational agent to guide users' ongoing, real-world activities away from the desktop. Past research has demonstrated that the relationship between verbal and nonverbal behavior exists at the level of intonational phrases, conversational turns, discourse units, and the negotiation of reference to objects and actions, that mental representations of shared space are structured in such a way as to allow participants in a dialogue to draw on them, that dialogue systems must be based on models of coordination and collaboration, and that users are willing to engage in persistent, natural , trusting conversation with embodied conversational systems. In this talk, these diverse strands of research are brought together in the service of a single underlying modality-independent model of action and language, non-verbal behaviors and words, production and comprehension that can lead to a physically-located, spatially aware , collaborative embodied conversational agent.","cites":"0","conferencePercentile":"9.259259259"},{"venue":"SIGDIAL Workshop","id":"0e9db4f229c9abb2c87444d69c5cc9784118b646","venue_1":"SIGDIAL Workshop","year":"2002","title":"A new Taxonomy for the Quality of Telephone Services Based on Spoken Dialogue Systems","authors":"Sebastian Möller","author_ids":"1785752","abstract":"This document proposes a new taxon-omy for describing the quality of services which are based on spoken dialogue systems (SDSs), and operated via a telephone interface. It is used to classify instrumentally or expert–derived dialogue and system measures, as well as quality features perceived by the user of the service. A comparison is drawn to the quality of human–to–human telephone services, and implications for the development of evaluation frameworks such as PARADISE are discussed.","cites":"16","conferencePercentile":"77.27272727"},{"venue":"SIGDIAL Workshop","id":"2d5dca7e990923a8f93c5ec3f5114c70f8392578","venue_1":"SIGDIAL Workshop","year":"2008","title":"A Framework for Model-based Evaluation of Spoken Dialog Systems","authors":"Sebastian Möller, Nigel Ward","author_ids":"1785752, 6836962","abstract":"Improvements in the quality, usability and acceptability of spoken dialog systems can be facilitated by better evaluation methods. To support early and efficient evaluation of dialog systems and their components, this paper presents a tripartite framework describing the evaluation problem. One part models the behavior of user and system during the interaction , the second one the perception and judgment processes taking place inside the user, and the third part models what matters to system designers and service providers. The paper reviews available approaches for some of the model parts, and indicates how anticipated improvements may serve not only developers and users but also researchers working on advanced dialog functions and features.","cites":"8","conferencePercentile":"56.66666667"},{"venue":"SIGDIAL Workshop","id":"448e6e8e5c3a0307e4df67ad13e64accf2da93cc","venue_1":"SIGDIAL Workshop","year":"2001","title":"An Empirical Study of Speech Recognition Errors in a Task-Oriented Dialogue System","authors":"Marc Cavazza","author_ids":"1696638","abstract":"The development of spoken dialogue systems is often limited by the performance of their speech recognition component. The impact of speech recognition errors on dialogue systems is often studied at the global level of task completion. In this paper, we carry an empirical study on the consequences of speech recognition errors on a fully-implemented dialogue prototype, based on a speech acts formalisms. We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors.","cites":"4","conferencePercentile":"42.10526316"},{"venue":"SIGDIAL Workshop","id":"3fd246805943b64e84d1c46bcd6a3e423b0c761a","venue_1":"SIGDIAL Workshop","year":"2000","title":"Issues in the Transcription of English Conversational Grunts","authors":"Nigel Ward","author_ids":"6836962","abstract":"Conversational grunts, such as uh-huh, un-hn, rnrn, and oh are ubiquitous in spoken English, but no satisfactory scheme for transcribing these items exists. This paper describes previous approaches, presents some facts about the phonetics of grunts, proposes a transcription scheme, and evaluates its accuracy. 1 1 The Importance of Conversational Grunts :Conversational grunts, such as uh-huh, un-hn~ ram, and oh are ubiquitous in spoken English. In our conversation data, these grunts occur an average of once every 5 seconds in Amer-ican English conversation. In a sample of 79 conversations from a larger corpus, Switchboard , urn was the 6th most frequent item (after /, and, the, you, and a), and the four items uh, uh-huh, um and urn-hum accounted for 4% of the total. These sounds are not only frequent, they are important in language use. To mention just one example, people learning English as a second language are handicapped in informal interactions if they cannot produce and recognize these sounds. 1I would like to tb.nlr Takeki Kamiyama for pho-netic label cross-checld-g, all those who let me record their conversations, and the anonymous referees; and also the Japanese 1Vr;nlqtry of Education, the Sound Technology Promotion Foundation, the Nakayama Foundation, the Inamori Foundation, the International Communications Fonndation and the Okawa Foundation for support. Just to be clear about definitions, in this paper 'grunts 2' means sounds which are ~not words', where a prototypical \"word\" is a sound having 1. a clear meaning, 2. the ability to participate in syntactic constructions, and 3. a phonotactically normal pronunciation. For example, uh-huh is a grunt since it has no referential meaning, has no syntactic affinities, and has salient breathiness. In this paper 'conversational' refers to sounds which occur in conversation and are at least in part directed at the interlocutor, rather than being purely self-directed 3. Both of these definitions have flaws, but they provide a fairly objective criterion for delimiting the set of items which any transcription scheme should be able to handle. The phenomena circumscribed by this definition are a subset of \"vocal segregates\" (Trager, 1958) and of \"interjections\": the difference is that it limits attention to sounds occurring in conversations. This definition also roughly delimits the subset of \"discourse markers\" or \"discourse particles\" which occur in informal spoken discourse. As the phonetics and meanings of conversational grunts are currently not well understood , we have begun a project aiming to elucidate …","cites":"4","conferencePercentile":"50"},{"venue":"SIGDIAL Workshop","id":"3f8fd770440593846124d61293fe4268c7bf0c89","venue_1":"SIGDIAL Workshop","year":"2004","title":"Bootstrapping Spoken Dialog Systems with Data Reuse","authors":"Giuseppe Di Fabbrizio, Gökhan Tür, Dilek Z. Hakkani-Tür","author_ids":"2281755, 1748051, 1712619","abstract":"Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances. Furthermore , the design of such complex systems consists of several manual steps. The User Experience (UE) expert analyzes and defines by hand the system core functionalities: the system semantic scope (call-types) and the dialog manager strategy which will drive the human-machine interaction. This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment. Moreover, scalability is compromised by time, costs and the high level of UE know-how needed to reach a consistent design. We propose a novel approach for bootstrapping spoken dialog systems based on reuse of existing transcribed and labeled data, common reusable dialog templates and patterns, generic language and understanding models, and a consistent design process. We demonstrate that our approach reduces design and development time while providing an effective system without any application specific data.","cites":"12","conferencePercentile":"59.25925926"},{"venue":"SIGDIAL Workshop","id":"21616992c2a37fad0d1192392830c638f96ba02f","venue_1":"SIGDIAL Workshop","year":"2008","title":"Modelling and Detecting Decisions in Multi-party Dialogue","authors":"Raquel Fernández, Matthew Frampton, Patrick Ehlen, Matthew Purver, Stanley Peters","author_ids":"2081839, 2719623, 2408671, 1701461, 1689837","abstract":"We describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party, human-human meetings. Extending our previous work on action item identification, we propose a struc-tured approach that takes into account the different roles utterances play in the decision-making process. We show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on flat annotations, while enabling the extraction of more fine-grained information that can be used for summarization and reporting.","cites":"19","conferencePercentile":"93.33333333"},{"venue":"SIGDIAL Workshop","id":"3a131cd4bf119c63f713d18c7cdea73a1c3704b3","venue_1":"SIGDIAL Workshop","year":"2008","title":"What Are Meeting Summaries? An Analysis of Human Extractive Summaries in Meeting Corpus","authors":"Fei Liu, Yang Liu","author_ids":"2682754, 1750084","abstract":"Significant research efforts have been devoted to speech summarization, including automatic approaches and evaluation metrics. However, a fundamental problem about what summaries are for the speech data and whether humans agree with each other remains unclear. This paper performs an analysis of human annotated extractive summaries using the ICSI meeting corpus with an aim to examine their consistency and the factors impacting human agreement. In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem.","cites":"4","conferencePercentile":"33.33333333"},{"venue":"SIGDIAL Workshop","id":"8b2ebc503c2af1d43be43ff61d3f2ab9ae2d25cb","venue_1":"SIGDIAL Workshop","year":"2004","title":"Speech Graffiti Habitability: What Do Users Really Say?","authors":"Stefanie Tomko, Ronald Rosenfeld","author_ids":"2494548, 4661000","abstract":"The Speech Graffiti interface is designed to be a portable, transparent interface for spoken language interaction with simple machines and information servers. Because it is a subset language, users must learn and adhere to the constraints of the language. We conducted a user study to determine habitability and found that more than 80% of utterances were Speech Graffiti-grammatical, suggesting that the language is acceptably learnable and usable for most users. We also analyzed deviations from grammaticality and found that natural language input accounted for the most deviations from Speech Graffiti. The results will suggest changes to the interface and can also inform design choices in other speech interfaces.","cites":"2","conferencePercentile":"20.37037037"},{"venue":"SIGDIAL Workshop","id":"37e4b1b325b0d5abef84a4d1a7514dffaeccfa55","venue_1":"SIGDIAL Workshop","year":"2006","title":"Multi-Domain Spoken Dialogue System with Extensibility and Robustness against Speech Recognition Errors","authors":"Kazunori Komatani, Naoyuki Kanda, Mikio Nakano, Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata, Hiroshi G. Okuno","author_ids":"1762435, 1833359, 1734868, 1764429, 2206851, 1691205, 1775800","abstract":"We developed a multi-domain spoken dialogue system that can handle user requests across multiple domains. Such systems need to satisfy two requirements: extensi-bility and robustness against speech recognition errors. Extensibility is required to allow for the modification and addition of domains independent of other domains. Robustness against speech recognition errors is required because such errors are inevitable in speech recognition. However , the systems should still behave appropriately , even when their inputs are erroneous. Our system was constructed on an extensible architecture and is equipped with a robust and extensible domain selection method. Domain selection was based on three choices: (I) the previous domain, (II) the domain in which the speech recognition result can be accepted with the highest recognition score, and (III) other domains. With the third choice we newly introduced, our system can prevent dialogues from continuously being stuck in an erroneous domain. Our experimental results, obtained with 10 subjects, showed that our method reduced the domain selection errors by 18.3%, compared to a conventional method.","cites":"19","conferencePercentile":"81.81818182"},{"venue":"SIGDIAL Workshop","id":"493725054d3c58fe149a3ff8df7182120635ebb4","venue_1":"SIGDIAL Workshop","year":"2008","title":"Speaking More Like You: Lexical, Acoustic/Prosodic, and Discourse Entrainment in Spoken Dialogue Systems","authors":"Julia Hirschberg","author_ids":"1784850","abstract":"When people engage in conversation, they adapt the way they speak to the speaking style of their conversational partner in a variety of ways. For example, they may adopt a certain way of describing something based upon the way their conversational partner describes it, or adapt their pitch range or speaking rate to a conversational partner's. They may even align their turn-taking style or use of cue phrases to match their partner's. These types of entrainment have been shown to correlate with various measures of task success and dialogue naturalness. While there is considerable evidence for lexical entrainment from laboratory experiments, much less is known about other types of acoustic-prosodic and discourse-level entrainment and little work has been done to examine entrainments in multiple modalities for the same dialogue. We will discuss work on entrainment in multiple dimensions in the Columbia Games Corpus. Our goal is to understand how the different varieties of entrainment correlate with one another and to determine which types of entrainment will be both useful and feasible for Spoken Dialogue Systems.","cites":"3","conferencePercentile":"25"},{"venue":"SIGDIAL Workshop","id":"858a9151274d14a213c49ef0eb6d96af9b7941e5","venue_1":"SIGDIAL Workshop","year":"2004","title":"Stochastic Language Generation in a Dialogue System: Toward a Domain Independent Generator","authors":"Nathanael Chambers, James F. Allen","author_ids":"1729918, 1749025","abstract":"Until recently, surface generation in dialogue systems has served the purpose of simply providing a backend to other areas of research. The generation component of such systems usually consists of templates and canned text, providing inflexible, unnatural output. To make matters worse, the resources are typically specific to the domain in question and not portable to new tasks. In contrast, domain-independent generation systems typically require large grammars, full lexicons, complex collocational information, and much more. Furthermore, these frameworks have primarily been applied to text applications and it is not clear that the same systems could perform well in a dialogue application. This paper explores the feasibility of adapting such systems to create a domain-independent generation component useful for dialogue systems. It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochas-tic surface generation module. We show that a written text language model can be used to predict dialogue utterances from an over-generated word forest. We also present results from a human oriented evaluation in an emergency planning domain.","cites":"15","conferencePercentile":"66.66666667"},{"venue":"SIGDIAL Workshop","id":"45917b61a4b476c730368fd0b43cf98851bdf986","venue_1":"SIGDIAL Workshop","year":"2002","title":"Training a Dialogue Act Tagger for Human-human and Human-computer Travel dialogues","authors":"Rashmi Prasad, Marilyn A. Walker","author_ids":"1701993, 1760530","abstract":"While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the human-computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.","cites":"12","conferencePercentile":"65.90909091"},{"venue":"SIGDIAL Workshop","id":"a11a791daf4f526754aa2839f46cab56db196086","venue_1":"SIGDIAL Workshop","year":"2008","title":"Modeling Vocal Interaction for Text-Independent Participant Characterization in Multi-Party Conversation","authors":"Kornel Laskowski, Mari Ostendorf, Tanja Schultz","author_ids":"1709213, 1745016, 1713194","abstract":"An important task in automatic conversation understanding is the inference of social structure governing participant behavior. We explore the dependence between several social dimensions, including assigned role, gender, and seniority, and a set of low-level features descriptive of talkspurt deployment in a mul-tiparticipant context. Experiments conducted on two large, publicly available meeting corpora suggest that our features are quite useful in predicting these dimensions, excepting gender. The classification experiments we present exhibit a relative error rate reduction of 37% to 67% compared to choosing the majority class.","cites":"37","conferencePercentile":"96.66666667"},{"venue":"SIGDIAL Workshop","id":"adaca190ae5c47cbfdc89a357aeb5d9064eed294","venue_1":"SIGDIAL Workshop","year":"2002","title":"Adaptive Dialogue Systems - Interaction with Interact","authors":"Kristiina Jokinen, Antti Kerminen, Tommi Lagus, Jukka Kuusisto, Graham Wilcock, Markku Turunen, Jaakko Hakulinen, Krista Jauhiainen","author_ids":"1780486, 2074471, 7321804, 3217430, 2640632, 2016931, 3105421, 7482818","abstract":"Technological development has made computer interaction more common and also commercially feasible, and the number of interactive systems has grown rapidly. At the same time, the systems should be able to adapt to various situations and various users, so as to provide the most efficient and helpful mode of interaction. The aim of the Interact project is to explore natural human-computer interaction and to develop dialogue models which will allow users to interact with the computer in a natural and robust way. The paper describes the innovative goals of the project and presents ways that the Interact system supports adaptivity on different system design and interaction management levels.","cites":"20","conferencePercentile":"90.90909091"},{"venue":"SIGDIAL Workshop","id":"e3353489952d8618040fffe8d88d0244ae23b760","venue_1":"SIGDIAL Workshop","year":"2003","title":"Spoken Dialogue for Virtual Advisers in a semi-immersive Command and Control environment","authors":"Dominique Estival, Michael Broughton, Andrew Zschorn, Elizabeth Pronger","author_ids":"2672421, 2065516, 3781188, 8427130","abstract":"We present the spoken dialogue system designed and implemented for Virtual Advisers in the FOCAL environment. Its architectureisbasedon:DialogueAgents using propositional attitudes, a Natural Language Understanding component using typed unification grammar, and a commercial speaker-independent speech recognition system. The current application aims to facilitate the multi-media presentation of military planning information in a semi-immersive environment.","cites":"6","conferencePercentile":"50"},{"venue":"SIGDIAL Workshop","id":"3fb1e2488da3d1f4b705a8b75de2c9e2e8859354","venue_1":"SIGDIAL Workshop","year":"2004","title":"Prosodic Cues to Discourse Segment Boundaries in Human-Computer Dialogue","authors":"Gina-Anne Levow","author_ids":"1733034","abstract":"Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured. While substantial work has been done on identifying and automatically recognizing the textual and prosodic correlates of discourse structure in monologue , comparable cues for dialogue or multi-party conversation, and in particular human-computer dialogue remain relatively less studied. In this paper, we explore prosodic cues to discourse segmentation in human-computer dialogue. Using data drawn from 60 hours of interactions with a voice-only conversational spoken language system, we identify pitch and intensity features that signal segment boundaries. Specifically, based on 473 pairs of segment-final and segment-initiating utterances, we find significant increases for segment-initial utterances in maximum pitch, average pitch, and average intensity , while segment-final utterances show significantly lower minimum pitch. These results suggest that even in the artificial environment of human-computer dialogue, prosodic cues robustly signal discourse segment structure, comparably to the contrastive uses of pitch and amplitude identified in natural monologues.","cites":"7","conferencePercentile":"51.85185185"},{"venue":"SIGDIAL Workshop","id":"293fa659c9470dc4d15acc811777f13cdbe05899","venue_1":"SIGDIAL Workshop","year":"2006","title":"Automatically Detecting Action Items in Audio Meeting Recordings","authors":"William Morgan, Pi-Chuan Chang, Surabhi Gupta, Jason M. Brenier","author_ids":"7678808, 2923276, 3113712, 1758551","abstract":"Identification of action items in meeting recordings can provide immediate access to salient information in a medium notoriously difficult to search and summarize. To this end, we use a maximum entropy model to automatically detect action item-related utterances from multi-party audio meeting recordings. We compare the effect of lexical, temporal, syntactic, semantic , and prosodic features on system performance. We show that on a corpus of action item annotations on the ICSI meeting recordings, characterized by high imbalance and low inter-annotator agreement, the system performs at an F measure of 31.92%. While this is low compared to better-studied tasks on more mature corpora , the relative usefulness of the features towards this task is indicative of their usefulness on more consistent annotations, as well as to related tasks.","cites":"10","conferencePercentile":"54.54545455"},{"venue":"SIGDIAL Workshop","id":"38afebde7b3f1831ed7693597f9562437e0e4c8a","venue_1":"SIGDIAL Workshop","year":"2003","title":"Learning to Speak to a Spoken Language System: Vocabulary Convergence in Novice Users","authors":"Gina-Anne Levow","author_ids":"1733034","abstract":"A key challenge for users and designers of spoken language systems is determining the form of the commands that the system can recognize. Using more than 60 hours of interactions, we quantitatively analyze the acquisition of system vocabulary by novice users. We contrast the longitudinal performance of long-term novice users with both expert system developers and guest users. We find that novice users successfully learn the form of system requests , achieving a significant decrease in ill-formed utterances. However, the working vocabulary on which novice users converge is significantly smaller than that of expert users, and their rate of speech recognition errors remains higher. Finally, we observe that only 50% of each user's small vocabulary is shared with any other, indicating the importance of the flexibility of a conversational interface that allows users to converge to their own preferred vocabulary.","cites":"4","conferencePercentile":"39.28571429"},{"venue":"SIGDIAL Workshop","id":"00ee91cc12c559e4bbb85b6b104d29414334ecce","venue_1":"SIGDIAL Workshop","year":"2002","title":"Dialogue Act Recognition with Bayesian Networks for Dutch Dialogues","authors":"Simon Keizer, Rieks op den Akker, Anton Nijholt","author_ids":"1796426, 1782503, 1745198","abstract":"This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue system for Dutch dialogues. The Bayesian networks can be constructed from the data in an annotated dialogue corpus. For two series of experiments-using different corpora but the same annotation scheme-recognition results are presented and evaluated.","cites":"34","conferencePercentile":"100"},{"venue":"SIGDIAL Workshop","id":"b49202f52a7180da20e432a6b860357123cdc209","venue_1":"SIGDIAL Workshop","year":"2003","title":"Example-based Spoken Dialogue System using WOZ System Log","authors":"Hiroya Murao, Nobuo Kawaguchi, Shigeki Matsubara, Yukiko Yamaguchi, Yasuyoshi Inagaki","author_ids":"2923384, 1701521, 4697991, 1760017, 2756399","abstract":"This paper proposes a new framework for a spoken dialogue system based on dialogue examples between human subjects and the Wizard of OZ (WOZ) system. Using this framework and a model of information retrieval dialogue, a spoken dialogue system for retrieving shop information while driving in a car has been designed. The system refers to the dialogue examples to find an example that is suitable for generating a query or a reply. The authors have also constructed a large-scale dialogue database using a WOZ system, which enables efficient collection of dialogue examples.","cites":"14","conferencePercentile":"69.64285714"},{"venue":"SIGDIAL Workshop","id":"15035f4b8137b358b73190a71542369170768aa2","venue_1":"SIGDIAL Workshop","year":"2001","title":"Empirical Methods for Evaluating Dialog Systems","authors":"Tim Paek","author_ids":"3049377","abstract":"We examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose. The methods include a protocol for conducting a wizard-of-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or \" gold standard \" for comparative judgments. The methods also provide a practical means of optimizing the system through component analysis and cost valuation. Abstract We examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose. The methods include a protocol for conducting a wizard-of-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or \" gold standard \" for comparative judgments. The methods also provide a practical means of optimizing the system through component analysis and cost valuation.","cites":"29","conferencePercentile":"78.94736842"},{"venue":"SIGDIAL Workshop","id":"63dc0950505a8241ab1817ee804af991fed165b0","venue_1":"SIGDIAL Workshop","year":"2002","title":"Multi-tasking and Collaborative Activities in Dialogue Systems","authors":"Oliver Lemon, Alexander Gruenstein, Alexis Battle, Stanley Peters","author_ids":"1782798, 2091240, 2523256, 1689837","abstract":"We explain dialogue management techniques for collaborative activities with humans , involving multiple concurrent tasks. Conversational context for multiple concurrent activities is represented using a \" Dialogue Move Tree \" and an \" Activity Tree \" which support multiple interleaved threads of dialogue about different activities and their execution status. We also describe the incremental message selection, aggregation, and generation method employed in the system.","cites":"30","conferencePercentile":"95.45454545"},{"venue":"SIGDIAL Workshop","id":"7e7f13dc3ddc6f37b16e48946110040fa1569887","venue_1":"SIGDIAL Workshop","year":"2002","title":"Bridging the Gap between Dialogue management and dialogue models","authors":"Weiqun Xu, Bo Xu, Taiyi Huang, Hairong Xia","author_ids":"7978422, 1749224, 2108824, 8303126","abstract":"Why do few working spoken dialogue systems make use of dialogue models in their dialogue management? We find out the causes and propose a generic dialogue model. It promises to bridge the gap between practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act.","cites":"9","conferencePercentile":"56.81818182"}]}