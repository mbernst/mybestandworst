{"WACV.csv":[{"venue":"WACV","id":"049ad2deb4ce7d1f98057694406879816c4ac049","venue_1":"WACV","year":"2016","title":"Self-taught Object Localization with Deep Networks","authors":"Alessandro Bergamo, Loris Bazzani, Dragomir Anguelov, Lorenzo Torresani","author_ids":"3184250, 1809420, 1838674, 1732879","abstract":"The reliance on plentiful and detailed manual annotations for training is a critical limitation of the current state of the art in object localization and detection. This paper introduces self-taught object localization, a novel approach that leverages deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision, i.e., without using any ground-truth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artificially masking out different regions of the image. The masking out of a region that contains an object typically causes a significant drop in recognition. This idea is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. For a small number of hypotheses, our object localization scheme yields a relative gain of more than 22% in both precision and recall with respect to the state of the art (BING and Selective Search) for top-1 subwindow proposal. Our experiments on a challenging dataset of 200 classes indicate that our automatically-generated annotations are accurate enough to train object detectors in a weakly-supervised fashion with recognition results remarkably close to those obtained by training on manually annotated bounding boxes.","cites":"17","conferencePercentile":"98.48484848"},{"venue":"WACV","id":"02cc6f402b53330fb9ee04383a0483c5a212b270","venue_1":"WACV","year":"2015","title":"Finding Temporally Consistent Occlusion Boundaries in Videos Using Geometric Context","authors":"S. Hussain Raza, Ahmad Humayun, Irfan A. Essa, Matthias Grundmann, David Anderson","author_ids":"2373009, 3162535, 1714295, 2050315, 5993792","abstract":"We present an algorithm for finding temporally consistent occlusion boundaries in videos to support segmentation of dynamic scenes. We learn occlusion boundaries in a pairwise Markov random field (MRF) framework. We first estimate the probability of an spatio-temporal edge being an occlusion boundary by using appearance, flow, and geometric features. Next, we enforce occlusion boundary continuity in a MRF model by learning pairwise occlusion probabilities using a random forest. Then, we temporally smooth boundaries to remove temporal inconsistencies in occlusion boundary estimation. Our proposed framework provides an efficient approach for finding temporally consistent occlusion boundaries in video by utilizing causality, redundancy in videos, and semantic layout of the scene. We have developed a dataset with fully annotated ground-truth occlusion boundaries of over 30 videos ($5000 frames). This dataset is used to evaluate temporal occlusion boundaries and provides a much needed baseline for future studies. We perform experiments to demonstrate the role of scene layout, and temporal information for occlusion reasoning in dynamic scenes.","cites":"2","conferencePercentile":"55.61797753"},{"venue":"WACV","id":"e58c7c3cf4d4bd8e7647bc6ba39fd371927175db","venue_1":"WACV","year":"2015","title":"Leveraging Context to Support Automated Food Recognition in Restaurants","authors":"Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory D. Abowd, Irfan A. Essa","author_ids":"3115428, 2314308, 2943897, 1732524, 1714295","abstract":"The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures reflecting what people eat. In this paper, we study how taking pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with additional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demonstrate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's on-line menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food images taken in 10 restaurants across 5 different types of food","cites":"11","conferencePercentile":"98.87640449"},{"venue":"WACV","id":"474c93dddbac14fce8f550af860bf1929a588075","venue_1":"WACV","year":"2015","title":"3D Reconstruction from Hyperspectral Images","authors":"Ali Zia, Jie Liang, Jun Zhou, Yongsheng Gao","author_ids":"3142523, 1805696, 1728391, 8128647","abstract":"3D reconstruction from hyperspectral images has seldom been addressed in the literature. This is a challenging problem because 3D models reconstructed from different spectral bands demonstrate different properties. If we use a single band or covert the hyperspectral image to grayscale image for the reconstruction, fine structural information may be lost. In this paper, we present a novel method to reconstruct a 3D model from hyperspectral images. Our proposed method first generates 3D point sets from images at each wavelength using the typical structure from motion approach. A structural descriptor is developed to characterize the spatial relationship between the points, which allows robust point matching between two 3D models at different wavelength. Then a 3D registration method is introduced to combine all band-level models into a single and complete hyperspectral 3D model. As far as we know, this is the first attempt in reconstructing a complete 3D model from hyper-spectral images. This work allows fine structural-spectral information of an object be captured and integrated into the 3D model, which can be used to support further research and applications.","cites":"2","conferencePercentile":"55.61797753"},{"venue":"WACV","id":"219bb61382bffe6e2c182df3590c9673d745ec65","venue_1":"WACV","year":"2008","title":"Tracking Down Under: Following the Satin Bowerbird","authors":"Aniruddha Kembhavi, Ryan Farrell, Yuancheng Luo, David W. Jacobs, Ramani Duraiswami, Larry S. Davis","author_ids":"2684226, 2860196, 2908285, 1771485, 1719541, 1693428","abstract":"Sociobiologists collect huge volumes of video to study animal behavior (our collaborators work with 30,000 hours of video). The scale of these datasets demands the development of automated video analysis tools. Detecting and tracking animals is a critical first step in this process. However, off-the-shelf methods prove incapable of handling videos characterized by poor quality, drastic illumination changes, non-stationary scenery and foreground objects that become motionless for long stretches of time. We improve on existing approaches by taking advantage of specific aspects of this problem: by using information from the entire video we are able to find animals that become motionless for long intervals of time; we make robust decisions based on regional features; for different parts of the image, we tailor the selection of model features, choosing the features most helpful in differentiating the target animal from the background in that part of the image. We evaluate our method, achieving almost 83% tracking accuracy on a more than 200,000 frame dataset of Satin Bowerbird courtship videos.","cites":"4","conferencePercentile":"38.46153846"},{"venue":"WACV","id":"8b7c712a8e1fef25741f0141c6b65d053b8b0598","venue_1":"WACV","year":"2014","title":"Learning mid-level features from object hierarchy for image classification","authors":"Somayah Albaradei, Yang Wang, Liangliang Cao, Li-Jia Li","author_ids":"1996970, 1747927, 2464399, 2451991","abstract":"We propose a new approach for constructing mid-level visual features for image classification. We represent an image using the outputs of a collection of binary classifiers. These binary classifiers are trained to differentiate pairs of object classes in an object hierarchy. Our feature representation implicitly captures the hierarchical structure in object classes. We show that our proposed approach out-performs other baseline methods in image classification.","cites":"2","conferencePercentile":"49.00990099"},{"venue":"WACV","id":"669f9ad2d0a57e7b32ccca435892bfb0a9d97345","venue_1":"WACV","year":"2007","title":"Automatic Extraction of Femur Contours from Calibrated Fluoroscopic Images","authors":"Xiao Dong, Miguel Ángel González Ballester, Guoyan Zheng","author_ids":"1765847, 2473209, 2533175","abstract":"Automatic identification and extraction of bone contours from x-ray images is an essential first step task for further medical image analysis. In this paper we propose a 3D statistical model based framework for the proximal femur contour extraction from calibrated x-ray images. The automatic initialization is solved by an Estimation of Bayesian Network Algorithm to fit a multiple component geometrical model to the x-ray data. The contour extraction is accomplished by a non-rigid 2D/3D registration between a 3D statistical model and the x-ray images, in which bone contours are extracted by a graphical model based Bayesian inference. Preliminary experiments on clinical data sets verified its validity. Fluoroscopic images are still playing a crucial role in diagnosis and surgery. Accurate detection and extraction of bone contours from fluoroscopic images is an essential component for computer analysis of medical images for diagnosis [1][2][3], planning [4][5][6] or 3D reconstruction of anatomic structures [7][8][9][10]. Fluoroscopic images can vary a lot in terms of brightness and contrast as well as in the imaged region of anatomy. Therefore conventional segmen-tation techniques [1][5][6] can not offer a satisfactory solution and model based segmentation is usually implemented (ASM), are constructed from a training image set under the assumption that the images are taken from a certain view direction. 2D statistical models can contain both the shape and texture information learnt from training data set, which is helpful in improving robustness and accuracy in noisy images. 2D statistical model asks for a proper initialization due to the limited convergence region. Fully automatic ini-tialization can be accomplished by the generalized Hough transformation [12], neural nets [13] or evolutionary algorithms [14][15]. But both the initialization and segmenta-tion performance relies on that the view direction assumption can be fulfilled. In [7][8][9][10] 3D statistical models are used for 2D segmentation and 3D reconstruction from calibrated 2D fluoroscopic images (location and orientation of the flu-oroscopic source w.r.t. the image acquisition planes are known). Compared with 2D statistical modes, 3D statistical model usually only contains shape information but not the intensity information on the 2D images. In principle it can be used for segmenting an image taken from an arbitary view direction. 3D statistical model also need an initializa-tion, which is usually manually defined [7][9]. Due to the dense mesh of the 3D statistical model [16], fully automated solutions based on evolutionary algorithm is computational expensive [17]. In this paper we propose a 3D statistical …","cites":"4","conferencePercentile":"48.48484848"},{"venue":"WACV","id":"350c6627a8d6e954ff0ef6e91ca07752fea64f49","venue_1":"WACV","year":"2016","title":"Underwater 3D capture using a low-cost commercial depth camera","authors":"Sundara Tejaswi Digumarti, Gaurav Chaurasia, Aparna Taneja, Roland Siegwart, Amber Thomas, Paul A. Beardsley","author_ids":"1749691, 2585067, 2632660, 1720483, 2247148, 1777539","abstract":"This paper presents underwater 3D capture using a commercial depth camera. Previous underwater capture systems use ordinary cameras, and it is well-known that a calibration procedure is needed to handle refraction. The same is true for a depth camera being used underwater. We describe a calibration method that corrects the depth maps of refraction effects. Another challenge is that depth cameras use infrared light (IR) which is heavily attenuated in water. We demonstrate scanning is possible with commercial depth cameras for ranges up to 20 cm in water. The motivation for using a depth camera under water is the same as in air – it provides dense depth data and higher quality 3D reconstruction than multi-view stereo. Underwater 3D capture is being increasingly used in marine biology and oceanology; our approach offers exciting prospects for such applications. To the best of our knowledge, ours is the first approach that successfully demonstrates underwater 3D capture using low cost depth cameras like Intel RealSense. We describe a complete system, including protective housing for the depth camera which is suitable for handheld use by a diver. Our main contribution is an easy-to-use calibration method, which we evaluate on exemplar data as well as 3D reconstructions in a lab aquarium. We also present initial results of ocean deployment.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"f1449d502c61f47b42eeff58289730cd90adaf73","venue_1":"WACV","year":"2012","title":"Online discriminative object tracking with local sparse representation","authors":"Qing Wang, Feng Chen, Wenli Xu, Ming-Hsuan Yang","author_ids":"7135234, 1692998, 1779759, 3452739","abstract":"We propose an online algorithm based on local sparse representation for robust object tracking. Local image patches of a target object are represented by their sparse codes with an over-complete dictionary constructed online, and a classifier is learned to discriminate the target from the background. To alleviate the visual drift problem often encountered in object tracking, a two-stage algorithm is proposed to exploit both the ground truth information of the first frame and observations obtained online. Different from recent discriminative tracking methods that use a pool of features or a set of boosted classifiers, the proposed algorithm learns sparse codes and a linear classifier directly from raw image patches. In contrast to recent sparse representation based tracking methods which encode holis-tic object appearance within a generative framework, the proposed algorithm employs a discrimination formulation which facilitates the tracking task in complex environments. Experiments on challenging sequences with evaluation of the state-of-the-art methods show effectiveness of the proposed algorithm.","cites":"28","conferencePercentile":"97.77777778"},{"venue":"WACV","id":"40aaeb3a5eb216343bdf6950db13cc6030119b08","venue_1":"WACV","year":"2013","title":"SAGE: An approach and implementation empowering quick and reliable quantitative analysis of segmentation quality","authors":"Danna Gurari, Suele Ki Kim, Eugene Yang, Brett Isenberg, Tuan A. Pham, Alberto Purwada, Patricia Solski, Matthew L. Walker, Joyce Y. Wong, Margrit Betke","author_ids":"2028946, 1969680, 6998607, 2535117, 7237319, 2529992, 2245445, 2111764, 2528307, 1723703","abstract":"Finding the outline of an object in an image is a fundamental step in many vision-based applications. It is important to demonstrate that the segmentation found accurately represents the contour of the object in the image. The discrepancy measure model for segmentation analysis focuses on selecting an appropriate discrepancy measure to compute a score that indicates how similar a query segmen-tation is to a gold standard segmentation. Observing that the score depends on the gold standard segmentation, we propose a framework that expands this approach by introducing the consideration of how to establish the gold standard segmentation. The framework shows how to obtain project-specific performance indicators in a principled way that links annotation tools, fusion methods, and evaluation algorithms into a unified model we call SAGE. We also describe a freely available implementation of SAGE that enables quick segmentation validation against either a single annotation or a fused annotation. Finally, three studies are presented to highlight the impact of annotation tools, an-notators, and fusion methods on establishing trusted gold standard segmentations for cell and artery images.","cites":"5","conferencePercentile":"66.66666667"},{"venue":"WACV","id":"069cadd9d8e52ad2715a3551012a06e506191626","venue_1":"WACV","year":"2013","title":"Person re-identification using semantic color names and RankBoost","authors":"Cheng-Hao Kuo, Sameh Khamis, Vinay D. Shet","author_ids":"2754708, 3144576, 1961864","abstract":"We address the problem of appearance-based person re-identification, which has been drawing an increasing amount of attention in computer vision. It is a very challenging task since the visual appearance of a person can change dramatically due to different backgrounds, camera characteristics, lighting conditions, viewpoints , and human poses. Among the recent studies on person re-id, color information plays a major role in terms of performance. Traditional color information like color histogram, however , still has much room to improve. We propose to apply semantic color names to describe a person image, and compute probability distribution on those basic color terms as image descriptors. To be better combined with other features, we define our appearance affinity model as linear combination of similarity measurements of corresponding local descriptors, and apply the RankBoost algorithm to find the optimal weights for the similarity measurements. We evaluate our proposed system on the highly challenging VIPeR dataset, and show improvements over the state-of-the-art methods in terms of widely used person re-id evaluation metrics.","cites":"12","conferencePercentile":"86.2745098"},{"venue":"WACV","id":"409246aa99b32443c0d2cb936e5b7fe1ba7a8a76","venue_1":"WACV","year":"2014","title":"3D pose estimation of bats in the wild","authors":"Mikhail Breslav, Nathan W. Fuller, Stan Sclaroff, Margrit Betke","author_ids":"2025025, 1726218, 1749590, 1723703","abstract":"Vision-based methods have gained popularity as a tool for helping to analyze the behavior of bats. Though, for bats in the wild, there are still no tools capable of estimating and subsequently analyzing articulated 3D bat pose. We propose a model-based multi-view articulated 3D bat pose estimation framework for this novel problem. Key challenges include the large search space associated with articulated 3D pose, the ambiguities that arise from 2D projections of 3D bodies, and the low resolution image data we have available. Our method uses multi-view camera geometry and temporal constraints to reduce the state space of possible articulated 3D bat poses and finds an optimal set using a Markov Random Field based model. Our experiments use real video data of flying bats and gold-standard annotations by a bat biologist. Our results show, for the first time in the literature, articulated 3D pose estimates being generated automatically for video sequences of bats flying in the wild. The average differences in body orientation and wing joint angles, between estimates produced by our method and those based on gold-standard annotations, ranged from 16 • – 21 • (i.e., ≈ 17% – 23%) for orientation and 14 • – 26 • (i.e., ≈ 7% – 14%) for wing joint angles.","cites":"2","conferencePercentile":"49.00990099"},{"venue":"WACV","id":"0251ca9090947138bdb833eccb14997c6a871082","venue_1":"WACV","year":"2015","title":"How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the Performance of Experts, Crowdsourced Non-experts, and Algorithms","authors":"Danna Gurari, Diane H. Theriault, Mehrnoosh Sameki, Brett Isenberg, Tuan A. Pham, Alberto Purwada, Patricia Solski, Matthew L. Walker, Chentian Zhang, Joyce Y. Wong, Margrit Betke","author_ids":"2028946, 8365893, 2128305, 2535117, 7237319, 2529992, 2245445, 2111764, 2919192, 2528307, 1723703","abstract":"Analyses of biomedical images often rely on demarcat-ing the boundaries of biological structures (segmentation). While numerous approaches are adopted to address the segmentation problem including collecting annotations from domain-experts and automated algorithms, the lack of comparative benchmarking makes it challenging to determine the current state-of-art, recognize limitations of existing approaches, and identify relevant future research directions. To provide practical guidance, we evaluated and compared the performance of trained experts, crowdsourced non-experts, and algorithms for annotating 305 objects coming from six datasets that include phase contrast, fluorescence, and magnetic resonance images. Compared to the gold standard established by expert consensus, we found the best annotators were experts, followed by non-experts, and then algorithms. This analysis revealed that online paid crowdsourced workers without domain-specific backgrounds are reliable annotators to use as part of the laboratory protocol for segmenting biomedical images. We also found that fusing the seg-mentations created by crowdsourced internet workers and algorithms yielded improved segmentation results over segmentations created by single crowdsourced or algorithm annotations respectively. We invite extensions of our work by sharing our data sets and associated segmentation annotations","cites":"10","conferencePercentile":"96.62921348"},{"venue":"WACV","id":"b587e16014e2afec486ec52fc719149f6d7ad472","venue_1":"WACV","year":"2016","title":"Discovering useful parts for pose estimation in sparsely annotated datasets","authors":"Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke","author_ids":"2025025, 1711465, 1749590, 1723703","abstract":"Our work introduces a novel way to increase pose estimation accuracy by discovering parts from unannotated regions of training images. Discovered parts are used to generate more accurate appearance likelihoods for traditional part-based models like Pictorial Structures [13] and its derivatives. Our experiments on images of a hawkmoth in flight show that our proposed approach significantly improves over existing work [27] for this application, while also being more generally applicable. Our proposed approach localizes landmarks at least twice as accurately as a baseline based on a Mixture of Pictorial Structures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) dataset is made publicly available with annotations.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"1b489aa73e9bb6970415c474a5ca937dcc78d8a8","venue_1":"WACV","year":"2015","title":"Fast Approximate Matching of Videos from Hand-Held Cameras for Robust Background Subtraction","authors":"Raffay Hamid, Atish Das Sarma, Dennis DeCoste, Neel Sundaresan","author_ids":"1978185, 2541992, 1703049, 1751312","abstract":"We identify a novel instance of the background subtraction problem that focuses on extracting near-field foreground objects captured using handheld cameras. Given two user-generated videos of a scene, one with and the other without the foreground object(s), our goal is to efficiently generate an output video with only the foreground object(s) present in it. We cast this challenge as a spatio-temporal frame matching problem, and propose an efficient solution for it that exploits the temporal smoothness of the video sequences. We present theoretical analyses for the error bounds of our approach, and validate our findings using a detailed set of simulation experiments. Finally, we present the results of our approach tested on multiple real videos captured using handheld cameras, and compare them to several alternate foreground extraction approaches.","cites":"0","conferencePercentile":"10.11235955"},{"venue":"WACV","id":"2962c346694ffec5fb60c50533238d0a69dbfd27","venue_1":"WACV","year":"2012","title":"New hope for recognizing twins by using facial motion","authors":"Li Zhang, Ning Ye, Elisa Martínez Marroquín, Dong Guo, Terence Sim","author_ids":"1712838, 1726134, 3227642, 1990644, 1715286","abstract":"Distinguishing between identical twins is the Holy Grail in face recognition because of the great similarity between the faces of a pair of twins. Most existing face recognition systems choose to simply ignore it. However, as the population of twins increases quickly, such an \" ostrich strategy \" is no longer acceptable. The biometric systems that overlook the twins problem are presenting a serious security hole. Inspired by recent advances in motion-based face recognition techniques, we propose to use facial motion to address the twins problem. We collect a twins facial expression database and conduct a series of experiments in two assumed scenarios: the Social Party Scenario and the Access Control Scenario. The experimental results show that facial motion ourperforms facial appearance in distinguishing between twins. Based on this finding, we propose a two-stage cascaded General Access Control System, which combines facial appearance with facial motion. The experimental results show that, compared with an appearance-based face recognition system, this cascaded system is much more secure against an \" evil-twin \" imposter attack, while performing as good for normal population.","cites":"1","conferencePercentile":"22.22222222"},{"venue":"WACV","id":"7c7ab59a82b766929defd7146fd039b89d67e984","venue_1":"WACV","year":"2014","title":"Improving multiview face detection with multi-task deep convolutional neural networks","authors":"Cha Zhang, Zhengyou Zhang","author_ids":"1706673, 1732465","abstract":"Multiview face detection is a challenging problem due to dramatic appearance changes under various pose, illumination and expression conditions. In this paper, we present a multi-task deep learning scheme to enhance the detection performance. More specifically, we build a deep convolutional neural network that can simultaneously learn the face/nonface decision, the face pose estimation problem , and the facial landmark localization problem. We show that such a multi-task learning scheme can further improve the classifier's accuracy. On the challenging FDDB data set, our detector achieves over 3% improvement in detection rate at the same false positive rate compared with other state-of-the-art methods.","cites":"20","conferencePercentile":"96.53465347"},{"venue":"WACV","id":"1737421619ea21e566a11bf0b8ef7ad2e59ab4bf","venue_1":"WACV","year":"2015","title":"Semantic Instance Labeling Leveraging Hierarchical Segmentation","authors":"Steven Hickson, Irfan A. Essa, Henrik I. Christensen","author_ids":"2935619, 1714295, 1723059","abstract":"Most of the approaches for indoor RGBD semantic labeling focus on using pixels or superpixels to train a classi-fier. In this paper, we implement a higher level segmentation using a hierarchy of superpixels to obtain a better segmen-tation for training our classifier. By focusing on meaningful segments that conform more directly to objects, regardless of size, we train a random forest of decision trees as a clas-sifier using simple features such as the 3D size, LAB color histogram, width, height, and shape as specified by a his-togram of surface normals. We test our method on the NYU V2 depth dataset, a challenging dataset of cluttered indoor environments. Our experiments using the NYU V2 depth dataset show that our method achieves state of the art results on both a general semantic labeling introduced by the dataset (floor, structure, furniture, and objects) and a more object specific semantic labeling. We show that training a classifier on a segmentation from a hierarchy of super pixels yields better results than training directly on super pixels, patches, or pixels as in previous work.","cites":"0","conferencePercentile":"10.11235955"},{"venue":"WACV","id":"9258cc39b07a4924c3d77cdebf7bf01c012045a7","venue_1":"WACV","year":"2002","title":"Activity maps for location-aware computing","authors":"David Demirdjian, Konrad Tollmar, Kimberle Koile, Neal Checka, Trevor Darrell","author_ids":"2444581, 1692304, 2105944, 3341976, 1753210","abstract":"Location-based context is important for many applications. Previous systems offered only coarse room-level features or used manually specified room regions to determine fine-scale features. We propose a location context mechanism based on activity maps, which define regions of similar context based on observations of 3-D patterns of location and motion in an environment. We describe an algorithm for obtaining activity maps using the spatio-temporal clustering of visual tracking data. We show how the recovered maps correspond to regions for common tasks in the environment and describe their use in some applications.","cites":"18","conferencePercentile":"60.60606061"},{"venue":"WACV","id":"6c31a0a903b7f88a16bb922170f5ecbb15430858","venue_1":"WACV","year":"2016","title":"Direct 3D pose estimation of a planar target","authors":"Hung-Yu Tseng, Po-Chen Wu, Ming-Hsuan Yang, Shao-Yi Chien","author_ids":"2126078, 3239304, 3452739, 1733505","abstract":"Estimating 3D pose of a known object from a given 2D image is an important problem with numerous studies for robotics and augmented reality applications. While the state-of-the-art Perspective-n-Point algorithms perform well in pose estimation, the success hinges on whether feature points can be extracted and matched correctly on targets with rich texture. In this work, we propose a robust direct method for 3D pose estimation with high accuracy that performs well on both textured and textureless planar targets. First, the pose of a planar target with respect to a calibrated camera is approximately estimated by posing it as a template matching problem. Next, the object pose is further refined and disambiguated with a gradient descent search scheme. Extensive experiments on both synthetic and real datasets demonstrate the proposed direct pose estimation algorithm performs favorably against state-of-the-art feature-based approaches in terms of robustness and accuracy under several varying conditions.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"a0894368bba22f93fe4f4f7ddfb0bbd1c95a6252","venue_1":"WACV","year":"2016","title":"Omnidirectional image capture on mobile devices for fast automatic generation of 2.5D indoor maps","authors":"Giovanni Pintore, Valeria Garro, Fabio Ganovelli, Enrico Gobbetti, Marco Agus","author_ids":"1778838, 1781717, 1731043, 1708999, 2544127","abstract":"We introduce a lightweight automatic method to quickly capture and recover 2.5D multi-room indoor environments scaled to real-world metric dimensions. To minimize the user effort required, we capture and analyze a single omni-directional image per room using widely available mobile devices. Through a simple tracking of the user movements between rooms, we iterate the process to map and reconstruct entire floor plans. In order to infer 3D clues with a minimal processing and without relying on the presence of texture or detail, we define a specialized spatial transform based on catadioptric theory to highlight the room's structure in a virtual projection. From this information, we define a parametric model of each room to formalize our problem as a global optimization solved by Levenberg-Marquardt iterations. The effectiveness of the method is demonstrated on several challenging real-world multi-room indoor scenes.","cites":"3","conferencePercentile":"74.24242424"},{"venue":"WACV","id":"502a41bd39b12a5939810b7cce2009fc37e4a05c","venue_1":"WACV","year":"2014","title":"AutoCaption: Automatic caption generation for personal photos","authors":"Krishnan Ramnath, Simon Baker, Lucy Vanderwende, Motaz El-Saban, Sudipta N. Sinha, Anitha Kannan, Noran Hassan, Michel Galley, Yi Yang, Deva Ramanan, Alessandro Bergamo, Lorenzo Torresani","author_ids":"1751663, 1737297, 1909300, 3144122, 1757937, 2590455, 3065987, 1947267, 1698559, 1770537, 3184250, 1732879","abstract":"AutoCaption is a system that helps a smartphone user generate a caption for their photos. It operates by upload-ing the photo to a cloud service where a number of parallel modules are applied to recognize a variety of entities and relations. The outputs of the modules are combined to generate a large set of candidate captions, which are returned to the phone. The phone client includes a convenient user interface that allows users to select their favorite caption , reorder, add, or delete words to obtain the grammatical style they prefer. The user can also select from multiple candidates returned by the recognition modules.","cites":"5","conferencePercentile":"84.15841584"},{"venue":"WACV","id":"6c3526cf008d35ea342ab6350ec8938881c8612c","venue_1":"WACV","year":"2011","title":"Stacked spatial-pyramid kernel: An object-class recognition method to combine scores from random trees","authors":"Natalia Larios, Junyuan Lin, Mengzi Zhang, David A. Lytle, Andrew Moldenke, Linda G. Shapiro, Thomas G. Dietterich","author_ids":"2022023, 3292042, 2951026, 4231109, 2091712, 1809809, 1699720","abstract":"The combination of local features, complementary feature types, and relative position information has been successfully applied to many object-class recognition tasks. Stacking is a common classification approach that combines the results from multiple classifiers, having the added benefit of allowing each classifier to handle a different feature space. However, the standard stacking method by its own nature discards any spatial information contained in the features, because only the combination of raw classification scores are input to the final classifier. The object-class recognition method proposed in this paper combines different feature types in a new stacking framework that efficiently quantizes input data and boosts classification accuracy , while allowing the use of spatial information. This classification method is applied to the task of automated insect-species identification for biomonitoring purposes. The test data set for this work contains 4722 images with 29 insect species, belonging to the three most common orders used to measure stream water quality, several of which are closely related and very difficult to distinguish. The specimens are in different 3D positions, different orienta-tions, and different developmental and degradation stages with wide intra-class variation. On this very challenging data set, our new algorithm outperforms other classifiers, showing the benefits of using spatial information in the stacking framework with multiple dissimilar feature types.","cites":"3","conferencePercentile":"46.82539683"},{"venue":"WACV","id":"2fdbd2cdb5b028e77b7640150496ae13eab05f30","venue_1":"WACV","year":"2016","title":"Coupled depth learning","authors":"Mohammad Haris Baig, Lorenzo Torresani","author_ids":"2299796, 1732879","abstract":"In this paper we propose a method for estimating depth from a single image using a coarse to fine approach. We argue that modeling the fine depth details is easier after a coarse depth map has been computed. We express a global (coarse) depth map of an image as a linear combination of a depth basis learned from training examples. The depth basis captures spatial and statistical regularities and reduces the problem of global depth estimation to the task of predicting the input-specific coefficients in the linear combination. This is formulated as a regression problem from a holistic representation of the image. Crucially, the depth basis and the regression function are coupled and jointly optimized by our learning scheme. We demonstrate that this results in a significant improvement in accuracy compared to direct regression of depth pixel values or approaches learning the depth basis disjointly from the regression function. The global depth estimate is then used as a guidance by a local refinement method that introduces depth details that were not captured at the global level. Experiments on the NYUv2 and KITTI datasets show that our method outper-forms the existing state-of-the-art at a considerably lower computational cost for both training and testing.","cites":"3","conferencePercentile":"74.24242424"},{"venue":"WACV","id":"89363ad213fadcfdbd6e39c2c5451cbbeb031a69","venue_1":"WACV","year":"2011","title":"TranslatAR: A mobile augmented reality translator","authors":"Victor Fragoso, Steffen Gauglitz, Shane Zamora, Jim Kleban, Matthew Turk","author_ids":"2245862, 1940938, 3214389, 2425559, 1752714","abstract":"We present a mobile augmented reality (AR) translation system, using a smartphone's camera and touchscreen, that requires the user to simply tap on the word of interest once in order to produce a translation, presented as an AR overlay. The translation seamlessly replaces the original text in the live camera stream, matching background and foreground colors estimated from the source images. For this purpose, we developed an efficient algorithm for accurately detecting the location and orientation of the text in a live camera stream that is robust to perspective distortion, and we combine it with OCR and a text-to-text translation engine. Our experimental results, using the ICDAR 2003 dataset and our own set of video sequences, quantify the accuracy of our detection and analyze the sources of failure among the system's components. With the OCR and translation running in a background thread, the system runs at 26 fps on a current generation smartphone (Nokia N900) and offers a particularly easy-to-use and simple method for translation, especially in situations in which typing or correct pronunciation (for systems with speech input) is cumbersome or impossible.","cites":"11","conferencePercentile":"86.50793651"},{"venue":"WACV","id":"1683d957b01756dd71cf18a1c76383cefd0f1ae8","venue_1":"WACV","year":"2011","title":"Realistic stereo error models and finite optimal stereo baselines","authors":"Tao Zhang, Terrance E. Boult","author_ids":"1743147, 1760117","abstract":"Stereo reconstruction is an important research and application area, both for general 3D reconstruction and for operations like robotic navigation and remote sensing. This paper addresses the determination of parameters for a stereo system to optimize/minimize 3D reconstruction errors. Previous work on error analysis in stereo reconstruction optimized error in disparity space which led to the erroneous conclusion that, ignoring matching errors, errors decrease when the baseline goes to infinity. In this paper, we derive the first formal error model based on the more realistic \" point-of-closest-approach \" ray model used in modern stereo systems. We then show this results in finite optimal baseline that minimizes reconstruction errors in all three world directions. We also show why previous oversimplified error analysis results in infinite baselines. We derive the mathematical relationship between the error variances and the stereo system parameters. In our analysis, we consider the situations where errors exist in only one camera as well as errors in both cameras. We have derived the results for both parallel and verged systems, though only the simpler models are presented algebraically herein. The paper includes simulations to highlight the results and validate the approximations in the error propagation. The results should allow stereo system designers, or those using motion-stereo, to improve their system.","cites":"0","conferencePercentile":"5.555555556"},{"venue":"WACV","id":"16afc373f6176d2c87ac6ccb5953de16ffda9ecf","venue_1":"WACV","year":"2015","title":"Egocentric Field-of-View Localization Using First-Person Point-of-View Devices","authors":"Vinay Bettadapura, Irfan A. Essa, Caroline Pantofaru","author_ids":"3115428, 1714295, 2997956","abstract":"We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors. We demonstrate single and multiuser egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions.","cites":"10","conferencePercentile":"96.62921348"},{"venue":"WACV","id":"736ca9f52203c6269bcf0248b657e34343b26adf","venue_1":"WACV","year":"2014","title":"Physical querying with multi-modal sensing","authors":"Iljoo Baek, Taylor Stine, Denver Dash, Fanyi Xiao, Yaser Sheikh, Yair Movshovitz-Attias, Mei Chen, Martial Hebert, Takeo Kanade","author_ids":"3356778, 2683699, 8515079, 2299381, 1774867, 1860671, 1809936, 1709305, 7642093","abstract":"We present Marvin, a system that can search physical objects using a mobile or wearable device. It integrates HOG-based object recognition, SURF-based localization information, automatic speech recognition, and user feedback information with a probabilistic model to recognize the \" object of interest \" at high accuracy and at interactive speeds. Once the object of interest is recognized, the information that the user is querying, e.g. reviews, options, etc., is displayed on the user's mobile or wearable device. We tested this prototype in a real-world retail store during business hours, with varied degree of background noise and clutter. We show that this multi-modal approach achieves superior recognition accuracy compared to using a vision system alone, especially in cluttered scenes where a vision system would be unable to distinguish which object is of interest to the user without additional input. It is computa-tionally able to scale to large numbers of objects by focus-ing compute-intensive resources on the objects most likely to be of interest, inferred from user speech and implicit localization information. We present the system architecture , the probabilistic model that integrates the multi-modal information, and empirical results showing the benefits of multi-modal integration.","cites":"0","conferencePercentile":"9.405940594"},{"venue":"WACV","id":"2661eb00c344d303ac19cd0c7c1f04bcad049b4b","venue_1":"WACV","year":"2008","title":"Localization and Segmentation of A 2D High Capacity Color Barcode","authors":"Devi Parikh, Gavin Jancke","author_ids":"1713589, 2193264","abstract":"A 2D color barcode can hold much more information than a binary barcode. Barcodes are often intended for consumer use, such as, a consumer can take an image with her cell-phone camera of a barcode on a product, and retrieve relevant information about the product. The barcode must be read using computer vision techniques. While a color bar-code can hold more information, it makes this vision task unusually challenging because of the varying color balancing in different cameras, poor quality of images taken with current cellphone cameras and webcams, varying lighting conditions, arbitrary rotation and even perspective transforms of the barcodes in images. We present our approach to the localization and segmentation of a 2D color barcode in such challenging scenarios, along with its evaluation on a diverse collection of images containing Microsoft's recently launched High Capacity Color Barcode (HCCB). The problem of reading barcodes has a special trait compared to other computer vision localization problems-the results are verifiable in that the decoder can give the vision algorithm feedback of whether the barcode was successfully decoded or not. We exploit this trait in an interesting way and develop a progressive strategy to achieve a fine balance between the requirement of the algorithm to be effective in drastically varying scenarios as well as computationally inexpensive .","cites":"51","conferencePercentile":"100"},{"venue":"WACV","id":"483ef1ad8509c8e5fefc32628c8ceefa8b25fcc5","venue_1":"WACV","year":"2016","title":"Naming TV characters by watching and analyzing dialogs","authors":"Monica-Laura Haurilet, Makarand Tapaswi, Ziad Al-Halah, Rainer Stiefelhagen","author_ids":"3408009, 2103464, 2256981, 1742325","abstract":"Penny asked me to do her a favor. We haven't met. I'm Kate.","cites":"2","conferencePercentile":"63.63636364"},{"venue":"WACV","id":"81b2000a4e8ae658b80a08aa0829c213353cfa38","venue_1":"WACV","year":"2010","title":"Feature fusion for vehicle detection and tracking with low-angle cameras","authors":"Jun Yang, Yang Wang, Arcot Sowmya, Zhidong Li, Bang Zhang, Jie Xu","author_ids":"1750537, 1803215, 1703150, 7718797, 2500157, 1720015","abstract":"Vision-based vehicle detection is a critical task for traffic monitoring in modern Intelligent Traffic Systems (ITS). Due to the low-angle nature of most traffic surveillance cameras installed in the real world, vehicle detection in such case has to deal with one fundamental challenge — occlusion, which renders most traditional vehicle detection methods ineffective. In this paper, instead of detecting the vehicle as a whole, we propose a vehicle detection algorithm based on windshield model matching. By detecting windshield directly, the algorithm achieves robustness to occlusion. Together with camera calibration and vehicle tracking, the system is able to provide reliable traffic state estimation. Experiments on real traffic videos demonstrate the better performance of our system compared to the state-of-the-art algorithm.","cites":"2","conferencePercentile":"100"},{"venue":"WACV","id":"a41f3ce00f07085f4d00943f1f687e1be847fc29","venue_1":"WACV","year":"2013","title":"Berkeley MHAD: A comprehensive Multimodal Human Action Database","authors":"Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, René Vidal, Ruzena Bajcsy","author_ids":"1727159, 2995090, 2862376, 1745721, 1784213","abstract":"Over the years, a large number of methods have been proposed to analyze human pose and motion information from images, videos, and recently from depth data. Most methods, however, have been evaluated on datasets that were too specific to each application, limited to a particular modality, and more importantly, captured under unknown conditions. To address these issues, we introduce the Berke-ley Multimodal Human Action Database (MHAD) consisting of temporally synchronized and geometrically calibrated data from an optical motion capture system, multi-baseline stereo cameras from multiple views, depth sensors , accelerometers and microphones. This controlled mul-timodal dataset provides researchers an inclusive testbed to develop and benchmark new algorithms across multiple modalities under known capture conditions in various research domains. To demonstrate possible use of MHAD for action recognition, we compare results using the popular Bag-of-Words algorithm adapted to each modality independently with the results of various combinations of modalities using the Multiple Kernel Learning. Our comparative results show that multimodal analysis of human motion yields better action recognition rates than unimodal analysis.","cites":"47","conferencePercentile":"100"},{"venue":"WACV","id":"4907a834b176dd9053de35f531e1d87f202cbd31","venue_1":"WACV","year":"2008","title":"Flexible Edge Arrangement Templates for Object Detection","authors":"Yan Li, Yanghai Tsin, Yakup Genc, Takeo Kanade","author_ids":"1723002, 1683980, 2534129, 7642093","abstract":"We present a novel feature representation for categorical object detection. Unlike previous approaches that have concentrated on generic interest-point detectors, we construct object-specific features directly from the training images. Our feature is represented by a collection of Flexible Edge Arrangement Templates (FEATs). We propose a two-stage semi-supervised learning approach to feature selection. A subset of frequent templates are first selected from a large template pool. In the second stage, we formulate feature selection as a regression problem and use LASSO method to find the most discriminative templates from the pre-selected ones. FEATs adaptively capture the image structure and naturally accommodate local shape variations. We show that this feature can be complemented by the traditional holistic patch method, thus achieving both efficiency and accuracy. We evaluate our method on three well-known car datasets, showing performance competitive with existing methods.","cites":"0","conferencePercentile":"7.692307692"},{"venue":"WACV","id":"5804d6725ca4e2d7db0ae5dc41c0d7ef874496ec","venue_1":"WACV","year":"2002","title":"A PDA-based Face Recognition System","authors":"Jie Yang, Xilin Chen, William Kunz","author_ids":"1688428, 1710220, 3087575","abstract":"In this paper, we present a PDA-based face recognition system as well as some of the associated challenges of developing a PDA-based face recognition system. We describe a prototype system built from an off the shelf PDA, and introduce algorithms for image preprocessing to enhance the quality of the image by sharpening focus, and normalizing both lighting condition and head rotation. We use a unified LDA/PCA algorithm for face recognition. The algorithm maximizes the LDA criterion directly without a separate PCA step, which eliminates the possibility of losing discriminative information due to a separate PCA step. We demonstrate effectiveness of these algorithms and the feasibility of this system by experimental results. This system has many applications including information retrieval and law enforcement.","cites":"13","conferencePercentile":"46.96969697"},{"venue":"WACV","id":"2ec55c3fb5fa493ebfacc58115cf28f283a50a02","venue_1":"WACV","year":"2015","title":"How to Transfer? Zero-Shot Object Recognition via Hierarchical Transfer of Semantic Attributes","authors":"Ziad Al-Halah, Rainer Stiefelhagen","author_ids":"2256981, 1742325","abstract":"Attribute based knowledge transfer has proven very successful in visual object analysis and learning previously unseen classes. However, the common approach learns and transfers attributes without taking into consideration the embedded structure between the categories in the source set. Such information provides important cues on the intra-attribute variations. We propose to capture these variations in a hierarchical model that expands the knowledge source with additional abstraction levels of attributes. We also provide a novel transfer approach that can choose the appropriate attributes to be shared with an unseen class. We evaluate our approach on three public datasets: aPascal, Animals with Attributes and CUB-200-2011 Birds. The experiments demonstrate the effectiveness of our model with significant improvement over state-of-the-art.","cites":"5","conferencePercentile":"76.40449438"},{"venue":"WACV","id":"ccb04ca578feecf94e14fb865a7e33b445bb190b","venue_1":"WACV","year":"2011","title":"Image matching with distinctive visual vocabulary","authors":"Hongwen Kang, Martial Hebert, Takeo Kanade","author_ids":"2336356, 1709305, 7642093","abstract":"In this paper we propose an image indexing and matching algorithm that relies on selecting distinctive high dimensional features. In contrast with conventional techniques that treated all features equally, we claim that one can benefit significantly from focusing on distinctive features. We propose a bag-of-words algorithm that combines the feature distinctiveness in visual vocabulary generation. Our approach compares favorably with the state of the art in image matching tasks on the University of Kentucky Recognition Benchmark dataset and on an indoor localization dataset. We also show that our approach scales up more gracefully on a large scale Flickr dataset.","cites":"5","conferencePercentile":"60.31746032"},{"venue":"WACV","id":"166ea975c70459756814ee359a615ab00fe3e669","venue_1":"WACV","year":"1996","title":"Real-time recognition of activity using temporal templates","authors":"Aaron F. Bobick, James W. Davis","author_ids":"1688328, 1714040","abstract":"A new view-based approach to the representation and recognition of action is presented. The basis of the representation is a motion-history image (MHI) | a static image where intensity is a function of the recency of motion in a sequence. We develop a recognition method which uses both binary and scalar-valued versions of the MHI as temporal templates to match against stored instances of actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on a standard platform. The applications we have begin to develop include simple room monitoring and an interactive game.","cites":"76","conferencePercentile":"91.66666667"},{"venue":"WACV","id":"255f47b547409b12db592ef00d682eb0673df28d","venue_1":"WACV","year":"2002","title":"Automatic Detection of Signs with Affine Transformation","authors":"Xilin Chen, Jie Yang, Jing Zhang, Alexander H. Waibel","author_ids":"1710220, 1688428, 2733196, 4500589","abstract":"In this paper, we propose an approach for detecting signs from natural scenes. The approach efficiently embeds multi-resolution, adaptive search, and affine rectification algorithms in a hierarchical framework, with different emphases at each layer. We combine multi-resolution and multi-scale edge detection techniques to effectively detect text in different sizes. Different from the existing approaches, by using the cues from text inside the image, we introduce affine rectification transformation to recover deformation of the text region caused by an inappropriate camera view angle. This procedure can significantly improve text detection rate and OCR (Optical Character Recognition) accuracy. Experimental results have demonstrated feasibility of the proposed algorithms. We have applied the proposed approach to a Chinese sign translation system, which can automatically detect Chinese text input from a camera, recognize, and translate the recognized text into English or voice stream.","cites":"8","conferencePercentile":"31.81818182"},{"venue":"WACV","id":"79c2de91e7b5e721b3e3aff4abf6d5d6128c00be","venue_1":"WACV","year":"2013","title":"Robust rank-4 affine factorization for structure from motion","authors":"Guanghui Wang, John S. Zelek, Q. M. Jonathan Wu, Ruzena Bajcsy","author_ids":"1792900, 1702003, 8716302, 1784213","abstract":"The paper focuses on 3D structure and motion factor-ization from uncalibrated image sequences. A rank-4 affine factorization algorithm and a robust structure and motion factorization scheme are proposed to handle outlying and missing data. The novelty and main contribution of the paper are as follows: (i) The rank-4 factorization algorithm is a new addition to previous affine factorization family using rank-3 constraint; (ii) the outliers and image uncertainty are estimated directly from the image reprojection residu-als; and (iii) the robust factorization scheme is proved empirically to be more efficient and accurate than other robust algorithms. Extensive experiments on synthetic data and real images validate the proposed approach.","cites":"4","conferencePercentile":"57.84313725"},{"venue":"WACV","id":"086328c9ac12db1e5464a8468b43ed956b13afad","venue_1":"WACV","year":"2015","title":"Photometric Stereo in the Wild","authors":"Chun Ho Hung, Tai-Pang Wu, Yasuyuki Matsushita, Li Xu, Jiaya Jia, Chi-Keung Tang","author_ids":"2487776, 1817118, 1774618, 6736904, 1739989, 2546217","abstract":"Conventional photometric stereo requires to capture images or videos in a dark room to obstruct complex environment light as much as possible. This paper presents a new method that capitalizes on environment light to avail geometry reconstruction, thus bringing photometric stereo to the wild, such as an outdoor scene, with uncontrolled lighting. We do not make restrictive assumption, and only use simple capture equipments, which include a mirror sphere and a video camera. Qualitative and quantitative experiments indicate the potential and practicality of our system to generalize existing frameworks.","cites":"3","conferencePercentile":"65.73033708"},{"venue":"WACV","id":"155ec3d1522515286d140bf673a800082596f261","venue_1":"WACV","year":"2007","title":"Automated Insect Identification through Concatenated Histograms of Local Appearance Features","authors":"Natalia Larios, Hongli Deng, Wei Zhang, Matt Sarpola, Jenny Yuen, Robert Paasch, Andrew Moldenke, David A. Lytle, Ruiz Correa, Eric N. Mortensen, Linda G. Shapiro, Thomas G. Dietterich","author_ids":"2022023, 2860015, 2920545, 7362977, 1761137, 2939731, 2091712, 4231109, 1851626, 1890531, 1809809, 1699720","abstract":"This paper describes a fully automated stonefly-larvae classification system using a local features approach. It compares the three region detectors employed by the system: the Hessian-affine detector, the Kadir entropy detector and a new detector we have developed called the principal curvature based region detector (PCBR). It introduces a concatenated feature histogram (CFH) methodology that uses histograms of local region descriptors as feature vectors for classification and compares the results using this methodology to that of Opelt [11] on three stonefly identification tasks. Our results indicate that the PCBR detector outperforms the other two detectors on the most difficult discrimination task and that the use of all three detectors outperforms any other configuration. The CFH methodology also outperforms the Opelt methodology in these tasks.","cites":"19","conferencePercentile":"93.93939394"},{"venue":"WACV","id":"6339e9385ae3609cb22f6b87175c7e6850f2c05b","venue_1":"WACV","year":"2012","title":"Estimating the spatial extents of geospatial objects using hierarchical models","authors":"Yi Yang, Shawn D. Newsam","author_ids":"1698559, 1705135","abstract":"The goal of this work is to estimate the spatial extents of complex geospatial objects such as high schools and golf courses. Gazetteers are deficient in that they currently specify the spatial extents of these objects using a single lat-itude/longitude point. We propose a framework that uses readily available high resolution overhead imagery to estimate the boundaries of known object instances in order to update the gazetteers. Key to our approach is a hierarchical object model with three levels. The lowest level characterizes an object using local invariant features; an intermediate, latent level characterizes the land-use/land-cover (LULC) classes that constitute an object; and, the top level models an object as a distribution over these classes. We evaluate our approach using a manually labeled ground truth dataset of four object types: high schools, golf courses, mobile home parks, and Costco shopping centers.","cites":"2","conferencePercentile":"36.66666667"},{"venue":"WACV","id":"1b161e2a2a5735380455e4692077e7cc3cc7a549","venue_1":"WACV","year":"2011","title":"Cell image analysis: Algorithms, system and applications","authors":"Takeo Kanade, Zhaozheng Yin, Ryoma Bise, Seungil Huh, Sungeun Eom, Michael F. Sandbothe, Mei Chen","author_ids":"7642093, 2466689, 3230086, 7735839, 2257433, 3299779, 1809936","abstract":"We present several algorithms for cell image analysis including microscopy image restoration, cell event detection and cell tracking in a large population. The algorithms are integrated into an automated system capable of quantifying cell proliferation metrics in vitro in real-time. This offers unique opportunities for biological applications such as efficient cell behavior discovery in response to different cell culturing conditions and adaptive experiment control. We quantitatively evaluated our system's performance on 16 microscopy image sequences with satisfactory accuracy for biologists' need. We have also developed a public web-site compatible to the system's local user interface, thereby allowing biologists to conveniently check their experiment progress online. The website will serve as a community resource that allows other research groups to upload their cell images for analysis and comparison.","cites":"22","conferencePercentile":"96.82539683"},{"venue":"WACV","id":"c948928fb2ec16deeefe83ebe1dfb828c18a2d45","venue_1":"WACV","year":"2014","title":"\"Important stuff, everywhere!\" Activity recognition with salient proto-objects as context","authors":"Lukas Rybok, Boris Schauerte, Ziad Al-Halah, Rainer Stiefelhagen","author_ids":"3195828, 2602925, 2256981, 1742325","abstract":"Object information is an important cue to discriminate between activities that draw part of their meaning from context. Most of current work either ignores this information or relies on specific object detectors. However, such object detectors require a significant amount of training data and complicate the transfer of the action recognition framework to novel domains with different objects and object-action relationships. Motivated by recent advances in saliency detection, we propose to use proto-objects to detect object candidate regions in videos without any need of prior knowledge. Our experimental evaluation on three publicly available data sets shows that the integration of proto-objects and simple motion features substantially improves recognition performance, outperforming the state-of-the-art.","cites":"1","conferencePercentile":"30.1980198"},{"venue":"WACV","id":"5db9717372332e7d102b41ef3ea7b47b0d43d5eb","venue_1":"WACV","year":"2015","title":"Composition Context Photography","authors":"Daniel A. Vaquero, Matthew Turk","author_ids":"2000950, 1752714","abstract":"Cameras are becoming increasingly aware of the picture-taking context, collecting extra information around the act of photographing. This contextual information enables the computational generation of a wide range of enhanced photographic outputs, effectively expanding the imaging experience provided by consumer cameras. Computer vision and computational photography techniques can be applied to provide image composites, such as panoramas , high dynamic range images, and stroboscopic images, as well as automatically selecting individual alternative frames. Our technology can be integrated into point-and-shoot cameras, and it effectively expands the photographic possibilities for casual and amateur users, who often rely on automatic camera modes.","cites":"1","conferencePercentile":"35.39325843"},{"venue":"WACV","id":"179902857c6bbfec2b40d7fc02858b2c0b3d42bb","venue_1":"WACV","year":"2012","title":"Predicting human gaze using quaternion DCT image signature saliency and face detection","authors":"Boris Schauerte, Rainer Stiefelhagen","author_ids":"2602925, 1742325","abstract":"We combine and extend the previous work on DCT-based image signatures and face detection to determine the visual saliency. To this end, we transfer the scalar definition of image signatures to quaternion images and thus introduce a novel saliency method using quaternion type-II DCT image signatures. Furthermore, we use MCT-based face detection to model the important influence of faces on the visual saliency using rotated elliptical Gaussian weight functions and evaluate several integration schemes. In order to demonstrate the performance of the proposed methods, we evaluate our approach on the Bruce-Tsotsos (Toronto) [2] and Cerf (FIFA) [3] benchmark eye-tracking data sets. Additionally, we present evaluation results on the Bruce-Tsotsos data set of the most important spectral saliency approaches. We achieve state-of-the-art results in terms of the well-established area under curve (AUC) measure on the Bruce-Tsotsos data set and come close to the ideal AUC on the Cerf data set – with less than one millisecond to calculate the bottom-up QDCT saliency map.","cites":"14","conferencePercentile":"91.11111111"},{"venue":"WACV","id":"448c2e735d98ff6409f1eb74d5d9b0f1b0b14210","venue_1":"WACV","year":"2011","title":"Car-Rec: A real time car recognition system","authors":"Daniel Marcus Jang, Matthew Turk","author_ids":"1897803, 1752714","abstract":"Recent advances in computer vision have significantly reduced the difficulty of object classification and recognition. Robust feature detector and descriptor algorithms are particularly useful, forming the basis for many recognition and classification applications. These algorithms have been used in divergent bag-of-words and structural matching approaches. This work demonstrates a recognition application , based upon the SURF feature descriptor algorithm, which fuses bag-of-words and structural verification techniques. The resulting system is applied to the domain of car recognition and achieves accurate (> 90%) and real-time performance when searching databases containing thousands of images.","cites":"11","conferencePercentile":"86.50793651"},{"venue":"WACV","id":"100105d6c97b23059f7aa70589ead2f61969fbc3","venue_1":"WACV","year":"2016","title":"Frontal to profile face verification in the wild","authors":"Soumyadip Sengupta, Jun-Cheng Chen, Carlos D. Castillo, Vishal M. Patel, Rama Chellappa, David W. Jacobs","author_ids":"2500202, 3109779, 2043486, 1741177, 1712305, 1771485","abstract":"We have collected a new face data set that will facilitate research in the problem of frontal to profile face verification 'in the wild'. The aim of this data set is to isolate the factor of pose variation in terms of extreme poses like profile , where many features are occluded, along with other 'in the wild' variations. We call this data set the Celebrities in Frontal-Profile (CFP) data set. We find that human performance on Frontal-Profile verification in this data set is only slightly worse (94.57% accuracy) than that on Frontal-Frontal verification (96.24% accuracy). However we evaluated many state-of-the-art algorithms, including Fisher Vector, Sub-SML and a Deep learning algorithm. We observe that all of them degrade more than 10% from Frontal-Frontal to Frontal-Profile verification. The Deep learning implementation, which performs comparable to humans on Frontal-Frontal, performs significantly worse (84.91% accuracy) on Frontal-Profile. This suggests that there is a gap between human performance and automatic face recognition methods for large pose variation in unconstrained images .","cites":"1","conferencePercentile":"47.72727273"},{"venue":"WACV","id":"85d953e60f1b2bc8705b64818eaf7ea9e480c618","venue_1":"WACV","year":"2016","title":"Recognition of ongoing complex activities by sequence prediction over a hierarchical label space","authors":"Wenbin Li, Mario Fritz","author_ids":"2394155, 1739548","abstract":"Human activity recognition from full video sequence has been extensively studied. Recently, there has been increasing interest in early recognition or recognition from partial observation. However, from a small fraction of the video, it might be demanding if not even impossible to make a fine grained prediction of the activity that is taking place. Therefore, we propose the first method to predict ongoing activities over a hierarchical label space. We approach this task as a sequence prediction problem in a recurrent neural network where we predict over a hierarchical label space of activities. Our model learns to realize accuracy-specificity trade-offs over time by starting with coarse labels and proceeding to more fine grained recognition as more evidence becomes available in order to meet a prescribed target accuracy. In order to study this task we have collected a large video dataset of complex activities with long duration. The activities are annotated in a hierarchical label space from coarse to fine. By directly training a sequence predictor over the hierarchical label space, our method outperforms several baselines including prior work on accuracy speci-ficity tradeoffs originally developed for object recognition.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"258436cc2066e6162d14e41bc314479a44fb3f00","venue_1":"WACV","year":"2013","title":"Handwritten text segmentation using average longest path algorithm","authors":"Dhaval Salvi, Jun Zhou, Jarrell W. Waggoner, Song Wang","author_ids":"2968009, 1728391, 1778332, 4549950","abstract":"Offline handwritten text recognition is a very challenging problem. Aside from the large variation of different handwriting styles, neighboring characters within a word are usually connected, and we may need to segment a word into individual characters for accurate character recognition. Many existing methods achieve text segmentation by evaluating the local stroke geometry and imposing constraints on the size of each resulting character, such as the character width, height and aspect ratio. These constraints are well suited for printed texts, but may not hold for handwritten texts. Other methods apply holistic approach by using a set of lexicons to guide and correct the segmentation and recognition. This approach may fail when the lexicon domain is insufficient. In this paper, we present a new global non-holistic method for handwritten text segmentation, which does not make any limiting assumptions on the character size and the number of characters in a word. Specifically,the proposed method finds the text segmentation with the maximum average likeliness for the resulting characters. For this purpose, we use a graph model that describes the possible locations for segmenting neighboring characters, and we then develop an average longest path algorithm to identify the globally optimal segmentation. We conduct experiments on real images of handwritten texts taken from the IAM handwriting database and compare the performance of the proposed method against an existing text segmenta-tion algorithm that uses dynamic programming.","cites":"8","conferencePercentile":"75.49019608"},{"venue":"WACV","id":"a0abec0939e93aa986580f1e360d668dbd33ccca","venue_1":"WACV","year":"2007","title":"Building Adaptive Camera Models for Video Surveillance","authors":"James W. Davis, Alexander M. Morison, David D. Woods","author_ids":"1714040, 2593754, 1928242","abstract":"We address the limited automatic scanning functionality of standard PTZ camera systems. We present an adaptive, scene-specific model using standard PTZ camera hardware. The adaptive model is constructed automatically by detecting human activity in Motion History Images (MHIs) using an iterative candidacy-classification-reduction process. The target motion is quantified and employed in the construction of a global Activity Map, which in turn is used to direct or navigate the camera.","cites":"7","conferencePercentile":"63.63636364"},{"venue":"WACV","id":"35c97cacb3999c34d0ada489faa680c0e083193a","venue_1":"WACV","year":"2016","title":"Discovering picturesque highlights from egocentric vacation videos","authors":"Vinay Bettadapura, Daniel Castro, Irfan A. Essa","author_ids":"3115428, 7367198, 1714295","abstract":"We present an approach for identifying picturesque highlights from large amounts of egocentric video data. Given a set of egocentric videos captured over the course of a vacation , our method analyzes the videos and looks for images that have good picturesque and artistic properties. We introduce novel techniques to automatically determine aesthetic features such as composition, symmetry and color vibrancy in egocentric videos and rank the video frames based on their photographic qualities to generate highlights. Our approach also uses contextual information such as GPS, when available, to assess the relative importance of each geographic location where the vacation videos were shot. Furthermore, we specifically leverage the properties of egocentric videos to improve our highlight detection. We demonstrate results on a new egocentric vacation dataset which includes 26.5 hours of videos taken over a 14 day vacation that spans many famous tourist destinations and also provide results from a user-study to access our results.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"b5a2ad2a5e8da82e3aa5bbd423a6239628e747cb","venue_1":"WACV","year":"2007","title":"Threshold-based 3D Tumor Segmentation using Level Set (TSL)","authors":"Sima Taheri, Sim Heng Ong, Vincent Chong","author_ids":"3217528, 1685644, 7723900","abstract":"Three-dimensional segmentation is reliable approach to achieve a proper estimation of tumor volume. Among all possible methods for this purpose, level set can be used as a powerful tool which implicitly extracts the tumor surface. In this paper, we introduce a threshold-based algorithm for 3D tumor segmentation using level set (TSL). This algorithm uses a global threshold to form the level set speed function which is updated iteratively throughout the level set growing process. An important feature of TSL is that no explicit knowledge about the tumor and non-tumor density functions is required. The proposed method can be implemented in the automatic and semi-automatic forms depending on the complexity of the tumor shape. TSL is examined on several clinical MRIs for both visual and quantitative evaluation. Experimental results demonstrate the effectiveness of our approach.","cites":"3","conferencePercentile":"36.36363636"},{"venue":"WACV","id":"9901f473aeea177a55e58bac8fd4f1b086e575a4","venue_1":"WACV","year":"2016","title":"Human and Sheep Facial Landmarks Localisation by Triplet Interpolated Features","authors":"Heng Yang, Renqiao Zhang, Peter Robinson","author_ids":"2389837, 2271111, 1780152","abstract":"In this paper we present a method for localisation of facial landmarks on human and sheep. We introduce a new feature extraction scheme called triplet-interpolated feature used at each iteration of the cascaded shape regression framework. It is able to extract features from similar semantic location given an estimated shape, even when head pose variations are large and the facial landmarks are very sparsely distributed. Furthermore, we study the impact of training data imbalance on model performance and propose a training sample augmentation scheme that produces more initialisations for training samples from the minority. More specifically, the augmentation number for a training sample is made to be negatively correlated to the value of the fitted probability density function at the sam-ple's position. We evaluate the proposed scheme on both human and sheep facial landmarks localisation. On the benchmark 300w human face dataset, we demonstrate the benefits of our proposed methods and show very competitive performance when comparing to other methods. On a newly created sheep face dataset, we get very good performance despite the fact that we only have a limited number of training samples and a set of sparse landmarks are annotated .","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"f75c03f8a20708d1e090589e992c79af2c7efe15","venue_1":"WACV","year":"1998","title":"3D reconstruction of environments for virtual collaboration","authors":"Ruzena Bajcsy, Reyes Enciso, Gerda Kamberova, Lucien Nocera, Radim Sára","author_ids":"1784213, 3033722, 2467777, 1925660, 2192813","abstract":"In this paper we address an application of computer vision which can in the future change completely our way of communicating over the network. We present our version of a testbed for telecollaboration. It is based on a highly accurate and precise stereo algorithm. The results demonstrate the live (on-line) recovery of 3D models of a dynamically changing environment and the simultaneous display and manipulation of the models 1 .","cites":"25","conferencePercentile":"90.54054054"},{"venue":"WACV","id":"a2766412073f9016e36212052f6192bd2bc8cc2c","venue_1":"WACV","year":"2016","title":"OpenFace: An open source facial behavior analysis toolkit","authors":"Tadas Baltrusaitis, Peter Robinson, Louis-Philippe Morency","author_ids":"1756344, 1780152, 1767184","abstract":"Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace – an open source tool intended for computer vision and machine learning researchers , affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation , facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple we-bcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.","cites":"14","conferencePercentile":"96.96969697"},{"venue":"WACV","id":"e428ae898ffb414bfea3d23a08609da018daf85b","venue_1":"WACV","year":"2002","title":"Eye Typing using Markov and Active Appearance Models","authors":"Dan Witzner Hansen, John Paulin Hansen, Mads Nielsen, Anders Sewerin Johansen, Mikkel B. Stegmann","author_ids":"2159660, 1688272, 1777184, 8675753, 2458578","abstract":"We propose a non-intrusive eye tracking system intended for the use of everyday gaze typing using web cameras. We argue that high precision in gaze tracking is not needed for on-screen typing due to natural language redundancy. This facilitates the use of low-cost video components for advanced multi-modal interactions based on video tracking systems. Robust methods are needed to track the eyes using web cameras due to the poor image quality. A real-time tracking scheme using a mean-shift color tracker and an Active Appearance Model of the eye is proposed. It is possible from this model to infer the state of the eye such as eye corners and the pupil location under scale and rota-tional changes.","cites":"34","conferencePercentile":"89.39393939"},{"venue":"WACV","id":"c752e5d3df03cf218e0b2daa742538c1ce43c1fd","venue_1":"WACV","year":"1998","title":"Applications of omnidirectional imaging: multi-body tracking and remote reality","authors":"Terrance E. Boult, Chen Qian, Weihong Yin, Ali Erkin, Peter Lewis, Chris Power, Ross J. Micheals","author_ids":"1760117, 1759935, 2356144, 2370146, 4874264, 3352978, 3107704","abstract":"Recently, S. Nayar introduced a parabolic imaging system that has a field of view of a full hemisphere or more. When used with a video camera the result is an omni-directional video stream that captures everything going around it. In the VAST Lab at Lehigh, we have been experimenting with these cameras, developing new variants, and developing omni-directional vision applications. We present an overview of omni-directional imaging and then two of our applications which we will be demonstrating. The first application is a frame-rate multi-body tracking system. The system uses an omni-directional imager and a standard PC to track multiple moving objects in all directions. The system is designed to provide perspective views of the most significant targets, either locally or over a network. The second application is something we call Remote Reality , which provides an immersive environment via omni-directional imaging. It can use live or pre-recorded video from a remote location. While less interactive than traditional VR, remote reality has important advantages: there is no need for \" model building \" and the objects, textures and motions are not graphical approximations. Recent research, [?], has revolutionized wide-field of view imaging by introducing the paracamera, a system that directly captures a full hemisphere (or more) while maintaining a single perspective viewpoint. Because it captures the viewing hemisphere (or more) simultaneously it be used for full motion video. Furthermore, placing two paracamera systems back-to-back allows a true viewing sphere, i.e. 360 x 360 viewing. Unlike fish-eye lenses, each image in the paracamera system can be processed to generate geometrically correct perspective images in any direction within the viewing hemisphere. The omni-directional imager combines an orthographic lens and a parabolic mirror, where the axis of the parabolic mirror is parallel to the optic axis. Because the lens is or-thographic, entering rays are parallel. By definition, rays parallel to the axis reflect off a parabolic surface at an angle such that they virtually intersect at the focus of the parabolic surface. Thus the focus of the paracamera provides a single \" virtual \" viewpoint. The single virtual viewpoint allows for consistent interpretation of the world in any viewing direction. To generate a proper perspective image from the para-image, consider an \" imaging array \" in the desired viewing direction. For each pixel, it logically casts rays through the focus and intersects the measured image. The resulting spatially varying resampling can be very …","cites":"9","conferencePercentile":"52.7027027"},{"venue":"WACV","id":"eb160af385ff16c5a39119d454103f2e7d152b38","venue_1":"WACV","year":"2011","title":"Generalized autofocus","authors":"Daniel A. Vaquero, Natasha Gelfand, Marius Tico, Kari Pulli, Matthew Turk","author_ids":"2000950, 1683095, 1717236, 1704409, 1752714","abstract":"All-in-focus imaging is a computational photography technique that produces images free of defocus blur by capturing a stack of images focused at different distances and merging them into a single sharp result. Current approaches assume that images have been captured offline, and that a reasonably powerful computer is available to process them. In contrast, we focus on the problem of how to capture such input stacks in an efficient and scene-adaptive fashion. Inspired by passive autofocus techniques, which select a single best plane of focus in the scene, we propose a method to automatically select a minimal set of images, focused at different depths, such that all objects in a given scene are in focus in at least one image. We aim to minimize both the amount of time spent metering the scene and capturing the images, and the total amount of high-resolution data that is captured. The algorithm first analyzes a set of low-resolution sharpness measurements of the scene while continuously varying the focus distance of the lens. From these measurements, we estimate the final lens positions required to capture all objects in the scene in acceptable focus. We demonstrate the use of our technique in a mobile computational photography scenario, where it is essential to minimize image capture time (as the camera is typically handheld) and processing time (as the computation and energy resources are limited).","cites":"10","conferencePercentile":"83.33333333"},{"venue":"WACV","id":"0ea21e64a04ad23e508452cf49f07d72dcac69ca","venue_1":"WACV","year":"2000","title":"Creating 3D models with uncalibrated cameras","authors":"Mei Han, Takeo Kanade","author_ids":"2939096, 7642093","abstract":"We describe a factorization-based method to recover 3D models from multiple perspective views with uncalibrated cameras. The method first performs a projective reconstruction using a bilinear factorization algorithm, and then converts the projective solution to a Euclidean one by enforcing metric constraints. We present three factorization-based normalization algorithms to generate the Euclidean reconstruction and the intrinsic parameters, assuming zero skews. The first two algorithms are linear, one for dealing with the case that only the focal lengths are unknown, and another for the case that the focal lengths and the constant principal point are unknown. The third algorithm is bilinear, dealing with the case that the focal lengths, the principal points and the aspect ratios are all unknown. We present the results of applying this method to building mod-eling, terrain recovery and multi-camera calibration.","cites":"30","conferencePercentile":"73.52941176"},{"venue":"WACV","id":"d9159ff532e33764334d76216bb93da28cf2f7ac","venue_1":"WACV","year":"2002","title":"Boosting Image Orientation Detection with Indoor vs. Outdoor Classification","authors":"Lei Zhang, Mingjing Li, HongJiang Zhang","author_ids":"1724298, 8392859, 1718558","abstract":"Automatic detection of image orientation is a very important operation in photo image management. In this paper, we propose an automated method based on the boosting algorithm to estimate image orientations. The proposed method has the capability of rejecting images based on the confidence score of the orientation detection. Also, images are classified into indoor and outdoor, and this classification result is used to further refine the orientation detection. To select features more sensitive to the rotation, we combine the features by subtraction operation and select the most useful features by boosting algorithm. The proposed method has several advantages: small model size, fast classification speed, and effective rejection scheme.","cites":"34","conferencePercentile":"89.39393939"},{"venue":"WACV","id":"2c27c6ea8e3f7e40f0fb3a7c8f6ddbd3dd2fb8bd","venue_1":"WACV","year":"2002","title":"A Real-time Precrash Vehicle Detection System","authors":"Zehang Sun, Ronald Miller, George Bebis, David DiMeo","author_ids":"1768524, 8174368, 1808451, 2629080","abstract":"— This paper presents an in-vehicle real-time monocular precrash vehicle detection system. The system acquires grey level images through a forward facing low light camera and achieves an average detection rate of 10Hz. The vehicle detection algorithm consists of two main steps: multi-scale driven hypothesis generation and appearance-based hypothesis verification. In the multi-scale hypothesis generation step, possible image locations where vehicles might be present are hypothesized. This step uses multi-scale techniques to speed up detection but also to improve system robustness by making system performance less sensitive to the choice of certain parameters. Appearance-based hypothesis verification verifies those hypothesis using Haar Wavelet decomposition for feature extraction and Support Vector Machines (SVMs) for classification. The monocu-lar system was tested under different traffic scenarios (e.g., simply structured highway, complex urban street, varying weather conditions), illustrating good performance.","cites":"26","conferencePercentile":"83.33333333"},{"venue":"WACV","id":"238f911b7400048b962fb77fb7191d388d35ca4a","venue_1":"WACV","year":"2002","title":"Genetic Feature Subset Selection for Gender Classification: A Comparison Study","authors":"Zehang Sun, George Bebis, Xiaojing Yuan, Sushil J. Louis","author_ids":"1768524, 1808451, 2157647, 1736465","abstract":"— We consider the problem of gender classification from frontal facial images using genetic feature subset selection. We argue that feature selection is an important issue in gender classification and demonstrate that Genetic Algorithms (GA) can select good subsets of features (i.e., features that encode mostly gender information), reducing the classification error. First, Principal Component Analysis (PCA) is used to represent each image as a feature vector (i.e., eigen-features) in a low-dimensional space. Genetic Algorithms (GAs) are then employed to select a subset of features from the low-dimensional representation by disregarding certain eigenvectors that do not seem to encode important gender information. Four different classifiers were compared in this study using genetic feature subset selection: a Bayes classifier, a Neural Network (NN) classifier, a Support Vector Machine (SVM) classifier, and a classifier based on Linear Discriminant Analysis (LDA). Our experimental results show a significant error rate reduction in all cases. The best performance was obtained using the SVM classifier. Using only 8.4% of the features in the complete set, the SVM classifier achieved an error rate of 4.7% from an average error rate of 8.9% using manually selected features .","cites":"68","conferencePercentile":"93.93939394"},{"venue":"WACV","id":"0324a22f71927bee2a448f800287cde562dc2726","venue_1":"WACV","year":"2016","title":"People detection in crowded scenes by context-driven label propagation","authors":"Jingjing Liu, Quanfu Fan, Sharath Pankanti, Dimitris N. Metaxas","author_ids":"3889747, 3024917, 1767897, 1711560","abstract":"People detection in images is a fundamental vision problem, which is central to a wide range of applications such as video surveillance, robotics and autonomous driving. Along other effects, exploiting contextual cues has been a key idea to improve people detection in crowded scenes. In this talk, I will present a unified framework for people detection that integrates visual recognition with graph-based context modeling. The detection task is formulated as an optimization problem where the goal is to find a maximum set of human hypotheses that agree on both visual detection and their contextual interactions in an image. While such an optimization is not theoretically tractable, we show that it can be approximately addressed by label propagation in a progressive way. Label propagation was originally proposed for predicting unlabeled instances from labeled data. It propagates labels to data in the same class iteratively by proximity. In our case, true detections are supposed to be contextually compatible with each other, but irrelevant to false alarms. This suggests that strong detections with high confidence can boost up weak ones by spreading rewards through contex-tual proximity and meanwhile penalize false positives according to contextual incompatibility, in a similar spirit to label propagation. Our approach was validated using two challenging crowd datasets, one for people detection with variations in pose and size, and the other for pedestrian detection in low resolution images. The results confirm that our method can significantly improve people detection in crowded scenarios, achieving performance comparable to state of the art reported in the literature.","cites":"1","conferencePercentile":"47.72727273"},{"venue":"WACV","id":"017e04898b4a5bb1531866ac57537facaf40b97c","venue_1":"WACV","year":"1998","title":"Video occupant detection for airbag deployment","authors":"John Krumm, Greg Kirk","author_ids":"1690256, 3255264","abstract":"When an airbag deploys on a rear-facing infant seat, it can injure or kill the infant. When an airbag deploys on an empty seat, the airbag and the money to replace it are wasted. We have shown that video images can be used to determine whether or not to deploy the passenger-side airbag in a crash. Images of the passenger seat, taken from a video camera mounted inside the vehicle, can be used to classify the seat as either empty, containing a rear-facing infant seat, or occupied. Our first experiment used a single, monochrome video camera. The system was automatically trained on a series of test images. Using a principle components (eigenimages) nearest neighbor classifier, it achieved a correct classification rate of 99.5% on a test of 910 images. Our second experiment used a pair of monochrome video cameras to compute stereo disparity (a function of 3D range) instead of intensity images. Using a similar algorithm, the second approach achieved a correct classification rate of 95.1% on a test of 890 images. The stereo technique has the advantage of being less sensitive to illumination, and would likely work best in a real system.","cites":"17","conferencePercentile":"79.72972973"},{"venue":"WACV","id":"db5a046aa454cbdf762fc1f9743104f30655356a","venue_1":"WACV","year":"2011","title":"Classification of image registration problems using support vector machines","authors":"Steve Oldridge, Sidney Fels, Gregor Miller","author_ids":"1788011, 1749457, 1781538","abstract":"This paper introduces a system that automatically classifies image pairs based on the type of registration required to align them. The system uses support vector machines to classify between panoramas, high-dynamic-range images , focal stacks, super-resolution, and unrelated image pairs. A feature vector was developed to describe the images , and 1100 pairs were used to train and test the system with 5-fold cross validation. The system is able to classify the desired registration application using a 1:Many classi-fier with an accuracy of 91.18%. Similarly 1:1 classifiers were developed for each class with classification rates as follows: Panorama image pairs are classified at 93.15%, high-dynamic-range pairs at 97.56%, focal stack pairs at 95.68%, super-resolution pairs at 99.25%, and finally unrelated image pairs at 95.79%. An investigation into feature importance outlines the utility of each feature individually. In addition, the invariance of the classification system towards the size of the image used to calculate the feature vector was explored. The classification of our system remains level at˜91% until the image size is scaled to 10% (150 x 100 pixels), suggesting that our feature vector is image size invariant within this range.","cites":"3","conferencePercentile":"46.82539683"},{"venue":"WACV","id":"c1c094ac5469e7a254dca842b7c6469a0c33e621","venue_1":"WACV","year":"2014","title":"Linear Local Distance coding for classification of HEp-2 staining patterns","authors":"Xiang Xu, Feng Lin, Carol Ng, Khai Pang Leong","author_ids":"3648422, 3673120, 1890391, 2904670","abstract":"Indirect Immunofluorescence (IIF) on Human Epithelial-2 (HEp-2) cells is the recommended methodology for detecting some specific autoimmune diseases by searching for antinuclear antibodies (ANAs) within a patient's serum. Due to the limitations of IIF such as subjective evaluation, automated Computer-Aided Diagnosis (CAD) system is required for diagnostic purposes. In particular, staining patterns classification of HEp-2 cells is a challenging task. In this paper, we adopt a feature extraction-coding-pooling framework which has shown impressive performance in image classification tasks, because it can obtain discrimina-tive and effective image representation. However, the information loss is inevitable in the coding process. Therefore , we propose a Linear Local Distance (LLD) coding method to capture more discriminative information. LLD transforms original local feature to local distance vector by searching for local nearest few neighbors of local feature in the class-specific manifolds. The obtained local distance vector is further encoded and pooled together to get salient image representation. We demonstrate the effectiveness of LLD method on a public HEp-2 cells dataset containing six major staining patterns. Experimental results show that our approach has a superior performance to the state-of-the-art coding methods for staining patterns classification of HEp-2 cells.","cites":"1","conferencePercentile":"30.1980198"},{"venue":"WACV","id":"72472247bd52303d8ac2987d2a5d08962fd72ca4","venue_1":"WACV","year":"2011","title":"An assisted photography method for street scenes","authors":"Marynel Vázquez, Aaron Steinfeld","author_ids":"2312232, 1792714","abstract":"We present an interactive, computational approach for assisting users with visual impairments during photographic documentation of transit problems. Our technique can be described as a method to improve picture composition , while retaining visual information that is expected to be most relevant. Our system considers the position of the estimated region of interest (ROI) of a photo, and camera orientation. Saliency maps and Gestalt theory are used for guiding the user towards a more balanced picture. Our current implementation for mobile phones uses optic flow to update the internal knowledge of the position of the ROI and tilt sensor readings to correct non horizontal or vertical camera orientations. Using ground truth labels, we confirmed our method proposes valid strategies for improving image composition. Future work includes an optimized implementation and user studies.","cites":"8","conferencePercentile":"73.80952381"},{"venue":"WACV","id":"5ca412cb806b0a6706937de9887e821450393503","venue_1":"WACV","year":"2015","title":"Re-ranking by Multi-feature Fusion with Diffusion for Image Retrieval","authors":"Fan Yang, Bogdan Matei, Larry S. Davis","author_ids":"1752128, 2787579, 1693428","abstract":"We present a re-ranking algorithm for image retrieval by fusing multi-feature information. We utilize pairwise similarity scores between images to exploit the underlying relationships among images. The initial ranked list for a query from each feature is represented as an undirect-ed graph, where edge strength comes from feature-specific image similarity. Graphs from multiple features are combined by a mixture Markov model. In addition, we utilize a probabilistic model based on the statistics of similarity scores of similar and dissimilar image pairs to determine the weight for each graph. The weight for a feature is query-specific, where the ranked lists of different queries receive different weights. Our approach for calculating weights is data-driven and does not require any learning. A diffusion process is then applied to the fused graph to reduce noise and achieve better retrieval performance. Experiments demonstrate that our approach significantly improves performance over baseline methods and outperforms many state-of-the-art retrieval methods.","cites":"1","conferencePercentile":"35.39325843"},{"venue":"WACV","id":"2878f5c7f04b468b254307181904d1c5cc568c59","venue_1":"WACV","year":"2015","title":"Motion Segmentation of Truncated Signed Distance Function Based Volumetric Surfaces","authors":"Samunda Perera, Nick Barnes, Xuming He, Shahram Izadi, Pushmeet Kohli, Ben Glocker","author_ids":"1682250, 6217653, 2081936, 1699068, 1685185, 1709824","abstract":"Truncated signed distance function (TSDF) based volu-metric surface reconstructions of static environments can be readily acquired using recent RGB-D camera based mapping systems. If objects in the environment move then a previously obtained TSDF reconstruction is no longer current. Handling this problem requires segmenting moving objects from the reconstruction. To this end, we present a novel solution to the motion segmentation of TSDF volumes. The segmentation problem is cast as CRF-based MAP inference in the voxel space. We propose: a novel data term by solving sparse multi-body motion segmentation and computing likelihoods for each motion label in the RGB-D image space; and, a novel pairwise term based on gradients of the TSDF volume. Experimental evaluation shows that the proposed approach achieves successful segmentations on reconstructions acquired with KinectFusion. Unlike the existing solutions which only work if the objects move completely from their initially occupied spaces, the proposed method permits segmentation of objects when they start to move.","cites":"2","conferencePercentile":"55.61797753"},{"venue":"WACV","id":"5b0140373c6ad2419a8932f760cdb9e40aed15e5","venue_1":"WACV","year":"2015","title":"Estimating Drivable Collision-Free Space from Monocular Video","authors":"Jian Yao, Srikumar Ramalingam, Yuichi Taguchi, Yohei Miki, Raquel Urtasun","author_ids":"5559215, 1699414, 2246577, 2192947, 2422559","abstract":"In this paper we propose a novel algorithm for estimating the drivable collision-free space for autonomous navigation of on-road and on-water vehicles. In contrast to previous approaches that use stereo cameras or LIDAR, we show a method to solve this problem using a single camera. Inspired by the success of many vision algorithms that employ dynamic programming for efficient inference, we reduce the free space estimation task to an inference problem on a 1D graph, where each node represents a column in the image and its label denotes a position that separates the free space from the obstacles. Our algorithm exploits several image and geometric features based on edges, color, and homography to define potential functions on the 1D graph, whose parameters are learned through structured SVM. We show promising results on the challenging KITTI dataset as well as video collected from boats.","cites":"3","conferencePercentile":"65.73033708"},{"venue":"WACV","id":"468a15b86aac2275e3d254b1eb1906a56f34e570","venue_1":"WACV","year":"2011","title":"Fast and scalable keypoint recognition and image retrieval using binary codes","authors":"Jonathan Ventura, Tobias Höllerer","author_ids":"2199678, 1743721","abstract":"In this paper we report an evaluation of keypoint descrip-tor compression using as little as 16 bits to describe a single keypoint. We use spectral hashing to compress keypoint de-scriptors, and match them using the Hamming distance. By indexing the keypoints in a binary tree, we can quickly recognize keypoints with a very small database, and efficiently insert new keypoints. Our tests using image datasets with perspective distortion show the method to enable fast key-point recognition and image retrieval with a small code size, and point towards potential applications for scalable visual SLAM on mobile phones.","cites":"2","conferencePercentile":"30.95238095"},{"venue":"WACV","id":"d5b21060533faf92baf594022dfbcb28c3fa063a","venue_1":"WACV","year":"1996","title":"Identifying nude pictures","authors":"David A. Forsyth, Margaret M. Fleck","author_ids":"1744452, 1735767","abstract":"This paper demonstrates an automatic system for telling whether there are naked people present in an image. The approach combines color and texture properties to obtain a mask for skin regions, which is shown to be eeective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human gure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on color and texture and constraints on geometric properties such as the structure of individual parts and the relationships between parts. The system demonstrates excellent performance on a test set of 565 uncontrolled images of naked people , mostly obtained from the internet, and 4289 assorted control images, drawn from a wide collection of sources.","cites":"30","conferencePercentile":"77.08333333"},{"venue":"WACV","id":"bf60d17ee7320a6c6877687123d5b8318674d3fb","venue_1":"WACV","year":"1998","title":"A qualitative approach to classifying head and eye pose","authors":"Paul A. Beardsley","author_ids":"1777539","abstract":"The goal of this work is to classify the focus of attention of a subject who is switching his or her attention between a number of surrounding objects. The speciic application is to classify the focus of attention of a car driver as straight-ahead, towards the rear-view mirror, towards the dashboard etc. An explicit quantitative approach to this problem requires a a priori information about the interior geometry of the car and the calibration of the camera, and b accurate computation of the subject's location and eye direction. This paper describes a qualitative approach. The subject is observed over an extended p e-riod of time, a n d a a p ose-space histogram\" is used t o record the frequency with which particular head poses occur. For observation of a c ar driver, peaks will appear in the histogram corresponding to the most frequently viewed directions-straight-ahead and toward the mirrors. Each peak is labelled, and the head pose of the driver in all subsequent images is then classiied by use of the histogram. The head pose classiication is reened by a qualitative measurement of the eye pose.","cites":"5","conferencePercentile":"41.89189189"},{"venue":"WACV","id":"3a6658210fc8c1837c61eb61381addd10787e78a","venue_1":"WACV","year":"2014","title":"Car make and model recognition using 3D curve alignment","authors":"Edward Hsiao, Sudipta N. Sinha, Krishnan Ramnath, Simon Baker, C. Lawrence Zitnick, Richard Szeliski","author_ids":"2878557, 1757937, 1751663, 1737297, 1699161, 1717841","abstract":"We present a new approach for recognizing the make and model of a car from a single image. While most previous methods are restricted to fixed or limited viewpoints, our system is able to verify a car's make and model from an arbitrary view. Our model consists of 3D space curves obtained by backprojecting image curves onto silhouette-based visual hulls and then refining them using three-view curve matching. These 3D curves are then matched to 2D image curves using a 3D view-based alignment technique. We present two different methods for estimating the pose of a car, which we then use to initialize the 3D curve matching. Our approach is able to verify the exact make and model of a car over a wide range of viewpoints in cluttered scenes.","cites":"6","conferencePercentile":"89.6039604"},{"venue":"WACV","id":"2d31ab536b3c8a05de0d24e0257ca4433d5a7c75","venue_1":"WACV","year":"2014","title":"Materials discovery: Fine-grained classification of X-ray scattering images","authors":"M. Hadi Kiapour, Kevin Yager, Alexander C. Berg, Tamara L. Berg","author_ids":"1772294, 1780708, 1743555, 1685538","abstract":"We explore the use of computer vision methods for organizing , searching, and classifying x-ray scattering images. X-ray scattering is a technique that shines an intense beam of x-rays through a sample of interest. By recording the intensity of x-ray deflection as a function of angle, scientists can measure the structure of materials at the molecular and nano-scale. Current and planned synchrotron instruments are producing x-ray scattering data at an unprecedented rate, making the design of automatic analysis techniques crucial for future research. In this paper, we devise an attribute-based approach to recognition in x-ray scattering images and demonstrate applications to image annotation and retrieval.","cites":"1","conferencePercentile":"30.1980198"},{"venue":"WACV","id":"191d9773ca3bca96ff95be7c3c683337afabef73","venue_1":"WACV","year":"2009","title":"Uniform image and camera access","authors":"Gregor Miller, Sidney Fels","author_ids":"1781538, 1749457","abstract":"We introduce a work-in-progress camera access scheme we call the Unified Camera Framework. Attempts have been made in the past to provide simple access to cameras, however these are generally OS specific or lacking in function-ality. We present a novel interface which works across operating systems, and provides access to native images through a descriptor. A unified configuration model is presented to allow manipulation of camera parameters to the level each camera supports. Validation of the ideas presented is given in the form of a proof-of-concept implementation called the All Seeing Eye.","cites":"1","conferencePercentile":"19.49152542"},{"venue":"WACV","id":"7722acfbb0cea8ff80ba355f00cabf02f1f2e0df","venue_1":"WACV","year":"2015","title":"Extending the Performance of Human Classifiers Using a Viewpoint Specific Approach","authors":"Endri Dibra, Jérôme Maye, Olga Diamanti, Roland Siegwart, Paul A. Beardsley","author_ids":"3294266, 2755185, 1868022, 1720483, 1777539","abstract":"This paper describes human classifiers that are 'view-point specific', meaning specific to subjects being observed by a particular camera in a particular scene. The advantages of the approach are (a) improved human detection in the presence of perspective foreshortening from an elevated camera, (b) ability to handle partial occlusion of subjects e.g. partial occlusion by furniture in an indoor scene, and (c) ability to detect subjects when partially truncated at the top, bottom or sides of the image. Elevated camera views will typically generate truncated views for subjects at the image edges but our viewpoint specific method handles such cases and thereby extends overall detection coverage. The approach is-(a) define a tiling on the ground plane of the 3D scene, (b) generate training images per tile using virtual humans, (c) train a classifier per tile (d) run the classifiers on the real scene. The approach would be prohibitive if each new deployment required real training images, but it is feasible because training is done with a virtual humans inserted into a scene model. The classifier is a linear SVM and HOGs. Experimental results provide a comparative analysis with existing algorithms to demonstrate the advantages described above.","cites":"0","conferencePercentile":"10.11235955"},{"venue":"WACV","id":"1c875065e6ab978bc5e61b9e2bbd82bb282944e2","venue_1":"WACV","year":"2015","title":"Runway to Realway: Visual Analysis of Fashion","authors":"Sirion Vittayakorn, Kota Yamaguchi, Alexander C. Berg, Tamara L. Berg","author_ids":"3302783, 1721910, 1743555, 1685538","abstract":"Clothing and fashion are an integral part of our everyday lives. In this paper we present an approach to studying fashion both on the runway and in more real-world settings, computationally, and at large scale, using computer vision. Our contributions include collecting a new runway dataset, designing features suitable for capturing outfit appearance, collecting human judgments of outfit similarity, and learning similarity functions on the features to mimic those judgments. We provide both intrinsic and extrinsic evaluations of our learned models to assess performance on outfit similarity prediction as well as season, year, and brand estimation. An example application tracks visual trends as runway fashions filter down to \" realway \" street fashions.","cites":"9","conferencePercentile":"91.57303371"},{"venue":"WACV","id":"3f03270fbf73e777a194709d940f1cac049e386a","venue_1":"WACV","year":"2014","title":"Composite Discriminant Factor analysis","authors":"Vlad I. Morariu, Ejaz Ahmed, Venkataraman Santhanam, David Harwood, Larry S. Davis","author_ids":"2852035, 2127681, 2536462, 2854176, 1693428","abstract":"We propose a linear dimensionality reduction method, Composite Discriminant Factor (CDF) analysis, which searches for a discriminative but compact feature subspace that can be used as input to classifiers that suffer from problems such as multi-collinearity or the curse of dimensional-ity. The subspace selected by CDF maximizes the performance of the entire classification pipeline, and is chosen from a set of candidate subspaces that are each discrimina-tive. Our method is based on Partial Least Squares (PLS) analysis, and can be viewed as a generalization of the PLS1 algorithm, designed to increase discrimination in classification tasks. We demonstrate our approach on the UCF50 action recognition dataset, two object detection datasets (INRIA pedestrians and vehicles from aerial imagery), and machine learning datasets from the UCI Machine Learning repository. Experimental results show that the proposed approach improves significantly in terms of accuracy over linear SVM, and also over PLS in terms of compactness and efficiency, while maintaining or improving accuracy.","cites":"0","conferencePercentile":"9.405940594"},{"venue":"WACV","id":"41f238083acb53425035a41e59efd140835840e5","venue_1":"WACV","year":"2013","title":"OpenVL: A task-based abstraction for developer-friendly computer vision","authors":"Gregor Miller, Sidney Fels","author_ids":"1781538, 1749457","abstract":"Research into computer vision techniques has far out-paced the development of interfaces (such as APIs) to support the techniques' accessibility, especially to developers who are not experts in the field. We present a new description-based interface designed to be mainstream-developer-friendly while retaining sufficient power and flexibility to solve a wide variety of computer vision problems. The interface presents vision at the task level (hiding al-gorithmic detail) and uses a task-based description derived from definitions of vision problems. We show that after interpretation , the description can be used to invoke an appropriate method to provide the developer's requested result. Our implementation interprets the description and invokes various vision methods with automatically derived parameters , which we demonstrate on a range of tasks.","cites":"3","conferencePercentile":"49.01960784"},{"venue":"WACV","id":"1adc0522df07533a9f8375fe59eff70d757aa90a","venue_1":"WACV","year":"2008","title":"Cata-Fisheye Camera for Panoramic Imaging","authors":"Gurunandan Krishnan, Shree K. Nayar","author_ids":"1868772, 1750470","abstract":"We present a novel panoramic imaging system which uses a curved mirror as a simple optical attachment to a fish-eye lens. When compared to existing panoramic cameras, our \" cata-fisheye \" camera has a simple, compact and inexpensive design, and yet yields high optical performance. It captures the desired panoramic field of view in two parts. The upper part is obtained directly by the fisheye lens and the lower part after reflection by the curved mirror. These two parts of the field of view have a small overlap that is used to stitch them into a single seamless panorama. The cata-fisheye concept allows us to design cameras with a wide range of fields of view by simply varying the parameters and position of the curved mirror. We provide an automatic method for the one-time calibration needed to stitch the two parts of the panoramic field of view. We have done a complete performance evaluation of our concept with respect to (i) the optical quality of the captured images, (ii) the working range of the camera over which the parallax is negligible, and (iii) the spatial resolution of the computed panorama. Finally, we have built a prototype cata-fisheye video camera with a spherical mirror that can capture high resolution panoramic images (3600x550 pixels) with a 360 • (horizontal) x 55 • (vertical) field of view.","cites":"6","conferencePercentile":"46.15384615"},{"venue":"WACV","id":"2f47496cb007efa3b4f0ff4774d43c861197f3da","venue_1":"WACV","year":"2011","title":"Human gait estimation using a wearable camera","authors":"Yoshihiro Watanabe, Tetsuo Hatanaka, Takashi Komuro, Masatoshi Ishikawa","author_ids":"2279584, 7590497, 1696204, 1734807","abstract":"We focus on the growing need for a technology that can achieve motion capture in outdoor environments. The conventional approaches have relied mainly on fixed installed cameras. With this approach, however, it is difficult to capture motion in everyday surroundings. This paper describes a new method for motion estimation using a single wear-able camera. We focused on walking motion. The key point is how the system can estimate the original walking state using limited information from a wearable sensor. This paper describes three aspects: the configuration of the sensing system, gait representation, and the gait estimation method.","cites":"1","conferencePercentile":"15.87301587"},{"venue":"WACV","id":"b7ac5ad4df6137e7988cf2c366a7cad38f78db22","venue_1":"WACV","year":"2015","title":"Interleaved Regression Tree Field Cascades for Blind Image Deconvolution","authors":"Kevin Schelten, Sebastian Nowozin, Jeremy Jancsary, Carsten Rother, Stefan Roth","author_ids":"2610370, 2388416, 2137033, 7699610, 2434581","abstract":"Image blur from camera shake is a common cause for poor image quality in digital photography, prompting a significant recent interest in image deblurring. The vast majority of work on blind deblurring splits the problem into two subsequent steps: First, the blur process (i.e., blur kernel) is estimated; then the image is restored given the estimated kernel using a non-blind deblurring algorithm. Recent work in non-blind deblurring has shown that discrim-inative approaches can have clear image quality and run-time benefits over typical generative formulations. In this paper, we propose a cascade for blind deblurring that alternates between kernel estimation and discriminative deblur-ring using regression tree fields (RTFs). We further contribute a new dataset of realistic image blur kernels from human camera shake, which we use to train the discrim-inative component. Extensive qualitative and quantitative experiments show a clear gain in image quality by inter-leaving kernel estimation and discriminative deblurring in an iterative cascade.","cites":"1","conferencePercentile":"35.39325843"},{"venue":"WACV","id":"63cd82dfc573a1ed63395fb1c28b7cc5e8d527c8","venue_1":"WACV","year":"2014","title":"Attribute-based vehicle recognition using viewpoint-aware multiple instance SVMs","authors":"Kun Duan, Luca Marchesotti, David J. Crandall","author_ids":"2481141, 1761987, 2821130","abstract":"Vehicle recognition is a challenging task with many useful applications. State-of-the-art methods usually learn dis-criminative classifiers for different vehicle categories or different viewpoint angles, but little work has explored vehicle recognition using semantic visual attributes. In this paper , we propose a novel iterative multiple instance learning method to model local attributes and viewpoint angles together in the same framework. We expand the standard MI-SVM formulation to incorporate pairwise constraints based on viewpoint relations within positive exemplars. We show that our method is able to generate discriminative and semantic local attributes for vehicle categories. We also show that we can estimate viewpoint labels more accurately than baselines when these annotations are not available in the training set. We test the technique on the Stanford cars and INRIA vehicles datasets, and compare with other methods.","cites":"1","conferencePercentile":"30.1980198"},{"venue":"WACV","id":"7969cc315bbafcd38a637eb8cd5d45ba897be319","venue_1":"WACV","year":"2016","title":"An Enhanced Deep Feature Representation for Person Re-identification","authors":"Shangxuan Wu, Ying-Cong Chen, Xiang Li, Ancong Wu, Jinjie You, Wei-Shi Zheng","author_ids":"3393496, 2070527, 1737850, 2084013, 2597247, 3333315","abstract":"Feature representation and metric learning are two critical components in person re-identification models. In this paper, we focus on the feature representation and claim that hand-crafted histogram features can be complementary to Convolutional Neural Network (CNN) features. We propose a novel feature extraction model called Feature Fusion Net (FFN) for pedestrian image representation. In FFN, back propagation makes CNN features constrained by the hand-crafted features. Utilizing color histogram features (RGB, HSV, YCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientation Gabor features), we get a new deep feature representation that is more discriminative and compact. Experiments on three challenging datasets (VIPeR, CUHK01, PRID450s) validates the effectiveness of our proposal .","cites":"8","conferencePercentile":"94.6969697"},{"venue":"WACV","id":"622a0952433ff9fccd44a525dc52c4b3855c3f34","venue_1":"WACV","year":"2012","title":"Dynamic and invisible messaging for visual MIMO","authors":"Wenjia Yuan, Kristin J. Dana, Ashwin Ashok, Marco Gruteser, Narayan B. Mandayam","author_ids":"2260397, 1710772, 3035665, 1708469, 1679559","abstract":"The growing ubiquity of cameras in hand-held devices and the prevalence of electronic displays in signage creates a novel framework for wireless communications. Traditionally , the term MIMO is used for multiple-input multiple-output where the multiple-input component is a set of radio transmitters and the multiple-output component is a set of radio receivers. We employ the concept of visual MIMO where pixels are transmitters and cameras are receivers. In this manner, the techniques of computer vision can be combined with principles from wireless communications to create an optical line-of-sight communications channel. Two major challenges are addressed: (1) The message for transmission must be embedded in the observed display so that the message is hidden from the observer and the electronic display can simultaneously be used for its originally intended purpose (e.g. signage, advertisements, maps); (2) Photometric and geometric distortions during the imaging process corrupt the information channel between the transmitter display and the receiver camera. These distortions must be modeled and removed. In this paper, we present a real-time messaging paradigm and its implementation in an operational visual MIMO optical systems. As part of the system, we develop a novel algorithm for photographic message extraction which includes automatic display detection , message embedding and message retrieval. Experiments show that the system achieves an average accuracy of 94.6% at the bitrate of 6222.2 bps.","cites":"14","conferencePercentile":"91.11111111"},{"venue":"WACV","id":"11d965c2901f3c29568fdabce4be9c36bbfa24bf","venue_1":"WACV","year":"2016","title":"Optimal Radiometric Calibration for Camera-Display Communication","authors":"Wenjia Yuan, Eric Wengrowski, Kristin J. Dana, Ashwin Ashok, Marco Gruteser, Narayan B. Mandayam","author_ids":"2260397, 1980531, 1710772, 3035665, 1708469, 1679559","abstract":"• A handheld camera pointed at the display can receive not only the display image, but also an underlying message. • Differencing the camera-captured alternate frames leaves the small intensity pattern, but results in errors due to photometric effects that depend on camera pose. • The online radiometric calibration algorithms described in this paper significantly reduces message recovery errors, especially for low intensity messages and oblique camera angles. Figure 1: Online Radiometric Calibration mitigates the distorting effects of the CDTF to enable more accurate message recovery. From the display to the camera, the light signal is affected by display photometry, camera pose and camera radiometry. In each pair of intensity histograms shown above, the left represents an image histogram before passing through the CDTF, and the right represents the histogram after the CDTF.","cites":"2","conferencePercentile":"63.63636364"},{"venue":"WACV","id":"a2bd2cd7ae70bf8052a023eafd29e330840b2c5e","venue_1":"WACV","year":"2014","title":"Task-based control of articulated human pose detection for OpenVL","authors":"Georgii Oleinikov, Gregor Miller, James J. Little, Sidney Fels","author_ids":"2670893, 1781538, 1710980, 1749457","abstract":"Human pose detection is the foundation for many applications , particularly those using gestures as part of a natural user interface. We introduce a novel task-based control method for human pose detection, encoding specialist knowledge in a descriptive abstraction for application by non-experts, such as developers, artists and students. The abstraction hides the details of a set of algorithms which specialise either in different estimations of pose (e.g. articulated , body part) or under different conditions (e.g. occlu-sion, clutter). Users describe the conditions of their problem , which is used to select the most suitable algorithm (and automatically set up the parameters). The task-based control is evaluated with images described using the abstraction. Expected outcomes are compared to results and demonstrate that describing the conditions is sufficient to allow the abstraction to produce the required result.","cites":"1","conferencePercentile":"30.1980198"},{"venue":"WACV","id":"57ef9f29bd64e4eae690270675471cd18f594b25","venue_1":"WACV","year":"2009","title":"Adaptive, real-time visual simultaneous localization and mapping","authors":"Brian Clipp, Christopher Zach, Jongwoo Lim, Jan-Michael Frahm, Marc Pollefeys","author_ids":"2628245, 1713941, 1688664, 1745886, 1742208","abstract":"In this paper we present a real-time simultaneous local-ization and mapping system which uses a stereo camera as its only input. We combine the benefits of KLT feature tracking , which include high speed and robustness to repetitive features, with wide baseline features, which allow for feature matching after large camera motions. Updating the map of feature locations and camera poses is considerably more expensive than performing KLT tracking. For this reason we use the optical flow measured by the KLT tracker to adaptively select key frames for which we do a full map and camera pose update. In this way we limit the processing to only \" interesting \" parts of the video sequence. Additionally , we maintain a consistent scene scale at low cost by using a GPU implementation of multi-camera scene flow, a generalization of KLT to the motion of image features in three dimensions. The system uses multiple sub-maps; scal-able, bag of features recognition and geometric verification to recover from motion estimation failure or \" kidnapping \". This architecture allows the robot to grow the existing map online and in real time while storing all of the data necessary for an off-line optimization to complete loops. We demonstrate the robustness of our system in a challenging indoor environment that includes semi-reflective glass walls and people moving in the scene.","cites":"4","conferencePercentile":"57.62711864"},{"venue":"WACV","id":"7597f1ed5e887ae3f189cf2006e4b4254fc73226","venue_1":"WACV","year":"2015","title":"Predicting Geo-informative Attributes in Large-Scale Image Collections Using Convolutional Neural Networks","authors":"Stefan Lee, Haipeng Zhang, David J. Crandall","author_ids":"2297229, 2822359, 2821130","abstract":"Geographic location is a powerful property for organizing large-scale photo collections, but only a small fraction of online photos are geo-tagged. Most work in automatically estimating geo-tags from image content is based on comparison against models of buildings or landmarks , or on matching to large reference collections of geo-tagged images. These approaches work well for frequently-photographed places like major cities and tourist destinations , but fail for photos taken in sparsely photographed places where few reference photos exist. Here we consider how to recognize general geo-informative attributes of a photo, e.g. the elevation gradient, population density, de-mographics, etc. of where it was taken, instead of trying to estimate a precise geo-tag. We learn models for these attributes using a large (noisy) set of geo-tagged images from Flickr by training deep convolutional neural networks (CNNs). We evaluate on over a dozen attributes, showing that while automatically recognizing some attributes is very difficult, others can be automatically estimated with about the same accuracy as a human.","cites":"10","conferencePercentile":"96.62921348"},{"venue":"WACV","id":"7b9ad0661c6c1693bc7cb3b10389b9d389173788","venue_1":"WACV","year":"2015","title":"Clauselets: Leveraging Temporally Related Actions for Video Event Analysis","authors":"Hyungtae Lee, Vlad I. Morariu, Larry S. Davis","author_ids":"2445131, 2852035, 1693428","abstract":"We propose clauselets, sets of concurrent actions and their temporal relationships, and explore their application to video event analysis. We train clauselets in two stages. We initially train first level clauselet detectors that find a limited set of actions in particular qualitative temporal configurations based on Allen's interval relations. In the second stage, we apply the first level detectors to training videos, and discriminatively learn temporal patterns between activations that involve more actions over longer durations and lead to improved second level clauselet models. We demonstrate the utility of clauselets by applying them to the task of \" in-the-wild \" video event recognition on the TRECVID MED 11 dataset. Not only do clauselets achieve state-of-the-art results on this task, but qualitative results suggest that they may also lead to semantically meaningful descriptions of videos in terms of detected actions and their temporal relationships.","cites":"1","conferencePercentile":"35.39325843"},{"venue":"WACV","id":"630dec8327e3c6e528ce99562c8d4378b023a05f","venue_1":"WACV","year":"2011","title":"Real-time detection and reading of LED/LCD displays for visually impaired persons","authors":"Ender Tekin, James M. Coughlan, Huiying Shen","author_ids":"2940704, 8436134, 7969151","abstract":"Modern household appliances, such as microwave ovens and DVD players, increasingly require users to read an LED or LCD display to operate them, posing a severe obstacle for persons with blindness or visual impairment. While OCR-enabled devices are emerging to address the related problem of reading text in printed documents, they are not designed to tackle the challenge of finding and reading characters in appliance displays. Any system for reading these characters must address the challenge of first locating the characters among substantial amounts of background clutter; moreover, poor contrast and the abundance of specular highlights on the display surface - which degrade the image in an unpredictable way as the camera is moved - motivate the need for a system that processes images at a few frames per second, rather than forcing the user to take several photos, each of which can take seconds to acquire and process, until one is readable.We describe a novel system that acquires video, detects and reads LED/LCD characters in real time, reading them aloud to the user with synthesized speech. The system has been implemented on both a desktop and a cell phone. Experimental results are reported on videos of display images, demonstrating the feasibility of the system.","cites":"8","conferencePercentile":"73.80952381"},{"venue":"WACV","id":"3651e3a0080e34abbd159c3487491c380c50a9d1","venue_1":"WACV","year":"2011","title":"Localizing blurry and low-resolution text in natural images","authors":"Pannag R. Sanketi, Huiying Shen, James M. Coughlan","author_ids":"2840758, 7969151, 8436134","abstract":"There is a growing body of work addressing the problem of localizing printed text regions occurring in natural scenes, all of it focused on images in which the text to be localized is resolved clearly enough to be read by OCR. This paper introduces an alternative approach to text localization based on the fact that it is often useful to localize text that is identifiable as text but too blurry or small to be read, for two reasons. First, an image can be decimated and processed at a coarser resolution than usual, resulting in faster localization before OCR is performed (at full resolution, if needed). Second, in real-time applications such as a cell phone app to find and read text, text may initially be acquired from a lower-resolution video image in which it appears too small to be read; once the text's presence and location have been established, a higher-resolution image can be taken in order to resolve the text clearly enough to read it.We demonstrate proof of concept of this approach by describing a novel algorithm for binarizing the image and extracting candidate text features, called \"blobs,\" and grouping and classifying the blobs into text and non-text categories. Experimental results are shown on a variety of images in which the text is resolved too poorly to be clearly read, but is still identifiable by our algorithm as text.","cites":"9","conferencePercentile":"79.36507937"},{"venue":"WACV","id":"80815b2652357d2c6b06a5afa0b2524e041708e0","venue_1":"WACV","year":"2009","title":"An algorithm enabling blind users to find and read barcodes","authors":"Ender Tekin, James M. Coughlan","author_ids":"2940704, 8436134","abstract":"Most camera-based systems for finding and reading barcodes are designed to be used by sighted users (e.g. the Red Laser iPhone app), and assume the user carefully centers the barcode in the image before the barcode is read. Blind individuals could benefit greatly from such systems to identify packaged goods (such as canned goods in a supermarket), but unfortunately in their current form these systems are completely inaccessible because of their reliance on visual feedback from the user.To remedy this problem, we propose a computer vision algorithm that processes several frames of video per second to detect barcodes from a distance of several inches; the algorithm issues directional information with audio feedback (e.g. \"left,\" \"right\") and thereby guides a blind user holding a webcam or other portable camera to locate and home in on a barcode. Once the barcode is detected at sufficiently close range, a barcode reading algorithm previously developed by the authors scans and reads aloud the barcode and the corresponding product information. We demonstrate encouraging experimental results of our proposed system implemented on a desktop computer with a webcam held by a blindfolded user; ultimately the system will be ported to a camera phone for use by visually impaired users.","cites":"10","conferencePercentile":"83.05084746"},{"venue":"WACV","id":"8034cb93714f6e28794f86e8f2ab7633aab8f614","venue_1":"WACV","year":"2009","title":"Elevation-based MRF stereo implemented in real-time on a GPU","authors":"Volodymyr Ivanchenko, Huiying Shen, James M. Coughlan","author_ids":"3032862, 7969151, 8436134","abstract":"We describe a novel framework for calculating dense, accurate elevation maps from stereo, in which the height of each point in the scene is estimated relative to the ground plane. The key to our framework's ability to estimate elevation accurately is an MRF formulation of stereo that directly represents elevation at each pixel instead of the usual disparity. By enforcing smoothness of elevation rather than disparity (using pairwise interactions in the MRF), the usual fronto-parallel bias is transformed into a horizontal (parallel to the ground) bias – a bias that is more appropriate for scenes characterized by a dominant ground plane viewed from an angle. This horizontal bias amounts to a more informative prior for such scenes, which results in more accurate surface reconstruction, with sub-pixel accuracy. We apply this framework to the problem of finding small obstacles, such as curbs and other small deviations from the ground plane, a few meters in front of a vehicle (such as a wheelchair or robot) that are missed by standard real-time correlation stereo algorithms. We demonstrate a real-time implementation of our framework on a GPU (we have made the code publicly available), which processes a 640 x 480 stereo image pair in 160 ms using either our elevation model or a standard disparity-based model (with 32 elevation or disparity levels), and describe experimental results.","cites":"2","conferencePercentile":"36.44067797"},{"venue":"WACV","id":"c0da04975861a8010d177cb192a4494343f1399f","venue_1":"WACV","year":"2015","title":"Unsupervised Feature Extraction Inspired by Latent Low-Rank Representation","authors":"Yaming Wang, Vlad I. Morariu, Larry S. Davis","author_ids":"7708967, 2852035, 1693428","abstract":"Latent Low-Rank Representation (LatLRR) has the empirical capability of identifying \" salient \" features. However , the reason behind this feature extraction effect is still not understood. Its optimization leads to non-unique solutions and has high computational complexity, limiting its potential in practice. We show that LatLRR learns a transformation matrix which suppresses the most significant principal components corresponding to the largest singular values while preserving the details captured by the components with relatively smaller singular values. Based on this, we propose a novel feature extraction method which directly designs the transformation matrix and has similar behavior to LatLRR. Our method has a simple analytical solution and can achieve better performance with little computational cost. The effectiveness and efficiency of our method are validated on two face recognition datasets.","cites":"0","conferencePercentile":"10.11235955"},{"venue":"WACV","id":"f00e7b54de89b13dd0f64b5132f2a995ae1cf6b8","venue_1":"WACV","year":"2011","title":"Active stereo vision for improving long range hearing using a Laser Doppler Vibrometer","authors":"Tao Wang, Rui Li, Zhigang Zhu, Yufu Qu","author_ids":"1685072, 1704992, 4697712, 2046752","abstract":"Laser Doppler Vibrometers (LDVs) have been widely applied for detecting vibrations in applications such as mechanics, bridge inspection, biometrics, as well as long-range surveillance in which acoustic signatures can be obtained at a large distance. However, in both industrial and scientific applications, the LDVs are manually controlled in surface selection, laser focusing, and acoustic acquisition. In this paper, we propose an active stereo vision approach to facilitate fast and automated laser pointing and tracking for long-range LDV hearing. The system contains: 1) a mirror on a Pan-Tilt-Unit (PTU) to reflect the laser beam to any locations freely and quickly, and 2) two Pan-Tilt-Zoom (PTZ) cameras, one of which is mounted on the Pan-Tilt-Unit (PTU) and aligned with the laser beam synchronously. The distance measurement using the stereo vision system as well as triangulation between camera and the LDV laser beam allow us to fast focus the laser beam on selected surfaces and to obtain acoustic signals up to 200 meters in real time. We present some promising results with the collaborative visual and LDV measurements for laser pointing and focusing in order to achieve long range audio detection.","cites":"2","conferencePercentile":"30.95238095"},{"venue":"WACV","id":"3c9afc0c5bc2e24a9a0ab19d34fa77183a574a40","venue_1":"WACV","year":"2016","title":"Weighted atlas auto-context with application to multiple organ segmentation","authors":"Telmo Amaral, Ilias Kyriazakis, Stephen J. McKenna, Thomas Plötz","author_ids":"2885917, 3038615, 1740379, 7606729","abstract":"General rights Copyright and moral rights for the publications made accessible in Discovery Research Portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. • Users may download and print one copy of any publication from Discovery Research Portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain. • You may freely distribute the URL identifying the publication in the public portal. Take down policy If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim. Abstract Difficulties can arise from the segmentation of three-dimensional objects formed by multiple non-rigid parts represented in two-dimensional images. Problems involving parts whose spatial arrangement is subject to weak restrictions , and whose appearance and form change across images , can be particularly challenging. Segmentation methods that take into account spatial context information have addressed these types of problem, which often involve image data of a multi-modal nature. An attractive feature of the auto-context (AC) technique is that a prior \" atlas \" , typically obtained by averaging multiple label maps created by experts, can be used as an initial source of contextual data. However, a prior obtained in this way is likely to hide the inherent multi-modality of the data. We propose a modification of AC in which a probabilistic atlas of part locations is iteratively improved and made available as an additional source of information. We illustrate this technique with the problem of segmenting individual organs in images of pig offal, reporting statistically significant improvements in relation to both conventional AC and a state-of-the-art technique based on conditional random fields.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"8372bb5eb75d21b5b1bb0298395a3d035c170c45","venue_1":"WACV","year":"2016","title":"Adapting attributes by selecting features similar across domains","authors":"Siqi Liu, Adriana Kovashka","author_ids":"2628544, 1770205","abstract":"Attributes are semantic visual properties shared by objects. They have been shown to improve object recognition and to enhance content-based image search. While attributes are expected to cover multiple categories, e.g. a dalmatian and a whale can both have \" smooth skin \" , we find that the appearance of a single attribute varies quite a bit across categories. Thus, an attribute model learned on one category may not be usable on another category. We show how to adapt attribute models towards new categories. We ensure that positive transfer can occur between a source domain of categories and a novel target domain, by learning in a feature subspace found by feature selection where the data distributions of the domains are similar. We demonstrate that when data from the novel domain is limited, regularizing attribute models for that novel domain with models trained on an auxiliary domain (via Adaptive SVM) improves the accuracy of attribute prediction.","cites":"0","conferencePercentile":"19.6969697"},{"venue":"WACV","id":"002aaf4412f91d0828b79511f35c0863a1a32c47","venue_1":"WACV","year":"1996","title":"A real-time face tracker","authors":"Jie Yang, Alexander H. Waibel","author_ids":"1688428, 4500589","abstract":"We present a real-time face tracker in this paper The system has achieved a rate of 30% frameshecond using an HP-9000 workstation with a framegrabber and a Canon VC-CI camera. It can track a person 'sface while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, we present a stochastic model to characterize skin-color distributions of human faces. The information provided by the model is sufJicient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second , a motion model e' s used to estimate image motion and to predict search window. Third, a camera model is used topre-diet and to compensate for camera motion. The system can be applied to tele-conferencing and many HCI applications including lipreading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand.","cites":"224","conferencePercentile":"100"},{"venue":"WACV","id":"4577d988652c34e3597ddc767e1a347c2a710f14","venue_1":"WACV","year":"2014","title":"Online discriminative dictionary learning for visual tracking","authors":"Fan Yang, Zhuolin Jiang, Larry S. Davis","author_ids":"1752128, 1703292, 1693428","abstract":"Dictionary learning has been applied to various computer vision problems, such as image restoration, object classification and face recognition. In this work, we propose a tracking framework based on sparse representation and online discriminative dictionary learning. By associating dictionary items with label information, the learned dictionary is both reconstructive and discriminative, which better distinguishes target objects from the background. During tracking, the best target candidate is selected by a joint decision measure. Reliable tracking results and augmented training samples are accumulated into two sets to update the dictionary. Both online dictionary learning and the proposed joint decision measure are important for the final tracking performance. Experiments show that our approach outperforms several recently proposed trackers.","cites":"5","conferencePercentile":"84.15841584"},{"venue":"WACV","id":"1ed39db202e606f25aff93f3e4fe135283a50cc2","venue_1":"WACV","year":"2014","title":"Video text detection and recognition: Dataset and benchmark","authors":"Phuc Xuan Nguyen, Kai Wang, Serge J. Belongie","author_ids":"1879100, 3751236, 1769406","abstract":"This paper focuses on the problem of text detection and recognition in videos. Even though text detection and recognition in images has seen much progress in recent years, relatively little work has been done to extend these solutions to the video domain. In this work, we extend an existing end-to-end solution for text recognition in natural images to video. We explore a variety of methods for training local character models and explore methods to capitalize on the temporal redundancy of text in video. We present detection performance using the Video Analysis and Content Extraction (VACE) benchmarking framework on the ICDAR 2013 Robust Reading Challenge 3 video dataset and on a new video text dataset. We also propose a new performance metric based on precision-recall curves to measure the performance of text recognition in videos. Using this metric, we provide early video text recognition results on the above mentioned datasets.","cites":"4","conferencePercentile":"72.77227723"},{"venue":"WACV","id":"6eca884d907b5b611f6a59407cbb12e6583e36c7","venue_1":"WACV","year":"2014","title":"Predicting movie ratings from audience behaviors","authors":"Rajitha Navarathna, Patrick Lucey, Peter Carr, Elizabeth J. Carter, Sridha Sridharan, Iain A. Matthews","author_ids":"2843723, 1713496, 3429219, 3316296, 1729760, 1711695","abstract":"We propose a method of representing audience behavior through facial and body motions from a single video stream, and use these motions to predict the rating for feature-length movies. This is a very challenging problem as: i) the movie viewing environment is dark and contains views of people at different scales and viewpoints; ii) the duration of feature-length movies is long (80-120 mins) so tracking people uninterrupted for this length of time is an unsolved problem ; and iii) expressions and motions of audience members are subtle, short and sparse making labeling of activities unreliable. To circumvent these issues, we use an infra-red illuminated test-bed to obtain a visually uniform input. We then utilize motion-history features which capture the subtle movements of a person within a pre-defined volume, and then form a group representation of the audience by a his-togram of pair-wise correlations over small time windows. Using this group representation, we learn a movie rating classifier from crowd-sourced ratings collected by rotten-tomatoes.com and show our prediction capability on audiences from 30 movies across 250 subjects (> 50 hours).","cites":"5","conferencePercentile":"84.15841584"},{"venue":"WACV","id":"9bf12fb15049087d1796ebae6c7b0a23c39e294b","venue_1":"WACV","year":"2014","title":"Relative facial action unit detection","authors":"Mahmoud Khademi, Louis-Philippe Morency","author_ids":"1736464, 1767184","abstract":"—This paper presents a subject-independent facial action unit (AU) detection method by introducing the concept of relative AU detection, for scenarios where the neutral face is not provided. We propose a new classification objective function which analyzes the temporal neighborhood of the current frame to decide if the expression recently increased, decreased or showed no change. This approach is a significant change from the conventional absolute method which decides about AU classification using the current frame, without an explicit comparison with its neighboring frames. Our proposed method improves robustness to individual differences such as face scale and shape, age-related wrinkles, and transitions among expressions (e.g., lower intensity of expressions). Our experiments on three publicly available datasets (Extended Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement of our approach over conventional absolute techniques.","cites":"2","conferencePercentile":"49.00990099"},{"venue":"WACV","id":"c4733064979f6066e58dc7566ba36ca631d7e6d6","venue_1":"WACV","year":"2002","title":"Foreground Object Detection in Changing Background Based on Color Co-Occurrence Statistics","authors":"Liyuan Li, Weimin Huang, Irene Y. H. Gu, Qi Tian","author_ids":"1746918, 1742173, 1761436, 1724745","abstract":"This paper proposes a novel method for detecting foreground objects in nonstationary complex environments containing moving background objects. We derive a Bayes decision rule for classification of background and foreground changes based on inter-frame color co-occurrence statistics. An approach to store and fast retrieve color co-occurrence statistics is also established. In the proposed method, foreground objects are detected in two steps. First, both foreground and background changes are extracted using background subtraction and temporal differ-encing. The frequent background changes are then recognized using the Bayes decision rule based on the learned color co-occurrence statistics. Both short-term and long-term strategies to learn the frequent background changes are proposed. Experiments have shown promising results in detecting foreground objects from video containing wavering tree branches and flickering screens/water surface. The proposed method has shown better performance as compared with two existing methods.","cites":"20","conferencePercentile":"66.66666667"},{"venue":"WACV","id":"66a7cfaca67cc69b6b08397a884e10ff374d710c","venue_1":"WACV","year":"2015","title":"Menu-Match: Restaurant-Specific Food Logging from Images","authors":"Oscar Beijbom, Neel Joshi, Dan Morris, T. Scott Saponas, Siddharth Khullar","author_ids":"3258919, 2641664, 1779342, 1766388, 2928268","abstract":"Logging food and calorie intake has been shown to facilitate weight management. Unfortunately, current food logging methods are time-consuming and cumbersome, which limits their effectiveness. To address this limitation, we present an automated computer vision system for logging food and calorie intake using images. We focus on the \" restaurant \" scenario, which is often a challenging aspect of diet management. We introduce a key insight that addresses this problem specifically: restaurant plates are often both nutritionally and visually consistent across many servings. This insight provides a path to robust calorie estimation from a single RGB photograph: using a database of known food items together with restaurant-specific clas-sifiers, calorie estimation can be achieved through identification followed by calorie lookup. As demonstrated on a challenging Menu-Match dataset and an existing third-party dataset, our approach outperforms previous computer vision methods and a commercial calorie estimation app. Our Menu-Match dataset of realistic restaurant meals is made publicly available.","cites":"9","conferencePercentile":"91.57303371"},{"venue":"WACV","id":"8ef0d2b781823295fc5c0ff21a9816cf35ac7193","venue_1":"WACV","year":"2016","title":"A fast and robust text spotter","authors":"Siyang Qin, Roberto Manduchi","author_ids":"3407327, 1737048","abstract":"We introduce an algorithm for text detection and localization (\"spotting\") that is computationally efficient and produces state-of-the-art results. Our system uses multi-channel MSERs to detect a large number of promising regions, then subsamples these regions using a clustering approach. Representatives of region clusters are binarized and then passed on to a deep network. A final line grouping stage forms word-level segments. On the ICDAR 2011 and 2015 benchmarks, our algorithm obtains an F-score of 82% and 83%, respectively, at a computational cost of 1.2 seconds per frame. We also introduce a version that is three times as fast, with only a slight reduction in performance.","cites":"1","conferencePercentile":"47.72727273"},{"venue":"WACV","id":"5a60ff9141152e6fe30de14c43cdba75560fdfec","venue_1":"WACV","year":"2009","title":"ML-fusion based multi-model human detection and tracking for robust human-robot interfaces","authors":"Liyuan Li, Kah Eng Hoe, Shuicheng Yan, Xinguo Yu","author_ids":"1746918, 2891546, 1698982, 1802609","abstract":"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection , color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections , false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.","cites":"3","conferencePercentile":"48.30508475"},{"venue":"WACV","id":"a7dd6761db50971362c551feeeb5d96c59a5746c","venue_1":"WACV","year":"2014","title":"Understanding the 3D layout of a cluttered room from multiple images","authors":"Sid Ying-Ze Bao, Axel Furlan, Li Fei-Fei, Silvio Savarese","author_ids":"7646349, 2168361, 3216322, 1702137","abstract":"We present a novel framework for robustly understanding the geometrical and semantic structure of a cluttered room from a small number of images captured from different viewpoints. The tasks we seek to address include: i) estimating the 3D layout of the room – that is, the 3D configuration of floor, walls and ceiling; ii) identifying and localizing all the foreground objects in the room. We jointly use multiview geometry constraints and image appearance to identify the best room layout configuration. Extensive experimental evaluation demonstrates that our estimation results are more complete and accurate in estimating 3D room structure and recognizing objects than alternative state-of-the-art algorithms. In addition, we show an augmented reality mobile application to highlight the high accuracy of our method, which may be beneficial to many computer vision applications.","cites":"4","conferencePercentile":"72.77227723"},{"venue":"WACV","id":"203ad791225ab0c53ad0ed132507a097da684fbb","venue_1":"WACV","year":"2009","title":"Combining multiple kernels for efficient image classification","authors":"Behjat Siddiquie, Shiv Naga Prasad Vitaladevuni, Larry S. Davis","author_ids":"1832513, 3306372, 1693428","abstract":"We investigate the problem of combining multiple feature channels for the purpose of efficient image classification. Discriminative kernel based methods, such as SVMs, have been shown to be quite effective for image classification. To use these methods with several feature channels, one needs to combine base kernels computed from them. Multiple kernel learning is an effective method for combining the base kernels. However, the cost of computing the kernel similarities of a test image with each of the support vectors for all feature channels is extremely high. We propose an alternate method, where training data instances are selected, using AdaBoost, for each of the base kernels. A composite decision function, which can be evaluated by computing kernel similarities with respect to only these chosen instances, is learnt. This method significantly reduces the number of kernel computations required during testing. Experimental results on the benchmark UCI datasets, as well as on a challenging painting dataset, are included to demonstrate the effectiveness of our method.","cites":"6","conferencePercentile":"71.18644068"},{"venue":"WACV","id":"35c72f9627b3a24dd4c434f9a32e80dfcafb1337","venue_1":"WACV","year":"2009","title":"A fast multi-model approach for object duplicate extraction","authors":"Paolo Piccinini, Andrea Prati, Rita Cucchiara","author_ids":"3126200, 1733945, 1741922","abstract":"This paper presents an innovative approach for localizing and segmenting duplicate objects for industrial applications. The working conditions are challenging, with complex heavily-occluded objects, arranged at random in the scene. To account for high flexibility and processing speed, this approach exploits SIFT keypoint extraction and mean shift clustering to efficiently partition the correspondences between the object model and the duplicates onto the different object instances. The re-projection (by means of an Euclidean transform) of some delimiting points onto the current image is used to segment the object shapes. This procedure is compared in terms of accuracy with existing homography-based solutions which make use of RANSAC to eliminate outliers in the homography estimation. Moreover , in order to improve the extraction in the case of reflective or transparent objects, multiple object models are used and fused together. Experimental results on different and challenging kinds of objects are reported.","cites":"2","conferencePercentile":"36.44067797"},{"venue":"WACV","id":"5b5167813efb7c5c426be55860584da7238bd286","venue_1":"WACV","year":"2009","title":"Applying Bayes Markov chains for the detection of ATM related scenarios","authors":"Dejan Arsic, Atanas Lyutskanov, Moritz Kaiser, Björn W. Schuller, Gerhard Rigoll","author_ids":"1740530, 2442021, 1719227, 1705602, 1705843","abstract":"Video surveillance systems have been introduced in various fields of our daily life to enhance security and protect individuals and sensitive infrastructure. Up to now it has been usually utilized as a forensic tool for after the fact investigations and are commonly monitored by human operators. In order to assist these and to be able to react in time, a fully automated system is desired. In this work we will present a multi camera surveillance system, which is required to resolve heavy occlusions, to detect robberies at ATM machines. The resulting trajectories will be analyzed for so called Low Level Activities (LLA), such as walking, running and stationarity, applying simple but robust approaches. The results of the LLA analysis will subsequently be fed into a Bayesian Network, that is used as a stochastic model to model so called High Level Activities (HLA). Introducing state transitions between HLAs will allow a temporal modeling of a complex scene. This can be represented by a Markovian process.","cites":"2","conferencePercentile":"36.44067797"},{"venue":"WACV","id":"85f208f52cb6328b40824f913f9163e27b308c6e","venue_1":"WACV","year":"2011","title":"Multisensory embedded pose estimation","authors":"Eyrun Eyjolfsdottir, Matthew Turk","author_ids":"2948199, 1752714","abstract":"We present a multisensory method for estimating the transformation of a mobile phone between two images taken from its camera. Pose estimation is a necessary step for applications such as 3D reconstruction and panorama construction , but detecting and matching robust features can be computationally expensive. In this paper we propose a method for combining the inertial sensors (accelerometers and gyroscopes) of a mobile phone with its camera to provide a fast and accurate pose estimation.We use the inertial based pose to warp two images into the same perspective frame. We then employ an adaptive FAST feature detector and image patches, normalized with respect to illumination , as feature descriptors. After the warping the images are approximately aligned with each other so the search for matching key-points also becomes faster and in certain cases more reliable. Our results show that by incorporating the inertial sensors we can considerably speed up the process of detecting and matching key-points between two images, which is the most time consuming step of the pose estimation.","cites":"3","conferencePercentile":"46.82539683"},{"venue":"WACV","id":"16206dea1bfd6a58accbfc6b583dea595f275df3","venue_1":"WACV","year":"1994","title":"Image mosaicing for tele-reality applications","authors":"Richard Szeliski","author_ids":"1717841","abstract":"This paper presents some techniques for automatically deriving realistic 2-D scenes and 3-D geometric models from video sequences. These techniques can be used to build environments and 3-D models for virtual reality application based on recreating a true scene, i.e., tele-realityapplications. The fundamental technique used in this paper is imagemosaicing, the automatic alignment of multiple images into larger aggregates which are then used to represent portions of a 3-D scene. The paper first examines the easiest problems, those of flat scene and panoramic scene mosaicing. It then progresses to more complicated scenes with depth, and concludes with full 3-D models. The paper also discusses a number of novel applications based on tele-reality technology.","cites":"231","conferencePercentile":"100"},{"venue":"WACV","id":"58b0be2db0aeda2edb641273fe52946a24a714c3","venue_1":"WACV","year":"2009","title":"Attribute-based people search in surveillance environments","authors":"Daniel A. Vaquero, Rogério Schmidt Feris, Duan Tran, Lisa M. Brown, Arun Hampapur, Matthew Turk","author_ids":"2000950, 1723233, 2828154, 5730081, 1690709, 1752714","abstract":"We propose a novel framework for searching for people in surveillance environments. Rather than relying on face recognition technology, which is known to be sensitive to typical surveillance conditions such as lighting changes, face pose variation, and low-resolution imagery, we approach the problem in a different way: we search for people based on a parsing of human parts and their attributes, including facial hair, eyewear, clothing color, etc. These attributes can be extracted using detectors learned from large amounts of training data. A complete system that implements our framework is presented. At the interface, the user can specify a set of personal characteristics, and the system then retrieves events that match the provided description. For example, a possible query is \" show me the bald people who entered a given building last Saturday wearing a red shirt and sunglasses. \" This capability is useful in several applications, such as finding suspects or missing people. To evaluate the performance of our approach, we present extensive experiments on a set of images collected from the Internet, on infrared imagery, and on two-and-a-half months of video from a real surveillance environment. We are not aware of any similar surveillance system capable of automatically finding people in video based on their fine-grained body parts and attributes.","cites":"70","conferencePercentile":"100"},{"venue":"WACV","id":"07909972d01df8edbb4b9c9c28c46a1c71556549","venue_1":"WACV","year":"2011","title":"Dense point-to-point correspondences between 3D faces using parametric remeshing for constructing 3D Morphable Models","authors":"Moritz Kaiser, Gernot Heym, Nicolas H. Lehment, Dejan Arsic, Gerhard Rigoll","author_ids":"1719227, 1710581, 1702496, 1740530, 1705843","abstract":"In this contribution a novel method to compute dense point-to-point correspondences between 3D faces is presented. The correspondences can be employed for various face processing applications, for example for building up a 3D Morphable Model (3DMM). Paths connecting landmarks are traced on the 3D facial surface and the resulting patches are mapped into a uv-space. Triangle quadrisec-tion is used to build up remeshes with high point density for each 3D facial surface. Each vertex of a remesh has one corresponding vertex in another remesh and all remeshes have the same connectivity. The quality of the point-to-point correspondences is demonstrated on the bases of two applications , namely morphing and constructing a 3DMM.","cites":"0","conferencePercentile":"5.555555556"},{"venue":"WACV","id":"3514f66f155c271981a734f1523572edcd8fd10e","venue_1":"WACV","year":"2012","title":"A complementary local feature descriptor for face identification","authors":"Jonghyun Choi, William Robson Schwartz, Huimin Guo, Larry S. Davis","author_ids":"3826759, 1707014, 7330670, 1693428","abstract":"In many descriptors, spatial intensity transforms are often packed into a histogram or encoded into binary strings to be insensitive to local misalignment and compact. Dis-criminative information, however, might be lost during the process as a trade-off. To capture the lost pixel-wise local information, we propose a new feature descriptor, Circular Center Symmetric-Pairs of Pixels (CCS-POP). It concate-nates the symmetric pixel differences centered at a pixel position along various orientations with various radii; it is a generalized form of Local Binary Patterns, its variants and Pairs-of-Pixels (POP). Combining CCS-POP with existing descriptors achieves better face identification performance on FRGC Ver. 1.0 and FERET datasets compared to state-of-the-art approaches.","cites":"4","conferencePercentile":"57.77777778"},{"venue":"WACV","id":"2c23e77f9e1c8db1e5fb3093c14f6d49c337c305","venue_1":"WACV","year":"2013","title":"Domain adaptive object detection","authors":"Fatemeh Mirrashed, Vlad I. Morariu, Behjat Siddiquie, Rogério Schmidt Feris, Larry S. Davis","author_ids":"2486720, 2852035, 1832513, 1723233, 1693428","abstract":"We study the use of domain adaptation and transfer learning techniques as part of a framework for adaptive object detection. Unlike recent applications of domain adaptation work in computer vision, which generally focus on image classification, we explore the problem of extreme class imbalance present when performing domain adaptation for object detection. The main difficulty caused by this imbalance is that test images contain millions or billions of negative image subwindows but just a few positive ones, which makes it difficult to adapt to the changes in the positive class distributions by simple techniques such as random sampling. We propose an initial approach to address this problem and apply our technique to vehicle detection in a challenging urban surveillance dataset, demonstrating the performance of our approach with various amounts of supervision, including the fully unsupervised case.","cites":"4","conferencePercentile":"57.84313725"},{"venue":"WACV","id":"e94d789df16e3aa405f85c432da4e3d2cadc4a1b","venue_1":"WACV","year":"2015","title":"Multi-shot Re-identification with Random-Projection-Based Random Forests","authors":"Yang Li, Ziyan Wu, Richard J. Radke","author_ids":"1678662, 7807136, 1772337","abstract":"Human re-identification remains one of the fundamental , difficult problems in video surveillance and analysis. Current metric learning algorithms mainly focus on finding an optimized vector space such that observations of the same person in this space have a smaller distance than observations of two different people. In this paper, we propose a novel metric learning approach to the human re-identification problem, with an emphasis on the multi-shot scenario. First, we perform dimensionality reduction on image feature vectors through random projection. Next, a random forest is trained based on pairwise constraints in the projected subspace. This procedure repeats with a number of random projection bases, so that a series of random forests are trained in various feature subspaces. Finally, we select personalized random forests for each subject using their multi-shot appearances. We evaluate the performance of our algorithm on three benchmark datasets.","cites":"5","conferencePercentile":"76.40449438"},{"venue":"WACV","id":"db7c596c5ba8bc5c3bc09bfb939c33427dba4e29","venue_1":"WACV","year":"1998","title":"Interactive 3D modeling from multiple images using scene regularities","authors":"Harry Shum, Richard Szeliski, Simon Baker, Mei Han, P. Anandan","author_ids":"1698102, 1717841, 1737297, 2939096, 8387311","abstract":"We present some recent progress in designing and implementing two interactive image-based 3D modeling systems. The first system constructs 3D models from a collection of panoramic image mosaics. A panoramic mosaic consists of a set of images taken around the same viewpoint, and a camera matrix associated with each input image. The user first interactively specifies features such as points, lines, and planes. Our system recovers the camera pose for each mosaic from known line directions and reference points. It then constructs the 3D model using all available geometrical constraints. The second system extracts structure from stereo by representing the scene as a collection of approximately planar layers. The user first interactively segments the images into corresponding planar regions. Our system recovers a composite mosaic for each layer, estimates the plane equation for the layer, and optionally recovers the camera locations as well as out-of-plane displacements. By taking advantage of known scene regularities, our interactive systems avoid difficult feature correspondence problems that occur in traditional automatic modeling systems. They also shift the interactive high-level structural model specification stage to precede (or intermix with) the 3D geometry recovery. They are thus able to extract accurate wire frame and texture-mapped 3D models from multiple image sequences.","cites":"14","conferencePercentile":"67.56756757"},{"venue":"WACV","id":"0000d0d159ad3d9702ad82d83a3e20e7fc6637cb","venue_1":"WACV","year":"2000","title":"Removal of interfering strokes in double-sided document images","authors":"Chew Lim Tan, Ruini Cao, Peiyi Shen, Qian Wang, Julia Chee, Josephine Chang","author_ids":"1679749, 2403388, 3100360, 1729612, 2764411, 1886299","abstract":"This paper addresses a special problem with historical document images where handwritten characters from the reverse side appear as noise on the front side and even interfere with the front side characters. A novel method to extract clear textual images from interfering and overlapping areas of text is presented here. The proposed algorithm is interesting in that, with an observation that the edges of the sipping strokes from the reverse side are not as sharp as those on the front side, it adopts an edge detection approach to suppress unwanted background patterns. By further concentrating on the orientation of the strokes, other remaining long and strong noisy edges are removed by using an orientation filter and a size filter. The proposed method proves to perform well regardless of the intensity differences between the foreground writing and the interfering strokes. The segmentation results of real images are shown and evaluated.","cites":"20","conferencePercentile":"58.82352941"},{"venue":"WACV","id":"c58c22ae16d467d4dd0ddba83a06b062b1c777ef","venue_1":"WACV","year":"2014","title":"Interactive video segmentation using occlusion boundaries and temporally coherent superpixels","authors":"Radu Dondera, Vlad I. Morariu, Yulu Wang, Larry S. Davis","author_ids":"2777109, 2852035, 2304609, 1693428","abstract":"We propose an interactive video segmentation system built on the basis of occlusion and long term spatio-temporal structure cues. User supervision is incorporated in a superpixel graph clustering framework that differs crucially from prior art in that it modifies the graph according to the output of an occlusion boundary detector. Working with long temporal intervals (up to 100 frames) enables our system to significantly reduce annotation effort with respect to state of the art systems. Even though the segmentation results are less than perfect, they are obtained efficiently and can be used in weakly supervised learning from video or for video content description. We do not rely on a discrimina-tive object appearance model and allow extracting multiple foreground objects together, saving user time if more than one object is present. Additional experiments with unsu-pervised clustering based on occlusion boundaries demonstrate the importance of this cue for video segmentation and thus validate our system design.","cites":"3","conferencePercentile":"60.89108911"},{"venue":"WACV","id":"4899890b69c2594841cfde1691065b89d1c28873","venue_1":"WACV","year":"2014","title":"Object co-labeling in multiple images","authors":"Xi Chen, Arpit Jain, Larry S. Davis","author_ids":"1714741, 3146327, 1693428","abstract":"We introduce a new problem called object co-labeling where the goal is to jointly annotate multiple images of the same scene which do not have temporal consistency. We present an adaptive framework for joint segmentation and recognition to solve this problem. We propose an objective function that considers not only appearance but also appearance and context consistency across images of the scene. A relaxed form of the cost function is minimized using an efficient quadratic programming solver. Our approach improves labeling performance compared to labeling each image individually. We also show the application of our co-labeling framework to other recognition problems such as label propagation in videos and object recognition in similar scenes. Experimental results demonstrates the efficacy of our approach.","cites":"2","conferencePercentile":"49.00990099"},{"venue":"WACV","id":"ec44bf99ba11f3b6d0bb32dd5bc6ac08dd0c63d6","venue_1":"WACV","year":"2016","title":"Combining multiple sources of knowledge in deep CNNs for action recognition","authors":"Eunbyung Park, Xufeng Han, Tamara L. Berg, Alexander C. Berg","author_ids":"2155311, 1682965, 1685538, 1743555","abstract":"Although deep convolutional neural networks (CNNs) have shown remarkable results for feature learning and prediction tasks, many recent studies have demonstrated improved performance by incorporating additional hand-crafted features or by fusing predictions from multiple CNNs. Usually, these combinations are implemented via feature concatenation or by averaging output prediction scores from several CNNs. In this paper, we present new approaches for combining different sources of knowledge in deep learning. First, we propose feature amplification, where we use an auxiliary, hand-crafted, feature (e.g. optical flow) to perform spatially varying soft-gating on intermediate CNN feature maps. Second, we present a spatially varying multiplicative fusion method for combining multiple CNNs trained on different sources that results in robust prediction by amplifying or suppressing the feature activations based on their agreement. We test these methods in the context of action recognition where information from spatial and temporal cues is useful, obtaining results that are comparable with state-of-the-art methods and outperform methods using only CNNs and optical flow features.","cites":"5","conferencePercentile":"87.87878788"},{"venue":"WACV","id":"71897661483d90ab9a205e3b2e98c8216fd9ebd2","venue_1":"WACV","year":"2009","title":"Reading challenging barcodes with cameras","authors":"Orazio Gallo, Roberto Manduchi","author_ids":"3218156, 1737048","abstract":"Current camera-based barcode readers do not work well when the image has low resolution, is out of focus, or is motion-blurred. One main reason is that virtually all existing algorithms perform some sort of binarization, either by gray scale thresholding or by finding the bar edges. We propose a new approach to barcode reading that never needs to binarize the image. Instead, we use deformable barcode digit models in a maximum likelihood setting. We show that the particular nature of these models enables efficient integration over the space of deformations. Global optimization over all digits is then performed using dynamic programming. Experiments with challenging UPC-A barcode images show substantial improvement over other state-of-the-art algorithms.","cites":"4","conferencePercentile":"57.62711864"},{"venue":"WACV","id":"767daed62ac32e5e158a7a775c882847a73376cb","venue_1":"WACV","year":"2009","title":"Non-rigid registration of 3D facial surfaces with robust outlier detection","authors":"Moritz Kaiser, Andre Störmer, Dejan Arsic, Gerhard Rigoll","author_ids":"1719227, 2692703, 1740530, 1705843","abstract":"Non-rigid registration of 3D facial surfaces is a crucial step in a variety of applications. Outliers, i.e., features in a facial surface that are not present in the reference face, often perturb the registration process. In this paper, we present a novel method which registers facial surfaces reliably also in the presence of huge outlier regions. A cost function incorporating several channels (red, green, blue, etc.) is proposed. The weight of each point of the facial surface in the cost function is controlled by a weight map, which is learned iteratively. Ideally, outliers will get a zero weight so that their disturbing effect is decreased. Results show that with an intelligent initialization the weight map improves the registration results considerably.","cites":"3","conferencePercentile":"48.30508475"}]}