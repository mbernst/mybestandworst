{"WASPAA.csv":[{"venue":"WASPAA","id":"73c808e1bad37970a8f6b78448fcd9371e8414ef","venue_1":"WASPAA","year":"2015","title":"Modeling musical rhythmatscale with the music Genome project","authors":"Matthew Prockup, Andreas F. Ehmann, Fabien Gouyon, Erik M. Schmidt, Youngmoo E. Kim","author_ids":"2227419, 1742797, 3173571, 2040943, 1730150","abstract":"Musical meter and attributes of the rhythmic feel such as swing, syncopation, and danceability are crucial when defining musical style. However, they have attracted relatively little attention from the Music Information Retrieval (MIR) community and, when addressed , have proven difficult to model from music audio signals. In this paper, we propose a number of audio features for model-ing meter and rhythmic feel. These features are first evaluated and compared to timbral features in the common task of ballroom genre classification. These features are then used to learn individual models for a total of nine rhythmic attributes covering meter and feel using an industrial-sized corpus of over one million examples labeled by experts from Pandora Internet Radio's Music Genome Project. Linear models are shown to be powerful, representing these attributes with high accuracy at scale.","cites":"1","conferencePercentile":"62.96296296"},{"venue":"WASPAA","id":"6d13329860137bfabf3afbcfbfdce6d4e751a2af","venue_1":"WASPAA","year":"2015","title":"Directional NMF for joint source localization and separation","authors":"Johannes Traa, Paris Smaragdis, Noah D. Stein, David Wingate","author_ids":"3146432, 1718742, 8032118, 1798193","abstract":"We propose an unsupervised method for simultaneously localizing and separating speech signals by factorizing a non-negative matrix of Steered Response Power (SRP) measurements. We use a prob-abilistic interpretation of the SRP function to compute a wideband SRP matrix. Non-negative Matrix Factorization (NMF) is used to decompose it into three terms that describe (1) the source distributions over spatial directions, (2) the overall source activations, and (3) the source activations over the time-frequency (TF) plane. The first term indicates the sources' Directions-of-Arrival (DOA) and the latter two terms provide TF weights for separating the sources. Experiments show that this joint approach out-performs a sequential SRP localization + beamforming method.","cites":"1","conferencePercentile":"62.96296296"},{"venue":"WASPAA","id":"90d0328e6e34041625deb88211d5a9e3021fac9f","venue_1":"WASPAA","year":"2015","title":"Sidechain harmonic enhancement of noise corrupted speech for hearing impaired listeners","authors":"Kamil K. Wójcicki, Kelly Fitz, Karrie Recker, Don Reynolds, Tao Zhang","author_ids":"2501445, 6886518, 2227601, 1969720, 1743147","abstract":"This work presents a single channel speech enhancement approach aimed at improving speech clarity for hearing impaired listeners under challenging listening conditions. The proposed method applies nonlinear distortions to speech components isolated from the observed noisy signal using aggressive speech enhancement. The enhanced components are then mixed back into the noisy signal. The results show that the proposed approach significantly improves speech clarity in noise.","cites":"0","conferencePercentile":"24.07407407"},{"venue":"WASPAA","id":"ae3b96579032d18a7e143abaf2bc1e56ee9008ff","venue_1":"WASPAA","year":"2009","title":"A novel framework for recognizing phonemes of singing voice in polyphonic music","authors":"Hiromasa Fujihara, Masataka Goto, Hiroshi G. Okuno","author_ids":"2536751, 1720652, 1775800","abstract":"A novel method is described that can be used to recognize the phoneme of a singing voice (vocal) in polyphonic music. Though we focus on the voiced phoneme in this paper, this method is design to concurrently recognize other elements of a singing voice such as fundamental frequency and singer. Thus, this method is considered to be a new framework for recognizing a singing voice in polyphonic music. Our method stochastically models a mixture of a singing voice and other instrumental sounds without segregating the singing voice. It can also estimate a reliable spectral envelope by estimating it from many harmonic structures with various fundamental frequencies (F0s). The results of phoneme recognition experiments with 10 popular-music songs by 6 singers showed that our method improves the recognition accuracy by 8.7 points and achieves a 20.0% decrease in error rate. 1. INTRODUCTION Music is an important media content in both industrial and cultural aspects, and a singing voice (vocal) is one of the most important elements of music. Among the many other considerable elements of a singing voice, we deemed the lyrics, voice quality (or name of a singer), and fundamental frequencies (F0s) to be the most fundamental elements of a singing voice. To develop a computer that can recognize a singing voice, we have been working on an automatic synchronization method for singing voice and lyrics [1], an automatic singer identification method [2], and an F0 estimation method for a singing voice [3]. As a stepping stone to further develop these methods, we propose a novel framework for recognizing a singing voice within polyphonic music, which is named the W-PST (Weighted-composition of Probabilistic Spectral Template) method. We also verify the effectiveness of W-PST method by conducting a phoneme recognition experiment on condition that correct F0s are given. Although we mainly focused on lyrics, we designed W-PST method to be applicable in recognizing F0s and names of singers. Research in automatic lyric recognition from polyphonic music is important because it can be directly applied to a new music information retrieval system. Even if perfect lyric recognition cannot be achieved, the basic techniques for recognizing the phonemes can be used in an automatic synchronization method for lyrics and music. However, automatic recognition of lyrics (or phonemes) of a singing voice is more difficult than that of speech because a singing voice fluctuates wildly due to the vibrato, a wide range of fundamental …","cites":"6","conferencePercentile":"64.1509434"},{"venue":"WASPAA","id":"391807fc795be0c6c94b6fb02d34a3f77e11f519","venue_1":"WASPAA","year":"2009","title":"Regularized HRTF fitting using spherical harmonics","authors":"Dmitry N. Zotkin, Ramani Duraiswami, Nail A. Gumerov","author_ids":"1806440, 1719541, 1697880","abstract":"By the Helmholtz reciprocity principle, the head-related transfer function (HRTF) is equivalent to an acoustic field created by a transmitter placed at the ear location. Therefore, it can be represented as a spherical harmonics spectrum – a weighted sum of spherical harmonics. Such representations are useful in theoretical and computational analysis. Many different (often severely undersampled) grids are used for HRTF measurement, making the spectral reconstruction difficult. In this paper, two methods of obtaining the spectrum are presented and analyzed both on synthetic (ground-truth data available) and real HRTF measurements. 1. INTRODUCTION Sound localization cues used by humans are produced by scattering of the acoustic wave off the listener's anatomical parts (in particular , outer ear (pinna), head, and torso) [1]. The head related transfer function (HRTF) quantifies how the signal is modified by this scattering as a function of arrival direction and of source range. It is usually assumed that the range effects are small at typical measurement distances. If the HRTF of a particular person is known, the illusion of a sound arriving from a specific direction can be created [2]. However, HRTFs are different for different individuals and consequently HRTF measurement is necessary for accurate spatial audio reproduction. During such a measurement, a sound source (e.g., a loudspeaker) is moved to many positions on a certain grid around the subject, and at each position a test signal is produced by the source and is recorded by a microphone placed in the subject's ear, directly giving the desired transfer function. It can be shown that the infinite-range HRTF for direction (θ, ϕ) is just the acoustic field potential at the ear when the incoming acoustic field is a plane wave arriving from (θ, ϕ) [3]. Recently, an alternative way of measuring HRTF was proposed and tested in [4]. Assume that in a linear time-invariant scene the source and the receiver positions are swapped. By the Helmholtz reciprocity principle, the recordings done before and after the swap must be identical. It follows that the HRTF can be measured reciprocally by placing the (miniature) loudspeaker in the person's ear and recording the test signal in parallel by a number of microphones arranged around the subject. That means that HRTF is equivalent to the potential of an acoustic field created by the loudspeaker and sampled by microphones (many authors use this fact when computing HRTF using numerical methods on a subject's …","cites":"8","conferencePercentile":"75.47169811"},{"venue":"WASPAA","id":"58a7506525ed499c085a0e25bb21198a453690b6","venue_1":"WASPAA","year":"2013","title":"Hierarchical modeling using automated sub-clustering for sound event recognition","authors":"Maria E. Niessen, Tim van Kasteren, Andreas Merentitis","author_ids":"3185944, 1972036, 1808603","abstract":"The automatic recognition of sound events allows for novel applications in areas such as security, mobile and multimedia. In this work we present a hierarchical hidden Markov model for sound event detection that automatically clusters the inherent structure of the events into sub-events. We evaluate our approach on an IEEE audio challenge dataset consisting of office sound events and provide a systematic comparison of the various building blocks of our approach to demonstrate the effectiveness of incorporating certain dependencies in the model. The hierarchical hidden Markov model achieves an average frame-based F-measure recognition performance of 45.5% on a test dataset that was used to evaluate challenge submissions. We also show how the hierarchical model can be used as a meta-classifier, although in the particular application this did not lead to an increase in performance on the test dataset.","cites":"1","conferencePercentile":"32.89473684"},{"venue":"WASPAA","id":"87de34c31891955c71b39afa273d0260d9ffb24a","venue_1":"WASPAA","year":"2013","title":"Virtual autoencoder based recommendation system for individualizing head-related transfer functions","authors":"Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami","author_ids":"2908285, 1806440, 1719541","abstract":"We propose a virtual autoencoder based recommendation system for learning a user's Head-related Transfer Functions (HRTFs) without subjecting a listener to impulse response or anthropometric measurements. Autoencoder neural-networks generalize principal component analysis (PCA) and learn non-linear feature spaces that supports both out-of-sample embedding and reconstruction; this may be applied to developing a more expressive low-dimensional HRTF representation. One application is to individualize HRTFs by tuning along the autoencoder feature spaces. We demonstrate this new approach by developing a virtual (black-box) user that can localize sound from query HRTFs reconstructed from those spaces. Standard optimization methods tune the autoencoder features based on the virtual user's feedback. Experiments with CIPIC HRTFs show that the virtual user can localize along out-of-sample directions and that optimization in the autoencoder feature space improves upon initial non-individualized HRTFs. Other applications of the representation are also discussed.","cites":"3","conferencePercentile":"69.73684211"},{"venue":"WASPAA","id":"fa5c48351005594e6d0705f109b94c6066b5d7c2","venue_1":"WASPAA","year":"2013","title":"Gaussian process data fusion for heterogeneous HRTF datasets","authors":"Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami","author_ids":"2908285, 1806440, 1719541","abstract":"Head-Related Transfer Function (HRTF) measurement and extraction are important tasks for personalized-spatial audio. Many laboratories have their own apparatuses for data-collection but few studies have compared their results to a common subject or have mod-eled inter-dataset variances. We present a Bayesian fusion method based on Gaussian process (GP) modeling of joint spatial-frequency HRTFs over different spherical-measurement grids. Neumann KU-100 dummy HRTFs from 7 labs in the \" Club Fritz \" study are compared and fused to each other based on learning a set of transformations from the GP data-likelihood and covariance assumptions; parameter and hyperparameter training is automatic. Experimental results show that fused models for horizontal and median-plane HRTFs generalize the datasets better than non-transformed ones.","cites":"5","conferencePercentile":"82.89473684"},{"venue":"WASPAA","id":"a586dfdf5b8fd03a2c52e5e46b777cce3b56104c","venue_1":"WASPAA","year":"2015","title":"Synthesis of a sound field scattered by a virtual object using near-field compensated higher-order Ambisonics","authors":"Nara Hahn, Sascha Spors","author_ids":"1930215, 2265188","abstract":"For a physically accurate reproduction of a complex virtual sound field, the interaction of sound waves with objects or boundaries have to be taken into account. In this paper, we attempt to synthesize a sound field scattered by an acoustic obstacle. Based on an analytic representation of the incident and scattered sound fields, the loudspeaker driving functions are derived. We explicitly pay attention to the sound obstruction effect behind the scatterer, which is caused by the destructive interference of the incident sound field and the forward scattering. The physical properties of the presented approach are examined through numerical simulations, and the perceptual aspects are discussed in an informal listening test.","cites":"0","conferencePercentile":"24.07407407"},{"venue":"WASPAA","id":"e2a48f965d8a0b14bdad415bf41d99e7db4a1d5a","venue_1":"WASPAA","year":"2011","title":"Efficient realization of model-based rendering for 2.5-dimensional near-field compensated higher order Ambisonics","authors":"Sascha Spors, Vincent Kuscher, Jens Ahrens","author_ids":"2265188, 3169359, 1935254","abstract":"Near-field compensated higher order Ambisonics is a sound field synthesis technique which is based upon a mathematical representation in terms of surface spherical harmonics. The generation of loudspeaker driving signals using digital signal processing is a numerically challenging task due to the involved special functions. This paper presents an efficient algorithm for 2.5-dimensional near-field compensated higher order Ambisonics. It is based upon a parametric representation of recursive filters realized in first-and second-order sections. The performance of the proposed technique is evaluated at an illustrative example.","cites":"2","conferencePercentile":"35.36585366"},{"venue":"WASPAA","id":"f30de215a9bd803d1f4df09683c4efab499fd20f","venue_1":"WASPAA","year":"2009","title":"Artifacts in the sound field of a moving sound source reconstructed from a microphone array recording","authors":"Jens Ahrens, Sascha Spors","author_ids":"1935254, 2265188","abstract":"We present an analysis of the sound field of a moving sound source reconstructed from recordings of a virtual dual-radius open-sphere microphone array. As a consequence of the discrete property of such microphone distributions artifacts arise, most notably spatial aliasing and spatial bandwidth limitation artifacts. We show that these artifacts are much more pronounced for moving sound sources than for static ones. We analyze the artifacts with a focus on a possible perceptual impairment when such recordings are used for audition purposes.","cites":"0","conferencePercentile":"3.773584906"},{"venue":"WASPAA","id":"722125ff39c545ff10c4f349bac2d7f37913f4bf","venue_1":"WASPAA","year":"2011","title":"Excitation modeling and synthesis for plucked guitar tones","authors":"Raymond Migneco, Youngmoo E. Kim","author_ids":"1737371, 1730150","abstract":"The analysis and synthesis of plucked-guitar tones via source-filter approximations is a popular and established method for modeling the resonant behavior of the string as well as the driving excitation signal. By varying the source signal, a nearly unlimited number of unique tones can be produced using a given filter model. However , it as unclear as to how exactly the model excitation signals should be parameterized in order to capture the nuances of a gui-tarist's articulation from a recorded performance. In this paper, we apply principal components analysis to a corpus of excitation signals derived from plucked-guitar recordings in order to design a codebook that captures the unique characteristics of certain string articulations. The development of an excitation codebook has several applications, including expressive synthesis of guitar tones for virtual music interfaces and insight into the expressive intentions of a performer through audio analysis.","cites":"3","conferencePercentile":"43.90243902"},{"venue":"WASPAA","id":"ba7894cd2605a782a2d6b5db5200a23fad11d829","venue_1":"WASPAA","year":"2009","title":"Improving separation of harmonic sources with iterative estimation of spatial cues","authors":"Jinyu Han, Bryan Pardo","author_ids":"2803402, 1744936","abstract":"Spatial cues (cross-channel amplitude and phase difference coefficients) have been widely used in source separation of two-channel mixtures. However, as sources increasingly overlap in the time-frequency domain or the angle between sources decreases, these spatial cues become unreliable. We introduce an iterative method to re-estimate the spatial cues for mixtures of harmonic sources. Results on a set of three-source mixtures of musical instruments show this approach significantly improves separation performance of two existing time-frequency masking systems.","cites":"2","conferencePercentile":"29.24528302"},{"venue":"WASPAA","id":"7fb5c2c24b9f7c182b8ec1670d69e9b840fc9ada","venue_1":"WASPAA","year":"2015","title":"Method of moments learning for left-to-right Hidden Markov models","authors":"Y. Cem Sübakan, Johannes Traa, Paris Smaragdis, Daniel J. Hsu","author_ids":"3422615, 3146432, 1718742, 2027185","abstract":"We propose a method-of-moments algorithm for parameter learning in Left-to-Right Hidden Markov Models. Compared to the conventional Expectation Maximization approach, the proposed algorithm is computationally more efficient, and hence more appropriate for large datasets. It is also asymptotically guaranteed to estimate the correct parameters. We show the validity of our approach with a synthetic data experiment and a word utterance onset detection experiment .","cites":"0","conferencePercentile":"24.07407407"},{"venue":"WASPAA","id":"acc8b1545b5880632de638efcee21cfb03e34df2","venue_1":"WASPAA","year":"2011","title":"Learning emotion-based acoustic features with deep belief networks","authors":"Erik M. Schmidt, Youngmoo E. Kim","author_ids":"2040943, 1730150","abstract":"The medium of music has evolved specifically for the expression of emotions, and it is natural for us to organize music in terms of its emotional associations. But while such organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions on the emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) representation of affect as a time-varying, stochastic distribution. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this new work we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. While the system is applied to the specific problem of music emotion recognition, it could be easily applied to any regression-based audio feature learning problem.","cites":"20","conferencePercentile":"92.68292683"},{"venue":"WASPAA","id":"9237ac2a602ee348b637c22f140a86982363397b","venue_1":"WASPAA","year":"2009","title":"Blind alignment of asynchronously recorded signals for distributed microphone array","authors":"Nobutaka Ono, Hitoshi Kohno, Nobutaka Ito, Shigeki Sagayama","author_ids":"1733587, 7324005, 1798203, 1734761","abstract":"In this paper, aiming to utilize independent recording devices as a distributed microphone array, we present a novel method for alignment of recorded signals with localizing microphones and sources. Unlike conventional microphone array, signals recorded by independent devices have different origins of time, and microphone positions are generally unknown. In order to estimate both of them from only recorded signals, time differences between channels for each source are detected, which still include the differences of time origins, and an objective function defined by their square errors is minimized. For that, simple iterative update rules are derived through auxiliary function approach. The validity of our approach is evaluated by simulative experiment.","cites":"20","conferencePercentile":"94.33962264"},{"venue":"WASPAA","id":"4549cef63bf1f4e3b6f3451ae8b830f10a4e11e3","venue_1":"WASPAA","year":"2015","title":"Rotational reset strategy for online semi-supervised NMF-based speech enhancement for long recordings","authors":"Jun Zhou, Shuo Chen, Zhiyao Duan","author_ids":"1728391, 4857085, 3270912","abstract":"Non-negative matrix factorization (NMF) has been successfully applied to speech enhancement in non-stationary noisy environments. Recently proposed online semi-supervised NMF algorithms are of particular interest as they carry the two nice properties (online and semi-supervised) of classical speech enhancement approaches. These algorithms, however, have only been evaluated using noisy mixtures shorter than 30 seconds. In this paper we find that these algorithms work well when it is run for less than 1 minute, but degradation of the enhanced speech signal starts to appear after 2 minutes. We analyze that the reason is due to the inappropriate dictionary update rule, which gradually loses its ability in updating the speech dictionary. We then propose a simple rotational reset strategy to solve the problem: Instead of continuously updating the entire speech dictionary, we periodically and rotationally select elements and reset their values to random numbers. Experiments show that this strategy successfully solves the degradation problem and the improved algorithm outperforms classical speech enhancement algorithms significantly even when they are run for 10 minutes.","cites":"0","conferencePercentile":"24.07407407"},{"venue":"WASPAA","id":"c210a4d47b02ec8cac9284990571cbaa9eb6785d","venue_1":"WASPAA","year":"2011","title":"New formulations and efficient algorithms for multichannel NMF","authors":"Hiroshi Sawada, Hirokazu Kameoka, Shoko Araki, Naonori Ueda","author_ids":"1741725, 1787190, 1727951, 1735221","abstract":"This paper proposes new formulations and algorithms for a multi-channel extension of nonnegative matrix factorization (NMF), intending convolutive sound source separation with multiple microphones. The proposed formulation employs Hermitian positive semidefinite matrices to represent a multichannel version of non-negative elements. Such matrices are basically estimated for NMF bases, but a source separation task can be performed by introducing variables that relate NMF bases and sources. Efficient optimization algorithms in the form of multiplicative updates are derived by using properly designed auxiliary functions. Experimental results show that two instrumental sounds coming from different directions were successfully separated by the proposed algorithm.","cites":"6","conferencePercentile":"67.07317073"},{"venue":"WASPAA","id":"f470d22bd79c608d91004484c931724e6e6051dc","venue_1":"WASPAA","year":"2013","title":"Non-negative matrix factorization for irregularly-spaced transforms","authors":"Paris Smaragdis, Minje Kim","author_ids":"1718742, 1708560","abstract":"Non-negative factorizations of spectra have been a very popular tool for various audio tasks recently. A long-standing problem with these methods methods is that they cannot be easily applied on other kinds of spectral decompositions such as sinusoidal models, constant-Q transforms, wavelets and reassigned spectra. This is because with these transforms the frequency and/or time values are real-valued and not sampled on a regular grid. We therefore cannot represent them as a matrix that we can later factorize. In this paper we present a formulation of non-negative matrix factorization that can be applied on data with real-valued indices, thereby making the application of this family of methods feasible on a broader family of time/frequency transforms.","cites":"1","conferencePercentile":"32.89473684"},{"venue":"WASPAA","id":"52a8ecbcb1e30c5f51ea9d70eba027128be2bdf1","venue_1":"WASPAA","year":"2009","title":"Separation by \"humming\": User-guided sound extraction from monophonic mixtures","authors":"Paris Smaragdis, Gautham J. Mysore","author_ids":"1718742, 1781063","abstract":"In this paper we present a novel approach for isolating and removing sounds from dense monophonic mixtures. The approach is user-based, and requires the presentation of a guide sound that mimics the desired target the user wishes to extract. The guide sound can be simply produced from a user by vocalizing or otherwise replicating the target sound marked for separation. Using that guide as a prior in a statistical sound mixtures model, we propose a methodology that allows us to efficiently extract complex structured sounds from dense mixtures.","cites":"49","conferencePercentile":"100"},{"venue":"WASPAA","id":"acb02faf84ada295fad8320c8833c1a0994ec22f","venue_1":"WASPAA","year":"2011","title":"Polyphonic pitch tracking by example","authors":"Paris Smaragdis","author_ids":"1718742","abstract":"We introduce a novel approach for pitch tracking of multiple sources in mixture signals. Unlike traditional approaches to pitch tracking, which explicitly attempt to detect periodicities, this approach is using a learning framework by making use of previously pitch-tagged recordings as training data to teach spectrum/pitch associations. We show how the mixture case of this task is a nearest subspace search problem which is efficiently solved by transforming it to an overcomplete sparse coding formulation. We demonstrate the use of this algorithm with real mixtures ranging from solo up to a quintet recordings.","cites":"6","conferencePercentile":"67.07317073"},{"venue":"WASPAA","id":"37b50325437deb49b7d8619b250292a49e99be4a","venue_1":"WASPAA","year":"2011","title":"A temporally-constrained convolutive probabilistic model for pitch detection","authors":"Emmanouil Benetos, Simon Dixon","author_ids":"2109397, 1715788","abstract":"A method for pitch detection which models the temporal evolution of musical sounds is presented in this paper. The proposed model is based on shift-invariant probabilistic latent component analysis, constrained by a hidden Markov model. The time-frequency representation of a produced musical note can be expressed by the model as a temporal sequence of spectral templates which can also be shifted over log-frequency. Thus, this approach can be effectively used for pitch detection in music signals that contain amplitude and frequency modulations. Experiments were performed using extracted sequences of spectral templates on mono-phonic music excerpts, where the proposed model outperforms a non-temporally constrained convolutive model for pitch detection. Finally, future directions are given for multipitch extensions of the proposed model.","cites":"7","conferencePercentile":"73.17073171"},{"venue":"WASPAA","id":"b23309bd2281d957180af6cc2daba3fd182d6593","venue_1":"WASPAA","year":"2015","title":"Incorporating AM-FM effect in voiced speech for probabilistic acoustic tube model","authors":"Yang Zhang, Zhijian Ou, Mark Hasegawa-Johnson","author_ids":"4449904, 1717830, 1744418","abstract":"A complete speech model can improve performance for many speech applications. Probabilistic Acoustic Tube (PAT) is a proba-bilistic generative model of speech that has been shown potentially useful in a number of speech processing tasks. A point missing in previous PAT models is that they overlook AM/FM effect in voiced speech, which is in fact common and non-negligible. In this paper, we significantly improve the voiced modeling of PAT with a proba-bilistic model of AM/FM effect, which is developed from Bayesian Spectrum Estimation method. Experiments show that the new PAT is able to fit the voiced speech spectrum with greater accuracy in the presence of AM/FM effect.","cites":"1","conferencePercentile":"62.96296296"},{"venue":"WASPAA","id":"7b3b8b0bd2364118e463e65b135ca2e6316ceb46","venue_1":"WASPAA","year":"2013","title":"Low-artifact source separation using probabilistic latent component analysis","authors":"Nasser Mohammadiha, Paris Smaragdis, Arne Leijon","author_ids":"3058699, 1718742, 1776262","abstract":"We propose a method based on the probabilistic latent component analysis (PLCA) in which we use exponential distributions as priors to decrease the activity level of a given basis vector. A straightforward application of this method is when we try to extract a desired source from a mixture with low artifacts. For this purpose, we propose a maximum a posteriori (MAP) approach to identify the common basis vectors between two sources. A low-artifact estimate can now be obtained by using a constraint such that the common basis vectors in the interfering signal's dictionary tend to remain inactive. We discuss applications of this method in source separation with similar-gender speakers and in enhancing a speech signal that is contaminated with babble noise. Our simulations show that the proposed method not only reduces the artifacts but also increases the overall quality of the estimated signal.","cites":"1","conferencePercentile":"32.89473684"}]}