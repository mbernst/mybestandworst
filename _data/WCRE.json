{"WCRE.csv":[{"venue":"WCRE","id":"c392967a4ae8676544b77394eb7e75aeb8907dbe","venue_1":"WCRE","year":"2011","title":"Requirements Traceability for Object Oriented Systems by Partitioning Source Code","authors":"Nasir Ali, Yann-Gaël Guéhéneuc, Giuliano Antoniol","author_ids":"2197997, 1718050, 1692292","abstract":"—Requirements traceability ensures that source code is consistent with documentation and that all requirements have been implemented. During software evolution, features are added, removed, or modified, the code drifts away from its original requirements. Thus traceability recovery approaches becomes necessary to re-establish the traceability relations between requirements and source code. This paper presents an approach (Coparvo) complementary to existing traceability recovery approaches for object-oriented programs. Coparvo reduces false positive links recovered by traditional traceability recovery processes thus reducing the manual validation effort. Coparvo assumes that information extracted from different entities (e.g., class names, comments, class variables, or methods signatures) are different information sources; they may have different level of reliability in requirements traceability and each information source may act as a different expert recommending traceability links. We applied Coparvo on three data sets, Pooka, SIP Communicator , and iTrust, to filter out false positive links recovered via the information retrieval approach i.e., vector space model. The results show that Coparvo significantly improves the of the recovered links accuracy and also reduces up to 83% effort required to manually remove false positive links.","cites":"11","conferencePercentile":"74.28571429"},{"venue":"WCRE","id":"844abffde65aebe4fe16b8cedfc8b1ccbdf2871c","venue_1":"WCRE","year":"1997","title":"Program Plan Recognition for Year 2000 Tools","authors":"Arie van Deursen, Steven Woods, Alex Quilici","author_ids":"1737202, 3052532, 2751736","abstract":"There are many commercial tools that address various aspects of the Year 2000 problem. None of these tools, however , make any documented use of plan-based techniques for automated concept recovery. This implies a general perception that plan-based techniques is not useful for this problem. This paper argues that this perception is incorrect and these techniques are in fact mature enough to make a significant contribution. In particular, we show representative code fragments illustrating \" Year 2000 \" problems, discuss the problems inherent in recognizing the higher level concepts these fragments implement using pattern-based and rule-based techniques, demonstrate that they can be represented in a programming plan framework, and present some initial experimental evidence that suggests that current algorithms can locate these plans in linear time. Finally , we discuss several ways to integrate plan-based techniques with existing Year 2000 tools. Note: Work carried out under project SEN-1.1, Software Renovation.","cites":"20","conferencePercentile":"55.55555556"},{"venue":"WCRE","id":"d09dbae801c56db074f8fe858db6fcda7b5635a2","venue_1":"WCRE","year":"2001","title":"Program Comprehension Risks and Opportunities in Extreme Programming","authors":"Arie van Deursen","author_ids":"1737202","abstract":"We investigate the relationship between reverse engineering and program comprehension on the one hand, and software process on the other. To understand this relationship, we select one particular existing software process, extreme programming (XP), and study the role played in it by program comprehension and reverse engineering. To that end, we analyze five key XP practices in depth: pair programming, unit testing, refactoring, evolutionary design, and collaborative planning. The contributions of this paper are (1) the identification of promising research areas in the field of program comprehension ; (2) the identification of new application perspectives for reverse engineering technology; (3) a critical analysis of XP resulting in research questions that could help resolve some of the uncertainties surrounding XP; (4) a process assessment framework for analyzing software processes from the comprehension and reverse engineering point of view. Note: Work carried out under projects SEN 1.1, Software Renovation and SEN 1.3, Domain-Specific Languages.","cites":"22","conferencePercentile":"59.52380952"},{"venue":"WCRE","id":"15c0bc7e08249ceaf98b635d765860af17cc00be","venue_1":"WCRE","year":"2008","title":"A Bayesian Network Based Approach for Change Coupling Prediction","authors":"Yu Zhou, Michael Würsch, Emanuel Giger, Harald C. Gall, Jian Lu","author_ids":"1696179, 1712335, 1732227, 1707328, 3782814","abstract":"Source code coupling and change history are two important data sources for change coupling analysis. The popularity of public open source projects in recent years makes both sources available. Based on our previous research , in this paper, we inspect different dimensions of software changes including change significance or source code dependency levels, extract a set of features from the two sources and propose a bayesian network-based approach for change coupling prediction. By combining the features from the co-changed entities and their dependency relation , the approach can model the underlying uncertainty. The empirical case study on two medium-sized open source projects demonstrates the feasibility and effectiveness of our approach compared to previous work.","cites":"24","conferencePercentile":"86.20689655"},{"venue":"WCRE","id":"618ad3376480912aed3b6418b2f05aef1a573695","venue_1":"WCRE","year":"2001","title":"REportal: A Web-Based Portal Site for Reverse Engineering","authors":"Spiros Mancoridis, Timothy S. Souder, Yih-Farn Robin Chen, Emden R. Gansner, Jeffrey L. Korn","author_ids":"2693288, 2885690, 2231574, 1725593, 2757949","abstract":"We present a web-based portal site for reverse engineering software systems called REportal. REportal enables authorized users to upload their code to a secure web site and then, through the guide of wizards, browse and analyze their code. Currently the portal services include code analysis, browsing, querying, and design extraction for C, C++, and Java programs. The REportal services are implemented by several reverse engineering tools that our team has developed over the years. With this work, we aim to assist professional software engineers, educators, and other researchers who need to analyze code. Specifically, we present a technology that provides a simple and easily accessible user interface to a number of reverse engineering tools. More importantly, this technology saves the user from the time and effort required to install, administer, and integrate these tools.","cites":"24","conferencePercentile":"71.42857143"},{"venue":"WCRE","id":"38144db7fac1cf7bdf81802833fa571186f87fe4","venue_1":"WCRE","year":"2012","title":"ATLANTIS - Assembly Trace Analysis Environment","authors":"Brendan Cleary, Margaret-Anne D. Storey, Laura Chan, Martin Salois, Frédéric Painchaud","author_ids":"1913718, 1734938, 2436928, 1738842, 1770182","abstract":"For malware authors, software is an ever fruitful source of vulnerabilities to exploit. Exploitability assessment through fuzzing aims to proactively identify potential vulnerabilities by monitoring the execution of a program while attempting to induce a crash. In order to determine if a particular program crash is exploitable (and to create a patch), the root cause of the crash must be identified. For particular classes of programs this analysis must be conducted without the aid of the original source code using execution traces generated at the assembly layer. Currently this analysis is a highly manual, text-driven activity with poor tool support. In this paper we present ATLANTIS, an assembly trace analysis environment that combines many of the features of modern IDEs with novel trace annotation and navigation techniques to support software security engineers performing exploitability analysis.","cites":"1","conferencePercentile":"9.677419355"},{"venue":"WCRE","id":"be98d6024e15730b2e60cb41ccf04f8ea2765b79","venue_1":"WCRE","year":"2007","title":"Documenting Typical Crosscutting Concerns","authors":"Marius Marin, Leon Moonen, Arie van Deursen","author_ids":"1780656, 1762006, 1737202","abstract":"Our analysis of crosscutting concerns in real-life software systems (totaling over 500,000 LOC) and in reports from literature indicated a number of properties that allow for their decomposition in primitive building blocks which are atomic crosscutting concerns. We classify these blocks in crosscut-ting concern sorts, and we use them to describe the cross-cutting structure of many (well-known) designs and common mechanisms in software systems. In this paper, we formalize the notion of crosscutting concern sorts by means of relational queries over (object-oriented) source models. Based on these queries, we present a concern management tool called SOQUET, which can be used to document the occurrences of crosscutting concerns in object-oriented systems. We assess the sorts-based approach by using the tool to cover various crosscutting concerns in two open-source systems: JHOTDRAW and Java PETSTORE.","cites":"7","conferencePercentile":"52"},{"venue":"WCRE","id":"1627f985c72f993bff666bb1c21a5d7ed4818740","venue_1":"WCRE","year":"1999","title":"Chava: Reverse Engineering and Tracking of Java Applets","authors":"Jeffrey L. Korn, Yih-Farn Robin Chen, Eleftherios Koutsofios","author_ids":"2757949, 2231574, 2621476","abstract":"Java applets have been used increasingly on web sites to perform client-side processing and provide dynamic content. While many web site analysis tools are available, their focus has been on static HTML content and most ignore ap-plet code completely. This paper presents Chava, a system that analyzes and tracks changes in Java applets. The tool extracts information from applet code about classes, methods , fields and their relationships into a relational database. Supplementary checksum information in the database is used to detect changes in two versions of a Java applet. Given our Java data model, a suite of programs that query, visualize, and analyze the structural information were generated automatically from CIAO, a retargetable reverse engineering system. Chava is able to process either Java source files or compiled class files, making it possible to analyze remote applets whose source code is unavailable. The information can be combined with HTML analysis tools to track both the static and dynamic content of many web sites. This paper presents our data model for Java and describes the implementation of Chava. Advanced reverse engineering tasks such as reachability analysis, clustering, and program differencing can be built on top of Chava to support design recovery and selective regression testing. In particular , we show how Chava is used to compare several Java Development Kit (JDK) versions to help spot changes that might impact Java developers. Performance numbers indicate that the tool scales well.","cites":"41","conferencePercentile":"90.47619048"},{"venue":"WCRE","id":"d48356b37f01ba5d876e45093ba67a6367f9f06c","venue_1":"WCRE","year":"2004","title":"Identifying Aspects Using Fan-In Analysis","authors":"Marius Marin, Arie van Deursen, Leon Moonen","author_ids":"1780656, 1737202, 1762006","abstract":"The issues of code scattering and tangling, thus of achieving a better modularity for a system's concerns, are addressed by the paradigm of aspect orientation. Aspect mining is a reverse engineering process that aims at finding crosscutting concerns in existing systems. This paper describes a technique based on determining methods that are called from many different places (and hence have a high fan-in) to identify candidate aspects in a number of open-source Java systems. The most interesting aspects identified are discussed in detail, which includes several concerns not previously discussed in the aspect-oriented literature. The results show that a significant number of aspects can be recognized using fan-in analysis, and that the technique is suitable for a high degree of automation.","cites":"97","conferencePercentile":"92.30769231"},{"venue":"WCRE","id":"0851da908bc1a90bbf97c129b27404fd1129887b","venue_1":"WCRE","year":"2004","title":"An Initial Experiment in Reverse Engineering Aspects","authors":"Magiel Bruntink, Arie van Deursen, Tom Tourwé","author_ids":"1805327, 1737202, 2850065","abstract":"In this paper, we evaluate the benefits of applying aspect-oriented software development techniques in the context of a large-scale industrial embedded software system implementing a number of crosscutting concerns. Additionally , we assess the feasibility of automatically extracting these crosscutting concerns from the source code. In order to achieve this, we present an approach for reverse engineering aspects from an ordinary application automatically. This approach incorporates both a concern verification and an aspect construction phase. Our results show that such automated support is feasible, and can lead to significant improvements in source code quality.","cites":"7","conferencePercentile":"30.76923077"},{"venue":"WCRE","id":"27ef16f57f3153ee865d404f54654335c711ee3f","venue_1":"WCRE","year":"2006","title":"A common framework for aspect mining based on crosscutting concern sorts","authors":"Marius Marin, Leon Moonen, Arie van Deursen","author_ids":"1780656, 1762006, 1737202","abstract":"reserved. No part of this series may be reproduced in any form or by any means without prior written permission of the publisher. Abstract The increasing number of aspect mining techniques proposed in literature calls for a methodological way of comparing and combining them in order to assess, and improve on, their quality. This paper addresses this situation by proposing a common framework based on crosscutting concern sorts which allows for consistent assessment, comparison and combination of aspect mining techniques. The framework identifies a set of requirements that ensure homogeneity in formulating the mining goals, presenting the results and assessing their quality. We demonstrate feasibility of the approach by retrofitting an existing aspect mining technique to the framework, and by using it to design and implement two new mining techniques. We apply the three techniques to a known aspect mining benchmark and show how they can be consistently assessed and combined to increase the quality of the results. The techniques and combinations are implemented in FINT, our publicly available free aspect mining tool.","cites":"20","conferencePercentile":"66.66666667"},{"venue":"WCRE","id":"08608cfc1fb14feb33e37152ed90d488f40d359b","venue_1":"WCRE","year":"2006","title":"Monitoring Requirements Coverage using Reconstructed Views: An Industrial Case Study","authors":"Marco Lormans, Hans-Gerhard Groß, Arie van Deursen, Rini van Solingen, André Stehouwer","author_ids":"2962315, 2577221, 1737202, 2318579, 2695140","abstract":"Requirements views, such as coverage and status views, are an important asset for monitoring and managing software development. We have developed a method that automates the process for reconstructing these views, and built a tool, ReqAnalyst, to support this method. In this paper, we investigate to what extent we can automatically generate requirements views to monitor requirements in test categories and test cases. The technique used for retrieving the necessary data is an information retrieval technique called Latent Semantic Indexing (LSI). We applied our method in a case study at LogicaCMG. We defined a number of requirements views and experimented with different reconstruction settings to generate these views.","cites":"9","conferencePercentile":"37.03703704"},{"venue":"WCRE","id":"8342e176a73fb4e9619e5969b6f6483230505904","venue_1":"WCRE","year":"2006","title":"FINT: Tool Support for Aspect Mining","authors":"Marius Marin, Leon Moonen, Arie van Deursen","author_ids":"1780656, 1762006, 1737202","abstract":"Aspect mining requires adequate tool support to locate source code elements implementing crosscutting concerns (aka seeds), to explore and understand relations describing these elements, and to manage concerns and seeds during the project's life cycle. FINT is a tool implemented as an Eclipse plug-in that presently supports a number of techniques for the automatic identification of crosscutting concern seeds in source code. Furthermore, FINT allows for combination of mining techniques (results), facilitates code navigation and comprehension to reason and decide about candidate-seeds, and supports seeds management and persistence.","cites":"6","conferencePercentile":"18.51851852"},{"venue":"WCRE","id":"2f482b10014f8dd7ecc66966f9c78b5380175e1d","venue_1":"WCRE","year":"2008","title":"Generating Version Convertors for Domain-Specific Languages","authors":"Gerardo de Geest, Sander Vermolen, Arie van Deursen, Eelco Visser","author_ids":"3346070, 1807786, 1737202, 2447783","abstract":"Domain-specific languages (DSLs) improve programmer productivity by providing high-level abstractions for the development of applications in a particular domain. However, the smaller distance to the application domain entails more frequent changes to the language. As a result, existing DSL models need to be converted to the new version. Manual conversion is tedious and error prone. This paper presents an approach to support DSL evolution by generation of convertors between DSLs. By analyzing the differences between DSL meta-models, a mapping is reverse engineered which can be used to generate reengineer-ing tools to automatically convert models between different versions of a DSL. The approach has been implemented for the Microsoft DSL Tools infrastructure in two tools called DSLCompare and ConverterGenerator. The approach has been evaluated by means of three case studies taken from Avanade's software development practice.","cites":"0","conferencePercentile":"6.896551724"},{"venue":"WCRE","id":"148c6071cdf5cfc09faade289a7f17aa00d2f803","venue_1":"WCRE","year":"2010","title":"Understanding Plug-in Test Suites from an Extensibility Perspective","authors":"Michaela Greiler, Hans-Gerhard Groß, Arie van Deursen","author_ids":"2873548, 2577221, 1737202","abstract":"—Plug-in architectures enable developers to build extensible software products. Such products are assembled from plug-ins, and their functionality can be enriched by adding or configuring plug-ins. The plug-ins themselves consist also of multiple plug-ins, and offer dedicated points through which their functionality can be influenced. A well-known example of such an architecture is Eclipse, best known for its use to create a series of extensible IDEs. In order to test systems built from plug-ins developers use extensive automated test suites. Unfortunately, current testing tools offer little insight in which of the many possible combinations of plug-ins and plug-in configurations are actually tested. To remedy this problem, we propose three architectural views that provide an extensibility perspective on plug-in-based systems and their test suites. The views combine static and dynamic information on plug-in dependencies, extension initialization, and extension usage. The views are implemented in ETSE, the Eclipse Plug-in Test Suite Exploration tool. We evaluate the proposed views by analyzing eGit and Mylyn, two open source Eclipse plug-ins.","cites":"4","conferencePercentile":"35.71428571"},{"venue":"WCRE","id":"a571f45ea6cd1f5f95ffef0acb49ee8d087eafed","venue_1":"WCRE","year":"2008","title":"The Metric Lens: Visualizing Metrics and Structure on Software Diagrams","authors":"Heorhiy Byelas, Alexandru Telea","author_ids":"1800356, 1686665","abstract":"We present the metric lens, a new visualization of method-level code metrics atop UML class diagrams, which allows performing metric-metric and metric-structure correlations on large diagrams. We demonstrate an interactive visualiza-tion tool in which users can quickly specify a wide palette of analyses, based on color-mapping, scaling, and sorting metric tables on UML diagrams. We illustrate our technique and tool by a sample complexity assessment analysis of a real-world C++ software system.","cites":"1","conferencePercentile":"20.68965517"},{"venue":"WCRE","id":"a86ac8cd8a92432301398cfd8d94f8df6be04de6","venue_1":"WCRE","year":"2011","title":"On the Effectiveness of Simhash for Detecting Near-Miss Clones in Large Scale Software Systems","authors":"Md. Sharif Uddin, Chanchal Kumar Roy, Kevin A. Schneider, Abram Hindle","author_ids":"3062780, 1785754, 1786898, 1692741","abstract":"— Clone detection techniques essentially cluster textually, syntactically and/or semantically similar code fragments in or across software systems. For large datasets, similarity identification is costly both in terms of time and memory, and especially so when detecting near-miss clones where lines could be modified, added and/or deleted in the copied fragments. The capability and effectiveness of a clone detection tool mostly depends on the code similarity measurement technique it uses. A variety of similarity measurement approaches have been used for clone detection, including fingerprint based approaches, which have had varying degrees of success notwithstanding some limitations. In this paper, we investigate the effectiveness of simhash, a state of the art fingerprint based data similarity measurement technique for detecting both exact and near-miss clones in large scale software systems. Our experimental data show that simhash is indeed effective in identifying various types of clones in a software system despite wide variations in experimental circumstances. The approach is also suitable as a core capability for building other tools, such as tools for: incremental clone detection, code searching, and clone management.","cites":"25","conferencePercentile":"100"},{"venue":"WCRE","id":"cef32d7865688371b8dc8adf2264ba7d83e44e20","venue_1":"WCRE","year":"2011","title":"An Exploratory Study of Software Reverse Engineering in a Security Context","authors":"Christoph Treude, Fernando Marques Figueira Filho, Margaret-Anne D. Storey, Martin Salois","author_ids":"1685418, 1719483, 1734938, 1738842","abstract":"—Illegal cyberspace activities are increasing rapidly and many software engineers are using reverse engineering methods to respond to attacks. The security-sensitive nature of these tasks, such as the understanding of malware or the decryption of encrypted content, brings unique challenges to reverse engineering: work has to be done offline, files can rarely be shared, time pressure is immense, and there is a lack of tool and process support for capturing and sharing the knowledge obtained while trying to understand plain assembly code. To help us gain an understanding of this reverse engineering work, we report on an exploratory study done in a security context at a research and development government organization to explore their work processes, tools, and artifacts. In this paper, we identify challenges, such as the management and navigation of a myriad of artifacts, and we conclude by offering suggestions for tool and process improvements.","cites":"2","conferencePercentile":"18.57142857"},{"venue":"WCRE","id":"9acba809f4138a11b204db9543f287ea11cea023","venue_1":"WCRE","year":"2013","title":"Leveraging historical co-change information for requirements traceability","authors":"Nasir Ali, Fehmi Jaafar, Ahmed E. Hassan","author_ids":"2197997, 1763625, 1786816","abstract":"—Requirements traceability (RT) links requirements to the corresponding source code entities, which implement them. Information Retrieval (IR) based RT links recovery approaches are often used to automatically recover RT links. However, such approaches exhibit low accuracy, in terms of precision, recall, and ranking. This paper presents an approach (CoChaIR), complementary to existing IR-based RT links recovery approaches. CoChaIR leverages historical co-change information of files to improve the accuracy of IR-based RT links recovery approaches. We evaluated the effectiveness of CoChaIR on three datasets, i.e., iTrust, Pooka, and SIP Communicator. We compared CoChaIR with two different IR-based RT links recovery approaches, i.e., vector space model and Jensen–Shannon divergence model. Our study results show that CoChaIR significantly improves precision and recall by up to 12.38% and 5.67% respectively; while decreasing the rank of true positive links by up to 48% and reducing false positive links by up to 44%.","cites":"1","conferencePercentile":"22.22222222"},{"venue":"WCRE","id":"0c7ea7f429a87eb12ce61f3ae73f9aa3fc4d189d","venue_1":"WCRE","year":"2003","title":"An Experimentation Framework for Evaluating Disassembly and Decompilation Tools for C++ and Java","authors":"Lori Vinciguerra, Linda M. Wills, Nidhi Kejriwal, Paul Martino, Ralph L. Vinciguerra","author_ids":"3245462, 1695824, 1970043, 2300173, 2169153","abstract":"The inherent differences between C++ and Java programs dictate that the methods used for reverse engineering their compiled executables will be language-specific. This paper looks at the history of decompilers, disassemblers, and obfuscators in C++ and Java and presents the current state of the art for binary reverse engineering. An experimentation framework for evaluating tools is described, including methodology, benchmark programs, metrics, and reverse engineering tasks. Preliminary results of experiments conducted so far to assess the capability of a small select set of chosen popular tools are given. These results reveal language-specific differences in the feasibility of the binary reverse engineering tasks on input programs with varying degrees of obfuscation (e.g., stripped vs. unstripped binaries). In addition, the results reveal the relative effort required to complete a task and an assessment of the value of the tools and techniques.","cites":"11","conferencePercentile":"41.66666667"},{"venue":"WCRE","id":"1f02ce5bdc02950a182de6fadf9a6a026d560541","venue_1":"WCRE","year":"2012","title":"SMURF: A SVM-based Incremental Anti-pattern Detection Approach","authors":"Abdou Maiga, Nasir Ali, Neelesh Bhattacharya, Aminata Sabane, Yann-Gaël Guéhéneuc, Esma Aïmeur","author_ids":"3344206, 2197997, 2818885, 2135263, 1718050, 1765537","abstract":"—In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication , and–or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique—support vector machines—and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback.","cites":"10","conferencePercentile":"74.19354839"},{"venue":"WCRE","id":"b9609d3607093e01389e06416d45c748e318fbf9","venue_1":"WCRE","year":"2013","title":"Reconstructing program memory state from multi-gigabyte instruction traces to support interactive analysis","authors":"Brendan Cleary, Patrick Gorman, H. M. W. Verbeek, Margaret-Anne D. Storey, Martin Salois, Frédéric Painchaud","author_ids":"1913718, 2087160, 1734654, 1734938, 1738842, 1770182","abstract":"—Exploitability analysis is the process of attempting to determine if a vulnerability in a program is exploitable. Fuzzing is a popular method of finding such vulnerabilities, in which a program is subjected to millions of generated program inputs until it crashes. Each program crash indicates a potential vulnerability that needs to be prioritized according to its potential for exploitation. The highest priority vulnerabilities need to be investigated by a security analyst by re-executing the program with the input that caused the crash while recording a trace of all executed assembly instructions and then performing analysis on the resulting trace. Recreating the entire memory state of the program at the time of the crash, or at any other point in the trace, is very important for helping the analyst build an understanding of the conditions that led to the crash. Unfortunately, tracing even a small program can create multi-million line trace files from which reconstructing memory state is a computationally intensive process and virtually impossible to do manually. In this paper we present an analysis of the problem of memory state reconstruction from very large execution traces. We report on a novel approach for reconstructing the entire memory state of a program from an execution trace that allows near real-time queries on the state of memory at any point in a program's execution trace. Finally we benchmark our approach showing storage and performance results in line with our theoretical calculations and demonstrate memory state query response times of less than 200ms for trace files up to 60 million lines.","cites":"0","conferencePercentile":"6.944444444"},{"venue":"WCRE","id":"78858c4b32cbb259f9df80718d3adcd2f8a57992","venue_1":"WCRE","year":"2000","title":"Designing an XML-based Exchange Format for Harmonia","authors":"Marat Boshernitsan, Susan L. Graham","author_ids":"1821433, 2111178","abstract":"In this paper we present our design for a program data exchange format for Harmonia, a framework for constructing language-sensitive interactive CASE tools. We discuss the various design issues we faced while developing an encoding for the syntax tree informationin the XML format, including chosing an appropriate encoding, the generation of data schemas based on programming language syntax, and representing program text along with its structure.","cites":"17","conferencePercentile":"68"},{"venue":"WCRE","id":"24465b9ce0cee73a85788252fd924d6d960ffff2","venue_1":"WCRE","year":"1998","title":"Type Inference for COBOL Systems","authors":"Arie van Deursen, Leon Moonen","author_ids":"1737202, 1762006","abstract":"Types are a good starting point for various software reengi-neering tasks. Unfortunately, programs requiring reengi-neering most desperately are written in languages without an adequate type system (such as COBOL). To solve this problem, we propose a method of automated type inference for these languages. The main ingredients are that if variables are compared using some relational operator their types must be the same; likewise if an expression is assigned to a variable, the type of the expression must be a subtype of that of the variable. We present the formal type system and inference rules for this approach, show their effect on various real life COBOL fragments, describe the implementation of our ideas in a prototype type inference tool for COBOL, and discuss a number of applications. Note: Work carried out under project SEN-1.1, Software Renovation.","cites":"40","conferencePercentile":"66.66666667"},{"venue":"WCRE","id":"12507188b60611be90f2434153b48443886c03d6","venue_1":"WCRE","year":"2011","title":"Application Architecture Discovery - Towards Domain-driven, Easily-Extensible Code Structure","authors":"Hitesh Sajnani, Ravindra Naik, Cristina V. Lopes","author_ids":"1906848, 2644656, 8265496","abstract":"—The architecture of a software system and its code structure have a strong impact on its maintainability – the ability to fix problems, and make changes to the system efficiently. To ensure maintainability, software systems are usually organized as subsystems or modules, each with atomically defined responsibilities. However, as the system evolves, the structure of the system undergoes continuous modifications, drifting away from its original design, leading to functionally non-atomic modules and intertwined dependencies between the modules. In this paper, we propose an approach to improve the code structure and architecture by leveraging the domain knowledge of the system. Our approach exploits the knowledge about the functional architecture of the system to restructure the source code and align physically with the functional elements and the re-usable library layers. The approach is validated by applying to a case study which is an existing financial system. The preliminary analysis for the case-study reveals that the approach creates meaningful structure from the legacy code, which enables the developers to quickly identify the code that implements a given functionality.","cites":"4","conferencePercentile":"35.71428571"},{"venue":"WCRE","id":"ce4e87767eb48392123bb1c785a5cd6acf06c732","venue_1":"WCRE","year":"2003","title":"Ontological Excavation: Unearthing the core concepts of the application","authors":"Idris Hsi, Colin Potts, Melody Moore Jackson","author_ids":"1781997, 7373711, 3454635","abstract":"Applications possess and implement a specific \"theory of the world\" or ontology. Recovering and modeling this ontology may help inform software developers seeking to extend or adapt an application's functionality for its next release. We have developed a method for the black-box reverse engineering or excavation of an application's ontology. The ontology is represented as a semantic network, and graph theoretic measures are used to identify core concepts. Core concepts contribute disproportionately to the structural integrity of the ontology. We present analyses of ontologies excavated from several interactive applications. From a set of several candidate metrics for identifying core concepts we find node betweenness centrality is a good measure of a concept's influence on ontological integrity and that the k-core algorithm may be useful for identifying cohesive subgroups of core features. We conclude by discussing how these analyses can be applied to support application evolution.","cites":"16","conferencePercentile":"53.33333333"},{"venue":"WCRE","id":"7e945c7db1739b216e17c04367882031ccac22ad","venue_1":"WCRE","year":"2002","title":"Applying Spectral Methods to Software Clustering","authors":"Ali Shokoufandeh, Spiros Mancoridis, Matthew Maycock","author_ids":"1740078, 2693288, 1996059","abstract":"The application of spectral methods to the software clustering problem has the advantage of producing results that are within a known factor of the optimal solution. Heuris-tic search methods, such as those supported by the Bunch clustering tool, only guarantee local optimality which may be far from the global optimum. In this paper, we apply the spectral methods to the software clustering problem and make comparisons to Bunch using the same clustering criterion. We conducted a case study, involving 13 software systems, to draw our comparisons. There is a dual benefit to making these comparisons. Specifically, we gain insight into (1) the quality of the spectral methods solutions; and (2) the proximity of the results produced by Bunch to the optimal solution.","cites":"8","conferencePercentile":"22.72727273"},{"venue":"WCRE","id":"7762aeefd84d37bbe3730f29b66104b86d13336f","venue_1":"WCRE","year":"2006","title":"On Computing the Canonical Features of Software Systems","authors":"Jay Kothari, Trip Denton, Spiros Mancoridis, Ali Shokoufandeh","author_ids":"2992141, 3299739, 2693288, 1740078","abstract":"Software applications typically have many features that vary in their similarity. We define a measurement of similarity between pairs of features based on their underlying implementations and use this measurement to compute a set of canonical features. The Canonical Features Set (CFS) consists of a small number of features that are as dissimilar as possible to each other, yet are most representative of the features that are not in the CFS. The members of the CFS are distinguishing features and understanding their implementation provides the engineer with an overview of the system undergoing scrutiny. The members of the CFS can also be used as cluster centroids to partition the entire set of features. Partitioning the set of features can simplify the understanding of large and complex software systems. Additionally, when a specific feature must undergo maintenance, it is helpful to know which features are most closely related to it. We demonstrate the utility of our method through the analysis of the Jext, Firefox, and Gaim software systems.","cites":"25","conferencePercentile":"70.37037037"},{"venue":"WCRE","id":"36f6c967d3d9c567098c4d6a543894c9cc7b074a","venue_1":"WCRE","year":"2012","title":"Reverse Engineering iOS Mobile Applications","authors":"Mona Erfani Joorabchi, Ali Mesbah","author_ids":"2272413, 2155776","abstract":"I. ABSTRACT As a result of the ubiquity and popularity of smartphones, the number of third party mobile applications is explosively growing. With the increasing demands of users for new dependable applications, novel software engineering techniques and tools geared towards the mobile platform are required to support developers in their program comprehension and analysis tasks. In this paper, we propose a reverse engineering technique that automatically (1) hooks into, dynamically runs, and analyzes a given iOS mobile application, (2) exercises its user interface to cover the interaction state space and extracts information about the runtime behaviour, and (3) generates a state model of the given application, capturing the user interface states and transitions between them. Our technique is implemented in a tool called ICRAWLER. To evaluate our technique, we have conducted a case study using six open-source iPhone applications. The results indicate that ICRAWLER is capable of automatically detecting the unique states and generating a correct model of a given mobile application. II. INTRODUCTION According to recent estimations [1], by 2015 over 70 percent of all handset shipments will be smartphones, capable of running mobile applications. 1 Currently, there are over 600,000 mobile applications on Apple's AppStore [2] and more than 400,000 on Android Market [3]. Some of the challenges involved in mobile application development include handling different devices, multiple operating systems (Android, Apple iOS, Windows Mobile), and different programming languages (Java, Objective-C, Visual C++). Moreover, mobile applications are developed mostly in small-scale, fast-paced projects to meet the competitive market's demand [4]. Given the plethora of different mobile applications to choose from, users show low tolerance for buggy unstable applications, which puts an indirect pressure on developers to comprehend and analyze the quality of their applications before deployment. 1 There are two kinds of mobile applications: Native applications and Web-based applications. Throughout this paper, 'mobile application' refers to native mobile applications. With the ever increasing demands of smartphone users for new applications, novel software engineering techniques and tools geared towards the mobile platform are required [5], [6], [7] to support mobile developers in their program comprehension, analysis and testing tasks [8], [9]. According to a recent study [10], many developers interact with the graphical user interface (GUI) to comprehend the software by creating a mental model of the application. For traditional desktop applications, an average of 48% of the application's code is devoted to GUI [11]. Because of their highly …","cites":"11","conferencePercentile":"79.03225806"},{"venue":"WCRE","id":"1dae8cd83ddfcaf70a6c87b95974d72a02a93bce","venue_1":"WCRE","year":"1996","title":"On Designing an Experiment to Evaluate a Reverse Engineering Tool","authors":"Margaret-Anne D. Storey, Kenny Wong, P. Fong, D. Hooper, K. Hopkins, Hausi A. Müller","author_ids":"1734938, 1746487, 7873329, 2947973, 7897074, 1747880","abstract":"The Rigi reverse engineering system is designed to analyze and summarize the structure of large software systems. Two contrasting approaches are available for visualizing software structures in the Rigi graph editor. The first approach displays the structures through multiple, individual windows. The second approach, Simple Hierarchical Multi-Perspective (SHriMP) views, employs fisheye views of nested graphs. This paper describes the design of an experiment to evaluate these alternative user interfaces. Various results from a preliminary pilot study to test the experiment design are reported.","cites":"48","conferencePercentile":"87.5"},{"venue":"WCRE","id":"66666ec8d57328105b6e929e97063e79de560be6","venue_1":"WCRE","year":"2003","title":"Unscheduling, Unpredication, Unspeculation: Reverse Engineering Itanium Executables","authors":"Noah Snavely, Saumya K. Debray, Gregory R. Andrews","author_ids":"1830653, 1695853, 2542622","abstract":"EPIC (Explicitly Parallel Instruction Computing) archi-tectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations to take advantage of such architectural features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables.","cites":"1","conferencePercentile":"8.333333333"},{"venue":"WCRE","id":"5600aff895ee977da91571907c5b9184cd2e828c","venue_1":"WCRE","year":"2003","title":"Revealing Class Structure with Concept Lattices","authors":"Uri Dekel, Joseph Gil","author_ids":"2363404, 1767363","abstract":"This paper promotes the use of a mathematical concept lattice based upon the binary relation of accesses between methods and fields as a novel visualization of individual JAVA classes. We demonstrate in a detailed real-life case study that such a lattice is valuable for reverse-engineering purposes, in that it helps reason about the interface and structure of the class and find errors in the absence of source code. Our technique can also serve as a heuristic for automatic feature categorization, enabling it to assist efforts of re-documentation.","cites":"16","conferencePercentile":"53.33333333"},{"venue":"WCRE","id":"f0a1e8ace19ad662bc764654468383cc64e924d5","venue_1":"WCRE","year":"2004","title":"Updating Legacy Databases through Wrappers: Data Consistency Management","authors":"Philippe Thiran, Geert-Jan Houben, Jean-Luc Hainaut, Djamal Benslimane","author_ids":"2592801, 1703821, 2537776, 1689640","abstract":"Wrapping databases allows them to be reused in earlier unforeseen contexts, such as Web-based applications or fed-erated systems. Data wrappers, and more specifically updating wrappers (that not only access legacy data but also update them), can provide external clients of an existing (legacy) database with a neutral interface and augmented capabilities. For instance, a collection of COBOL files can be wrapped in order to allow external application programs to access them through a relational, object-oriented or XML interface, while providing referential integrity control. In this paper, we explore the principles of a wrapper architecture that addresses the problems of legacy data consistency management. The transformational paradigm is used as a rigorous formalism to define schema mappings as well as to generate as much as possible of the code of the wrappers. The generation is supported by an operational CASE tool.","cites":"6","conferencePercentile":"26.92307692"},{"venue":"WCRE","id":"640f2dd9624624feed9ff000681db74567df4e5e","venue_1":"WCRE","year":"2006","title":"An Empirical Study of Executable Concept Slice Size","authors":"David Binkley, Nicolas E. Gold, Mark Harman, Zheng Li, Kiarash Mahdavi","author_ids":"7697204, 2705225, 1707775, 1728030, 2700294","abstract":"An Executable Concept Slice extracts from a program an executable subprogram that captures the semantics of a specified high-level concept from the program. Executable concept slicing combines the executability of program slicing , with the expressive domain level criteria of concept assignment. This paper presents results from an investigation of executable concept slice size to assess the effectiveness of executable concept slicing. The results show that the coherence of concept-based slicing criteria allows them to produce smaller executable concept slices than arbitrary criteria , providing evidence for the applicability of Executable Concept Slicing.","cites":"3","conferencePercentile":"9.259259259"},{"venue":"WCRE","id":"90e2cf98c008d1280b4541b08a062b08a31ea918","venue_1":"WCRE","year":"2002","title":"Semantic Grep: Regular Expressions + Relational Abstraction","authors":"R. Ian Bull, Andrew Trevors, Andrew J. Malton, Michael W. Godfrey","author_ids":"8663237, 2705780, 2809730, 8353017","abstract":"Searching source code is one of the most common activities of software engineers. Text editors and other support tools normally provide searching based on lexical expressions (regular expressions). Some more advanced editors provide a way to add semantic direction to some of the searches. Recent research has focused on advancing the semantic options available to text-based queries. Most of these results make use of heavy weight relational database management technology. In this paper we explore the extension of lexical pattern matching by means of light weight relational queries, implemented using a tool called grok. A \" semantic grep \" (sgrep) command was implemented, which translates queries in a mixed algebraic and lexical language into a combination of grok queries and grep commands. This paper presents the design decisions behind sgrep, and example queries that can be posed. The paper concludes with a case study in which sgrep was used to identify architectural anomalies in PostgreSQL, an open source Database Management System.","cites":"17","conferencePercentile":"68.18181818"},{"venue":"WCRE","id":"6696793981453c495768fa109993beeb2aaadd5d","venue_1":"WCRE","year":"2000","title":"Next Generation Data Interchange: Tool-to-Tool Application Program Interfaces","authors":"Susan Elliott Sim","author_ids":"1705823","abstract":"Data interchange in the form of a standard exchange format(SEF) is only a first step towards tool interoperability. Inter-tool communication using files is slow and cumbersome; a better approach would be an application program interface, or API, that allowed tools to communicate with each other directly. This paper argues such an AP is a logical next step that builds on the current drive towards an SEF. It presents high-level descriptions of three approaches to tool-to-tool APIs and illustrates how requirements for the SEF also apply to the API.","cites":"16","conferencePercentile":"62"},{"venue":"WCRE","id":"451abdffdc2595bd5a1d37f6a6cb2c73eeb26bf1","venue_1":"WCRE","year":"2001","title":"Towards a Standard Schema for C/C++","authors":"Rudolf Ferenc, Susan Elliott Sim, Richard C. Holt, Rainer Koschke, Tibor Gyimóthy","author_ids":"3172077, 1705823, 1747273, 1704279, 1681248","abstract":"Developing a standard schema at the abstract syntax tree level for C/C++ to be used by reverse engineering and reengineering tools is a complex and difficult problem. In this paper, we present a catalogue of issues that need to be considered in order to design a solution. Three categories of issues are discussed. Lexical structure is the first category and pertains to characteristics of the source code, such as spaces and comments. The second category, syntax , includes both the mundane and hard problems in the C++ programming language. The final category is semantics and covers aspects such as naming and reference resolution. Example solutions to these challenges are provided from the Datrix schema from Bell Canada and the Colum-bus schema from University of Szeged. The paper concludes with a discussion of lessons learnt and plans for future work on a C/C++ AST standard schema.","cites":"34","conferencePercentile":"76.19047619"},{"venue":"WCRE","id":"0404308fa386a683f1eab0fe1ad48a1b28842ee3","venue_1":"WCRE","year":"2005","title":"RETR: Reverse Engineering to Requirements","authors":"Yijun Yu, John Mylopoulos, Yiqiao Wang, Sotirios Liaskos, Alexei Lapouchnian, Ying Zou, Martin Littou, Julio Cesar Sampaio do Prado Leite","author_ids":"1718749, 1750566, 1836976, 2268434, 3265656, 4947174, 2014377, 1720039","abstract":"Reverse engineering aims at extracting m a n y kinds of information from existing software and using this information for system renovation and program understanding. The goal of this full day WCRE'OS workshop is to identify methods and techniques for Reverse E n-gineering from software t o Requirements (R E T R) .","cites":"3","conferencePercentile":"31.25"},{"venue":"WCRE","id":"11d457b8854ccc270ef9fe1eb066bc732878e864","venue_1":"WCRE","year":"2004","title":"An Initial Approach to Assessing Program Comprehensibility Using Spatial Complexity, Number of Concepts and Typographical Style","authors":"Andrew Mohan, Nicolas E. Gold, Paul J. Layzell","author_ids":"2246011, 2705225, 1683086","abstract":"Software evolution can result in making a program harder to maintain, as it becomes more difficult to comprehend. This difficulty is related to the way the source code is formatted, the complexity of the code, and the amount of information contained within it. This paper presents an initial approach that uses measures of typographical style, spatial complexity and concept assignment to measure these factors, and to model the comprehensibility of an evolving program. The ultimate aim of which is to identify when a program becomes more difficult to comprehend, triggering a corrective action to be taken to prevent this. We present initial findings from applying this approach. These findings show that this approach, through measuring these three factors, can model the change in comprehensibility of an evolving program. Our findings support the well-known claim that programs become more complex as they evolve, explaining this increase in complexity in terms of layout changes, conceptual coherence, spatial relationships between source code elements, and the relationship between these factors. This in turn can then be used to understand how maintenance affects program comprehensibility and to ultimately reduce its burden on software maintenance.","cites":"3","conferencePercentile":"13.46153846"},{"venue":"WCRE","id":"b8152369182dd0deaa4a6b67935a68a94896d81e","venue_1":"WCRE","year":"2001","title":"Requirements-Driven Software Re-engineering Framework","authors":"Ladan Tahvildari, Kostas Kontogiannis, John Mylopoulos","author_ids":"1728879, 1722056, 1750566","abstract":"Software re-engineering projects such as migrating code from one platform to another, or restructuring a monolithic system into a modular architecture are popular maintenance tasks. Usually, projects of this type have to conform to hard and soft quality constraints (or non-functional requirements) such as \" the migrant system must run as fast as the original \" , or \" the new system should be more main-tainable than the original \". This paper proposes a framework that allows for specific design and quality requirements (performance and maintainability) of the target migrant system to be considered during the re-engineering process. Quality requirements for the migrant system can be encoded using soft-goal interdependency graphs and be associated with specific software transformations that need to be carried out for achieving the target quality requirement. These transformations can be applied as a series of the iterative and incremental steps that pertain both to the design (architecture) and source code (implementation) levels. An evaluation procedure can be used at each transformation step to determine whether specific goals have been achieved.","cites":"14","conferencePercentile":"35.71428571"},{"venue":"WCRE","id":"e52fa6f4fef185ab5fc5f4539b219c556d2d1973","venue_1":"WCRE","year":"2005","title":"Enhancing Security Using Legality Assertions","authors":"Lei Wang, James R. Cordy, Thomas R. Dean","author_ids":"1743559, 1683822, 1716997","abstract":"Buffer overflows have been the most common form of security vulnerability in the past decade. A number of techniques have been proposed to address such attacks. Some are limited to protecting the return address on the stack; others are more general, but have undesirable properties such as large overhead and false warnings. The approach described in this paper uses legality assertions , source code assertions inserted before each subscript and pointer dereference that explicitly check that the referencing expression actually specifies a location within the array or object pointed at run time. A transformation system is developed to analyze a program and annotate it with appropriate assertions automatically. This approach detects buffer vulnerabilities in both stack and heap memory as well as potential buffer overflows in library functions. Runtime checking through using automatically inferred assertions considerably enhances the accuracy and efficiency of buffer overflow detection. A number of example buffer overflow exploiting C programs are used to demonstrate the effectiveness of this approach.","cites":"8","conferencePercentile":"50"},{"venue":"WCRE","id":"7f3cd635d40cb10be2890b6efd870dda460e097b","venue_1":"WCRE","year":"1999","title":"Architectural Synthesis: Integrating Multiple Architectural Perspectives","authors":"Robert Waters, Gregory D. Abowd","author_ids":"1986359, 1732524","abstract":"There are many tools and techniques available to understand and analyze a system at the architectural level. Each of these may provide its own perspective of the key architectural elements of the system. We introduce the process of architectural synthesis to deal with the problem of integrating these different architectural perspectives. We emphasize the utility of the synthesis process in supporting the evolution of legacy software systems. The four steps of the synthesis process form a cycle and include acquiring different perspectives (generation), grouping common perspectives (classification), combining all perspectives that apply to the same view (union), and finally composing views to determine consistency (fusion). We give an example of the application of the synthesis process applied to the architecture of a software visualization tool. Our preliminary investigation shows that synthesized architectural perspectives provide a more complete representation of a real system. Use of the synthesis process uncovers inconsistencies pointing to areas needing further analysis or explanation. We conclude with directions for future research in this area and a description of the REMORA synthesis environment we are building to support architectural synthesis. 1 INTRODUCTION A newly appointed technical lead on the extension of a large company's software product is pondering where to begin the development effort. The requirements have been gathered and analyzed, and now the decision is made that doing an architectural analysis would be the best place to start the design phase. In the manual on architectural analysis, the first step is simply stated as \" Generate an architecture. \" Lacking any other guidance, the technical lead peruses the existing design documentation and finds the original architecture which she brings to the first meeting. As soon as the viewgraph is placed on the screen, several developers begin talking at once, pointing out the changes over the years and arguing over what the architecture really looks like. The remainder of the meeting is wasted with bickering over","cites":"5","conferencePercentile":"19.04761905"},{"venue":"WCRE","id":"22ac4a5f8a05b0bce339631a9722b97009e70817","venue_1":"WCRE","year":"1997","title":"How Do Program Understanding Tools Affect How Programmers Understand Programs?","authors":"Margaret-Anne D. Storey, Kenny Wong, Hausi A. Müller","author_ids":"1734938, 1746487, 1747880","abstract":"In this paper, we explore the question of whether program understanding tools enhance or change the way that programmers understand programs. The strategies that programmers use to comprehend programs vary widely. Program understanding tools should enhance or ease the pro-grammer's preferred strategies, rather than impose a fixed strategy that may not always be suitable. We present observations from a user study that compares three tools for browsing program source code and exploring software structures. In this study, 30 participants used these tools to solve several high-level program understanding tasks. These tasks required a broad range of comprehension strategies. We describe how these tools supported or hindered the diverse comprehension strategies used.","cites":"124","conferencePercentile":"88.88888889"},{"venue":"WCRE","id":"08411a3d4f98fb394139684bf4adb578f0dd5df4","venue_1":"WCRE","year":"2000","title":"A Structured Demonstration of Program Comprehension Tools","authors":"Susan Elliott Sim, Margaret-Anne D. Storey","author_ids":"1705823, 1734938","abstract":"This paper describes a structured tool demonstration, a hybrid evaluation technique that combines elements from experiments, case studies, and technology demonstrations. Developers of program understanding tools were invited to bring their tools to a common location to participate in a scenario with a common subject system. Working simultaneously, the tool teams were given reverse engineering tasks and maintenance tasks to complete on an unfamiliar subject system. Observers were assigned to each team to find out how useful the observed program comprehension tool would be in an industrial setting. The demonstration was followed by a workshop panel where the development teams and the observers presented their results and findings from this experience.","cites":"27","conferencePercentile":"80"},{"venue":"WCRE","id":"9552a04eec3c89503d4fc41728568300b1338dc4","venue_1":"WCRE","year":"2008","title":"Navigating Through the Design of Object-Oriented Programs","authors":"Epameinondas Gasparis, Jon Nicholson, Amnon H. Eden, Rick Kazman","author_ids":"2395697, 1720605, 2445597, 1699020","abstract":"The Design Navigator is a tool for reverse-engineering object-oriented programs into formal charts of any level of abstraction. We show how the Design Navigator discovers abstract building-blocks in the design of programs and how it visualises them in terms of LePUS3, a formal Design Description Language. We demonstrate why reverse engineering programs into a formal modelling and specification language is not only possible in principle but also of practical benefit.","cites":"2","conferencePercentile":"34.48275862"},{"venue":"WCRE","id":"01e90e360114da419a98591c2b58ec54154d6a0b","venue_1":"WCRE","year":"2010","title":"Reverse Engineering Self-Modifying Code: Unpacker Extraction","authors":"Saumya K. Debray, Jay Patel","author_ids":"1695853, 1797164","abstract":"—An important application of binary-level reverse engineering is in reconstructing the internal logic of computer malware. Most malware code is distributed in encrypted (or \" packed \") form; at runtime, an unpacker routine transforms this to the original executable form of the code, which is then executed. Most of the existing work on analysis of such programs focuses on detecting unpacking and extracting the unpacked code. However, this does not shed any light on the functionality of different portions of the code so obtained, and in particular does not distinguish between code that performs unpacking and code that does not; identifying such functionality can be helpful for reverse engineering the code. This paper describes a technique for identifying and extracting the unpacker code in a self-modifying program. Our algorithm uses offline analysis of a dynamic instruction trace both to identify the point(s) where unpacking occurs and to identify and extract the corresponding unpacker code.","cites":"7","conferencePercentile":"58.92857143"},{"venue":"WCRE","id":"3a71cf8aef8ad4e3c4bf4f8a14c0f1e7237f4ed5","venue_1":"WCRE","year":"1999","title":"Software Architectural Transformation","authors":"S. Jeromy Carrière, Steven G. Woods, Rick Kazman","author_ids":"2754273, 1701616, 1699020","abstract":"Software architecture, as a vehicle for communication and reasoning about software systems and their quality, is becoming an area of focus in both the forward-and reverse-engineering communities. In the past, we have attempted to unify these areas via a semantic model of reengineering called CORUM II. In this paper we present a concrete example of an architecturally-motivated reengineering task. In executing this task, we perform architecture reconstruction , reason about the reconstructed architecture, motivate an architectural transformation with new architectural quality requirements, and realize this architectural transformation via an automated code transformation.","cites":"22","conferencePercentile":"54.76190476"},{"venue":"WCRE","id":"c05f4265f7fcc7b523e1bce21a78d124052658d1","venue_1":"WCRE","year":"1998","title":"Requirements for Integrating Software Architecture and Reengineering Models: CORUM II","authors":"Rick Kazman, Steven S. Woods, S. Jeromy Carrière","author_ids":"1699020, 3000806, 2754273","abstract":"This paper motivates, discusses the requirements for, and presents a generic framework for, the integration of architectural and code-based reengineer-ing tools. This framework is needed because there are a wide variety of standalone reengineering tools that operate at diierent levels of abstraction ranging from \\code-level\" to software architecture. For the purposes of reengineering a complete system however, these tools need to be able to share information so that not only can the code be updated or corrected, but also so the system's software architecture can be simultaneously rationalized or modernized. To this end, we have built upon the CORUM model of reengineer-ing tool interoperation to include software architecture concepts and tools. This extended framework|called CORUM II|is organized around the metaphor of a \\horseshoe\", where the left-hand side of the horseshoe consists of fact extraction from an existing system, the right hand side consists of development activities, and the bridge between the sides consists of a set of transformations from the old to the new.","cites":"63","conferencePercentile":"88.88888889"},{"venue":"WCRE","id":"72bc0db7973df44737cf235b1f41f3e779d29dda","venue_1":"WCRE","year":"2011","title":"Got Issues? Do New Features and Code Improvements Affect Defects?","authors":"Daryl Posnett, Abram Hindle, Premkumar T. Devanbu","author_ids":"2220200, 1692741, 1730296","abstract":"—There is a perception that when new features are added to a system that those added and modified parts of the source-code are more fault prone. Many have argued that new code and new features are defect prone due to immaturity, lack of testing, as well unstable requirements. Unfortunately most previous work does not investigate the link between a concrete requirement or new feature and the defects it causes, in particular the feature, the changed code and the subsequent defects are rarely investigated. In this paper we investigate the relationship between improvements, new features and defects recorded within an issue tracker. A manual case study is performed to validate the accuracy of these issue types. We combine defect issues and new feature issues with the code from version-control systems that introduces these features; we then explore the relationship of new features with the fault-proneness of their implementations. We describe properties and produce models of the relationship between new features and fault proneness, based on the analysis of issue trackers and version-control systems. We find, surprisingly, that neither improvements nor new features have any significant effect on later defect counts, when controlling for size and total number of changes.","cites":"5","conferencePercentile":"44.28571429"},{"venue":"WCRE","id":"eb967b26285a28ca58dc1be97e8d4fd49bf460b7","venue_1":"WCRE","year":"2004","title":"Towards an Effective Approach for Reverse Engineering","authors":"Vinicius Cardoso Garcia, Daniel Lucrédio, Antônio Francisco do Prado, Alexandre Alvaro, Eduardo Santana de Almeida","author_ids":"1755263, 1924183, 3077306, 2258686, 2219256","abstract":"Currently, the demand for the reverse engineering has been growing significantly. The need of different business sectors to adapt their systems to Web or to use other technologies is stimulating the research for methods, tools and infrastructures that support the evolution of existing applications. In this paper, we present the main research trends on reverse engineering, and discuss how should be an efficient reverse engineering approach, aiming at higher reuse levels.","cites":"4","conferencePercentile":"19.23076923"},{"venue":"WCRE","id":"0da2189b06ba955c96b2615e8864a4390f871310","venue_1":"WCRE","year":"2000","title":"A Structured Demonstration of Five Program Comprehension Tools: Lessons Learnt","authors":"Susan Elliott Sim, Margaret-Anne D. Storey, Andreas Winter","author_ids":"1705823, 1734938, 1799214","abstract":"The purpose of this panel is to report on a structured demonstration for comparing program comprehension tools. Five teams of program comprehension tool designers applied their tools to a set of maintenance tasks on a common subject system. By applying a variety of reverse engineering techniques to a predefined set of tasks, the tools can be compared using a common playing field. A secondary topic of discussion will address the development of \" guinea pig \" systems and how to use them in a structured demonstration for evaluating software tools. 1 Background Many tools to support program comprehension have been developed in both industry and research during the past few years. Although these tools share the common goal of simplifying the task of understanding large software systems, they have different functionality and different approaches. Various tools extract system arti-facts and their relationships at different levels of granu-larity, ranging from fine-grained data at the abstract syntax tree level to coarse-grained data at the architectural level. Other tools support techniques for abstracting these artifacts e.g. using metrics-based discovery of software architecture, query-based investigations of software structure, or browsing capability to support a struc-tured exploration of the system. Furthermore, tools can be distinguished by how they present the results of analyses. Some tools show different kinds of graphs, reporting the results in terms of source code, or presenting these data in textual representations such as lists or tables. Despite the commonly held assumption that effective tools could be of huge economic gain, there have been relatively few tool evaluations. The evaluations that have been done have limited results that cannot easily be transferred to other tools or studies. There is no single way to demonstrate the facilities of the various program comprehension tools and compare tool capabilities in a realistic reengineering context in an equitable manner. Over the past few years, a consensus has been developing within the reverse engineering community that more evaluation of tools and more methods to evaluate are needed. One approach that has sparked a great deal of interest is the benchmark technique where tools are deployed on a common subject system, or \" guinea pig. \" In this vein, Chikofsky organized a demonstration project where participants analyzed the WELTAB III Election Tabulation System (presented at CSMR'98 in Florence). A derivative of the benchmark technique, a structured demonstration was held at CASCON99. CASCON is an IBM sponsored Canadian software …","cites":"7","conferencePercentile":"24"},{"venue":"WCRE","id":"956dd36ef07ea2c3f8b02ff3c0ae0603f0034910","venue_1":"WCRE","year":"2003","title":"Orion-RE: A Component-Based Software Reengineering Environment","authors":"Alexandre Alvaro, Daniel Lucrédio, Vinicius Cardoso Garcia, Antônio Francisco do Prado, Luís Carlos Trevelin, Eduardo Santana de Almeida","author_ids":"2258686, 1924183, 1755263, 3077306, 3077420, 2219256","abstract":"Software reuse is the process of implementing or updating software systems using existing software assets, resulting in a software quality increase, productivity and reducing time to market. One way to achieve reuse is through software reengineering. This papers presents Orion-RE, a Component-Based Software Reengineering Environment that uses software reengineering and component-based development techniques to rebuilt legacy systems, reusing their available documentation and knowledge embedded in the code. Orion-RE integrates several tools: a software transformation system, modeling/development tools, a software artifacts repository, and a middleware platform. A software process model drives the environment usage through the reverse engineering, to recover the system design, and forward engineering, where the system is rebuilt using modern technologies, such as design patterns, frameworks, CBD principles and middleware.","cites":"6","conferencePercentile":"35"},{"venue":"WCRE","id":"63cf43da814643c78fa1562ca1a29e08610ddcfb","venue_1":"WCRE","year":"2013","title":"Accurate developer recommendation for bug resolution","authors":"Xin Xia, David Lo, Xinyu Wang, Bo Zhou","author_ids":"1704025, 6005908, 2142614, 3738572","abstract":"—Bug resolution refers to the activity that developers perform to diagnose, fix, test, and document bugs during software development and maintenance. It is a collaborative activity among developers who contribute their knowledge, ideas, and expertise to resolve bugs. Given a bug report, we would like to recommend the set of bug resolvers that could potentially contribute their knowledge to fix it. We refer to this problem as developer recommendation for bug resolution. In this paper, we propose a new and accurate method named DevRec for the developer recommendation problem. DevRec is a composite method which performs two kinds of analysis: bug reports based analysis (BR-Based analysis), and developer based analysis (D-Based analysis). In the BR-Based analysis, we characterize a new bug report based on past bug reports that are similar to it. Appropriate developers of the new bug report are found by investigating the developers of similar bug reports appearing in the past. In the D-Based analysis, we compute the affinity of each developer to a bug report based on the characteristics of bug reports that have been fixed by the developer before. This affinity is then used to find a set of developers that are \" close \" to a new bug report. We evaluate our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 107,875 bug reports. We show that DevRec could achieve recall@5 and recall@10 scores of 0.4826-0.7989, and 0.6063-0.8924, respectively. We also compare DevRec with other state-of-art methods, such as Bugzie and DREX. The results show that DevRec on average improves recall@5 and recall@10 scores of Bugzie by 57.55% and 39.39% respectively. DevRec also outperforms DREX by improving the average recall@5 and recall@10 scores by 165.38% and 89.36%, respectively.","cites":"24","conferencePercentile":"100"},{"venue":"WCRE","id":"3e503cbf7fac96b414560f5a52964545dcd4eea4","venue_1":"WCRE","year":"1998","title":"Reengineering of Legacy Systems Based on Transformation Using the Object-Oriented Paradigm","authors":"Rosângela Dellosso Penteado, Paulo César Masiero, Antônio Francisco do Prado","author_ids":"3215918, 2982809, 3077306","abstract":"Legacy systems that were originally developed using the procedure-oriented approach can be reengineered according to the object-oriented paradigm by the process here proposed. Three phases are included in such process: an object oriented reverse engineering phase, in which an object oriented analysis model of the legacy system is produced by Fusion/RE; a segmentation phase, in which the original code is segmented into object oriented methods, keeping the procedural language; and finally a transformation phase, in which the segmented code is transformed to an object oriented language by the Draco-Puc machine. The application of this process to a real mechanic and electric car repair shop system, with 20.000 lines of source code, is described. Samples of the Java code obtained by transforming the original Clipper code are supplied.","cites":"3","conferencePercentile":"22.22222222"},{"venue":"WCRE","id":"39e80b5c6a1a86a4acf37427ceb482d67dc71ddf","venue_1":"WCRE","year":"2013","title":"Detecting dependencies in Enterprise JavaBeans with SQuAVisiT","authors":"Alexandru Sutii, Serguei A. Roubtsov, Alexander Serebrenik","author_ids":"2334148, 1722802, 1747171","abstract":"—We present recent extensions to SQuAVisiT, Software Quality Assessment and Visualization Toolset. While SQuA-VisiT has been designed with traditional software and traditional caller-callee dependencies in mind, recent popularity of Enterprise JavaBeans (EJB) required extensions that enable analysis of additional forms of dependencies: EJB dependency injections, object-relational (persistence) mappings and Web service map-pings. In this paper we discuss the implementation of these extensions in SQuAVisiT and the application of SQuAVisiT to an open-source software system.","cites":"1","conferencePercentile":"22.22222222"},{"venue":"WCRE","id":"9bfde2fe856bbdbf30db2999c8de5b4855288842","venue_1":"WCRE","year":"2005","title":"Equipping the Reflexion Method with Automated Clustering","authors":"Andreas Christl, Rainer Koschke, Margaret-Anne D. Storey","author_ids":"2947791, 1704279, 1734938","abstract":"A significant aspect in applying the Reflexion Method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. To date, this mapping is established manually, which requires a lot of work for large software systems. In this paper, we present a new approach , in which clustering techniques are applied to support the user in the mapping activity. The result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user's hypothesized knowledge about the system's architecture. This paper describes also a case study in which our semi-automated mapping technique has been applied successfully to extend a partial map of a real-world software application.","cites":"28","conferencePercentile":"81.25"},{"venue":"WCRE","id":"a18edf9734f5d7a9741651899789c3c00064fb5b","venue_1":"WCRE","year":"2011","title":"Recommending People in Developers' Collaboration Network","authors":"Didi Surian, Nian Liu, David Lo, Hanghang Tong, Ee-Peng Lim, Christos Faloutsos","author_ids":"2609488, 4301398, 6005908, 8163721, 1709901, 1702392","abstract":"—Many software developments involve collaborations of developers across the globe. This is true for both open-source and closed-source development efforts. Developers collaborate on different projects of various types. As with any other teamwork endeavors, finding compatibility among members in a development team is helpful towards the realization of the team's goal. Compatible members tend to share similar programming style and naming strategy, communicate well with one another, etc. However, finding the right person to work with is not an easy task. In this work, we extract information available from Sourceforge.Net, the largest database of open source software, and build developer collaboration network comprising of information on developers, projects, and project properties. Based on an input developer, we then recommend a list of top developers that are most compatible based on their programming language skills, past projects and project categories they have worked on before, via a random walk with restart procedure. Our quantitative and qualitative experiments show that we are able to recommend reasonable developer candidates from snapshots of Sourceforge.Net consisting of tens of thousands of developers and projects, and hundreds of project properties.","cites":"20","conferencePercentile":"95.71428571"},{"venue":"WCRE","id":"ef0cfbf4e36533baac1888a194d5d873d80b55c5","venue_1":"WCRE","year":"2004","title":"A Reverse Engineering Approach to Support Software Maintenance: Version Control Knowledge Extraction","authors":"Xiaomin Wu, Adam Murray, Margaret-Anne D. Storey, Robert Lintern","author_ids":"3040126, 1846443, 1734938, 2927013","abstract":"Most traditional reverse engineering tools focus on abstraction and analysis of source code, presenting a visual representation of the software architecture. This approach can be both helpful and cost effective in software maintenance tasks. However, where large software teams are concerned, with moderate levels of employee turnover, traditional reverse engineering tools can be inadequate. To address this issue, we examine the use of software process data, such as software artifact change history and developer activities. We propose the application of this data confers additional information developers need to better understand, maintain and develop software in large team settings. To explore this hypothesis, we evaluate the use of a tool, Xia, in the navigation of both software artifacts and their version history. This paper introduces Xia, reveals the results of our evaluation and proposes directions for future research in this area.","cites":"28","conferencePercentile":"59.61538462"},{"venue":"WCRE","id":"58b57e151f622cfe09a5aabdd4bcccc7ac378f83","venue_1":"WCRE","year":"2000","title":"Exploring Legacy Systems using Types","authors":"Arie van Deursen, Leon Moonen","author_ids":"1737202, 1762006","abstract":"We show how hypertext-based program understanding tools can achieve new levels of abstraction by using inferred type information for cases where the subject software system is written in a weakly typed language. We propose TYPEEX-PLORER, a tool for browsing COBOL legacy systems based on these types. The paper addresses (1) how types, an invented abstraction, can be presented meaningfully to software re-engineers; (2) the implementation techniques used to construct TYPEEXPLORER; and (3) the use of TYPE-EXPLORER for understanding legacy systems, at the level of individual statements as well as at the level of the software architecture – which is illustrated by using TYPEEX-PLORER to browse an industrial COBOL system of 100,000 lines of code.","cites":"16","conferencePercentile":"62"},{"venue":"WCRE","id":"3c3ee87d6af5faaf36b145006ae50e926aeca430","venue_1":"WCRE","year":"2009","title":"Benchmarking Lightweight Techniques to Link E-Mails and Source Code","authors":"Alberto Bacchelli, Marco D'Ambros, Michele Lanza, Romain Robbes","author_ids":"1686038, 7200066, 1744456, 1853058","abstract":"—During the evolution of a software system, a large amount of information, which is not always directly related to the source code, is produced. Several researchers have provided evidence that the contents of mailing lists represent a valuable source of information: Through e-mails, developers discuss design decisions, ideas, known problems and bugs, etc. which are otherwise not to be found in the system. A technical challenge in this context is how to establish the missing link between free-form e-mails and the system artifacts they refer to. Although the range of approaches is vast, establishing their accuracy remains a problem, as there is no benchmark against which to compare their performance. To overcome this issue, we manually inspected a statistically significant number of e-mails pertaining to the ArgoUML system. Based on this benchmark, we present a variety of lightweight techniques to assign e-mails to software artifacts and measure their effectiveness in terms of precision and recall.","cites":"20","conferencePercentile":"86.95652174"}]}