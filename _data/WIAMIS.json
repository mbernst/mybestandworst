{"WIAMIS.csv":[{"venue":"WIAMIS","id":"bdf90b6d6e79b698ea4e2e8389ad20e01ca8a7da","venue_1":"WIAMIS","year":"2008","title":"Exploiting Spatial Context in Image Region Labelling Using Fuzzy Constraint Reasoning","authors":"Carsten Saathoff, Steffen Staab","author_ids":"2760019, 1752093","abstract":"We present an approach for integrating explicit knowledge about the spatial context of objects into image region labelling. Our approach is based on spatial prototypes that represent the typical arrangement of objects in images. We use Fuzzy Constraint Satisfaction Problems as the underlying formal model for producing a labelling that is consistent with the spatial constraints of prototypes.","cites":"8","conferencePercentile":"85.29411765"},{"venue":"WIAMIS","id":"c882a26e9329af4e45571493d79899e5a06a2ca2","venue_1":"WIAMIS","year":"2013","title":"A nested infinite Gaussian mixture model for identifying known and unknown audio events","authors":"Yoko Sasaki, Kazuyoshi Yoshii, Satoshi Kagami","author_ids":"1936377, 3169830, 1735977","abstract":"This paper presents a novel statistical method that can classify given audio events into known classes or recognize them as an unknown class. We propose a nested infinite Gaussian mixture model (iGMM) to represent varied audio events in real environment. One of the main problems of conventional classification methods is that we need to specify a fixed number of classes in advance. Therefore, all audio events are forced to be classified into known classes. To solve the problem, the proposed method formulates a infinite Gaussian mixture model (iGMM) in which the number of classes are allowed to increase without bound. Another problem is that the complexity of each audio event is different. Then, the nested iGMM using nonpara-metric Bayesian approach is applied to adjust the needed dimension of each audio model. Experimental results show the effectiveness for these two problems to represent the given audio events.","cites":"0","conferencePercentile":"20"},{"venue":"WIAMIS","id":"de7ae62faac9a54a20a95d3c1f08115cdc3ef141","venue_1":"WIAMIS","year":"2010","title":"Bag-of-words classification of miniature illustrations","authors":"Costantino Grana, Daniele Borghesani, Giovanni Gualdi, Rita Cucchiara","author_ids":"1705203, 3301949, 1766510, 1741922","abstract":"In this paper a system for illuminated manuscripts images analysis is presented. In particular the bag-of-keypoints strategy , commonly adopted for object recognition, image classification and scene recognition, is applied to the classification of automatically extracted miniatures. Pictures are characterized by SURF descriptors, and a classification procedure is performed, comparing the results of Na¨ıve Bayes and his-togram intersection distance measures.","cites":"0","conferencePercentile":"25"},{"venue":"WIAMIS","id":"b7ff238866c69de663375d289dd674f400ad9491","venue_1":"WIAMIS","year":"2009","title":"Modelling user attention for human-agent interaction","authors":"Christopher E. Peters, Stylianos Asteriadis, Genaro Rebolledo-Mendez","author_ids":"2225789, 1753719, 1731562","abstract":"In this work, we propose a design for a user attention model featuring three core components. Our system components can work in real-time, offering indications of user attention from different sensory inputs (both visual and neurophysiological). Intention of the current work is to keep the equipment as unintrusive as possible, while keeping the confidence of the inputs as high as possible. We discuss potential applications of such a system, particularly with respect to evaluating user attentive behaviour during human-agent interactions and as a more natural interface for interacting with agents.","cites":"6","conferencePercentile":"94.44444444"},{"venue":"WIAMIS","id":"26c312873b9b242d46385e78351ff703be00be41","venue_1":"WIAMIS","year":"2013","title":"Challenges of finding aesthetically pleasing images","authors":"João Faria, Stanislav Bagley, Stefan M. Rüger, Toby P. Breckon","author_ids":"3298920, 2894845, 2375038, 1803808","abstract":"We present an analysis of existing methods to automatic classification of photos according to aesthetics. We review different components of the classification process: existing evaluation datasets, their properties, most commonly-used image features, qualitative and quantitative, and classification results where comparable. We argue there are methodology gaps in the existing approaches to evaluating the classification results. We introduce the results of our experiments with Random Forest classification applied to image aesthetics classification and compare them to AdaBoost and SVM approaches.","cites":"0","conferencePercentile":"20"},{"venue":"WIAMIS","id":"841d21b168e686df5307d2fb3968e5da983e51c5","venue_1":"WIAMIS","year":"2009","title":"Contextual information in virtual collaboration systems beyond current standards","authors":"Anna Carreras, Maria T. Andrade, Tim Masterton, Hemantha Kodikara Arachchi, Vitor Barbosa, Safak Dogan, Jaime Delgado, Ahmet M. Kondoz","author_ids":"2301635, 1957223, 1891893, 1907301, 2900207, 2267648, 1788693, 1696305","abstract":"Context-aware applications are fast becoming popular as a means of enriching users' experiences in various multimedia content access and delivery scenarios. Nevertheless, the definition, identification, and representation of contextual information are still open issues that need to be addressed. In this paper, we briefly present our work developed within the VISNET II Network of Excellence (NoE) project on context-based content adaptation in Virtual Collaboration Systems (VCSs). Based on the conducted research, we conclude that MPEG-21 Digital Item Adaptation (DIA) is the most complete standardization initiative to represent context for content adaptation. However, tools defined in MPEG-21 DIA Usage Environment Descriptors (UEDs) are not adequate for Virtual Collaboration application scenarios, and thus, we propose potential extensions to the available UEDs.","cites":"1","conferencePercentile":"38.88888889"},{"venue":"WIAMIS","id":"42f02f802698334cd2bb0adba8e7146ac08f9a9d","venue_1":"WIAMIS","year":"2008","title":"Digital Rights Metadata Management and Retrieval on Structured Overlay Networks","authors":"Walter Allasia, Francesco Gallo, Marco Milanesio, Rossano Schifanella, Filippo Chiariglione, Angelo Difino","author_ids":"2141263, 3283312, 1864546, 2251027, 2754939, 2854777","abstract":"This paper introduces a suitable way for indexing mul-timedia metadata on a structured Peer-to-Peer overlay network , with special care to the management of rights meta-data expressed by MPEG-21. We have selected a suitable subset of MPEG-21 Rights Expression Language elements to be indexed, in order to map governed contents into a flat space and allow insertion and retrieval of digital contents. Furthermore, we present a distributed application built on a structured overlay network enabling the search of mul-timedia items using rights related information. Our solution is completely decentralized and can be exploited in any MPEG-21 compliant metadata representation.","cites":"1","conferencePercentile":"25"},{"venue":"WIAMIS","id":"1e3593834d29886c2bf8ac44aec7fafc99998847","venue_1":"WIAMIS","year":"2008","title":"Interest Based Selection of User Generated Content for Rich Multimedia Services","authors":"Olivier Van Laere, Matthias Strobbe, Samuel Dauwe, Bart Dhoedt, Filip De Turck, Piet Demeester, Orlando Verde, Frank Hülsken","author_ids":"2219806, 3200385, 2622621, 1733741, 1715957, 1742060, 2133032, 3010427","abstract":"In view of the overwhelming popularity of user generated content, both in terms of production and consumption, new intelligent services are needed to help users finding the content they need and enhance existing services with suitably selected content. In this paper we present a set of algorithms for retrieving content, based on dynamic user profiles and learning capabilities (e.g. based on user feedback). The profile information is used in content searches as well as for assisting the user input analysis process (i.e. speech recognition). To illustrate the approach taken, a rich communication service is presented. Here, the basic service (i.e. voice/video conferencing) is enhanced by showing pictures in real time to the users based on the topic of their conversation and their specific interests.","cites":"0","conferencePercentile":"8.823529412"},{"venue":"WIAMIS","id":"83f2e7a447b8378340185407e4bf9c6d0ef0ea43","venue_1":"WIAMIS","year":"2010","title":"On the use of audio events for improving video scene segmentation","authors":"Panagiotis Sidiropoulos, Vasileios Mezaris, Yiannis Kompatsiaris, Hugo Meinedo, Miguel Bugalho, Isabel Trancoso","author_ids":"1726550, 1737436, 1906503, 1748419, 3341697, 1691021","abstract":"This work deals with the problem of automatic temporal segmenta-tion of a video into elementary semantic units known as scenes. Its novelty lies in the use of high-level audio information in the form of audio events for the improvement of scene segmentation performance. More specifically, the proposed technique is built upon a recently proposed audiovisual scene segmentation approach that involves the construction of multiple scene transition graphs (STGs) that separately exploit information coming from different modalities. In the extension of the latter approach presented in this work, audio event detection results are introduced to the definition of an audio-based scene transition graph, while a visual-based scene transition graph is also defined independently. The results of these two types of STGs are subsequently combined. The application of the proposed technique to broadcast videos demonstrates the usefulness of audio events for scene segmentation.","cites":"13","conferencePercentile":"94.44444444"},{"venue":"WIAMIS","id":"23e51a0652fc08a66c4c07b2e0926fb89adda613","venue_1":"WIAMIS","year":"2013","title":"Group detection in still images by F-formation modeling: A comparative study","authors":"Francesco Setti, Hayley Hung, Marco Cristani","author_ids":"2793423, 1756464, 1723008","abstract":"Automatically detecting groups of conversing people has become a hot challenge, although a formal, widely-accepted definition of them is lacking. This gap can be filled by considering the social psychological notion of an F-formation as a loose geometric arrangement. In the literature, two main approaches followed this line, exploiting Hough voting [1] from one side and Graph Theory [2] on the other. This paper offers a thorough comparison of these two methods, highlighting the strengths and weaknesses of both in different real life scenarios. Our experiments demonstrate a deeper understanding of the problem by identifying the circumstances in which to adopt a particular method. Finally our study outlines what aspects of the problem are important to address for future improvements to this task.","cites":"9","conferencePercentile":"96"},{"venue":"WIAMIS","id":"1dbb36ed69f433b924e6137d7aa583d462eecb21","venue_1":"WIAMIS","year":"2010","title":"3D object duplicate detection for video retrieval","authors":"Peter Vajda, Ivan Ivanov, Lutz Goldmann, Jong-Seok Lee, Touradj Ebrahimi","author_ids":"3312919, 5795499, 2379709, 5722542, 1681498","abstract":"Content-based video retrieval has become a very active research area in the last decade due to the increasing number of video shared on social networks such as YouTube and Daily-Motion. While most of the content-based video retrieval approaches employ visual low-level features for a global analysis of the video, this paper proposes an object-based retrieval method as an alternative. The goal of the proposed method is to retrieve those key frames and shots of a video that contain a particular object, which is a challenging task due to different viewpoints, illuminations and partial occlusions. In order to increase the reliability for 3D objects, our approach combines viewpoint-invariant region descriptors to describe the appearance of an object with a graph model to describe the spatial layout of the individual regions. Given a query object, provided by the user in form of an image and a region of interest, the system retrieves shots containing this object by analyzing a set of key frames for each shot. The robustness of our approach is demonstrated using a video in which one 3D object is recorded in from different view points and with partial occlusions.","cites":"0","conferencePercentile":"25"},{"venue":"WIAMIS","id":"7152a4b32ab82e1b5f91fd3d221c45d68a225e83","venue_1":"WIAMIS","year":"2012","title":"Why did you record this video? An exploratory study on user intentions for video production","authors":"Mathias Lux, Jochen Huber","author_ids":"6517033, 1755184","abstract":"Why do people record videos and share them? While the question seems to be simple, user intentions have not yet been investigated for video production and sharing. A general tax-onomy would lead to adapted information systems and mul-timedia interfaces tailored to the users' intentions. We contribute (1) an exploratory user study with 20 participants, examining the various facets of user intentions for video production and sharing in detail and (2) a novel set of user intention clusters for video production, grounded empirically in our study results. We further reflect existing work in specialized domains (i.e. video blogging and mobile phone cameras) and show that prevailing models used in other multime-dia fields (e.g. photography) cannot be used as-is to reason about video recording and sharing intentions.","cites":"7","conferencePercentile":"96.875"},{"venue":"WIAMIS","id":"49d447f33801410a4881a4a63d5ff20b190e3a95","venue_1":"WIAMIS","year":"2013","title":"Getting RID of pain-related behaviour to improve social and self perception: A technology-based perspective","authors":"M. S. Hane Aung, Bernardino Romera-Paredes, Aneesha Singh, S. Lim, Natalie Kanakam, Amanda C. de C. Williams, Nadia Bianchi-Berthouze","author_ids":"1702589, 1713421, 1858550, 5790591, 3093693, 2094118, 1690885","abstract":"People with chronic musculoskeletal pain can experience pain-related fear of physical activity and low confidence in their own motor capabilities. These pain-related emotions and thoughts are often communicated through communicative and protective non-verbal behaviours. Studies in clinical psychology have shown that protective behaviours affect well-being not only physically and psychologically, but also socially. These behaviours appear to be used by others to appraise not just a person's physical state but also to make inferences about their personality traits, with protective pain-related behaviour more negatively evaluated than the communicative behaviour. Unfortunately, people with chronic pain may have difficulty in controlling the triggers of protective behaviour and often are not even aware they exhibit such behaviour. New sensing technology capable of detecting such behaviour or its triggers could be used to support rehabilitation in this regard. In this paper we briefly discuss the above issues and present our approach in developing a rehabilitation system.","cites":"12","conferencePercentile":"100"},{"venue":"WIAMIS","id":"1bace193bdde3d160acf0cec6f983131fc46ba1e","venue_1":"WIAMIS","year":"2009","title":"Comparative evaluation of spatial context techniques for semantic image analysis","authors":"Georgios Th. Papadopoulos, Carsten Saathoff, Marcin Grzegorzek, Vasileios Mezaris, Yiannis Kompatsiaris, Steffen Staab, Michael G. Strintzis","author_ids":"1724332, 2760019, 1727057, 1737436, 1906503, 1752093, 1721460","abstract":"In this paper, two approaches to utilizing contextual information in semantic image analysis are presented and comparatively evaluated. Both approaches make use of spatial context in the form of fuzzy directional relations. The first one is based on a Genetic Algorithm (GA), which is employed in order to decide upon the optimal semantic image interpretation by treating semantic image analysis as a global optimization problem. On the other hand, the second method follows a Binary Integer Programming (BIP) technique for estimating the optimal solution. Both spatial context techniques are evaluated with several different combinations of classifiers and low-level features, in order to demonstrate the improvements attained using spatial context in a number of different image analysis schemes.","cites":"4","conferencePercentile":"86.11111111"},{"venue":"WIAMIS","id":"dff64bbaaa3d15cb8bac06c7dc1aaf1c031666db","venue_1":"WIAMIS","year":"2012","title":"An intelligent depth-based obstacle detection system for visually-impaired aid applications","authors":"Chia-Hsiang Lee, Yu-Chi Su, Liang-Gee Chen","author_ids":"2849336, 1759390, 1714180","abstract":"In this paper, we present a robust depth-based obstacle detection system in computer vision. The system aims to assist the visually-impaired in detecting obstacles with distance information for safety. With analysis of the depth map, segmen­ tation and noise elimination are adopted to distinguish different objects according to the related depth information. Obstacle extraction mechanism is proposed to capture obstacles by various object proprieties revealing in the depth map. The proposed system can also be applied to emerging vision-based mobile applications, such as robots, intelligent vehicle navigation, and dynamic surveillance systems. Experimental results demonstrate the proposed system achieves high accuracy. In the indoor environment, the average detection rate is above 96.1 %. Even in the outdoor environment or in complete darkness, 93.7% detection rate is achieved on average.","cites":"7","conferencePercentile":"96.875"},{"venue":"WIAMIS","id":"88c0bb613db5f5d58cc07863fdb757ba484d43b2","venue_1":"WIAMIS","year":"2007","title":"A Semantic Event Detection Approach for Soccer Video based on Perception Concepts and Finiste State Machines","authors":"Liang Bai, Songyang Lao, Weiming Zhang, Gareth J. F. Jones, Alan F. Smeaton","author_ids":"1712642, 1716428, 4804820, 1750400, 1680223","abstract":"A significant application area for automated video analysis technology is the generation of personalized highlights of sports events. Sports games are always composed of a range of significant events. Automatically detecting these events in a sports video can enable users to interactively select their own highlights. In this paper we propose a semantic event detection approach based on Perception Concepts and Finite State Machines to automatically detect significant events within soccer video. Firstly we define a Perception Concept set for soccer videos based on identifiable feature elements within a soccer video. Secondly we design PC-FSM models to describe semantic events in soccer videos. A particular strength of this approach is that users are able to design their own semantic events and transfer event detection into graph matching. Experimental results based on recorded soccer broadcasts are used to illustrate the potential of this approach. 1. INTRODUCTION One of the areas of greatest expansion in video content is sports broadcasting. An important component of sports broadcasting is highlights of sports games which are usually prepared manually. However, these are not always available and the material included is selected by a single editor. This situation is inflexible with respect to individual viewers who may want a longer or shorter summary or to focus on certain event types; and the need for manual editing means generation of summaries is often not cost effective. Video technology can potentially provide new ways to view soccer videos which are more interactive and personal, rather than to view passively pre-edited highlights (when they are actually available). It is important for video processing and retrieval researchers to develop new ways for users to search for semantic events within soccer videos. To date work in this area has focused on query-by-text. The user enters the word \" goal \" , then the system searches for video clips including the word goal in the soundtrack or possibly in manually added metadata [1][2]. This clearly relies on either a well annotated soundtrack or a costly manual labeling of semantic events. Automatic detection of semantic events that captures the essential contents of a game is becoming more and more important. Related prior work towards automatic events detection in sports videos is described in [3][4][5]and[6]. In most existing work the event detection algorithms are embedded in systems and cannot easily be redefined. This means that users cannot adapt the event types detected or the …","cites":"9","conferencePercentile":"89.18918919"},{"venue":"WIAMIS","id":"6f1d0efeba5a5882038f1d306b7fff6318ab4e2b","venue_1":"WIAMIS","year":"2012","title":"A machine learning approach to determining tag relevance in geotagged Flickr imagery","authors":"Mark Hughes, Noel E. O'Connor, Gareth J. F. Jones","author_ids":"3326362, 1730463, 1750400","abstract":"We present a novel machine learning based approach to determining the semantic relevance of community contributed image annotations for the purposes of image retrieval. Current large scale community image retrieval systems typically rely on human annotated tags which are subjectively assigned and may not provide useful or semantically meaningful labels to the images. Homogeneous tags which fail to distinguish between are a common occurrence, which can lead to poor search effectiveness on this data. We described a method to improve text based image retrieval systems by eliminating generic or non relevant image tags. To classify tag relevance, we propose a novel feature set based on statistical information available for each tag within a collection of geotagged images harvested from Flickr. Using this feature set machine learning models are trained to classify the relevance of each tag to its associated image. The goal of this process is to allow for rich and accurate captioning of these images, with the objective of improving the accuracy of text based image retrieval systems. A thorough evaluation is carried out using a human annotated benchmark collection of Flickr tags.","cites":"0","conferencePercentile":"15.625"},{"venue":"WIAMIS","id":"7bef66b2b50279894449a3a81f71d8927d786da8","venue_1":"WIAMIS","year":"2008","title":"Analysis of Historical Artistic Documentaries","authors":"Matthias Zeppelzauer, Dalibor Mitrovic, Christian Breiteneder","author_ids":"1749201, 1749222, 1737188","abstract":"The paper introduces a novel interdisciplinary project addressing the analysis of historical artistic films. The type of employed material has not been subject to automatic analyses, so far. It poses challenges in all areas of content-based analysis and retrieval due to its complex temporal structure and due to substantial degradations. We propose robust features and a method for shot cut detection for this material that outperforms established techniques.","cites":"4","conferencePercentile":"58.82352941"},{"venue":"WIAMIS","id":"d28a2234bcb91c1559a744be303e017f81a3a88b","venue_1":"WIAMIS","year":"2010","title":"Eye tracking as an accessible assistive tool","authors":"Alfredo Armanini, Nicola Conci","author_ids":"2621775, 3058987","abstract":"In this work we present a tool for eye-tracking based on a cascade classifier, able to detect the area observed by the user in a grid of NxM cells. The system has been implemented with the goal of developing an accessible assistive tool for ALS patients, therefore the implementation relies on common off-the-shelf components to significantly reduce the deployment costs. Starting from the eye-tracker as the main component , the tool consists of a core architecture, complemented by number of reconfigurable plugins, customizable on the basis of the specific requirements of the user.","cites":"0","conferencePercentile":"25"}]}