{"WIDM.csv":[{"venue":"WIDM","id":"24434cd71e4edd1b4cab90e82febb79dd63b8dab","venue_1":"WIDM","year":"2008","title":"Web content summarization using social bookmarks: a new approach for social summarization","authors":"Jaehui Park, Tomohiro Fukuhara, Ikki Ohmukai, Hideaki Takeda, Sang-goo Lee","author_ids":"2052729, 2815435, 1701584, 1696371, 1734101","abstract":"An increasing number of Web applications are allowing users to play more active roles for enriching the source content. The enriched data can be used for various applications such as text summarization, opinion mining and ontology creation. In this paper, we propose a novel Web content summarization method that creates a text summary by exploiting user feedback (comments and tags) in a social bookmarking service. We had manually analyzed user feedback in several representative social services including del.icio.us, Digg, YouTube, and Amazon.com. We found that (1) user comments in each social service have its own characteristics with respect to summarization, and (2) a tag frequency rank does not necessarily represent its usefulness for summarization. Based on these observations, we conjecture that user feedback in social bookmarking services is more suitable for summarization than other type of social services. We implemented prototype system called SSNote that analyzes tags and user comments in del.icio.us, and extracts summaries. Performance evaluations of the system were conducted by comparing its output summary with manual summaries generated by human evaluators. Experimental results show that our approach highlights the potential benefits of user feedback in social bookmarking services.","cites":"8","conferencePercentile":"75"},{"venue":"WIDM","id":"247a300857343eedc5d9e723bfa8a0b7737aa87e","venue_1":"WIDM","year":"2008","title":"Quantify music artist similarity based on style and mood","authors":"Bo Shao, Tao Li, Mitsunori Ogihara","author_ids":"7705332, 1726351, 1774705","abstract":"Music artist similarity has been an active research topic in music information retrieval for a long time since it is especially useful for music recommendation and organization. However, it is a difficult problem. The similarity varies significantly due to different artistic aspects considered and most importantly, it is hard to quantify. In this paper, we propose a new framework for quantifying artist similarity. In the framework, we focus on style and mood aspects of artists whose descriptions are extracted from the authoritative information available at the All Music Guide website. We then generate style--mood joint taxonomies using hierarchical co-clustering algorithm, and quantify the semantic similarities between the style/mood terms based on the taxonomy structure and the positions of these terms in the taxonomies. Finally we calculate the artist similarities according to all the style/mood terms used to describe them. Experiments are conducted to show the effectiveness of our framework.","cites":"5","conferencePercentile":"60"},{"venue":"WIDM","id":"32cd262e3e07a2da2f3c5f12e2b5b8066b77098a","venue_1":"WIDM","year":"2012","title":"Web crawler middleware for search engine digital libraries: a case study for citeseerX","authors":"Jian Wu, Pradeep B. Teregowda, Madian Khabsa, Stephen Carman, Douglas Jordan, Jose San Pedro Wandelmer, Xin Lu, Prasenjit Mitra, C. Lee Giles","author_ids":"1734550, 1712873, 2072010, 2681304, 3056037, 2165519, 1706990, 1714911, 1749125","abstract":"Middleware is an important part of many search engine web crawling processes. We developed a middleware, the Crawl Document Importer (CDI), which selectively imports documents and the associated metadata to the digital library CiteSeerX crawl repository and database. This middleware is designed to be extensible as it provides a universal interface to the crawl database. It is designed to support input from multiple open source crawlers and archival formats, e.g., ARC, WARC. It can also import files downloaded via FTP. To use this middleware for another crawler, the user only needs to write a new log parser which returns a resource object with the standard metadata attributes and tells the middleware how to access downloaded files. When importing documents, users can specify document mime types and obtain text extracted from PDF/postscript documents. The middleware can adaptively identify academic research papers based on document context features. We developed a web user interface where the user can submit importing jobs. The middleware package can also work on supplemental jobs related to the crawl database and respository. Though designed for the CiteSeerX search engine, we feel this design would be appropriate for many search engine web crawling systems.","cites":"2","conferencePercentile":"63.63636364"},{"venue":"WIDM","id":"1b02bdcab8794f1f8592dbec702a53d772cdc9f9","venue_1":"WIDM","year":"2004","title":"A two-phase sampling technique for information extraction from hidden web databases","authors":"Yih-Ling Hedley, Muhammad Younas, Anne E. James, Mark Sanderson","author_ids":"2025114, 3353501, 1709814, 5007429","abstract":"Hidden Web databases maintain a collection of specialised documents, which are dynamically generated in response to users' queries. However, the documents are generated by Web page templates, which contain information that is irrelevant to queries. This paper presents a Two-Phase Sampling (2PS) technique that detects templates and extracts query-related information from the sampled documents of a database. In the first phase, 2PS queries databases with terms contained in their search interface pages and the subsequently sampled documents. This process retrieves a required number of documents. In the second phase, 2PS detects Web page templates in the sampled documents in order to extract information relevant to queries. We test 2PS on a number of real-world Hidden Web databases. Experimental results demonstrate that 2PS effectively eliminates irrelevant information contained in Web page templates and generates terms and frequencies with improved accuracy.","cites":"13","conferencePercentile":"50"},{"venue":"WIDM","id":"8c63e7c7d79459d78ea8ccf953802f3a88e31cf1","venue_1":"WIDM","year":"2008","title":"Modeling the mashup space","authors":"Serge Abiteboul, Ohad Greenshpan, Tova Milo","author_ids":"1711516, 2201956, 1702212","abstract":"We introduce a formal model for capturing the notion of mashup in its globality. The basic component in our model is the mashlet. A mashlet may query data sources, import other mashlets, use external Web services, and specify complex interaction patterns between its components. A mashlet state is modeled by a set of relations and its logic specified by datalog-style active rules. We are primarily concerned with changes in a mashlet state relations and rules. The interactions with users and other applications, as well as the consequent effects on the mashlets composition and behavior, are captured by streams of changes. The model facilitates dynamic mashlets composition, interaction and reuse, and captures the fundamental behavioral aspects of mashups.","cites":"23","conferencePercentile":"95"},{"venue":"WIDM","id":"159cb9a7337c552e1f27e2620596ac45fd4f1bfd","venue_1":"WIDM","year":"2006","title":"An architecture for creating collaborative semantically capable scientific data sharing infrastructures","authors":"Anuj R. Jaiswal, C. Lee Giles, Prasenjit Mitra, James Ze Wang","author_ids":"3225963, 1749125, 1714911, 1699550","abstract":"Increasingly, scientists are seeking to collaborate and share data among themselves. Such sharing is can be readily done by publishing data on the World-Wide Web. Meaningful querying and searching on such data depends upon the availability of accurate and adequate metadata that describes the data and the sources of the data. In this paper, we outline the architecture of an implemented cyber-infrastructure for chemistry that provides tools for users to upload datasets and their metadata to a database. Our proposal combines a two level metadata system with a centralized database repository and analysis tools to create an effective and capable data sharing infrastructure. Our infrastructure is extensible in that it can handle data in different formats and allows different analytic tools to be plugged in.","cites":"5","conferencePercentile":"41.66666667"},{"venue":"WIDM","id":"1f9003cc4cf0822af2b80ffff5bf766f573b7a59","venue_1":"WIDM","year":"2005","title":"A search result clustering method using informatively named entities","authors":"Hiroyuki Toda, Ryoji Kataoka","author_ids":"1717891, 1736683","abstract":"Clustering the results of a search helps the user to overview the information returned. In this paper, we regard the clustering task as indexing the search results. Here, an index means a structured label list that can makes it easier for the user to comprehend the labels and search results. To realize this goal, we make three proposals. First is to use Named Entity Extraction for term extraction. Second is a new label selecting criterion based on importance in the search result and the relation between terms and search queries. The third is label categorization using category information of labels, which is generated by NE extraction. We implement a prototype system based on these proposals and find that it offers much higher performance than existing methods; we focus on news articles in this paper.","cites":"27","conferencePercentile":"61.53846154"},{"venue":"WIDM","id":"33138f50e376d8342e1cc89549b818cf305b762f","venue_1":"WIDM","year":"2008","title":"Personalized ranking for digital libraries based on log analysis","authors":"Yang Sun, Huajing Li, Isaac G. Councill, Jian Huang, Wang-Chien Lee, C. Lee Giles","author_ids":"3136781, 7178438, 3291696, 1782736, 1686360, 1749125","abstract":"Given the exponential increase of indexable context on the Web, ranking is an increasingly difficult problem in information retrieval systems. Recent research shows that implicit feedback regarding user preferences can be extracted from web access logs in order to increase ranking performance. We analyze the implicit user feedback from access logs in the CiteSeer academic search engine and show how site structure can better inform the analysis of clickthrough feedback providing accurate personalized ranking services tailored to individual information retrieval systems. Experiment and analysis shows that our proposed method is more accurate on predicting user preferences than any non-personalized ranking methods when user preferences are stable over time. We compare our method with several non-personalized ranking methods including ranking SVM<i><sup>light</sup></i> as well as several ranking functions specific to the academic document domain. The results show that our ranking algorithm can reach 63.59% accuracy in comparison to 50.02% for ranking SVM<i><sup>light</sup></i> and below 43% for all other single feature ranking methods. We also show how the derived personalized ranking vectors can be employed for other ranking-related purposes such as recommendation systems.","cites":"2","conferencePercentile":"17.5"},{"venue":"WIDM","id":"51c6bcd3e8dab401ef089ec38401823d0bfed9b0","venue_1":"WIDM","year":"2007","title":"Data allocation scheme based on term weight for P2P information retrieval","authors":"Hisashi Kurasawa, Hiromi Wakaki, Atsuhiro Takasu, Jun Adachi","author_ids":"2579581, 2501810, 1707601, 1742146","abstract":"Many Peer-to-Peer information retrieval systems that use a global index have already been proposed that can retrieve documents relevant to a query. Since documents are allocated to peers regardless of the query, the system needs to connect many peers to gather the relevant documents. We propose a new data allocation scheme for P2P information retrieval that we call Concordia. Concordia uses a node to allocate a document based on the weight of each term in the document to efficiently assemble all the documents relevant to a query from the P2P Network. Moreover, the node encodes the binary data of a document with an erasure code, and Concordia produces an efficient redundancy for counteracting node failures.","cites":"1","conferencePercentile":"32.5"},{"venue":"WIDM","id":"96c9db733c65fac3d1b342efa0eb8881dcf8d296","venue_1":"WIDM","year":"2009","title":"Finding intermediate entity between two examples on the web","authors":"Naoto Asahi, Takehiro Yamamoto, Satoshi Nakamura, Katsumi Tanaka","author_ids":"2630517, 7715678, 4807127, 1750132","abstract":"We propose a method for finding an intermediate entity between two examples on the Web. For example, a user wants to find events that occurred between the Battle of Red Cliffs and the death of Cao Cao. In this situation, the user wants to find something intermediate between two events, processes, or objects. We first describe the problem of finding an entity between two examples. We then propose a method for extracting an intermediate entity between two inputs using a Web search engine. The method focuses on the positions of words in Web pages and then extracts words that are likely to appear between the two inputs. Finally, we show the usefulness of our method based on experiments.","cites":"0","conferencePercentile":"20.58823529"},{"venue":"WIDM","id":"291bca6ddeb125a7437337957b83da1c705e9b25","venue_1":"WIDM","year":"2007","title":"Detecting age of page content","authors":"Adam Jatowt, Yukiko Kawai, Katsumi Tanaka","author_ids":"1774986, 1700611, 1750132","abstract":"Web pages often contain objects created at different times. The information about the age of such objects may provide useful context for understanding page content and may serve many potential uses. In this paper, we describe a novel concept for detecting approximate creation dates of content elements in Web pages. Our approach is based on dynamically reconstructing page histories using data extracted from external sources - Web archives and efficiently searching inside them to detect insertion dates of content elements. We discuss various issues involving the proposed approach and demonstrate the example of an application that enhances browsing the Web by inserting annotations with temporal metadata into page content on user request.","cites":"14","conferencePercentile":"90"},{"venue":"WIDM","id":"ec12b7e294c91fae1e51b90d6addefb94d574b79","venue_1":"WIDM","year":"2007","title":"Toward editable web browser: edit-and-propagate operation for web browsing","authors":"Satoshi Nakamura, Takehiro Yamamoto, Katsumi Tanaka","author_ids":"4807127, 7715678, 1750132","abstract":"This paper proposes a novel technique of browsing Web pages called <i>\"Edit-and-Propagate\"</i> operation based browsing, where edit operation is regarded as the third operation for interacting with the <b>WWW</b> after conventional clicking and scrolling towards realizing the <i>Editable Web Browser</i>. Our method enables users to delete/emphasize any portion of a browsed Web page at any time and modifies the page by propagating the edit operation. For example, the user can easily delete almost any uninteresting portion of a Web page merely by deleting an example of an unwanted portion. While browsing a Web search result page, the user can rerank search results by deleting an unwanted term or by emphasizing an important term. In this paper, we describe the concept of <i>\"Edit-and-Propagate\"</i> based browsing, and the implementation of our prototypes. Then we describe the results of our evaluation, which demonstrate the usefulness of our system.","cites":"0","conferencePercentile":"15"},{"venue":"WIDM","id":"027a779d1073ae694b5242342c1f25176c76b276","venue_1":"WIDM","year":"2002","title":"Ranking user's relevance to a topic through link analysis on web logs","authors":"Jidong Wang, Zheng Chen, Li Tao, Wei-Ying Ma, Wenyin Liu","author_ids":"4315466, 1705657, 1798514, 1705244, 4163820","abstract":"Computing the web-user's relevance to a give topic is an important task for any personalization service on the Web. Since the interest and preference of a web-user are revealed in his Web browsing history, in this paper we develop a novel approach that utilizes Web logs to compute the relevance of a web-user to a given query. In contrast to traditional methods that are purely based on textual analysis, our approach calculates the web-user's relevance through link analysis under a unified framework where the importance of web-pages and web-users mutually reinforce each other in an iterative way. The experimental results show that our approach has achieved 53 of accuracy when ranking the web-user's relevance to a search topic.","cites":"40","conferencePercentile":"92.10526316"},{"venue":"WIDM","id":"85e7b99f516232aeaec087b89e934c1e607d4018","venue_1":"WIDM","year":"2007","title":"Web based linkage","authors":"Ergin Elmacioglu, Min-Yen Kan, Dongwon Lee, Yi Zhang","author_ids":"3179332, 1807775, 1784227, 1805872","abstract":"When a variety of names are used for the same real-world entity, the problem of detecting all such variants has been known as the <i>(record) linkage</i> or <i>entity resolution</i> problem. In this paper, toward this problem, we propose a novel approach that uses the Web as the collective knowledge source in addition to contents of entities. Our hypothesis is that if an entity e1 is a duplicate of another entity <i>e</i><sub>2</sub>, and if e1 frequently appears together with information <i>I</i> on the Web, then <i>e</i><sub>2</sub> may appear frequently with <i>I</i> on the Web. By using search engines, we analyze the frequency, URLs, or contents of the returned web pages to capture the information <i>I</i> of an entity. Extensive experiments verify that our hypothesis holds in many real settings, and the idea of using the Web as the additional source for the linkage problem is promising. Our proposal shows 51% (on average) and 193% (at best) improvement in precision/recall compared to a baseline approach.","cites":"11","conferencePercentile":"70"},{"venue":"WIDM","id":"3cbad453f459d7c10b6338504e4d8a0d6de51792","venue_1":"WIDM","year":"2002","title":"Indexing web access-logs for pattern queries","authors":"Alexandros Nanopoulos, Yannis Manolopoulos, Maciej Zakrzewicz, Tadeusz Morzy","author_ids":"1728442, 1796253, 1679247, 1693699","abstract":"In this paper, we develop a new indexing method for large web access-logs. We are concerned with pattern queries, which advocate the search for access sequences that contain certain query patterns. This kind of queries find applications in processing web-log mining results (e.g., finding typical/atypical access-sequences). The proposed method focuses on scalability to web-logs' sizes. For this reason, we examine the gains due to signature-trees, which can further improve the scalability to very large web-logs. Experimental results illustrate the superiority of the proposed method.","cites":"13","conferencePercentile":"63.15789474"},{"venue":"WIDM","id":"2e75bad125df1183e179d508215301754fcba881","venue_1":"WIDM","year":"2004","title":"Next generation CiteSeer","authors":"C. Lee Giles","author_ids":"1749125","abstract":"CiteSeer, a computer and information science search engine and digital library, has been a radical departure for scientific document access and analysis. With nearly 700,000 documents, it has sometimes two million page views a day making it one of the most popular document access engines in science. CiteSeer is also portable, having been extended to ebusiness (eBizSearch) and more recently to academic business documents (SMEALSearch). CiteSeer is based on two features: actively acquiring new documents and automatic tagging and linking of metadata information inherent in an academic document's syntactic structure. Why is CiteSeer so popular? We discuss this and methods for providing new tagged metadata such as institutions and acknowledgements, new data resources and services and the issues in automation. We then discuss the next generation of CiteSeer.","cites":"0","conferencePercentile":"7.5"},{"venue":"WIDM","id":"de1516476833cf8cf110e21a25752c2bd030e706","venue_1":"WIDM","year":"2008","title":"From Web 1.0 to Web 2.0 and back -: how did your grandma use to tag?","authors":"Sheila Kinsella, Adriana Budura, Gleb Skobeltsyn, Sebastian Michel, John G. Breslin, Karl Aberer","author_ids":"1746404, 2726644, 3305996, 8436249, 1707342, 1751802","abstract":"We consider the applicability of terms extracted from anchortext as a source of Web page descriptions in the form of tags. With a relatively simple and easy-to-use method, we show that anchortext significantly overlaps with tags obtained from the popular tagging portal del.icio.us. Considering the size and diversity of the user community potentially involved in social tagging, this observation is rather surprising. Furthermore, we show by an evaluation using human-created relevance assessments the general suitability of the anchortext tag generation in terms of user-perceived precision values. The awareness of this easy-to-obtain source of tags could trigger the rise of new tagging portals pushed by this automatic bootstrapping process or be applied in already existing portals to increase the number of tags per page by merely looking at the anchortext which exists anyway.","cites":"3","conferencePercentile":"27.5"},{"venue":"WIDM","id":"d1839ba5013287ab58a810cb9bc8f32499a74dc7","venue_1":"WIDM","year":"2009","title":"Investigation of children's characteristics for web browsing","authors":"Mayu Iwata, Yuki Arase, Takahiro Hara, Shojiro Nishio","author_ids":"1707450, 3043844, 1697569, 1717916","abstract":"Owing to the proliferation of Internet environment, young children have started accessing the Web. However, most existing pages are oriented for grown-ups. Particularly, children are not good at browsing Web pages with full of characters, such as news site. Though such general pages have variety and detailed information than pages for children, children cannot understand their information in a current presentation style. In this paper, we investigate children's characteristics on Web browsing. First, we implemented a Web browser for children, which converts general pages into a children-friendly presentation. Then, we conducted a user experiment to compare our browser with two conventional Web browsers. The result shows that dividing a Web page into contents and showing each of them with animations and changing difficult expressions into easy and friendly ones are effective to keep children's interest and help them to understand the contents.","cites":"0","conferencePercentile":"20.58823529"}]}