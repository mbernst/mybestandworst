{"WISE_Workshops.csv":[{"venue":"WISE Workshops","id":"0c8b7b80f39cb2589a9f508a18c4b9e9c461a329","venue_1":"WISE Workshops","year":"2003","title":"Extending WebML for modeling multi-channel context-aware Web applications","authors":"Stefano Ceri, Florian Daniel, Maristella Matera","author_ids":"1705257, 1797572, 1727982","abstract":"This paper focuses on conceptual modeling of multi-channel, context aware Web applications, and in particular proposes some solutions conceived within the WebML method. WebML is a conceptual model for data-intensive Web applications, which already offers some constructs for one-to-one personalization and multi-channel delivery. In this paper we introduce some new extensions that will allow representing a context model at data level, and exploit it at hypertext level for offering context-aware, customized services and contents, accessible through multiple channels.","cites":"30","conferencePercentile":"84.61538462"},{"venue":"WISE Workshops","id":"7917d9245d68fe91e7ccc90cd1b8b2d9a322d9b1","venue_1":"WISE Workshops","year":"2005","title":"Scalable Instance Retrieval for the Semantic Web by Approximation","authors":"Holger Wache, Perry Groot, Heiner Stuckenschmidt","author_ids":"1765833, 3261285, 1698459","abstract":"Approximation has been identified as a potential way of reducing the complexity of logical reasoning. Here we explore approximation for speeding up instance retrieval in a Semantic Web context. known that reasoning is a hard problem. Especially in instance retrieval when the number of instances that need to be retrieved becomes very large. We discuss two approximation methods for retrieving instances to conjunctive queries over DL T-Boxes and the results of experiments carried out with a modified version of the Instance Store System. A central issue in the Semantic Web research community is the expressivity of its underlying language and the complexity of the reasoning services it supports. There is a direct correspondence between the current Semantic Web ontology language OWL and Description Logic (DL). 1 Research in DL has lead to sophisticated DL reasoners [6, 3, 5] that can be used to reason with OWL ontologies on the Semantic Web. Considering T-Box reasoning, current state of the art techniques seem capable of dealing with real world ontologies [7, 4]. However, besides T-Box reasoning, an important application domain of ontologies is A-Box reasoning , i.e., reasoning and retrieving the individuals in an ontology. Experiments have shown that state of the art DL reasoners break down for A-Box reasoning when the number of instances becomes large [8]. Present work focusses at approximation techniques to make A-Box reasoning in DLs more scalable when retrieving instances from an ontology with a large number of instances. In this paper, we investigate optimization techniques that are based on approximate logical reasoning. The underlying idea of these techniques is to replace certain inference problems by simpler problems such that either the soundness or the completeness, but not both, of the solutions is preserved. The solutions to the simpler problems are approximate solutions to the original problem.","cites":"26","conferencePercentile":"83.33333333"},{"venue":"WISE Workshops","id":"2a89b16d2a171cc1e5da213d0c5ba9b92ba4b256","venue_1":"WISE Workshops","year":"2007","title":"Using Ontology with Semantic Web Services to Support Modeling in Systems Biology","authors":"Zhouyang Sun, Anthony Finkelstein, Jonathan Ashmore","author_ids":"2248006, 1714549, 3305356","abstract":"Modeling in systems biology is concerned with using experimental information and mathematical methods to build quantitative models at different biological scales. This requires interoperation among various knowledge sources and services, such as biological databases, mathematical equations, data analysis tools, and so on. Semantic Web Services provide an infrastructure that allows a consistent representation of these knowledge sources as web-based information units, and enables discovery, composition, and execution of these units by associating machine-processable semantics description with them. In this paper, we show a method of using ontology alongside a semantic web services infrastructure to provide a knowledge standardisation framework in order to support modeling in systems biology. We demonstrate how ontologies are used to control the transformation of biological databases and data analysis methods into Web Services, and how ontology-based web services descriptions (OWLS), are used to enable the composition between these services. 1. Motivation Systems biology is an emergent discipline that involves integrating biological knowledge across scales and domains, in order to understand the dynamics of diverse and interacting biological processes as integral systems [1]. In the study of systems biology, one of the essential tasks is to couple experimental biologists' observations with scientific models. This task encompasses several collaboration processes involving experimenters and modelers: using experimental observations as the ground for constructing models of biological entities and the relations among them; qualitatively and quantitatively analysing the resulting models and then comparing the analyses against experimental data for model validation; providing instructive feedback in order to refine both the models and experimental protocols. The progress of systems biology relies on the success of these experimenter-modeler collaboration processes.","cites":"3","conferencePercentile":"26.66666667"},{"venue":"WISE Workshops","id":"a0e632bc7aaf2f5a89f42091c8d1c3be9dcaabee","venue_1":"WISE Workshops","year":"2003","title":"Metatemplate driven multi-channel presentation","authors":"Michael Grossniklaus, Moira C. Norrie, Patrick BÃ¼chler","author_ids":"1786155, 1685670, 3165664","abstract":"The separation of content, layout, structure and view that is enforced by the various emerging content management systems and web modelling tools, renders the implementation of suitable editors for content providers very difficult. As most of these systems work on the basis of XML and XSLT and support multiple languages and presentation channels, creating and editing content has become more tedious and challenging. Various attempts at creating graph-ical XSLT editors have been made but most of them lack the versatility to be used in conjunction with a content management system. In this paper, we present a solution based on generic metatemplates. We then describe how we exploited this approach for an editor integrated into a content management system that we have developed.","cites":"4","conferencePercentile":"46.15384615"},{"venue":"WISE Workshops","id":"580895cec783b559889bfe5252c6ed2342776660","venue_1":"WISE Workshops","year":"2003","title":"Trajectory representation in location-based services: problems & solution","authors":"Nirvana Meratnia, Rolf A. de By","author_ids":"1784678, 2516125","abstract":"Recently, much work has been done in feasibility studies on services offered to moving objects in an environment equipped with mobile telephony, network technology and GIS. However, despite of all work on GIS and databases, the situations in which the whereabouts of objects are constantly monitored and stored for future analysis are an important class of problems that present-day database/GIS has difficulty to handle. Considering the fact that data about whereabouts of moving objects are acquired in a discrete way, providing the data when no observation is available is a must. Therefore, obtaining a \" faithful representation \" of trajectories with a sufficient number of discrete (though possibly erroneous) data points is the objective of this research.","cites":"5","conferencePercentile":"53.84615385"},{"venue":"WISE Workshops","id":"de55ea04a9fae25d1cf56338ac006fab7ba9fc18","venue_1":"WISE Workshops","year":"2005","title":"SPARQL Query Processing with Conventional Relational Database Systems","authors":"Stephen Harris, Nigel Shadbolt","author_ids":"6457181, 1705314","abstract":"This paper describes an evolution of the 3store RDF storage system, extended to provide a SPARQL query interface and informed by lessons learned in the area of scalable RDF storage.","cites":"61","conferencePercentile":"91.66666667"},{"venue":"WISE Workshops","id":"281c1681e36326301f1184d4143c59074954a0fa","venue_1":"WISE Workshops","year":"2005","title":"Time - Space Trade-Offs in Scaling up RDF Schema Reasoning","authors":"Heiner Stuckenschmidt, Jeen Broekstra","author_ids":"1698459, 3037266","abstract":"A common way of reducing run time complexity of RDF Schema reasoning is to compute (parts of) the deductive closure of a model offline. This reduces the complexity at run time, but increases the space requirements and model maintenance because derivable facts have to be stored explicitly and checked for validity when the model is updated. In this paper we experimentally identify certain kinds of statements as the major sources for the increase. Based on this observation , we develop a new approach for RDF reasoning that only computes a small part of the implied statements offline thereby reducing space requirements, up-load time and maintenance overhead. The computed fragment is chosen in such a way that the problem of inferring implied statements at run time can be reduced to a simple form of query rewriting. This new methods has two benefits: it reduces the amount of storage space needed and it allows to perform online reasoning without using a dedicated inference engine. A common way of reducing run time complexity of RDF Schema reasoning is to compute (parts of) the deductive closure of a model offline using the deduction rules specified in the RDF Semantics Specification [6] and work on the expanded model at query time. Most implementations of RDF reasoning use existing approaches like the RETE algorithm [3] that have originally been invented for Deductive Databases and Rule-based Expert systems. Obviously there is a trade-off between run-time complexity and the amount of space needed to store the deductive closure. In the first part of this paper we analyze these space requirements of computing the deductive closure using a number of large real-life RDF models and compare it to the minimal space needed for storing the model. We argue that the fact that existing algorithms for offline closure computation work quite well is mainly a consequence of the fact that the scenarios in which they were applied are still far away from the vision of Semantic Web reasoning as they work on a relatively small amount of centrally stored data. In a recent study Guo et al. revealed the limitations of current systems with respect to handling large amounts of data both in terms of upload and query time [4]. Another serious problem of closure computation is the need to recheck the validity of derived statements when the model is changed. This revision process is known to be very expensive â¦","cites":"14","conferencePercentile":"75"},{"venue":"WISE Workshops","id":"d03a2779c27fcef48df863adc5a11efb9e68a9d5","venue_1":"WISE Workshops","year":"2005","title":"A Semantic Distance Measure for Matching Web Services","authors":"Arif Bramantoro, Shonali Krishnaswamy, Maria Indrawan","author_ids":"2591609, 1781256, 2865302","abstract":"A key issue in web services is matching that involves comparing user requests with advertised services and finding the best available ones. In semantic web services, an ontology is used by the matching system to determine the semantic relationship between the requests and the registered services. In this paper, we propose that the semantic relationship can be measured quantitatively in order to provide a more precise similarity measures between the requested and advertised services and to produce a better ranking of relevant services. We proposes and develops a Semantic Distance Measure that is tailored to provide a quantitative measure that indicates similarity between advertised and requested services. We establish that such a measure is an effective means of discriminating services at a level of granularity that is able to enhance the matching process in semantic web services.","cites":"7","conferencePercentile":"58.33333333"},{"venue":"WISE Workshops","id":"19af92153ef7babcbf983f7b7dc515b6ba2fabad","venue_1":"WISE Workshops","year":"2002","title":"Log Mining to Improve the Performance of Site Search","authors":"Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, Wei-Ying Ma, Chao-Jun Lu","author_ids":"1701421, 1741348, 1705657, 1705244, 3235115","abstract":"Despite of the popularity of global search engines, people still suffer from low accuracy of site search. The primary reason lies in the difference of link structures and data scale between global Web and website, which leads to failures of traditional re-ranking methods such as HITS, PageRank and DirectHit. This paper proposes a novel re-ranking method based on user logs within websites. With the help of website taxonomy, we mine for generalized association rules and abstract access patterns of different levels. Mining results are subsequently used to re-rank the retrieved pages. One of the advantages of our mining algorithm is that it resolves the diversity problem of user's access behavior and discovers general patterns. Experiment shows that the proposed method outperforms keyword-based method by 15% and DirectHit by 13% respectively.","cites":"19","conferencePercentile":"93.75"},{"venue":"WISE Workshops","id":"e57673e23abb3d6aa099e7ac9820ec807a6328bb","venue_1":"WISE Workshops","year":"2008","title":"Engineering Issues for the Web 2.0","authors":"Yanchun Zhang, Florian Daniel, Santiago MeliÃ¡, Katsumi Tanaka, Athman Bouguettaya, Daniela Nicklas","author_ids":"1722358, 1797572, 1694010, 1750132, 1705708, 1696621","abstract":"1 Editorial The Web has changed the way in which we work nowadays and it is not surprising that the Web itself is changing fast. From its inception as a static repository of documents in its youth as the site for different kinds of business and learning applications, it has become now a platform for multiple and disparate uses. New applications like social networks (in the style of Facebook, Hi5, etc.) and multimedia repositories (such as Flickr and Youtube) co-exist with the \" old \" news and e-commerce sites which have been improved with the ideas of this new generation of sites. At the same time the Web offers new interaction possibilities that facilitate desktop-like interfaces; it is also the way in which new software is built by mashing up distributed components using service-oriented communication. Creativity has been enhanced and every day we can discover new Web-based communities and blogs; millions of people contribute to Wikis and tag pages giving rise to a growing set of folksonomies. And with the advent of the Semantic Web we can give more meanings to the flat data in blogs, Wikis and social sites. Some years ago the word Web 2.0 was coined to indicate the transition of the Web as a repository to the Web as a platform. Though many people argue that this name is just a buzzword because most of the so-called Web 2.0 applications were possible years before, and while the underlying Web technologies have not yet undergone a revolution, it is a matter of fact that more and more people associate the term Web 2.0 with the above-mentioned applications and facilities.","cites":"0","conferencePercentile":"16.66666667"},{"venue":"WISE Workshops","id":"65b804a3dc02169e46eb990abbff7ff2b1d49545","venue_1":"WISE Workshops","year":"2002","title":"An XML Specification Language to Support a Virtual Marketplace of Data Mining E-Services","authors":"Shonali Krishnaswamy, E. P. See, J. N. Ho, W. Gunawan","author_ids":"1781256, 2384552, 2638323, 2852940","abstract":"The emergence of Application Service Providers (ASP) hosting Internet-based data mining services is being seen as a viable alternative for organisations that value their knowledge resources but are constrained by the high cost of data mining software. The current modus operandi for data mining ASPs does not represent a true marketplace environment characterised by open competition. We have developed an XML specification language to enable interactions between clients and service providers in a virtual marketplace environment. In this paper we present this language and discuss how it supports processes such as matching, ranking and negotiation in a virtual marketplace. We also present the design and implementation of our prototype system.","cites":"1","conferencePercentile":"15.625"},{"venue":"WISE Workshops","id":"2833bf6676f9c5c3e876407a449dff77853d28bb","venue_1":"WISE Workshops","year":"2002","title":"Information Concepts for Content Management","authors":"Michael Grossniklaus, Moira C. Norrie","author_ids":"1786155, 1685670","abstract":"Content delivery is rapidly emerging as a complex systems domain concerned with multi-channel, multi-format publication of information across user and application domains. A variety of content management solutions have been developed in response to these challenges based on, not only differing technologies, but also heterogeneous approaches. However, none of these present a solution that is both sufficient and consistent. Here we present an analysis of requirements leading to a general model of the information concepts central to content management. This model is the basis for a web content management solution currently under development.","cites":"23","conferencePercentile":"100"},{"venue":"WISE Workshops","id":"816605c56b6c2a78a93ca77d8d7d5f30b1ca5b39","venue_1":"WISE Workshops","year":"2002","title":"Efficiency and Performance of Web Cache Reporting Strategies","authors":"John C.-I. Chuang, Steve Kafka, Kim Norlen","author_ids":"1924565, 2171388, 2285462","abstract":"World Wide Web content providers often resort to \" cache-busting \" in order to obtain demographic information. Object usage reporting methods have been proposed to address this problem. We quantitatively compare strategies for reporting object hits from proxy caches back to origin servers, and propose novel strategies for improving reporting performance and efficiency. Examining hit-metering and usage-limiting approaches proposed in RFC 2227, we find a fundamental trade-off between reporting latency and efficiency. Further, we find the temporal locality in the server reference stream to be significantly stronger than that in the object reference stream. We propose a server report aggregation strategy that leverages this fact, and show that it can reduce reporting latency and improve efficiency by as much as 80% and 100% respectively. We also propose and evaluate additional strategies to improve performance. These include: dynamic reporting thresholds, report aggregation in a cache hierarchy, and piggybacking reports on existing HTTP messages.","cites":"3","conferencePercentile":"50"},{"venue":"WISE Workshops","id":"c4178d0faec183a1ea9a87967a9f30425ada7bdd","venue_1":"WISE Workshops","year":"2007","title":"How Do Users Express Goals on the Web? - An Exploration of Intentional Structures in Web Search","authors":"Markus Strohmaier, Mathias Lux, Michael Granitzer, Peter Scheir, Sotirios Liaskos, Eric S. K. Yu","author_ids":"1743043, 6517033, 2389675, 2142781, 2268434, 1746021","abstract":"Many activities on the web are driven by high-level goals of users, such as \" plan a trip \" or \" buy some product \". In this paper, we are interested in exploring the role and structure of users' goals in web search. We want gain insights into how users express goals, and how their goals can be represented in a semi-formal way. The paper presents results from an exploratory study that focused on analyzing selected search sessions from a search engine log. In a detailed example, we demonstrate how goal-oriented search can be represented and understood as a traversal of goal graphs. Finally, we provide some ideas on how to construct large-scale goal graphs in a semi-algorithmic, collaborative way. We conclude with a description of a series of challenges that we consider to be important for future research. 1 Motivation In a highly influential article regarding the future of the web [1], Tim Berners-Lee sketches a scenario that describes a set of agents collaborating on the web to address different needs of users â such as \" get medication \" , \" find medical providers \" or \" coordinate appointments \". In fact, many activities on the web are already implicitly driven by goals today. Users utilize the web for buying products, planning trips, conducting business, doing research or seeking health advice. Many of these activities involve rather high-level goals of users, which are typically knowledge intensive and often benefit from social relations and collaboration. Yet, the web in its current form is largely non-intentional. That means the web lacks explicit intentional structures and representations, which would allow systems to, for example, associate users' goals with resources available on the web. As a consequence, every time users turn to the web for a specific purpose they are required to cognitively translate their high-level goals into the non-intentional structure of the web. They need to break down their goals into specific search queries, tag concepts, classification terms or ontological vocabulary. This prevents users from,","cites":"10","conferencePercentile":"80"},{"venue":"WISE Workshops","id":"4231d4a6e2f2ba753c9a9e970125cd6cab87547b","venue_1":"WISE Workshops","year":"2010","title":"A Security and High-Availability Layer for Cloud Storage","authors":"Maxim Schnjakin, Rehab Alnemr, Christoph Meinel","author_ids":"1833217, 1969770, 1708312","abstract":"Cloud Computing as a service on demand architecture has become a topic of interest in the last few years. The outsourcing of duties and infrastructure to external parties enables new services to be established quickly, scaled on demand, and with low financial risk. Cloud storage enables organizations to manage their data with low operational expenses. Nevertheless, several issues such as security and the risk to become dependent on a provider for its service should be considered before entering the cloud. In general, a switch of a storage provider is associated with high costs of adapting new APIs and additional charges for inbound and outbound bandwidth and requests. In this paper we use the principle of RAID-technology in cloud infrastructure to manage data distribution across cloud storage providers. The distribution is based on users expectations regarding providers geographic location, quality of service, providers reputation, and budget preferences. Our approach allows users to avoid vendor lock-in, reduce cost of switching providers and increase security and availability of their data. We also explain on how the proposed system removes the complexity of interacting with multiple storage providers while maintaining security.","cites":"6","conferencePercentile":"100"},{"venue":"WISE Workshops","id":"c13889cf6c5a42f68b8a2eba5e4a475c8ea6206f","venue_1":"WISE Workshops","year":"2007","title":"Quality of Web Usability Evaluation Methods: An Empirical Study on MiLE+","authors":"Davide Bolchini, Franca Garzotto","author_ids":"1780806, 1754440","abstract":"Outline zMotivation zGoals zMiLE+ zEmpirical study: {Experiment 1: \" quick inspection \" {Experiment 2: \" usability project \" zResults and Discussion 3 Motivation z A proliferation of Usability Evaluation Methods (UEMs) { Different philosophies-conception of \" quality \" , \" usability \" and their interrelationships-, thus approaches and techniques z Every method should support the improvement of the usability of the application z However, ongoing discussion (and little agreeement) on the salient \" quality attributes \" for UEMsâ¦ 4 Motivation-2 z Quality attributes concerning the output of the evaluation { Effectiveness? { Number of usability problems discovered â¦ { Thoroughness (found problems vs existing problems) â¦ { Reliability (consistency of results across different inspectors)â¦ { Validity (correcting predicting user's behaviour, no or minimized false positivesâ¦) { Productivityâ¦ { Scopeâ¦ z Quality attributes concerning acceptability and adoption { Learnability { Applicability and Compatibility in current practice { Verticalization on domains { Reusability { Cost-effectiveness z Supporting arguments with empirical data 5 Goals z Evaluate MiLE+ {Ongoing development from 2000 {Used in consultancy projects, in training professionals + ca. 200 students a year. z Focus on few key attributes that we could measure in a realistic setting and that support effective adoption: { Performance { Efficiency { Cost-effectiveness { Learnability 6 Goals zVerifying key \" acceptability \" requirements for a UEM: {Become able to use the method after a reasonable time (1-3 person/days) of \" study \" {Being able to detect the largest amount of usability problems with the minimum effort {Producing a first set of results in a few hours and a complete analysis in a few days z MiLE+ is a usability inspection method {Evolution of two previous usability methods: z SUE (Systematic Usability Evaluation) z MiLE z Borrow general concepts from mainstream usability inspection approaches z Foster a systematic, structured approach to the analysis, yet aimed at being particularly suitable to novice evaluators 8","cites":"15","conferencePercentile":"90"}]}