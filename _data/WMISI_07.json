{"WMISI_07.csv":[{"venue":"WMISI '07","id":"80254dc799d937f36c22ef97d02234fee252043a","venue_1":"WMISI '07","year":"2007","title":"Bringing context into play: supporting game interaction through real-time context acquisition","authors":"Radu-Daniel Vatavu, Stefan-Gheorghe Pentiuc, Tudor Ioan Cerlinca","author_ids":"1794666, 8412565, ","abstract":"We present a new interaction technique that we call Context Interaction and we discuss it in relation with computer games due to their popularity. Although the HCI in gaming benefits of many devices and controllers as well as from many interaction metaphors, they only allow players to control their characters in the game and not the context of the action or the game's environment. The environment change option, if at all supported, may sometimes be carried out in special editing sessions before the actual game begins, e.g. by choosing the track for car racing. We present a simple computer vision technique that allows players to interact with the game environment in real-time and thus to perform Context Interaction. Objects placed on a table are captured by a video camera and transformed into game elements with a real-time feedback within the game. Context Interaction comes as complementary multimodal interaction to the commonly encountered game controllers. It is simple, intuitive and provides real-time feedback.","cites":"1","conferencePercentile":"36.36363636"},{"venue":"WMISI '07","id":"9356c02005fa602b5af82cabef99b19136c08154","venue_1":"WMISI '07","year":"2007","title":"Learning the meaning of action commands based on \"no news is good news\" criterion","authors":"Kazuaki Tanaka, Xiang Zuo, Yasuaki Sagano, Natsuki Oka","author_ids":"7507619, 2983155, , 2505709","abstract":"In the future, robots will become common in our daily life. For using the robot more efficiently, it is desirable that the robot would have learning ability. However, a human teaching process for robot learning in the real environment usually takes a very long period of time. We hence believe that the robot should learn from implicit information which is included in human natural behavior. We direct our attention to the lack of utterance as a kind of implicit information, and insist that the lack of utterance should be interpreted as a positive evaluation of the ongoing action, which we call No News Criterion, in a robot navigation context. In this paper, we propose an efficient command learning algorithm based on the No News Criterion, and demonstrate its effectiveness by a human-robot interaction experiment in the real environment.","cites":"1","conferencePercentile":"36.36363636"},{"venue":"WMISI '07","id":"094e2de34fa1ac126fabe4f408544ed672a09d0d","venue_1":"WMISI '07","year":"2007","title":"Introducing semantic information during conceptual modelling of interaction for virtual environments","authors":"Lode Vanacken, Chris Raymaekers, Karin Coninx","author_ids":"2823309, 1736922, 1695519","abstract":"The integration of semantic information in virtual environment interaction is mostly still ad-hoc. The system is usually designed in such a way that the design of the framework incorporates the semantic information which then can be used to utilise these semantics during interaction. We introduce a model-based user interface approach which introduces semantic information, represented using ontologies, during the modelling phase. This semantic information itself is created during the design of the virtual world. The approach we propose is system independent and makes it possible for the semantic information content to be chosen and adapted in complete freedom without considering the underlying framework. We incorporate semantics in NiMMiT, our notation for multimodal interaction modelling. We present two case studies which validate the flexibility of our approach.","cites":"3","conferencePercentile":"68.18181818"},{"venue":"WMISI '07","id":"44202b55ae27c4bf47c9e94679e89ab34254cdfe","venue_1":"WMISI '07","year":"2007","title":"Towards adaptive object recognition for situated human-computer interaction","authors":"Kate Saenko, Trevor Darrell","author_ids":"2903226, 1753210","abstract":"Object recognition is an important part of human-computer interaction in situated environments, such as a home or an office. Especially useful is category-level recognition (e.g., recognizing the class of chairs, as opposed to a particular chair.) While humans can employ multimodal cues for cate-gorizing objects during situated conversational interactions, most computer algorithms currently rely on vision-only or speech-only recognition. We are developing a method for learning about physical objects found in a situated environment based on visual and spoken input provided by the user. The algorithm operates on generic databases of labeled object images and transcribed speech data, plus unlabeled audio and images of a user refering to objects in the environment. By exploiting the generic labeled databases, the algorithm would associate probable object-referring words with probable visual representations of those objects, and use both modalities to determine the object label. The first advantage of this approach over visual-only or speech-only recognition is the ability to disambiguate object categories using complementary information sources. The second advantage is that, using the additional unlabeled data gathered during the interaction, the system can potentially improve its recognition of new category instances in the physical environment in which it is situated, as well as of new utterances spoken by the same user, compared to a system that uses only the generic labeled databases. It can achieve this by adapting its generic object classifiers and its generic speech and language models","cites":"4","conferencePercentile":"81.81818182"}]}