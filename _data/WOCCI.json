{"WOCCI.csv":[{"venue":"WOCCI","id":"2a12fbf48a7e73b01bbecf9e5484ef35cc5d4a5e","venue_1":"WOCCI","year":"2009","title":"Pilot experiments on children's voice recording","authors":"Sawit Kasuriya, Alistair D. N. Edwards","author_ids":"2684784, 1684360","abstract":"Automatic speech recognition is being used increasingly in a variety of applications. There is great potential for its use in educational applications for children. However, the accuracy of recognition of child speech is very low. There are probably a number of reasons for this, but one is the difficulty in collecting high-quality recordings of children to be used in the building of speech models. If a better interface can be provided between the child and the recording equipment then it may be possible to collect better samples. Interfaces have been designed to be tested to that end, using alternative interface paradigms: push-to-talk and a limited time recording with and without a progress bar. These alternatives will be compared by collecting speech samples and measuring their quality.","cites":"0","conferencePercentile":"25"},{"venue":"WOCCI","id":"c6fad307ff723f51794f165f2911b9accf68f323","venue_1":"WOCCI","year":"2014","title":"Automatic assessment of language background in toddlers through phonotactic and pitch pattern modeling of short vocalizations","authors":"Hynek Boril, Qian Zhang, Ali Ziaei, John H. L. Hansen, Dongxin Xu, Jill Gilkerson, Jeffrey A. Richards, Yiwen Zhang, Xiaojuan Xu, Hongmei Mao, Lei Xiao, Fan Jiang","author_ids":"3319398, 1737486, 2531460, 1741834, 2314900, 3179839, 2358470, 2732894, 4684514, 3384555, 1695447, 4674693","abstract":"This study utilizes phonotactic and pitch pattern model-ing for automatic assessment of toddlers' language background from short vocalization segments. The experiments are conducted on audio recordings of twelve 25–31 months old US-born and Shanghainese toddlers. Each recording captures a whole-day sound track of an ordinary day in the toddlers' life spent in their natural environment. In a preliminary study, we observed that in spite of the limited presence of linguistic content in the early age child vocalizations, certain phonotactic and prosodic patterns were correlated with the child's language background. In the current effort, we analyze to what extent these language-salient cues can be leveraged in the context of automatic language background classification. Besides a traditional parallel phone recognition with statistical language mod-eling (PPRLM) and phone recognition with support vector machines (PRSVM), a novel scheme that utilizes pitch patterns (PPSVM) is proposed. The classification results on very short vocalizations (on average less than 3 seconds long) confirm that both phonotactic and prosodic features capture a language-specific content, reaching equal error rates (EER) of 32.45 % for PRSVM, 31.33 % for PPSVM, and 29.97 % in a fusion of PRSVM and PPSVM systems. The competitive performance of PPSVM suggests that pitch contours carry a significant portion of the language-specific information in toddlers' vocalizations.","cites":"3","conferencePercentile":"83.33333333"},{"venue":"WOCCI","id":"7d9d6c51aba6206f9d17d805f1d2195ab7861b61","venue_1":"WOCCI","year":"2014","title":"Correlating ASR errors with developmental changes in speech production: a study of 3-10-year-old European Portuguese children's speech","authors":"Annika Hämäläinen, Sara Candeias, Hyongsil Cho, Hugo Meinedo, Alberto Abad, Thomas Pellegrini, Michael Tjalve, Isabel Trancoso, José Miguel Salles Dias","author_ids":"1732432, 3136797, 2944047, 1748419, 1718361, 2937969, 3129675, 1691021, 2402985","abstract":"Automatically recognising children's speech is a very difficult task. This difficulty can be attributed to the high variability in children's speech, both within and across speakers. The variability is due to developmental changes in children's anatomy, speech production skills et cetera, and manifests itself, for example, in fundamental and formant frequencies, the frequency of disfluencies, and pronunciation quality. In this paper, we report the results of acoustic and auditory analyses of 3-10-year-old European Portuguese children's speech. Furthermore, we are able to correlate some of the pronunciation error patterns revealed by our analyses – such as the truncation of consonant clusters – with the errors made by a children's speech recogniser trained on speech collected from the same age group. Other pronunciation error patterns seem to have little or no impact on speech recognition performance. In future work, we will attempt to use our findings to improve the performance of our recogniser.","cites":"0","conferencePercentile":"25"},{"venue":"WOCCI","id":"3a0e79bb612f9ef055142cd2bd22002ab301237b","venue_1":"WOCCI","year":"2009","title":"A reference verification framework and its application to a children's speech reading tracker","authors":"Daniel Bolaños, Wayne H. Ward, Ronald A. Cole","author_ids":"2342842, 1866226, 1705868","abstract":"In this article we present a novel approach to reference verification, the problem of determining if a speakers utterance matches a specific reference (text) string. We will then discuss its application to a reading tracker system for childrens speech.\n  Unlike other reading tracker systems proposed in the literature, that are built over conventional speech recognizers with ad-hoc language models, the reading tracker described here is designed specifically for the task of estimating whether a child has read an expected sequence of words out loud. The tracker is designed to handle in a natural and flexible way the disfluencies that frequently appear in childrens speech while reading out loud, (e.g., partial-words, repetitions, self-corrections, sentence-restarts, etc), and to overcome problems caused by using language models within the reference verification task. Three mechanisms have been introduced for this purpose: the utilization of filler models and the inclusion of forward and backward inter-word transitions in the static decoding network.\n  While this article focuses on the approach used to overcome errors observed in previous systems, the performance of this system will be evaluated on a corpus of childrens speech while reading out loud and compared to the performance of a traditional reading tracker system that is built on top of a speech recognition system. The results of this comparison will be presented at WOCCI 2009.","cites":"1","conferencePercentile":"59.09090909"},{"venue":"WOCCI","id":"27c64986bdde56025dfa841f1e77f96114a6ffea","venue_1":"WOCCI","year":"2012","title":"Automatic assessment of oral reading fluency for Spanish speaking ELs","authors":"Daniel Bolaños, Patricia Elhazaz Walsh, Wayne H. Ward, Ronald A. Cole","author_ids":"2342842, 1896075, 1866226, 1705868","abstract":"This article presents an approach to the automatic assessment of the oral reading fluency (ORF) of children in Spain who are learning to read English. We compared different acoustic modeling configurations and adaptation methods to determine the most accurate means of estimating reliable children's oral reading fluency scores using the standard metric of words correct per minute (WCPM). We addressed the problem of identifying word errors by extracting a series of features in order to learn how the human experts are actually annotating individual words. Experimental results show that the difference between WCPM scores produced by the proposed system and two human judges on the same text is smaller than the average difference between the scores produced by the two judges. In addition , the system scored individual words in texts as correctly or incorrectly read with an accuracy similar to that of human annotators.","cites":"0","conferencePercentile":"21.42857143"},{"venue":"WOCCI","id":"447c582ae823046132fc0dff0ecc8de33757a4be","venue_1":"WOCCI","year":"2009","title":"Whole body interaction for child-centered multimodal language learning","authors":"Berto Gonzalez, John Borland, Kathleen Geraghty","author_ids":"2018227, 2606516, 3247943","abstract":"Children engage with the world with their whole bodies, and we suggest here that during dialect learning, as during other learning activities, technology be capable of responding in whole body ways. As the child becomes more engaged in a shared-reality environment, the coordination of the whole-body behaviors between the VP and child should increase, thereby enhancing the experience. In this paper, we present our work on developing a virtual agent that embodies whole-body behaviors and a shared-reality environment that encourages children to use whole-body expression in the context of learning dialect, and science talk.","cites":"0","conferencePercentile":"25"},{"venue":"WOCCI","id":"31ac384dca77e441872177eb903e7acaa0b2544f","venue_1":"WOCCI","year":"2009","title":"Interacting with stories","authors":"Ashley Robinson, Chao Peng, Francis K. H. Quek, Yong Cao","author_ids":"3357377, 2978968, 1740663, 4610410","abstract":"In todays media-saturated world, students are consuming media both actively and passively. To facilitate active interaction with media, we address a specific kind of audiovisual media interaction in which we call a hyper-drama. We address hyper-drama interaction preferences across two age groups: grades one to five and grades 6 to 9. These hyper-drama interactions include a token on a horizontal display versus mouse on a desktop display for story navigation, desktop display versus tablet display for scene viewing, and virtual buttons versus speech for character interaction and decision making within the hyper-drama. We conducted a within-subjects pilot study to evaluate these interaction techniques.","cites":"1","conferencePercentile":"59.09090909"},{"venue":"WOCCI","id":"48a7d02f9d92fd0ab80b2dbade00eac2dda5db4e","venue_1":"WOCCI","year":"2012","title":"Investigating the influence of virtual peers as dialect models on students' prosodic inventory","authors":"Samantha L. Finkelstein, Stefan Scherer, Amy Ogan, Louis-Philippe Morency, Justine Cassell","author_ids":"1703122, 1770312, 8679311, 1767184, 1741405","abstract":"Children who speak non-standard dialects of English show reduced performance not just in language-oriented topics in school but also in math and science. Technological solutions have been rare exactly because of the non-mainstream nature of their talk, and hence the difficulty in automatically recognizing their speech and responding to it with, for example, computer tutors. In order to work towards overcoming this achievement gap, in this work we investigate African American students' prosodic inventories in different contexts as a first-step towards building a system that will be able to automatically recognize, and respond to, the dialect in which a child is speaking. We presented children with recordings of a peer (confederate) speaking in either African American English (AAE) or Mainstream American English (MAE) during both a social task and a science task. We found that children showed decreased prosodic variation and peak slopes during speech segments which did not contain AAE features, resulting in more monotone and breathy utterances than when they are speaking in AAE. We also found that children who were speaking with a \" peer \" who uses AAE have increased articulation rates, energy, and pitch variation. We discuss potential interpretations of these results that are important to the design of a system to support linguistic diversity and decrease the achievement gap.","cites":"9","conferencePercentile":"92.85714286"},{"venue":"WOCCI","id":"968918db695d8c17cfffc699e91036c65ac3ddcb","venue_1":"WOCCI","year":"2008","title":"HMM-based synthesis of child speech","authors":"Oliver Watts, Junichi Yamagishi, Kay M. Berkling, Simon King","author_ids":"1720858, 1716857, 1913461, 1721016","abstract":"The synthesis of child speech presents challenges both in the collection of data and in the building of a synthesiser from that data. Because only limited data can be collected, and the domain of that data is constrained, it is difficult to obtain the type of phonetically-balanced corpus usually used in speech synthesis. As a consequence, building a synthe-siser from this data is difficult. Concatenative synthesisers are not robust to corpora with many missing units (as is likely when the corpus content is not carefully designed), so we chose to build a statistical parametric synthesiser using the HMM-based system HTS. This technique has previously been shown to perform well for limited amounts of data, and for data collected under imperfect conditions. We compared 6 different configurations of the synthesiser, using both speaker-dependent and speaker-adaptive modelling techniques, and using varying amounts of data. The output from these systems was evaluated alongside natural and vocoded speech, in a Blizzard-style listening test.","cites":"6","conferencePercentile":"72.72727273"},{"venue":"WOCCI","id":"0a24a54af1c36b444b7ec22d3d44169d17c9876b","venue_1":"WOCCI","year":"2012","title":"Acoustical analysis of engagement behavior in children","authors":"Rahul Gupta, Chi-Chun Lee, Daniel Bone, Agata Rozga, Sungbok Lee, Shrikanth S. Narayanan","author_ids":"4414000, 2467369, 1884086, 7622486, 1797399, 8722462","abstract":"In this work we analyze the expressive manifestation of a child's engagement behavior on his speech as well as in the speech of psychologist interacting with the child. Visual cues such as facial gestures and gaze are known to be informative of engagement, but here, we examine the less studied speech cues of the children's non-verbal vocalizations. We study the spectral, prosodic and duration features obtained from the child and the psychologist's vocal data. We observe that these measures carry discriminative power in assessing specific engagement levels of the children (49.2% accuracy in classifying 3 levels of engagement compared to 33% chance accuracy). We also present our results as a detection task for disengagement with precision, recall and f-measure of .70, .42, .53, respectively. The unweighted accuracy for binary classification between engagement and disengagement is 62.9%. Our results suggest that vocal cues bear useful information in capturing the state of engagement in speech, indicating that speech can play an effective role in engagement assessment.","cites":"5","conferencePercentile":"78.57142857"}]}