{"WORKSSC.csv":[{"venue":"WORKS@SC","id":"0f4a6f83b22b9ebe05073888c76354d1bb593edc","venue_1":"WORKS@SC","year":"2011","title":"Making data analysis expertise broadly accessible through workflows","authors":"Matheus Hauder, Yolanda Gil, Ricky J. Sethi, Yan Liu, Hyunjoon Jo","author_ids":"2357910, 1688750, 2581042, 1681842, 2970865","abstract":"The demand for advanced skills in data analysis spans many areas of science, computing, and business analytics. This paper discusses how non-expert users reuse workflows created by experts and representing complex data mining processes for text analytics. They include workflows for document classification, document clustering, and topic detection, all assembled from components available in well-known text analytics software libraries. The workflows expose to non-experts expert-level knowledge on how these individual components need to be combined with data preparation and feature selection steps to make the underlying statistical learning algorithms most effective. The framework allows non-experts to easily experiment with different combinations of data analysis processes, represented as workflows of computations that they can easily reconfigure. We report on our experiences to date on having users with limited data analytic knowledge and even basic programming skills to apply workflows to their data.","cites":"10","conferencePercentile":"80"},{"venue":"WORKS@SC","id":"bb58cdf21c412b26aed92940b39988dea40a0982","venue_1":"WORKS@SC","year":"2014","title":"Combining workflow templates with a shared space-based execution model","authors":"Javier Rojas Balderrama, Matthieu Simonin, Lavanya Ramakrishnan, Valerie Hendrix, Christine Morin, Deborah A. Agarwal, CÃ©dric Tedeschi","author_ids":"1795118, 2122228, 1792683, 3035301, 1760074, 1962078, 3275703","abstract":"The growth for scientific data has led to data analysis being a critical step in the scientific process. The next-generation scientific data analysis environment needs to address two challenges i) productivity of the end-user and ii) scalability of the workflows. The need to ensure both goals requires us to revisit the design and implementation of workflow tools. In this paper, we study the interaction of Tigres and HOCL-TS towards meeting these goals. Tigres and HOCL-TS have evolved separately; however their complementary foci allows us to study these issues in greater detail. We describe the pros and cons of an approach that integrates Tigres and HOCL-TS and HOCL-TS extension to support common non-functional requirements such as logging and monitoring that can be made available to the users through the Tigres API.","cites":"0","conferencePercentile":"20"},{"venue":"WORKS@SC","id":"9cc298fc6b3c328f057ea0101860751c02484e2a","venue_1":"WORKS@SC","year":"2015","title":"Interlanguage parallel scripting for distributed-memory scientific computing","authors":"Justin M. Wozniak, Timothy G. Armstrong, Ketan Maheshwari, Daniel S. Katz, Michael Wilde, Ian T. Foster","author_ids":"1687044, 2618428, 7244165, 1687854, 7845728, 1698701","abstract":"Scripting languages such as Python and R have been widely adopted as tools for the development of scientific software because of the expressiveness of the languages and their available libraries. However, deploying scripted applications on large-scale parallel computer systems such as the IBM Blue Gene/Q or Cray XE6 is a challenge because of issues including operating system limitations, interoperability challenges, and parallel filesystem overheads due to the small file system accesses common in scripted approaches. We present a new approach to these problems in which the Swift scripting system is used to integrate high-level scripts written in Python, R, and Tcl with native code developed in C, C++, and Fortran, by linking Swift to the library interfaces to the script interpreters. We present a technique to efficiently launch scripted applications on supercomputers, and we demonstrate high performance, such as invoking 14M Python interpreters per second on <i>Blue Waters</i>.","cites":"1","conferencePercentile":"83.33333333"},{"venue":"WORKS@SC","id":"bfc7a2b7294176faa90660586499f06ceb9a1cbc","venue_1":"WORKS@SC","year":"2013","title":"Distributed tools deployment and management for multiple galaxy instances in globus genomics","authors":"Dinanath Sulakhe, Alex Rodriguez, Nick Prozorovsky, Nilesh Kavthekar, Ravi K. Madduri, Amol Parikh, Paul Dave, Lukasz Lacinski, Ian T. Foster","author_ids":"1802055, 3524187, 2828629, 1974418, 1860802, 2980387, 2005582, 3035939, 1698701","abstract":"Workflow systems play an important role in the analysis of the fast-growing genomics data produced by low-cost next generation sequencing (NGS) technologies. Many biomedical research groups lack the expertise to assemble and run the sophisticated computational pipelines required for high-throughput analysis of such data. There is an urgent need for services that can allow researchers to run their analytical workflows where they can define their own research methodologies by selecting the tools of their interest. We present the challenges associated with managing multiple Galaxy instances on the cloud for various research groups using Globus Genomics, a cloud based platform-as-a-service (PaaS) that provides the Galaxy workflow system as a hosted service along with data management capabilities using Globus Online. We address the unique challenges, our strategy, and a tool for automatically deploying and managing hundreds of analytical tools coming from the public Galaxy Tool Shed, new tools wrapped by our group, and tools wrapped by end users across multiple Galaxy instances hosted with Globus Genomics.","cites":"1","conferencePercentile":"46.15384615"},{"venue":"WORKS@SC","id":"b739e40733e972c984fab52e3c9ba259d857afc3","venue_1":"WORKS@SC","year":"2013","title":"Semantics and provenance for processing element composition in dispel workflows","authors":"Eric Griffis, Paul Martin, James Cheney","author_ids":"2646041, 2531053, 8067393","abstract":"Dispel is a scripting language for constructing workflow graph which can then be executed by some other computational infrastructure. It facilitates construction of abstract components (called Processing Elements, or PEs) that can be instantiated in different ways to produce a concrete, executable workflow. In this paper, we present a formal semantics for Dispel that explains its key features, particularly definition and use of composite PEs. We also develop an alternative semantics of Dispel programs that constructs a workflow enriched with PEs that can record provenance for the original workflow. The semantics is work in progress that will inform future development of Dispel and of provenance management techniques for Dispel.","cites":"1","conferencePercentile":"46.15384615"},{"venue":"WORKS@SC","id":"4f670ec1648fb914ef8db3de867aed0af353c29f","venue_1":"WORKS@SC","year":"2011","title":"AME: an anyscale many-task computing engine","authors":"Zhao Zhang, Daniel S. Katz, Matei Ripeanu, Michael Wilde, Ian T. Foster","author_ids":"1709665, 1687854, 1747805, 7845728, 1698701","abstract":"Many-Task Computing (MTC) is a new application category that encompasses increasingly popular applications in biology, economics, and statistics. The high inter-task parallelism and data-intensive processing capabilities of these applications pose new challenges to existing supercomputer hardware-software stacks. These challenges include resource provisioning; task dispatching, dependency resolution, and load balancing; data management; and resilience.\n This paper examines the characteristics of MTC applications which create these challenges, and identifies related gaps in the middleware that supports these applications on extreme-scale systems. Based on this analysis, we propose AME, an Anyscale MTC Engine, which addresses the scalability aspects of these gaps. We describe the AME framework and present performance results for both synthetic benchmarks and real applications. Our results show that AME's dispatching performance linearly scales up to 14,120 tasks/second on 16,384 cores with high efficiency. The overhead of the intermediate data management scheme does not increase significantly up to 16,384 cores. AME eliminates 73% of the file transfer between compute nodes and the global filesystem for the Montage astronomy application running on 2,048 cores. Our results indicate that AME scales well on today's petascale machines, and is a strong candidate for exascale machines.","cites":"9","conferencePercentile":"66.66666667"}]}