{"WPES.csv":[{"venue":"WPES","id":"4a1e9acec6f3ea23314c7aeffcd917d1d3329bdf","venue_1":"WPES","year":"2013","title":"Inferring social ties in academic networks using short-range wireless communications","authors":"Igor Bilogrevic, Kévin Huguenin, Murtuza Jadliwala, Florent Lopez, Jean-Pierre Hubaux, Philip Ginzboorg, Valtteri Niemi","author_ids":"1934140, 1716108, 2235825, 2068574, 1757221, 3194088, 1679115","abstract":"WiFi base stations are increasingly deployed in both public spaces and private companies, and the increase in their density poses a significant threat to the privacy of connected users. Prior studies have provided evidence that it is possible to infer the social ties of users from their location and co-location traces but they lack one important component: the comparison of the inference accuracy between an internal attacker (e.g., a curious application running on a mobile device) and a realistic external eavesdropper in the same field trial. In this paper, we experimentally show that such an eavesdropper is able to infer the type of social relationships between mobile users better than an internal attacker. Moreover, our results indicate that by exploiting the underlying social community structure of mobile users, the accuracy of the inference attacks doubles. Based on our findings, we propose countermeasures to help users protect their privacy against eavesdroppers.","cites":"5","conferencePercentile":"65.51724138"},{"venue":"WPES","id":"1f678160875fdf1f6e487fa39df20a9d003f8fa8","venue_1":"WPES","year":"2005","title":"Privacy for RFID through trusted computing","authors":"David Molnar, Andrea Soppera, David Wagner","author_ids":"2804783, 2212526, 4073051","abstract":"Radio Frequency Identification (RFID) technology raises significant privacy issues because it enables tracking of items and people possibly without their knowledge or consent. One of the biggest challenges for RFID technology is to provide privacy protection without raising tag production and management cost. We introduce a new architecture that uses trusted computing primitives to solve this problem. Our design splits the RFID reader into three software modules: a Reader Core with basic functionality, a Policy Engine that controls the use of RFID-derived data, and a Consumer Agent that performs privacy audits on the RFID reader and exports audit results to third party auditors. Readers use <i>remote attestation</i> to prove they are running a specific Reader Core, Policy Engine, and Consumer Agent. As a result, remote attestation allows concerned individuals to verify that RFID readers comply with privacy regulations, while also allowing the reader owner to verify that the reader has not been compromised.Furthermore, industry standards bodies have suggested several mechanisms to protect privacy in which authorized readers use a shared secret to authenticate themselves to the tag. These standards have not fully addressed issues of key management. First, how is the shared secret securely provided to the legitimate reader? Second, how do we guarantee that the reader will comply with a specific privacy policy? We show how, with remote attestation, the key-issuing authority can demand such a proof before releasing shared secrets to the reader. We also show how <i>sealed storage</i> can protect secrets even if the reader is compromised. Finally, we sketch how our design could be implemented today using existing RFID reader hardware.","cites":"28","conferencePercentile":"78.57142857"},{"venue":"WPES","id":"059f97b6ed741bfb005a347922b687d26d9abd53","venue_1":"WPES","year":"2002","title":"Use of a P3P user agent by early adopters","authors":"Lorrie Faith Cranor, Manjula Arjula, Praveen Guduru","author_ids":"1699751, 1858247, 2948647","abstract":"The Platform for Privacy Preferences (P3P), developed by the World Wide Web Consortium (W3C), provides a standard computer-readable format for privacy policies and a protocol that enables web browsers to read and process privacy policies automatically. P3P enables machine-readable privacy policies that can be retrieved automatically by web browsers and other user agent tools that can display symbols, prompt users, or take other appropriate actions. We developed the AT&amp;T Privacy Bird as a P3P user agent that can compare P3P policies against a user's privacy preferences. Since P3P was adopted as a W3C recommendation in April 2002, little work has been done to study how it is being used and, especially, its impact on users. Many questions have been raised about whether and how Internet users will make use of P3P, and how to build P3P user agents that will prove most useful to end users. In this paper we first provide a brief introduction to P3P and the AT&T Privacy Bird. Then we discuss a survey of AT&T Privacy Bird users that we conducted in August 2002. We found that a large proportion of AT&T Privacy Bird users began reading privacy policies more often and being more proactive about protecting their privacy as a result of using this software. Unfortunately, the usefulness of P3P user agents is severely limited by the number of web sites that have implemented P3P. Our survey results also suggest that if it becomes easier to compare privacy policy across e-commerce web sites, a significant group of consumers would likely use this information in their purchase decisions.","cites":"40","conferencePercentile":"70"},{"venue":"WPES","id":"264b337811e4add2b8f6c77fe5c09312ef8a9776","venue_1":"WPES","year":"2011","title":"Loose tweets: an analysis of privacy leaks on twitter","authors":"Huina Mao, Xin Shuai, Apu Kapadia","author_ids":"2389224, 1736363, 3163956","abstract":"Twitter has become one of the most popular microblogging sites for people to broadcast (or \"tweet\") their thoughts to the world in 140 characters or less. Since these messages are available for public consumption, one may expect these tweets not to contain private or incriminating information. Nevertheless we observe a large number of users who unwittingly post sensitive information about <i>themselves and other people</i> for whom there may be negative consequences. While some awareness exists of such privacy issues on social networks such as Twitter and Facebook, there has been no quantitative, scientific study addressing this problem.\n In this paper we make three major contributions. First, we characterize the nature of privacy leaks on Twitter to gain an understanding of what types of private information people are revealing on it. We specifically analyze three types of leaks: divulging vacation plans, tweeting under the influence of alcohol, and revealing medical conditions. Second, using this characterization we build automatic classifiers to detect incriminating tweets for these three topics in real time in order to demonstrate the real threat posed to users by, e.g., burglars and law enforcement. Third, we characterize who leaks information and how. We study both self- incriminating primary leaks and secondary leaks that reveal sensitive information about others, as well as the prevalence of leaks in status updates and conversation tweets. We also conduct a cross-cultural study to investigate the prevalence of leaks in tweets originating from the United States, United Kingdom and Singapore. Finally, we discuss how our classification system can be used as a defense mechanism to alert users of potential privacy leaks.","cites":"36","conferencePercentile":"90"},{"venue":"WPES","id":"62f280e00f2cfdc846d43b1c82a238c7c524c2f3","venue_1":"WPES","year":"2011","title":"Pythia: a privacy aware, peer-to-peer network for social search","authors":"Shirin Nilizadeh, Naveed Alam, Nathaniel Husted, Apu Kapadia","author_ids":"2267110, 7254297, 2464101, 3163956","abstract":"Emerging \"live social search\" systems such as Aardvark.com allow users to pose questions to their social network in real time. People can thus obtain answers from real humans for questions that prove too complex for web searches. Centralized systems that broker such queries and answers, however, do not provide adequate privacy. The success of these systems will be limited since users may avoid asking or answering questions related to sensitive topics such as health, political activism, or even innocuous questions which may make the querier seem ignorant.\n Since social search systems leverage the structure of the social network to better match askers and answerers, standard ideas that hide this structure such as \"connect to Aardvark via Tor\" fall short. Thus new techniques are needed to preserve the privacy of askers and answerers beyond the currently understood anonymity techniques. We explore the new and unique challenges for privacy, and propose Pythia, a decentralized architecture based on \"controlled flooding\" to enable privacy-enhanced social search that retains some degree of social network structure.","cites":"7","conferencePercentile":"67.5"},{"venue":"WPES","id":"0f5f6322e8eb3bdc8e45de6d78727b98c8e7726d","venue_1":"WPES","year":"2004","title":"Will your digital butlers betray you?","authors":"Frank Stajano","author_ids":"2389090","abstract":"The cost of data storage is now so low that there is little necessity ever to delete anything. The consequence is &#60;i>denied oblivion&#60;/i>---digital systems that remember forever and can be data-mined retroactively, years after the event, ignoring any privacy promise under which the original data may have been acquired.\n Even for systems under your own control, though, the situation is alarming. As your capacious digital butlers faithfully collect as much data as possible about you, your private information is increasingly likely to become compromised. New solutions are needed. But technical countermeasures alone are not the whole story.","cites":"11","conferencePercentile":"40.47619048"},{"venue":"WPES","id":"09367be7c58a4009840f6f291f0b12e829e10376","venue_1":"WPES","year":"2004","title":"Privacy management for portable recording devices","authors":"J. Alex Halderman, Brent Waters, Edward W. Felten","author_ids":"2349976, 1715120, 1752733","abstract":"The growing popularity of inexpensive, portable recording devices, such as cellular phone cameras and compact digital audio recorders, presents a significant new threat to privacy. We propose a set of technologies that can be integrated into recording devices to provide stronger, more accurately targeted privacy protections than other legal and technical measures now under consideration. Our design is based on an informed consent principle, which it supports by the use of novel devices and protocols that automate negotiations over consent and ensure appropriate safeguards on recorded data. We define the protocols needed for this purpose and establish their security. We also describe a working prototype implementation that safeguards audio recorded by laptop PCs in a wireless network.","cites":"19","conferencePercentile":"52.38095238"},{"venue":"WPES","id":"a76cc081efc3ea3c35de69694a412ec87ddd62dc","venue_1":"WPES","year":"2003","title":"Hidden Credentials","authors":"Jason E. Holt, Robert W. Bradshaw, Kent E. Seamons, Hilarie K. Orman","author_ids":"3113648, 3067473, 1774940, 2620860","abstract":"Hidden Credentials are useful in situations where requests for service, credentials, access policies and resources are extremely sensitive. We show how transactions which depend on fulfillment of policies described by monotonic boolean formulae can take place in a single round of messages. We further show how credentials that are never revealed can be used to retrieve sensitive resources.","cites":"88","conferencePercentile":"100"},{"venue":"WPES","id":"22ef4514bcdf001fd7b9edd8c517922b79ac34d6","venue_1":"WPES","year":"2009","title":"Faking contextual data for fun, profit, and privacy","authors":"Richard Chow, Philippe Golle","author_ids":"2227735, 2779068","abstract":"The amount of contextual data collected, stored, mined, and shared is increasing exponentially. Street cameras, credit card transactions, chat and Twitter logs, e-mail, web site visits, phone logs and recordings, social networking sites, all are examples of data that persists in a manner not under individual control, leading some to declare the death of privacy. We argue here that the ability to generate convincing fake contextual data can be a basic tool in the fight to preserve privacy. One use for the technology is for an individual to make his actual data indistinguishable amongst a pile of false data.\n In this paper we consider two examples of contextual data, search engine query data and location data. We describe the current state of faking these types of data and our own efforts in this direction.","cites":"33","conferencePercentile":"92.85714286"},{"venue":"WPES","id":"03a954adf8bdb1516b4dff037676fad4981fe539","venue_1":"WPES","year":"2012","title":"Genodroid: are privacy-preserving genomic tests ready for prime time?","authors":"Emiliano De Cristofaro, Sky Faber, Paolo Gasti, Gene Tsudik","author_ids":"1728207, 2596635, 1710346, 1702391","abstract":"As fast and accurate sequencing of human genomes becomes affordable, it is expected that individuals will soon be able to carry around copies of their sequenced DNA, using it for medical, identification, and social purposes. This will undoubtedly prompt a wide range of new and interesting genomic applications. However, the very same progress raises some worrisome privacy issues, since a genome represents a treasure trove of highly personal and sensitive information. Some recent research explored privacy-preserving personal genomic operations by applying (or customizing) cryptographic protocols based on techniques such as: conditional oblivious transfer, garbled circuits, and homomorphic encryption. In this paper, we take this line of work a step further by investigating real-world practicality and usability of (as well as interest in) some of these methods. Motivated by both medical and social applications, we aim to test viability of privacy-agile computational genomic tests in a portable and pervasive setting of modern smartphones. We design a personal genomic toolkit (called GenoDroid), implement it on the Android platform, assess its performance, and conduct a pilot usability study that yields some interesting results.","cites":"19","conferencePercentile":"85.71428571"},{"venue":"WPES","id":"144e98cfe5c6c04f828a94334a1fe2a3822e37e6","venue_1":"WPES","year":"2013","title":"Secure genomic testing with size- and position-hiding private substring matching","authors":"Emiliano De Cristofaro, Sky Faber, Gene Tsudik","author_ids":"1728207, 2596635, 1702391","abstract":"Recent progress in genomics and bioinformatics is bringing complete and on-demand sequencing of human (and other) genomes closer and closer to reality. Despite exciting new opportunities, affordable and ubiquitous genome sequencing prompts some serious privacy and ethical concerns, owing to extreme sensitivity and uniqueness of genomic information. At the same time, new medical applications, such as personalized medicine, require testing genomes for specific markers that themselves represent sensitive (e.g., proprietary) material. This paper focuses on privacy challenges posed by such genetic tests. It presents a secure and efficient protocol called: Size- and Position-Hiding Private Substring Match- ing (SPH-PSM). This protocol allows two parties -- one with a digitized genome and the other with a set of DNA markers -- to conduct a test, such that the result is only learned by the former, and no other information is learned by either party. In particular, the genome owner does not even learn the size or the position of the markers, which makes SPH-PSM the first of its kind. Finally, we report on a prototype of the proposed technique which attests to its practicality.","cites":"12","conferencePercentile":"84.48275862"},{"venue":"WPES","id":"bf8519788c91b9738ba358aba5db071c4e2e397f","venue_1":"WPES","year":"2004","title":"Radio frequency Id and privacy with information goods","authors":"Nathaniel Good, David Molnar, Jennifer M. Urban, Deirdre K. Mulligan, Elizabeth Miles, Laura Quilter, David Wagner","author_ids":"6201430, 2804783, 2167049, 2858396, 8336673, 2510100, 4073051","abstract":"This paper examines the privacy impacts of using radio frequency identification (RFID) to tag information goods such as books, music, and video. Individuals have strong expectations of privacy in their choice of information goods. These expectations are supported by both social norms and law. As a matter of practice, people may generally purchase and browse information goods without identifying themselves or the subject of their inquiry. People may pay in cash and avoid creating records that provide opportunities for third parties to learn of their information habits. Information providers that maintain records, such as libraries and bookstores, have staunchly defended their patrons' privacy, and indeed are often bound legally to demand due process of law before disclosing those records. Data holders can examine subpoenas for authenticity and cause, and challenge them in court before disclosing private information. Bookstores have done so in recent high-profile cases. [6][9] Libraries have developed elaborate policy mechanisms to ensure records are kept private, [1] and lobbied for laws protecting library records. U.S. law has also been protective of individuals' rights of privacy and free inquiry, grounding those protections in the First and Fourth Amendments of the Constitution. [4][5] Supplementing these constitutional protections, Congress and state legislatures have created a patchwork of industry-specific statutes that shield records of individual inquiry from disclosure to both public and private parties. For example, the Cable Television Privacy Act protects cable television subscribers from unfair data collection and use [3], and the Video Privacy Protection Act protects video rental records from release without a court order. [2] Laws in 48 states protect library records from release with without a court order. [1] These laws and business practices are generally based on Fair Information Practices, which mandate notice to consumers about data collection practices, the opportunity to discover and correct inaccurate records, and limitations on the use of data. [14] Using RFID to tag information goods creates new risks to personal privacy. Put simply, in the RFID-enabled world, anyone with an RFID reader can potentially discover individuals' informational preferences without their permission. When information goods can be \"interrogated\" over the radio, revealing the goods' identity or other information, neither the individual consumer nor the third-party record-holder, has the opportunity to prevent disclosure of the information on the RFID tag. All RFID operates through radio, which by its nature, anyone within range can receive. Current generation tags lack access control. Thus anyone, …","cites":"6","conferencePercentile":"19.04761905"},{"venue":"WPES","id":"53423571526db6c43c2665cc17bc7da24406cde2","venue_1":"WPES","year":"2008","title":"Forensic genomics: kin privacy, driftnets and other open questions","authors":"Frank Stajano, Lucia Bianchi, Pietro Liò, Douwe Korff","author_ids":"2389090, 7338545, 7506403, 2939406","abstract":"DNA analysis is increasingly used in forensics, where it is being pushed as the holy grail of identification. But we are approaching a dramatic \"phase change\" as we move from genetics to genomics: when sequencing the entire genome of a person becomes sufficiently cheap as to become a routine operation, as is likely to happen in the coming decades, then each DNA examination will expose a wealth of very sensitive personal information about the examined individual, as well as her relatives. In this interdisciplinary discussion paper we highlight the complexity of DNA-related privacy issues as we move into the genomic (as opposed to genetic) era: the \"driftnet\" approach of comparing scene-of-crime samples against the DNA of the whole population rather than just against that of chosen suspects; the potential for errors in forensic DNA analysis and the consequences on security and privacy; the civil liberties implications of the interaction between medical and forensic applications of genomics. For example, your kin can provide valuable information in a database matching procedure against you even if you don't; and being able to read the whole of a sampled genome, rather than just 13 specific markers from it, provides information about the medical and physical characteristics of the individual.\n Our aim is to offer a simple but thought-provoking and technically accurate summary of the many issues involved, hoping to stimulate an informed public debate on the statutes by which DNA collection, storage and processing should be regulated.","cites":"10","conferencePercentile":"46.42857143"},{"venue":"WPES","id":"af1c694dd2ad74a4673a09e1a86b3d67c6aa2bbb","venue_1":"WPES","year":"2010","title":"Token attempt: the misrepresentation of website privacy policies through the misuse of p3p compact policy tokens","authors":"Pedro Giovanni Leon, Lorrie Faith Cranor, Aleecia M. McDonald, Robert McGuire","author_ids":"2591254, 1699751, 2175102, 1924886","abstract":"P3P compact policies (CPs) are a collection of three-character and four-character tokens that summarize a website's privacy policy pertaining to cookies. User agents, including Microsoft's Internet Explorer (IE) web browser, use CPs to evaluate websites' data collection practices and allow, reject, or modify cookies based on sites' privacy practices. CPs can provide a technical means to enforce users' privacy preferences if CPs accurately reflect websites' practices. Confirming the accuracy of CPs would require first-hand knowledge of each site's practices. However, through automated analysis we can identify CPs that are erroneous due to syntax errors or semantic conflicts. We collected CPs from 33,139 websites and detected errors in 11,176 of them. We found large numbers of sites using identical invalid CPs that had been recommended as workarounds for IE cookie blocking. Other sites had CPs with typos in their tokens, or other errors. Most invalid CPs resulted in cookies remaining unblocked by IE under it's default cookie settings. It appears that large numbers of websites that use CPs are misrepresenting their privacy practices, thus misleading users and rendering privacy protection tools ineffective.","cites":"18","conferencePercentile":"71.42857143"},{"venue":"WPES","id":"08ff894edacf529d22f45f30885b424d61569fd7","venue_1":"WPES","year":"2003","title":"Covert channels and anonymizing networks","authors":"Ira S. Moskowitz, Richard E. Newman, Daniel P. Crepeau, Allen R. Miller","author_ids":"2771462, 3127433, 2985499, 1804817","abstract":"There have long been threads of investigation into covert channels, and threads of investigation into anonymity, but these two closely related areas of information hiding have not been directly associated. This paper represents an initial inquiry into the relationship between covert channel capacity and anonymity, and poses more questions than it answers. Even this preliminary work has proven difficult, but in this investigation lies the hope of a deeper understanding of the nature of both areas. MIXes have been used for anonymity, where the concern is shielding the identity of the sender or the receiver of a message, or both. In contrast to traffic analysis prevention methods which conceal larger traffic patterns, we are concerned with how much information a sender to a MIX can leak to an eavesdropping outsider, despite the concealment efforts of MIXes acting as firewalls.","cites":"58","conferencePercentile":"86.66666667"},{"venue":"WPES","id":"1697e9ec157639c4c0cab092c1ce99907672f6e3","venue_1":"WPES","year":"2004","title":"Off-the-record communication, or, why not to use PGP","authors":"Nikita Borisov, Ian Goldberg, Eric A. Brewer","author_ids":"1723454, 1786828, 1759010","abstract":"Quite often on the Internet, cryptography is used to protect private, personal communications. However, most commonly, systems such as PGP are used, which use long-lived encryption keys (subject to compromise) for confidentiality, and digital signatures (which provide strong, and in some jurisdictions, legal, proof of authorship) for authenticity.\n In this paper, we argue that most social communications online should have just the opposite of the above two properties; namely, they should have &#60;i>perfect forward secrecy&#60;/i> and &#60;i>repudiability&#60;/i>. We present a protocol for secure online communication, called \"off-the-record messaging\", which has properties better-suited for casual conversation than do systems like PGP or S/MIME. We also present an implementation of off-the-record messaging as a plugin to the Linux GAIM instant messaging client. Finally, we discuss how to achieve similar privacy for high-latency communications such as email.","cites":"111","conferencePercentile":"100"},{"venue":"WPES","id":"651f36d4396c060a48aa4aa3c3294b0754ac30f4","venue_1":"WPES","year":"2012","title":"Understanding sharing preferences and behavior for mHealth devices","authors":"Aarathi Prasad, Jacob Sorber, Timothy Stablein, Denise L. Anthony, David Kotz","author_ids":"2960411, 2790738, 2403606, 8441116, 1710979","abstract":"If people are not in control of the collection and sharing of their personal health information collected using mobile health (mHealth) devices and applications, privacy concerns could limit their willingness to use and reduce potential benefits provided via mHealth. We investigated users' willingness to share their personal information, collected using mHealth sensing devices, with their family, friends, third parties, and the public. Previous work employed hypothetical scenarios, surveys and interviews to understand people's information-sharing behavior; to the best of our knowledge, ours is the first privacy study where participants actually have the option to share their own information with real people. We expect our results can guide the development of privacy controls for mobile devices and applications that collect any personal and activity information, not restricted to health or fitness information.\n Our study revealed three interesting findings about people's privacy concerns regarding their sensed health information: 1) We found that people share certain health information less with friends and family than with strangers, but more with specific third parties than the public. 2) Information that people were less willing to share could be information that is indirectly collected by the mobile devices. 3) We confirmed that privacy concerns are not static; mHealth device users may change their sharing decisions over time. Based on our findings, we emphasize the need for sensible default settings and flexible privacy controls to allow people to choose different settings for different recipients, and to change their sharing settings at any time.","cites":"9","conferencePercentile":"60.71428571"},{"venue":"WPES","id":"52d97bf944f37d7b015300d3b35919e88ac31fad","venue_1":"WPES","year":"2004","title":"Private collaborative forecasting and benchmarking","authors":"Mikhail J. Atallah, Marina Blanton, Jiangtao Li, Keith B. Frikken, Mercan Topkara","author_ids":"1719237, 2111328, 1787570, 1965461, 7179616","abstract":"Suppose a number of hospitals in a geographic area want to learn how their own heart-surgery unit is doing compared with the others in terms of mortality rates, subsequent complications, or any other quality metric. Similarly, a number of small businesses might want to use their recent point-of-sales data to cooperatively forecast future demand and thus make more informed decisions about inventory, capacity, employment, etc. These are simple examples of cooperative benchmarking and (respectively) forecasting that would benefit all participants as well as the public at large, as they would make it possible for participants to avail themselves of more precise and reliable data collected from many sources, to assess their own local performance in comparison to global trends, and to avoid many of the inefficiencies that currently arise because of having less information available for their decision-making. And yet, in spite of all these advantages, cooperative benchmarking and forecasting typically do not take place, because of the participants' unwillingness to share their information with others. Their reluctance to share is quite rational, and is due to fears of embarrassment, lawsuits, weakening their negotiating position (e.g., in case of over-capacity), revealing corporate performance and strategies, etc. The development and deployment of &#60;i>private&#60;/i> benchmarking and forecasting technologies would allow such collaborations to take place without revealing any participant's data to the others, reaping the benefits of collaboration while avoiding the drawbacks. Moreover, this kind of technology would empower smaller organizations who could then cooperatively base their decisions on a much broader information base, in a way that is today restricted to only the largest corporations. This paper is a step towards this goal, as it gives protocols for forecasting and benchmarking that reveal to the participants the desired answers yet do not reveal to any participant any other participant's private data. We consider several forecasting methods, including linear regression and time series techniques such as moving average and exponential smoothing. One of the novel parts of this work, that further distinguishes it from previous work in secure multi-party computation, is that it involves floating point arithmetic, in particular it provides protocols to securely and efficiently perform division.","cites":"60","conferencePercentile":"90.47619048"},{"venue":"WPES","id":"a2810124871e46448ed0c9f5c4bf7c78235ed073","venue_1":"WPES","year":"2005","title":"Quantitative evaluation of unlinkable ID matching schemes","authors":"Yasunobu Nohara, Sozo Inoue, Kensuke Baba, Hiroto Yasuura","author_ids":"2577092, 3194073, 3015311, 2778058","abstract":"As pervasive computing environments become popular, RFID devices, such as contactless smart cards and RFID tags, are introduced into our daily life. However, there exists a privacy problem that a third party can trace user's behavior by linking device's ID.The concept of unlinkability, that a third party cannot recognize whether some outputs are from the same user, is important to solve the privacy problem. A scheme using hash function satisfies unlinkability against a third party by changing the outputs of RFID devices every time. However, the schemes are not scalable since the server needs <i>O</i>(<i>N</i>) hash calculations for every ID matching, where N is the number of RFID devices.In this paper, we propose the <i>K-steps ID matching scheme</i>, which can reduce the number of the hash calculations on the server to <i>O</i>(log <i>N</i>). Secondly, we propose a quantification of unlinkability using conditional entropy and mutual information. Finally, we analyze the K-steps ID matching scheme using the proposed quantification, and show the relation between the time complexity and unlinkability.","cites":"16","conferencePercentile":"46.42857143"},{"venue":"WPES","id":"076bd9264480b622b01d0aefd6729905a1188211","venue_1":"WPES","year":"2014","title":"Prolonging the Hide-and-Seek Game: Optimal Trajectory Privacy for Location-Based Services","authors":"George Theodorakopoulos, Reza Shokri, Carmela Troncoso, Jean-Pierre Hubaux, Jean-Yves Le Boudec","author_ids":"1786107, 2520493, 2130458, 1757221, 1737578","abstract":"Human mobility is highly predictable. Individuals tend to only visit a few locations with high frequency, and to move among them in a certain sequence reflecting their habits and daily routine. This predictability has to be taken into account in the design of location privacy preserving mechanisms (LPPMs) in order to effectively protect users when they expose their whereabouts to location-based services (LBSs) continuously. In this paper, we describe a method for creating LPPMs tailored to a user's mobility profile taking into her account privacy and quality of service requirements. By construction, our LPPMs take into account the sequential correlation across the user's exposed locations, providing the maximum possible trajectory privacy, i.e., privacy for the user's past, present location, and expected future locations. Moreover, our LPPMs are optimal against a strategic adversary, i.e., an attacker that implements the strongest inference attack knowing both the LPPM operation and the user's mobility profile.\n The optimality of the LPPMs in the context of trajectory privacy is a novel contribution, and it is achieved by formulating the LPPM design problem as a Bayesian Stackelberg game between the user and the adversary. An additional benefit of our formal approach is that the design parameters of the LPPM are chosen by the optimization algorithm.","cites":"9","conferencePercentile":"86.53846154"},{"venue":"WPES","id":"2b23677fdfecee24605ebc2b622b6b73206ec832","venue_1":"WPES","year":"2014","title":"Reconciling Utility with Privacy in Genomics","authors":"Mathias Humbert, Erman Ayday, Jean-Pierre Hubaux, Amalio Telenti","author_ids":"2230149, 2898372, 1757221, 4524349","abstract":"Direct-to-consumer genetic testing makes it possible for everyone to learn their genome sequences. In order to contribute to medical research, a growing number of people publish their genomic data on the Web, sometimes under their real identities. However, this is at odds not only with their own privacy but also with the privacy of their relatives. The genomes of relatives being highly correlated, some family members might be opposed to revealing any of the family's genomic data. In this paper, we study the trade-off between utility and privacy in genomics. We focus on the most relevant kind of variants, namely single nucleotide polymorphisms (SNPs). We take into account the fact that the SNPs of an individual contain information about the SNPs of his family members and that SNPs are correlated with each other. Furthermore, we assume that SNPs can have different utilities in medical research and different levels of sensitivity for individuals. We propose an obfuscation mechanism that enables the genomic data to be publicly available for research, while protecting the genomic privacy of the individuals in a family. Our genomic-privacy preserving mechanism relies upon combinatorial optimization and graphical models to optimize utility and meet privacy requirements. We also present an extension of the optimization algorithm to cope with the non-linear constraints induced by the correlations between SNPs. Our results on real data show that our proposed technique maximizes the utility for genomic research and satisfies family members' privacy constraints.","cites":"11","conferencePercentile":"96.15384615"},{"venue":"WPES","id":"5f9f9259c30753a8ca3c9a57d99171fcf5bcc3e7","venue_1":"WPES","year":"2004","title":"Conflict and combination in privacy policy languages","authors":"Adam Barth, John C. Mitchell, Justin Rosenstein","author_ids":"2908330, 1685743, 3221892","abstract":"Many modern enterprises require methods for guaranteeing compliance with privacy legislation and announced privacy policies. IBM has proposed a formal language, the Enterprise Privacy Authorization Language (EPAL), for describing privacy policies rigorously. In this paper, we identify four desirable properties of a privacy policy language: guaranteed consistency, guaranteed safety, admitting local reasoning, and closure under combination. While EPAL achieves only one of these four goals, an extended language framework allows us to achieve three out of four, while retaining the basic EPAL framework of restricting access and imposing obligations on users of confidential information.","cites":"25","conferencePercentile":"71.42857143"},{"venue":"WPES","id":"0e5fda6cb854a5815b506c8684a09e89c5591f24","venue_1":"WPES","year":"2003","title":"Reusable anonymous return channels","authors":"Philippe Golle, Markus Jakobsson","author_ids":"2779068, 2836467","abstract":"Mix networks are used to deliver messages anonymously to recipients, but do not straightforwardly allow the recipient of an anonymous message to reply to its sender. Yet the ability to reply one or more times, and to further reply to replies, is essential to a complete anonymous conversation. We propose a protocol that allows a sender of anonymous messages to establish a <i>reusable anonymous return channel</i>. This channel enables any recipient of one of these anonymous messages to send back one or more anonymous replies. Recipients who reply to different messages can not test whether two return channels are the same, and there-fore can not learn whether they are replying to the same person. Yet the fact that multiple recipients may send multiple replies through the <i>same</i> return channel helps defend against the counting attacks that defeated earlier proposals for return channels. In these attacks, an adversary traces the origin of a message by sending a specific number of replies and observing who collects the same number of messages. Our scheme resists these attacks because the replies sent by an attacker are mixed with other replies submitted by other recipients through the same return channel. Moreover, our protocol straightforwardly allows for replies to replies, etc. Our protocol is based upon a re-encryption mix network, and requires four times the amount of computation and communication of a basic mixnet.","cites":"17","conferencePercentile":"53.33333333"},{"venue":"WPES","id":"7cce6ce6a7e3dcd496346502bfd06b8ea457f170","venue_1":"WPES","year":"2004","title":"Defending email communication against profiling attacks","authors":"Philippe Golle, Ayman Farahat","author_ids":"2779068, 2097475","abstract":"We define message privacy against a &#60;i>profiling&#60;/i> adversary, whose goal is to classify a population of users into categories according to the messages they exchange. This adversary models the most common privacy threat against email communication. We propose a protocol that protects senders and receivers of email messages from profiling attacks.","cites":"5","conferencePercentile":"9.523809524"},{"venue":"WPES","id":"1e18a99ec4caccf9a0ba1526b04d5a2a12974f2d","venue_1":"WPES","year":"2006","title":"Privacy management for secure mobility","authors":"Janne Lindqvist, Laura Takkinen","author_ids":"1735867, 2454941","abstract":"Anonymous Internet access has been researched extensively and many proposals exist for enhancing the privacy of users. However, there are vast amounts of legacy authentication systems that do not take the privacy of the users into consideration. Many networks use, for example, MAC address or IP address based authentication, despite of their limited security properties. These authentication systems hinder the possibility to use e.g. pseurandom MAC addresses for privacy protection. In this paper, we propose a privacy management system for layers below the transport layer in the IP stack. Our implementation allows the users to decide their privacy parameters depending on their current situation. The implementation uses the Host Identity Protocol to provide authenticated and secure seamless handovers for mobile nodes. The approach is also applicable to an IP stack without the Host Identity Protocol.","cites":"11","conferencePercentile":"50"},{"venue":"WPES","id":"20f268ef309f882be1c51ef6ae5aadbe71d969b7","venue_1":"WPES","year":"2008","title":"Protecting privacy with protocol stack virtualization","authors":"Janne Lindqvist, Juha-Matti Tapio","author_ids":"1735867, 2534926","abstract":"Previously proposed host-based privacy protection mechanisms use pseudorandom or disposable identifiers on some or all layers of the protocol stack. These approaches either require changes to all hosts participating in the communication or do not provide privacy for the whole protocol stack or the system. Building on previous work, we propose a relatively simple approach: <i>protocol stack virtualization</i>. The key idea is to provide isolation for traffic sent to the network. The granularity of the isolation can be, for example, flow or process based. With process based granularity, every application uses a distinct identifier space on all layers of the protocol stack. This approach does not need any infrastructure support from the network and requires only minor changes to the single host that implements the privacy protection mechanism. To show that no changes to typical applications are required, we implemented the protocol stack virtualization as a user space daemon and tested it with various legacy applications.","cites":"5","conferencePercentile":"28.57142857"},{"venue":"WPES","id":"8cfecf258430c4b736e15ee3eed7e7955837583e","venue_1":"WPES","year":"2014","title":"Private Browsing: an Inquiry on Usability and Privacy Protection","authors":"Xianyi Gao, Yulong Yang, Huiqing Fu, Janne Lindqvist, Yang Wang","author_ids":"2871674, 7607863, 1687967, 1735867, 1747927","abstract":"Private browsing is a feature in web browsers to prevent local users from gaining information about browsing sessions. However, it is not clear how well people interpret private browsing's functionalities and what are the privacy gains from using it. Towards studying people's understanding of private browsing, we conducted a survey on Amazon Mechanical Turk. Our survey results show that (1) one third of our participants were not aware of this privacy-enhancing feature, and (2) for people who knew or even used this feature, they had various misconceptions which could put them at risk. In the end, we provide design suggestions to help address these misconceptions.","cites":"2","conferencePercentile":"38.46153846"},{"venue":"WPES","id":"3e768de55050260559e61576ce5a3182ebe63d75","venue_1":"WPES","year":"2006","title":"Revisiting the uniqueness of simple demographics in the US population","authors":"Philippe Golle","author_ids":"2779068","abstract":"According to a famous study [10] of the 1990 census data, 87% of the US population can be uniquely identified by gender, ZIP code and full date of birth. This short paper revisits the uniqueness of simple demographics in the US population based on the most recent census data (the 2000 census). We offer a detailed, comprehensive and up-to-date picture of the threat to privacy posed by the disclosure of simple demographic information. Our results generally agree with the findings of [10], although we find that disclosing one's gender, ZIP code and full date of birth allows for unique identification of fewer individuals (63% of the US population) than reported in [10]. We hope that our study will be a useful reference for privacy researchers who need simple estimates of the comparative threat of disclosing various demographic data.","cites":"73","conferencePercentile":"93.75"},{"venue":"WPES","id":"344aa9a1d88da7c02345114978215d33ec4fb2a4","venue_1":"WPES","year":"2008","title":"CPG: closed pseudonymous groups","authors":"Reed S. Abbott, Timothy W. van der Horst, Kent E. Seamons","author_ids":"2749483, 2882066, 1774940","abstract":"This paper presents the design and implementation of Closed Pseudonymous Groups (CPG), a pseudonymous communication system for a closed user community (e.g., a class of students, team of employees, residents of a neighborhood). In CPG, each legitimate user is known by a pseudonym that, while unlinkable to a true identity, enables service providers to link users' behavior and blacklist any abuser of the system. This system is useful for providing honest feedback without fear of reprisals (e.g., instructor/course ratings, employee comments, community feedback for local politics). CPG is designed to be easy to understand, to implement (using existing techniques), and to use. This paper also presents the results of an initial user study that resulted in an important design change.","cites":"4","conferencePercentile":"17.85714286"},{"venue":"WPES","id":"293c5b35668ad16cc255bb029cdc0f5eb95dcdca","venue_1":"WPES","year":"2014","title":"General Area or Approximate Location?: How People Understand Location Permissions","authors":"Huiqing Fu, Janne Lindqvist","author_ids":"1687967, 1735867","abstract":"More than half of American adults use smartphones and about two thirds of them use location-based services. On Android smartphones, these location-based services are implemented by apps. Android phones provide two location-related permissions: \"precise\" location and \"approximate\" location. In this paper, we present an online survey of 106 Android users to investigate how people understand location descriptions related to their apps. Our results suggest that most participants considered the \"precise\" location to mean their exact location and the \"approximate\" location as a general area. This mental model of the \"approximate\" location seems to allay people's privacy concerns related to their apps. However, after participants were shown the ground truth of how accurate \"approximate\" location actually is, twice as many participants no longer thought \"approximate\" location offered enough protection, compared to before showing the ground truth. Our results indicate that the location permissions might mislead smartphone users about the privacy protections the apps are providing.","cites":"4","conferencePercentile":"63.46153846"},{"venue":"WPES","id":"6d6e2f3297749f3d7901d2e8163cda6cb9c5be1a","venue_1":"WPES","year":"2006","title":"Private social network analysis: how to assemble pieces of a graph privately","authors":"Keith B. Frikken, Philippe Golle","author_ids":"1965461, 2779068","abstract":"Connections in distributed systems, such as social networks, online communities or peer-to-peer networks, form complex graphs. These graphs are of interest to scientists in fields as varied as marketing, epidemiology and psychology. However, knowledge of the graph is typically distributed among a large number of subjects, each of whom knows only a small piece of the graph. Efforts to assemble these pieces often fail because of privacy concerns: subjects refuse to share their local knowledge of the graph. To assuage these privacy concerns, we propose reconstructing the whole graph privately, i.e., in a way that hides the correspondence between the nodes and edges in the graph and the real-life entities and relationships that they represent. We first model the privacy threats posed by the private reconstruction of a distributed graph. Our model takes into account the possibility that malicious nodes may report incorrect information about the graph in order to facilitate later attempts to de-anonymize the reconstructed graph. We then propose protocols to privately assemble the pieces of a graph in ways that mitigate these threats. These protocols severely restrict the ability of adversaries to compromise the privacy of honest subjects.","cites":"34","conferencePercentile":"75"},{"venue":"WPES","id":"8a8db60e5c0f654fe6f7aa58595fd88e547940dd","venue_1":"WPES","year":"2013","title":"Protecting and evaluating genomic privacy in medical tests and personalized medicine","authors":"Erman Ayday, Jean Louis Raisaro, Jean-Pierre Hubaux, Jacques Rougemont","author_ids":"2898372, 2201389, 1757221, 2919038","abstract":"In this paper, we propose privacy-enhancing technologies for medical tests and personalized medicine methods that use patients' genomic data. Focusing on genetic disease-susceptibility tests, we develop a new architecture (between the patient and the medical unit) and propose a \"privacy-preserving disease susceptibility test\" (PDS) by using homomorphic encryption and proxy re-encryption. Assuming the whole genome sequencing to be done by a certified institution, we propose to store patients' genomic data encrypted by their public keys at a \"storage and processing unit\" (SPU). Our proposed solution lets the medical unit retrieve the encrypted genomic data from the SPU and process it for medical tests and personalized medicine methods, while preserving the privacy of patients' genomic data. We also quantify the genomic privacy of a patient (from the medical unit's point of view) and show how a patient's genomic privacy decreases with the genetic tests he undergoes due to (i) the nature of the genetic test, and (ii) the characteristics of the genomic data. Furthermore, we show how basic policies and obfuscation methods help to keep the genomic privacy of a patient at a high level. We also implement and show, via a complexity analysis, the practicality of PDS.","cites":"26","conferencePercentile":"96.55172414"},{"venue":"WPES","id":"5cd90ca09be8bf642b3e682c513ad52bb5194f38","venue_1":"WPES","year":"2010","title":"Unraveling an old cloak: k-anonymity for location privacy","authors":"Reza Shokri, Carmela Troncoso, Claudia Díaz, Julien Freudiger, Jean-Pierre Hubaux","author_ids":"2520493, 2130458, 3179148, 1706269, 1757221","abstract":"There is a rich collection of literature that aims at protecting the privacy of users querying location-based services. One of the most popular location privacy techniques consists in cloaking users' locations such that <i>k</i> users appear as potential senders of a query, thus achieving <i>k</i>-anonymity. This paper analyzes the effectiveness of <i>k</i>-anonymity approaches for protecting location privacy in the presence of various types of adversaries. The unraveling of the scheme unfolds the inconsistency between its components, mainly the cloaking mechanism and the <i>k</i>-anonymity metric. We show that constructing cloaking regions based on the users' locations does not reliably relate to location privacy, and argue that this technique may even be detrimental to users' location privacy. The uncovered flaws imply that existing <i>k</i>-anonymity scheme is a tattered cloak for protecting location privacy.","cites":"33","conferencePercentile":"85.71428571"},{"venue":"WPES","id":"10732a4555e532cf1f9996f5a2858c34839b0e85","venue_1":"WPES","year":"2009","title":"Key allocation schemes for private social networks","authors":"Keith B. Frikken, Preethi Srinivas","author_ids":"1965461, 3329527","abstract":"In this paper we introduce a novel scheme for key management in social networks that is a first step towards the creation of a private social network. A social network graph (i.e., the graph of friendship relationships) is private and social networks are often used to share content, which may be private, amongst its users. In the status quo, the social networking server has access to both this graph and to all of the content, effectively requiring that it is a trusted third party. The goal of this paper is to produce a mechanism through which users can control how their content is shared with other users, without relying on a trusted third party to manage the social network graph and the users' data. The specific access control model considered here is that users will specify access policies based on distance in the social network; for example some content is visible to friends only, while other content is visible to friends of friends, etc. This access control is enforced via key management. That is for each user, there is a key that only friends should be able to derive, there is a key that both friends of the user and friends of friends can derive, etc. The proposed scheme enjoys the following properties: i) the scheme is asynchronous in that it does not require users to be online at the same time, ii) the scheme provides key indistinguishability (that is if a user is not allowed to derive a key according to the access policy, then that key is indistinguishable from a random value), iii) the scheme is efficient in terms of server storage and key derivation time, and iv) the scheme is collusion resistant.","cites":"10","conferencePercentile":"42.85714286"},{"venue":"WPES","id":"46ba61649de35403b04c581f61e7df399f573ec1","venue_1":"WPES","year":"2009","title":"A distortion-based metric for location privacy","authors":"Reza Shokri, Julien Freudiger, Murtuza Jadliwala, Jean-Pierre Hubaux","author_ids":"2520493, 1706269, 2235825, 1757221","abstract":"We propose a novel framework for measuring and evaluating location privacy preserving mechanisms in mobile wireless networks. Within this framework, we first present a formal model of the system, which provides an efficient representation of the network users, the adversaries, the location privacy preserving mechanisms and the resulting location privacy of the users. This model is general enough to accurately express and analyze a variety of location privacy metrics that were proposed earlier. By using the proposed model, we provide formal representations of four metrics among the most relevant categories of location privacy metrics. We also present a detailed comparative analysis of these metrics based on a set of criteria for location privacy measurement. Finally, we propose a novel and effective metric for measuring location privacy, called the distortion-based metric, which satisfies these criteria for privacy measurement and is capable of capturing the mobile users' location privacy more precisely than the existing metrics. Our metric estimates location privacy as the expected distortion in the reconstructed users' trajectories by an adversary.","cites":"40","conferencePercentile":"100"},{"venue":"WPES","id":"201ebf5b3f3628118842b8d2d05b03c48e9678ae","venue_1":"WPES","year":"2008","title":"TwoKind authentication: protecting private information in untrustworthy environments","authors":"Katelin Bailey, Apu Kapadia, Linden Vongsathorn, Sean W. Smith","author_ids":"2330753, 3163956, 2034646, 1769821","abstract":"Users often log in to Internet sites from insecure computers and more recently have started divulging their email passwords to social-networking sites, thereby putting their private communications at risk. We propose and evaluate <i>TwoKind Authentication</i>, a simple and effective technique for limiting access to private information in untrustworthy environments. In its simplest form, <i>TwoKind</i> offers two modes of authentication by providing a <b>low</b> and a <b>high</b> authenticator. By using a <b>low</b> authenticator, users can signal to the server that they are in an untrusted environment, following which the server restricts the user's actions.\n We seek to evaluate the effectiveness of multiple authenticators in promoting safer behavior in users. We demonstrate the effectiveness of this approach through a user experiment - we find that users make a distinction between the two authenticators and generally behave in a security-conscious way, protecting their <b>high</b> authenticator the ma jority of the time. Our study suggests that <i>TwoKind</i> will be beneficial to several Internet applications, particularly if the privileges associated with the <b>low</b> authenticator can be customized to a user's security preferences.","cites":"4","conferencePercentile":"17.85714286"},{"venue":"WPES","id":"733ae82eee9e9097a73ee7261fb5f49f7014b4d6","venue_1":"WPES","year":"2014","title":"Stylometric Linkability of Tweets","authors":"Mishari Al Mishari, Dali Kaafar, Ekin Oguz, Gene Tsudik","author_ids":"3257553, 1971681, 3270425, 1702391","abstract":"Microblogging is a very popular Internet activity that informs and entertains a large number of people via terse messages; e.g., tweets on Twitter. Even though microblogging does not emphasize privacy, authors can easily hide behind pseudonyms and multiple accounts on the same, or across multiple, site(s). In this paper, we explore stylometric linkability of tweets. Our results clearly demonstrate that multiple sets of tweets by the same author are easily linkable even when the number of possible authors is large. This is also confirmed by showing that linkability holds for a set of <i>actual Twitter users</i> who admittedly tweet via multiple accounts.","cites":"1","conferencePercentile":"26.92307692"},{"venue":"WPES","id":"567f9126469f47d92a00f8b92fd9ce0c16e240b5","venue_1":"WPES","year":"2002","title":"Disclosing users' data in an environment that preserves privacy","authors":"Bruno Gusmão Rocha, Virgílio A. F. Almeida, Lucila Ishitani, Wagner Meira","author_ids":"1801818, 7360316, 1799656, 1691267","abstract":"The conflict between Web service personalization and privacy is a challenge in the information society. In this paper we address this challenge by introducing MASKS, an architecture that provides data on the users' interests to Web services, without violating their privacy. The proposed approach hides the actual identity of users by classifying them into groups, according to their interests exhibited during the interaction with a Web service. By making requests on behalf of a group, instead of an individual user, MASKS provides relevant information to the Web services, without disclosing the identity of the users. We have implemented and tested a grouping algorithm, based on categories defined by the semantic tree of DMOZ. We used access logs from actual e-commerce sites to evaluate the grouping algorithm. Our tests show that 64% of the requests made to the e-commerce service could be grouped into meaningful categories. This indicates that the e-commerce sites could use the information provided by MASKS to do personalization of services, without having access to the individual users in the groups.","cites":"4","conferencePercentile":"10"},{"venue":"WPES","id":"3bd58e47c59baa1d97b187515fb3b5d0166a868f","venue_1":"WPES","year":"2005","title":"Peripheral privacy notifications for wireless networks","authors":"Braden Kowitz, Lorrie Faith Cranor","author_ids":"2431176, 1699751","abstract":"When using wireless networks, some chats, web searches, and other information are broadcast out onto the local network. Other users on the same network may intercept and read this information. Unfortunately, without detailed knowledge of underlying technologies, many users are unable to properly evaluate the risks involved in everyday communication tasks. This study aims to develop techniques for allowing users without technical backgrounds to form more accurate expectations of privacy. We have developed a method for notifying users when their computer leaks such information. A large projected display placed in a common workplace shows excerpts from network traffic. A two-week trial was conducted to measure the effects of the display. Data was collected from network traffic monitoring and two paper surveys, which were conducted before and after the trial.","cites":"16","conferencePercentile":"46.42857143"},{"venue":"WPES","id":"1fe52596302e982c4008ba810693b7b384c21efd","venue_1":"WPES","year":"2010","title":"Americans' attitudes about internet behavioral advertising practices","authors":"Aleecia M. McDonald, Lorrie Faith Cranor","author_ids":"2175102, 1699751","abstract":"This paper presents empirical data on American Internet users' knowledge about and perceptions of Internet advertising techniques. We present the results of in-depth interviews and an online survey focusing on participants' views of online advertising and their ability to make decisions about privacy tradeoffs. We find users hold misconceptions about the purpose of cookies and the effects of clearing them. Only 11% of respondents understood the text description of NAI opt-out cookies, which are a self-help mechanism that enables user choice. 86% believe ads are tailored to websites they have visited in the past, but only 39% believe there are currently ads based on email content, and only 9% think it is ok to see ads based on email content as long as their email service is free. About 20% of participants want the benefits of targeted advertising, but 64% find the idea invasive, and we see signs of a possible chilling effect with 40% self-reporting they would change their online behavior if advertisers were collecting data. We find a gap between people's willingness to pay to protect their privacy and their willingness to accept discounts in exchange for private information. 69% believe privacy is a right and 61% think it is \"extortion\" to pay to keep their data private. Only 11% say they would pay to avoid ads. We find participants are comfortable with the idea that advertising supports free online content, but they do not believe their data are part of that exchange.","cites":"42","conferencePercentile":"100"},{"venue":"WPES","id":"69a3527f4e2d301536cbe28e02d3789bcdc66c11","venue_1":"WPES","year":"2014","title":"CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense","authors":"Xiang Cai, Rishab Nithyanand, Rob Johnson","author_ids":"6983912, 2405355, 2702065","abstract":"Website fingerprinting attacks enable an adversary to infer which website a victim is visiting, even if the victim uses an encrypting proxy, such as Tor. Previous work has shown that all proposed defenses against website fingerprinting attacks are ineffective. This paper advances the study of website fingerprinting defenses by first laying out the complete specifications of the CS-BuFlo scheme outlined by Cai, et al. CS-BuFlo, which is based on the BuFlo defense proposed by Dyer, et al., was not fully-specified by Cai, et al, but has nonetheless attracted the attention of the Tor developers. Next, a full working implementation of CS-BuFlo is provided. Finally, a thorough evaluation of CS-BuFlo is performed using empirical data (rather than data from simulations). Our experiments find that CS-BuFlo has high overhead (around 2.3-2.8x) but can get 6times closer to the bandwidth/security trade-off lower bound than Tor or SSH.","cites":"9","conferencePercentile":"86.53846154"},{"venue":"WPES","id":"88e322f1db9e3a0e8875cb71daff1145de985c42","venue_1":"WPES","year":"2014","title":"Glove: A Bespoke Website Fingerprinting Defense","authors":"Rishab Nithyanand, Xiang Cai, Rob Johnson","author_ids":"2405355, 6983912, 2702065","abstract":"Website fingerprinting attacks have recently emerged as a serious threat against web browsing privacy mechanisms, such as SSL, Tor, and encrypting tunnels. Researchers have proposed numerous attacks and defenses, and the Tor project currently includes both network- and browser-level defenses against these attacks, but published defenses have high overhead, poor security, or both.\n In this paper we present preliminary results of {Glove}, a new SSH based defense. Glove is based on the observation that current defenses are expensive not because website traces are different, but because the defense, operating blindly, does not know how to add cover traffic and therefore, puts it everywhere. Instead, Glove uses existing knowledge of a websites traces to add cover traffic conservatively while maintaining high levels of security. Further, Glove satisfies the information theoretic definitions of security defined in prior work -- i.e., it is resistant to any fingerprinting adversary. Our simulations show that Glove performs better than all currently proposed SSH based defenses in terms of the security-overhead trade-off.","cites":"9","conferencePercentile":"86.53846154"},{"venue":"WPES","id":"34f67fe697e26c93f5d29a771ef8cf5d0abc348b","venue_1":"WPES","year":"2011","title":"I know where you live: analyzing privacy protection in public databases","authors":"Manya Sleeper, Divya Sharma, Lorrie Faith Cranor","author_ids":"2939732, 3237802, 1699751","abstract":"Policymakers struggle to determine the proper tradeoffs between data accessibility and data-subject privacy as public records move online. For example, Allegheny County, Pennsylvania recently eliminated the ability to search the county property assessment database using property owners' names. We conducted a user study to determine whether this strategy provides effective privacy protection against a non-expert adversary. We found that removing search by name provides some increased privacy protection, because some users were unable to use other means to determine the address of an individual. However, this privacy protection is limited, and interface usability problems presented a comparable barrier. Our analysis suggests that if policymakers use removal of search by name as a privacy mechanism they should attempt to mitigate usability issues that can hinder legitimate use of public records databases.","cites":"1","conferencePercentile":"12.5"},{"venue":"WPES","id":"4c1f31e968b7e8d529e60380bc501e14788aa9ef","venue_1":"WPES","year":"2003","title":"I didn't buy it for myself' privacy and ecommerce personalization","authors":"Lorrie Faith Cranor","author_ids":"1699751","abstract":"Ecommerce personalization can help web sites build and retain relationships with customers, but it also raises a number of privacy concerns. This paper outlines the privacy risks associated with personalization and describes a number of approaches to personalization system design that can reduce these risks. This paper also provides an overview of the fair information practice principles and discusses how they may be applied to the design of personalization systems, and introduces privacy laws and self-regulatory guidelines relevant to personalization. Privacy risks can be reduced when personalization system designs allow for pseudonymous interactions, client-side data stores, and task-based personalization. In addition, interfaces that allow users to control the collection and use of their profile information can further ease privacy concerns.","cites":"45","conferencePercentile":"80"},{"venue":"WPES","id":"abdc94c176171aface4f3c7404bcb28832d4b446","venue_1":"WPES","year":"2004","title":"How to achieve blocking resistance for existing systems enabling anonymous web surfing","authors":"Stefan Köpsell, Ulf Hillig","author_ids":"1700539, 1968147","abstract":"We are developing a blocking resistant, practical and usable system for anonymous web surfing. This means, the system tries to provide as much reachability and availability as possible, even to users in countries where the free flow of information is legally, organizationally and physically restricted. The proposed solution is an add-on to existing anonymity systems. First we give a classification of blocking criteria and some general countermeasures. Using these techniques, we outline a concrete design, which is based on the JAP-Web Mixes (aka AN.ON).","cites":"22","conferencePercentile":"57.14285714"},{"venue":"WPES","id":"0bae979592b98d79f8a9cf91eda89ae112eb00e0","venue_1":"WPES","year":"2013","title":"The password allocation problem: strategies for reusing passwords effectively","authors":"Rishab Nithyanand, Rob Johnson","author_ids":"2405355, 2702065","abstract":"Each Internet user has, on average, 25 password-protected accounts, but only 6.5 distinct passwords[webhabits]. Despite the advice of security experts, users are obviously re-using passwords across multiple sites. So this paper asks the question: given that users are going to re-use passwords across multiple sites, how should they best allocate those passwords to sites so as to minimize their losses from accidental password disclosures?\n We provide both theoretical and practical results. First, we provide a mathematical formulation of the Password Allocation (PA) problem and show that it is NP-complete with a reduction via the 3-Partition problem. We then study several special cases and show that the optimal solution is often a contiguous allocation -- i.e., similar accounts share passwords. Next, we evaluate several human- and machine-computable heuristics that have very good performance and produce solutions that are reasonably close to optimal. We find that the human-computable heuristics do not perform nearly as well as the machine-computable heuristics, however, they provide a useful and easy to follow set of guidelines for re-using passwords.","cites":"3","conferencePercentile":"48.27586207"},{"venue":"WPES","id":"e000054e16e37818b7f0c485f48163b4c0fcd39d","venue_1":"WPES","year":"2013","title":"No surprises: measuring intrusiveness of smartphone applications by detecting objective context deviations","authors":"Frances Zhang, Fuming Shih, Daniel J. Weitzner","author_ids":"1881975, 2834454, 3152129","abstract":"We address the challenge of improving transparency for smartphone applications by creating tools that assesses privacy risk. Specifically, we invented a framework for qualitatively assessing and quantitatively measuring the intrusiveness of smartphone applications based on their data access behaviors. Our framework has two essential components. The first component is the Privacy Fingerprint, a novel visualization that is concise yet holistic. It captures each app's unique access patterns to sensitive personal data, including which types of behaviors and under which privacy-relevant usage contexts the data are collected. The second component is a new Intrusiveness Score that numerically measures out-of-context data collection, based on real data accesses gathered from empirical testing on 33 popular Android apps across 4 app categories. Specific attention is paid to the proportion of data accesses that occurs while the user is idle, raising the perceived level of intrusiveness and exposing the profiling potential of an app. Together, these components will help smartphone users decide whether to install an app because they will be able to easily and accurately assess the relative intrusiveness of apps. Our study also demonstrates that the Intrusiveness Score is helpful to compare apps that exhibit similar types of data accesses.","cites":"2","conferencePercentile":"34.48275862"},{"venue":"WPES","id":"2bfd15be8282fefa091cd438afde68bb73a8c25b","venue_1":"WPES","year":"2007","title":"Towards understanding user perceptions of authentication technologies","authors":"Laurie A. Jones, Annie I. Antón, Julia Brande Earp","author_ids":"7661452, 3254638, 2533382","abstract":"Digital identities are increasingly being used to facilitate the execution of transactions in various domains. When developing and analyzing digital identity technologies, it is important to consider the perceptions and responses of end users. Users are typically concerned about privacy and security, but do not necessarily understand how these issues are impacted by the use of digital identities. In this paper, we discuss preliminary results of a survey regarding authentication technologies used to generate digital identities. Most respondents were unfamiliar with a majority of the technologies in question (e.g. hand geometry scans), and expressed uncertainty about their use. Perceptions were more positive for the use of authentication technologies in the financial domain, and more negative for their use in the retail domain. The results may inform the design of future systems.","cites":"23","conferencePercentile":"70"},{"venue":"WPES","id":"857c319099f3b2c8fdea421fc609ec708e4d0cd1","venue_1":"WPES","year":"2005","title":"Mining rule semantics to understand legislative compliance","authors":"Travis D. Breaux, Annie I. Antón","author_ids":"1679197, 3254638","abstract":"Organizations in privacy-regulated industries (e.g. healthcare and financial institutions) face significant challenges when developing policies and systems that are properly aligned with relevant privacy legislation. We analyze privacy regulations derived from the Health Insurance Portability and Accountability Act (HIPAA) that affect information sharing practices and consumer privacy in healthcare systems. Our analysis shows specific natural language semantics that formally characterize rights, obligations, and the meaningful relationships between them required to build value into systems. Furthermore, we evaluate semantics for rules and constraints necessary to develop machine-enforceable policies that bridge between laws, policies, practices, and system requirements. We believe the results of our analysis will benefit legislators, regulators and policy and system developers by focusing their attention on natural language policy semantics that are implementable in software systems.","cites":"24","conferencePercentile":"71.42857143"},{"venue":"WPES","id":"345947186f190649c582204776071ac9a62e8d67","venue_1":"WPES","year":"2007","title":"Low-resource routing attacks against tor","authors":"Kevin S. Bauer, Damon McCoy, Dirk Grunwald, Tadayoshi Kohno, Douglas C. Sicker","author_ids":"1744469, 1703426, 1748465, 1769675, 1694069","abstract":"Tor has become one of the most popular overlay networks for anonymizing TCP traffic. Its popularity is due in part to its perceived strong anonymity properties and its relatively low latency service. Low latency is achieved through Tor&#226;  s ability to balance the traffic load by optimizing Tor router selection to probabilistically favor routers with highbandwidth capabilities.\n We investigate how Tor&#226;  s routing optimizations impact its ability to provide strong anonymity. Through experiments conducted on PlanetLab, we show the extent to which routing performance optimizations have left the system vulnerable to end-to-end traffic analysis attacks from non-global adversaries with minimal resources. Further, we demonstrate that entry guards, added to mitigate path disruption attacks, are themselves vulnerable to attack. Finally, we explore solutions to improve Tor&#226;  s current routing algorithms and propose alternative routing strategies that prevent some of the routing attacks used in our experiments.","cites":"136","conferencePercentile":"100"},{"venue":"WPES","id":"57f74f1ce692034762fd594e0f1db40e9490e300","venue_1":"WPES","year":"2004","title":"Specifying privacy policies with P3P and EPAL: lessons learned","authors":"William H. Stufflebeam, Annie I. Antón, Qingfeng He, Neha Jain","author_ids":"2366611, 3254638, 2716902, 2392097","abstract":"As computing becomes more ubiquitous and Internet use continues to rise, it is increasingly important for organizations to construct accurate and effective privacy policies that document their information handling and usage practices. Most privacy policies are derived and specified in a somewhat ad-hoc manner, leading to policies that are of limited use to the consumers they are intended to serve. To make privacy policies more readable and enforceable, two privacy policy specification languages have emerged, P3P and EPAL. This paper discusses a case study in which the authors systematically formalized two real and complex, healthcare website privacy statements, and measured the results against well-known requirements engineering criteria.","cites":"23","conferencePercentile":"61.9047619"},{"venue":"WPES","id":"aa9e3fc9015fb95787eefe454f944b2c9646af92","venue_1":"WPES","year":"2010","title":"Investigating privacy-aware distributed query evaluation","authors":"Nicholas L. Farnan, Adam J. Lee, Ting Yu","author_ids":"1845599, 1685100, 5262515","abstract":"Historically, privacy and efficiency have largely been at odds with one another when querying remote data sources: traditional query optimization techniques provide efficient retrieval by exporting information about the intension of a query to data sources, while private information retrieval (PIR) schemes hide query intension at the cost of extreme computational or communication overheads. Given the increasing use of Internet-scale distributed databases, exploring the spectrum between these two extremes is worthwhile. In this paper, we explore the degree to which query intension is leaked to remote data sources when a variety of existing query processing and view materialization techniques are used. We show that these information flows can be quantified in a concrete manner, and investigate the notion of privacy-aware distributed query evaluation. We then propose two techniques to improve the balance between privacy and efficiency when processing distributed queries, and discuss a number of interesting directions for future work.","cites":"4","conferencePercentile":"21.42857143"},{"venue":"WPES","id":"3a38c29268d15294287eb49a2598cc98db9d6522","venue_1":"WPES","year":"2004","title":"Soft blocking: flexible blocker tags on the cheap","authors":"Ari Juels, John G. Brainard","author_ids":"1687161, 1687904","abstract":"A \"blocker\" tag is a privacy-enhancing radio-frequency identification (RFID) tag. It operates by interfering with the protocol in which a reader communicates individually with other RFID tags. While inexpensive to manufacture in quantity, blockers are nonetheless special-purpose devices, and thus introduce level of complexity that may pose an obstacle to their deployment.\n We propose a variant on the blocker concept that we call &#60;i>soft blocking&#60;/i>. This involves software (or firmware) modules that offer a different balance of characteristics than ordinary blockers. Soft blocking offers somewhat weaker privacy enforcement that is essentially voluntary or internally auditable (much like P3P). It has the significant advantage, however, of relying on standard (or very slightly modified) RFID tags. Additionally, soft blocking offers the possibility of flexible privacy policies in which partial or scrubbed data is revealed about \"private\" tags, in lieu of the all-or-nothing policy enforced by a blocker.\n We show, moreover, how the correct functioning of a soft-blocker system may be rendered externally auditable with minor modifications to the basic tag-reading protocol. We also briefly discuss the special, attractive approach of &#60;i>unblocking&#60;/i>, a soft-blocking variant that permits an \"opt-in\" approach to consumer privacy.","cites":"55","conferencePercentile":"85.71428571"},{"venue":"WPES","id":"12e957372e45e3f759bd04ecbf8e11573085371c","venue_1":"WPES","year":"2008","title":"Secure aggregation in a publish-subscribe system","authors":"Kazuhiro Minami, Adam J. Lee, Marianne Winslett, Nikita Borisov","author_ids":"1681687, 1685100, 1750874, 1723454","abstract":"A publish-subscribe system is an information dissemination infrastructure that supports many-to-many communications among publishers and subscribers. In many publish-subscribe systems, in-network aggregation of input data is considered to be an important service that reduces the bandwidth requirements of the system significantly. In this paper, we present a scheme for securing the aggregation of inputs to such a publish-subscribe system. Our scheme, which focuses on the additive aggregate function, <i>sum</i>, preserves the confidentiality and integrity of aggregated data in the presence of untrusted routing nodes. Our scheme allows a group of publishers to publish aggregate data to authorized subscribers without revealing their individual private inputs to either the routing nodes or the subscribers. In addition, our scheme allows subscribers to verify that routing nodes perform the aggregation operation correctly. We use a message authentication code (MAC) scheme based on the discrete logarithm property to allow subscribers to verify the correctness of aggregated data without receiving the digitally-signed raw data used as input to the aggregation. In addition to describing our secure aggregation scheme, we provide formal proofs of its soundness and safety.","cites":"17","conferencePercentile":"71.42857143"},{"venue":"WPES","id":"356e0eece5bbd700ea3c388af8ea3e088baf7c6e","venue_1":"WPES","year":"2004","title":"Location diversity in anonymity networks","authors":"Nick Feamster, Roger Dingledine","author_ids":"1800154, 2672153","abstract":"Anonymity networks have long relied on diversity of node location for protection against attacks---typically an adversary who can observe a larger fraction of the network can launch a more effective attack. We investigate the diversity of two deployed anonymity networks, Mixmaster and Tor, with respect to an adversary who controls a single Internet administrative domain.\n Specifically, we implement a variant of a recently proposed technique that passively estimates the set of administrative domains (also known as autonomous systems, or ASes) between two arbitrary end-hosts without having access to either end of the path. Using this technique, we analyze the AS-level paths that are likely to be used in these anonymity networks. We find several cases in each network where multiple nodes are in the same administrative domain. Further, many paths between nodes, and between nodes and popular endpoints, traverse the same domain.","cites":"66","conferencePercentile":"95.23809524"},{"venue":"WPES","id":"bf3416ea02ea0d9327a7886136d7d7d5a66cf491","venue_1":"WPES","year":"2012","title":"What do online behavioral advertising privacy disclosures communicate to users?","authors":"Pedro Giovanni Leon, Justin Cranshaw, Lorrie Faith Cranor, Jim Graves, Manoj Hastak, Blase Ur, Guzi Xu","author_ids":"2591254, 2660203, 1699751, 2246476, 2397388, 2222651, 2184700","abstract":"Online Behavioral Advertising (OBA), the practice of tailoring ads based on an individual's online activities, has led to privacy concerns. In an attempt to mitigate these privacy concerns, the online advertising industry has proposed the use of OBA disclosures: icons, accompanying taglines, and landing pages intended to inform users about OBA and provide opt-out options. We conducted a 1,505-participant online study to investigate Internet users' perceptions of OBA disclosures. The disclosures failed to clearly notify participants about OBA and inform them about their choices. Half of the participants remembered the ads they saw but only 12% correctly remembered the disclosure taglines attached to ads. When shown the disclosures again, the majority mistakenly believed that ads would pop up if they clicked on disclosures, and more participants incorrectly thought that clicking the disclosures would let them purchase advertisements than correctly understood that they could then opt out of OBA. \"AdChoices\", the most commonly used tagline, was particularly ineffective at communicating notice and choice. A majority of participants mistakenly believed that opting out would stop all online tracking, not just tailored ads. We dicuss challenges in crafting disclosures and provide suggestions for improvement.","cites":"7","conferencePercentile":"50"},{"venue":"WPES","id":"1219cd468a8c6fa670641da161f2e14926091d9d","venue_1":"WPES","year":"2013","title":"The post anachronism: the temporal dimension of facebook privacy","authors":"Lujo Bauer, Lorrie Faith Cranor, Saranga Komanduri, Michelle L. Mazurek, Michael K. Reiter, Manya Sleeper, Blase Ur","author_ids":"1790378, 1699751, 1893226, 2079420, 1746214, 2939732, 2222651","abstract":"This paper reports on two studies that investigate empirically how privacy preferences about the audience and emphasis of Facebook posts change over time. In a 63-participant longitudinal study, participants gave their audience and emphasis preferences for up to ten of their Facebook posts in the week they were posted, again one week later, and again one month later. In a 234-participant retrospective study, participants expressed their preferences about posts made in the past week, as well as one year prior. We found that participants did not want content to fade away wholesale with age; the audience participants wanted to be able to access posts remained relatively constant over time. However, participants did want a handful of posts to become more private over time, as well as others to become more visible. Participants' predictions about how their preferences would change correlated poorly with their actual changes in preferences over time, casting doubt on ideas for setting an expiration date for content. Although older posts were seen as less relevant and had often been forgotten, participants found value in these posts for reminiscence. Surprisingly, we observed few concerns about privacy or self-presentation for older posts. We discuss our findings' implications for retrospective privacy mechanisms.","cites":"12","conferencePercentile":"84.48275862"},{"venue":"WPES","id":"d48e85be4ebcf513f57d83c2df899017c0187c83","venue_1":"WPES","year":"2014","title":"Exploiting Users' Inconsistent Preferences in Online Social Networks to Discover Private Friendship Links","authors":"Lei Jin, Hassan Takabi, Xuelian Long, James B. D. Joshi","author_ids":"1747636, 1718305, 1894385, 1719496","abstract":"In a social network system, a friendship relation between two users is usually represented by an undirected link and it is visible in both users' friend lists. Such a dual visibility of a friendship link may raise privacy threats. This is because both the users of a friendship link can separately control its visibility to other users and their preferences of sharing such a friendship link may not be consistent. Even if one of them conceals the friendship link from a third user, that third user may find the link through the other user's friend list. In addition, as most social network users allow their friends to see their friend lists, an adversary can exploit these inconsistent policies caused by users' conflicting preferences to identify and infer many of a targeted user's friends and even reconstruct the topology of an entire social network. In this paper, we propose, characterize and evaluate such an attack referred as the Friendship Identification and Inference (FII) attack. In an FII attack scenario, an adversary first accumulates the initial attack relevant information based on the friend lists visible to him in a social network. Then, he utilizes this information to identify and infer a target's friends using a random walk based approach. We formally define the attack and present the attack steps, the attack algorithm and various attack schemes. Our experimental results using three real social network datasets show that FII attacks are effective in inferring private friendship links of a target and predicting the topology of the social network. Currently, most popular social network systems, such as Facebook, LinkedIn and Foursquare, are susceptible to FII attacks.","cites":"0","conferencePercentile":"11.53846154"},{"venue":"WPES","id":"4c5f479f2a7604796ae7f2a115b45863139c056c","venue_1":"WPES","year":"2006","title":"Scanning electronic documents for personally identifiable information","authors":"Tuomas Aura, Thomas A. Kuhn, Michael Roe","author_ids":"1710566, 3125803, 2398096","abstract":"Sometimes, it is necessary to remove author names and other personally identifiable information (PII) from documents before publication. We have implemented a novel defensive tool for detecting such data automatically. By using the detection tool, we have learned about where PII may be stored in documents and how it is put there. A key observation is that, contrary to common belief, user and machine identifiers and other metadata are not embedded in documents only by a single piece of software, such as a word processor, but by various tools used at different stages of the document authoring process.","cites":"10","conferencePercentile":"40.625"},{"venue":"WPES","id":"673c8126c4d05444cd6e96095c7ebbe50941cfb5","venue_1":"WPES","year":"2011","title":"Cover locations: availing location-based services without revealing the location","authors":"Sai Teja Peddinti, Avis Dsouza, Nitesh Saxena","author_ids":"2002188, 2363763, 1716235","abstract":"Location-Based Services (LBSs) have been gaining popularity due to a wide range of interesting and important applications being developed. However, the users availing such services are concerned about their location privacy, in that they are forced to reveal their sensitive location information to untrusted third-parties. In this paper, we propose a new privacy-preserving approach, <i>Cover Locations,</i> which allows a user to access an LBS without revealing his/her actual location. Based on its current location, the user's device queries for a few specifically chosen surrounding locations and constructs the results corresponding to its location from the results obtained for each queried location. Since the user location <i>does not leave</i> the user's device - as either a latitude and longitude pair, or as an obfuscated region - the user is guaranteed very high level of privacy. The Cover Locations approach only requires minimal changes on the user's device and can be readily deployed by privacy-conscious users. An adversary, trying to identify the user location, can only resolve the location to few triangular regions and not to the actual location itself. We evaluate the privacy provided by Cover Locations based on the number of locations queried and the total area under the resolved triangular regions. We also ascertain the robustness of Cover Locations approach when the adversary has access to a short-term user history, employing machine learning techniques. Overall, our results show that the proposed solution, which requires minor computations without the need for any out-of-band information such as traffic densities in a region or the road network information, is superior to other client-based solutions.","cites":"5","conferencePercentile":"50"},{"venue":"WPES","id":"39f1dc941378d18cb82f9f8a37b9ccee90ea35aa","venue_1":"WPES","year":"2006","title":"A privacy-preserving interdomain audit framework","authors":"Adam J. Lee, Parisa Tabriz, Nikita Borisov","author_ids":"1685100, 2230201, 1723454","abstract":"Recent trends in Internet computing have led to the popularization of many forms of virtual organizations. Examples include supply chain management, grid computing, and collaborative research environments like PlanetLab. Unfortunately, when it comes to the security analysis of these systems, the whole is certainly greater than the sum of its parts. That is, local intrusion detection and audit practices are insufficient for detecting distributed attacks such as coordinated network reconnaissance, stepping-stone attacks, and violations of application-level trust constraints between security domains. A distributed process that coordinates information from each member could detect these types of violations, but privacy concerns between member organizations or safety concerns about centralizing sensitive information often restrict this level of information flow. In this paper, we propose a privacy-preserving framework for distributed audit that allows member organizations to detect distributed attacks without requiring the release of excessive private information. We discuss both the architecture and mechanisms used in our approach and comment on the performance of a prototype implementation.","cites":"15","conferencePercentile":"59.375"},{"venue":"WPES","id":"91c61aa59e8b5b42b168d708fd5bc374849f651a","venue_1":"WPES","year":"2010","title":"A framework for privacy-conducive recommendations","authors":"Richard Chow, Jessica Staddon","author_ids":"2227735, 2143669","abstract":"Recommendations and advertisements based on consumer behavior patterns are increasingly prevalent, yet carry significant privacy concerns. We propose an easily implemented alternative framework in which publicly available Web data is mined to discover product preference associations.","cites":"0","conferencePercentile":"7.142857143"},{"venue":"WPES","id":"647bd969e9c46fd8094d1ac10325a3024d350e81","venue_1":"WPES","year":"2014","title":"ProfileGuard: Privacy Preserving Obfuscation for Mobile User Profiles","authors":"Imdad Ullah, Roksana Boreli, Salil S. Kanhere, Sanjay Chawla","author_ids":"2428843, 1840101, 1733096, 1768898","abstract":"Analytics companies have become an integral part of the mobile advertising industry, enabling successful user targeting via user profiles, derived from the mobile apps installed by specific users. This poses a threat to privacy of such users, when apps indicating sensitive information, e.g., a gaming app showing a gambling problem, are the basis for profiling. In this paper, we propose a ProfileGuard, novel app-based obfuscation mechanism to remove the dominance (prevalence amongst the interest categories present in a user profile) of selected private user profile interest categories. We show, based on extensive experimental evaluation using 2700 Android apps during a 9 month test campaign, that the best trade-off between the level of effort required by the obfuscating system and the resulting privacy protection can be achieved by choosing the obfuscating apps based on <i>similarity</i> with user's existing apps (while ensuring that the selected apps belong to a non-private category). We implement a POC ProfileGuard app to demonstrate the feasibility of an automated obfuscation mechanism. We also provide insights into the broad Google AdMob profiling rules, showing that there is a deterministic mapping of individual apps to profile interests, that profiles based on multiple apps represent a union of individual app profiles and that there is a minimum level of activity necessary for AdMob to build a stable user profile. Finally, we show the resulting effect of obfuscation on the received ads, demonstrating that modifying user profiles to include a richer set of interests results in correspondingly more diverse received ads.","cites":"0","conferencePercentile":"11.53846154"},{"venue":"WPES","id":"2d2b1f9446e9b4cdb46327cda32a8d9621944e29","venue_1":"WPES","year":"2005","title":"Information revelation and privacy in online social networks","authors":"Ralph Gross, Alessandro Acquisti","author_ids":"1741233, 1683053","abstract":"Participation in social networking sites has dramatically increased in recent years. Services such as Friendster, Tribe, or the Facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends - and, often, unknown numbers of strangers. In this paper we study patterns of information revelation in online social networks and their privacy implications. We analyze the online behavior of more than 4,000 Carnegie Mellon University students who have joined a popular social networking site catered to colleges. We evaluate the amount of information they disclose and study their usage of the site's privacy settings. We highlight potential attacks on various aspects of their privacy, and we show that only a minimal percentage of users changes the highly permeable privacy preferences.","cites":"523","conferencePercentile":"100"}]}