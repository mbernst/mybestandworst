{"WSDM.csv":[{"venue":"WSDM","id":"04c37d4af883ff8118d8061a9b1c922fe8dd1e5e","venue_1":"WSDM","year":"2009","title":"Clustering the tagged web","authors":"Daniel Ramage, Paul Heymann, Christopher D. Manning, Hector Garcia-Molina","author_ids":"1878835, 2912194, 1812612, 1695250","abstract":"Automatically clustering web pages into semantic groups promises improved search and browsing on the web. In this paper, we demonstrate how user-generated tags from large-scale social bookmarking websites such as del.icio.us can be used as a complementary data source to page text and anchor text for improving automatic clustering of web pages. This paper explores the use of tags in 1) K-means clustering in an extended vector space model that includes tags as well as page text and 2) a novel generative clustering algorithm based on latent Dirichlet allocation that jointly models text and tags. We evaluate the models by comparing their output to an established web directory. We find that the naive inclusion of tagging data improves cluster quality versus page text alone, but a more principled inclusion can substantially improve the quality of all models with a statistically significant absolute F-score increase of 4%. The generative model outperforms K-means with another 8% F-score increase.","cites":"99","conferencePercentile":"92.10526316"},{"venue":"WSDM","id":"3d663af94807663c5df519da8792720321efa11f","venue_1":"WSDM","year":"2008","title":"SoftRank: optimizing non-smooth rank metrics","authors":"Michael J. Taylor, John Guiver, Stephen E. Robertson, Tom Minka","author_ids":"1796726, 3229103, 4844342, 1801025","abstract":"We address the problem of learning large complex ranking functions. Most IR applications use evaluation metrics that depend only upon the ranks of documents. However, most ranking functions generate document scores, which are sorted to produce a ranking. Hence IR metrics are innately non-smooth with respect to the scores, due to the sort. Unfortunately, many machine learning algorithms require the gradient of a training objective in order to perform the optimization of the model parameters,and because IR metrics are non-smooth,we need to find a smooth proxy objective that can be used for training. We present a new family of training objectives that are derived from the rank distributions of documents, induced by smoothed scores. We call this approach SoftRank. We focus on a smoothed approximation to Normalized Discounted Cumulative Gain (NDCG), called SoftNDCG and we compare it with three other training objectives in the recent literature. We present two main results. First, SoftRank yields a very good way of optimizing NDCG. Second, we show that it is possible to achieve state of the art test set NDCG results by optimizing a soft NDCG objective on the training set with a different discount function","cites":"84","conferencePercentile":"76"},{"venue":"WSDM","id":"b84389bd8da742b98fe1be8c96829b79db3a5c89","venue_1":"WSDM","year":"2013","title":"Econometric analysis and digital marketing: how tomeasure the effectiveness of an ad","authors":"Ayman Farahat, James Shanahan","author_ids":"2097475, 3867792","abstract":"Over the past 18 years online advertising has grown to a $70 billion industry worldwide annually. Despite this impressive growth, online advertising faces many (and some would say traditional) challenges including how to measure the efficiency or the potential loss of sales caused by the inefficient use of advertising dollars. Consequently, it is vital to measure, maximize, and benchmark the efficiency of advertising media expenditures. This tutorial introduces the field of econometrics as a means of measuring the effectiveness of digital marketing. Econometrics is a field that extends and applies statistical methods to the analysis of economic phenomena. In that vein, econometrics goes beyond traditional statistics and explicitly recognizes the complexities of human behavior. Consider for example the impact of deep discounts on survival of restaurants. Struggling businesses are more likely to offer these deep discounts and eventually fail. A na&#239;ve application of statistical techniques will overestimate the impact of deep discounts on business survival. In this case, the discounts are an endogenous variable as compared to an exogenous variable. This type of specification error highlights why we need a deeper look at the variables that go into statistical models. Econometrics addresses these and other issues in a formal and rigorous manner.","cites":"1","conferencePercentile":"14.13043478"},{"venue":"WSDM","id":"13d72ef522b405c18f7d228c5744687609b4c3a4","venue_1":"WSDM","year":"2008","title":"An experimental comparison of click position-bias models","authors":"Nick Craswell, Onno Zoeter, Michael J. Taylor, Bill Ramsey","author_ids":"1703980, 1769092, 1796726, 2869240","abstract":"Search engine click logs provide an invaluable source of relevance information, but this information is biased. A key source of bias is presentation order: the probability of click is influenced by a document's position in the results page. This paper focuses on explaining that bias, modelling how probability of click depends on position. We propose four simple hypotheses about how position bias might arise. We carry out a large data-gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. We then explore which of the four hypotheses best explains the real-world position effects, and compare these to a simple logistic regression model. The data are not well explained by simple position models, where some users click indiscriminately on rank 1 or there is a simple decay of attention over ranks. A &#226;  cascade' model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks","cites":"315","conferencePercentile":"92"},{"venue":"WSDM","id":"8d1b2f6a915e9930b8b53e66ab9652dccd3420e9","venue_1":"WSDM","year":"2012","title":"A straw shows which way the wind blows: ranking potentially popular items from early votes","authors":"Peifeng Yin, Ping Luo, Min Wang, Wang-Chien Lee","author_ids":"3193264, 1693209, 4480979, 1686360","abstract":"Prediction of popular items in online content sharing systems has recently attracted a lot of attention due to the tremendous need of users and its commercial values. Different from previous works that make prediction by fitting a popularity growth model, we tackle this problem by exploiting the latent <i>conforming</i> and <i>maverick</i> personalities of those who vote to assess the quality of on-line items. We argue that the former personality prompts a user to cast her vote conforming to the majority of the service community while on the contrary the later personality makes her vote different from the community. We thus propose a <i>Conformer-Maverick (CM)</i> model to simulate the voting process and use it to rank top-<i>k</i> potentially popular items based on the early votes they received. Through an extensive experimental evaluation, we validate our ideas and find that our proposed CM model achieves better performance than baseline solutions, especially for smaller <i>k</i>.","cites":"17","conferencePercentile":"52.90697674"},{"venue":"WSDM","id":"bfae28852a84f3fdd8293a7812ca9fd5d85c5b28","venue_1":"WSDM","year":"2013","title":"News recommendation via hypergraph learning: encapsulation of user behavior and news content","authors":"Lei Li, Tao Li","author_ids":"4236221, 1726351","abstract":"Personalized news recommender systems have gained increasing attention in recent years. Within a news reading community, the implicit correlations among news readers, news articles, topics and named entities, e.g., what types of named entities in articles are preferred by users, and why users like the articles, could be valuable for building an effective news recommender. In this paper, we propose a novel news personalization framework by mining such correlations. We use hypergraph to model various high-order relations among different objects in news data, and formulate news recommendation as a ranking problem on fine-grained hypergraphs. In addition, by transductive inference, our proposed algorithm is capable of effectively handling the so-called cold-start problem. Extensive experiments on a data set collected from various news websites have demonstrated the effectiveness of our proposed algorithm.","cites":"9","conferencePercentile":"48.91304348"},{"venue":"WSDM","id":"5931ca67dcb60e9f730a79cf6a7857b8c0810288","venue_1":"WSDM","year":"2011","title":"We feel fine and searching the emotional web","authors":"Sepandar D. Kamvar, Jonathan Harris","author_ids":"2833700, 1766187","abstract":"We present <i>We Feel Fine</i>, an emotional search engine and web-based artwork whose mission is to collect the world's emotions to help people better understand themselves and others. We Feel Fine continuously crawls blogs, microblogs, and social networking sites, extracting sentences that include the words \"I feel\" or \"I am feeling\", as well as the gender, age, and location of the people authoring those sentences. The We Feel Fine search interface allows users to search or browse over the resulting sentence-level index, asking questions such as \"How did young people in Ohio feel when Obama was elected?\" While most research in sentiment analysis focuses on algorithms for extraction and classification of sentiment about given topics, we focus instead on building an interface that provides an engaging means of qualitative exploration of emotional data, and a flexible data collection and serving architecture that enables an ecosystem of data analysis applications. We use our observations on the usage of We Feel Fine to suggest a class of visualizations called Experiential Data Visualization, which focus on immersive item-level interaction with data. We also discuss the implications of such visualizations for crowdsourcing qualitative research in the social sciences.","cites":"51","conferencePercentile":"82.02247191"},{"venue":"WSDM","id":"0591e5ce18dbb93f254adc6162f48bccb78080ce","venue_1":"WSDM","year":"2010","title":"Folks in Folksonomies: social link prediction from shared metadata","authors":"Rossano Schifanella, Alain Barrat, Ciro Cattuto, Benjamin Markines, Filippo Menczer","author_ids":"2251027, 1732049, 8024566, 2399519, 1727649","abstract":"Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm's suggestions based on listening patterns.","cites":"89","conferencePercentile":"78.88888889"},{"venue":"WSDM","id":"1753c2dc85cc40e0a2e8b4a405c1690eab066d8d","venue_1":"WSDM","year":"2014","title":"FENNEL: streaming graph partitioning for massive scale graphs","authors":"Charalampos E. Tsourakakis, Christos Gkantsidis, Bozidar Radunovic, Milan Vojnovic","author_ids":"2023899, 2903620, 2757057, 1782150","abstract":"Balanced graph partitioning in the streaming setting is a key problem to enable scalable and efficient computations on massive graph data such as web graphs, knowledge graphs, and graphs arising in the context of online social networks. Two families of heuristics for graph partitioning in the streaming setting are in wide use: place the newly arrived vertex in the cluster with the largest number of neighbors or in the cluster with the least number of non-neighbors.\n In this work, we introduce a framework which unifies the two seemingly orthogonal heuristics and allows us to quantify the interpolation between them. More generally, the framework enables a well principled design of scalable, streaming graph partitioning algorithms that are amenable to distributed implementations. We derive a novel one-pass, streaming graph partitioning algorithm and show that it yields significant performance improvements over previous approaches using an extensive set of real-world and synthetic graphs.\n Surprisingly, despite the fact that our algorithm is a one-pass streaming algorithm, we found its performance to be in many cases comparable to the <i>de-facto</i> standard offline software METIS and in some cases even superiror. For instance, for the Twitter graph with more than 1.4 billion of edges, our method partitions the graph in about 40 minutes achieving a balanced partition that cuts as few as 6.8% of edges, whereas it took more than 81/2 hours by METIS to produce a balanced partition that cuts 11.98% of edges. We also demonstrate the performance gains by using our graph partitioner while solving standard PageRank computation in a graph processing platform with respect to the communication cost and runtime.","cites":"45","conferencePercentile":"100"},{"venue":"WSDM","id":"44e4fbf0d346c8d93ecbd9bcf8fb78741c143038","venue_1":"WSDM","year":"2016","title":"Modeling and Predicting Learning Behavior in MOOCs","authors":"Jiezhong Qiu, Jie Tang, Tracy Xiao Liu, Jie Gong, Chenhui Zhang, Qian Zhang, Yufei Xue","author_ids":"3207002, 1750766, 3355580, 6816917, 2915439, 1737486, 2474828","abstract":"Massive Open Online Courses (MOOCs), which collect complete records of all student interactions in an online learning environment, offer us an unprecedented opportunity to analyze students' learning behavior at a very fine granularity than ever before.\n Using dataset from xuetangX, one of the largest MOOCs from China, we analyze key factors that influence students' engagement in MOOCs and study to what extent we could infer a student's learning effectiveness. We observe significant behavioral heterogeneity in students' course selection as well as their learning patterns. For example, students who exert higher effort and ask more questions are not necessarily more likely to get certificates. Additionally, the probability that a student obtains the course certificate increases dramatically (3 x higher) when she has one or more \"certificate friends\".\n Moreover, we develop a unified model to predict students' learning effectiveness, by incorporating user demographics, forum activities, and learning behavior. We demonstrate that the proposed model significantly outperforms (+2.03-9.03% by F1-score) several alternative methods in predicting students' performance on assignments and course certificates. The model is flexible and can be applied to various settings. For example, we are deploying a new feature into xuetangX to help teachers dynamically optimize the teaching process.","cites":"1","conferencePercentile":"52.15053763"},{"venue":"WSDM","id":"40ac13d8d6899961de76d9a2d9e4a6fb4c891358","venue_1":"WSDM","year":"2013","title":"Connecting comments and tags: improved modeling of social tagging systems","authors":"Dawei Yin, Shengbo Guo, Boris Chidlovskii, Brian D. Davison, Cédric Archambeau, Guillaume Bouchard","author_ids":"2115608, 3095943, 1729294, 1800527, 7430037, 1684865","abstract":"Collaborative tagging systems are now deployed extensively to help users share and organize resources. Tag prediction and recommendation can simplify and streamline the user experience, and by modeling user preferences, predictive accuracy can be significantly improved. However, previous methods typically model user behavior based only on a log of prior tags, neglecting other behaviors and information in social tagging systems, e.g., commenting on items and connecting with other users. On the other hand, little is known about the connection and correlations among these behaviors and contexts in social tagging systems.\n In this paper, we investigate improved modeling for predictive social tagging systems. Our explanatory analyses demonstrate three significant challenges: coupled high order interaction, data sparsity and cold start on items. We tackle these problems by using a generalized latent factor model and fully Bayesian treatment. To evaluate performance, we test on two real-world data sets from Flickr and Bibsonomy. Our experiments on these data sets show that to achieve best predictive performance, it is necessary to employ a fully Bayesian treatment in modeling high order relations in social tagging system. Our methods noticeably outperform state-of-the-art approaches.","cites":"16","conferencePercentile":"66.30434783"},{"venue":"WSDM","id":"c77be34db96695159244723fe9ffa4a88dc4a36d","venue_1":"WSDM","year":"2015","title":"Understanding and Predicting Graded Search Satisfaction","authors":"Jiepu Jiang, Ahmed Hassan Awadallah, Xiaolin Shi, Ryen W. White","author_ids":"1730464, 2461958, 1724322, 1734415","abstract":"Understanding and estimating satisfaction with search engines is an important aspect of evaluating retrieval performance. Research to date has modeled and predicted search satisfaction on a binary scale, i.e., the searchers are either satisfied or dissatisfied with their search outcome. However, users' search experience is a complex construct and there are different degrees of satisfaction. As such, binary classification of satisfaction may be limiting. To the best of our knowledge, we are the first to study the problem of understanding and predicting graded (multi-level) search satisfaction. We ex-amine sessions mined from search engine logs, where searcher satisfaction was also assessed on multi-point scale by human annotators. Leveraging these search log data, we observe rich and non-monotonous changes in search behavior in sessions with different degrees of satisfaction. The findings suggest that we should predict finer-grained satisfaction levels. To address this issue, we model search satisfaction using features indicating search outcome, search effort, and changes in both outcome and effort during a session. We show that our approach can predict subtle changes in search satisfaction more accurately than state-of-the-art methods, affording greater insight into search satisfaction. The strong performance of our models has implications for search providers seeking to accu-rately measure satisfaction with their services.","cites":"12","conferencePercentile":"86.8852459"},{"venue":"WSDM","id":"54559aa6f7ce6f07e9baac611fa4410c0b92e819","venue_1":"WSDM","year":"2013","title":"App recommendation: a contest between satisfaction and temptation","authors":"Peifeng Yin, Ping Luo, Wang-Chien Lee, Min Wang","author_ids":"3193264, 1693209, 1686360, 4480979","abstract":"Due to the huge and still rapidly growing number of mobile applications (apps), it becomes necessary to provide users an app recommendation service. Different from conventional item recommendation where the user interest is the primary factor, app recommendation also needs to consider factors that invoke a user to replace an old app (if she already has one) with a new app. In this work we propose an Actual- Tempting model that captures such factors in the decision process of mobile app adoption. The model assumes that each owned app has an actual satisfactory value and a new app under consideration has a tempting value. The former stands for the real satisfactory value the owned app brings to the user while the latter represents the estimated value the new app may seemingly have. We argue that the process of app adoption therefore is a contest between the owned apps' actual values and the candidate app's tempting value. Via the extensive experiments we show that the AT model performs significantly better than the conventional recommendation techniques such as collaborative filtering and content-based recommendation. Furthermore, the best recommendation performance is achieved when the AT model is combined with them.","cites":"20","conferencePercentile":"72.82608696"},{"venue":"WSDM","id":"ab21e24201e6117ee6879a58624b655a52e9dd54","venue_1":"WSDM","year":"2014","title":"Modeling dwell time to predict click-level satisfaction","authors":"Youngho Kim, Ahmed Hassan Awadallah, Ryen W. White, Imed Zitouni","author_ids":"4422730, 2461958, 1734415, 1954563","abstract":"Clicks on search results are the most widely used behavioral signals for predicting search satisfaction. Even though clicks are correlated with satisfaction, they can also be noisy. Previous work has shown that clicks are affected by position bias, caption bias, and other factors. A popular heuristic for reducing this noise is to only consider clicks with long dwell time, usually equaling or exceeding 30 seconds. The rationale is that the more time a searcher spends on a page, the more likely they are to be satisfied with its contents. However, having a single threshold value assumes that users need a fixed amount of time to be satisfied with any result click, irrespective of the page chosen. In reality, clicked pages can differ significantly. Pages have different topics, readability levels, content lengths, etc. All of these factors may affect the amount of time spent by the user on the page. In this paper, we study the effect of different page characteristics on the time needed to achieve search satisfaction. We show that the topic of the page, its length and its readability level are critical in determining the amount of dwell time needed to predict whether any click is associated with satisfaction. We propose a method to model and provide a better understanding of click dwell time. We estimate click dwell time distributions for SAT (satisfied) or DSAT (dissatisfied) clicks for different click segments and use them to derive features to train a click-level satisfaction model. We compare the proposed model to baseline methods that use dwell time and other search performance predictors as features, and demonstrate that the proposed model achieves significant improvements.","cites":"36","conferencePercentile":"97.43589744"},{"venue":"WSDM","id":"cac1c8945f915fd315d32ae68149f00b069181ee","venue_1":"WSDM","year":"2016","title":"Understanding Offline Political Systems by Mining Online Political Data","authors":"David Lazer, Oren Tsur, Tina Eliassi-Rad","author_ids":"5170261, 1842598, 1684421","abstract":"\"Man is by nature a political animal\", as asserted by Aristotle. This political nature manifests itself in the data we produce and the traces we leave online. In this tutorial, we address a number of fundamental issues regarding mining of political data: What types of data could be considered political? What can we learn from such data? Can we use the data for prediction of political changes, etc? How can these prediction tasks be done efficiently? Can we use online socio-political data in order to get a better understanding of our political systems and of recent political changes? What are the pitfalls and inherent shortcomings of using online data for political analysis? In recent years, with the abundance of data, these questions, among others, have gained importance, especially in light of the global political turmoil and the upcoming 2016 US presidential election. We introduce relevant political science theory, describe the challenges within the framework of computational social science and present state of the art approaches bridging social network analysis, graph mining, and natural language processing.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"4b6be994cfafa3d04f9844b57dad8f2d81195d07","venue_1":"WSDM","year":"2013","title":"Characterizing and supporting cross-device search tasks","authors":"Yu Wang, Xiao Huang, Ryen W. White","author_ids":"1682675, 5540503, 1734415","abstract":"Web searchers frequently transition from desktop computers and laptops to mobile devices, and vice versa. Little is known about the nature of cross-device search tasks, yet they represent an important opportunity for search engines to help their users, especially those on the target (post-switch) device. For example, the search engine could save the current session and re-instate it post switch, or it could capitalize on down-time between devices to proactively re-trieve content on behalf of the searcher. In this paper, we present a log-based study to define and characterize cross-device search be-havior and predict the resumption of cross-device tasks. Using data from a large commercial search engine, we show that there are dis-cernible and noteworthy patterns of search behavior associated with device transitions. We also develop learned models for predicting task resumption on the target device using behavioral, topical, geo-spatial, and temporal features. Our findings show that our models can attain strong prediction accuracy and have direct implications for the development of tools to help people search more effectively in a multi-device world.","cites":"8","conferencePercentile":"42.93478261"},{"venue":"WSDM","id":"704fc1d5a602ea01da1288c85ded799fae3bf99e","venue_1":"WSDM","year":"2010","title":"Revisiting globally sorted indexes for efficient document retrieval","authors":"Fan Zhang, Shuming Shi, Hao Yan, Ji-Rong Wen","author_ids":"1726798, 2819530, 3227933, 3120972","abstract":"There has been a large amount of research on efficient document retrieval in both IR and web search areas. One important technique to improve retrieval efficiency is early termination, which speeds up query processing by avoiding scanning the entire inverted lists. Most early termination techniques first build new inverted indexes by sorting the inverted lists in the order of either the term-dependent information, e.g., term frequencies or term IR scores, or the term-independent information, e.g., static rank of the document; and then apply appropriate retrieval strategies on the resulting indexes. Although the methods based only on the static rank have been shown to be ineffective for the early termination, there are still many advantages of using the methods based on term-independent information. In this paper, we propose new techniques to organize inverted indexes based on the term-independent information beyond static rank and study the new retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Our results on the TREC GOV and GOV2 data sets show that our techniques can improve query efficiency significantly.","cites":"13","conferencePercentile":"16.66666667"},{"venue":"WSDM","id":"cb2d40cf9da942ef6caea22f21378fa4df3a0dc8","venue_1":"WSDM","year":"2008","title":"Preferential behavior in online groups","authors":"Lars Backstrom, Ravi Kumar, Cameron Marlow, Jasmine Novak, Andrew Tomkins","author_ids":"2612372, 3455556, 5361584, 2891737, 1779903","abstract":"Online communities in the form of message boards, listservs, and newsgroups continue to represent a considerable amount of the social activity on the Internet. Every year thousands of groups ourish while others decline into relative obscurity; likewise, millions of members join a new community every year, some of whom will come to manage or moderate the conversation while others simply sit by the sidelines and observe. These processes of group formation, growth, and dissolution are central in social science, and in an online venue they have ramifications for the design and development of community software\n In this paper we explore a large corpus of thriving online communities. These groups vary widely in size, moderation and privacy, and cover an equally diverse set of subject matter. We present a broad range of descriptive statistics of these groups. Using metadata from groups, members, and individual messages, we identify users who post and are replied-to frequently by multiple group members; we classify these high-engagement users based on the longevity of their engagements. We show that users who will go on to become long-lived, highly-engaged users experience significantly better treatment than other users from the moment they join the group, well before there is an opportunity for them to develop a long-standing relationship with members of the group\n We present a simple model explaining long-term heavy engagement as a combination of user-dependent and group-dependent factors. Using this model as an analytical tool, we show that properties of the user alone are sufficient to explain 95% of all memberships, but introducing a small amount of per-group information dramatically improves our ability to model users belonging to multiple groups.","cites":"40","conferencePercentile":"54"},{"venue":"WSDM","id":"29efbdf3f95cee97405accafdebd3bd374f1f003","venue_1":"WSDM","year":"2011","title":"Supervised random walks: predicting and recommending links in social networks","authors":"Lars Backstrom, Jure Leskovec","author_ids":"2612372, 1702139","abstract":"Predicting the occurrence of links is a fundamental problem in networks. In the link prediction problem we are given a snapshot of a network and would like to infer which interactions among existing members are likely to occur in the near future or which existing interactions are we missing. Although this problem has been extensively studied, the challenge of how to effectively combine the information from the network structure with rich node and edge attribute data remains largely open.\n We develop an algorithm based on <i>Supervised Random Walks</i> that naturally combines the information from the network structure with node and edge level attributes. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function.\n Our experiments on the Facebook social graph and large collaboration networks show that our approach outperforms state-of-the-art unsupervised approaches as well as approaches that are based on feature extraction.","cites":"302","conferencePercentile":"97.75280899"},{"venue":"WSDM","id":"2dfe4aac9b60b3fbd1d29fd17439359baa0fb758","venue_1":"WSDM","year":"2011","title":"Identifying topical authorities in microblogs","authors":"Aditya Pal, Scott Counts","author_ids":"1908227, 1721345","abstract":"Content in microblogging systems such as Twitter is produced by tens to hundreds of millions of users. This diversity is a notable strength, but also presents the challenge of finding the most interesting and authoritative authors for any given topic. To address this, we first propose a set of features for characterizing social media authors, including both nodal and topical metrics. We then show how probabilistic clustering over this feature space, followed by a within-cluster ranking procedure, can yield a final list of top authors for a given topic. We present results across several topics, along with results from a user study confirming that our method finds authors who are significantly more interesting and authoritative than those resulting from several baseline conditions. Additionally our algorithm is computationally feasible in near real-time scenarios making it an attractive alternative for capturing the rapidly changing dynamics of microblogs.","cites":"110","conferencePercentile":"92.13483146"},{"venue":"WSDM","id":"b5944b1b5834e3e135badc4f0b7ba616441c4849","venue_1":"WSDM","year":"2013","title":"GOP primary season on twitter: \"popular\" political sentiment in social media","authors":"Yelena Mejova, Padmini Srinivasan, Bob Boynton","author_ids":"3329961, 2072078, 2381124","abstract":"As mainstream news media and political campaigns start to pay attention to the political discourse online, a systematic analysis of political speech in social media becomes more critical. What exactly do people say on these sites, and how useful is this data in estimating political popularity? In this study we examine Twitter discussions surrounding seven US Republican politicians who were running for the US Presidential nomination in 2011. We show this largely negative rhetoric to be laced with sarcasm and humor and dominated by a small portion of users. Furthermore, we show that using out-of-the-box classification tools results in a poor performance, and instead develop a highly optimized multi-stage approach designed for general-purpose political sentiment classification. Finally, we compare the change in sentiment detected in our dataset before and after 19 Republican debates, concluding that, at least in this case, the Twitter political chatter is not indicative of national political polls.","cites":"14","conferencePercentile":"60.86956522"},{"venue":"WSDM","id":"2b8307abcc48adfeea85f2185ce01c0f813dcd51","venue_1":"WSDM","year":"2011","title":"A comparative analysis of cascade measures for novelty and diversity","authors":"Charles L. A. Clarke, Nick Craswell, Ian Soboroff, Azin Ashkan","author_ids":"1751287, 1703980, 1783237, 1715458","abstract":"Traditional editorial effectiveness measures, such as nDCG, remain standard for Web search evaluation. Unfortunately, these traditional measures can inappropriately reward redundant information and can fail to reflect the broad range of user needs that can underlie a Web query. To address these deficiencies, several researchers have recently proposed effectiveness measures for novelty and diversity. Many of these measures are based on simple <i>cascade</i> models of user behavior, which operate by considering the relationship between successive elements of a result list. The properties of these measures are still poorly understood, and it is not clear from prior research that they work as intended. In this paper we examine the properties and performance of cascade measures with the goal of validating them as tools for measuring effectiveness. We explore their commonalities and differences, placing them in a unified framework; we discuss their theoretical difficulties and limitations, and compare the measures experimentally, contrasting them against traditional measures and against other approaches to measuring novelty. Data collected by the TREC 2009 Web Track is used as the basis for our experimental comparison. Our results indicate that these measures reward systems that achieve an balance between novelty and overall precision in their result lists, as intended. Nonetheless, other measures provide insights not captured by the cascade measures, and we suggest that future evaluation efforts continue to report a variety of measures.","cites":"64","conferencePercentile":"86.51685393"},{"venue":"WSDM","id":"2f472bade0005b21d575fd91cd7a2d39020dc1ad","venue_1":"WSDM","year":"2009","title":"Measuring the similarity between implicit semantic relations using web search engines","authors":"Danushka Bollegala, Yutaka Matsuo, Mitsuru Ishizuka","author_ids":"2720656, 1692267, 1687719","abstract":"Measuring the similarity between implicit semantic relations is an important task in information retrieval and natural language processing. For example, consider the situation where you know an entity-pair (e.g. <i>Google, YouTube</i>), between which a particular relation holds (e.g. acquisition), and you are interested in retrieving other entity-pairs for which the same relation holds (e.g. <i>Yahoo, Inktomi</i>). Existing keyword-based search engines cannot be directly applied in this case because in keyword-based search, the goal is to retrieve documents that are relevant to the words used in the query -- not necessarily to the relations implied by a pair of words. Accurate measurement of relational similarity is an important step in numerous natural language processing tasks such as identification of word analogies, and classification of noun-modifier pairs. We propose a method that uses Web search engines to efficiently compute the relational similarity between two pairs of words. Our method consists of three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different semantic relations implied by them, and measuring the similarity between different semantic relations using an inter-cluster correlation matrix. We propose a pattern extraction algorithm to extract a large number of lexical patterns that express numerous semantic relations. We then present an efficient clustering algorithm to cluster the extracted lexical patterns. Finally, we measure the relational similarity between word-pairs using inter-cluster correlation. We evaluate the proposed method in a relation classification task. Experimental results on a dataset covering multiple relation types show a statistically significant improvement over the current state-of-the-art relational similarity measures.","cites":"6","conferencePercentile":"21.05263158"},{"venue":"WSDM","id":"00e2dee9f00f96eb690e4d52afd1cd7391b90e30","venue_1":"WSDM","year":"2012","title":"2nd international workshop on diversity in document retrieval (DDR 2012)","authors":"Craig MacDonald, Jun Wang, Charles L. A. Clarke","author_ids":"8467972, 1793911, 1751287","abstract":"When an ambiguous query is received, a sensible approach is for the information retrieval (IR) system to diversify the results retrieved for this query, in the hope that at least one of the interpretations of the query intent will satisfy the user. Diversity is an increasingly important topic, of interest to both academic researchers (such as participants in the TREC Web and Blog track diversity tasks, or the NTCIR INTENT task), as well as to search engines professionals. In the 2nd edition of the Diversity in Document Retrieval workshop (DDR 2012), we solicited submissions both on approaches and models for diversity, the evaluation of diverse search results, and on applications of diverse search results. This workshop builds upon a successful 1st edition of DDR which was held at ECIR 2011 in Dublin, Ireland.","cites":"1","conferencePercentile":"11.04651163"},{"venue":"WSDM","id":"2a3e10b0ff84f36ae642b9431c9be408703533c7","venue_1":"WSDM","year":"2013","title":"NCDawareRank: a novel ranking method that exploits the decomposable structure of the web","authors":"Athanasios N. Nikolakopoulos, John D. Garofalakis","author_ids":"2143706, 1680221","abstract":"Research about the topological characteristics of the hyperlink graph has shown that Web possesses a nested block structure, indicative of its innate hierarchical organization. This crucial observation opens the way for new approaches that can usefully regard Web as a Nearly Completely Decomposable(NCD) system; In recent years, such approaches gave birth to various efficient methods and algorithms that exploit NCD from a computational point of view and manage to considerably accelerate the extraction of the PageRank vector. However, very little have been done towards the qualitative exploitation of NCD.\n In this paper we propose NCDawareRank, a novel ranking method that uses the intuition behind NCD to generalize and refine PageRank. NCDawareRank considers both the link structure and the hierarchical nature of the Web in a way that preserves the mathematically attractive characteristics of PageRank and at the same time manages to successfully resolve many of its known problems, including Web Spamming Susceptibility and Biased Ranking of Newly Emerging Pages. Experimental results show that NCDawareRank is more resistant to direct manipulation, alleviates the problems caused by the sparseness of the link graph and assigns more reasonable ranking scores to newly added pages, while maintaining the ability to be easily implemented on a large-scale and in a computationally efficient manner.","cites":"6","conferencePercentile":"33.15217391"},{"venue":"WSDM","id":"5586746dbfa25057371d7c83dfc9d51e1b462323","venue_1":"WSDM","year":"2012","title":"Evaluating search in personal social media collections","authors":"Chia-Jung Lee, W. Bruce Croft, Jinyoung Kim","author_ids":"2308054, 1736975, 1774427","abstract":"The prevalence of social media applications is generating potentially large personal archives of posts, tweets, and other communications. The existence of these archives creates a need for search tools, which can be seen as an extension of current desktop search services. Little is currently known about the best search techniques for personal archives of social data, because of the difficulty of creating test collections. In this paper, we describe how test collections for personal social data can be created by using games to collect queries. We then compare a range of retrieval models that exploit the semi-structured nature of social data. Our results show that a mixture of language models with field distribution estimation can be effective for this type of data, with certain fields, such as the name of the poster, being particularly important. We also analyze the properties of the queries that were generated by users with two versions of the games.","cites":"5","conferencePercentile":"20.34883721"},{"venue":"WSDM","id":"6e9f4b63eaae7705776fbee81f44aa36b264c28d","venue_1":"WSDM","year":"2013","title":"Characterizing and curating conversation threads: Expansion, focus, volume, re-entry","authors":"Lars Backstrom, Jon M. Kleinberg, Lillian Lee, Cristian Danescu-Niculescu-Mizil","author_ids":"2612372, 3371403, 1736369, 1774019","abstract":"Discussion threads form a central part of the experience on many Web sites, including social networking sites such as Facebook and Google Plus and knowledge creation sites such as Wikipedia. To help users manage the challenge of allocating their attention among the discussions that are relevant to them, there has been a growing need for the algorithmic curation of on-line conversations --- the development of automated methods to select a subset of discussions to present to a user.\n Here we consider two key sub-problems inherent in conversational curation: length prediction --- predicting the number of comments a discussion thread will receive --- and the novel task of re-entry prediction --- predicting whether a user who has participated in a thread will later contribute another comment to it. The first of these sub-problems arises in estimating how interesting a thread is, in the sense of generating a lot of conversation; the second can help determine whether users should be kept notified of the progress of a thread to which they have already contributed. We develop and evaluate a range of approaches for these tasks, based on an analysis of the network structure and arrival pattern among the participants, as well as a novel dichotomy in the structure of long threads. We find that for both tasks, learning-based approaches using these sources of information.","cites":"34","conferencePercentile":"88.58695652"},{"venue":"WSDM","id":"4f5da510d64fdfe4706adbdfe8a63033eac1f030","venue_1":"WSDM","year":"2012","title":"Collaborative information seeking: understanding users, systems, and content","authors":"Chirag Shah","author_ids":"1727519","abstract":"The course will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. Traditionally, IR is considered an individual pursuit, and not surprisingly, the majority of tools, techniques, and models developed for addressing information need, retrieval, and usage have focused on single users. The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past. This course will introduce such works to the students, with an emphasis on understanding models and systems that support collaborative search or browsing. In addition, the course will provide samples of data collected through several experiments to demonstrate various mining and analysis techniques. \n Specifically, the course will (1) outline the research and latest developments in the field of collaborative IR, (2) list the challenges for designing and evaluating collaborative IR systems, and (3) show how traditional single user IR models and systems could be mapped to those for CIS. This will be achieved through introduction to appropriate literature, algorithms and interfaces that facilitate CIS, and methodologies for studying and evaluating them. Thus, the course will offer a balance between theoretical and practical elements of CIS.","cites":"1","conferencePercentile":"11.04651163"},{"venue":"WSDM","id":"82b6b865756ce595dc08420ebc0d88739005d185","venue_1":"WSDM","year":"2014","title":"Data design for personalization: current challenges and emerging opportunities","authors":"Elizabeth F. Churchill, Atish Das Sarma","author_ids":"1801572, 2541992","abstract":"There are several definitions of personalization but one that relates specifically to internet technologies is the following: <i>Personalization technology enables the dynamic insertion, customization or suggestion of content in any format that is relevant to the individual user, based on the user's implicit behavior and preferences, and explicitly given details.</i>\n Personalization is central to most Internet experiences. Personalization is a data-driven process, whether the data are explicitly gathered (e.g., by asking people to fill out forms) or implicitly (e.g. through analysis of behavioral data).\n It is clear that designing for effective personalization poses interesting engineering and computer science challenges. However, personalization is also a user experience issue. We believe that encouraging dialogue and collaboration between data mining experts, content providers, and user-focused researchers will offer gains in the area of personalization for search and for other domains. This is increasingly the case as devices enable more forms of data to be gathered, are always on/connected and are always with users. This workshop brings researchers interested in the area of personalization to share their research, explore possibilities for collaboration, and work on defining an agenda for Data Design for Personalization.","cites":"0","conferencePercentile":"7.692307692"},{"venue":"WSDM","id":"6d9c1a197304dd8a374869395631c70950b2cd8f","venue_1":"WSDM","year":"2013","title":"Balanced label propagation for partitioning massive graphs","authors":"Johan Ugander, Lars Backstrom","author_ids":"1760812, 2612372","abstract":"Partitioning graphs at scale is a key challenge for any application that involves distributing a graph across disks, machines, or data centers. Graph partitioning is a very well studied problem with a rich literature, but existing algorithms typically can not scale to billions of edges, or can not provide guarantees about partition sizes.\n In this work we introduce an efficient algorithm, balanced label propagation, for precisely partitioning massive graphs while greedily maximizing edge locality, the number of edges that are assigned to the same shard of a partition. By combining the computational efficiency of label propagation --- where nodes are iteratively relabeled to the same 'label' as the plurality of their graph neighbors --- with the guarantees of constrained optimization --- guiding the propagation by a linear program constraining the partition sizes --- our algorithm makes it practically possible to partition graphs with billions of edges.\n Our algorithm is motivated by the challenge of performing graph predictions in a distributed system. Because this requires assigning each node in a graph to a physical machine with memory limitations, it is critically necessary to ensure the resulting partition shards do not overload any single machine.\n We evaluate our algorithm for its partitioning performance on the Facebook social graph, and also study its performance when partitioning Facebook's 'People You May Know' service (PYMK), the distributed system responsible for the feature extraction and ranking of the friends-of-friends of all active Facebook users. In a live deployment, we observed average query times and average network traffic levels that were 50.5% and 37.1% (respectively) when compared to the previous naive random sharding.","cites":"43","conferencePercentile":"91.30434783"},{"venue":"WSDM","id":"a4c5bdcd71de84ef495df661831fa09d63b95aec","venue_1":"WSDM","year":"2016","title":"Second Workshop on Search and Exploration of X-Rated Information (SEXI'16): WSDM Workshop Summary","authors":"Vanessa Murdock, Charles L. A. Clarke, Jaap Kamps, Jussi Karlgren","author_ids":"1684660, 1751287, 1753628, 1742359","abstract":"Adult content is pervasive on the web, has been a driving factor in the adoption of the Internet medium, and is responsible for a significant fraction of traffic and revenues, yet rarely attracts attention in research. The research questions surrounding adult content access behaviors are unique, and interesting and valuable research in this area can be done ethically. WSDM 2016 features a half day workshop on Search and Exploration of X-Rated Information (SEXI) for information access tasks related to adult content. While the scope of the workshop remains broad, special attention is devoted to the privacy and security issues surrounding adult content by inviting keynote speakers with extensive experience on these topics. The recent release of the personal data belonging to customers of the adult dating site Ashley Madison provides a timely context for the focus on privacy and security.","cites":"1","conferencePercentile":"52.15053763"},{"venue":"WSDM","id":"bcb4e1e12d715060ee7c83e5424657dd401be4a2","venue_1":"WSDM","year":"2016","title":"Serving a Billion Personalized News Feeds","authors":"Lars Backstrom","author_ids":"2612372","abstract":"Feed ranking's goal is to provide perople with over a billion personalized experiences. We strive to provide the most compelling content to each person, personalized to them so that they are most likely to see the content that is most interesting to them.\n Similar to a newspaper, putting the right stories above the fold has always been critical to engaging customers and interesting them in the rest of the paper. In feed ranking, we face a similar challenge, but on a grander scale. Each time a person visits, we need to find the best piece of content out of all the available stories and put it at the top of feed where people are most likely to see it. To accomplish this, we do large-scale machine learning to model each person, figure out which friends, pages and topics they care about and pick the stories each particular person is interested in. In addition to the large-scale machine learning problems we work on, another primary area of research is understanding the value we are creating for people and making sure that our objective function is in alignment with what people want.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"6f7c0ca6643840f38f44a8983fed4b99469d169e","venue_1":"WSDM","year":"2013","title":"Optimizing parallel algorithms for all pairs similarity search","authors":"Maha Alabduljalil, Xun Tang, Tao Yang","author_ids":"1866878, 3276980, 1720792","abstract":"All pairs similarity search is used in many web search and data mining applications. Previous work has used comparison filtering, inverted indexing, and parallel accumulation of partial intermediate results to expedite its execution. However, shuffling intermediate results can incur significant communication overhead as data scales up. This paper studies a scalable two-step approach called Partition-based Similarity Search (PSS) which incorporates several optimization techniques. First, PSS uses a static partitioning algorithm that places dissimilar vectors into different groups and balance the comparison workload with a circular assignment. Second, PSS executes comparison tasks in parallel, each using a hybrid data structure that combines the advantages of forward and inverted indexing. Our evaluation results show that the proposed approach leads to an early elimination of unnecessary I/O and data communication while sustaining parallel efficiency. As a result, it improves performance by an order of magnitude when dealing with large datasets.","cites":"8","conferencePercentile":"42.93478261"},{"venue":"WSDM","id":"21a0f88ba4c4481bb31f683376bbdc6c87986b02","venue_1":"WSDM","year":"2014","title":"Detecting non-gaussian geographical topics in tagged photo collections","authors":"Christoph Carl Kling, Jérôme Kunegis, Sergej Sizov, Steffen Staab","author_ids":"2267136, 1697769, 1704989, 1752093","abstract":"Nowadays, large collections of photos are tagged with GPS coordinates. The modelling of such large geo-tagged corpora is an important problem in data mining and information retrieval, and involves the use of geographical information to detect topics with a spatial component. In this paper, we propose a novel geographical topic model which captures dependencies between geographical regions to support the detection of topics with complex, non-Gaussian distributed spatial structures. The model is based on a multi-Dirichlet process (MDP), a novel generalisation of the hierarchical Dirichlet process extended to support multiple base distributions. Our method thus is called the MDP-based geographical topic model (MGTM). We show how to use a MDP to dynamically smooth topic distributions between groups of spatially adjacent documents. In systematic quantitative and qualitative evaluations using independent datasets from prior related work, we show that such a model can exploit the adjacency of regions and leads to a significant improvement in the quality of topics compared to the state of the art in geographical topic modelling.","cites":"10","conferencePercentile":"76.28205128"},{"venue":"WSDM","id":"4c4280c16ae3680c2cac3052916f095e815e0aa2","venue_1":"WSDM","year":"2012","title":"Tips, dones and todos: uncovering user profiles in foursquare","authors":"Marisa A. Vasconcelos, Saulo M. R. Ricci, Jussara M. Almeida, Fabrício Benevenuto, Virgílio A. F. Almeida","author_ids":"2632256, 1858335, 8118988, 2810330, 7360316","abstract":"Online Location Based Social Networks (LBSNs), which combine social network features with geographic information sharing, are becoming increasingly popular. One such application is Foursquare, which doubled its user population in less than six months. Among other features, Foursquare allows users to leave tips (i.e., reviews or recommendations) at specific venues as well as to give feedback on previously posted tips by adding them to their to-do lists or marking them as done. In this paper, we analyze how Foursquare users exploit these three features - tips, dones and to-dos - uncovering different behavior profiles. Our study reveals the existence of very active and influential users, some of which are famous businesses and brands, that seem engaged in posting tips at a large variety of venues while also receiving a great amount of user feedback on them. We also provide evidence of spamming, showing the existence of users that post tips whose contents are unrelated to the nature or domain of the venue where the tips were left.","cites":"34","conferencePercentile":"78.48837209"},{"venue":"WSDM","id":"3166341641b41b22da4ce21bbe9abf128ca5e403","venue_1":"WSDM","year":"2011","title":"LambdaMerge: merging the results of query reformulations","authors":"Daniel Sheldon, Milad Shokouhi, Martin Szummer, Nick Craswell","author_ids":"1777803, 1703337, 1778989, 1703980","abstract":"Search engines can automatically reformulate user queries in a variety of ways, often leading to multiple queries that are candidates to replace the original. However, selecting a replacement can be risky: a reformulation may be more effective than the original or significantly worse, depending on the nature of the query, the source of reformulation candidates, and the corpus. In this paper, we explore methods to mitigate this risk by issuing several versions of the query (including the original) and merging their results. We focus on reformulations generated by random walks on the click graph, a method that can produce very good reformulations but is also variable and prone to topic drift. Our primary contribution is &#955;-Merge, a supervised merging method that is trained to directly optimize a retrieval metric (such as NDCG or MAP) using features that describe both the reformulations and the documents they return. In experiments on Bing data and GOV2, &#955;-Merge outperforms the original query and several unsupervised merging methods. &#955;-Merge also outperforms a supervised method to predict and select the best single formulation, and is competitive with an oracle that <i>always</i> selects the best formulation.","cites":"36","conferencePercentile":"75.28089888"},{"venue":"WSDM","id":"0120c11fd84d66d208953e333d549dec7c97a27a","venue_1":"WSDM","year":"2009","title":"The web changes everything: understanding the dynamics of web content","authors":"Eytan Adar, Jaime Teevan, Susan T. Dumais, Jonathan L. Elsas","author_ids":"2630700, 1691357, 1728602, 2507380","abstract":"The Web is a dynamic, ever changing collection of information. This paper explores changes in Web content by analyzing a crawl of 55,000 Web pages, selected to represent different user visitation patterns. Although change over long intervals has been explored on random (and potentially unvisited) samples of Web pages, little is known about the nature of finer grained changes to pages that are actively consumed by users, such as those in our sample. We describe algorithms, analyses, and models for characterizing changes in Web content, focusing on both time (by using hourly and sub-hourly crawls) and structure (by looking at page-, DOM-, and term-level changes). Change rates are higher in our behavior-based sample than found in previous work on randomly sampled pages, with a large portion of pages changing more than hourly. Detailed content and structure analyses identify stable and dynamic content within each page. The understanding of Web change we develop in this paper has implications for tools designed to help people interact with dynamic Web content, such as search engines, advertising, and Web browsers.","cites":"65","conferencePercentile":"84.21052632"},{"venue":"WSDM","id":"4225fa81add538e2c90d5f1855bd855185878667","venue_1":"WSDM","year":"2011","title":"Mining social images with distance metric learning for automated image tagging","authors":"Pengcheng Wu, Steven C. H. Hoi, Peilin Zhao, Ying He","author_ids":"8327053, 1741126, 1714894, 1734129","abstract":"With the popularity of various social media applications, massive social images associated with high quality tags have been made available in many social media web sites nowadays. Mining social images on the web has become an emerging important research topic in web search and data mining. In this paper, we propose a machine learning framework for mining social images and investigate its application to automated image tagging. To effectively discover knowledge from social images that are often associated with multimodal contents (including visual images and textual tags), we propose a novel Unified Distance Metric Learning (UDML) scheme, which not only exploits both visual and textual contents of social images, but also effectively unifies both inductive and transductive metric learning techniques in a systematic learning framework. We further develop an efficient stochastic gradient descent algorithm for solving the UDML optimization task and prove the convergence of the algorithm. By applying the proposed technique to the automated image tagging task in our experiments, we demonstrate that our technique is empirically effective and promising for mining social images towards some real applications.","cites":"26","conferencePercentile":"64.04494382"},{"venue":"WSDM","id":"730531d7f3b3ae28f861534fe822c4d034c08223","venue_1":"WSDM","year":"2012","title":"No search result left behind: branching behavior with browser tabs","authors":"Jeff Huang, Thomas Lin, Ryen W. White","author_ids":"3404131, 2782360, 1734415","abstract":"Today's Web browsers allow users to open links in new windows or tabs. This action, which we call 'branching', is sometimes performed on search results when the user plans to eventually visit multiple results. We detect branching behavior on a large commercial search engine with a client-side script on the results page. Two-fifths of all users spawned new tabs on search results in the timeframe of our study; branching usage varied with different query types and vertical. Both branching and backtracking are viable methods for visiting multiple search results. To understand user search strategies, we treat multiple result clicks following a query as ordered events to understand user search strategies. Users branching in a query are more likely to click search results from top to bottom, while users who backtrack are less likely to do so; this is especially true for queries involving more than two clicks. These findings inform an experiment in which we take a popular click model and modify it to account for the differing user behavior when branching. By understanding that users continue examining search results before viewing a branched result, we can improve the click model for branching queries.","cites":"15","conferencePercentile":"47.09302326"},{"venue":"WSDM","id":"36460a9cf3ad857e767130620f820be86f57ffc6","venue_1":"WSDM","year":"2013","title":"Playing by the rules: mining query associations to predict search performance","authors":"Youngho Kim, Ahmed Hassan Awadallah, Ryen W. White, Yi-Min Wang","author_ids":"4422730, 2461958, 1734415, 4374095","abstract":"Understanding the characteristics of queries where a search engine is failing is important for improving engine performance. Previous work largely relies on user-interaction features (e.g., clickthrough statistics) to identify such underperforming queries. However, relying on interaction behavior means that searchers need to become dissatisfied and need to exhibit that in their search behavior, by which point it may be too late to help them. In this paper, we propose a method to generate underperforming query identification rules instantly using topical and lexical attributes. The method first generates query attributes using sources such as topics, concepts (entities), and keywords in queries. Then, association rules are learned by exploiting the FP-growth algorithm and decision trees using underperforming query examples. We develop a query classification model capable of accurately estimating dissatisfaction using the generated rules, and demonstrate significant performance gains over state-of-the-art query performance prediction models.","cites":"8","conferencePercentile":"42.93478261"},{"venue":"WSDM","id":"f33cb88a87d2077953f682a6d4fcd57d7149562c","venue_1":"WSDM","year":"2015","title":"Toward Predicting the Outcome of an A/B Experiment for Search Relevance","authors":"Lihong Li, Jinyoung Kim, Imed Zitouni","author_ids":"1811156, 1774427, 1954563","abstract":"A standard approach to estimating online click-based metrics of a ranking function is to run it in a controlled experiment on live users. While reliable and popular in practice, configuring and running an online experiment is cumbersome and time-intensive. In this work, inspired by recent successes of offline evaluation techniques for recommender systems, we study an alternative that uses historical search log to reliably predict online click-based metrics of a \\emph{new} ranking function, without actually running it on live users. To tackle novel challenges encountered in Web search, variations of the basic techniques are proposed. The first is to take advantage of diversified behavior of a search engine over a long period of time to simulate randomized data collection, so that our approach can be used at very low cost. The second is to replace exact matching (of recommended items in previous work) by \\emph{fuzzy} matching (of search result pages) to increase data efficiency, via a better trade-off of bias and variance. Extensive experimental results based on large-scale real search data from a major commercial search engine in the US market demonstrate our approach is promising and has potential for wide use in Web search.","cites":"9","conferencePercentile":"76.2295082"},{"venue":"WSDM","id":"fe740c69e6d88e37e9729b3b06564a41ae96353c","venue_1":"WSDM","year":"2008","title":"Web information management: past, present and future","authors":"Hector Garcia-Molina","author_ids":"1695250","abstract":"In this talk I will give a brief retrospective on Web Information Management, and will discuss some of the key challenges for the future. I will <i>not</i> give a survey of all work in the area; instead I will give my personal perspective based on work in the InfoLab at Stanford. In particular, I will touch on our lab's work on crawling, indexing, ranking, personalization, and more recently on spam detection and social networking.","cites":"2","conferencePercentile":"4"},{"venue":"WSDM","id":"4aeb57256f65e72a6c23be9d339afb80087b3f6b","venue_1":"WSDM","year":"2012","title":"Extracting search-focused key n-grams for relevance ranking in web search","authors":"Chen Wang, Keping Bi, Yunhua Hu, Hang Li, Guihong Cao","author_ids":"1710899, 2112104, 7741830, 3701964, 3320836","abstract":"In web search, relevance ranking of popular pages is relatively easy, because of the inclusion of strong signals such as anchor text and search log data. In contrast, with less popular pages, relevance ranking becomes very challenging due to a lack of information. In this paper the former is referred to as head pages, and the latter tail pages. We address the challenge by learning a model that can extract search-focused key n-grams from web pages, and using the key n-grams for searches of the pages, particularly, the tail pages. To the best of our knowledge, this problem has not been previously studied. Our approach has four characteristics. First, key n-grams are search-focused in the sense that they are defined as those which can compose \"good queries\" for searching the page. Second, key n-grams are learned in a relative sense using learning to rank techniques. Third, key n-grams are learned using search log data, such that the characteristics of key n-grams in the search log data, particularly in the heads; can be applied to the other data, particularly to the tails. Fourth, the extracted key n-grams are used as features of the relevance ranking model also trained with learning to rank techniques. Experiments validate the effectiveness of the proposed approach with large-scale web search datasets. The results show that our approach can significantly improve relevance ranking performance on both heads and tails; and particularly tails, compared with baseline approaches. Characteristics of our approach have also been fully investigated through comprehensive experiments.","cites":"5","conferencePercentile":"20.34883721"},{"venue":"WSDM","id":"5e68847021c334cfa0b233c67b57f5a0eeb18a4b","venue_1":"WSDM","year":"2015","title":"The 2nd workshop on Vertical Search Relevance at WSDM 2015","authors":"Dawei Yin, Chih-Chieh Hung, Rui Li, Yi Chang","author_ids":"2115608, 2508700, 1704992, 1787097","abstract":"As the web information exponentially grows and the needs of users become more specific, traditional general web search engines are not able to perfectly satisfy the nowadays user requirement. Vertical search engines have emerged in various domains, which more focus on specific segments of online content, including local, shopping, medical information, travel search, etc. Vertical search engines start attracting more attention while relevance ranking in different vertical search engines is becoming the key technology. In addition, vertical search results are often slotted into general Web search results. Hence, designing effective ranking functions for vertical search has become practically important to improve users' experience in both web search and vertical search. The workshop bring together researchers from IR, ML, NLP, and other areas of computer and information science, who are working on or interested in this area. It provides a forum for the researchers to identify the issues and the challenges, to share their latest research results, to express a diverse range of opinions about this topic, and to discuss future directions.","cites":"0","conferencePercentile":"13.1147541"},{"venue":"WSDM","id":"0f6e59032e88bcba4e78abd97d55ac27336b0036","venue_1":"WSDM","year":"2011","title":"Topical semantics of twitter links","authors":"Michael J. Welch, Uri Schonfeld, Dan He, Junghoo Cho","author_ids":"1874681, 1924339, 1773550, 2581979","abstract":"Twitter, a micro-blogging platform with an estimated 20 million unique monthly visitors and over 100 million registered users, offers an abundance of rich, structured data at a rate exceeding 600 tweets per second. Recent efforts to leverage this social data to rank users by quality and topical relevance have largely focused on the \"follow\" relationship. Twitter's data offers additional implicit relationships between users, however, such as \"retweets\" and \"mentions\". In this paper we investigate the semantics of the follow and retweet relationships. Specifically, we show that the transitivity of topical relevance is better preserved over retweet links, and that retweeting a user is a significantly stronger indicator of topical interest than following him. We demonstrate these properties by ranking users with two variants of the PageRank algorithm; one based on the follows sub-graph and one based on the implicit retweet sub-graph. We perform a user study to assess the topical relevance of the resulting top-ranked users.","cites":"38","conferencePercentile":"77.52808989"},{"venue":"WSDM","id":"3b3a6a725d7ce70a01046bc66d979b9b35f4e882","venue_1":"WSDM","year":"2010","title":"I tag, you tag: translating tags for advanced user models","authors":"Robert Wetzker, Carsten Zimmermann, Christian Bauckhage, Sahin Albayrak","author_ids":"3184596, 2219425, 1692283, 1722170","abstract":"Collaborative tagging services (folksonomies) have been among the stars of the Web 2.0 era. They allow their users to label diverse resources with freely chosen keywords (tags). Our studies of two real-world folksonomies unveil that individual users develop highly personalized vocabularies of tags. While these meet individual needs and preferences, the considerable differences between personal tag vocabularies (personomies) impede services such as social search or customized tag recommendation. In this paper, we introduce a novel user-centric tag model that allows us to derive mappings between personal tag vocabularies and the corresponding folksonomies. Using these mappings, we can infer the meaning of user-assigned tags and can predict choices of tags a user may want to assign to new items. Furthermore, our translational approach helps in reducing common problems related to tag ambiguity, synonymous tags, or multilingualism. We evaluate the applicability of our method in tag recommendation and tag-based social search. Extensive experiments show that our translational model improves the prediction accuracy in both scenarios.","cites":"28","conferencePercentile":"40"},{"venue":"WSDM","id":"68bc7e61ab0ffe341cf44aaed6a077447f9c1f80","venue_1":"WSDM","year":"2013","title":"Anomaly, event, and fraud detection in large network datasets","authors":"Leman Akoglu, Christos Faloutsos","author_ids":"3255268, 1702392","abstract":"Detecting anomalies and events in data is a vital task, with numerous applications in security, finance, health care, law enforcement, and many others. While many techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points, with graph data becoming ubiquitous, techniques for structured graph data have been of focus recently. As objects in graphs have long-range correlations, novel technology has been developed for abnormality detection in graph data.\n The goal of this tutorial is to provide a general, comprehensive overview of the state-of-the-art methods for anomaly, event, and fraud detection in data represented as graphs. As a key contribution, we provide a thorough exploration of both data mining and machine learning algorithms for these detection tasks. We give a general framework for the algorithms, categorized under various settings: unsupervised vs.(semi-)supervised, for static vs. dynamic data. We focus on the scalability and effectiveness aspects of the methods, and highlight results on crucial real-world applications, including accounting fraud and opinion spam detection.","cites":"9","conferencePercentile":"48.91304348"},{"venue":"WSDM","id":"1bbc1ef48ef36f11f3b2c0bbad845374436429f7","venue_1":"WSDM","year":"2015","title":"Listwise Approach for Rank Aggregation in Crowdsourcing","authors":"Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, Lei Yu, Guoping Long","author_ids":"2944201, 2232327, 1777025, 1978584, 1739927, 3191589","abstract":"Inferring a gold-standard ranking over a set of objects, such as documents or images, is a key task to build test collections for various applications like Web search and recommender systems. Crowdsourcing services provide an efficient and inexpensive way to collect judgments via labeling by sets of annotators. We thus study the problem of finding a consensus ranking from crowdsourced judgments. In contrast to conventional rank aggregation methods which minimize the distance between predicted ranking and input judgments from either pointwise or pairwise perspective, we argue that it is critical to consider the distance in a listwise way to emphasize the position importance in ranking. Therefore, we introduce a new listwise approach in this paper, where ranking measure based objective functions are utilized for optimization. In addition, we also incorporate the annotator quality into our model since the reliability of annotators can vary significantly in crowdsourcing. For optimization, we transform the optimization problem to the Linear Sum Assignment Problem, and then solve it by a very efficient algorithm named CrowdAgg guaranteeing the optimal solution. Experimental results on two benchmark data sets from different crowdsourcing tasks show that our algorithm is much more effective, efficient and robust than traditional methods.","cites":"1","conferencePercentile":"33.60655738"},{"venue":"WSDM","id":"c1885303abec299c028b86208eb02f71540ac436","venue_1":"WSDM","year":"2014","title":"Democracy is good for ranking: towards multi-view rank learning and adaptation in web search","authors":"Wei Gao, Pei Yang","author_ids":"1698396, 3294988","abstract":"Web search ranking models are learned from features originated from different views or perspectives of document relevancy, such as query dependent or independent features. This seems intuitively conformant to the principle of multi-view approach that leverages distinct complementary views to improve model learning. In this paper, we aim to obtain optimal separation of ranking features into non-overlapping subsets (i.e., views), and use such different views for rank learning and adaptation. We present a novel semi-supervised multi-view ranking model, which is then extended into an adaptive ranker for search domains where no training data exists. The core idea is to proactively strengthen view consistency (i.e., the consistency between different rankings each predicted by a distinct view-based ranker) especially when training and test data follow divergent distributions. For this purpose, we propose a unified framework based on listwise ranking scheme to mutually reinforce the view consistency of target queries and the appropriate weighting of source queries that act as prior knowledge. Based on LETOR and Yahoo Learning to Rank datasets, our method significantly outperforms some strong baselines including single-view ranking models commonly used and multi-view ranking models that do not impose view consistency on target data.","cites":"5","conferencePercentile":"51.92307692"},{"venue":"WSDM","id":"37c373dcb366062e088dc055d839eff6a1bcc543","venue_1":"WSDM","year":"2015","title":"Concept Graph Learning from Educational Data","authors":"Yiming Yang, Hanxiao Liu, Jaime G. Carbonell, Wanli Ma","author_ids":"1723413, 2391802, 1707929, 7884448","abstract":"This paper addresses an open challenge in educational data mining, i.e., the problem of using observed prerequisite relations among courses to learn a directed universal concept graph, and using the induced graph to predict unobserved prerequisite relations among a broader range of courses. This is particularly useful to induce prerequisite relations among courses from different providers (universities, MOOCs, etc.). We propose a new framework for inference within and across two graphs---at the course level and at the induced concept level---which we call Concept Graph Learning (CGL). In the training phase, our system projects the course-level links onto the concept space to induce directed concept links; in the testing phase, the concept links are used to predict (unobserved) prerequisite links for test-set courses within the same institution or across institutions. The dual mappings enable our system to perform an interlingua-style transfer learning, e.g. treating the concept graph as the interlingua, and inducing prerequisite links in a transferable manner across different universities. Experiments on our newly collected data sets of courses from MIT, Caltech, Princeton and CMU show promising results, including the viability of CGL for transfer learning.","cites":"13","conferencePercentile":"90.98360656"},{"venue":"WSDM","id":"04b2b364dfe51940c0ba971b5df83759b659f925","venue_1":"WSDM","year":"2008","title":"Fast learning of document ranking functions with the committee perceptron","authors":"Jonathan L. Elsas, Vitor R. Carvalho, Jaime G. Carbonell","author_ids":"2507380, 3226952, 1707929","abstract":"This paper presents a new variant of the perceptron algorithm using selective committee averaging (or voting). We apply this agorithm to the problem of learning ranking functions for document retrieval, known as the \"Learning to Rank\" problem. Most previous algorithms proposed to address this problem focus on minimizing the number of misranked document pairs in the training set. The committee perceptron algorithm improves upon existing solutions by biasing the final solution towards maximizing an arbitrary rank-based performance metrics. This method performs comparably or better than two state-of-the-art rank learning algorithms, and also provides significant training time improvements over those methods, showing over a 45-fold reduction in training time compared to ranking SVM","cites":"23","conferencePercentile":"36"},{"venue":"WSDM","id":"eece8973db1966befc52bd8949a64f28fdebf6d7","venue_1":"WSDM","year":"2015","title":"Modeling Website Popularity Competition in the Attention-Activity Marketplace","authors":"Bruno F. Ribeiro, Christos Faloutsos","author_ids":"1692244, 1702392","abstract":"How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace? This question is of great interest to academics, technologists, and financial investors alike. In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace, Hi5, Friendster, and Multiply to guide the design of a new popularity competition model. Our model provides new insights into what Nobel Laure- ate Herbert A. Simon called the \"marketplace of attention,\" which we recast as the attention-activity marketplace. Our model design is further substantiated by user-level activity of 250,000 MySpace users obtained between 2004 and 2009. The resulting model not only accurately fits the observed Daily Active Users (DAU) of Facebook and its competitors but also predicts their fate four years into the future.","cites":"4","conferencePercentile":"50.81967213"},{"venue":"WSDM","id":"93c5e68ece401f5e298c1f9dffbc33df50d043a9","venue_1":"WSDM","year":"2012","title":"Sponsored search auctions with conflict constraints","authors":"Panagiotis Papadimitriou, Hector Garcia-Molina","author_ids":"3359266, 1695250","abstract":"In sponsored search auctions advertisers compete for ad slots in the search engine results page, by bidding on keywords of interest. To improve advertiser expressiveness, we augment the bidding process with <i>conflict</i> constraints. With such constraints, advertisers can condition their bids on the non-appearance of certain undesired ads on the results page. We study the complexity of the <i>allocation</i> problem in these augmented SSA and we introduce an algorithm that can efficiently allocate the ad slots to advertisers. We evaluate the algorithm run time in simulated conflict scenarios and we study the implications of the conflict constraints on search engine revenue. Our results show that the allocation problem can be solved within few tens of milliseconds and that the adoption of conflict constraints can potentially increase search engine revenue.","cites":"2","conferencePercentile":"14.53488372"},{"venue":"WSDM","id":"05e6ca796d628f958d9084eee481b3d2f0046c1b","venue_1":"WSDM","year":"2011","title":"On the selection of tags for tag clouds","authors":"Petros Venetis, Georgia Koutrika, Hector Garcia-Molina","author_ids":"1734332, 1680709, 1695250","abstract":"We examine the creation of a tag cloud for exploring and understanding a set of objects (e.g., web pages, documents). In the first part of our work, we present a formal system model for reasoning about tag clouds. We then present metrics that capture the structural properties of a tag cloud, and we briefly present a set of tag selection algorithms that are used in current sites (e.g., del.icio.us, Flickr, Technorati) or that have been described in recent work. In order to evaluate the results of these algorithms, we devise a novel synthetic user model. This user model is specifically tailored for tag cloud evaluation and assumes an \"ideal\" user. We evaluate the algorithms under this user model, as well as the model itself, using two datasets: CourseRank (a Stanford social tool containing information about courses) and del.icio.us (a social bookmarking site). The results yield insights as to when and why certain selection schemes work best.","cites":"24","conferencePercentile":"61.23595506"},{"venue":"WSDM","id":"c8c4a820973364d5f39f49e24686154d504755bd","venue_1":"WSDM","year":"2009","title":"Speeding up algorithms on compressed web graphs","authors":"Chinmay Karande, Kumar Chellapilla, Reid Andersen","author_ids":"1754072, 1809791, 1830820","abstract":"A variety of lossless compression schemes have been proposed to reduce the storage requirements of web graphs. One successful approach is virtual node compression [7], in which often-used patterns of links are replaced by links to virtual nodes, creating a compressed graph that succinctly represents the original. In this paper, we show that several important classes of web graph algorithms can be extended to run directly on virtual node compressed graphs, such that their running times depend on the size of the compressed graph rather than the original. These include algorithms for link analysis, estimating the size of vertex neighborhoods, and a variety of algorithms based on matrix-vector products and random walks. Similar speed-ups have been obtained previously for classical graph algorithms like shortest paths and maximum bipartite matching. We measure the performance of our modified algorithms on several publicly available web graph datasets, and demonstrate significant empirical speedups that nearly match the compression ratios.","cites":"20","conferencePercentile":"46.05263158"},{"venue":"WSDM","id":"bd0905575e068467644ee1d893ef5bc0ea552dd9","venue_1":"WSDM","year":"2013","title":"Bursty subgraphs in social networks","authors":"Milad Eftekhar, Nick Koudas, Yashar Ganjali","author_ids":"1748880, 1721062, 1731999","abstract":"Data available through social media and content sharing platforms present opportunities for analysis and mining. In the context of social networks, it is interesting to formalize and locate bursts of activities amongst users, related to a particular event and to report sets of socially connected users participating in such bursts. Such collections present new opportunities for understanding social events, and render new ways of online marketing.\n In this paper, we model social information using two conceptualized graph models. The first one (the action graph) provides a detailed model of all activities of all users while the second one (the holistic graph) provides an aggregate view on each user in the social media. We also propose two models to define the notion of \"burst\". The first model (intrinsic burst model) takes the intrinsic characteristics of each user into account to recognize the bursty behaviors; while the second model (social burst model) considers neighbors' influences when identifying bursts. We provide two linear algorithms to detect bursts based on the proposed models. These algorithms have been extensively evaluated on a month of full Twitter dataset certifying the practicality of our approach. A detailed qualitative study of our techniques is also presented.","cites":"4","conferencePercentile":"23.36956522"},{"venue":"WSDM","id":"9f3583769b9226c59ca2469f230d2db0d6499647","venue_1":"WSDM","year":"2008","title":"A scalable pattern mining approach to web graph compression with communities","authors":"Gregory Buehrer, Kumar Chellapilla","author_ids":"2095183, 1809791","abstract":"A link server is a system designed to support efficient implementations of graph computations on the web graph. In this work, we present a compression scheme for the web graph specifically designed to accommodate community queries and other random access algorithms on link servers. We use a frequent pattern mining approach to extract meaningful connectivity formations. Our <i>Virtual Node Miner</i> achieves graph compression without sacrificing random access by generating virtual nodes from frequent itemsets in vertex adjacency lists. The mining phase guarantees scalability by bounding the pattern mining complexity to <i>O</i>(<i>E</i> log <i>E</i>). We facilitate global mining, relaxing the requirement for the graph to be sorted by URL, enabling discovery for both inter-domain as well as intra-domain patterns. As a consequence, the approach allows incremental graph updates. Further, it not only facilitates but can also expedite graph computations such as PageRank and local random walks by implementing them directly on the compressed graph. We demonstrate the effectiveness of the proposed approach on several publicly available large web graph data sets. Experimental results indicate that the proposed algorithm achieves a 10- to 15-fold compression on most real word web graph data sets","cites":"81","conferencePercentile":"72"},{"venue":"WSDM","id":"33cd6adc3fc865c7b8f9745674a38aaa21dea912","venue_1":"WSDM","year":"2008","title":"Can social bookmarking improve web search?","authors":"Paul Heymann, Georgia Koutrika, Hector Garcia-Molina","author_ids":"2912194, 1680709, 1695250","abstract":"Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio. us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact","cites":"229","conferencePercentile":"84"},{"venue":"WSDM","id":"14af018e67a16b20a12acb311f4881cc2f257ee9","venue_1":"WSDM","year":"2011","title":"Everyone's an influencer: quantifying influence on twitter","authors":"Eytan Bakshy, Jake M. Hofman, Winter A. Mason, Duncan J. Watts","author_ids":"3071703, 2081766, 1798971, 1783914","abstract":"In this paper we investigate the attributes and relative influence of 1.6M Twitter users by tracking 74 million diffusion events that took place on the Twitter follower graph over a two month interval in 2009. Unsurprisingly, we find that the largest cascades tend to be generated by users who have been influential in the past and who have a large number of followers. We also find that URLs that were rated more interesting and/or elicited more positive feelings by workers on Mechanical Turk were more likely to spread. In spite of these intuitive results, however, we find that predictions of which particular user or URL will generate large cascades are relatively unreliable. We conclude, therefore, that word-of-mouth diffusion can only be harnessed reliably by targeting large numbers of potential influencers, thereby capturing average effects. Finally, we consider a family of hypothetical marketing strategies, defined by the relative cost of identifying versus compensating potential \"influencers.\" We find that although under some circumstances, the most influential users are also the most cost-effective, under a wide range of plausible assumptions the most cost-effective performance can be realized using \"ordinary influencers\"---individuals who exert average or even less-than-average influence.","cites":"432","conferencePercentile":"100"},{"venue":"WSDM","id":"6a1cca8faa22bb14371743db780d4e02c6e71b85","venue_1":"WSDM","year":"2016","title":"Extracting Search Query Patterns via the Pairwise Coupled Topic Model","authors":"Takuya Konishi, Takuya Ohwa, Sumio Fujita, Kazushi Ikeda, Kohei Hayashi","author_ids":"1975230, 3181115, 1721803, 1775198, 2730698","abstract":"A fundamental yet new challenge in information retrieval is the identification of patterns behind search queries. For example, the query \"NY restaurant\" and \"boston hotel\" shares the common pattern \"LOCATION SERVICE\". However, because of the diversity of real queries, existing approaches require data preprocessing by humans or specifying the target query domains, which hinders their applicability.\n We propose a probabilistic topic model that assumes that each term (e.g., \"NY\") has a topic (LOCATION). The key idea is that we consider topic co-occurrence in a query rather than a topic sequence, which significantly reduces computational cost yet enables us to acquire coherent topics without the preprocessing. Using two real query datasets, we demonstrate that the obtained topics are intelligible by humans, and are highly accurate in keyword prediction and query generation tasks.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"9692603326c01760dd5c2cc2bb1eb25c5a26307a","venue_1":"WSDM","year":"2013","title":"Mining the web to predict future events","authors":"Kira Radinsky, Eric Horvitz","author_ids":"2357385, 1688884","abstract":"We describe and evaluate methods for learning to forecast forthcoming events of interest from a corpus containing 22 years of news stories. We consider the examples of identifying significant increases in the likelihood of disease outbreaks, deaths, and riots in advance of the occurrence of these events in the world. We provide details of methods and studies, including the automated extraction and generalization of sequences of events from news corpora and multiple web resources. We evaluate the predictive power of the approach on real-world events withheld from the system.","cites":"35","conferencePercentile":"90.2173913"},{"venue":"WSDM","id":"c1bc44c4a2d7b7fe5f06750fc4fe0b1f569d9508","venue_1":"WSDM","year":"2016","title":"Mining the Web for Intelligent Problem Solving for Programmers","authors":"Xin Rong","author_ids":"2479660","abstract":"Programming can be hard to learn and master. Novice programmers often find themselves struggling with terminology, concepts, or different solutions to the same problem with little clue on how to choose the best one. Professional programmers often spend a considerable amount of time learning to use third-party libraries, APIs, or an unfamiliar piece of code. Although programmers can turn to search engines or question-and-answer websites for help, the problem solving process can often take multiple iterations and can be time-consuming. An integrated system that can recognize a programmer's difficulties and provide contextualized solutions is thus desirable, as it may significantly reduce the amount of manual effort required in the loop of troubleshooting.\n Ideally, a programmer should be able to interact with such an intelligent system using natural language, in a way similar to how they document code or communicate with peers. However, using automatic natural language processing techniques to address programming questions is very difficult, mainly due to the following reasons: (1) the terms and common expressions vary greatly across different domains and individual programmers, making it difficult to associate relevant concepts together; (2) the solution to the user's trouble in programming often requires multiple steps or different resources, which requires deep understanding of the relations or dependencies of the possible solutions, as well as the user's personal capability of handling those solutions; (3) the documents in the training data usually include a mixture of general-domain expressions with mentions of variables, functions, and classes, as well as source code, making low-level text processing difficult; (4) the evaluation of the system generally requires skilled experts to provide ground truth, which is expensive and often unreliable.\n We address the above difficulties and build an intelligent programming helper system by mining the massive data available online related to programming, including question-and-answer websites, tutorials, blogs, and code repositories. In specific, the study involves three important components. First, we use information extraction techniques to extract common programming tasks, issues, and solutions from the Web data, and establish connections between these extracted elements by leveraging their discrete or distributed representations (e.g., using neural embedding models). Such techniques have been shown to be useful in helping general users solve problems that require interactions with a complex computer software application through the interface of natural language. Second, we study how to handle complicated problems that require multiple steps to solve. The existing troubleshooting instances documented online are collectively modeled as a heterogeneous network, on which the random walk paths can be exploited to recommend solutions. Third, we study how to personalize the problem-solving process for users with varying levels of skills and background knowledge. In particular, each user's past adoptions of technologies and the adoption behavior in his/her social community can be jointly leveraged to provide the appropriate recommendations of technologies and may even promote innovations (e.g., new algorithms) in the process. Collectively, these three components form an integral solution to computer-assisted problem solving for programmers driven by big data, and may have impact on various different domains, including information extraction, language modeling, natural language understanding, automatic problem solving, and social network analysis.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"199dcbb1e5287eedb458c867b171cc83c06b0d2a","venue_1":"WSDM","year":"2013","title":"Pairwise ranking aggregation in a crowdsourced setting","authors":"Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson, Eric Horvitz","author_ids":"1714741, 2142374, 2582990, 1688884","abstract":"Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.","cites":"66","conferencePercentile":"96.73913043"},{"venue":"WSDM","id":"071cf58e1ded101fb105e27d2add18b75c2607e8","venue_1":"WSDM","year":"2016","title":"The Past and Future of Systems for Current Events","authors":"Mor Naaman","author_ids":"1687465","abstract":"People share in social media an overwhelming amount of content from real-world events. These events range from major global events like an uprising or an earthquake, to local events and emergencies such as a fire or a parade; from media events like the Oscar's, to events that enjoy little media coverage such as a conference or a music concert. This shared media represents an important part of our society, culture and history. At the same time, this social media content is still fragmented across services, hard to find, and difficult to consume and understand.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"58141fc33c0750b68332c5d5994204dc2d0478b5","venue_1":"WSDM","year":"2015","title":"On the Accuracy of Hyper-local Geotagging of Social Media Content","authors":"David Flatow, Mor Naaman, Ke Eddie Xie, Yana Volkovich, Yaron Kanza","author_ids":"2581362, 1687465, 7953234, 1782794, 1742591","abstract":"Social media users share billions of items per year, only a small fraction of which is geotagged. We present a data-driven approach for identifying non-geotagged content items that can be associated with a hyper-local geographic area by modeling the location distributions of n-grams that appear in the text. We explore the trade-off between accuracy and coverage of this method. Further, we explore differences across content received from multiple platforms and devices, and show, for example, that content shared via different sources and applications produces significantly different geographic distributions, and that it is preferred to model and predict location for items according to their source. Our findings show the potential and the bounds of a data-driven approach to assigning location data to short social media texts, and offer implications for all applications that use data-driven approaches to locate content.","cites":"8","conferencePercentile":"72.95081967"},{"venue":"WSDM","id":"2a40df9ce0f80e0764843abd6612b715eb98ce46","venue_1":"WSDM","year":"2011","title":"A two-view learning approach for image tag ranking","authors":"Jinfeng Zhuang, Steven C. H. Hoi","author_ids":"2146277, 1741126","abstract":"Tags of social images play a central role for text-based social image retrieval and browsing tasks. However, the original tags annotated by web users could be noisy, irrelevant, and often incomplete for describing the image contents, which may severely deteriorate the performance of text-based image retrieval models. In this paper, we aim to overcome the challenge of <i>social tag ranking</i> for a corpus of social images with rich user-generated tags by proposing a novel two-view learning approach. It can effectively exploit both textual and visual contents of social images to discover the complicated relationship between tags and images. Unlike the conventional learning approaches that usually assume some parametric models, our method is completely data-driven and makes no assumption of the underlying models, making the proposed solution practically more effective. We formally formulate our method as an optimization task and present an efficient algorithm to solve it. To evaluate the efficacy of our method, we conducted an extensive set of experiments by applying our technique to both text-based social image retrieval and automatic image annotation tasks, in which encouraging results showed that the proposed method is more effective than the conventional approaches.","cites":"27","conferencePercentile":"67.41573034"},{"venue":"WSDM","id":"36c9a07be7189db412a632fca7d35183ddcb1de8","venue_1":"WSDM","year":"2013","title":"Online multi-modal distance learning for scalable multimedia retrieval","authors":"Hao Xia, Pengcheng Wu, Steven C. H. Hoi","author_ids":"1751367, 8327053, 1741126","abstract":"In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are represented by diverse types of representation, which is often referred to as \"multi-modal data\". The definition of distance between any two objects/items on multi-modal data is a key challenge encountered by many real-world applications, including multimedia retrieval. In this paper, we present a novel online learning framework for learning distance functions on multi-modal data through the combination of multiple kernels. In order to attack large-scale multimedia applications, we propose Online Multi-modal Distance Learning (OMDL) algorithms, which are significantly more efficient and scalable than the state-of-the-art techniques. We conducted an extensive set of experiments on multi-modal image retrieval applications, in which encouraging results validate the efficacy of the proposed technique.","cites":"6","conferencePercentile":"33.15217391"},{"venue":"WSDM","id":"8c5c728bb26c39f71272f4380f991e4446f781be","venue_1":"WSDM","year":"2015","title":"SimApp: A Framework for Detecting Similar Mobile Applications by Online Kernel Learning","authors":"Ning Chen, Steven C. H. Hoi, Shaohua Li, Xiaokui Xiao","author_ids":"3524369, 1741126, 1808712, 1711922","abstract":"With the popularity of smart phones and mobile devices, the number of mobile applications (a.k.a. \"apps\") has been growing rapidly. Detecting semantically similar apps from a large pool of apps is a basic and important problem, as it is beneficial for various applications, such as app recommendation, app search, etc. However, there is no systematic and comprehensive work so far that focuses on addressing this problem. In order to fill this gap, in this paper, we explore multi-modal heterogeneous data in app markets (e.g., description text, images, user reviews, etc.), and present \"SimApp\" -- a novel framework for detecting similar apps using machine learning. Specifically, it consists of two stages: (i) a variety of kernel functions are constructed to measure app similarity for each modality of data; and (ii) an online kernel learning algorithm is proposed to learn the optimal combination of similarity functions of multiple modalities. We conduct an extensive set of experiments on a real-world dataset crawled from Google Play to evaluate SimApp, from which the encouraging results demonstrate that SimApp is effective and promising.","cites":"6","conferencePercentile":"64.75409836"},{"venue":"WSDM","id":"3ebe2bd33cdbe2a2e0cd3d01b955f8dc96c9923e","venue_1":"WSDM","year":"2016","title":"Mobile App Tagging","authors":"Ning Chen, Steven C. H. Hoi, Shaohua Li, Xiaokui Xiao","author_ids":"3524369, 1741126, 1808712, 1711922","abstract":"Mobile app tagging aims to assign a list of keywords indicating core functionalities, main contents, key features or concepts of a mobile app. Mobile app tags can be potentially useful for app ecosystem stakeholders or other parties to improve app search, browsing, categorization, and advertising, etc. However, most mainstream app markets, e.g., Google Play, Apple App Store, etc., currently do not explicitly support such tags for apps. To address this problem, we propose a novel auto mobile app tagging framework for annotating a given mobile app automatically, which is based on a search-based annotation paradigm powered by machine learning techniques. Specifically, given a novel query app without tags, our proposed framework (i) first explores online kernel learning techniques to retrieve a set of top-N similar apps that are semantically most similar to the query app from a large app repository; and (ii) then mines the text data of both the query app and the top-N similar apps to discover the most relevant tags for annotating the query app. To evaluate the efficacy of our proposed framework, we conduct an extensive set of experiments on a large real-world dataset crawled from Google Play. The encouraging results demonstrate that our technique is effective and promising.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"5e703880637f833bab0ff249371363052844c17d","venue_1":"WSDM","year":"2009","title":"Contrasting Controlled Vocabulary and Tagging: Experts Choose the Right Names to Label the Wrong Things","authors":"Paul Heymann, Hector Garcia-Molina","author_ids":"2912194, 1695250","abstract":"Social cataloging sites—tagging systems where users tag books—provide us with a rare opportunity to contrast tags to other information organization systems. We contrast tags to a controlled vocabulary, the Library of Congress Subject Headings, which has been developed over several decades. We find that many of the keywords designated by tags and LCSH are similar or the same, but that usage of keywords by annotators is quite different.","cites":"3","conferencePercentile":"13.15789474"},{"venue":"WSDM","id":"bec4d1d5a86b0e99a07e23204c10a7e67613a6e7","venue_1":"WSDM","year":"2012","title":"Finding the right consumer: optimizing for conversion in display advertising campaigns","authors":"Yandong Liu, Sandeep Pandey, Deepak Agarwal, Vanja Josifovski","author_ids":"1706060, 1719322, 1729939, 1679460","abstract":"The ultimate goal of advertisers are <i>conversions</i> representing desired user actions on the advertisers' websites in the form of purchases and product information request. In this paper we address the problem of finding the right audience for display campaigns by finding the users that are most likely to convert. This challenging problem is at the heart of display campaign optimization and has to deal with several issues such as very small percentage of converters in the general population, high-dimensional representation of the user profiles, large churning rate of users and advertisers. To overcome these difficulties, in our approach we use two sources of information: a <i>seed</i> set of users that have converted for a campaign in the past; and a description of the campaign based on the advertiser's website. We explore the importance of the information provided by each of these two sources in a principled manner and then combine them to propose models for predicting converters. In particular, we show how seed set can be used to capture the campaign-specific targeting constraints, while the campaign metadata allows to share targeting knowledge across campaigns. We give methods for learning these models and perform experiments on real-world advertising campaigns. Our findings show that the seed set and the campaign metadata are complimentary to each other and both sources provide valuable information for conversion optimization.","cites":"4","conferencePercentile":"18.60465116"},{"venue":"WSDM","id":"5ee3da8b1577de0de75e45ba498095c08bc349ff","venue_1":"WSDM","year":"2013","title":"Using early view patterns to predict the popularity of youtube videos","authors":"Henrique Pinto, Jussara M. Almeida, Marcos André Gonçalves","author_ids":"2387078, 8118988, 7170789","abstract":"Predicting Web content popularity is an important task for supporting the design and evaluation of a wide range of systems, from targeted advertising to effective search and recommendation services. We here present two simple models for predicting the future popularity of Web content based on historical information given by early popularity measures. Our approach is validated on datasets consisting of videos from the widely used YouTube video-sharing portal. Our experimental results show that, compared to a state-of-the-art baseline model, our proposed models lead to significant decreases in relative squared errors, reaching up to 20% reduction on average, and larger reductions (of up to 71%) for videos that experience a high peak in popularity in their early days followed by a sharp decrease in popularity.","cites":"65","conferencePercentile":"95.65217391"},{"venue":"WSDM","id":"df6ac734f59131abec9beca63fb93605c27305e8","venue_1":"WSDM","year":"2014","title":"Data that matter: opportunities in crisis informatics research","authors":"Leysia Palen","author_ids":"1759447","abstract":"In an increasingly global society and on a planet experiencing effects of climate change, large-scale emergencies both instigated by humans and arising from nature can devastate human life and our tightly- woven social fabric. With a promise of improved warning and coordination, a prevailing hope is that information and communication technology (ICT) can help reduce the impacts of large-scale disruptions, including political crises, natural disasters, pandemics, and terrorist threats. Much of the focus of development has been on the formal emergency response effort.\n However, social computing is changing the way we understand information distribution. By viewing the citizenry as a powerful, self-organizing, and collectively intelligent force, ICT is now playing a remarkable and transformational role in the way society responds to mass emergencies and disasters. Furthermore, this view of a civil society that can be augmented by ICT is based on social and behavioral knowledge about how people truly respond in disaster, rather than on simplified and mythical portrayals of people unable to help themselves [2]. Indeed, long before the advent of widely available social computing platforms, research has shown that disaster victims themselves are the true first responders, frequently acting on the basis of knowledge not available to officials [1, 3, 6].\n We argue that this transformative view is critical to our global future: When large-scale emergencies happen, there is often no way to survive it in practical terms unless we rely on each other for help. The urgency and scale of many disaster events are such that no one, not even the most experienced and best technology- equipped responders' can rescue all victims or direct all people over the span of the event as to what the best course of action might be. Climate change and population migration to geographically vulnerable areas mean that naturally occurring hazards will exert increasingly extensive damage. Man-made and terrorist threats can also have greater potential to cause lasting damage to the social and built environment. It is instead necessary, through innovative ICT, to leverage the power of the collective intelligence of the citizenry to support natural instincts, which are to search for reliable information using any means possible to optimize for local conditions [5].","cites":"0","conferencePercentile":"7.692307692"},{"venue":"WSDM","id":"a539812ac698a05afb6a60bdd7bfa483eefd0da4","venue_1":"WSDM","year":"2012","title":"Beyond ten blue links: enabling user click modeling in federated web search","authors":"Danqi Chen, Weizhu Chen, Haixun Wang, Zheng Chen, Qiang Yang","author_ids":"2318091, 7307263, 1745000, 1705657, 1733090","abstract":"Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.","cites":"27","conferencePercentile":"65.69767442"},{"venue":"WSDM","id":"c094f037905d0b690e393984859f3f5a2cbaadee","venue_1":"WSDM","year":"2012","title":"Personalized click model through collaborative filtering","authors":"Si Shen, Botao Hu, Weizhu Chen, Qiang Yang","author_ids":"2508784, 3282304, 7307263, 1733090","abstract":"Click modeling aims to interpret the users' search click data in order to predict their clicking behavior. Existing models can well characterize the position bias of documents and snippets in relation to users' mainstream click behavior. Yet, current advances depict users' search actions only in a general setting by implicitly assuming that all users act in the same way, regardless of the fact that anyone, motivated with some individual interest, is more likely to click on a link than others. It is in light of this that we put forward a novel personalized click model to describe the user-oriented click preferences, which applies and extends matrix / tensor factorization from the view of collaborative filtering to connect users, queries and documents together. Our model serves as a generalized personalization framework that can be incorporated to the previously proposed click models and, in many cases, to their future extensions. Despite the sparsity of search click data, our personalized model demonstrates its advantage over the best click models previously discussed in the Web-search literature, supported by our large-scale experiments on a real dataset. A delightful bonus is the model's ability to gain insights into queries and documents through latent feature vectors, and hence to handle rare and even new query-document pairs much better than previous click models.","cites":"24","conferencePercentile":"61.62790698"},{"venue":"WSDM","id":"1287d91718f3913a32e148a25fcbb730d3dde68b","venue_1":"WSDM","year":"2013","title":"Big data, lifelong machine learning and transfer learning","authors":"Qiang Yang","author_ids":"1733090","abstract":"A major challenge in today's world is the Big Data problem, which manifests itself in Web and Mobile domains as rapidly changing and heterogeneous data streams. A data-mining system must be able to cope with the influx of changing data in a continual manner. This calls for Lifelong Machine Learning, which in contrast to the traditional one-shot learning, should be able to identify the learning tasks at hand and adapt to the learning problems in a sustainable manner. A foundation for lifelong machine learning is transfer learning, whereby knowledge gained in a related but different domain may be transferred to benefit learning for a current task. To make effective transfer learning, it is important to maintain a continual and sustainable channel in the life time of a user in which the data are annotated. In this talk, I outline the lifelong machine learning situations, give several examples of transfer learning and applications for lifelong machine learning, and discuss cases of successful extraction of data annotations to meet the Big Data challenge.","cites":"0","conferencePercentile":"5.434782609"},{"venue":"WSDM","id":"6c5bb5630863adcf80841975c07781291aafc95d","venue_1":"WSDM","year":"2016","title":"EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion","authors":"Xin Rong, Zhe Chen, Qiaozhu Mei, Eytan Adar","author_ids":"2479660, 1696121, 1743469, 2630700","abstract":"A key challenge of entity set expansion is that multifaceted input seeds can lead to significant incoherence in the result set. In this paper, we present a novel solution to handling multifaceted seeds by combining existing user-generated ontologies with a novel word-similarity metric based on skip-grams. By blending the two resources we are able to produce sparse word ego-networks that are centered on the seed terms and are able to capture semantic equivalence among words. We demonstrate that the resulting networks possess internally-coherent clusters, which can be exploited to provide non-overlapping expansions, in order to reflect different semantic classes of the seeds. Empirical evaluation against state-of-the-art baselines shows that our solution, EgoSet, is able to not only capture multiple facets in the input query, but also generate expansions for each facet with higher precision.","cites":"3","conferencePercentile":"84.40860215"},{"venue":"WSDM","id":"621c6eb4108031bceaedeb9fe492393a06958c23","venue_1":"WSDM","year":"2009","title":"Information arbitrage across multi-lingual Wikipedia","authors":"Eytan Adar, Michael Skinner, Daniel S. Weld","author_ids":"2630700, 5299672, 1780531","abstract":"The rapid globalization of Wikipedia is generating a parallel, multi-lingual corpus of unprecedented scale. Pages for the same topic in many different languages emerge both as a result of manual translation and independent development. Unfortunately, these pages may appear at different times, vary in size, scope, and quality. Furthermore, differential growth rates cause the conceptual mapping between articles in different languages to be both complex and dynamic. These disparities provide the opportunity for a powerful form of <i>information arbitrage</i>--leveraging articles in one or more languages to improve the content in another. Analyzing four large language domains (English, Spanish, French, and German), we present <i>Ziggurat</i>, an automated system for aligning Wikipedia infoboxes, creating new infoboxes as necessary, filling in missing information, and detecting discrepancies between parallel pages. Our method uses self-supervised learning and our experiments demonstrate the method's feasibility, even in the absence of dictionaries.","cites":"45","conferencePercentile":"71.05263158"},{"venue":"WSDM","id":"7388fb5d38aa67f9b90d8f1876938f7fbfec8c08","venue_1":"WSDM","year":"2011","title":"Aspect and sentiment unification model for online review analysis","authors":"Yohan Jo, Alice H. Oh","author_ids":"2689873, 1725588","abstract":"User-generated reviews on the Web contain sentiments about detailed aspects of products and services. However, most of the reviews are plain text and thus require much effort to obtain information about relevant details. In this paper, we tackle the problem of automatically discovering what aspects are evaluated in reviews and how sentiments for different aspects are expressed. We first propose Sentence-LDA (SLDA), a probabilistic generative model that assumes all words in a single sentence are generated from one aspect. We then extend SLDA to Aspect and Sentiment Unification Model (ASUM), which incorporates aspect and sentiment together to model sentiments toward different aspects. ASUM discovers pairs of {aspect, sentiment} which we call senti-aspects. We applied SLDA and ASUM to reviews of electronic devices and restaurants. The results show that the aspects discovered by SLDA match evaluative details of the reviews, and the senti-aspects found by ASUM capture important aspects that are closely coupled with a sentiment. The results of sentiment classification show that ASUM outperforms other generative models and comes close to supervised classification methods. One important advantage of ASUM is that it does not require any sentiment labels of the reviews, which are often expensive to obtain.","cites":"174","conferencePercentile":"95.50561798"},{"venue":"WSDM","id":"18cdf57195143ed3a3681440ddfb6f1fa839475b","venue_1":"WSDM","year":"2011","title":"On composition of a federated web search result page: using online users to provide pairwise preference for heterogeneous verticals","authors":"Ashok Kumar Ponnuswami, Kumaresh Pattabiraman, Qiang Wu, Ran Gilad-Bachrach, Tapas Kanungo","author_ids":"2178021, 2025775, 1690519, 5719660, 2273388","abstract":"Modern web search engines are federated --- a user query is sent to the numerous specialized search engines called <i>verticals</i> like web (text documents), News, Image, Video, etc. and the results returned by these engines are then aggregated and composed into a search result page (SERP) and presented to the user. For a specific query, multiple verticals could be relevant, which makes the placement of these vertical results within blocks of textual web results challenging: how do we represent, assess, and compare the relevance of these <i>heterogeneous</i> entities?\n In this paper we present a machine-learning framework for SERP composition in the presence of multiple relevant verticals. First, instead of using the traditional label generation method of human judgment guidelines and trained judges, we use a randomized online auditioning system that allows us to evaluate triples of the form query, web block, vertical&gt;. We use a pairwise click preference to evaluate whether the web block or the vertical block had a better users' engagement. Next, we use a hinged feature vector that contains features from the web block to create a common reference frame and augment it with features representing the specific vertical judged by the user. A gradient boosted decision tree is then learned from the training data. For the final composition of the SERP, we place a vertical result at a slot if the score is higher than a computed threshold. The thresholds are algorithmically determined to guarantee specific coverage for verticals at each slot.\n We use correlation of clicks as our offline metric and show that click-preference target has a better correlation than human judgments based models. Furthermore, on online tests for News and Image verticals we show higher user engagement for both head and tail queries.","cites":"33","conferencePercentile":"72.47191011"},{"venue":"WSDM","id":"6c70e18940fc198fce173a49bcd3c1492f02a835","venue_1":"WSDM","year":"2016","title":"On the Efficiency of the Information Networks in Social Media","authors":"Mahmoudreza Babaei, Przemyslaw A. Grabowicz, Isabel Valera, Krishna P. Gummadi, Manuel Gomez-Rodriguez","author_ids":"1818176, 1907673, 2123579, 1958921, 1804327","abstract":"Social media sites are information marketplaces, where users produce and consume a wide variety of information and ideas. In these sites, users typically choose their information sources, which in turn determine what specific information they receive, how much information they receive and how quickly this information is shown to them. In this context, a natural question that arises is how efficient are social media users at selecting their information sources. In this work, we propose a computational framework to quantify users' efficiency at selecting information sources. Our framework is based on the assumption that the goal of users is to acquire a set of unique pieces of information. To quantify user's efficiency, we ask if the user could have acquired the same pieces of information from another set of sources more efficiently. We define three different notions of efficiency -- link, in-flow, and delay -- corresponding to the number of sources the user follows, the amount of (redundant) information she acquires and the delay with which she receives the information. Our definitions of efficiency are general and applicable to any social media system with an underlying in- formation network, in which every user follows others to receive the information they produce.\n In our experiments, we measure the efficiency of Twitter users at acquiring different types of information. We find that Twitter users exhibit sub-optimal efficiency across the three notions of efficiency, although they tend to be more efficient at acquiring non- popular pieces of information than they are at acquiring popular pieces of information. We then show that this lack of efficiency is a consequence of the triadic closure mechanism by which users typically discover and follow other users in social media. Thus, our study reveals a tradeoff between the efficiency and discoverability of information sources. Finally, we develop a heuristic algorithm that enables users to be significantly more efficient at acquiring the same unique pieces of information.","cites":"2","conferencePercentile":"72.04301075"},{"venue":"WSDM","id":"15cd5edcbf4e865f879268a309682062b0abe63c","venue_1":"WSDM","year":"2016","title":"Quantifying Controversy in Social Media","authors":"Venkata Rama Kiran Garimella, Gianmarco De Francisci Morales, Aristides Gionis, Michael Mathioudakis","author_ids":"1699369, 1715892, 1682878, 2821914","abstract":"Which topics spark the most heated debates in social media? Identifying these topics is a first step towards creating systems which pierce echo chambers. In this paper, we perform a systematic methodological study of controversy detection using social media network structure and content.\n Unlike previous work, rather than identifying controversy in a single hand-picked topic and use domain-specific knowledge, we focus on comparing topics in any domain. Our approach to quantifying controversy is a graph-based three-stage pipeline, which involves (i) building a conversation graph about a topic, which represents alignment of opinion among users; (ii) partitioning the conversation graph to identify potential sides of the controversy; and (iii)measuring the amount of controversy from characteristics of the~graph.\n We perform an extensive comparison of controversy measures, as well as graph building approaches and data sources. We use both controversial and non-controversial topics on Twitter, as well as other external datasets. We find that our new random-walk-based measure outperforms existing ones in capturing the intuitive notion of controversy, and show that content features are vastly less helpful in this task.","cites":"5","conferencePercentile":"93.01075269"},{"venue":"WSDM","id":"30be2eb25cd482c53787e1d252808dc4460c7139","venue_1":"WSDM","year":"2014","title":"Who Watches (and Shares) What on YouTube? And When? Using Twitter to Understand YouTube Viewership","authors":"Adiya Abisheva, Venkata Rama Kiran Garimella, David Garcia, Ingmar Weber","author_ids":"2317652, 1699369, 3947040, 1684687","abstract":"By combining multiple social media datasets, it is possible to gain insight into each dataset that goes beyond what could be obtained with either individually. In this paper we combine user-centric data from Twitter with video-centric data from YouTube to build a rich picture of who watches and shares what on YouTube. We study 87K Twitter users, 5.6 million YouTube videos and 15 million video sharing events from user-, video- and sharing-event-centric perspectives. We show that features of Twitter users correlate with YouTube features and sharing-related features. For example, urban users are quicker to share than rural users. We find a superlinear relationship between initial Twitter shares and the final amounts of views. We discover that Twitter activity metrics play more role in video popularity than mere amount of followers. We also reveal the existence of correlated behavior concerning the time between video creation and sharing within certain timescales, showing the time onset for a coherent response, and the time limit after which collective responses are extremely unlikely. Response times depend on the category of the video, suggesting Twitter video sharing is highly dependent on the video content. To the best of our knowledge, this is the first large-scale study combining YouTube and Twitter data, and it reveals novel, detailed insights into who watches (and shares) what on YouTube, and when.","cites":"21","conferencePercentile":"94.23076923"},{"venue":"WSDM","id":"3f0f0eb4d1d353dcf0fc59b731378cfe9ac41a5d","venue_1":"WSDM","year":"2011","title":"Who uses web search for what: and how","authors":"Ingmar Weber, Alejandro Jaimes","author_ids":"1684687, 1730325","abstract":"We analyze a large query log of 2.3 million anonymous registered users from a web-scale U.S. search engine in order to jointly analyze their on-line behavior in terms of <i>who</i> they might be (demographics), <i>what</i> they search for (query topics), and <i>how</i> they search (session analysis). We examine basic demographics from registration information provided by the users, augmented with U.S. census data, analyze basic session statistics, classify queries into types (navigational, informational, transactional) based on click entropy, classify queries into topic categories, and cluster users based on the queries they issued. We then examine the resulting clusters in terms of demographics and search behavior. Our analysis of the data suggests that there are important differences in search behavior across different demographic groups in terms of the topics they search for, and how they search (e.g., white conservatives are those likely to have voted republican, mostly white males, who search for business, home, and gardening related topics; Baby Boomers tend to be primarily interested in Finance and a large fraction of their sessions consist of simple navigational queries related to online banking, etc.). Finally, we examine regional search differences, which seem to correlate with differences in local industries (e.g., gambling related queries are highest in Las Vegas and lowest in Salt Lake City; searches related to actors are about three times higher in L.A. than in any other region).","cites":"39","conferencePercentile":"78.65168539"},{"venue":"WSDM","id":"63c8980c73bb1197d36d6b490891a14488caab7d","venue_1":"WSDM","year":"2013","title":"Distinguishing topical and social groups based on common identity and bond theory","authors":"Przemyslaw A. Grabowicz, Luca Maria Aiello, Víctor M. Eguíluz, Alejandro Jaimes","author_ids":"1907673, 2905635, 1792418, 1730325","abstract":"Social groups play a crucial role in social media platforms because they form the basis for user participation and engagement. Groups are created explicitly by members of the community, but also form organically as members interact. Due to their importance, they have been studied widely (e.g., community detection, evolution, activity, etc.). One of the key questions for understanding how such groups evolve is whether there are different types of groups and how they differ. In Sociology, theories have been proposed to help explain how such groups form. In particular, the common identity and common bond theory states that people join groups based on identity (i.e., interest in the topics discussed) or bond attachment (i.e., social relationships). The theory has been applied qualitatively to small groups to classify them as either topical or social. We use the identity and bond theory to define a set of features to classify groups into those two categories. Using a dataset from Flickr, we extract user-defined groups and automatically-detected groups, obtained from a community detection algorithm. We discuss the process of manual labeling of groups into social or topical and present results of predicting the group label based on the defined features. We directly validate the predictions of the theory showing that the metrics are able to forecast the group type with high accuracy. In addition, we present a comparison between declared and detected groups along topicality and sociality dimensions.","cites":"15","conferencePercentile":"63.58695652"},{"venue":"WSDM","id":"0409e56956560dba788dfb3adc467f3475606c24","venue_1":"WSDM","year":"2010","title":"You are who you know: inferring user profiles in online social networks","authors":"Alan Mislove, Bimal Viswanath, Krishna P. Gummadi, Peter Druschel","author_ids":"1729928, 2559642, 1958921, 1736987","abstract":"Online social networks are now a popular way for users to connect, express themselves, and share content. Users in today's online social networks often post a profile, consisting of attributes like geographic location, interests, and schools attended. Such profile information is used on the sites as a basis for grouping users, for sharing content, and for suggesting users who may benefit from interaction. However, in practice, not all users provide these attributes.\n In this paper, we ask the question: given attributes for some fraction of the users in an online social network, can we infer the attributes of the remaining users? In other words, can the attributes of users, in combination with the social network graph, be used to predict the attributes of another user in the network? To answer this question, we gather fine-grained data from two social networks and try to infer user profile attributes. We find that users with common attributes are more likely to be friends and often form dense communities, and we propose a method of inferring user attributes that is inspired by previous approaches to detecting communities in social networks. Our results show that certain user attributes can be inferred with high accuracy when given information on as little as 20% of the users.","cites":"240","conferencePercentile":"95.55555556"},{"venue":"WSDM","id":"3be95f322f8f79f7f3c5f2a4cb6401ae30b3428f","venue_1":"WSDM","year":"2012","title":"Mining, searching and exploiting collaboratively generated content on the web","authors":"Eugene Agichtein, Evgeniy Gabrilovich","author_ids":"1685296, 1718798","abstract":"Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content.\n The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation.\n However, the quality and comprehensiveness of collaboratively created content vary significantly, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial.\n The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.","cites":"0","conferencePercentile":"4.651162791"},{"venue":"WSDM","id":"2a869f1ccc422d7d21ffd0a055d7469a15112468","venue_1":"WSDM","year":"2008","title":"Entropy of search logs: how hard is search? with personalization? with backoff?","authors":"Qiaozhu Mei, Kenneth Ward Church","author_ids":"1743469, 1832278","abstract":"How many pages are there on the Web? 5B? 20B? More? Less? Big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. Language modeling techniques are applied to MSN's search logs to estimate entropy. The perplexity is surprisingly small: millions, not billions.\n Entropy is a powerful tool for sizing challenges and opportunities. How hard is search? How hard are query suggestion mechanisms like auto-complete? How much does personalization help? All these difficult questions can be answered by estimation of entropy from search logs.\n What is the potential opportunity for personalization? In this paper, we propose a new way to personalize search, personalization with backoff. If we have relevant data for a particular user, we should use it. But if we don't, back off to larger and larger classes of similar users. As a proof of concept, we use the first few bytes of the IP address to define classes. The coefficients of each backoff class are estimated with an EM algorithm. Ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography","cites":"43","conferencePercentile":"60"},{"venue":"WSDM","id":"4c506aab9868a53251dc8dd43809040612a970f1","venue_1":"WSDM","year":"2012","title":"\"I loan because...\": understanding motivations for pro-social lending","authors":"Yang Liu, Roy Chen, Yan Chen, Qiaozhu Mei, Suzy Salib","author_ids":"3203358, 2033574, 1700696, 1743469, 2231369","abstract":"As a new paradigm of online communities, microfinance sites such as Kiva.org have attracted much public attention. To understand lender motivations on Kiva, we classify the lenders' self-stated motivations into ten categories with human coders and machine learning based classifiers. We employ text classifiers using lexical features, along with social features based on lender activity information on Kiva, to predict the categories of lender motivation statements. Although the task appears to be much more challenging than traditional topic-based categorization, our classifiers can achieve high precision in most categories. Using the results of this classification along with Kiva teams information, we predict lending activity from lender motivation and team affiliations. Finally, we make design recommendations regarding Kiva practices which might increase pro-social lending.","cites":"10","conferencePercentile":"36.62790698"},{"venue":"WSDM","id":"06d0c3e9065357d484ee4f4652a1125e85d9277f","venue_1":"WSDM","year":"2013","title":"Towards Twitter context summarization with user influence models","authors":"Yi Chang, Xuanhui Wang, Qiaozhu Mei, Yan Liu","author_ids":"1787097, 1734449, 1743469, 1681842","abstract":"Twitter has become one of the most popular platforms for users to share information in real time. However, as an individual tweet is short and lacks sufficient contextual information, users cannot effectively understand or consume information on Twitter, which can either make users less engaged or even detached from using Twitter. In order to provide informative context to a Twitter user, we propose the task of Twitter context summarization, which generates a succinct summary from a large but noisy Twitter context tree. Traditional summarization techniques only consider text information, which is insufficient for Twitter context summarization task, since text information on Twitter is very sparse. Given that there are rich user interactions in Twitter, we thus study how to improve summarization methods by leveraging such signals. In particular, we study how user influence models, which project user interaction information onto a Twitter context tree, can help Twitter context summarization within a supervised learning framework. To evaluate our methods, we construct a data set by asking human editors to manually select the most informative tweets as a summary. Our experimental results based on this editorial data set show that Twitter context summarization is a promising research topic and pairwise user influence signals can significantly improve the task performance.","cites":"16","conferencePercentile":"66.30434783"},{"venue":"WSDM","id":"1772143a707f02685e3445485b89c888b2535e7a","venue_1":"WSDM","year":"2016","title":"Beyond Ranking: Optimizing Whole-Page Presentation","authors":"Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, Qiaozhu Mei","author_ids":"1723309, 2115608, 3898305, 1771116, 1724380, 1787097, 1743469","abstract":"Modern search engines aggregate results from different verticals: webpages, news, images, video, shopping, knowledge cards, local maps, etc. Unlike \"ten blue links\", these search results are heterogeneous in nature and not even arranged in a list on the page. This revolution directly challenges the conventional \"ranked list\" formulation in ad hoc search. Therefore, finding proper presentation for a gallery of heterogeneous results is critical for modern search engines.\n We propose a novel framework that learns the optimal page presentation to render heterogeneous results onto search result page (SERP). Page presentation is broadly defined as the strategy to present a set of items on SERP, much more expressive than a ranked list. It can specify item positions, image sizes, text fonts, and any other styles as long as variations are within business and design constraints. The learned presentation is content-aware, i.e. tailored to specific queries and returned results. Simulation experiments show that the framework automatically learns eye-catchy presentations for relevant results. Experiments on real data show that simple instantiations of the framework already outperform leading algorithm in federated search result presentation. It means the framework can learn its own result presentation strategy purely from data, without even knowing the \"probability ranking principle\".","cites":"2","conferencePercentile":"72.04301075"},{"venue":"WSDM","id":"15aca6d6c880784443528a1812f3bfb39378c1ab","venue_1":"WSDM","year":"2014","title":"Discovering common motifs in cursor movement data for improving web search","authors":"Dmitry Lagun, Mikhail Ageev, Qi Guo, Eugene Agichtein","author_ids":"2568110, 2793796, 5022607, 1685296","abstract":"Web search behavior and interaction data, such as mouse cursor movements, can provide valuable information on how searchers examine and engage with the web search results. This interaction data is far richer than traditional search click data, and can be used to improve search ranking, evaluation, and presentation. Unfortunately, the diversity and complexity inherent in this interaction data make it more difficult to capture salient behavior characteristics through traditional feature engineering. To address this problem, we introduce a novel approach of automatically discovering frequent subsequences, or motifs, in mouse cursor movement data. In order to scale our approach to realistic datasets, we introduce novel optimizations for motif discovery, specifically designed for mining cursor movement data. As a practical application, we show that by encoding the motifs discovered from thousands of real web search sessions as features, enables significant improvements on result relevance estimation and re-ranking tasks, compared to a state-of-the-art baseline that relies on extensive feature engineering. These results, complemented with visualization and qualitative analysis, demonstrate that our approach is able to automatically capture key characteristics of mouse cursor movement behavior, providing a valuable new tool for search behavior analysis.","cites":"13","conferencePercentile":"82.69230769"},{"venue":"WSDM","id":"20bd197114af847d23410a1a3de7dac563a60948","venue_1":"WSDM","year":"2014","title":"Trust, but verify: predicting contribution quality for knowledge base construction and curation","authors":"Chun How Tan, Eugene Agichtein, Panagiotis G. Ipeirotis, Evgeniy Gabrilovich","author_ids":"2831229, 1685296, 2942126, 1718798","abstract":"The largest publicly available knowledge repositories, such as Wikipedia and Freebase, owe their existence and growth to volunteer contributors around the globe. While the majority of contributions are correct, errors can still creep in, due to editors' carelessness, misunderstanding of the schema, malice, or even lack of accepted ground truth. If left undetected, inaccuracies often degrade the experience of users and the performance of applications that rely on these knowledge repositories. We present a new method, CQUAL, for automatically predicting the quality of contributions submitted to a knowledge base. Significantly expanding upon previous work, our method holistically exploits a variety of signals, including the user's domains of expertise as reflected in her prior contribution history, and the historical accuracy rates of different types of facts. In a large-scale human evaluation, our method exhibits precision of 91% at 80% recall. Our model verifies whether a contribution is correct immediately after it is submitted, significantly alleviating the need for post-submission human reviewing.","cites":"3","conferencePercentile":"36.53846154"},{"venue":"WSDM","id":"5ba7592fe3310106543cffad644addeffc2ad2b3","venue_1":"WSDM","year":"2014","title":"Struggling or exploring?: disambiguating long search sessions","authors":"Ahmed Hassan Awadallah, Ryen W. White, Susan T. Dumais, Yi-Min Wang","author_ids":"2461958, 1734415, 1728602, 4374095","abstract":"Web searchers often exhibit directed search behaviors such as navigating to a particular Website. However, in many circumstances they exhibit different behaviors that involve issuing many queries and visiting many results. In such cases, it is not clear whether the user's rationale is to intentionally explore the results or whether they are struggling to find the information they seek. Being able to disambiguate between these types of long search sessions is important for search engines both in performing retrospective analysis to understand search success, and in developing real-time support to assist searchers. The difficulty of this challenge is amplified since many of the characteristics of exploration (e.g., multiple queries, long duration) are also observed in sessions where people are struggling. In this paper, we analyze struggling and exploring behavior in Web search using log data from a commercial search engine. We first compare and contrast search behaviors along a number dimensions, including query dynamics during the session. We then build classifiers that can accurately distinguish between exploring and struggling sessions using behavioral and topical features. Finally, we show that by considering the struggling/exploring prediction we can more accurately predict search satisfaction.","cites":"20","conferencePercentile":"92.30769231"},{"venue":"WSDM","id":"db61c78e42686e4ba954842dd858b6be69e19a4b","venue_1":"WSDM","year":"2013","title":"Temporal web dynamics and its application to information retrieval","authors":"Kira Radinsky, Fernando Diaz, Susan T. Dumais, Milad Shokouhi, Anlei Dong, Yi Chang","author_ids":"2357385, 3438482, 1728602, 1703337, 3300216, 1787097","abstract":"The World Wide Web is highly dynamic and is constantly evolving to cover the latest information about the physical and social updates in the world. At the same time, the changes in web contents are entangled with new information needs and time-sensitive user interactions with information sources. To address these temporal information needs effectively, it is essential for the search engines to model web dynamics and understand the changes in user behavior over time that are caused by them.\n In this full-day tutorial, we focus on modeling time-sensitive content on the web, and discuss the state-of-the-art approaches for integrating temporal signals in web search. We address many of the related research topics including those associated with searching dynamic collections, defining time-sensitive relevance, understanding user query behavior over time, and investigating the mains reasons behind content changes. We also cover algorithms, architectures, evaluation methodologies and metrics for time-aware search, and discuss the latest breakthroughs and open challenges, both from the algorithmic and the architectural perspectives.","cites":"5","conferencePercentile":"27.7173913"},{"venue":"WSDM","id":"3b425b922263de318ca9ae54a997b59604e7cddf","venue_1":"WSDM","year":"2013","title":"Personalizing atypical web search sessions","authors":"Carsten Eickhoff, Kevyn Collins-Thompson, Paul N. Bennett, Susan T. Dumais","author_ids":"2285517, 2582990, 2142374, 1728602","abstract":"Most research in Web search personalization models users as static or slowly evolving entities with a given set of preferences defined by their past behavior. However, recent publications as well as empirical evidence suggest that for a significant number of search sessions, users diverge from their regular search profiles in order to satisfy atypical, limited-duration information needs. In this work, we conduct a large-scale inspection of real-life search sessions to further understand this scenario. Subsequently, we design an automatic means of detecting and supporting such atypical sessions. We demonstrate significant improvements over state-of-the-art Web search personalization techniques by accounting for the typicality of search sessions. The proposed method is evaluated based on Web-scale search session data spanning several months of user activity.","cites":"11","conferencePercentile":"54.89130435"},{"venue":"WSDM","id":"7c7e6251abf0dccd563c9e84761e713b1e9a2d20","venue_1":"WSDM","year":"2012","title":"Probabilistic models for personalizing web search","authors":"David Sontag, Kevyn Collins-Thompson, Paul N. Bennett, Ryen W. White, Susan T. Dumais, Bodo Billerbeck","author_ids":"4608166, 2582990, 2142374, 1734415, 1728602, 1748336","abstract":"We present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries.","cites":"41","conferencePercentile":"84.30232558"},{"venue":"WSDM","id":"28d2ce9425134cbd6d354a4d7d170fd98666da5d","venue_1":"WSDM","year":"2012","title":"Characterizing web content, user interests, and search behavior by reading level and topic","authors":"Jin Young Kim, Kevyn Collins-Thompson, Paul N. Bennett, Susan T. Dumais","author_ids":"8709365, 2582990, 2142374, 1728602","abstract":"A user's expertise or ability to understand a document on a given topic is an important aspect of that document's relevance. However, this aspect has not been well-explored in information retrieval systems, especially those at Web scale where the great diversity of content, users, and tasks presents an especially challenging search problem. To help improve our modeling and understanding of this diversity, we apply automatic text classifiers, based on reading difficulty and topic prediction, to estimate a novel type of profile for important entities in Web search -- users, websites, and queries. These profiles capture topic and reading level distributions, which we then use in conjunction with search log data to characterize and compare different entities.\n We find that reading level and topic distributions provide an important new representation of Web content and user interests, and that using both together is more effective than using either one separately. In particular we find that: 1) the reading level of Web content and the diversity of visitors to a website can vary greatly by topic; 2) the degree to which a user's profile matches with a site's profile is closely correlated with the user's preference of the website in search results, and 3) site or URL profiles can be used to predict 'expertness' whether a given site or URL is oriented toward expert vs. non-expert users. Our findings provide strong evidence in favor of <i>jointly</i> incorporating reading level and topic distribution metadata into a variety of critical tasks in Web information systems.","cites":"31","conferencePercentile":"75"},{"venue":"WSDM","id":"43c5255cbf63d9e0828810304ff49b044d18301b","venue_1":"WSDM","year":"2010","title":"Leveraging temporal dynamics of document content in relevance ranking","authors":"Jonathan L. Elsas, Susan T. Dumais","author_ids":"2507380, 1728602","abstract":"Many web documents are dynamic, with content changing in varying amounts at varying frequencies. However, current document search algorithms have a static view of the document content, with only a single version of the document in the index at any point in time. In this paper, we present the first published analysis of using the temporal dynamics of document content to improve relevance ranking. We show that there is a strong relationship between the amount and frequency of content change and relevance. We develop a novel probabilistic document ranking algorithm that allows differential weighting of terms based on their temporal characteristics. By leveraging such content dynamics we show significant performance improvements for navigational queries.","cites":"43","conferencePercentile":"54.44444444"},{"venue":"WSDM","id":"d5550ca8cd5fe63403566673c966271930690ec2","venue_1":"WSDM","year":"2016","title":"Multi-Score Position Auctions","authors":"Denis Xavier Charles, Nikhil R. Devanur, Balasubramanian Sivan","author_ids":"1771871, 7692691, 3181897","abstract":"In this paper we propose a general family of position auctions used in paid search, which we call multi-score position auctions. These auctions contain the GSP auction and the GSP auction with squashing as special cases. We show experimentally that these auctions contain special cases that perform better than the GSP auction with squashing, in terms of revenue, and the number of clicks on ads. In particular, we study in detail the special case that squashes the first slot alone and show that this beats pure squashing (which squashes all slots uniformly). We study the equilibria that arise in this special case to examine both the first order and the second order effect of moving from the squashing-all-slots auction to the squash-only-the-top-slot auction. For studying the second order effect, we simulate auctions using the value-relevance correlated distribution suggested in Lahaie and Pennock [2007]. Since this distribution is derived from a study of value and relevance distributions in Yahoo! we believe the insights derived from this simulation to be valuable. For measuring the first order effect, in addition to the said simulation, we also conduct experiments using auction data from Bing over several weeks that includes a random sample of all auctions.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"98f50ecbb2ee6faf04478857388f158f1077076b","venue_1":"WSDM","year":"2012","title":"Inferring social ties across heterogenous networks","authors":"Jie Tang, Tiancheng Lou, Jon M. Kleinberg","author_ids":"1750766, 1712875, 3371403","abstract":"It is well known that different types of social ties have essentially different influence on people. However, users in online social networks rarely categorize their contacts into \"family\", \"colleagues\", or \"classmates\". While a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of inferring social ties over multiple heterogeneous networks. In this work, we develop a framework for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of inferring the type of social relationships in a target network by borrowing knowledge from a different source network. Our empirical study on five different genres of networks validates the effectiveness of the proposed framework. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, the proposed framework is able to obtain an F1-score of 90% (8-28% improvements over alternative methods) for inferring manager-subordinate relationships in an enterprise email network.","cites":"55","conferencePercentile":"90.69767442"},{"venue":"WSDM","id":"0d3932acf7fbd3824c239cfff6bdf9a144e3b1e4","venue_1":"WSDM","year":"2016","title":"Information Evolution in Social Networks","authors":"Lada A. Adamic, Thomas M. Lento, Eytan Adar, Pauline C. Ng","author_ids":"1778398, 2148602, 2630700, 2329892","abstract":"Social networks readily transmit information, albeit with less than perfect fidelity. We present a large-scale measurement of this imperfect information copying mechanism by examining the dissemination and evolution of thousands of memes, collectively replicated hundreds of millions of times in the online social network Facebook. The information undergoes an evolutionary process that exhibits several regularities. A meme's mutation rate characterizes the population distribution of its variants, in accordance with the Yule process. Variants further apart in the diffusion cascade have greater edit distance, as would be expected in an iterative, imperfect replication process. Some text sequences can confer a replicative advantage; these sequences are abundant and transfer \"laterally\" between different memes. Subpopulations of the social network can preferentially transmit a specific variant of a meme if the variant matches their beliefs or culture. Understanding the mechanism driving change in diffusing information has important implications for how we interpret and harness the information that reaches us through our social networks.","cites":"8","conferencePercentile":"98.38709677"},{"venue":"WSDM","id":"4d76072a133a39144b5fe4b16611b603b641d487","venue_1":"WSDM","year":"2012","title":"Identifying content for planned events across social media sites","authors":"Hila Becker, Dan Iter, Mor Naaman, Luis Gravano","author_ids":"3054813, 3310951, 1687465, 1684012","abstract":"User-contributed Web data contains rich and diverse information about a variety of events in the physical world, such as shows, festivals, conferences and more. This information ranges from known event features (e.g., title, time, location) posted on event aggregation platforms (e.g., Last.fm events, EventBrite, Facebook events) to discussions and reactions related to events shared on different social media sites (e.g., Twitter, YouTube, Flickr). In this paper, we focus on the challenge of automatically identifying user-contributed content for events that are planned and, therefore, known in advance, across different social media sites. We mine event aggregation platforms to extract event features, which are often noisy or missing. We use these features to develop query formulation strategies for retrieving content associated with an event on different social media sites. Further, we explore ways in which event content identified on one social media site can be used to retrieve additional relevant event content on other social media sites. We apply our strategies to a large set of user-contributed events, and analyze their effectiveness in retrieving relevant event content from Twitter, YouTube, and Flickr.","cites":"43","conferencePercentile":"86.04651163"},{"venue":"WSDM","id":"db99c9a793944af259b725f75e790fe73960b39a","venue_1":"WSDM","year":"2013","title":"Modeling the impact of lifestyle on health at scale","authors":"Adam Sadilek, Henry A. Kautz","author_ids":"1743087, 1690271","abstract":"Research in computational epidemiology to date has concentrated on estimating summary statistics of populations and simulated scenarios of disease outbreaks. Detailed studies have been limited to small domains, as scaling the methods involved poses considerable challenges. By contrast, we model the associations of a large collection of social and environmental factors with the health of particular individuals. Instead of relying on surveys, we apply scalable machine learning techniques to noisy data mined from online social media and infer the health state of any given person in an automated way. We show that the learned patterns can be subsequently leveraged in descriptive as well as predictive fine-grained models of human health. Using a unified statistical model, we quantify the impact of social status, exposure to pollution, interpersonal interactions, and other important lifestyle factors on one's health. Our model explains more than 54% of the variance in people's health (as estimated from their online communication), and predicts the future health status of individuals with 91% accuracy. Our methods complement traditional studies in life sciences, as they enable us to perform large-scale and timely measurement, inference, and prediction of previously elusive factors that affect our everyday lives.","cites":"23","conferencePercentile":"77.7173913"},{"venue":"WSDM","id":"2d10744b3f6e1a8afd10abaca41c12f99cfffc6f","venue_1":"WSDM","year":"2015","title":"Learning to Recommend Related Entities to Search Users","authors":"Bin Bi, Hao Ma, Bo-June Paul Hsu, Wei Chu, Kuansan Wang, Junghoo Cho","author_ids":"2555622, 7186567, 1842882, 3011017, 3163131, 2581979","abstract":"Over the past few years, major web search engines have introduced knowledge bases to offer popular facts about people, places, and things on the entity pane next to regular search results. In addition to information about the entity searched by the user, the entity pane often provides a ranked list of related entities. To keep users engaged, it is important to develop a recommendation model that tailors the related entities to individual user interests. We propose a probabilistic Three-way Entity Model (TEM) that provides personalized recommendation of related entities using three data sources: knowledge base, search click log, and entity pane log. Specifically, TEM is capable of extracting hidden structures and capturing underlying correlations among users, main entities, and related entities. Moreover, the TEM model can also exploit the click signals derived from the entity pane log. We further provide an inference technique to learn the parameters in TEM, and propose a principled preference learning method specifically designed for ranking related entities. Extensive experiments with two real-world datasets show that TEM with our probabilistic framework significantly outperforms a state of the art baseline, confirming the effectiveness of TEM and our probabilistic framework in related entity recommendation.","cites":"6","conferencePercentile":"64.75409836"},{"venue":"WSDM","id":"2ac0e0d78aefeb412243bcbadb3d5faa3c5a3ead","venue_1":"WSDM","year":"2014","title":"On building entity recommender systems using user click log and freebase knowledge","authors":"Xiao Yu, Hao Ma, Bo-June Paul Hsu, Jiawei Han","author_ids":"4366206, 7186567, 1842882, 1722175","abstract":"Due to their commercial value, <i>search engines</i> and <i>recommender systems</i> have become two popular research topics in both industry and academia over the past decade. Although these two fields have been actively and extensively studied separately, researchers are beginning to realize the importance of the scenarios at their intersection: providing an integrated search and information discovery user experience. In this paper, we study a novel application, <i>i.e.</i>, personalized entity recommendation for search engine users, by utilizing user click log and the knowledge extracted from Freebase.\n To better bridge the gap between search engines and recommender systems, we first discuss important heuristics and features of the datasets. We then propose a generic, robust, and time-aware personalized recommendation framework to utilize these heuristics and features at different granularity levels. Using movie recommendation as a case study, with user click log dataset collected from a widely used commercial search engine, we demonstrate the effectiveness of our proposed framework over other popular and state-of-the-art recommendation techniques.","cites":"17","conferencePercentile":"87.82051282"},{"venue":"WSDM","id":"0f726e8637eeb761e48294e75aaefa013f99905f","venue_1":"WSDM","year":"2013","title":"Robust query rewriting using anchor data","authors":"Nick Craswell, Bodo Billerbeck, Dennis Fetterly, Marc Najork","author_ids":"1703980, 1748336, 1780761, 1763978","abstract":"Query rewriting algorithms can be used as a form of query expansion, by combining the user's original query with automatically generated rewrites. Rewriting algorithms bring linguistic datasets to bear without the need for iterative relevance feedback, but most studies of rewriting have used proprietary datasets such as large-scale search logs. By contrast this paper uses readily available data, particularly ClueWeb09 link text with over 1.2 billion anchor phrases, to generate rewrites. To avoid overfitting, our initial analysis is performed using Million Query Track queries, leading us to identify three algorithms which perform well. We then test the algorithms on Web and newswire data. Results show good properties in terms of robustness and early precision.","cites":"5","conferencePercentile":"27.7173913"},{"venue":"WSDM","id":"409f9924f8e7718595866de80805a22c65b54daa","venue_1":"WSDM","year":"2012","title":"Of hammers and nails: an empirical comparison of three paradigms for processing large graphs","authors":"Marc Najork, Dennis Fetterly, Alan Halverson, Krishnaram Kenthapadi, Sreenivas Gollapudi","author_ids":"1763978, 1780761, 1986309, 1769861, 1695567","abstract":"Many phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (<i>e.g.</i> PageRank) becomes challenging.\n This paper presents an empirical study of computations on such large graphs in three well-studied platform models, <i>viz</i>., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs <i>viz</i>., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.","cites":"11","conferencePercentile":"40.11627907"},{"venue":"WSDM","id":"637d77f85e65ead36a1fa16c1f0a3a03f03507eb","venue_1":"WSDM","year":"2012","title":"How user behavior is related to social affinity","authors":"Rina Panigrahy, Marc Najork, Yinglian Xie","author_ids":"1679451, 1763978, 1745402","abstract":"Previous research has suggested that people who are in the same social circle exhibit similar behaviors and tastes. The rise of social networks gives us insights into the social circles of web users, and recommendation services (including search engines, advertisement engines, and collaborative filtering engines) provide a motivation to adapt recommendations to the interests of the audience. An important primitive for supporting these applications is the ability to quantify how connected two users are in a social network. The shortest-path distance between a pair of users is an obvious candidate measure. This paper introduces a new measure of \"affinity\" in social networks that takes into account not only the distance between two users, but also the number of edge-disjoint paths between them, i.e. the \"robustness\" of their connection. Our measure is based on a sketch-based approach, and affinity queries can be answered extremely efficiently (at the expense of a one-time offline sketch computation). We compare this affinity measure against the \"approximate shortest-path distance\", a sketch-based distance measure with similar efficiency characteristics. Our empirical study is based on a Hotmail email exchange graph combined with demographic information and Bing query history, and a Twitter mention-graph together with the text of the underlying tweets. We found that users who are close to each other - either in terms of distance or affinity - have a higher similarity in terms of demographics, queries, and tweets.","cites":"11","conferencePercentile":"40.11627907"},{"venue":"WSDM","id":"1b255da4a909b725e415cb2ae18bc14d92326372","venue_1":"WSDM","year":"2010","title":"A sketch-based distance oracle for web-scale graphs","authors":"Atish Das Sarma, Sreenivas Gollapudi, Marc Najork, Rina Panigrahy","author_ids":"2541992, 1695567, 1763978, 1679451","abstract":"We study the fundamental problem of computing distances between nodes in large graphs such as the web graph and social networks. Our objective is to be able to answer distance queries between pairs of nodes in real time. Since the standard shortest path algorithms are expensive, our approach moves the time-consuming shortest-path computation offline, and at query time only looks up precomputed values and performs simple and fast computations on these precomputed values. More specifically, during the offline phase we compute and store a small \"sketch\" for each node in the graph, and at query-time we look up the sketches of the source and destination nodes and perform a simple computation using these two sketches to estimate the distance.","cites":"47","conferencePercentile":"57.77777778"},{"venue":"WSDM","id":"51d4d5f46cfd5f082fcf6cb2b8d49c1e03bf1b9e","venue_1":"WSDM","year":"2009","title":"Less is more: sampling the neighborhood graph makes SALSA better and faster","authors":"Marc Najork, Sreenivas Gollapudi, Rina Panigrahy","author_ids":"1763978, 1695567, 1679451","abstract":"In this paper, we attempt to improve the effectiveness and the efficiency of query-dependent link-based ranking algorithms such as HITS, MAX and SALSA. All these ranking algorithms view the results of a query as nodes in the web graph, expand the result set to include neighboring nodes, and compute scores on the induced neighborhood graph. In previous work it was shown that SALSA in particular is substantially more effective than query-independent link-based ranking algorithms such as PageRank. In this work, we show that whittling down the neighborhood graph through consistent sampling of nodes and edges makes SALSA and its cousins both faster (more efficient) and better (more effective). We offer a hypothesis as to why \"less is more\", <i>i.e.</i> why using a reduced graph improves performance.","cites":"28","conferencePercentile":"63.15789474"},{"venue":"WSDM","id":"2226c7ca7a035133836d55b45005bbe029b79b39","venue_1":"WSDM","year":"2010","title":"Anatomy of the long tail: ordinary people with extraordinary tastes","authors":"Sharad Goel, Andrei Z. Broder, Evgeniy Gabrilovich, Bo Pang","author_ids":"3184636, 7693748, 1718798, 7781208","abstract":"The success of \"infinite-inventory\" retailers such as Amazon.com and Netflix has been ascribed to a \"long tail\" phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these \"worst sellers,\" unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic.\n Our findings thus suggest an additional factor in the success of infinite-inventory retailers, namely, that tail availability may boost head sales by offering consumers the convenience of \"one-stop shopping\" for both their mainstream and niche interests. This hypothesis is further supported by our theoretical analysis that presents a simple model in which shared inventory stores, such as Amazon Marketplace, gain a clear advantage by satisfying tail demand, helping to explain the emergence and increasing popularity of such retail arrangements. Hence, we believe that the return-on-investment (ROI) of niche products goes beyond direct revenue, extending to second-order gains associated with increased consumer satisfaction and repeat patronage. More generally, our findings call into question the conventional wisdom that specialty products only appeal to a minority of consumers.","cites":"49","conferencePercentile":"60"},{"venue":"WSDM","id":"4c84386f9acb554431729dedd38c6148ba00a51b","venue_1":"WSDM","year":"2013","title":"Sharding social networks","authors":"Quang Duong, Sharad Goel, Jake M. Hofman, Sergei Vassilvitskii","author_ids":"3075600, 3184636, 2081766, 1749789","abstract":"Online social networking platforms regularly support hundreds of millions of users, who in aggregate generate substantially more data than can be stored on any single physical server. As such, user data are distributed, or sharded, across many machines. A key requirement in this setting is rapid retrieval not only of a given user's information, but also of all data associated with his or her social contacts, suggesting that one should consider the topology of the social network in selecting a sharding policy. In this paper we formalize the problem of efficiently sharding large social network databases, and evaluate several sharding strategies, both analytically and empirically. We find that random sharding---the de facto standard---results in provably poor performance even when frequently accessed nodes are replicated to many shards. By contrast, we demonstrate that one can substantially reduce querying costs by identifying and assigning tightly knit communities to shards. In particular, our theoretical analysis motivates a novel, scalable sharding algorithm that outperforms both random and location-based sharding schemes.","cites":"6","conferencePercentile":"33.15217391"},{"venue":"WSDM","id":"c486f9077f3c0d94482ddd34b479b53de85bac5e","venue_1":"WSDM","year":"2014","title":"Who likes it more?: mining worth-recommending items from long tails by modeling relative preference","authors":"Yu-Chieh Ho, Yi-Ting Chiang, Jane Yung-jen Hsu","author_ids":"3331636, 1736889, 1717095","abstract":"Recommender systems are useful tools that help people to filter and explore massive information. While the accuracy of recommender systems is important, many recent research indicated that focusing merely on accuracy not only is insufficient to meet user needs, but also may be harmful. Other characteristics such as novelty, unexpectedness and diversity should also be taken into consideration. Previous work has shown that more the sales of long-tail items could be more beneficial to both customers and some business models. However, the majority of collaborative filtering approaches tends to recommend popular selling items. \n In this work, we focus on long-tail item promotion and aggregate diversity enhancement, and propose a novel approach which diversifies the results of recommender systems by considering ``recommendations\" as resources to be allocated to the items. Our approach increases the quantity and quality of long-tail item recommendations by adding more variation into the recommendation and maintains a certain level of accuracy simultaneously. The experimental results show that this approach can discover more worth-recommending items from Long Tails and improves user experience.","cites":"1","conferencePercentile":"17.30769231"},{"venue":"WSDM","id":"95e5a0bf77b28b87263115421cc1ceb321adb407","venue_1":"WSDM","year":"2016","title":"An Information-Theoretic Approach to Individual Sequential Data Sanitization","authors":"Luca Bonomi, Liyue Fan, Hongxia Jin","author_ids":"2809221, 2430152, 1705713","abstract":"Fine-grained, personal data has been largely, continuously generated nowadays, such as location check-ins, web histories, physical activities, etc. Those data sequences are typically shared with untrusted parties for data analysis and promotional services. However, the individually-generated sequential data contains behavior patterns and may disclose sensitive information if not properly sanitized. Furthermore, the utility of the released sequence can be adversely affected by sanitization techniques. In this paper, we study the problem of individual sequence data sanitization with minimum utility loss, given user-specified sensitive patterns. We propose a privacy notion based on information theory and sanitize sequence data via generalization. We show the optimization problem is hard and develop two efficient heuristic solutions. Extensive experimental evaluations are conducted on real-world datasets and the results demonstrate the efficiency and effectiveness of our solutions.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"751e342bdd3cd7a34b0cadd2fcbb00dcc08bea77","venue_1":"WSDM","year":"2015","title":"Personalized Mobile App Recommendation: Reconciling App Functionality and User Privacy Preference","authors":"Bin Liu, Deguang Kong, Lei Cen, Neil Zhenqiang Gong, Hongxia Jin, Hui Xiong","author_ids":"1702756, 8720637, 2422924, 1990973, 1705713, 1707713","abstract":"Recent years have witnessed a rapid adoption of mobile devices and a dramatic proliferation of mobile applications (Apps for brevity). However, the large number of mobile Apps makes it difficult for users to locate relevant Apps. Therefore, recommending Apps becomes an urgent task. Traditional recommendation approaches focus on learning the interest of a user and the functionality of an item (e.g., an App) from a set of user-item ratings, and they recommend an item to a user if the item's functionality well matches the user's interest. However, Apps could have privileges to access a user's sensitive resources ( e.g., contact, message, and location). As a result, a user chooses an App not only because of its functionality, but also because it respects the user's privacy preference. To the best of our knowledge, this paper presents the first systematic study on incorporating both interest-functionality interactions and users' privacy preferences to perform personalized App recommendations. Specifically, we first construct a new model to capture the trade-off between functionality and user privacy preference. Then we crawled a real-world dataset (16,344 users, 6,157 Apps, and 263,054 ratings) from Google Play and use it to comprehensively evaluate our model and previous methods. We find that our method consistently and substantially outperforms the state-of-the-art approaches, which implies the importance of user privacy preference on personalized App recommendations. Moreover, we explore the impact of different levels of privacy information on the performances of our method, which gives us insights on what resources are more likely to be treated as private by users and influence users' behaviors at selecting Apps.","cites":"5","conferencePercentile":"56.55737705"},{"venue":"WSDM","id":"376c9da3a7b046dd16ba265f93523b437746639b","venue_1":"WSDM","year":"2014","title":"Entity linking at the tail: sparse signals, unknown entities, and phrase models","authors":"Yuzhe Jin, Emre Kiciman, Kuansan Wang, Ricky Loynd","author_ids":"2450925, 1754874, 3163131, 2815483","abstract":"Web search is seeing a paradigm shift from keyword based search to an entity-centric organization of web data. To support web search with this deeper level of understanding, a web-scale entity linking system must have 3 key properties: First, its feature extraction must be robust to the diversity of web documents and their varied writing styles and content structures. Second, it must maintain high-precision linking for \"tail\" (unpopular) entities that is robust to the existence of confounding entities outside of the knowledge base and entity profiles with minimal information. Finally, the system must represent large-scale knowledge bases with a scalable and powerful feature representation. We have built and deployed a web-scale unsupervised entity linking system for a commercial search engine that addresses these requirements by combining new developments in sparse signal recovery to identify the most discriminative features from noisy, free-text web documents; explicit modeling of out-of-knowledge-base entities to improve precision at the tail; and the development of a new phrase-unigram language model to efficiently capture high-order dependencies in lexical features. Using a knowledge base of 100M unique people from a popular social networking site, we present experimental results in the challenging domain of people-linking at the tail, where most entities have limited web presence. Our experimental results show that this system substantially improves on the precision-recall tradeoff over baseline methods, achieving precision over 95% with recall over 60%.","cites":"8","conferencePercentile":"68.58974359"},{"venue":"WSDM","id":"548017ed15b0a983ae46be1e5727957e5f69ee08","venue_1":"WSDM","year":"2013","title":"Estimating content concreteness for finding comprehensible documents","authors":"Shinya Tanaka, Adam Jatowt, Makoto P. Kato, Katsumi Tanaka","author_ids":"8203240, 1774986, 2877085, 1750132","abstract":"Document comprehensibility is one of key factors determining document quality and, in result, user's satisfaction. Relevant web pages are of little utility if they are incomprehensible or impose too much cognitive burden on readers. Traditional measures of text difficulty focus often on syntactic factors of text such as sentence length, word length, syllable count, or they utilize fixed list of common terms. However, document comprehensibility depends on many factors, of which concreteness and the ease of concept visualization are crucial ones. In this paper, we first propose a method for predicting the concreteness of terms using SVM regression. We then extend it to calculating document concreteness level. The experimental results indicate satisfactory accuracy in estimating both term and document concreteness as well as demonstrate positive correlation between the document concreteness and comprehensibility. Our ultimate goal is to enable comprehension-driven search, which will return both relevant and comprehensible results.","cites":"5","conferencePercentile":"27.7173913"},{"venue":"WSDM","id":"f049ea3ca734c217c380a1802d15a6d85378f55d","venue_1":"WSDM","year":"2012","title":"Efficient misbehaving user detection in online video chat services","authors":"Hanqiang Cheng, Yu-Li Liang, Xinyu Xing, Xue Liu, Richard Han, Qin Lv, Shivakant Mishra","author_ids":"2629472, 2143336, 1680554, 1723807, 1719013, 2345189, 1751290","abstract":"Online video chat services, such as Chatroulette, Omegle, and vChatter are becoming increasingly popular and have attracted millions of users. One critical problem encountered in such applications is the presence of misbehaving users (\"flashers\") and obscene content. Automatically filtering out obscene content from these systems in an <i>efficient</i> manner poses a difficult challenge. This paper presents a novel Fine-Grained Cascaded (FGC) classification solution that significantly speeds up the compute-intensive process of classifying misbehaving users by dividing image feature extraction into multiple stages and filtering out easily classified images in earlier stages, thus saving unnecessary computation costs of feature extraction in later stages. Our work is further enhanced by integrating new webcam-related contextual information (illumination and color) into the classification process, and a 2-stage soft margin SVM algorithm for combining multiple features. Evaluation results using real-world data set obtained from Chatroulette show that the proposed FGC based classification solution significantly outperforms state-of-the-art techniques.","cites":"6","conferencePercentile":"23.8372093"},{"venue":"WSDM","id":"4c12bf7749dc121a9b3928aaf835b45f329fad6b","venue_1":"WSDM","year":"2016","title":"Improving Website Hyperlink Structure Using Server Logs","authors":"Ashwin Paranjape, Robert West, Leila Zia, Jure Leskovec","author_ids":"2617475, 1913101, 2538277, 1702139","abstract":"Good websites should be easy to navigate via hyperlinks, yet maintaining a high-quality link structure is difficult. Identifying pairs of pages that should be linked may be hard for human editors, especially if the site is large and changes frequently. Further, given a set of useful link candidates, the task of incorporating them into the site can be expensive, since it typically involves humans editing pages. In the light of these challenges, it is desirable to develop data-driven methods for automating the link placement task. Here we develop an approach for automatically finding useful hyperlinks to add to a website. We show that passively collected server logs, beyond telling us which existing links are useful, also contain implicit signals indicating which nonexistent links would be useful if they were to be introduced. We leverage these signals to model the future usefulness of yet nonexistent links. Based on our model, we define the problem of link placement under budget constraints and propose an efficient algorithm for solving it. We demonstrate the effectiveness of our approach by evaluating it on Wikipedia, a large website for which we have access to both server logs (used for finding useful new links) and the complete revision history (containing a ground truth of new links). As our method is based exclusively on standard server logs, it may also be applied to any other website, as we show with the example of the biomedical research site Simtk.","cites":"3","conferencePercentile":"84.40860215"},{"venue":"WSDM","id":"c48f824497f9f2eda15ff7bf81c8828b243073d3","venue_1":"WSDM","year":"2015","title":"New Directions in Recommender Systems","authors":"Jure Leskovec","author_ids":"1702139","abstract":"Recommender systems are an integral part of how we experience the Web today and they have become so ubiquitous that we do not even notice them anymore. However, today's recommender systems mostly treat items they recommend as black boxes and primarily focus on extracting correlations and co-counts from user behavior data. In this talk I argue that next generation recommender systems will require deep understanding of items being recommended as well as modeling the relationships between those items. I will present examples how auxiliary data about items (descriptions, reviews, product specifications) can be used to improve recommendations.","cites":"0","conferencePercentile":"13.1147541"},{"venue":"WSDM","id":"afe782c243cf4bec976625c945f089f9d6413a16","venue_1":"WSDM","year":"2014","title":"The last click: why users give up information network navigation","authors":"Aju Thalappillil Scaria, Rose Marie Philip, Robert West, Jure Leskovec","author_ids":"7681167, 2479367, 1913101, 1702139","abstract":"An important part of finding information online involves clicking from page to page until an information need is fully satisfied. This is a complex task that can easily be frustrating and force users to give up prematurely. An empirical analysis of what makes users abandon click-based navigation tasks is hard, since most passively collected browsing logs do not specify the exact target page that a user was trying to reach. We propose to overcome this problem by using data collected via Wikispeedia, a Wikipedia-based human-computation game, in which users are asked to navigate from a start page to an explicitly given target page (both Wikipedia articles) by only tracing hyperlinks between Wikipedia articles. Our contributions are two-fold. First, by analyzing the differences between successful and abandoned navigation paths, we aim to understand what types of behavior are indicative of users giving up their navigation task. We also investigate how users make use of back clicks during their navigation. We find that users prefer backtracking to high-degree nodes that serve as landmarks and hubs for exploring the network of pages. Second, based on our analysis, we build statistical models for predicting whether a user will finish or abandon a navigation task, and if the next action will be a back click. Being able to predict these events is important as it can potentially help us design more human-friendly browsing interfaces and retain users who would otherwise have given up navigating a website.","cites":"8","conferencePercentile":"68.58974359"},{"venue":"WSDM","id":"283c8bde15f1b4160ee2842b2c7336521a5e49e4","venue_1":"WSDM","year":"2014","title":"Detecting cohesive and 2-mode communities indirected and undirected networks","authors":"Jaewon Yang, Julian J. McAuley, Jure Leskovec","author_ids":"4121200, 2255953, 1702139","abstract":"Networks are a general language for representing relational information among objects. An effective way to model, reason about, and summarize networks, is to discover sets of nodes with common connectivity patterns. Such sets are commonly referred to as network communities. Research on network community detection has predominantly focused on identifying communities of densely connected nodes in undirected networks.\n In this paper we develop a novel overlapping community detection method that scales to networks of millions of nodes and edges and advances research along two dimensions: the connectivity structure of communities, and the use of edge directedness for community detection. First, we extend traditional definitions of network communities by building on the observation that nodes can be densely interlinked in two different ways: In cohesive communities nodes link to each other, while in 2-mode communities nodes link in a bipartite fashion, where links predominate between the two partitions rather than inside them. Our method successfully detects both 2-mode as well as cohesive communities, that may also overlap or be hierarchically nested. Second, while most existing community detection methods treat directed edges as though they were undirected, our method accounts for edge directions and is able to identify novel and meaningful community structures in both directed and undirected networks, using data from social, biological, and ecological domains.","cites":"11","conferencePercentile":"80.12820513"},{"venue":"WSDM","id":"10e44c294a968ca91e361fac44aa9d0f2cdf3bd3","venue_1":"WSDM","year":"2013","title":"Overlapping community detection at scale: a nonnegative matrix factorization approach","authors":"Jaewon Yang, Jure Leskovec","author_ids":"4121200, 1702139","abstract":"Network communities represent basic structures for understanding the organization of real-world networks. A community (also referred to as a module or a cluster) is typically thought of as a group of nodes with more connections amongst its members than between its members and the remainder of the network. Communities in networks also overlap as nodes belong to multiple clusters at once. Due to the difficulties in evaluating the detected communities and the lack of scalable algorithms, the task of overlapping community detection in large networks largely remains an open problem.\n In this paper we present BIGCLAM (Cluster Affiliation Model for Big Networks), an overlapping community detection method that scales to large networks of millions of nodes and edges. We build on a novel observation that overlaps between communities are densely connected. This is in sharp contrast with present community detection methods which implicitly assume that overlaps between communities are sparsely connected and thus cannot properly extract overlapping communities in networks. In this paper, we develop a model-based community detection algorithm that can detect densely overlapping, hierarchically nested as well as non-overlapping communities in massive networks. We evaluate our algorithm on 6 large social, collaboration and information networks with ground-truth community information. Experiments show state of the art performance both in terms of the quality of detected communities as well as in speed and scalability of our algorithm.","cites":"109","conferencePercentile":"100"},{"venue":"WSDM","id":"0be50cf2aa76d21e6297f097f787908949f1ee74","venue_1":"WSDM","year":"2013","title":"Structure and Dynamics of Information Pathways in Online Media","authors":"Manuel Gomez-Rodriguez, Jure Leskovec, Bernhard Schölkopf","author_ids":"1804327, 1702139, 1707625","abstract":"Diffusion of information, spread of rumors and infectious diseases are all instances of stochastic processes that occur over the edges of an underlying network. Many times networks over which contagions spread are unobserved, and such networks are often dynamic and change over time. In this paper, we investigate the problem of inferring dynamic networks based on information diffusion data. We assume there is an unobserved dynamic network that changes over time, while we observe the results of a dynamic process spreading over the edges of the network. The task then is to infer the edges and the dynamics of the underlying network.\n We develop an on-line algorithm that relies on stochastic convex optimization to efficiently solve the dynamic network inference problem. We apply our algorithm to information diffusion among 3.3 million mainstream media and blog sites and experiment with more than 179 million different pieces of information spreading over the network in a one year period. We study the evolution of information pathways in the online media space and find interesting insights. Information pathways for general recurrent topics are more stable across time than for on-going news events. Clusters of news media sites and blogs often emerge and vanish in matter of days for on-going news events. Major social movements and events involving civil population, such as the Libyan'{}s civil war or Syria'{}s uprise, lead to an increased amount of information pathways among blogs as well as in the overall increase in the network centrality of blogs and social media sites.","cites":"69","conferencePercentile":"97.82608696"},{"venue":"WSDM","id":"7d67b551fcc35358334fa1f5899898bab35322d0","venue_1":"WSDM","year":"2013","title":"On the quest of discovering cultural trails in social media","authors":"Ruth Olimpia Garcia Gavilanes","author_ids":"7994754","abstract":"With the constant increasing reach of the Web and in particular of Social Media, people create and share content that harbors information about habits, norms, preferences and values. Consequently, studying how culture influences users in online social media has increased the interest of several sectors such as the advertising industry, search engines and corporations. As a consequence, anthropological and computational models need to interact and complement each other to better target these new demands. Recently, several studies have analyzed culture from large-scale data but not many took into consideration the cultural models proposed by anthropological theory. By carrying out several experiments on large-scale data from the Web, we propose to combine theoretical concepts of culture with information technology techniques to process, analyze, model and interpret data from the Web. We plan to discover synergies between traditional social studies of culture and those derived from our experiments.","cites":"1","conferencePercentile":"14.13043478"},{"venue":"WSDM","id":"1593a514af481b2a0f6909a234615b341a562fe2","venue_1":"WSDM","year":"2012","title":"Effects of user similarity in social media","authors":"Ashton Anderson, Daniel P. Huttenlocher, Jon M. Kleinberg, Jure Leskovec","author_ids":"1969479, 1713089, 3371403, 1702139","abstract":"There are many settings in which users of a social media application provide evaluations of one another. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. Earlier work has studied how the <i>relative status</i> between two users - that is, their comparative levels of status in the group - affects the types of evaluations that one user gives to another.\n Here we study how <i>similarity</i> in the characteristics of two users can affect the evaluation one user provides of another. We analyze this issue under a range of natural similarity measures, showing how the interaction of similarity and status can produce strong effects. Among other consequences, we find that evaluations are less status-driven when users are more similar to each other; and we use effects based on similarity to provide a plausible mechanism for a complex phenomenon observed in studies of user evaluation, that evaluations are particularly low among users of roughly equal status.\n Our work has natural applications to the prediction of evaluation outcomes based on user characteristics, and the use of similarity information makes possible a novel application that we introduce here - to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.","cites":"40","conferencePercentile":"82.55813953"},{"venue":"WSDM","id":"659c51dbd93d870ea120a2fc227c2f08bd2c6d7a","venue_1":"WSDM","year":"2012","title":"The life and death of online groups: predicting group growth and longevity","authors":"Sanjay Ram Kairam, Dan J. Wang, Jure Leskovec","author_ids":"3071429, 7496221, 1702139","abstract":"We pose a fundamental question in understanding how to identify and design successful communities: What factors predict whether a community will grow and survive in the long term? Social scientists have addressed this question extensively by analyzing offline groups which endeavor to attract new members, such as social movements, finding that new individuals are influenced strongly by their ties to members of the group. As a result, prior work on the growth of communities has treated growth primarily as a diffusion processes, leading to findings about group evolution which can be difficult to explain. The proliferation of online social networks and communities, however, has created new opportunities to study, at a large scale and with very fine resolution, the mechanisms which lead to the formation, growth, and demise of online groups.\n In this paper, we analyze data from several thousand online social networks built on the Ning platform with the goal of understanding the factors contributing to the growth and longevity of groups within these networks. Specifically, we investigate the role that two types of growth (growth through diffusion and growth by other means) play during a group's formative stages from the perspectives of both the individual member and the group. Applying these insights to a population of groups of different ages and sizes, we build a model to classify groups which will grow rapidly over the short-term and long-term. Our model achieves over 79% accuracy in predicting group growth over the following two months and over 78% accuracy in predictions over the following two years. We utilize a similar approach to predict which groups will die within a year. The results of our combined analysis provide insight into how both early non-diffusion growth and a complex set of network constraints appear to contribute to the initial and continued growth and success of groups within social networks. Finally we discuss implications of this work for the design, maintenance, and analysis of online communities.","cites":"69","conferencePercentile":"95.34883721"},{"venue":"WSDM","id":"017372aec4b163ed6300499d40e316d2a0a7a9dd","venue_1":"WSDM","year":"2011","title":"Patterns of temporal variation in online media","authors":"Jaewon Yang, Jure Leskovec","author_ids":"4121200, 1702139","abstract":"Online content exhibits rich temporal dynamics, and diverse realtime user generated content further intensifies this process. However, temporal patterns by which online content grows and fades over time, and by which different pieces of content compete for attention remain largely unexplored.\n We study temporal patterns associated with online content and how the content's popularity grows and fades over time. The attention that content receives on the Web varies depending on many factors and occurs on very different time scales and at different resolutions. In order to uncover the temporal dynamics of online content we formulate a time series clustering problem using a similarity metric that is invariant to scaling and shifting. We develop the K-Spectral Centroid (<i>K-SC</i>) clustering algorithm that effectively finds cluster centroids with our similarity measure. By applying an adaptive wavelet-based incremental approach to clustering, we scale <i>K-SC</i> to large data sets.\n We demonstrate our approach on two massive datasets: a set of 580 million Tweets, and a set of 170 million blog posts and news media articles. We find that <i>K-SC</i> outperforms the K-means clustering algorithm in finding distinct shapes of time series. Our analysis shows that there are six main temporal shapes of attention of online content. We also present a simple model that reliably predicts the shape of attention by using information about only a small number of participants. Our analyses offer insight into common temporal patterns of the content on theWeb and broaden the understanding of the dynamics of human attention.","cites":"328","conferencePercentile":"98.87640449"},{"venue":"WSDM","id":"26ae93cd4549f1bd2202578e6044a288a3490d10","venue_1":"WSDM","year":"2011","title":"Correcting for missing data in information cascades","authors":"Eldar Sadikov, Montserrat Medina, Jure Leskovec, Hector Garcia-Molina","author_ids":"2147631, 2004037, 1702139, 1695250","abstract":"Transmission of infectious diseases, propagation of information, and spread of ideas and influence through social networks are all examples of diffusion. In such cases we say that a contagion spreads through the network, a process that can be modeled by a cascade graph. Studying cascades and network diffusion is challenging due to missing data. Even a single missing observation in a sequence of propagation events can significantly alter our inferences about the diffusion process.\n We address the problem of missing data in information cascades. Specifically, given only a fraction C' of the complete cascade C, our goal is to estimate the properties of the complete cascade C, such as its size or depth. To estimate the properties of C, we first formulate k-tree model of cascades and analytically study its properties in the face of missing data. We then propose a numerical method that given a cascade model and observed cascade C' can estimate properties of the complete cascade C. We evaluate our methodology using information propagation cascades in the Twitter network (70 million nodes and 2 billion edges), as well as information cascades arising in the blogosphere. Our experiments show that the k-tree model is an effective tool to study the effects of missing data in cascades. Most importantly, we show that our method (and the k-tree model) can accurately estimate properties of the complete cascade C even when 90% of the data is missing.","cites":"51","conferencePercentile":"82.02247191"},{"venue":"WSDM","id":"0067e51fc241c24d5410806feea5c99f10165efd","venue_1":"WSDM","year":"2016","title":"To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation","authors":"Makoto P. Kato, Katsumi Tanaka","author_ids":"2877085, 1750132","abstract":"We propose a method of optimizing search result presentation for queries with diverse intents, by selectively presenting query suggestions for leading users to more relevant search results. The optimization is based on a probabilistic model of users who click on query suggestions in accordance with their intents, and modified versions of intent-aware evaluation metrics that take into account the co-occurrence between intents. Showing many query suggestions simply increases a chance to satisfy users with diverse intents in this model, while it in fact requires users to spend additional time for scanning and selecting suggestions, and may result in low satisfaction for some users. Therefore, we measured the loss of time caused by query suggestion presentation by conducting a user study in different settings, and included its negative effects in our optimization problem. Our experiments revealed that the optimization of search result presentation significantly improved that of a single ranked list, and was beneficial especially for patient users. Moreover, experimental results showed that our optimization was effective particularly when intents of a query often co-occur with a small subset of intents.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"82db03db1f9f90c46b393c07e4b0e0efb3d9a16f","venue_1":"WSDM","year":"2015","title":"Exploring the Space of Topic Coherence Measures","authors":"Michael Röder, Andreas Both, Alexander Hinneburg","author_ids":"2099872, 1697447, 1972250","abstract":"Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. nFinally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.","cites":"19","conferencePercentile":"98.36065574"},{"venue":"WSDM","id":"b19a8552b32a802a8b355139c1ee1b0bb4c38b09","venue_1":"WSDM","year":"2014","title":"Modelling growth of urban crowd-sourced information","authors":"Giovanni Quattrone, Afra J. Mashhadi, Daniele Quercia, Chris Smith-Clarke, Licia Capra","author_ids":"1742046, 1743965, 1757126, 1825334, 1699832","abstract":"Urban crowd-sourcing has become a popular paradigm to harvest spatial information about our evolving cities directly from citizens. OpenStreetMap is a successful example of such paradigm, with an accuracy of its user-generated content comparable to that of curated databases (e.g., Ordnance Survey). Coverage is however low and most importantly non-uniformly distributed across the city. Being able to model the spontaneous growth of digital information in these domains is required, so to be able to plan interventions aimed at gathering content about areas that would otherwise be neglected. Inspired by models of physical urban growth developed by urban planners, we build a model of digital growth of crowd-sourced spatial information that is both easy to interpret and dynamic, so to be able to determine what factors impact growth and how these change over time. We build and test the model against five years of OpenStreetMap data for the city of London, UK. We then run the model against two other cities, chosen for their different physical and digital growth's characteristics, so to stress-test the model. We conclude with a discussion of the implications of this work on both developers and users of urban crowd-sourcing applications.","cites":"4","conferencePercentile":"44.87179487"},{"venue":"WSDM","id":"59839c895a0cddcad0e4efdc953d5ccad302f115","venue_1":"WSDM","year":"2012","title":"Auralist: introducing serendipity into music recommendation","authors":"Yuan Cao Zhang, Diarmuid Ó Séaghdha, Daniele Quercia, Tamas Jambor","author_ids":"7549820, 8311581, 1757126, 2734634","abstract":"Recommendation systems exist to help users discover content in a large body of items. An ideal recommendation system should mimic the actions of a trusted friend or expert, producing a personalised collection of recommendations that balance between the desired goals of accuracy, diversity, novelty and serendipity. We introduce the <i>Auralist</i> recommendation framework, a system that - in contrast to previous work - attempts to balance and improve all four factors simultaneously. Using a collection of novel algorithms inspired by principles of \"serendipitous discovery\", we demonstrate a method of successfully injecting serendipity, novelty and diversity into recommendations whilst limiting the impact on accuracy. We evaluate <i>Auralist</i> quantitatively over a broad set of metrics and, with a user study on music recommendation, show that <i>Auralist's</i> emphasis on serendipity indeed improves user satisfaction.","cites":"48","conferencePercentile":"87.79069767"},{"venue":"WSDM","id":"57d76d5716a0007e0fbf451866bf665887628613","venue_1":"WSDM","year":"2012","title":"Finding your friends and following them to where you are","authors":"Adam Sadilek, Henry A. Kautz, Jeffrey P. Bigham","author_ids":"1743087, 1690271, 1744846","abstract":"Location plays an essential role in our lives, bridging our online and offline worlds. This paper explores the interplay between people's location, interactions, and their social ties within a large real-world dataset. We present and evaluate Flap, a system that solves two intimately related tasks: link and location prediction in online social networks. For link prediction, Flap infers social ties by considering patterns in friendship formation, the content of people's messages, and user location. We show that while each component is a weak predictor of friendship alone, combining them results in a strong model, accurately identifying the majority of friendships. For location prediction, Flap implements a scalable probabilistic model of human mobility, where we treat users with known GPS positions as noisy sensors of the location of their friends. We explore supervised and unsupervised learning scenarios, and focus on the efficiency of both learning and inference. We evaluate Flap on a large sample of highly active users from two distinct geographical areas and show that it (1) reconstructs the entire friendship graph with high accuracy even when no edges are given; and (2) infers people's fine-grained location, even when they keep their data private and we can only access the location of their friends. Our models significantly outperform current comparable approaches to either task.","cites":"126","conferencePercentile":"98.8372093"},{"venue":"WSDM","id":"84bffe8afd1c16ec2dc8eab3a6b9203f2473d6bc","venue_1":"WSDM","year":"2010","title":"Learning similarity metrics for event identification in social media","authors":"Hila Becker, Mor Naaman, Luis Gravano","author_ids":"3054813, 1687465, 1684012","abstract":"Social media sites (e.g., Flickr, YouTube, and Facebook) are a popular distribution outlet for users looking to share their experiences and interests on the Web. These sites host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events of different type and scale. By automatically identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can enable event browsing and search in state-of-the-art search engines. To address this problem, we exploit the rich \"context\" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). Using this rich context, which includes both textual and non-textual features, we can define appropriate document similarity metrics to enable online clustering of media to events. As a key contribution of this paper, we explore a variety of techniques for learning multi-feature similarity metrics for social media documents in a principled manner. We evaluate our techniques on large-scale, real-world datasets of event images from Flickr. Our evaluation results suggest that our approach identifies events, and their associated social media documents, more effectively than the state-of-the-art strategies on which we build.","cites":"147","conferencePercentile":"86.66666667"},{"venue":"WSDM","id":"1cf3968888852581594be37aa410cc7047493f08","venue_1":"WSDM","year":"2010","title":"Tagging human knowledge","authors":"Paul Heymann, Andreas Paepcke, Hector Garcia-Molina","author_ids":"2912194, 1750481, 1695250","abstract":"A fundamental premise of tagging systems is that regular users can organize large collections for browsing and other tasks using uncontrolled vocabularies. Until now, that premise has remained relatively unexamined. Using library data, we test the tagging approach to organizing a collection. We find that tagging systems have three major large scale organizational features: consistency, quality, and completeness. In addition to testing these features, we present results suggesting that users produce tags similar to the topics designed by experts, that paid tagging can effectively supplement tags in a tagging system, and that information integration may be possible across tagging systems.","cites":"25","conferencePercentile":"37.77777778"},{"venue":"WSDM","id":"8bab54e34da91a0a2c1c28a63c23d82c50878d9e","venue_1":"WSDM","year":"2015","title":"Robust Tree-based Causal Inference for Complex Ad Effectiveness Analysis","authors":"Pengyuan Wang, Wei Sun, Dawei Yin, Jian Yang, Yi Chang","author_ids":"1771116, 1712625, 2115608, 1704854, 1787097","abstract":"As the online advertising industry has evolved into an age of diverse ad formats and delivery channels, users are exposed to complex ad treatments involving various ad characteristics. The diversity and generality of ad treatments call for accurate and causal measurement of ad effectiveness, i.e., how the ad treatment causes the changes in outcomes without the confounding effect by user characteristics. Various causal inference approaches have been proposed to measure the causal effect of ad treatments. However, most existing causal inference methods focus on univariate and binary treatment and are not well suited for complex ad treatments. Moreover, to be practical in the data-rich online environment, the measurement needs to be highly general and efficient, which is not addressed in conventional causal inference approaches. In this paper we propose a novel causal inference framework for assessing the impact of general advertising treatments. Our new framework enables analysis on uni- or multi-dimensional ad treatments, where each dimension (ad treatment factor) could be discrete or continuous. We prove that our approach is able to provide an unbiased estimation of the ad effectiveness by controlling the confounding effect of user characteristics. The framework is computationally efficient by employing a tree structure that specifies the relationship between user characteristics and the corresponding ad treatment. This tree-based framework is robust to model misspecification and highly flexible with minimal manual tuning. To demonstrate the efficacy of our approach, we apply it to two advertising campaigns. In the first campaign we evaluate the impact of different ad frequencies, and in the second one we consider the synthetic ad effectiveness across TV and online platforms. Our framework successfully provides the causal impact of ads with different frequencies in both campaigns. Moreover, it shows that the ad frequency usually has a treatment effect cap, which is usually over-estimated by naive estimation.","cites":"2","conferencePercentile":"44.26229508"},{"venue":"WSDM","id":"425185bf66ae054d3caec193de5432c818ac975c","venue_1":"WSDM","year":"2010","title":"Precomputing search features for fast and accurate query classification","authors":"Venkatesh Ganti, Arnd Christian König, Xiao Li","author_ids":"1688230, 1777452, 1723588","abstract":"Query intent classification is crucial for web search and advertising. It is known to be challenging because web queries contain less than three words on average, and so provide little signal to base classification decisions on. At the same time, the vocabulary used in search queries is vast: thus, classifiers based on word-occurrence have to deal with a very sparse feature space, and often require large amounts of training data. Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine. However, these approaches induce high latency, making them unacceptable in practice.\n In this paper, we propose a new class of features that realizes the benefit of search-based features without high latency. These leverage co-occurrence between the query keywords and tags applied to documents in search results, resulting in a significant boost to web query classification accuracy. By pre-computing the tag incidence for a suitably chosen set of keyword-combinations, we are able to generate the features online with low latency and memory requirements. We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search.","cites":"5","conferencePercentile":"4.444444444"},{"venue":"WSDM","id":"7b50bb85c36d01d4286d6808ab9430b67f7d5621","venue_1":"WSDM","year":"2011","title":"CoBayes: bayesian knowledge corroboration with assessors of unknown areas of expertise","authors":"Gjergji Kasneci, Jurgen Van Gael, David H. Stern, Thore Graepel","author_ids":"1686448, 1689857, 2776523, 1686971","abstract":"Our work aims at building probabilistic tools for constructing and maintaining large-scale knowledge bases containing entity-relationship-entity triples (statements) extracted from the Web. In order to mitigate the uncertainty inherent in information extraction and integration we propose leveraging the \"wisdom of the crowds\" by aggregating truth assessments that users provide about statements. The suggested method, CoBayes, operates on a collection of statements, a set of deduction rules (e.g. transitivity), a set of users, and a set of truth assessments of users about statements. We propose a joint probabilistic model of the truth values of statements and the expertise of users for assessing statements. The truth values of statements are interconnected through derivations based on the deduction rules. The correctness of a user's assessment for a given statement is modeled by linear mappings from user descriptions and statement descriptions into a common latent knowledge space where the inner product between user and statement vectors determines the probability that the user assessment for that statement will be correct. Bayesian inference in this complex graphical model is performed using mixed variational and expectation propagation message passing. We demonstrate the viability of CoBayes in comparison to other approaches, on realworld datasets and user feedback collected from Amazon Mechanical Turk.","cites":"29","conferencePercentile":"69.66292135"},{"venue":"WSDM","id":"30bbf66bb0b4989efa5bebe64084cc89f6a40a6f","venue_1":"WSDM","year":"2014","title":"1st Workshop on Diffusion Networks and Cascade Analytics","authors":"Peng Cui, Fei Wang, Hanghang Tong, Manuel Gomez-Rodriguez","author_ids":"1685435, 1682816, 8163721, 1804327","abstract":"Diffusion and cascades have been studied for many years in sociology, and different theoretical models have been developed. However, experimental validation has been always carried out in relatively small datasets. In recent years, with the availability of large-scale network and cascade data, research on cascading and diffusion phenomena has aroused considerable interests from various fields in computer science. One of the main goals is to discover different propagation patterns from historical cascade data. In this context, understanding the mechanisms underlying diffusion in both micro- and macro-scale levels and further develop predictive model of diffusion are fundamental problems of crucial importance.","cites":"0","conferencePercentile":"7.692307692"},{"venue":"WSDM","id":"6eef2bf0a712f77df867fdc2229730130de8f93c","venue_1":"WSDM","year":"2011","title":"Searchable web sites recommendation","authors":"Yang Song, Nam Nguyen, Li-wei He, Scott Imig, Robert Rounthwaite","author_ids":"4121542, 3564038, 1689581, 1691159, 1745567","abstract":"In this paper, we propose a new framework for searchable web sites recommendation. Given a query, our system will recommend a list of searchable web sites ranked by relevance, which can be used to complement the web page results and ads from a search engine. We model the conditional probability of a searchable web site being relevant to a given query in term of three main components: the language model of the query, the language model of the content within the web site, and the reputation of the web site searching capability (static rank). The language models for queries and searchable sites are built using information mined from client-side browsing logs. The static rank for each searchable site leverages features extracted from these client-side logs such as number of queries that are submitted to this site, and features extracted from general search engines such as the number of web pages that indexed for this site, number of clicks per query, and the dwell-time that a user spends on the search result page and on the clicked result web pages. We also learn a weight for each kind of feature to optimize the ranking performance. In our experiment, we discover 10.5 thousand searchable sites and use 5 million unique queries, extracted from one week of log data to build and demonstrate the effectiveness of our searchable web site recommendation system.","cites":"6","conferencePercentile":"24.71910112"},{"venue":"WSDM","id":"a1dfec89e0842e3e1ffaa773da8b9ef53a89fb28","venue_1":"WSDM","year":"2013","title":"Geo topic model: joint modeling of user's activity area and interests for location recommendation","authors":"Takeshi Kurashima, Tomoharu Iwata, Takahide Hoshide, Noriko Takaya, Ko Fujimura","author_ids":"3104659, 2664600, 1910950, 3327518, 3053549","abstract":"This paper proposes a method that analyzes the location log data of multiple users to recommend locations to be visited. The method uses our new topic model, called Geo Topic Model, that can jointly estimate both the user's interests and activity area hosting the user's home, office and other personal places. By explicitly modeling geographical features of locations and users, the user's interests in other features of locations, which we call latent topics, can be inferred effectively. The topic interests estimated by our model 1) lead to high accuracy in predicting visit behavior as driven by personal interests, 2) make possible the generation of recommendations when the user is in an unfamiliar area (e.g. sightseeing), and 3) enable the recommender system to suggest an interpretable representation of the user profile that can be customized by the user. Experiments are conducted using real location logs of landmark and restaurant visits to evaluate the recommendation performance of the proposed method in terms of the accuracy of predicting visit selections. We also show that our model can estimate latent features of locations such as art, nature and atmosphere as latent topics, and describe each user's preference based on them.","cites":"44","conferencePercentile":"92.39130435"},{"venue":"WSDM","id":"da43effe313a12d389a6659f63046d4551e3b815","venue_1":"WSDM","year":"2012","title":"Query suggestion by constructing term-transition graphs","authors":"Yang Song, Dengyong Zhou, Li-wei He","author_ids":"4121542, 1794680, 1689581","abstract":"Query suggestion is an interactive approach for search engines to better understand users information need. In this paper, we propose a novel query suggestion framework which leverages user re-query feedbacks from search engine logs. Specifically, we mined user query reformulation activities where the user only modifies part of the query by (1) adding terms after the query, (2) deleting terms within the query, or (3) modifying terms to new terms. We build a term-transition graph based on the mined data. Two models are proposed which address topic-level and term-level query suggestions, respectively. In the first topic-based unsupervised Pagerank model, we perform random walk on each of the topic-based term-transition graph and calculate the Pagerank for each term within a topic. Given a new query, we suggest relevant queries based on its topic distribution and term-transition probability within each topic. Our second model resembles the supervised learning-to-rank (LTR) framework, in which term modifications are treated as <i>documents</i> so that each query reformulation is treated as a training instance. A rich set of features are constructed for each (query, document) pair from Pagerank, Wikipedia, N-gram, ODP and so on. This supervised model is capable of suggesting new queries on a term level which addresses the limitation of previous methods. Experiments are conducted on a large data set from a commercial search engine. By comparing the with state-of-the-art query suggestion methods [4, 2], our proposals exhibit significant performance increase for all categories of queries.","cites":"21","conferencePercentile":"59.30232558"},{"venue":"WSDM","id":"b6ed04d9ef3b71dc97e891c48357eb6a80a8cf0f","venue_1":"WSDM","year":"2016","title":"Inferring Latent Triggers of Purchases with Consideration of Social Effects and Media Advertisements","authors":"Yusuke Tanaka, Takeshi Kurashima, Yasuhiro Fujiwara, Tomoharu Iwata, Hiroshi Sawada","author_ids":"3293115, 3104659, 1935168, 2664600, 1741725","abstract":"This paper proposes a method for inferring from single-source data the factors that trigger purchases. Here, single-source data are the histories of item purchases and media advertisement views for each individual. We assume a sequence of purchase events to be a stochastic process incorporating the following three factors: (a) user preference, (b) social effects received from other users, and (c) media advertising effects. As our user-purchase model incorporates the latent relationships between users and advertisers, it can infer the latent triggers of purchases. Experiments on real single-source data show that our model can (a) achieve high prediction accuracy for purchases, (b) discover the key information, i.e., popular items, influential users, and influential advertisers, (c) estimate the relative impact of the three factors on purchases, and (d) find user segments according to the estimated factors.","cites":"1","conferencePercentile":"52.15053763"},{"venue":"WSDM","id":"27438a32772e25f25947edc5c56c4d5b37737e18","venue_1":"WSDM","year":"2016","title":"Crowdsourcing High Quality Labels with a Tight Budget","authors":"Qi Li, Fenglong Ma, Jing Gao, Lu Su, Christopher J. Quinn","author_ids":"1682467, 2988239, 1698083, 2170726, 2466071","abstract":"In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects.","cites":"1","conferencePercentile":"52.15053763"},{"venue":"WSDM","id":"27cd0b6b31e9e527e39cbba6d1f787c004bfc2cc","venue_1":"WSDM","year":"2016","title":"Quality Management in Crowdsourcing using Gold Judges Behavior","authors":"Gabriella Kazai, Imed Zitouni","author_ids":"1688470, 1954563","abstract":"Crowdsourcing relevance labels has become an accepted practice for the evaluation of IR systems, where the task of constructing a test collection is distributed over large populations of unknown users with widely varied skills and motivations. Typical methods to check and ensure the quality of the crowd's output is to inject work tasks with known answers (gold tasks) on which workers' performance can be measured. However, gold tasks are expensive to create and have limited application. A more recent trend is to monitor the workers' interactions during a task and estimate their work quality based on their behavior. In this paper, we show that without gold behavior signals that reflect trusted interaction patterns, classifiers can perform poorly, especially for complex tasks, which can lead to high quality crowd workers getting blocked while poorly performing workers remain undetected. Through a series of crowdsourcing experiments, we compare the behaviors of trained professional judges and crowd workers and then use the trained judges' behavior signals as gold behavior to train a classifier to detect poorly performing crowd workers. Our experiments show that classification accuracy almost doubles in some tasks with the use of gold behavior data.","cites":"3","conferencePercentile":"84.40860215"},{"venue":"WSDM","id":"8cfffe4f41b37c7325ba912f98254075602934c8","venue_1":"WSDM","year":"2012","title":"When will it happen?: relationship prediction in heterogeneous information networks","authors":"Yizhou Sun, Jiawei Han, Charu C. Aggarwal, Nitesh V. Chawla","author_ids":"1792614, 1722175, 1682418, 1681386","abstract":"Link prediction, i.e., predicting links or interactions between objects in a network, is an important task in network analysis. Although the problem has attracted much attention recently, there are several challenges that have not been addressed so far. First, most existing studies focus only on link prediction in homogeneous networks, where all objects and links belong to the same type. However, in the real world, <i>heterogeneous networks</i> that consist of multi-typed objects and relationships are ubiquitous. Second, most current studies only concern the problem of <i>whether</i> a link will appear in the future but seldom pay attention to the problem of <i>when</i> it will happen. In this paper, we address both issues and study the problem of <i>predicting when a certain relationship will happen in the scenario of heterogeneous networks</i>. First, we extend the link prediction problem to the relationship prediction problem, by systematically defining both the target relation and the topological features, using a meta path-based approach. Then, we directly model the distribution of relationship building time with the use of the extracted topological features. The experiments on citation relationship prediction between authors on the DBLP network demonstrate the effectiveness of our methodology.","cites":"68","conferencePercentile":"94.18604651"},{"venue":"WSDM","id":"d9d603dcf6a1cb0e215638b076eefd54dd6fd0e7","venue_1":"WSDM","year":"2009","title":"Wikipedia pages as entry points for book search","authors":"Marijn Koolen, Gabriella Kazai, Nick Craswell","author_ids":"1781703, 1688470, 1703980","abstract":"A lot of the world's knowledge is stored in books, which, as a result of recent mass-digitisation efforts, are increasingly available online. Search engines, such as Google Books, provide mechanisms for searchers to enter this vast knowledge space using queries as entry points. In this paper, we view Wikipedia as a summary of this world knowledge and aim to use this resource to guide users to relevant books. Thus, we investigate possible ways of using Wikipedia as an intermediary between the user's query and a collection of books being searched. We experiment with traditional query expansion techniques, exploiting Wikipedia articles as rich sources of information that can augment the user's query. We then propose a novel approach based on link distance in an extended Wikipedia graph: we associate books with Wikipedia pages that cite these books and use the link distance between these nodes and the pages that match the user query as an estimation of a book's relevance to the query. Our results show that a) classical query expansion using terms extracted from query pages leads to increased precision, and b) link distance between query and book pages in Wikipedia provides a good indicator of relevance that can boost the retrieval score of relevant books in the result ranking of a book search engine.","cites":"10","conferencePercentile":"31.57894737"},{"venue":"WSDM","id":"9ec881bdd40ab7d1471a0a78d88a28bda5d26a57","venue_1":"WSDM","year":"2013","title":"Exploring structure and content on the web: extraction and integration of the semi-structured web","authors":"Tim Weninger, Jiawei Han","author_ids":"1714166, 1722175","abstract":"In this tutorial we view the World Wide Web as a type of massive, decentralized database. At present, this \"Web database\" is presented in a manner largely devoid of any consistent meaning or schema. That is not to say that Web-data lacks an underlying organization; in fact, most Web content is generated from an underlying schema-bound, or otherwise structured database. Information extraction is generally concerned with the reconciliation of unstructured or semi-structured Web content with the neatly structured database paradigm. With this Web-database in hand, researchers and practitioners have recently begun developing mechanisms which return structured results in response to an unstructured query. These new developments are a product of (1) record, list and table extraction from large numbers of semi-structured Web pages, (2) integration of these disparate extraction results into a consistent form, and (3) analysis of the newly extracted and integrated Web data.\n Among the many fruits of this line of work is the ability for semi-structured Web data to enhance the search capabilities of a schema-bound database. Alternatively, structured database records have also been used to augment Web page collections typically used by Web search engines. We will cover several key technologies, and principles explored so far in the area of Web information extraction, search and exploration.","cites":"2","conferencePercentile":"18.47826087"},{"venue":"WSDM","id":"2cbe0ba73d02aabbeefedf841203219796a551b7","venue_1":"WSDM","year":"2014","title":"Personalized entity recommendation: a heterogeneous information network approach","authors":"Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, Jiawei Han","author_ids":"4366206, 5368662, 1792614, 8155584, 2066547, 3030219, 2172095, 1722175","abstract":"Among different hybrid recommendation techniques, network-based entity recommendation methods, which utilize user or item relationship information, are beginning to attract increasing attention recently. Most of the previous studies in this category only consider a single relationship type, such as friendships in a social network. In many scenarios, the entity recommendation problem exists in a <i>heterogeneous information network</i> environment. Different types of relationships can be potentially used to improve the recommendation quality. In this paper, we study the entity recommendation problem in heterogeneous information networks. Specifically, we propose to combine heterogeneous relationship information for each user differently and aim to provide high-quality personalized recommendation results using user implicit feedback data and personalized recommendation models.\n In order to take full advantage of the relationship heterogeneity in information networks, we first introduce meta-path-based latent features to represent the connectivity between users and items along different types of paths. We then define recommendation models at both global and personalized levels and use Bayesian ranking optimization techniques to estimate the proposed models. Empirical studies show that our approaches outperform several widely employed or the state-of-the-art entity recommendation techniques.","cites":"42","conferencePercentile":"98.71794872"},{"venue":"WSDM","id":"0021a5eebca0a44a7900e68154fc65690db5d7b0","venue_1":"WSDM","year":"2014","title":"Heterogeneous graph-based intent learning with queries, web pages and Wikipedia concepts","authors":"Xiang Ren, Yujing Wang, Xiao Yu, Jun Yan, Zheng Chen, Jiawei Han","author_ids":"5368662, 2895012, 4366206, 1712810, 1705657, 1722175","abstract":"The problem of learning user search intents has attracted intensive attention from both industry and academia. However, state-of-the-art intent learning algorithms suffer from different drawbacks when only using a single type of data source. For example, query text has difficulty in distinguishing ambiguous queries; search log is bias to the order of search results and users' noisy click behaviors. In this work, we for the first time leverage three types of objects, namely queries, web pages and Wikipedia concepts collaboratively for learning generic search intents and construct a heterogeneous graph to represent multiple types of relationships between them. A novel unsupervised method called heterogeneous graph-based soft-clustering is developed to derive an intent indicator for each object based on the constructed heterogeneous graph. With the proposed co-clustering method, one can enhance the quality of intent understanding by taking advantage of different types of data, which complement each other, and make the implicit intents easier to interpret with explicit knowledge from Wikipedia concepts. Experiments on two real-world datasets demonstrate the power of the proposed method where it achieves a 9.25% improvement in terms of NDCG on search ranking task and a 4.67% enhancement in terms of Rand index on object co-clustering task compared to the best state-of-the-art method.","cites":"8","conferencePercentile":"68.58974359"},{"venue":"WSDM","id":"84ee55656bd39c56a5f1ad8fa20ab5c966ba3092","venue_1":"WSDM","year":"2014","title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications","authors":"U. Kang, Leman Akoglu, Duen Horng Chau","author_ids":"8187818, 3255268, 1793506","abstract":"Graphs are everywhere: social networks, computer net- works, mobile call networks, the World Wide Web, protein interaction networks, and many more. The lower cost of disk storage, the success of social networking websites and Web 2.0 applications, and the high availability of data sources lead to graphs being generated at unprecedented size. They are now measured in terabytes or even petabytes, with more than billions of nodes and edges.\n Finding patterns on large graphs have a lot of applica- tions including cyber security on the Web, social media min- ing (Facebook, Twitter), and fraud detection, among others. This tutorial will cover topics related to finding patterns and anomalies and sensemaking in large-scale graphs with appli- cations to real-world problems in social media and the Web. Specifically, we aim to answer the following questions: How can we scale up graph mining algorithms for massive graphs with billions of edges? How can we find anomalies in such large-scale graphs? How can we make sense of disk-resident large graphs, what and how can we do visual analytics? How can we use the algorithms and anomaly detection techniques to solve challenging real-world problems that play key role in social media and the Web?\n Our tutorial consists of three main parts. We start with scalable graph mining algorithms for billion-scale graphs, in- cluding structure analysis, eigensolvers, storage and index- ing, and graph layout and graph compression. Next we de- scribe anomaly detection techniques for large scale graphs with applications on social media. Finally, we discuss vi- sual analytics techniques which leverage these algorithms and anomaly detection techniques in the previous parts.","cites":"4","conferencePercentile":"44.87179487"},{"venue":"WSDM","id":"dfef2eea68e1b952e7e67372b064efdf219eb654","venue_1":"WSDM","year":"2013","title":"On the prediction of popularity of trends and hits for user generated videos","authors":"Flavio Figueiredo","author_ids":"1734617","abstract":"User generated content (UGC) has emerged as the predominant form of media publishing on the Web 2.0. Motivated by the large adoption of video sharing on the Web 2.0, the objective of our work is to understand and predict popularity trends (e.g, will a video be viral?) and hits (e.g, how may views will a video receive?) of user generated videos. Such knowledge is paramount to the effective design of various services including content distribution and advertising. Thus, in this paper we formalize the problem of predicting trends and hits in user generated videos. Also, we describe our research methodology on approaching this problem. To the best of knowledge, our work is novel in focusing on the problem of predicting popularity trends complementary to hits. Moreover, we intend on evaluating efficacy of our results not only based on common statistical error metrics, but also on the possible online advertising revenues our predictions can generate. After describing our proposal, we here summarize our latest findings regarding (1) uncovering common popularity trends; (2) measuring associations between UGC features and popularity trends; and (3) assessing the effectiveness of models for predicting popularity trends.","cites":"9","conferencePercentile":"48.91304348"},{"venue":"WSDM","id":"9de6584c3ac723ee32731b663bc4ecc02899e695","venue_1":"WSDM","year":"2015","title":"On Integrating Network and Community Discovery","authors":"Jialu Liu, Charu C. Aggarwal, Jiawei Han","author_ids":"4815659, 1682418, 1722175","abstract":"The problem of community detection has recently been studied widely in the context of the web and social media networks. Most algorithms for community detection assume that the entire network is available for online analysis. In practice, this is not really true, because only restricted portions of the network may be available at any given time for analysis. Many social networks such as <i>Facebook</i> have privacy constraints, which do not allow the discovery of the entire structure of the social network. Even in the case of more open networks such as <i>Twitter</i>, it may often be challenging to crawl the entire network from a practical perspective. For many other scenarios such as adversarial networks, the discovery of the entire network may itself be a costly task, and only a small portion of the network may be discovered at any given time. Therefore, it can be useful to investigate whether network mining algorithms can integrate the network discovery process tightly into the mining process, so that the best results are achieved for particular constraints on discovery costs. In this context, we will discuss algorithms for integrating community detection with network discovery. We will tightly integrate with the cost of actually discovering a network with the community detection process, so that the two processes can support each other and are performed in a mutually cohesive way. We present experimental results illustrating the advantages of the approach.","cites":"0","conferencePercentile":"13.1147541"},{"venue":"WSDM","id":"2059c4d43d3b7e0ab285d060e352d717adcc8eb6","venue_1":"WSDM","year":"2016","title":"Modeling Check-in Preferences with Multidimensional Knowledge: A Minimax Entropy Approach","authors":"Jingjing Wang, Min Li, Jiawei Han, Xiaolong Wang","author_ids":"1734704, 1718942, 1722175, 1709719","abstract":"We propose a single unified minimax entropy approach for user preference modeling with multidimensional knowledge. Our approach provides a discriminative learning protocol which is able to simultaneously a) leverage explicit human knowledge, which are encoded as explicit features, and b) model the more ambiguous hidden intent, which are encoded as latent features. A latent feature can be carved by any parametric form, which allows it to accommodate arbitrary underlying assumptions. We present our approach in the scenario of check-in preference learning and demonstrate it is capable of modeling user preference in an optimized manner.\n Check-in preference is a fundamental component of Point-of-Interest (POI) prediction and recommendation. A user's check-in can be affected at multiple dimensions, such as the particular time, popularity of the place, his/her category and geographic preference, etc. With the geographic preferences modeled as latent features and the rest as explicit features, our approach provides an in-depth understanding of users' time-varying preferences over different POIs, as well as a reasonable representation of the hidden geographic clusters in a joint manner. Experimental results based on the task of POI prediction/recommendation with two real-world check-in datasets demonstrate that our approach can accurately model the check-in preferences and significantly outperforms the state-of-art models.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"364d42d6f1a06fde2c4ae6643e1f810c202a2d50","venue_1":"WSDM","year":"2016","title":"Evolution of Privacy Loss in Wikipedia","authors":"Marian-Andrei Rizoiu, Lexing Xie, Tiberio Caetano, Manuel Cebrián","author_ids":"1808087, 1719991, 2968026, 1709539","abstract":"The cumulative effect of collective online participation has an important and adverse impact on individual privacy. As an online system evolves over time, new digital traces of individual behavior may uncover previously hidden statistical links between an individual's past actions and her private traits. To quantify this effect, we analyze the evolution of individual privacy loss by studying the edit history of Wikipedia over 13 years, including more than 117,523 different users performing 188,805,088 edits. We trace each Wikipedia's contributor using apparently harmless features, such as the number of edits performed on predefined broad categories in a given time period (e.g. Mathematics, Culture or Nature). We show that even at this unspecific level of behavior description, it is possible to use off-the-shelf machine learning algorithms to uncover usually undisclosed personal traits, such as gender, religion or education. We provide empirical evidence that the prediction accuracy for almost all private traits consistently improves over time. Surprisingly, the prediction performance for users who stopped editing after a given time still improves. The activities performed by new users seem to have contributed more to this effect than additional activities from existing (but still active) users. Insights from this work should help users, system designers, and policy makers understand and make long-term design choices in online content creation systems.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"02812eef326da530bc9eec9c2d7a6b29e162a17e","venue_1":"WSDM","year":"2012","title":"Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities","authors":"Haipeng Zhang, Mohammed Korayem, Erkang You, David J. Crandall","author_ids":"2822359, 1775613, 3164159, 2821130","abstract":"Studying relationships between keyword tags on social sharing websites has become a popular topic of research, both to improve tag suggestion systems and to discover connections between the concepts that the tags represent. Existing approaches have largely relied on tag co-occurrences. In this paper, we show how to find connections between tags by comparing their distributions over time and space, discovering tags with similar geographic and temporal patterns of use. Geo-spatial, temporal and geo-temporal distributions of tags are extracted and represented as vectors which can then be compared and clustered. Using a dataset of tens of millions of geo-tagged Flickr photos, we show that we can cluster Flickr photo tags based on their geographic and temporal patterns, and we evaluate the results both qualitatively and quantitatively using a panel of human judges. We also develop visualizations of temporal and geographic tag distributions, and show that they help humans recognize semantic relationships between tags. This approach to finding and visualizing similar tags is potentially useful for exploring any data having geographic and temporal annotations.","cites":"16","conferencePercentile":"50"},{"venue":"WSDM","id":"ac62b1de8b224372961b81d8d19fca9212c728fc","venue_1":"WSDM","year":"2016","title":"Understanding and Measuring User Engagement and Attention in Online News Reading","authors":"Dmitry Lagun, Mounia Lalmas","author_ids":"2568110, 1684032","abstract":"Prior work on user engagement with online news sites identified\ndwell time as a key engagement metric. Whereas on\naverage, dwell time gives a reasonable estimate of user engagement\nwith a news article, it does not capture user engagement\nwith the news article at sub-document level nor it\nallows to measure the proportion of article read by the user.\nIn this paper, we analyze online news reading patterns\nusing large-scale viewport data collected from 267,210 page\nviews on 1,971 news articles on a major online news website.\nWe propose four engagement metrics that, unlike dwell time,\nmore accurately reflect how users engage with and attend to\nthe news content. The four metrics capture different levels\nof engagement, ranging from bounce to complete, providing\nclear and interpretable characterizations of user engagement\nwith online news. Furthermore, we develop a probabilistic\nmodel that combines both an article textual content and\nlevel of user engagement information in a joint model. In our\nexperiments we show that our model, called TUNE, is able\nto predict future level of user engagement based on textual\ncontent alone and outperform currently available methods.","cites":"3","conferencePercentile":"84.40860215"},{"venue":"WSDM","id":"21697f9e4ced5f10ade4d785a8be936ca6e888d2","venue_1":"WSDM","year":"2013","title":"Absence time and user engagement: evaluating ranking functions","authors":"Georges Dupret, Mounia Lalmas","author_ids":"1680327, 1684032","abstract":"In the online industry, user engagement is measured with various engagement metrics used to assess users' depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner -a reflection of their engagement with the site -than if this is not the case. This assumption has the advantage of being simple and intuitive and applicable to a large number of settings. As a case study, we use a community Q&#38;A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.","cites":"22","conferencePercentile":"76.08695652"},{"venue":"WSDM","id":"25f9bd1516725fe0c57060ef215a4f763c28c5b9","venue_1":"WSDM","year":"2013","title":"Arrival and departure dynamics in social networks","authors":"Shaomei Wu, Atish Das Sarma, Alex Fabrikant, Silvio Lattanzi, Andrew Tomkins","author_ids":"1968133, 2541992, 1983135, 1895843, 1779903","abstract":"In this paper, we consider the natural arrival and departure of users in a social network, and ask whether the dynamics of arrival, which have been studied in some depth, also explain the dynamics of departure, which are not as well studied.\n Through study of the DBLP co-authorship network and a large online social network, we show that the dynamics of departure behave differently from the dynamics of formation. In particular, the probability of departure of a user with few friends may be understood most accurately as a function of the raw number of friends who are active. For users with more friends, however, the probability of departure is best predicted by the overall fraction of the user's neighborhood that is active, independent of size. We then study global properties of the sub-graphs induced by active and inactive users, and show that active users tend to belong to a core that is densifying and is significantly denser than the inactive users. Further, the inactive set of users exhibit a higher density and lower conductance than the degree distribution alone can explain. These two aspects suggest that nodes at the fringe are more likely to depart and subsequent departure are correlated among neighboring nodes in tightly-knit communities.","cites":"8","conferencePercentile":"42.93478261"},{"venue":"WSDM","id":"0150be9a69bbb97f95502db1f9449dc173700d2c","venue_1":"WSDM","year":"2016","title":"Geographic Segmentation via Latent Poisson Factor Model","authors":"Rose Yu, Andrew Gelfand, Suju Rajan, Cyrus Shahabi, Yan Liu","author_ids":"2023052, 2454055, 1752750, 1773086, 1681842","abstract":"Discovering latent structures in spatial data is of critical importance to understanding the user behavior of location-based services. In this paper, we study the problem of geographic segmentation of spatial data, which involves dividing a collection of observations into distinct geo-spatial regions and uncovering abstract correlation structures in the data. We introduce a novel, Latent Poisson Factor (LPF) model to describe spatial count data. The model describes the spatial counts as a Poisson distribution with a mean that factors over a joint item-location latent space. The latent factors are constrained with weak labels to help uncover interesting spatial dependencies. We study the LPF model on a mobile app usage data set and a news article readership data set. We empirically demonstrate its effectiveness on a variety of prediction tasks on these two data sets.","cites":"0","conferencePercentile":"19.89247312"},{"venue":"WSDM","id":"3f745e6826260bfce94f82c227bc5cb2622cc73b","venue_1":"WSDM","year":"2008","title":"Personal name classification in web queries","authors":"Dou Shen, Toby Walker, Zijian Zheng, Qiang Yang, Ying Li","author_ids":"1680850, 2810230, 2823251, 1733090, 4744617","abstract":"Personal names are an important kind of Web queries in Web search, and yet they are special in many ways. Strategies for retrieving information on personal names should therefore be different from the strategies for other types of queries. To improve the search quality for personal names, a first step is to detect whether a query is a personal name. Despite the importance of this problem, relatively little previous research has been done on this topic. Since Web queries are usually short, conventional supervised machine-learning algorithms cannot be applied directly. An alternative is to apply some heuristic rules coupled with name-term dictionaries. However, when the dictionaries are small, this method tends to make false negatives; when the dictionaries are large, it tends to generate false positives. A more serious problem is that this method cannot provide a good trade-off between precision and recall. To solve these problems, we propose an approach based on the construction of probabilistic name-term dictionaries and personal name grammars, and use this algorithm to predict the probability of a query to be a personal name. In this paper, we develop four different methods for building probabilistic name-term dictionaries in which a term is assigned with a probability value of the term being a name term. We compared our approach with baseline algorithms such as dictionary-based look-up methods and supervised classification algorithms including logistic regression and SVM on some manually labeled test sets. The results validate the effectiveness of our approach, whose <i>F1</i> value is more than 79.8%, which outperforms the best baseline by more than 11.3%","cites":"8","conferencePercentile":"8"},{"venue":"WSDM","id":"67727ff25254942d55e624f95bafbed5585b675f","venue_1":"WSDM","year":"2009","title":"Tagging with Queries: How and Why?","authors":"Ioannis Antonellis, Hector Garcia-Molina, Jawed Karim","author_ids":"8011547, 1695250, 1791454","abstract":"Web search queries capture the information need of search engine users. Search engines store these queries in their logs and analyze them to guide their search results. In this work, we argue that not only a search engine can benefit from data stored in these logs, but also the web users. We first show how clickthrough logs can be collected in a distributed fashion using the http referer field in web server access logs. We then perform a set of experiments to study the information value of search engine queries when treated as \" tags \" or \" labels \" for the web pages that both appear as a result and the user actually clicks on. We ask how much extra information these query tags provide for web pages by comparing them to tags from the del.icio.us bookmark-ing site and to the pagetext. We find that query tags can provide substantially many (on average 250 tags per URL), new tags (on average 125 tags per URL are not present in the pagetext) for a large fraction of the Web. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.","cites":"10","conferencePercentile":"31.57894737"},{"venue":"WSDM","id":"4e7034ae30eae61db772d1b8f872ab6c90df1570","venue_1":"WSDM","year":"2010","title":"An optimization framework for query recommendation","authors":"Aris Anagnostopoulos, Luca Becchetti, Carlos Castillo, Aristides Gionis","author_ids":"1689290, 1802025, 3747087, 1682878","abstract":"Query recommendation is an integral part of modern search engines. The goal of query recommendation is to facilitate users while searching for information. Query recommendation also allows users to explore concepts related to their information needs.\n In this paper, we present a formal treatment of the problem of query recommendation. In our framework we model the querying behavior of users by a probabilistic reformula- tion graph, or query-flow graph [Boldi et al. CIKM 2008]. A sequence of queries submitted by a user can be seen as a path on this graph. Assigning score values to queries allows us to define suitable utility functions and to consider the expected utility achieved by a reformulation path on the query-flow graph. Providing recommendations can be seen as adding shortcuts in the query-flow graph that \"nudge\" the reformulation paths of users, in such a way that users are more likely to follow paths with larger expected utility.\n We discuss in detail the most important questions that arise in the proposed framework. In particular, we provide examples of meaningful utility functions to optimize, we discuss how to estimate the effect of recommendations on the reformulation probabilities, we address the complexity of the optimization problems that we consider, we suggest efficient algorithmic solutions, and we validate our models and algorithms with extensive experimentation. Our techniques can be applied to other scenarios where user behavior can be modeled as a Markov process.","cites":"35","conferencePercentile":"44.44444444"},{"venue":"WSDM","id":"b29e7912081e83997034ae8eacc76c444df883dd","venue_1":"WSDM","year":"2013","title":"Studying inter-national mobility through IP geolocation","authors":"Bogdan State, Ingmar Weber, Emilio Zagheni","author_ids":"2632078, 1684687, 2140221","abstract":"The increasing ubiquity of Internet use has opened up new avenues in the study of human mobility. Easily-obtainable geolocation data resulting from repeated logins to the same website offer the possibility of observing long-term patterns of mobility for a large number of individuals. We use data on the geographic locations from where over 100 million anonymized users log into Yahoo!~services to generate the first global map of short- and medium-term mobility flows. We develop a protocol to identify anonymized users who, over a one-year period, had spent more than 3 months in a different country from their stated country of residence (\"migrants\"), and users who spent less than a month in another country (\"tourists\"). We compute aggregate estimates of migration propensities between countries, as inferred from a user's location over the observed period. Geolocation data allow us to characterize also the pendularity of migration flows -- i.e., the extent to which migrants travel back and forth between their countries of origin and destination. We use data regarding visa regimes, colonial ties, geographic location and economic development to predict migration and tourism flows. Our analysis shows the persistence of traditional migration patterns as well as the emergence of new routes. Migrations tend to be more pendular between countries that are close to each other. We observe particularly high levels of pendularity within the European Economic Area, even after we control for distance and visa regimes. The dataset, methodology and results presented have important implications for the travel industry, as well as for several disciplines in social sciences, including geography, demography and the sociology of networks.","cites":"12","conferencePercentile":"57.06521739"},{"venue":"WSDM","id":"3fe276e03a3c0221523a9a22565caa4b3b0bb478","venue_1":"WSDM","year":"2008","title":"Deep classifier: automatically categorizing search results into large-scale hierarchies","authors":"Dikan Xing, Gui-Rong Xue, Qiang Yang, Yong Yu","author_ids":"1856714, 1701421, 1733090, 3578922","abstract":"Organizing Web search results into hierarchical categories facilitates users' browsing through Web search results, especially for ambiguous queries where the potential results are mixed together. Previous methods on search result classification are usually based on pre-training a classification model on some fixed and shallow hierarchical categories, where only the top-two-level categories of a Web taxonomy is used. Such classification methods may be too coarse for users to browse, since most search results would be classified into only two or three shallow categories. Instead, a deep hierarchical classifier must provide many more categories. However, the performance of such classifiers is usually limited because their classification effectiveness can deteriorate rapidly at the third or fourth level of a hierarchy. In this paper, we propose a novel algorithm known as <i>Deep Classifier</i> to classify the search results into detailed hierarchical categories with higher effectiveness than previous approaches. Given the search results in response to a query, the algorithm first prunes a wide-ranged hierarchy into a narrow one with the help of some Web directories. Different strategies are proposed to select the training data by utilizing the hierarchical structures. Finally, a discriminative na&#237;ve Bayesian classifier is developed to perform efficient and effective classification. As a result, the algorithm can provide more meaningful and specific class labels for search result browsing than shallow style of classification. We conduct experiments to show that the Deep Classifier can achieve significant improvement over state-of-the-art algorithms. In addition, with sufficient off-line preparation, the efficiency of the proposed algorithm is suitable for online application","cites":"13","conferencePercentile":"18"},{"venue":"WSDM","id":"a015a2b9de82e296ea199a135368d9c7b94c2c67","venue_1":"WSDM","year":"2012","title":"A noise-aware click model for web search","authors":"Weizhu Chen, Dong Wang, Yuchen Zhang, Zheng Chen, Adish Singla, Qiang Yang","author_ids":"7307263, 1726751, 6515592, 1705657, 1703727, 1733090","abstract":"Recent advances in click model have established it as an attractive approach to infer document relevance. Most of these advances consider the user click/skip behavior as binary events but neglect the context in which a click happens. We show that real click behavior in industrial search engines is often noisy and not always a good indication of relevance. For a considerable percentage of clicks, users select what turn out to be irrelevant documents and these clicks should not be directly used as evidence for relevance inference. Thus in this paper, we put forward an observation that the relevance indication degree of a click is not a constant, but can be differentiated by user preferences and the context in which the user makes her click decision. In particular, to interpret the click behavior discriminatingly, we propose a Noise-aware Click Model (NCM) by characterizing the noise degree of a click, which indicates the quality of the click for inferring relevance. Specifically, the lower the click noise is, the more important the click is in its role for relevance inference. To verify the necessity of explicitly accounting for the uninformative noise in a user click, we conducted experiments on a billion-scale dataset. Extensive experimental results demonstrate that as compared with two state-of-the-art click models in Web Search, NCM can better interpret user click behavior and achieve significant improvements in terms of both perplexity and NDCG.","cites":"10","conferencePercentile":"36.62790698"},{"venue":"WSDM","id":"2a5a8bcbe172602c26313fe69757453823cf77d6","venue_1":"WSDM","year":"2011","title":"Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited","authors":"Rahul Gupta, Sunita Sarawagi","author_ids":"4414000, 1770124","abstract":"We consider the problem of jointly training structured models for extraction from multiple web sources whose records enjoy <i>partial content overlap</i>. This has important applications in open-domain extraction, e.g. a user materializing a table of interest from multiple relevant unstructured sources; or a site like Freebase augmenting an incomplete relation by extracting more rows from web sources. Such applications require extraction over arbitrary domains, so one cannot use a pre-trained extractor or demand a huge labeled dataset. We propose to overcome this lack of supervision by using content overlap across the related web sources. Existing methods of exploiting overlap have been developed under settings that do not generalize easily to the scale and diversity of overlap seen on Web sources.\n We present an agreement-based learning framework that jointly trains the models by biasing them to agree on the <i>agreement regions</i>, i.e. shared text segments. We present alternatives within our framework to trade-off tractability, robustness to noise, and extent of agreement enforced; and propose a scheme of partitioning agreement regions that leads to efficient training while maximizing overall accuracy. Further, we present a principled scheme to discover low-noise agreement regions in unlabeled data across multiple sources.\n Through extensive experiments over 58 different extraction domains, we establish that our framework provides significant boosts over uncoupled training, and scores over alternatives such as collective inference, staged training, and multi-view learning.","cites":"15","conferencePercentile":"51.68539326"},{"venue":"WSDM","id":"7523f77561b012075431675ecf189edb8819bbeb","venue_1":"WSDM","year":"2012","title":"Answers, not links: extracting tips from yahoo! answers to address how-to web queries","authors":"Ingmar Weber, Antti Ukkonen, Aristides Gionis","author_ids":"1684687, 1693307, 1682878","abstract":"We investigate the problem of mining \"tips\" from Yahoo! Answers and displaying those tips in response to related web queries. Here, a \"tip\" is a short, concrete and self-contained bit of non-obvious advice such as \"To zest a lime if you don't have a zester : use a cheese grater.\"\n First, we estimate the volume of web queries with \"how-to\" intent, which could be potentially addressed by a tip. Second, we analyze how to detect such queries automatically without solely relying on literal \"how to *\" patterns. Third, we describe how to derive potential tips automatically from Yahoo! Answers, and we develop machine-learning techniques to remove low-quality tips. Finally, we discuss how to match web queries with \"how-to\" intent to tips. We evaluate both the quality of these direct displays as well as the size of the query volume that can be addressed by serving tips.","cites":"11","conferencePercentile":"40.11627907"},{"venue":"WSDM","id":"07b73ac3e6881865e518b72cff6d82ea08456241","venue_1":"WSDM","year":"2012","title":"Fair and balanced: learning to present news stories","authors":"Amr Ahmed, Choon Hui Teo, S. V. N. Vishwanathan, Alexander J. Smola","author_ids":"2700593, 2374866, 1709255, 1691629","abstract":"Relevance, diversity and personalization are key issues when presenting content which is apt to pique a user's interest. This is particularly true when presenting an engaging set of news stories. In this paper we propose an efficient algorithm for selecting a small subset of relevant articles from a streaming news corpus. It offers three key pieces of improvement over past work: 1) It is based on a detailed model of a user's viewing behavior which does not require explicit feedback. 2) We use the notion of submodularity to estimate the propensity of interacting with content. This improves over the classical context independent relevance ranking algorithms. Unlike existing methods, we learn the submodular function from the data. 3) We present an efficient online algorithm which can be adapted for personalization, story adaptation, and factorization models. Experiments show that our system yields a significant improvement over a retrieval system deployed in production.","cites":"9","conferencePercentile":"33.13953488"},{"venue":"WSDM","id":"928017dff976f1aa05472b725471f60035eec345","venue_1":"WSDM","year":"2013","title":"Data-driven political science","authors":"Ingmar Weber, Ana-Maria Popescu, Marco Pennacchiotti","author_ids":"1684687, 1752145, 2898756","abstract":"The tutorial will summarize the state-of-the art in the growing area of computational political science. Like many others, this research domain is being revolutionized by the availability of open, big data and the increasing reach and importance of social media. The surging interest on the part of the academic community is matched by intense efforts on the part of political campaigns to use online data in order to learn how to best disseminate information and reach the right potential donors or voters. In this context, a tutorial can summarize existing methods in a fascinating, high-interest area and allow participants with diverse backgrounds to get inspiration from the methods and problems studied. The tutorial will feature seminal research concerning (i) political polarization, (ii) election prediction and polling, and (iii) political campaigning and influence propagation. The goal is not only to familiarize attendees with ideas from related conferences such as WWW, ICWSM or CIKM, but also to present ideas and quantitative methods closer to political science such as Poole's and Rosenthal's NOMINATE score for a politician's political orientation.","cites":"1","conferencePercentile":"14.13043478"},{"venue":"WSDM","id":"124be068734cd32b58b3572cbd758a7b7d9f6bfb","venue_1":"WSDM","year":"2011","title":"Citation recommendation without author supervision","authors":"Qi He, Daniel Kifer, Jian Pei, Prasenjit Mitra, C. Lee Giles","author_ids":"3828525, 1852261, 1702840, 1714911, 1749125","abstract":"Automatic recommendation of citations for a manuscript is highly valuable for scholarly activities since it can substantially improve the efficiency and quality of literature search. The prior techniques placed a considerable burden on users, who were required to provide a representative bibliography or to mark passages where citations are needed. In this paper we present a system that considerably reduces this burden: a user simply inputs a query manuscript (<i>without</i> a bibliography) and our system automatically finds locations where citations are needed. We show that na&#239;ve approaches do not work well due to massive noise in the document corpus. We produce a successful approach by carefully examining the relevance between segments in a query manuscript and the representative segments extracted from a document corpus. An extensive empirical evaluation using the CiteSeerX data set shows that our approach is effective.","cites":"26","conferencePercentile":"64.04494382"},{"venue":"WSDM","id":"1ce98b403fa44b35221836074565bff88f0736f6","venue_1":"WSDM","year":"2012","title":"A large-scale sentiment analysis for Yahoo! answers","authors":"Onur Küçüktunç, Berkant Barla Cambazoglu, Ingmar Weber, Hakan Ferhatosmanoglu","author_ids":"3287470, 1776940, 1684687, 1787789","abstract":"Sentiment extraction from online web documents has recently been an active research topic due to its potential use in commercial applications. By sentiment analysis, we refer to the problem of assigning a quantitative positive/negative mood to a short bit of text. Most studies in this area are limited to the identification of sentiments and do not investigate the interplay between sentiments and other factors. In this work, we use a sentiment extraction tool to investigate the influence of factors such as gender, age, education level, the topic at hand, or even the time of the day on sentiments in the context of a large online question answering site. We start our analysis by looking at direct correlations, e.g., we observe more positive sentiments on weekends, very neutral ones in the Science &#38; Mathematics topic, a trend for younger people to express stronger sentiments, or people in military bases to ask the most neutral questions. We then extend this basic analysis by investigating how properties of the (asker, answerer) pair affect the sentiment present in the answer. Among other things, we observe a dependence on the pairing of some inferred attributes estimated by a user's ZIP code. We also show that the best answers differ in their sentiments from other answers, e.g., in the Business &#38; Finance topic, best answers tend to have a more neutral sentiment than other answers. Finally, we report results for the task of predicting the attitude that a question will provoke in answers. We believe that understanding factors influencing the mood of users is not only interesting from a sociological point of view, but also has applications in advertising, recommendation, and search.","cites":"52","conferencePercentile":"89.53488372"},{"venue":"WSDM","id":"30db5b95fbecc7e1183e33f552fd2b1d26d73f49","venue_1":"WSDM","year":"2009","title":"Camera brand congruence in the Flickr social graph","authors":"Adish Singla, Ingmar Weber","author_ids":"1703727, 1684687","abstract":"Given that my friends on Flickr use cameras of brand X, am I more likely to also use a camera of brand X? Given that one of these friends changes her brand, am I likely to do the same? These are the kind of questions addressed in this work. Direct applications involve personalized advertising in social networks.\n For our study we crawled a complete connected component of the Flickr friendship graph with a total of 67M edges and 3.9M users. Camera brands and models were assigned to users and time slots according to the model specific meta data pertaining to their images taken during these time slots. Similarly, we used, where provided in a user's profile, information about a user's geographic location and the groups joined on Flickr.\n Our main findings are the following. First, a pair of friends on Flickr has a significantly higher probability of being congruent, i.e., using the same brand, compared to two random users (27% vs. 19%). Second, the degree of congruence goes up for pairs of friends (i) in the same country (29%), (ii) who both only have very few friends (30%), and (iii) with a very high cliqueness (38%). Third, given that a user changes her camera model between March-May 2007 and March-May 2008, high cliqueness friends are more likely than random users to do the same (54% vs. 48%). Fourth, users using high-end cameras are far more loyal to their brand than users using point-and-shoot cameras, with a probability of staying with the same brand of 60% vs 33%, given that a new camera is bought. Fifth, these \"expert\" users' brand congruence reaches 66% (!) for high cliqueness friends.\n To the best of our knowledge this is the first time that the phenomenon of brand congruence is studied for hundreds of thousands of users and over a period of two years.","cites":"10","conferencePercentile":"31.57894737"},{"venue":"WSDM","id":"a331e03b10a17a130ada7ed65ebd6c21874b4a10","venue_1":"WSDM","year":"2014","title":"Customized tour recommendations in urban areas","authors":"Aristides Gionis, Theodoros Lappas, Konstantinos Pelechrinis, Evimaria Terzi","author_ids":"1682878, 1987564, 2096978, 7810849","abstract":"The ever-increasing urbanization coupled with the unprecedented capacity to collect and process large amounts of data have helped to create the vision of intelligent urban environments. One key aspect of such environments is that they allow people to effectively navigate through their city. While GPS technology and route-planning services have undoubtedly helped towards this direction, there is room for improvement in intelligent urban navigation. This vision can be fostered by the proliferation of location-based social networks, such as Foursquare or Path, which record the physical presence of users in different venues through check-ins. This information can then be used to enhance intelligent urban navigation, by generating customized path recommendations for users.\n In this paper, we focus on the problem of recommending customized tours in urban settings. These tours are generated so that they consider (a) the different types of venues that the user wants to visit, as well as the order in which the user wants to visit them, (b) limitations on the time to be spent or distance to be covered, and (c) the merit of visiting the included venues. We capture these requirements in a generic definition that we refer to as the TourRec problem. We then introduce two instances of the TourRec problem, study their complexity, and propose efficient algorithmic solutions. Our experiments on real data collected from Foursquare demonstrate the efficacy of our algorithms and the practical utility of the reported recommendations.","cites":"8","conferencePercentile":"68.58974359"},{"venue":"WSDM","id":"093228195521b15a4d41f3e29c6a07f5b0a2eb6a","venue_1":"WSDM","year":"2011","title":"Action prediction and identification from mining temporal user behaviors","authors":"Dakan Wang, Gang Wang, Xiaofeng Ke, Weizhu Chen","author_ids":"2509667, 4148672, 1941748, 7307263","abstract":"Predicting user's action provides many monetization opportunities to web service providers. If a user's future action can be predicted and identified correctly in time or in advance, we cannot only satisfy user's current need, but also facilitate and simplify user's future online activities. Traditional works on user behavior modeling such as implicit feedback or personalization mainly investigate on users' immediate, short-term or aggregate behaviors. Hence, it is difficult to understand the diversity in temporal user behavior and predict user's future action. In this paper, we consider a forecasting problem of temporal user behavior modeling. Our first objective is able to capture relevant users that will perform an action. The second objective is able to identify whether a user has finished the action, even when the action happened offline. We propose an ensemble algorithm to achieve both objectives. The experiment compares several implementation methods and demonstrates the temporal user behavior modeling using the ensemble algorithm significantly outperforms other methods.","cites":"1","conferencePercentile":"8.988764045"},{"venue":"WSDM","id":"cb0b4768531044b32358e224922e54547730fdb9","venue_1":"WSDM","year":"2014","title":"Understanding and promoting micro-finance activities in Kiva.org","authors":"Jaegul Choo, Changhyun Lee, Daniel Lee, Hongyuan Zha, Haesun Park","author_ids":"1795455, 1727835, 2402104, 1750350, 1685928","abstract":"Non-profit Micro-finance organizations provide loaning opportunities to eradicate poverty by financially equipping impoverished, yet skilled entrepreneurs who are in desperate need of an institution that lends to those who have little. Kiva.org, a widely-used crowd-funded micro-financial service, provides researchers with an extensive amount of publicly available data containing a rich set of heterogeneous information regarding micro-financial transactions. Our objective in this paper is to identify the key factors that encourage people to make micro-financing donations, and ultimately, to keep them actively involved. In our contribution to further promote a healthy micro-finance ecosystem, we detail our personalized loan recommendation system which we formulate as a supervised learning problem where we try to predict how likely a given lender will fund a new loan. We construct the features for each data item by utilizing the available connectivity relationships in order to integrate all the available Kiva data sources. For those lenders with no such relationships, e.g., first-time lenders, we propose a novel method of feature construction by computing joint nonnegative matrix factorizations. Utilizing gradient boosting tree methods, a state-of-the-art prediction model, we are able to achieve up to 0.92 AUC (area under the curve) value, which shows the potential of our methods for practical deployment. Finally, we point out several interesting phenomena on lenders' social behaviors in micro-finance activities.","cites":"7","conferencePercentile":"62.17948718"},{"venue":"WSDM","id":"2e49f8e9b818dcbaa09886477ad1ec004b1ce077","venue_1":"WSDM","year":"2011","title":"Clustering product features for opinion mining","authors":"Zhongwu Zhai, Bing Liu, Hua Xu, Peifa Jia","author_ids":"1774830, 2201323, 1718800, 2463381","abstract":"In sentiment analysis of product reviews, one important problem is to produce a summary of opinions based on product features/attributes (also called aspects). However, for the same feature, people can express it with many different words or phrases. To produce a useful summary, these words and phrases, which are domain synonyms, need to be grouped under the same feature group. Although several methods have been proposed to extract product features from reviews, limited work has been done on clustering or grouping of synonym features. This paper focuses on this task. Classic methods for solving this problem are based on unsupervised learning using some forms of distributional similarity. However, we found that these methods do not do well. We then model it as a semi-supervised learning problem. Lexical characteristics of the problem are exploited to automatically identify some labeled examples. Empirical evaluation shows that the proposed method outperforms existing state-of-the-art methods by a large margin.","cites":"45","conferencePercentile":"79.7752809"},{"venue":"WSDM","id":"a286535a29766f3764b73c9da79808651394f682","venue_1":"WSDM","year":"2008","title":"Collaboration over time: characterizing and modeling network evolution","authors":"Jian Huang, Ziming Zhuang, Jia Li, C. Lee Giles","author_ids":"1782736, 3017391, 1687366, 1749125","abstract":"A formal type of scientific and academic collaboration is coauthorship which can be represented by a coauthorship network. Coauthorship networks are among some of the largest social networks and offer us the opportunity to study the mechanisms underlying large-scale real world networks. We construct such a network for the Computer Science field covering research collaborations from 1980 to 2005, based on a large dataset of 451,305 papers authored by 283,174 distinct researchers. By mining this network, we first present a comprehensive study of the network statistical properties for a longitudinal network at the overall network level as well as for the intermediate community level. Major observations are that the database community is the best connected while the AI community is the most assortative, and that the Computer Science field as a whole shows a collaboration pattern more similar to Mathematics than to Biology. Moreover, the small world phenomenon and the scale-free degree distribution accompany the growth of the network. To study the individual collaborations, we propose a novel stochastic model, <i>Stochastic Poisson model with Optimization Tree</i> (S<scp>pot</scp>)to efficiently predict any increment of collaboration based on the local neighborhood structure. S<scp>pot</scp> models the non-stationary Poisson process by maximizing the log-likelihood with a tree structure. Empirical results show that S<scp>pot</scp> outperforms Support Vector Regression by better fitting collaboration records and predicting the rate of collaboration","cites":"40","conferencePercentile":"54"},{"venue":"WSDM","id":"ea50741d115b42928a85d46fd1f9954521811524","venue_1":"WSDM","year":"2015","title":"You Are Where You Go: Inferring Demographic Attributes from Location Check-ins","authors":"Yuan Zhong, Nicholas Jing Yuan, Wen Zhong, Fuzheng Zhang, Xing Xie","author_ids":"2189447, 2123146, 2667808, 2642200, 1687677","abstract":"User profiling is crucial to many online services. Several recent studies suggest that demographic attributes are predictable from different online behavioral data, such as users' \"Likes\" on Facebook, friendship relations, and the linguistic characteristics of tweets. But location check-ins, as a bridge of users' offline and online lives, have by and large been overlooked in inferring user profiles. In this paper, we investigate the predictive power of location check-ins for inferring users' demographics and propose a simple yet general location to profile (L2P) framework. More specifically, we extract rich semantics of users' check-ins in terms of spatiality, temporality, and location knowledge, where the location knowledge is enriched with semantics mined from heterogeneous domains including both online customer review sites and social networks. Additionally, tensor factorization is employed to draw out low dimensional representations of users' intrinsic check-in preferences considering the above factors. Meanwhile, the extracted features are used to train predictive models for inferring various demographic attributes.\n We collect a large dataset consisting of profiles of 159,530 verified users from an online social network. Extensive experimental results based upon this dataset validate that: 1) Location check-ins are diagnostic representations of a variety of demographic attributes, such as gender, age, education background, and marital status; 2) The proposed framework substantially outperforms compared models for profile inference in terms of various evaluation metrics, such as precision, recall, F-measure, and AUC.","cites":"21","conferencePercentile":"100"},{"venue":"WSDM","id":"3e51c282fd4d0afcb8e39deb65670af13b14f455","venue_1":"WSDM","year":"2012","title":"Large-scale analysis of individual and task differences in search result page examination strategies","authors":"Georg Buscher, Ryen W. White, Susan T. Dumais, Jeff Huang","author_ids":"1693190, 1734415, 1728602, 3404131","abstract":"Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.","cites":"28","conferencePercentile":"68.60465116"},{"venue":"WSDM","id":"cb2ce6e7b45a8b9064c29d52f818b57eb0989eb3","venue_1":"WSDM","year":"2013","title":"3rd Workshop on Context-awareness in Retrieval and Recommendation","authors":"Matthias Böhmer, Ernesto William De Luca, Alan Said, Jaime Teevan","author_ids":"3239493, 1692708, 1722414, 1691357","abstract":"Context-aware information is widely available in various ways and is becoming more and more important for enhancing retrieval performance and recommendation results. The current main issue to cope with is not only recommending or retrieving the most relevant items and content, but defining them ad hoc. Other relevant issues include personalizing and adapting the information and the way it is displayed to the user's current situation and interests. Ubiquitous computing further provides new means for capturing user feedback on items and providing information.","cites":"1","conferencePercentile":"14.13043478"},{"venue":"WSDM","id":"94dc02825b80910ffbb1a029e85f8231f3845e21","venue_1":"WSDM","year":"2016","title":"Learning Distributed Representations of Data in Community Question Answering for Question Retrieval","authors":"Kai Zhang, Wei Wu, Fang Wang, Ming Zhou, Zhoujun Li","author_ids":"4420840, 2581478, 1713374, 5962676, 1707275","abstract":"We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. We propose a principled and efficient approach to learning representations of data in CQA. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. We conduct experiments on large scale data from Yahoo! Answers and Baidu Knows, and compared our method with state-of-the-art methods on two public data sets. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. On 1 million training data, our method takes less than 50 minutes to learn a model on a single multicore machine, while the translation based language model needs more than 2 days to learn a translation table on the same machine.","cites":"2","conferencePercentile":"72.04301075"},{"venue":"WSDM","id":"3db601bad02c04a72a3ddfaf5601b70d52486ab9","venue_1":"WSDM","year":"2014","title":"Improving search relevance for short queries in community question answering","authors":"Haocheng Wu, Wei Wu, Ming Zhou, Enhong Chen, Lei Duan, Harry Shum","author_ids":"2173700, 2581478, 5962676, 1703319, 7667992, 1698102","abstract":"Relevant question retrieval and ranking is a typical task in community question answering (CQA). Existing methods mainly focus on long and syntactically structured queries. However, when an input query is short, the task becomes challenging, due to a lack information regarding user intent. In this paper, we mine different types of user intent from various sources for short queries. With these intent signals, we propose a new intent-based language model. The model takes advantage of both state-of-the-art relevance models and the extra intent information mined from multiple sources. We further employ a state-of-the-art learning-to-rank approach to estimate parameters in the model from training data. Experiments show that by leveraging user intent prediction, our model significantly outperforms the state-of-the-art relevance models in question search.","cites":"4","conferencePercentile":"44.87179487"},{"venue":"WSDM","id":"6fb6ac49a9d3a4980c02504c3550ed52726ce61c","venue_1":"WSDM","year":"2013","title":"Learning query and document similarities from click-through bipartite graph with metadata","authors":"Wei Wu, Hang Li, Jun Xu","author_ids":"2581478, 3701964, 1761160","abstract":"We consider learning query and document similarities from a click-through bipartite graph with metadata on the nodes. The metadata contains multiple types of features of queries and documents. We aim to leverage both the click-through bipartite graph and the features to learn query-document, document-document, and query-query similarities. The challenges include how to model and learn the similarity functions based on the graph data.\n We propose solving the problems in a principled way. Specifically, we use two different linear mappings to project the queries and documents in two different feature spaces into the same latent space, and take the dot product in the latent space as their similarity. Query-query and document-document similarities can also be naturally defined as dot products in the latent space. We formalize the learning of similarity functions as learning of the mappings that maximize the similarities of the observed query-document pairs on the enriched click-through bipartite graph. When queries and documents have multiple types of features, the similarity function is defined as a linear combination of multiple similarity functions, each based on one type of features. We further solve the learning problem by using a new technique called Multi-view Partial Least Squares (M-PLS). The advantages include the global optimum which can be obtained through Singular Value Decomposition (SVD) and the capability of finding high quality similar queries. We conducted large scale experiments on enterprise search data and web search data. The experimental results on relevance ranking and similar query finding demonstrate that the proposed method works significantly better than the baseline methods.","cites":"23","conferencePercentile":"77.7173913"},{"venue":"WSDM","id":"19c87fd2b04438562959e2bdc488dedcb6aff21f","venue_1":"WSDM","year":"2009","title":"Characterizing the influence of domain expertise on web search behavior","authors":"Ryen W. White, Susan T. Dumais, Jaime Teevan","author_ids":"1734415, 1728602, 1691357","abstract":"Domain experts search differently than people with little or no domain knowledge. Previous research suggests that domain experts employ different search strategies and are more successful in finding what they are looking for than non-experts. In this paper we present a large-scale, longitudinal, log-based analysis of the effect of domain expertise on web search behavior in four different domains (medicine, finance, law, and computer science). We characterize the nature of the queries, search sessions, web sites visited, and search success for users identified as experts and non-experts within these domains. Large-scale analysis of real-world interactions allows us to understand how expertise relates to vocabulary, resource use, and search task under more realistic search conditions than has been possible in previous small-scale studies. Building upon our analysis we develop a model to predict expertise based on search behavior, and describe how knowledge about domain expertise can be used to present better results and query suggestions to users and to help non-experts gain expertise.","cites":"100","conferencePercentile":"94.73684211"},{"venue":"WSDM","id":"44dc77496f400e76f70365a807dd187e3db35215","venue_1":"WSDM","year":"2009","title":"Discovering and using groups to improve personalized search","authors":"Jaime Teevan, Meredith Ringel Morris, Steve Bush","author_ids":"1691357, 1741255, 3275901","abstract":"Personalized Web search takes advantage of information about an individual to identify the most relevant results for that person. A challenge for personalization lies in collecting user profiles that are rich enough to do this successfully. One way an individual's profile can be augmented is by using data from other people. To better understand whether groups of people can be used to benefit personalized search, we explore the similarity of query selection, desktop information, and explicit relevance judgments across people grouped in different ways. The groupings we explore fall along two dimensions: the longevity of the group members' relationship, and how explicitly the group is formed. We find that some groupings provide valuable insight into what members consider relevant to queries related to the group focus, but that it can be difficult to identify valuable groups implicitly. Building on these findings, we explore an algorithm to \"groupize\" (versus <i>\"personalize\"</i>) Web search results that leads to a significant improvement in result ranking on group-relevant queries.","cites":"54","conferencePercentile":"78.94736842"},{"venue":"WSDM","id":"0aeef7a013cae46594bede178433c61c63418300","venue_1":"WSDM","year":"2010","title":"Large scale query log analysis of re-finding","authors":"Sarah K. Tyler, Jaime Teevan","author_ids":"3196594, 1691357","abstract":"Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of re-finding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.","cites":"62","conferencePercentile":"67.77777778"},{"venue":"WSDM","id":"8e6d963a9c1115de61e7672c6d4c54c781b3e54d","venue_1":"WSDM","year":"2014","title":"Lessons from the journey: a query log analysis of within-session learning","authors":"Carsten Eickhoff, Jaime Teevan, Ryen W. White, Susan T. Dumais","author_ids":"2285517, 1691357, 1734415, 1728602","abstract":"The Internet is the largest source of information in the world. Search engines help people navigate the huge space of available data in order to acquire new skills and knowledge. In this paper, we present an in-depth analysis of sessions in which people explicitly search for new knowledge on the Web based on the log files of a popular search engine. We investigate within-session and cross-session developments of expertise, focusing on how the language and search behavior of a user on a topic evolves over time. In this way, we identify those sessions and page visits that appear to significantly boost the learning process. Our experiments demonstrate a strong connection between clicks and several metrics related to expertise. Based on models of the user and their specific context, we present a method capable of automatically predicting, with good accuracy, which clicks will lead to enhanced learning. Our findings provide insight into how search engines might better help users learn as they search.","cites":"19","conferencePercentile":"90.38461538"},{"venue":"WSDM","id":"20d8be846239abcff3a817d4aa48cf61aa34724f","venue_1":"WSDM","year":"2011","title":"Understanding temporal query dynamics","authors":"Anagha Kulkarni, Jaime Teevan, Krysta Marie Svore, Susan T. Dumais","author_ids":"1706699, 1691357, 1742466, 1728602","abstract":"Web search is strongly influenced by time. The queries people issue change over time, with some queries occasionally spiking in popularity (e.g., <i>earthquake</i>) and others remaining relatively constant (e.g., <i>youtube</i>). The documents indexed by the search engine also change, with some documents always being about a particular query (e.g., the Wikipedia page on earthquakes is about the query <i>earthquake</i>) and others being about the query only at a particular point in time (e.g., the New York Times is only about earthquakes following a major seismic activity). The relationship between documents and queries can also change as people's intent changes (e.g., people sought different content for the query <i>earthquake</i> before the Haitian earthquake than they did after). In this paper, we explore how queries, their associated documents, and the query intent change over the course of 10 weeks by analyzing query log data, a daily Web crawl, and periodic human relevance judgments. We identify several interesting features by which changes to query popularity can be classified, and show that presence of these features, when accompanied by changes in result content, can be a good indicator of change in query intent.","cites":"70","conferencePercentile":"88.76404494"},{"venue":"WSDM","id":"bbfff05b4218b2c1b261c72a48c70cbeb6d36581","venue_1":"WSDM","year":"2011","title":"The tube over time: characterizing popularity growth of youtube videos","authors":"Flavio Figueiredo, Fabrício Benevenuto, Jussara M. Almeida","author_ids":"1734617, 2810330, 8118988","abstract":"Understanding content popularity growth is of great importance to Internet service providers, content creators and online marketers. In this work, we characterize the growth patterns of video popularity on the currently most popular video sharing application, namely YouTube. Using newly provided data by the application, we analyze how the popularity of individual videos evolves since the video's upload time. Moreover, addressing a key aspect that has been mostly overlooked by previous work, we characterize the types of the referrers that most often attracted users to each video, aiming at shedding some light into the mechanisms (e.g., searching or external linking) that often drive users towards a video, and thus contribute to popularity growth. Our analyses are performed separately for three video datasets, namely, videos that appear in the YouTube top lists, videos removed from the system due to copyright violation, and videos selected according to random queries submitted to YouTube's search engine. Our results show that popularity growth patterns depend on the video dataset. In particular, copyright protected videos tend to get most of their views much earlier in their lifetimes, often exhibiting a popularity growth characterized by a viral epidemic-like propagation process. In contrast, videos in the top lists tend to experience sudden significant bursts of popularity. We also show that not only search but also other YouTube internal mechanisms play important roles to attract users to videos in all three datasets.","cites":"79","conferencePercentile":"89.88764045"},{"venue":"WSDM","id":"d3366fce3084d4506e8f802c33b368927d789005","venue_1":"WSDM","year":"2016","title":"Exploiting New Sentiment-Based Meta-level Features for Effective Sentiment Analysis","authors":"Sérgio D. Canuto, Marcos André Gonçalves, Fabrício Benevenuto","author_ids":"2321943, 7170789, 2810330","abstract":"In this paper we address the problem of automatically learning to classify the sentiment of short messages/reviews by exploiting information derived from meta-level features i.e., features derived primarily from the original bag-of-words representation. We propose new meta-level features especially designed for the sentiment analysis of short messages such as: (i) information derived from the sentiment distribution among the <i>k</i> nearest neighbors of a given short test document <i>x</i>, (ii) the distribution of distances of <i>x</i> to their neighbors and (iii) the document polarity of these neighbors given by unsupervised lexical-based methods. Our approach is also capable of exploiting information from the neighborhood of document <i>x</i> regarding (highly noisy) data obtained from 1.6 million Twitter messages with emoticons. The set of proposed features is capable of transforming the original feature space into a new one, potentially smaller and more informed. Experiments performed with a substantial number of datasets (nineteen) demonstrate that the effectiveness of the proposed sentiment-based meta-level features is not only superior to the traditional bag-of-word representation (by up to 16%) but is also superior in most cases to state-of-art meta-level features previously proposed in the literature for text classification tasks that do not take into account some idiosyncrasies of sentiment analysis. Our proposal is also largely superior to the best lexicon-based methods as well as to supervised combinations of them. In fact, the proposed approach is the only one to produce the best results in all tested datasets in all scenarios.","cites":"3","conferencePercentile":"84.40860215"},{"venue":"WSDM","id":"57f96d1d4e5cbb78b9ca52a059e73b7ced674c41","venue_1":"WSDM","year":"2011","title":"#TwitterSearch: a comparison of microblog search and web search","authors":"Jaime Teevan, Daniel Ramage, Meredith Ringel Morris","author_ids":"1691357, 1878835, 1741255","abstract":"Social networking Web sites are not just places to maintain relationships; they can also be valuable information sources. However, little is known about how and why people search socially-generated content. In this paper we explore search behavior on the popular microblogging/social networking site Twitter. Using analysis of large-scale query logs and supplemental qualitative data, we observe that people search Twitter to find temporally relevant information (e.g., breaking news, real-time content, and popular trends) and information related to people (e.g., content directed at the searcher, information about people of interest, and general sentiment and opinion). Twitter queries are shorter, more popular, and less likely to evolve as part of a session than Web queries. It appears people repeat Twitter queries to monitor the associated search results, while changing and developing Web queries to learn about a topic. The results returned from the different corpora support these different uses, with Twitter results including more social chatter and social events, and Web results containing more basic facts and navigational content. We discuss the implications of these findings for the design of next-generation Web search tools that incorporate social media.","cites":"156","conferencePercentile":"94.38202247"},{"venue":"WSDM","id":"7d9ef8fd63b4f7f0448dea4ea6804a3bba5bec08","venue_1":"WSDM","year":"2011","title":"Understanding and predicting personal navigation","authors":"Jaime Teevan, Daniel J. Liebling, Gayathri Ravichandran Geetha","author_ids":"1691357, 1724850, 2747694","abstract":"This paper presents an algorithm that predicts with very high accuracy which Web search result a user will click for one sixth of all Web queries. Prediction is done via a straightforward form of personalization that takes advantage of the fact that people often use search engines to re-find previously viewed resources. In our approach, an individual's past navigational behavior is identified via query log analysis and used to forecast identical future navigational behavior by the same individual. We compare the potential value of personal navigation with general navigation identified using aggregate user behavior. Although consistent navigational behavior across users can be useful for identifying a subset of navigational queries, different people often use the same queries to navigate to different resources. This is true even for queries comprised of unambiguous company names or URLs and typically thought of as navigational. We build an understanding of what personal navigation looks like, and identify ways to improve its coverage and accuracy by taking advantage of people's consistency over time and across groups of individuals.","cites":"26","conferencePercentile":"64.04494382"},{"venue":"WSDM","id":"d1fdc4cccac676802a53a9c90c802d5afa3a4f3e","venue_1":"WSDM","year":"2015","title":"Big Data: New Paradigm or \"Sound and Fury, Signifying Nothing\"?","authors":"Andrei Z. Broder, Lada A. Adamic, Michael J. Franklin, Maarten de Rijke, Eric P. Xing, Kai Yu","author_ids":"7693748, 1778398, 1982103, 1696030, 1752601, 1736727","abstract":"The Gartner's 2014 Hype Cycle released last August moves Big Data technology from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment when interest starts to wane as reality does not live up to previous promises. As the hype is starting to dissipate it is worth asking what Big Data (however defined) means from a scientific perspective: Did the emergence of gigantic corpora exposed the limits of classical information retrieval and data mining and led to new concepts and challenges, the way say, the study of electromagnetism showed the limits of Newtonian mechanics and led to Relativity Theory, or is it all just \"sound and fury, signifying nothing\", simply a matter of scaling up well understood technologies? To answer this question, we have assembled a distinguished panel of eminent scientists, from both Industry and Academia: Lada Adamic (Facebook), Michael Franklin (University of California at Berkeley), Maarten de Rijke (University of Amsterdam), Eric Xing (Carnegie Mellon University), and Kai Yu (Baidu) will share their point of view and take questions from the moderator and the audience.","cites":"1","conferencePercentile":"33.60655738"},{"venue":"WSDM","id":"41096a34c9664868bc224e1307de3b2b7ac430dd","venue_1":"WSDM","year":"2015","title":"The Information Life of Social Networks","authors":"Lada A. Adamic","author_ids":"1778398","abstract":"Vast amounts of information are propagated in online social networks such as Facebook. This talk will describe several studies characterizing how information diffuses over social ties, from the growth of individual cascades to the predictability of their eventual size. It will also characterize the diffusion of specific kinds of information, including rumors, memes, and social movements.","cites":"0","conferencePercentile":"13.1147541"},{"venue":"WSDM","id":"3f298256b087e7d032d56fb66cd9c998077092a1","venue_1":"WSDM","year":"2013","title":"LaFT-tree: perceiving the expansion trace of one's circle of friends in online social networks","authors":"Jun Zhang, Chaokun Wang, Jianmin Wang, Philip S. Yu","author_ids":"1752214, 8205851, 1751179, 1703117","abstract":"Many patterns have been discovered to explain and analyze how people make friends. Among them is the triadic closure, supported by the principle of the transitivity of friendship, which means for an individual the friends of her friend are more likely to become her new friends. However, people's motivations under this principle haven't been well studied, and it's still unknown that how this principle works in diverse situations.\n In this paper, we try to study this principle deeply based on the behavior modeling. We study how one expands her egocentric network via her friends, also called intermediaries, based on the transitivity of friendship. We propose LaFT-Tree, a tree-based representation of friendship formation inspired from triadic closure. LaFT-Tree provides a hierarchical view of the flat structure of one's egocentric network by visualizing the expansion trace of one's egocentric network. We model people's friend-making behaviors using LaFT-LDA, a generative model for LaFT-Tree learning.\n The proposed model is evaluated on both synthetic and real-world social networks and experimental results demonstrate the effectiveness of LaFT-LDA for LaFT-Tree inference. We also present some interesting applications of the LaFT-Tree, showing that our model can be generalized and benefit other social network analysis tasks.","cites":"6","conferencePercentile":"33.15217391"},{"venue":"WSDM","id":"167da2ad2b01a43566b21a4715d1315ff7ac45ef","venue_1":"WSDM","year":"2016","title":"Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions","authors":"Nicholas Jing Yuan, Yuan Zhong, Fuzheng Zhang, Xing Xie, Chin-Yew Lin, Yong Rui","author_ids":"2123146, 2189447, 2642200, 1687677, 1781574, 1728806","abstract":"Friendships are dynamic. Previous studies have converged to suggest that social interactions, in both online and offline social networks, are diagnostic reflections of friendship relations (also called social ties). However, most existing approaches consider a social tie as either a binary relation, or a fixed value (named tie strength). In this paper, we investigate the dynamics of dyadic friend relationships through online social interactions, in terms of a variety of aspects, such as reciprocity, temporality, and contextuality. In turn, we propose a model to predict repliers and retweeters given a particular tweet posted at a certain time in a microblog-based social network. More specifically, we have devised a learning-to-rank approach to train a ranker that considers elaborate user-level and tweet-level features (like sentiment, self-disclosure, and responsiveness) to address these dynamics. In the prediction phase, a tweet posted by a user is deemed a query and the predicted repliers/retweeters are retrieved using the learned ranker. We have collected a large dataset containing 73.3 million dyadic relationships with their interactions (replies and retweets). Extensive experimental results based on this dataset show that by incorporating the dynamics of friendship relations, our approach significantly outperforms state-of-the-art models in terms of multiple evaluation metrics, such as MAP, NDCG and Topmost Accuracy. In particular, the advantage of our model is even more promising in predicting the exact sequence of repliers/retweeters considering their orders. Furthermore, the proposed approach provides emerging implications for many high-value applications in online social networks.","cites":"2","conferencePercentile":"72.04301075"},{"venue":"WSDM","id":"0e2bdc45c08acef3cabfd9a41bad4f79099c5166","venue_1":"WSDM","year":"2013","title":"From machu_picchu to \"rafting the urubamba river\": anticipating information needs via the entity-query graph","authors":"Ilaria Bordino, Gianmarco De Francisci Morales, Ingmar Weber, Francesco Bonchi","author_ids":"3033855, 1715892, 1684687, 1705764","abstract":"We study the problem of anticipating user search needs, based on their browsing activity. Given the current web page p that a user is visiting we want to recommend a small and diverse set of search queries that are relevant to the content of p, but also non-obvious and serendipitous.\n We introduce a novel method that is based on the content of the page visited, rather than on past browsing patterns as in previous literature. Our content-based approach can be used even for previously unseen pages.\n We represent the topics of a page by the set of Wikipedia entities extracted from it. To obtain useful query suggestions for these entities, we exploit a novel graph model that we call EQGraph (Entity-Query Graph), containing entities, queries, and transitions between entities, between queries, as well as from entities to queries. We perform Personalized PageRank computation on such a graph to expand the set of entities extracted from a page into a richer set of entities, and to associate these entities with relevant query suggestions. We develop an efficient implementation to deal with large graph instances and suggest queries from a large and diverse pool.\n We perform a user study that shows that our method produces relevant and interesting recommendations, and outperforms an alternative method based on reverse IR.","cites":"16","conferencePercentile":"66.30434783"},{"venue":"WSDM","id":"607b522c32d61474f2f05b2b23b7bc8e205cf821","venue_1":"WSDM","year":"2013","title":"Unsupervised graph-based topic labelling using dbpedia","authors":"Ioana Hulpus, Conor Hayes, Marcel Karnstedt, Derek Greene","author_ids":"2680937, 2653825, 1734216, 8560326","abstract":"Automated topic labelling brings benefits for users aiming at analysing and understanding document collections, as well as for search engines targetting at the linkage between groups of words and their inherent topics. Current approaches to achieve this suffer in quality, but we argue their performances might be improved by setting the focus on the structure in the data. Building upon research for concept disambiguation and linking to DBpedia, we are taking a novel approach to topic labelling by making use of structured data exposed by DBpedia. We start from the hypothesis that words co-occuring in text likely refer to concepts that belong closely together in the DBpedia graph. Using graph centrality measures, we show that we are able to identify the concepts that best represent the topics. We comparatively evaluate our graph-based approach and the standard text-based approach, on topics extracted from three corpora, based on results gathered in a crowd-sourcing experiment. Our research shows that graph-based analysis of DBpedia can achieve better results for topic labelling in terms of both precision and topic coverage.","cites":"34","conferencePercentile":"88.58695652"},{"venue":"WSDM","id":"766093cbbd95127767c4a92ceeedb8bd901b21a2","venue_1":"WSDM","year":"2014","title":"Search engine click spam detection based on bipartite graph propagation","authors":"Xin Li, Min Zhang, Yiqun Liu, Shaoping Ma, Yijiang Jin, Liyun Ru","author_ids":"1705796, 1770849, 8653031, 8093158, 2295627, 1790121","abstract":"Using search engines to retrieve information has become an important part of people's daily lives. For most search engines, click information is an important factor in document ranking. As a result, some websites cheat to obtain a higher rank by fraudulently increasing clicks to their pages, which is referred to as \"Click Spam\". Based on an analysis of the features of fraudulent clicks, a novel automatic click spam detection approach is proposed in this paper, which consists of 1. modeling user sessions with a triple sequence, which, to the best of our knowledge, takes into account not only the user action but also the action objective and the time interval between actions for the first time; 2. using the user-session bipartite graph propagation algorithm to take advantage of cheating users to find more cheating sessions; and 3. using the pattern-session bipartite graph propagation algorithm to obtain cheating session patterns to achieve higher precision and recall of click spam detection. Experimental results based on a Chinese commercial search engine using real-world log data containing approximately 80 million user clicks per day show that 2.6% of all clicks were detected as spam with a precision of up to 97%.","cites":"3","conferencePercentile":"36.53846154"},{"venue":"WSDM","id":"06614ff37476c9d2cc648e2728590b361de49103","venue_1":"WSDM","year":"2008","title":"Finding high-quality content in social media","authors":"Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, Gilad Mishne","author_ids":"1685296, 3747087, 2485865, 1682878, 1693513","abstract":"The quality of user-generated content varies drastically from excellent to abuse and spam. As the availability of such content increases, the task of identifying high-quality content sites based on user contributions --social media sites -- becomes increasingly important. Social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. In this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. As a test case, we focus on Yahoo! Answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. We introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. In particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans","cites":"359","conferencePercentile":"100"},{"venue":"WSDM","id":"72f5ede4c7161322bfafb4e1f584f0b1544a6f08","venue_1":"WSDM","year":"2012","title":"Correlating financial time series with micro-blogging activity","authors":"Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aristides Gionis, Alejandro Jaimes","author_ids":"2745197, 1754970, 3747087, 1682878, 1730325","abstract":"We study the problem of correlating micro-blogging activity with stock-market events, defined as changes in the price and traded volume of stocks. Specifically, we collect messages related to a number of companies, and we search for correlations between stock-market events for those companies and features extracted from the micro-blogging messages. The features we extract can be categorized in two groups. Features in the first group measure the overall activity in the micro-blogging platform, such as number of posts, number of re-posts, and so on. Features in the second group measure properties of an induced <i>interaction graph</i>, for instance, the number of connected components, statistics on the degree distribution, and other graph-based properties.\n We present detailed experimental results measuring the correlation of the stock market events with these features, using Twitter as a data source. Our results show that the most correlated features are the number of connected components and the number of nodes of the interaction graph. The correlation is stronger with the traded volume than with the price of the stock. However, by using a simulator we show that even relatively small correlations between price and micro-blogging features can be exploited to drive a stock trading strategy that outperforms other baseline strategies.","cites":"58","conferencePercentile":"91.86046512"}]}