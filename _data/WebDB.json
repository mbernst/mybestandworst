{"WebDB.csv":[{"venue":"WebDB","id":"69a076f962b868b0e276ddb0399041ba54f736d9","venue_1":"WebDB","year":"2004","title":"Spam, Damn Spam, and Statistics: Using Statistical Analysis to Locate Spam Web Pages","authors":"Dennis Fetterly, Mark S. Manasse, Marc Najork","author_ids":"1780761, 2083378, 1763978","abstract":"The increasing importance of search engines to commercial web sites has given rise to a phenomenon we call \"web spam\", that is, web pages that exist only to mislead search engines into (mis)leading users to certain web sites. Web spam is a nuisance to users as well as search engines: users have a harder time finding the information they need, and search engines have to cope with an inflated corpus, which in turn causes their cost per query to increase. Therefore, search engines have a strong incentive to weed out spam web pages from their index.We propose that some spam web pages can be identified through statistical analysis: Certain classes of spam pages, in particular those that are machine-generated, diverge in some of their properties from the properties of web pages at large. We have examined a variety of such properties, including linkage structure, page content, and page evolution, and have found that outliers in the statistical distribution of these properties are highly likely to be caused by web spam.This paper describes the properties we have examined, gives the statistical distributions we have observed, and shows which kinds of outliers are highly correlated with web spam.","cites":"197","conferencePercentile":"100"},{"venue":"WebDB","id":"48b9f9ddf17bd29b957b09f9000576e53acf8719","venue_1":"WebDB","year":"2013","title":"Ringtail: Feature Selection For Easier Nowcasting","authors":"Dolan Antenucci, Michael J. Cafarella, Margaret Levenstein, Christopher Ré, Matthew Shapiro","author_ids":"1908781, 1725561, 2542789, 1803218, 2944589","abstract":"In recent years, social media \" nowcasting \" —the use of on-line user activity to predict various ongoing real-world social phenomena—has become a popular research topic; yet, this popularity has not led to widespread actual practice. We believe a major obstacle to widespread adoption is the feature selection problem. Typical nowcasting systems require the user to choose a set of relevant social media objects, which is difficult, time-consuming, and can imply a statistical background that users may not have. We propose Ringtail, which helps the user choose relevant social media signals. It takes a single user input string (e.g., unemployment) and yields a number of relevant signals the user can use to build a nowcasting model. We evaluate Ringtail on six different topics using a corpus of almost 6 billion tweets, showing that features chosen by Ringtail in a wholly-automated way are better or as good as those from a human and substantially better if Ringtail receives some human assistance. In all cases, Ringtail reduces the burden on the user.","cites":"10","conferencePercentile":"88.88888889"},{"venue":"WebDB","id":"96531057874ad205c2ca3fc097082325c88d2599","venue_1":"WebDB","year":"2007","title":"Navigating Extracted Data with Schema Discovery","authors":"Michael J. Cafarella, Dan Suciu, Oren Etzioni","author_ids":"1725561, 1693752, 1741101","abstract":"Open Information Extraction (OIE) is a recently-introduced type of information extraction that extracts small individual pieces of data from input text without any domain-specific guidance such as special training data or extraction rules. For example, an OIE system might discover the triple Frenzy, year, 1972 from a set of documents about movies. Because OIE is domain-independent, it promises to help users when they have a corpus of structured data, but that structure is unknown, such as when browsing a novel domain or formulating a query. We can describe the structure to the user by displaying a relational schema that fits the extracted data. Unfortunately, the extractions do not carry full schema information: we have extracted values, but not the correct relations, their rows, or their columns. In response we propose TGen, an algorithm for schema discovery, which automatically derives a high-quality relational schema for the extracted data. Different applications have different schema-design requirements, which can be encoded as input to TGen. We show that our data-mining approach runs in minutes on millions of documents while still resulting in schemas that are useful for exploring unfamiliar data or for composing queries over extracted data.","cites":"17","conferencePercentile":"72.72727273"},{"venue":"WebDB","id":"a993a7f5cd04cbc7088a64916f25ee05ff7526e5","venue_1":"WebDB","year":"2001","title":"What Can Database Do for Peer-to-Peer?","authors":"Steven D. Gribble, Alon Y. Halevy, Zachary G. Ives, Maya Rodrig, Dan Suciu","author_ids":"1700451, 1770962, 1804315, 2581118, 1693752","abstract":"The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The grand vision — a decentralized community of machines pooling their resources to benefit everyone — is compelling for many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship. Existing peer-to-peer (P2P) systems have focused on specific application domains (e.g. music files) or on providing file-system-like capabilities; these systems ignore the semantics of data. An important question for the database community is how data management can be applied to P2P, and what we can learn from and contribute to the P2P area. We address these questions, identify a number of potential research ideas in the overlap between data management and P2P systems, present some preliminary fundamental results, and describe our initial work in constructing a P2P data management system.","cites":"156","conferencePercentile":"100"},{"venue":"WebDB","id":"1139b0083b8615be4c48c87f6699f4041ebdc5aa","venue_1":"WebDB","year":"1999","title":"Declarative Web Site Management with Tiramisu","authors":"Corin R. Anderson, Alon Y. Halevy, Daniel S. Weld","author_ids":"3106231, 1770962, 1780531","abstract":"Early research in declarative web-site management identiied a key principle: the separation of data management, site structure, and page presentation. This separation was made through the introduction of a logical representation, the site graph, deened as a view over underlying data. While the separation of these three tasks provides many beneets, existing systems require that the user implement the web site with the same system they use to design it, and this restriction is problematic for three reasons. First, many users are familiar with and prefer another implementation tool. Second, other tools may provide useful, new, or specialized features. Third, a complex site may be controlled by m ul-tiple organizations whose standards require different tools. In this paper we present a new architecture for declarative w eb-site management that separates design and implementation. We describe the challenges we faced during the implementation of Tiramisu, a system using this architecture.","cites":"25","conferencePercentile":"77.77777778"},{"venue":"WebDB","id":"d856ab405e2d1cb9a7181e3f6d0924c20717954a","venue_1":"WebDB","year":"2008","title":"Efficient Web-Based Linkage of Short to Long Forms","authors":"Yee Fan Tan, Ergin Elmacioglu, Min-Yen Kan, Dongwon Lee","author_ids":"2889313, 3179332, 1807775, 1784227","abstract":"Abbreviations, acronyms, initialisms, and shortenings frequently occur in many texts found on the Web, such as publication metadata, stock ticker codes, and biological articles. To connect these disparate forms together for knowledge discovery, short forms must be properly linked to their canonical long forms. In this paper, we demonstrate how a search engine can be efficiently utilized in mining the required contextual information, so that short forms can be effectively linked to long forms. We show that a count-based method consistently outperforms other methods, and that using the snippets is better than using the full web pages. We also consider adaptively combining a query probing algorithm together with our count-based method. This reduces running time and network bandwidth, while maintaining the strong linkage performance.","cites":"6","conferencePercentile":"42.85714286"},{"venue":"WebDB","id":"257e7a979e4bda3e9fbdbcf8a86df384015fc62f","venue_1":"WebDB","year":"2015","title":"Person-Name Parsing for Linking User Web Profiles","authors":"G. Sujatha Das, Xiang Li, Ang Sun, Hakan Kardes, Xin Wang","author_ids":"2279103, 1737850, 1823392, 3157001, 1736408","abstract":"A person-name parser involves the identification of constituent parts of a person's name. Due to multiple writing styles (\"John Smith\" versus \"Smith, John\"), extra information (\"John Smith, PhD\", \"Rev. John Smith\"), and country-specific last-name prefixes (\"Jean van de Velde\"), parsing fullname strings from user profiles on Web 2.0 applications is not straightforward. To the best of our knowledge, we are the first to address this problem systematically by proposing machine learning approaches for parsing noisy fullname strings.\n In this paper, we propose several types of features based on token statistics, surface-patterns, and specialized dictionaries and apply them within a sequence modeling framework to learn a fullname parser. In particular, we propose the use of \"bucket\" features based on (name-token, label) distributions in lieu of \"term\" features frequently used in various Natural Language Processing applications to prevent the growth of learning parameters as a function of the training data size. We experimentally illustrate the generalizability, effectiveness, and efficiency aspects of our proposed features for noisy fullname parsing on fullname strings from the popular, professional networking website LinkedIn and commonly-used person names in the United States. On these datasets, our fullname parser significantly outperforms both the parser trained using classification approaches and a commercially-available name parsing solution.","cites":"0","conferencePercentile":"31.25"},{"venue":"WebDB","id":"e524ccbf131f49b964239df896f777a4bcfb4a42","venue_1":"WebDB","year":"2016","title":"An index scheme for fast data stream to distributed append-only store","authors":"Parijat Mazumdar, Li Wang, Marianne Winslett, Zhenjie Zhang, Deokwoo Jung","author_ids":"2188283, 1703431, 1750874, 7970081, 2078699","abstract":"Distributed systems are now commonly used to manage massive data flooding from physical world, such as user-generated contents from online social media and communication records from mobile phones. The new generation of distributed data management systems, such as HBase, are designed to accept tuple insertions only, such that other database operations (e.g., deletion and update) are simply simulated by appending operation logs with keys associated to the target tuples. Such append-only store architecture maximizes the processing throughput on incoming data, but potentially incur higher costs on query processing, in which additional computation is needed to generate consistent snapshots of the database. Indexing is known as the key to enable efficient query processing by fast data retrieval and aggregation under such system architecture. This paper presents a new indexing scheme for distributed append-only stores. Our new scheme utilizes traditional index structures based on B-trees and its variant without the overhead of expensive node split, by using template-based tree construction. Optimized domain partitioning and multi-thread insertion techniques are further proposed to exploit the advantages of our template B-tree structure. Empirical evaluations show that our proposal outperforms existing solutions on insertion throughput by a large margin on a variety of real and synthetic workloads.","cites":"0","conferencePercentile":"38.88888889"},{"venue":"WebDB","id":"9fa68678effbe1df47e228a4d98b2661a0c1a9d6","venue_1":"WebDB","year":"2008","title":"LSH At Large - Distributed KNN Search in High Dimensions","authors":"Parisa Haghani, Sebastian Michel, Philippe Cudré-Mauroux, Karl Aberer","author_ids":"2911924, 8436249, 1680925, 1751802","abstract":"We consider K-Nearest Neighbor search for high dimensional data in large-scale structured Peer-to-Peer networks. We present an efficient mapping scheme based on p-stable Locality Sensitive Hashing to assign hash buckets to peers in a Chord-style overlay network. To minimize network traffic, we process queries in an incremental top-K fashion lever-aging on a locality preserving mapping to the peer space. Furthermore, we consider load balancing by harnessing estimates of the resulting data mapping, which follows a normal distribution. We report on a comprehensive performance evaluation using high dimensional real-world data, demonstrating the suitability of our approach.","cites":"13","conferencePercentile":"75"},{"venue":"WebDB","id":"132d9d65531c81ed687d2b24505eb8378db95b12","venue_1":"WebDB","year":"1999","title":"Exploiting Geographical Location Information of Web Pages","authors":"Orkut Buyukkokten, Junghoo Cho, Hector Garcia-Molina, Luis Gravano, Narayanan Shivakumar","author_ids":"2356107, 2581979, 1695250, 1684012, 1712337","abstract":"Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately , the geographical scope of web resources is largely ignored by web search engines. We make the case for identifying and exploiting the geographical location information of web sites so that web search engines can rank resources in a geographically sensitive fashion, in addition to using more traditional information-retrieval strategies. In this paper, we first consider how to compute the geographical location of web pages. Subsequently, we consider how to exploit such information in one specific \" proof-of-concept \" application we implemented in JAVA, and discuss other examples as well.","cites":"92","conferencePercentile":"100"},{"venue":"WebDB","id":"700029650fd08b7314d8ccc6359b5b1d95315c60","venue_1":"WebDB","year":"2002","title":"Data Management for Peer-to-Peer Computing : A Vision","authors":"Philip A. Bernstein, Fausto Giunchiglia, Anastasios Kementsietsidis, John Mylopoulos, Luciano Serafini, Ilya Zaihrayeu","author_ids":"1737944, 1720285, 1687892, 1750566, 4924053, 3358435","abstract":"We motivate special database problems introduced by peer-to-peer computing and propose the Local Relational Model (LRM) to solve some of them. As well, we summarize a formalization of LRM, present an architecture for a prototype implementation, and discuss open research questions.","cites":"290","conferencePercentile":"100"},{"venue":"WebDB","id":"84ba5ebe03df701a27d298e4c0617f29f61dfabb","venue_1":"WebDB","year":"2007","title":"A clustering-based sampling approach for refreshing search engine's database","authors":"Qingzhao Tan, Ziming Zhuang, Prasenjit Mitra, C. Lee Giles","author_ids":"2892700, 3017391, 1714911, 1749125","abstract":"Due to resource constraints, search engines usually have difficulties keeping the local database completely synchronized with the Web. To detect as many changes as possible, the crawler used by a search engine should be able to predict the change behavior of webpages so that it can use the limited resource to download those webpages that are most likely to change. Towards this goal, we propose using sampling approach at the level of a cluster. We first group all the local webpages into different clusters such that each cluster contains webpages with similar change patterns. We then sample webpages from each cluster to estimate the change frequency of all the webpages in that cluster, and the cluster containing webpages with higher change frequency will be revisited more often by our crawler. We run extensive experiments on a real Web data set of about 300,000 distinct URLs distributed among 210 websites. The results show that by applying our clustering algorithm, pages with similar change patterns are effectively clustered together. Our proposal significantly outperforms the comparators by improving the average freshness of the local database.","cites":"4","conferencePercentile":"45.45454545"},{"venue":"WebDB","id":"3b1f71abc6a725d462ad10c3628980f7dff78320","venue_1":"WebDB","year":"2010","title":"Learning Topical Transition Probabilities in Click Through Data with Regression Models","authors":"Xiao Zhang, Prasenjit Mitra","author_ids":"1695042, 1714911","abstract":"The transition of search engine users' intents has been studied for a long time. The knowledge of intent transition, once discovered, can yield a better understanding of how different topics are related and be used in many applications, such as building recommender systems, ranking and etc. In this paper, we study the problem of finding the transition probabilities of digital library users' intents among different topics. We use the click-through data from CiteSeerX and extract the click chains. Each document in the click chain is represented by a topical vector generated by LDA models. We then model the task of finding the topical transition probabilities as a multiple output linear regression problem, in which the input and output are two consecutive topical vectors in the click chain and the elements in the weight matrix correspond to the transition probabilities. Given the constraints of our task, we propose a new algorithm based on the exponentiated gradient. Our algorithm provides a good interpretability as well as a small sum-of-squares error comparable to existing regression methods. We are particular interested in the off-diagonal elements of the learned weight matrix since they represent the transition probabilities of different topics. The authors' interpretation of these transitions are given at the end of the paper.","cites":"0","conferencePercentile":"13.33333333"},{"venue":"WebDB","id":"1c474ca9904e9915a85a18683c6be1aa86631375","venue_1":"WebDB","year":"2003","title":"Modeling Query-Based Access to Text Databases","authors":"Eugene Agichtein, Panagiotis G. Ipeirotis, Luis Gravano","author_ids":"1685296, 2942126, 1684012","abstract":"Searchable text databases abound on the web. Applications that require access to such databases often resort to querying to extract relevant documents because of two main reasons. First, some text databases on the web are not \" crawlable, \" and hence the only way to retrieve their documents is via querying. Second, applications often require only a small fraction of a database's contents, so retrieving relevant documents via querying is an attractive choice from an efficiency viewpoint, even for crawlable databases. Often an application's query-based strategy starts with a small number of user-provided queries. Then, new queries are extracted –in an application-dependent way– from the documents in the initial query results, and the process iterates. The success of this common type of strategy relies on retrieved documents \" contributing \" new queries. If new documents fail to produce new queries, then the process might stall before all relevant documents are retrieved. In this paper, we develop a graph-based \" reachability \" metric that allows to characterize when an application's query-based strategy will successfully \" reach \" all documents that the application needs. We complement our metric with an efficient sampling-based technique that accurately estimates the reachability associated with a text database and an application's query-based strategy. We report preliminary experiments backing the usefulness of our metric and the accuracy of the associated estimation technique over real text databases and for two applications.","cites":"22","conferencePercentile":"58.33333333"},{"venue":"WebDB","id":"5611660a5f09be040e2a1c5c942fac88bd1e5d0e","venue_1":"WebDB","year":"2013","title":"Human-Powered Top-k Lists","authors":"Vassilis Polychronopoulos, Luca de Alfaro, James Davis, Hector Garcia-Molina, Neoklis Polyzotis","author_ids":"1804398, 1697774, 3050723, 1695250, 1763100","abstract":"We propose an algorithm that obtains the top-k list of items out of a larger itemset, using human workers (e.g., through crowdsourcing) to perform comparisons among items. An example application is finding the best photographs in a large collection by asking humans to evaluate different photos. Our algorithm has to address several challenges: obtaining worker input has high latency; workers may disagree on their judgments for the same items; some workers may provide wrong input on purpose; and, there is a varying difficulty in comparing different items. We provide experimental evidence for the good performance of the algorithm, using extensive simulations and actual experiments with workers from Amazon's Mechanical Turk.","cites":"6","conferencePercentile":"72.22222222"},{"venue":"WebDB","id":"095e9a637a7a2d12c161a257c264c29bf2d2b141","venue_1":"WebDB","year":"2005","title":"Design and Implementation of a Geographic Search Engine","authors":"Alexander Markowetz, Yen-Yu Chen, Torsten Suel, Xiaohui Long, Bernhard Seeger","author_ids":"1747393, 3002155, 1691664, 2863755, 3279660","abstract":"In this paper, we describe the design and initial implementation of a geographic search engine prototype for Germany, based on a large crawl of the de domain. Geographic search engines provide a flexible interface to the Web that allows users to constrain and order search results in an intuitive manner, by focusing a query on a particular geographic region. Geographic search technology has recently received significant commercial interest, but there has been only a limited amount of academic work. Our prototype performs massive extraction of geographic features from crawled data, which are then mapped to coordinates and aggregated across link and site structure. This assigns to each web page a set of relevant locations, called the geographic footprint of the page. The resulting footprint data is then integrated into a high-performance query processor on a cluster-based architecture. We discuss the various techniques, both new and existing , that are used for recognizing, matching, mapping, and aggre-gating geographic features, and describe how to integrate geographic query processing into a standard search architecture and interface.","cites":"39","conferencePercentile":"95.65217391"},{"venue":"WebDB","id":"5f6a614bfc6c33026a7fdb8b5b0c431f4558ec67","venue_1":"WebDB","year":"2004","title":"One Torus to Rule Them All: Multidimensional Queries in P2P Systems","authors":"Prasanna Ganesan, Beverly Yang, Hector Garcia-Molina","author_ids":"2745739, 2345503, 1695250","abstract":"Peer-to-peer systems enable access to data spread over an extremely large number of machines. Most P2P systems support only simple lookup queries. However, many new applications, such as P2P photo sharing and massively multi-player games, would benefit greatly from support for multidimensional range queries. We show how such queries may be supported in a P2P system by adapting traditional spatial-database technologies with novel P2P routing networks and load-balancing algorithms. We show how to adapt two popular spatial-database solutions - kd-trees and space-filling curves - and experimentally compare their effectiveness.","cites":"143","conferencePercentile":"93.75"},{"venue":"WebDB","id":"b75428bc47306492df2eeb61bb72a43d736d7e0e","venue_1":"WebDB","year":"2008","title":"Uncovering the Relational Web","authors":"Michael J. Cafarella, Alon Y. Halevy, Yang Zhang, Daisy Zhe Wang, Eugene Wu","author_ids":"1725561, 1770962, 4449904, 1786275, 1703739","abstract":"The WorldWide Web consists of a huge number of unstruc-tured hypertext documents, but it also contains structured data in the form of HTML tables. Many of these tables contain both relational-style data and a small \" schema \" of labeled and typed columns, making each such table a small structured database. The WebTables project is an effort to extract and make use of the huge number of these structured tables on the Web. A clean collection of relational-style tables could be useful for improving web search, schema design , and many other applications. This paper describes the first stage of the WebTables project. First, we give an in-depth study of the Web's HTML table corpus. For example, we extracted 14.1 billion HTML tables from a several-billion-page portion of Google's general-purpose web crawl, and estimate that 154 million of these tables contain high-quality relational-style data. We also describe the crawl's distribution of table sizes and data types. Second, we describe a system for performing relation recovery. The Web mixes relational and non-relational tables indiscriminately (often on the same page), so there is no simple way to distinguish the 1.1% of good relations from the remainder, nor to recover column label and type information. Our mix of handwritten detectors and statistical classifiers takes a raw Web crawl as input, and generates a collection of databases that is five orders of magnitude larger than any other collection we are aware of. Relation recovery achieves precision and recall that are comparable to other domain-independent information extraction systems.","cites":"63","conferencePercentile":"100"},{"venue":"WebDB","id":"9c903646133e19d3fe95c001588778f99069a904","venue_1":"WebDB","year":"2001","title":"Multicasting a Web Repository","authors":"Wang Lam, Hector Garcia-Molina","author_ids":"3288557, 1695250","abstract":"Web crawlers generate significant loads on Web servers, and are difficult to operate. Instead of running crawlers at many \" client \" sites, we propose a central crawler and Web repository that then multicasts appropriate subsets of the central repository to clients. Loads at Web servers are reduced because a single crawler visits the servers, as opposed to all the client crawlers. In this paper we model and evaluate such a central Web multicast facility. We develop multicast algorithms for the facility, comparing them with ones for \" broadcast disks. \" We also evaluate performance as several factors, such as object granularity and client batching, are varied.","cites":"5","conferencePercentile":"22.22222222"},{"venue":"WebDB","id":"d36bd40bc3b9be933f33bf58418bab0cf1014567","venue_1":"WebDB","year":"2012","title":"A General Approach for Securely Updating XML Data","authors":"Houari Mahfoud, Abdessamad Imine","author_ids":"2225914, 1766469","abstract":"Over the past years several works have proposed access control models for XML data where only read-access rights over non-recursive DTDs are considered. A small number of works have studied the access rights for updates. In this paper, we present a general model for specifying access control on XML data in the presence of the update operations of W3C XQuery Update Facility. Our approach for enforcing such update specification is based on the notion of query rewriting. A major issue is that query rewriting for recursive DTDs is still an open problem. We show that this limitation can be avoided using only the expressive power of the standard XPath, and we propose a linear algorithm to rewrite each update operation defined over an arbitrary DTD (re-cursive or not) into a safe one in order to be evaluated only over the XML data which can be updated by the user. This paper represents the first effort for securely XML updating in the presence of arbitrary DTDs (recursive or not) and a rich fragment of XPath. 1. MOTIVATION The XQuery Update Facility language [13] is a recommendation of W3C that provides a facility to modify some parts of an XML document and leave the rest unchanged, and this through different update operations. This includes rename, insert, replace and delete operations at the node level. The security requirement is the main problem when manipulating XML documents. An XML document may be queried and/or updated simultaneously by different users. For each class of users some rules can be defined to specify parts of the document which are accessible to the users and/or up-datable by them. A bulk of work has been published in the last decade to secure the XML content, but only read-access rights has been considered over non-recursive DTDs [3, 4, 9]. Moreover, a few works have considered update rights. In this paper, we investigate a general approach for securing XML update operations of the XQuery Update Facility language. Abstractly, for any update operation posed over an XML document, we ensure that the operation is performed only on XML nodes updatable by the user. Addressing such concerns requires first a specification model to define update constraints and a flexible mechanism to enforce these constraints at update time. We now present our motivating example for controlling update access. Consider the recursive DTD 1 of a hospital depicted as a graph in Fig. 1(b) …","cites":"1","conferencePercentile":"31.81818182"},{"venue":"WebDB","id":"1b41abbf9d3707a1a5c0fcf8e1f7734da0e61703","venue_1":"WebDB","year":"2009","title":"Beyond the Stars: Improving Rating Predictions using Review Text Content","authors":"Gayatree Ganu, Noémie Elhadad, Amélie Marian","author_ids":"2125359, 2763493, 2394972","abstract":"Online reviews are an important asset for users deciding to buy a product, see a movie, or go to a restaurant, as well as for businesses tracking user feedback. However, most reviews are written in a free-text format, and are therefore difficult for computer systems to understand, analyze, and aggregate. One consequence of this lack of structure is that searching text reviews is often frustrating for users. User experience would be greatly improved if the structure and sentiment conveyed in the content of the reviews were taken into account. Our work focuses on identifying this information from free-form text reviews, and using the knowledge to improve user experience in accessing reviews. Specifically, we focused on improving recommendation accuracy in a restaurant review scenario. In this paper, we report on our classification effort, and on the insight on user-reviewing behavior that we gained in the process. We propose new ad-hoc and regression-based recommendation measures, that both take into account the textual component of user reviews. Our results show that using textual information results in better general or personalized review score predictions than those derived from the numerical star ratings given by the users.","cites":"90","conferencePercentile":"100"},{"venue":"WebDB","id":"bb81f509a5516db2db4d607380c61a4fc7869e23","venue_1":"WebDB","year":"2009","title":"Extracting Route Directions from Web Pages","authors":"Xiao Zhang, Prasenjit Mitra, Sen Xu, Anuj R. Jaiswal, Alexander Klippel, Alan M. MacEachren","author_ids":"1695042, 1714911, 8182370, 3225963, 1725776, 7666349","abstract":"Linguists and geographers are more and more interested in route direction documents because they contain interesting motion descriptions and language patterns. A large number of such documents can be easily found on the Internet. A challenging task is to automatically extract meaningful route parts, i.e. destinations, origins and instructions, from route direction documents. However, no work exists on this issue. In this paper, we introduce our effort toward this goal. Based on our observation that sentences are the basic units for route parts, we extract sentences from HTML documents using both the natural language knowledge and HTML tag information. Additionally, we study the sentence classification problem in route direction documents and its sequential nature. Several machine learning methods are compared and analyzed. The impacts of different sets of features are studied. Based on the obtained insights, we propose to use sequence labelling models such as CRFs and MEMMs and they yield a high accuracy in route part extraction. The approach is evaluated on over 10,000 hand-tagged sentences in 100 documents. The experimental results show the effectiveness of our method. The above techniques have been implemented and published as the first module of the GeoCAM 1 system, which will also be briefly introduced in this paper.","cites":"11","conferencePercentile":"64.28571429"},{"venue":"WebDB","id":"f3a6725548c22d5bea6a0cb0b0a705d2e81475c9","venue_1":"WebDB","year":"2009","title":"Event Identification in Social Media","authors":"Hila Becker, Mor Naaman, Luis Gravano","author_ids":"3054813, 1687465, 1684012","abstract":"Social media sites such as Flickr, YouTube, and Facebook host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events. These range from widely known events, such as the presidential inauguration, to smaller, community-specific events, such as annual conventions and local gatherings. By identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can greatly improve local event browsing and search in state-of-the-art search engines. To address our problem of focus, we exploit the rich \" context \" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). We form a variety of representations of social media documents using different context dimensions, and combine these dimensions in a principled way into a single clustering solution—where each document cluster ideally corresponds to one event—using a weighted ensemble approach. We evaluate our approach on a large-scale, real-world dataset of event images, and report promising performance with respect to several baseline approaches. Our preliminary experiments suggest that our ensemble approach identifies events, and their associated images , more effectively than the state-of-the-art strategies on which we build.","cites":"37","conferencePercentile":"92.85714286"}]}