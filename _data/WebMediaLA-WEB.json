{"WebMediaLA-WEB.csv":[{"venue":"WebMedia/LA-WEB","id":"0b828b6c06328db320bb106103131d549230452d","venue_1":"WebMedia/LA-WEB","year":"2004","title":"Scheduling Algorithms for Web Crawling","authors":"Carlos Castillo, Mauricio Marín, M. Andrea Rodríguez, Ricardo A. Baeza-Yates","author_ids":"3747087, 1743311, 1699448, 1747635","abstract":"— This paper presents a comparative study of strategies for crawling the Web. In particular we show that a combination of breadth-first ordering with the largest sites first is a practical alternative since it is simple to implement, it is fast and is able to retrieve the best ranked pages at a rate that is closer to the optimal than the other alternatives. Our study was performed on an actual sample (90%) of the Chilean Web which was crawled by using simulators so that all strategies were compared under the same conditions and actual crawls to validate our conclusions. We also explored the effects of large scale parallelism in the page retrieval task and multiple-page requests in a single connection for effective amortization of waiting and latency times. I. INTRODUCTION Search engines play a central role in today's Web. Web search is currently generating more than 13% of the traffic to Web sites [1]. The main problem search engines have to deal with is the size of the Web, which currently is in the order of thousands of millions of pages. This large size induces a low coverage, with no search engine indexing more than one third of the publicly available Web [2]. In the early days of the Web, most search engines and directories included a prominent link to add your own Web page: Web site administrators were encouraged to submit their Web sites to the search engines. Today, that is no longer necessary, as search engine administrators consider that if the page cannot be easily reached by following links from the existent Web sites, then it is not \" valuable \" enough to be downloaded by the search engine's crawler. URLs of new pages are no longer a scarce resource. We would like to develop a scheduling policy for download-ing pages from the Web which guarantees that, even if we do not download all of the known pages, we still download the most \" valuable \" ones. As the number of pages grows, it will be increasingly important to download the \" better \" ones first, as it will be impossible to download them all. The main contributions of this paper are:","cites":"26","conferencePercentile":"100"}]}