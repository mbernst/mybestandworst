{"XSEDE.csv":[{"venue":"XSEDE","id":"fb023d7594cf98b51339493126f86ce3bd198cff","venue_1":"XSEDE","year":"2016","title":"SeedMe: A scientific data sharing and collaboration platform","authors":"Amit Chourasia, Mona Wong-Barnum, Dmitry Mishin, David R. Nadeau, Michael L. Norman","author_ids":"1952547, 1818730, 2978675, 1699959, 7868127","abstract":"Rapid secure data sharing and private online discussion are requirements for coordinating today's distributed science teams using High Performance Computing (HPC), visualization, and complex workflows. Modern HPC infrastructures enable fast computation, but the data produced remains within a site's storage and network environment tuned for performance rather than broad easy access. To share data and visualizations among distributed collaborators, manual efforts are required to move data out of HPC environments, stage it locally, bundle it with metadata and descriptions, manage versions, encode videos from visualization animations, and finally post it all online somewhere for secure access and discussion among project colleagues. While some of these tasks can be scripted, the effort remains cumbersome, time-consuming, and error prone. Thus, a more streamlined approach with a persistent infrastructure is needed.\n In this paper we describe SeedMe -- the Stream Encode Explore and Disseminate My Experiments platform for web-based scientific data sharing and discussion. SeedMe provides streamlined data movement from HPC and desktop environments, metadata management, data descriptions, video encoding, secure data sharing, threaded discussion, and, optionally, public access for education, outreach, and training.","cites":"0","conferencePercentile":"47.45762712"},{"venue":"XSEDE","id":"72a7cc84487558465563fc5c5c3d53b7029cfa1a","venue_1":"XSEDE","year":"2014","title":"Accelerating Sparse Canonical Correlation Analysis for Large Brain Imaging Genetics Data","authors":"Jingwen Yan, Hui Zhang, Lei Du, Eric Wernert, Andrew J. Saykin, Li Shen","author_ids":"3325036, 1732695, 1876810, 4132239, 7992909, 1712125","abstract":"Recent advances in acquiring high throughput neuroimaging and genomics data provide exciting new opportunities to study the influence of genetic variation on brain structure and function. Research in this emergent field, known as imaging genetics, aims to identify the association between genetic variations such as single nucleotide polymorphisms (SNPs) and neuroimaging quantitative traits (QTs). Sparse canonical correlation analysis (SCCA) is a bi-multivariate analysis method that has the potential to reveal complex multi-SNP-multi-QT associations. However, the scale and complexity of the imaging genetic data have presented critical computational bottlenecks requiring new concepts and enabling tools. In this paper, we present our initial efforts on developing a set of massively parallel strategies to accelerate a widely used SCCA implementation provided by the Penalized Multivariate Analysis (PMA) software package. In particular, we exploit parallel packages of R, optimized mathematical libraries, and the automatic offload model for Intel Many Integrated Core (MIC) architecture to accelerate SCCA. We create several simulated imaging genetics data sets of different sizes and use these synthetic data to perform comparative study. Our performance evaluation demonstrates that a 2-fold speedup can be achieved by the proposed acceleration. The preliminary results show that by combining data parallel strategy and the offload model for MIC we can significantly reduce the knowledge discovery timelines involving applying SCCA on large brain imaging genetics data.","cites":"0","conferencePercentile":"29.87012987"},{"venue":"XSEDE","id":"a994c8e22198fdad093d229d527b96ea35b2f875","venue_1":"XSEDE","year":"2015","title":"Grouping game players using parallelized k-means on supercomputers","authors":"Y. Dora Cai, Rabindra Robby Ratan, Cuihua Shen, Jay Alameda","author_ids":"8613863, 2944082, 1793905, 2579921","abstract":"Grouping game players based on their online behaviors has attracted a lot of attention recently. However, due to the huge volume and extreme complexity in online game data collections, grouping players is a challenging task. This study has applied parallelized K-Means on Gordon, a supercomputer hosted at San Diego Supercomputer Center, to meet the computational challenge on this task. By using the parallelization functions supported by R, this study was able to cluster 120,000 game players into eight non-overlapping groups and speed up the clustering process by one to four times under the two- to eight-degree of parallelization. This study has systematically examined a number of factors which may affect the quality of the clusters and/or the performance of the clustering processes; those factors include degree of parallelism, number of clusters, data dimensions, and variable combinations. This study invented a method to identify the optimal clustering schema, which can choose the most discriminative features and create an appropriate number of clusters in K-Means clustering. Besides demonstrating the effectiveness of parallelized K-Means in grouping game players, this study also highlights some lessons learned for using K-Means on very large datasets and some experience on applying parallel processing techniques in intensive data analysis.","cites":"0","conferencePercentile":"33.72093023"},{"venue":"XSEDE","id":"1ce78a63cdd5204f62ae3bb2273ce5cca7223cc3","venue_1":"XSEDE","year":"2013","title":"Exploring Twitter networks in parallel computing environments","authors":"Bo Xu, Yun Huang, Noshir S. Contractor","author_ids":"1749224, 1777347, 6203716","abstract":"Millions of users follow each other on Twitter and form a large and complex network. The size of the network creates statistical and computational challenges on exploring and examining individual behavior on Twitter. Using a sample of 697,628 Korean Twitter users and 34 million relations, this study investigates the patterns of unfollow behavior on Twitter, i.e. people removing others from their Twitter follow lists. We use Exponential Random Graph Models (p*/ERGMs) and Statnet in R to examine the impacts of reciprocity, status, embeddedness, homophily, and informativeness on tie dissolution. We perform data processing, statistics calculation, network sampling, and Markov chain Monte Carlo (MCMC) simulation on Gordon, a unique supercomputer at the San Diego Supercomputer Center (SDSC). The process demonstrates the role of advanced computing technologies in social science studies.","cites":"0","conferencePercentile":"21.01449275"},{"venue":"XSEDE","id":"7e95f39cd6c37cb2e2663e207fd0f7ebddd5db19","venue_1":"XSEDE","year":"2013","title":"The parallel system for integrating impact models and sectors (pSIMS)","authors":"Joshua Elliott, David Kelly, Neil Best, Michael Wilde, Michael Glotter, Ian T. Foster","author_ids":"1907378, 5912109, 3317180, 7845728, 3169330, 1698701","abstract":"We present a framework for massively parallel simulations of climate impact models in agriculture and forestry: the parallel System for Integrating Impact Models and Sectors (pSIMS). This framework comprises a) tools for ingesting large amounts of data from various sources and standardizing them to a versatile and compact data type; b) tools for translating this standard data type into the custom formats required for point-based impact models in agriculture and forestry; c) a scalable parallel framework for performing large ensemble simulations on various computer systems, from small local clusters to supercomputers and even distributed grids and clouds; d) tools and data standards for reformatting outputs for easy analysis and visualization; and d) a methodology and tools for aggregating simulated measures to arbitrary spatial scales such as administrative districts (counties, states, nations) or relevant environmental demarcations such as watersheds and river-basins. We present the technical elements of this framework and the results of an example climate impact assessment and validation exercise that involved large parallel computations on XSEDE.","cites":"6","conferencePercentile":"91.30434783"},{"venue":"XSEDE","id":"a19531313118fc96621f70b8a18401ea4e4941ba","venue_1":"XSEDE","year":"2016","title":"Globus: Recent Enhancements and Future Plans","authors":"Kyle Chard, Steven Tuecke, Ian T. Foster","author_ids":"3091414, 1720669, 1698701","abstract":"Globus offers a broad suite of research data management capabilities to the research community as web-accessible services. The initial service, launched in 2010, focused on reliable, high-performance, secure data transfer; since that time, Globus capabilities have been progressively enhanced in response to user demand. In 2015, secure data sharing and publication services were introduced. Other recent enhancements include support for secure HTTP data access, new storage system types (e.g., Amazon S3, HDFS, Ceph), endpoint search, and administrator management. A powerful new authentication and authorization platform service, Globus Auth, addresses identity, credential, and delegation management needs encountered in research environments. New REST APIs allow external and third-party services to leverage Globus data management, authentication, and authorization capabilities as a platform, for example when building research data portals. We describe these and other recent enhancements to Globus, review adoption trends (to date, 40,000 registered users have operated on more than 150PB and 25B files), and present future plans.","cites":"1","conferencePercentile":"97.45762712"},{"venue":"XSEDE","id":"1ce5dce68b6bf881ad27221746bc3eb84fd954f5","venue_1":"XSEDE","year":"2013","title":"Experiences in building a next-generation sequencing analysis service using galaxy, globus online and Amazon web service","authors":"Ravi K. Madduri, Paul Dave, Dinanath Sulakhe, Lukasz Lacinski, Bo Liu, Ian T. Foster","author_ids":"1860802, 2005582, 1802055, 3035939, 1724237, 1698701","abstract":"We describe Globus Genomics, a system that we have developed for rapid analysis of large quantities of next-generation sequencing (NGS) genomic data. This system is notable for its high degree of end-to-end automation, which encompasses every stage of the data analysis pipeline from initial data access (from remote sequencing center or database, by the Globus Online file transfer system) to on-demand resource acquisition (on Amazon EC2, via the Globus Provision cloud manager); specification, configuration, and reuse of multi-step processing pipelines (via the Galaxy workflow system); and efficient scheduling of these pipelines over many processors (via the Condor scheduler). The system allows biomedical researchers to perform rapid analysis of large NGS datasets using just a web browser in a fully automated manner, without software installation.","cites":"10","conferencePercentile":"99.27536232"},{"venue":"XSEDE","id":"fe5a398fcccfc802691a0cedb77b9109bda88f16","venue_1":"XSEDE","year":"2015","title":"FlowGate: towards extensible and scalable web-based flow cytometry data analysis","authors":"Yu Qian, Hyunsoo Kim, Shweta Purawat, Jianwu Wang, Rick Stanton, Alexandra Lee, Weijia Xu, Ilkay Altintas, Robert S. Sinkovits, Richard H. Scheuermann","author_ids":"4674306, 1687121, 2380825, 2479724, 2624665, 2223199, 2783434, 1729053, 2940708, 7406886","abstract":"Recent advances in cytometry instrumentation are enabling the generation of \"big data\" at the single cell level for the identification of cell-based biomarkers, which will fundamentally change the current paradigm of diagnosis and personalized treatment of immune system disorders, cancers, and blood diseases. However, traditional flow cytometry (FCM) data analysis based on manual gating cannot effectively scale to address this new level of data generation. Computational data analysis methods have recently been developed to cope with the increasing data volume and dimensionality generated from FCM experiments. Making these computational methods easily accessible to clinicians and experimentalists is one of the biggest challenges that algorithm developers and bioinformaticians need to address. This paper describes FlowGate, a novel prototype cyberinfrastructure for web-based FCM data analysis, which integrates graphical user interfaces (GUI), workflow engines, and parallel computing resources for extensible and scalable FCM data analysis. The goal of FlowGate is to allow users to easily access state-of-the-art FCM computational methods developed using different programming languages and software on the same platform, when the implementations of these methods follow standardized I/O. By adopting existing data and information standards, FlowGate can also be integrated as the back-end data analytical platform with existing immunology and FCM databases. Experimental runs of two representative FCM data analytical methods in FlowGate on different cluster computers demonstrated that the task runtime can be reduced linearly with the number of compute cores used in the analysis.","cites":"0","conferencePercentile":"33.72093023"},{"venue":"XSEDE","id":"53e787735a245c1aba90ca8336600092f7bbacbe","venue_1":"XSEDE","year":"2015","title":"Performance examinations of multiple time-stepping algorithms on stampede supercomputer","authors":"Na Zhang, Peng Zhang, Li Zhang, Xiao Zhu, Lei Huang, Yuefan Deng","author_ids":"1789039, , 1712838, 4434306, 1750444, 1757145","abstract":"Our examinations of the methodical implementation of the multiple time-stepping algorithms on the Stampede supercomputer reveal a speedup factor of 23 over single time-stepping algorithm, for the same problem, with the combined algorithmic and hardware accelerations. More specifically, the MTS algorithm is 11.5 times faster than the STS algorithm; the GPU-enabled system performs 2 times faster than the CPU-only system. Combining these speedups and using MTS algorithm on the Stampede with 16 GPU nodes, we can simulate 1- <i>ms</i> multiscale phenomena of flowing platelets in blood vessels within approximate 37 days, enabling practical modeling of millisecond-scale biological phenomena with spatial resolutions at the nanoscales. The mathematical algorithms and the advances of computer hardware, such as the Stampede supercomputer, that can be leveraged allow us to explore the new frontiers of cutting-edge applications in medical and life sciences.","cites":"1","conferencePercentile":"82.55813953"},{"venue":"XSEDE","id":"1d3678f244f2d98ebba7d0e667ee8ae76f010e24","venue_1":"XSEDE","year":"2015","title":"Jetstream: a self-provisioned, scalable science and engineering cloud environment","authors":"Craig A. Stewart, Tim Cockerill, Ian T. Foster, David Y. Hancock, Nirav Merchant, Edwin Skidmore, Daniel C. Stanzione, James Taylor, Steven Tuecke, George W. Turner, Matthew W. Vaughn, Niall Gaffney","author_ids":"1756947, 1937478, 1698701, 1794613, 1769885, 1785278, 1796844, 5068052, 1720669, 1752494, 8298287, 2829685","abstract":"Jetstream will be the first production cloud resource supporting general science and engineering research within the XD ecosystem. In this report we describe the motivation for proposing Jetstream, the configuration of the Jetstream system as funded by the NSF, the team that is implementing Jetstream, and the communities we expect to use this new system. Our hope and plan is that Jetstream, which will become available for production use in 2016, will aid thousands of researchers who need modest amounts of computing power interactively. The implementation of Jetstream should increase the size and disciplinary diversity of the US research community that makes use of the resources of the XD ecosystem.","cites":"8","conferencePercentile":"100"},{"venue":"XSEDE","id":"8fb19fe2ec8738c86c993dd5a7c5e47bb4301016","venue_1":"XSEDE","year":"2016","title":"Accelerating TauDEM for Extracting Hydrology Information from National-Scale High Resolution Topographic Dataset: Extended Abstract","authors":"Ahmet Artu Yildirim, David Tarboton, Yan Liu, Nazmus Shams Sazib, Shaowen Wang","author_ids":"3035883, 1961050, 1681842, 3452175, 1791947","abstract":"We present performance improvements on parallel hydrology algorithms in TauDEM suite that allowed us to process the 10m USGS 3DEP topographic dataset (667GB, 180 billion raster cells) by adopting a block-wise data decomposition feature and maximazing the disk parallelism.","cites":"0","conferencePercentile":"47.45762712"},{"venue":"XSEDE","id":"5c53b379d459fc1f0ab7ec5f8ed07cde68b5dc9f","venue_1":"XSEDE","year":"2013","title":"Human centered game design for bioinformatics and cyberinfrastructure learning","authors":"Daniel Perry, Cecilia R. Aragon, Stephanie Cruz, Mette A. Peters, Jeanne Ting Chowning","author_ids":"3435018, 1681036, 2027308, 7261092, 2466903","abstract":"Engaging students in science, technology, engineering, and math (STEM) fields is critical to ensure the success of the next generation of scientists and engineers. Given that 97% of American teens play video games, there is a tremendous opportunity to facilitate interest in STEM topics through the design of engaging learning games. While a growing number of serious games have been developed for biological science and computer science learning, few address the communication and technical challenges that arise in cyberinfrastructure intensive projects, where multiple domain scientists and computer scientists collaborate. This paper describes empirical data collected during a year-long human centered game design process, in which design ideas generated by high school students were bridged with cyberinfrastructure and bioinformatics learning concepts. Our research shows that \"fun\" and engaging game elements are well suited for addressing the sociotechnical aspects of cyberinfrastructure projects. In this research we provide a human centered game design methodology for science educators and science game designers, as well as design implications for integrating game-based experiences into the use of large-scale shared computing resources and services.","cites":"1","conferencePercentile":"55.07246377"},{"venue":"XSEDE","id":"75d8980d10b66ee55530b484c6ce1ff7b68a28ef","venue_1":"XSEDE","year":"2014","title":"FeatureSelector: an XSEDE-Enabled Tool for Massive Game Log Analysis","authors":"Y. Dora Cai, Bettina C. Riedl, Rabindra Robby Ratan, Cuihua Shen, Arnold Picot","author_ids":"8613863, 1773225, 2944082, 1793905, 1779241","abstract":"Due to the huge volume and extreme complexity in online game data collections, selecting essential features for the analysis of massive game logs is not only necessary, but also challenging. This study develops and implements a new XSEDE-enabled tool, FeatureSelector, which uses the parallel processing techniques on high performance computers to perform feature selection. By calculating probability distance measures, based on K-L divergence, this tool quantifies the distance between variables in data sets, and provides guidance for feature selection in massive game log analysis. This tool has helped researchers choose the high-quality and discriminative features from over 300 variables, and select the top pairs of countries with the greatest differences from 231 country-pairs in a 500 GB game log data set. Our study shows that (1) K-L divergence is a good measure for correctly and efficiently selecting important features, and (2) the high performance computing platform supported by XSEDE has substantially accelerated the feature selection processes by over 30 times. Besides demonstrating the effectiveness of FeatureSelector in a cross-country analysis using high performance computing, this study also highlights some lessons learned for feature selection in social science research and some experience on applying parallel processing techniques in intensive data analysis.","cites":"1","conferencePercentile":"72.07792208"},{"venue":"XSEDE","id":"91e7b14f6f565108b11f7a1334e8d42253fe7e66","venue_1":"XSEDE","year":"2015","title":"Leveraging DiaGrid hub for interactively generating and running parallel programs","authors":"Ritu Arora, Kevin Chen, Madhav Gupta, Steven M. Clark, Carol X. Song","author_ids":"2936311, 1792429, 2670421, 2723007, 3205382","abstract":"Interactive Parallelization Tool (IPT) is a semi-automatic tool that can be used by domain experts and students for transforming certain classes of existing applications into multiple parallel variants. An end-user of IPT provides existing application and high-level specifications for parallelization as input. On the basis of the specifications provided by the end-user, IPT carries out the code changes in the given existing application to generate parallel variants that can be run on different High Performance Computing (HPC) platforms. The parallel programming paradigms that are currently supported by IPT are MPI, OpenMP, and CUDA. The supported base languages are C and C++. Though IPT is still under active development, it has been recently made available on a web-enabled platform, named DiaGrid Hub, with the support from the XSEDE Extended Collaborative Support Service (ECSS). While the main goal of IPT is to make parallel programming easy for its end-users, the main goal of DiaGrid Hub is to enable the research community with instant access to HPC and High Throughput Computing platforms through a user-friendly web-interface. By deploying IPT on DiaGrid Hub, our goal is to enable the end-users to generate parallel versions of their existing applications without having to install IPT locally. They can also immediately compile and run the generated applications on Purdue and XSEDE resources that are available through DiaGrid Hub. Hence, the collaborative project that is reported in this paper lowers the entry-barriers to parallel programming and the usage of the national CyberInfrastructure (CI). In this paper, we present our ongoing work on deploying IPT over DiaGrid and testing the usability of IPT through a web-interface.","cites":"1","conferencePercentile":"82.55813953"},{"venue":"XSEDE","id":"dde8e0ddfa72ee394bad2238f87acc49d9e33512","venue_1":"XSEDE","year":"2014","title":"Performance Improvement and Workflow Development of Virtual Diffraction Calculations","authors":"Shawn P. Coleman, Sudhakar Pamidighantam, Mark Van Moer, Yang Wang, Lars Koesterke, Douglas E. Spearot","author_ids":"2648938, 1813756, 2678115, 1747927, 3047977, 2691348","abstract":"Electron and x-ray diffraction are well-established experimental methods used to explore the atomic scale structure of materials. In this work, a computational algorithm is presented to produce electron and x-ray diffraction patterns directly from atomistic simulation data. This algorithm advances beyond previous virtual diffraction methods by utilizing an ultra high-resolution mesh of reciprocal space which eliminates the need for a priori knowledge of the material structure. This paper focuses on (1) algorithmic advances necessary to improve performance, memory efficiency and scalability of the virtual diffraction calculation, and (2) the integration of the diffraction algorithm into a workflow across heterogeneous computing hardware for the purposes of integrating simulations, virtual diffraction calculations and visualization of electron and x-ray diffraction patterns.","cites":"1","conferencePercentile":"72.07792208"}]}