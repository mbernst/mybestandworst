venue,id,venue,year,title,authors,author_ids,abstract,cites,conferencePercentile
UIST,cbad41bcfd5b8a1b6c21047aae421d9d0f4d5dec,UIST,1991,SUIT: the Pascal of user interface toolkits,"Randy F. Pausch, Nathaniel R. Young, Robert DeLine","2617964, 2756211, 1710751","User interface support software, such as UI toolkits, UIMSS, and interface builders, are currently too complex for undergraduates. Tools typically require a learning period of several weeks, which is impractical in a semester course. Most tools are also limited to a specific platform, usually either Macintosh, DOS, or UNIX/X. This is problematic for students who switch from DOS or Macintosh machines to UNIX machines as they move through the curriculum. The situation is similar to programming languages before the introduction of Pascal, which provided an easily poned, easily learned language for undergraduate instruction. SUIT (the ~lmple ~ser ~rtterface ~oolkit), is a C subroutine library which provides an external control UIMS, an interactive layout editor, and a set of standard screen objects. SUIT applications run transparently across Macintosh, DOS, UNIX/X, and Silicon Graphics platfomts. Through careful design and extensive user testing of the system and its documentation, we have been able to reduce learning time. We have formally measured that new users are productive with SUIT in less than three hours. SUIT currently has over one hundred students using it for Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or spacific permission. undergraduate and graduate course work and for research projects.",19,52.1739130435
UIST,3a2b22884be16397c6a3d02261a988bf3ad6b005,UIST,2012,Facet: a multi-segment wrist worn system,"Kent Lyons, David H. Nguyen, Daniel Ashbrook, Sean White","2073793, 1767024, 2071682, 5047258","We present Facet, a multi-display wrist worn system consisting of multiple independent touch-sensitive segments joined into a bracelet. Facet automatically determines the pose of the system as a whole and of each segment individually. It further supports multi-segment touch, yielding a rich set of touch input techniques. Our work builds on these two primitives to allow the user to control how applications use segments alone and in coordination. Applications can expand to use more segments, collapses to encompass fewer, and be swapped with other segments. We also explore how the concepts from Facet could apply to other devices in this design space.",37,89.7058823529
UIST,d625264d985c248e4c4e4316695487d27c34930f,UIST,2013,GIST: a gestural interface for remote nonvisual spatial perception,"Vinitha Khambadkar, Eelke Folmer","2186148, 3267671","Spatial perception is a challenging task for people who are blind due to the limited functionality and sensing range of hands. We present GIST, a wearable gestural interface that offers spatial perception functionality through the novel appropriation of the user's hands into versatile sensing rods. Using a wearable depth-sensing camera, GIST analyzes the visible physical space and allows blind users to access spatial information about this space using different hand gestures. By allowing blind users to directly explore the physical space using gestures, GIST allows for the closest mapping between augmented and physical reality, which facilitates spatial interaction. A user study with eight blind users evaluates GIST in its ability to help perform everyday tasks that rely on spatial perception, such as grabbing an object or interacting with a person. Results of our study may help develop new gesture based assistive applications.",2,37.6146788991
UIST,160ffcc853f9c7b32f8ff33f2fe17737bdb612db,UIST,2000,The reading assistant: eye gaze triggered auditory prompting for reading remediation,"John L. Sibert, Mehmet Gokturk, Robert A. Lavine","1796576, 1739667, 2037296","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT We have developed a system for remedial reading instruction that uses visually controlled auditory prompting to help the user with recognition and pronunciation of words. Our underlying hypothesis is that the relatively unobtrusive assistance rendered by such a system will be more effective than previous computer aided approaches. We present a description of the design and implementation of our system and discuss a controlled study that we undertook to evaluate the usability of the Reading Assistant.",32,40.0
UIST,4374e0723b7ad4001f0c8dfe4e8b5cdcb30a4921,UIST,2016,SoEs: Attachable Augmented Haptic on Gaming Controller for Immersive Interaction,"Yang-Sheng Chen, Ping-Hsuan Han, Jui-Chun Hsiao, Kong-Chang Lee, Chiao-En Hsieh, Kuan-Yin Lu, Chien-Hsing Chou, Yi-Ping Hung","2094077, 2835232, 3455108, 3493049, 3460534, 2124353, 2037722, 7312257","We present SoEs (Sword of Elements), an attachable augmented haptic device for enhancing gaming controller in the immersive first-person game. Generally, Player can easily receive visual and auditory feedback through head-mounted displays (HMD) and headphones from first-person perspective in virtual world. However, the tactile feedback is less than those feedbacks in immersive environment. Although gaming controller, i.e. VIVE or Oculus controller, can provide tactile feedback by some vibration sensors, the haptic feedback is more complicated and various, it includes kinesthesia and cutaneous feedback. Our key idea is to provide a low-cost approach to simulate the haptic feedback of player manipulation in the immersive environment such as striking while the iron is hot which the player could feel the heat and reaction force. Eventually, the game makers could utilize the attachable device into their games for providing haptic feedback.",0,44.6540880503
UIST,211bb25046cc5b96f29692567aec3c07c1d7d7f6,UIST,2007,SketchWizard: Wizard of Oz prototyping of pen-based user interfaces,"Richard C. Davis, T. Scott Saponas, Michael Shilman, James A. Landay","2167560, 1766388, 2139615, 1708404","SketchWizard allows designers to create Wizard of Oz prototypes of pen-based user interfaces in the early stages of design. In the past, designers have been inhibited from participating in the design of pen-based interfaces because of the inadequacy of paper prototypes and the difficulty of developing functional prototypes. In SketchWizard, designers and end users share a drawing canvas between two computers, allowing the designer to simulate the behavior of recognition or other technologies. Special editing features are provided to help designers respond quickly to end-user input. This paper describes the SketchWizard system and presents two evaluations of our approach. The first is an early feasibility study in which Wizard of Oz was used to prototype a pen-based user interface. The second is a laboratory study in which designers used SketchWizard to simulate existing pen-based interfaces. Both showed that end users gave valuable feedback in spite of delays between end-user actions and wizard updates.",29,55.5555555556
UIST,b60d3f87dec8d67b69d809583239134417b3f417,UIST,2013,"Mime: compact, low power 3D gesture sensing for interaction with head mounted displays","Andrea Colaco, Ahmed Kirmani, Hye Soo Yang, Nan-Wei Gong, Chris Schmandt, Vivek K. Goyal","4716385, 2633065, 3224028, 1690147, 1729321, 1693023","We present <i>Mime</i>, a compact, low-power 3D sensor for unencumbered free-form, single-handed gestural interaction with head-mounted displays (HMDs). Mime introduces a real-time signal processing framework that combines a novel three-pixel time-of-flight (TOF) module with a standard RGB camera. The TOF module achieves accurate 3D hand localization and tracking, and it thus enables motion-controlled gestures. The joint processing of 3D information with RGB image data enables finer, shape-based gestural interaction.
 Our Mime hardware prototype achieves fast and precise 3D gestural control. Compared with state-of-the-art 3D sensors like TOF cameras, the Microsoft Kinect and the Leap Motion Controller, Mime offers several key advantages for mobile applications and HMD use cases: very small size, daylight insensitivity, and low power consumption. Mime is built using standard, low-cost optoelectronic components and promises to be an inexpensive technology that can either be a peripheral component or be embedded within the HMD unit. We demonstrate the utility of the Mime sensor for HMD interaction with a variety of application scenarios, including 3D spatial input using close-range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions.",24,87.1559633028
UIST,8927d81c4d53a20c611466bd3890dec48cbb6c96,UIST,2006,CINCH: a cooperatively designed marking interface for 3D pathway selection,David Akers,2029565,"To disentangle and analyze neural pathways estimated from magnetic resonance imaging data, scientists need an interface to select 3D pathways. Broad adoption of such an interface requires the use of commodity input devices such as mice and pens, but these devices offer only two degrees of freedom. CINCH solves this problem by providing a marking interface for 3D pathway selection. CINCH interprets pen strokes as pathway selections in 3D using a marking language <i>designed together with scientists</i>. Its bimanual interface employs a pen and a trackball (see Figure 1), allowing alternating selections and scene rotations without changes of mode. CINCH was evaluated by observing four scientists using the tool over a period of three weeks as part of their normal work activity. Event logs and interviews revealed dramatic improvements in both the speed and quality of scientists' everyday work, and a set of principles that should inform the design of future 3D marking interfaces. More broadly, CINCH demonstrates the value of the iterative, participatory design process that catalyzed its evolution.",21,31.25
UIST,2a6662f969d77e2d9a4e94b3867e8625a2b1cc32,UIST,2014,Kitty: sketching dynamic and interactive illustrations,"Rubaiat Habib Kazi, Fanny Chevalier, Tovi Grossman, George W. Fitzmaurice","2820401, 2840251, 3313809, 1703735","We present Kitty, a sketch-based tool for authoring dynamic and interactive illustrations. Artists can sketch animated drawings and textures to convey the living phenomena, and specify the functional relationship between its entities to characterize the dynamic behavior of systems and environments. An underlying graph model, customizable through sketching, captures the functional relationships between the visual, spatial, temporal or quantitative parameters of its entities. As the viewer interacts with the resulting dynamic interactive illustration, the parameters of the drawing change accordingly, depicting the dynamics and chain of causal effects within a scene. The generality of this framework makes our tool applicable for a variety of purposes, including technical illustrations, scientific explanation, infographics, medical illustrations, children's e-books, cartoon strips and beyond. A user study demonstrates the ease of usage, variety of applications, artistic expressiveness and creative possibilities of our tool.",10,80.2325581395
UIST,4e2d0ee0e63b23b33843c5276e635e90ea1036c0,UIST,2016,Uniformity Based Haptic Alert Network,"Salem Karani, Chaitanya Varanasi","3492864, 3492958","We experience haptic feedback on a wide variety of devices in the modern day, including cellphones, tablets, and smartwatches. However haptic alerts can quickly become disruptive rather than helpful to a user when multiple devices are providing feedback simultaneously or consecutively. Thus in this paper, we propose an intercommunicating, turn-based local network between a user's devices. This will allow a guaranteed minimal time span between device alerts. Additionally, when multiple devices provide a notification-based haptic alert, devices often produce different feedback due to the varying materials they are placed on. To address this, our framework allows devices to self-regulate their levels of haptic responses based on the material density of the surface they are placed on. This allows the framework to enforce a uniform level of haptic feedback across all the surface-device combinations. Finally, we will also utilize this common network to eliminate redundant alerts across devices.",0,44.6540880503
UIST,b704ea303ec87bbedac36369c3dcd248a67f6a7c,UIST,2015,Color Sommelier: Interactive Color Recommendation System Based on Community-Generated Color Palettes,"KyoungHee Son, Seo Young Oh, Yongkwan Kim, Hayan Choi, Seok-Hyung Bae, Ganguk Hwang","1961230, 8110278, 2785722, 2961374, 1715434, 1975286","We present Color Sommelier, an interactive color recommendation system based on community-generated color palettes that helps users to choose harmonious colors on the fly. We used an item-based collaborative filtering technique with Adobe Color CC palettes in order to take advantage of their ratings that reflect the general public?s color harmony preferences. Every time a user chooses a color(s), Color Sommelier calculates how harmonious each of the remaining colors is with the chosen color(s). This interactive recommendation enables users to choose colors iteratively until they are satisfied. To illustrate the usefulness of the algorithm, we implemented a coloring application with a specially designed color chooser. With the chooser, users can intuitively recognize the harmony score of each color based on its bubble size and use the recommendations at their discretion. The Color Sommelier algorithm is flexible enough to be applicable to any color chooser in any software package and is easy to implement.",0,16.2280701754
UIST,9266d12315f50e9ac0ae55a09dcf74cb08b05996,UIST,2011,Accessibility for individuals with color vision deficiency,David R. Flatla,3247725,"Individuals with Color Vision Deficiency (CVD) are often unable to distinguish between colors that individuals without CVD can distinguish. Recoloring tools exist that modify the colors in an image so they are more easily distinguishable for those with CVD. These tools use models of color differentiation that rely on many assumptions about the environment and user. However, these assumptions rarely hold in real-world use cases, leading to incorrect color modification by recoloring tools. In this doctoral symposium, I will present Situation-Specific Models (SSMs) as a solution to this problem. SSMs are color differentiation models created in-situ via a calibration procedure. This calibration procedure captures the exact color differentiation abilities of the user, allowing a color differentiation model to be created that fits the user and his/her environmental situation. An SSM-based recoloring tool will be able to provide recolored images that most accurately reflect the color differentiation abilities of a particular individual in a particular environment.",3,31.4285714286
UIST,94340e78f2b5f8f3c3208bb19599eaaec8c6e1bf,UIST,2012,Towards document engineering on pen and touch-operated interactive tabletops,Fabrice Matulic,3268147,"Touch interfaces have now become mainstream thanks to modern smartphones and tablets. However, there are still very few ""productivity"" applications, i.e. tools that support mundane but essential work, especially for large interactive surfaces such as digital tabletops. This work aims to partly fill the relative void in the area of document engineering by investigating what kind of intuitive and efficient tools can be provided to support the manipulation of documents on a digital workdesk, in particular the creation and editing of documents. The fundamental interaction model relies on bimanual pen and multitouch input, which was recently introduced to tabletops and enables richer interaction possibilities. The goal is ultimately to provide useful and highly accessible UIs for document-centric applications, whose design principles will hopefully pave the way from DTP towards DTTP (Digital Tabletop Publishing).",0,9.80392156863
UIST,20b7ad09a1ffed0bf7f0fbaa627bf257eecaf81b,UIST,2016,WhammyPhone: Exploring Tangible Audio Manipulation Using Bend Input on a Flexible Smartphone,"Antonio Gomes, Lahiru Lakmal Priyadarshana, Juan Pablo Carrascal, Roel Vertegaal","7318622, 3397072, 2883811, 1687608","We present WhammyPhone, a novel audio interface that supports physical manipulation of digital audio through bend gestures. WhammyPhone combines a high-resolution flexible display, bend sensors, and a set of intuitive interaction techniques that enable novice users to manipulate sound in a tangible fashion. With WhammyPhone, bend gestures can control both discrete (e.g. triggering a note) and continuous parameters (e.g. pitch bend). We showcase application scenarios that leverage the unique input modalities of WhammyPhone and discuss its potential for digital audio manipulation.",0,44.6540880503
UIST,bf260646d1c7cafc24899d42147883c42fc341ac,UIST,2010,LuminAR: portable robotic augmented reality interface design and prototype,"Natan Linder, Pattie Maes","2033673, 1701876","In this paper we introduce LuminAR: a prototype for a new portable and compact projector-camera system designed to use the traditional incandescent bulb interface as a power source, and a robotic desk lamp that carries it, enabling it with dynamic motion capabilities. We are exploring how the LuminAR system embodied in a familiar form factor of a classic Angle Poise lamp may evolve into a new class of robotic, digital information devices.",17,75.5813953488
UIST,cf738aee4c4a45e1d4b60c9dbe6d2cd2bc5691a2,UIST,2013,LivingClay: particle actuation to control display volume and stiffness,"Jefferson Pardomuan, Toshiki Sato, Hideki Koike","2328084, 3269647, 1684942","We present a new type of display actuation that is able to control both display geometry and stiffness properties using a filler material and air flow control technique. The display consists of a flat, flexible layer of cells on the surface and chamber filled with particles under it. Display geometries can be changed by transporting an amount of particles between display cells and the particle chamber using pressured air and vacuum to control the air flows. This system also allow for variable stiffness using vacuum technique to harden the particles inside chamber. In this paper, we present the design and control technique of this new type actuator and also possible interaction on a single actuator display. We also propose a low-cost, effective way to control an array of actuators where the air flow line and particle line are arranged in a multiplexed grid configuration.",0,10.5504587156
UIST,e27e85dfce07e039d849e20254d544c51b83890b,UIST,2016,SketchingWithHands: 3D Sketching Handheld Products with First-Person Hand Posture,"Yongkwan Kim, Seok-Hyung Bae","2785722, 1715434","We present SketchingWithHands, a 3D sketching system that incorporates a hand-tracking sensor. The system enables product designers to easily capture desired hand postures from a first-person point of view at any time and to use the captured hand information to explore handheld product concepts by 3D sketching while keeping the proper scale and usage of the products. Based on the analysis of design practices and drawing skills in the art and design literature, we suggest novel ideas for efficiently acquiring hand postures (palm-pinning widget, front and center mirrors, responsive spangles), for quickly creating and easily adjusting sketch planes (modified tick-triggered, orientable and shiftable sketch planes), for appropriately starting 3D sketching products with hand information (hand skeleton, grip axis), and for practically increasing user throughput (intensifier, rough and precise erasers)---all of which are coherently and consistently integrated in our system. A user test by ten industrial design students and an in-depth discussion show that our system is both useful and usable in designing handheld products.",0,44.6540880503
UIST,469a9ae8d96d26dc70e5b151a5589fb185305e7e,UIST,2012,Capturing indoor scenes with smartphones,"Aditya Sankar, Steven M. Seitz","3246173, 1679223","In this paper, we present a novel smartphone application designed to easily capture, visualize and reconstruct homes, offices and other indoor scenes. Our application leverages data from smartphone sensors such as the camera, accelerometer, gyroscope and magnetometer to help model the indoor scene. The output of the system is two-fold; first, an interactive visual tour of the scene is generated in real time that allows the user to explore each room and transition between connected rooms. Second, with some basic interactive photogrammetric modeling the system generates a 2D floor plan and accompanying 3D model of the scene, under a Manhattan-world assumption. The approach does not require any specialized equipment or training and is able to produce accurate floor plans.",18,72.5490196078
UIST,664a5a724c2a31ca508fb48cc6e31ae27f6d8313,UIST,2004,Who cares?: reflecting who is reading what on distributed community bulletin boards,"Toshiya Yamada, Jun Shingu, Elizabeth F. Churchill, Les Nelson, Jonathan Helfman, Paul Murphy","8222477, 2565333, 1801572, 2004203, 1798257, 7738470","In this paper, we describe the YeTi information sharing system that has been designed to foster community building through informal digital content sharing. The YeTi system is a general information parsing, hosting and distribution infrastructure, with interfaces designed for individual and public content reading. In this paper we describe the YeTi public display interface, with a particular focus on tools we have designed to provide lightweight awareness of others' interactions with posted content. Our tools augment content with metadata that reflect people's reading of content - captured video clips of who's reading and interacting with content, tools to allow people to leave explicit freehand annotations about content, and a visualization of the content access history to show when content is interacted with. Results from an initial evaluation are presented and discussed.",9,15.7894736842
UIST,9dab343fe90ecea2e4055ee76bd7bd4a8c843f02,UIST,2005,Moveable interactive projected displays using projector based tracking,"Johnny C. Lee, Scott E. Hudson, Jay Summet, Paul H. Dietz","1803308, 1749296, 2585396, 1805795","Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.",55,64.5161290323
UIST,9def1fd2e400ffca86f387bd27cc4c445779783a,UIST,2013,Capturing on site laser annotations with smartphones to document construction work,"Jörg Schweitzer, Ralf Dörner","2039730, 8360122","In the process of construction work, taking notes of real world objects like walls, pipes, cables and others is an important task. The ad hoc capturing of small information pieces on such objects on site can be challenging when there is no specialized technology available. Handwritten or hand drawn notes on paper are good for textual information like measurements whereas images are better to capture the physical state of objects. Without a proper combination however the benefit is limited. In this paper we present an interaction system for taking ad hoc notes on real world objects by using a combination of a smartphone and a laserpointer as input device. Our interface enables the user to directly annotate objects by drawing on them and to store these annotations for later reviewing. The deictic gestures of the user are then replayed on a stitched image of the scene. The users voice input is captured and analyzed to integrate additional Information. The user can mark positions and place hand taken measurements by pointing on the objects and speaking the corresponding voice commands.",0,10.5504587156
UIST,0e5c8779b7e043cb797ea396b54e769849e59f44,UIST,2014,LightWeight: wearable resistance visualizer for rehabilitation,"Zane Cochran, Brianna Tomlinson, Dar-Wei Chen, Kunal Patel","2516650, 2331752, 1957069, 4498748","People recovering from arm injuries are often prescribed limits to the amount of strain they can place on their muscles at a given point during the recovery process. However, it is sometimes difficult for them to know when a given activity creates strain in excess of these limits. To inform this process, we have developed a prototype, the LightWeight, and describe it here. The aim of the LightWeight is to inform users of the strain on targeted muscles as the activity occurs, and to display the relationship of that strain to the aforementioned limits. LightWeight is embedded within a compression sleeve that measures muscle strain through conductive fabric and EMG while displaying that information through an intuitive circular LED display.",0,12.015503876
UIST,d78b2ba2cb7b195f9bbb0486e64a353ee4e78333,UIST,2011,Scopemate: a tracking inspection microscope,"Cati N. Boulanger, Paul H. Dietz, Steven Bathiche","2230970, 1805795, 2275540",We propose a new interaction mechanism for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with ap-propriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.,0,6.19047619048
UIST,4df2ce1836ddd2ba38a71e059bd2aa87c9bc6dfc,UIST,2007,Hybrid infrared and visible light projection for location tracking,"Johnny C. Lee, Scott E. Hudson, Paul H. Dietz","1803308, 1749296, 1805795","A number of projects within the computer graphics, computer vision, and human-computer interaction communities have recognized the value of using projected structured light patterns for the purposes of doing range finding, location dependent data delivery, projector adaptation, or object discovery and tracking. However, most of the work exploring these concepts has relied on visible structured light patterns resulting in a caustic visual experience. In this work, we present the first design and implementation of a high-resolution, scalable, general purpose invisible near-infrared projector that can be manufactured in a practical manner. This approach is compatible with simultaneous visible light projection and integrates well with future Digital Light Processing (DLP) projector designs -- the most common type of projectors today. By unifying both the visible and non-visible pattern projection into a single device, we can greatly simply the implementation and execution of interactive projection systems. Additionally, we can inherently provide location discovery and tracking capabilities that are unattainable using other approaches.",22,31.9444444444
UIST,020036c75de8d47c3a2d5a96defce24e1f24d48e,UIST,2009,A practical pressure sensitive computer keyboard,"Paul H. Dietz, Benjamin D. Eidelson, Jonathan Westhues, Steven Bathiche","1805795, 1845095, 2711385, 2275540","A pressure sensitive computer keyboard is presented that independently senses the force level on every depressed key. The design leverages existing membrane technologies and is suitable for low-cost, high-volume manufacturing. A number of representative applications are discussed.",21,38.5714285714
UIST,f451c8a16ef4e3637448d085684f327c8755cf10,UIST,2011,Scopemate: a robotic microscope,"Cati N. Boulanger, Paul H. Dietz, Steven Bathiche","2230970, 1805795, 2275540",Scopemate is a robotic microscope that tracks the user for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with appropriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.,0,6.19047619048
UIST,f3a7ed47d485612b5481111e549af59792ec4ab1,UIST,2011,PocketTouch: through-fabric capacitive touch input,"T. Scott Saponas, Chris Harrison, Hrvoje Benko","1766388, 1730920, 2704133","PocketTouch is a capacitive sensing prototype that enables eyes-free multitouch input on a handheld device without having to remove the device from the pocket of one's pants, shirt, bag, or purse. PocketTouch enables a rich set of gesture interactions, ranging from simple touch strokes to full alphanumeric text entry. Our prototype device consists of a custom multitouch capacitive sensor mounted on the back of a smartphone. Similar capabilities could be enabled on most existing capacitive touchscreens through low-level access to the capacitive sensor. We demonstrate how touch strokes can be used to initialize the device for interaction and how strokes can be processed to enable text recognition of characters written over the same physical area. We also contribute a comparative study that empirically measures how different fabrics attenuate touch inputs, providing insight for future investigations. Our results suggest that PocketTouch will work reliably with a wide variety of fabrics used in today's garments, and is a viable input method for quick eyes-free operation of devices in pockets.",30,75.7142857143
UIST,d9d0fea2673e8a86478c065321690126b0932e33,UIST,2008,Browsing large HTML tables on small screens,"Keishi Tajima, Kaori Ohnishi","2792621, 2091514","We propose new interaction techniques that support better browsing of large HTML tables on small screen devices, such as mobile phones. We propose three modes for browsing tables: normal mode, record mode, and cell mode. Normal mode renders tables in the ordinary way, but provides various useful functions for browsing large tables, such as hiding unnecessary rows and columns. Record mode regards each row (or column) as the basic information unit and displays it in a record-like format with column (or row) headers, while cell mode regards each cell as the basic unit and displays each cell together with its corresponding row and column headers. For these table presentations, we need to identify row and column headers that explain the meaning of rows and columns. To provide users with both row and column headers even when the tables have attributes for only one of them, we introduce the concept of keys and develop a method of automatically discovering attributes and keys in tables. Another issue in these presentations is how to handle composite cells spanning multiple rows or columns. We determine the semantics of such composite cells and render them in appropriate ways in accordance with their semantics.",10,21.4285714286
UIST,2e4fec5e2c93627e55dff696f3178b87c7ba6b0f,UIST,2012,GamiCAD: a gamified tutorial system for first time autocad users,"Wei Li, Tovi Grossman, George W. Fitzmaurice","2121690, 3313809, 1703735","We present GamiCAD, a gamified in-product, interactive tutorial system for first time AutoCAD users. We introduce a software event driven finite state machine to model a user's progress through a tutorial, which allows the system to provide real-time feedback and recognize success and failures. GamiCAD provides extensive real-time visual and audio feedback that has not been explored before in the context of software tutorials. We perform an empirical evaluation of GamiCAD, comparing it to an equivalent in-product tutorial system without the gamified components. In an evaluation, users using the gamified system reported higher subjective engagement levels and performed a set of testing tasks faster with a higher completion ratio.",32,87.2549019608
UIST,481e8ce3cf15532fd0cc550a1856c0c991285f77,UIST,2012,Videoink: a pen-based approach for video editing,"Diogo Cabral, Nuno Correia","2815588, 1717306","Due the growth of video sharing, its manipulation is important, however still a hard task. In order to improve it, this work proposes a pen-based approach, called VideoInk. The concept exploits the painting metaphor, replacing digital ink with video frames. The method allows the user to paint video content in a canvas, which works as a two dimensional timeline. This approach includes transition effects and zoom features based on pen pressure. A Tablet PC prototype implementing the concept was also developed.",0,9.80392156863
UIST,20c2cb80e28197d29e65fd6662b2b416938694a9,UIST,2005,DT controls: adding identity to physical interfaces,"Paul H. Dietz, Bret Harsham, Clifton Forlines, Darren Leigh, William S. Yerazunis, Sam Shipman, Bent Schmidt-Nielsen, Kathy Ryall","1805795, 2293993, 1694854, 1786108, 5135329, 2673643, 3222918, 1712855","In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.",6,12.9032258065
UIST,686a9217272d4a28235d4b2bd4fc4f0b65ec4ef7,UIST,2013,A cuttable multi-touch sensor,"Simon Olberding, Nan-Wei Gong, John Tiab, Joseph A. Paradiso, Jürgen Steimle","3186111, 1690147, 2638321, 4798651, 1790324","We propose cutting as a novel paradigm for ad-hoc customization of printed electronic components. As a first instantiation, we contribute a printed capacitive multi-touch sensor, which can be cut by the end-user to modify its size and shape. This very direct manipulation allows the end-user to easily make real-world objects and surfaces touch-interactive, to augment physical prototypes and to enhance paper craft. We contribute a set of technical principles for the design of printable circuitry that makes the sensor more robust against cuts, damages and removed areas. This includes novel physical topologies and printed forward error correction. A technical evaluation compares different topologies and shows that the sensor remains functional when cut to a different shape.",25,88.9908256881
UIST,06a6f38f6d5d61356379dd00d554c588275eab9d,UIST,2002,The actuated workbench: computer-controlled actuation in tabletop tangible interfaces,"Gian Pangaro, Dan Maynes-Aminzade, Hiroshi Ishii","2745692, 2245650, 1749649","The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, providing an additional feedback loop for computer output, and helping to resolve inconsistencies that otherwise arise from the computer's inability to move objects on the table. We describe the Actuated Workbench in detail as an enabling technology, and then propose several applications in which this technology could be useful.",93,83.3333333333
UIST,d0e8f1d24e0f2d2efff83bd4f835a1e102111754,UIST,2014,Individual variation in susceptibility to cybersickness,"Lisa Rebenitsch, Charles B. Owen","3070984, 1787354","We examined background characteristics of virtual reality participants in order to determine correlations to cybersickness. As 3D media and new VR display technologies from companies such as Occulus and Sony become more popular, the incidence of cybersickness is likely to increase. Understanding the impact of individual backgrounds on susceptibility can help shed light on which individuals are more likely to be impacted. Past history of motion sickness and video game play have the best predictive power of cybersickness of the factors studied. A model to estimate the likelihood of cybersickness using background characteristics is posed.",0,12.015503876
UIST,6d0560d431f2f8a5cb3b698d545f8c8c8a7e9a9d,UIST,2015,Tracko: Ad-hoc Mobile 3D Tracking Using Bluetooth Low Energy and Inaudible Signals for Cross-Device Interaction,"Haojian Jin, Christian Holz, Kasper Hornbæk","2340572, 2794828, 1679367","While current mobile devices detect the presence of surrounding devices, they lack a truly <i>spatial</i> awareness to bring them into the user's natural 3D space. We present <i>Tracko</i>, a 3D tracking system between two or more commodity devices without added components or device synchronization. Tracko achieves this by fusing three signal types. 1) Tracko infers the <i>presence</i> of and rough distance to other devices from the strength of Bluetooth low energy signals. 2) Tracko exchanges a series of inaudible stereo sounds and derives a set of accurate distances between devices from the difference in their arrival times. A Kalman filter integrates both signal cues to place collocated devices in a shared <i>3D space</i>, combining the robustness of Bluetooth with the accuracy of audio signals for relative 3D tracking. 3) Tracko incorporates inertial sensors to refine 3D estimates and support quick interactions. Tracko robustly tracks devices in 3D with a mean error of 6.5 cm within 0.5 m and a 15.3 cm error within 1 m, which validates Trackoffs suitability for cross-device interactions.",2,60.5263157895
UIST,7ac6b9df1dadc0d8b5dc5df0badc19ca16411cb0,UIST,2010,OnObject: gestural play with tagged everyday objects,"Keywon Chung, Michael Shilman, Chris Merrill, Hiroshi Ishii","1811557, 2139615, 2694597, 1749649","Many Tangible User Interface (TUI) systems employ sensor-equipped physical objects. However they do not easily scale to users' actual environments; most everyday objects lack the necessary hardware, and modification requires hardware and software development by skilled individuals. This limits TUI creation by end users, resulting in inflexible interfaces in which the mapping of sensor input and output events cannot be easily modified reflecting the end user's wishes and circumstances. We introduce OnObject, a small device worn on the hand, which can program physical objects to respond to a set of gestural triggers. Users attach RFID tags to situated objects, grab them by the tag, and program their responses to grab, release, shake, swing, and thrust gestures using a built-in button and a microphone. In this paper, we demonstrate how novice end users including preschool children can instantly create engaging gestural object interfaces with sound feedback from toys, drawings, or clay.",12,66.8604651163
UIST,1fc4bf9167223e0a0349fcc9119395278681404d,UIST,2010,Interactive calibration of a multi-projector system in a video-wall multi-touch environment,"Alessandro Lai, Alessandro Soro, Riccardo Scateni","2705070, 2181723, 3204853","Wall-sized interactive displays gain more and more attention as a valuable tool for multiuser applications, but typically require the adoption of projectors tiles. Projectors tend to display deformed images, due to lens distortion and/or imperfection, and because they are almost never perfectly aligned to the projection surface. Multi-projector video-walls are typically bounded to the video architecture and to the specific application to be displayed. This makes it harder to develop interactive applications, in which a fine grained control of the coordinate transformations (to and from user space and model space) is required. This paper presents a solution to such issues: implementing the blending functionalities at an application level allows seamless development of multi-display interactive applications with multi-touch capabilities. The description of the multi-touch interaction, guaranteed by an array of cameras on the baseline of the wall, is beyond the scope of this work which focuses on calibration.",2,33.1395348837
UIST,59dfbd81f4886c6f148c6055d4a098d667e3414c,UIST,2012,Machine learning models for uncertain interaction,Daryl Weir,1966315,"As interaction methods beyond the static mouse and keyboard setup of the desktop era - such as touch, gesture sensing, and visual tracking - become more common, existing interaction paradigms are no longer good enough. These new modalities have high uncertainty, and conventional interfaces are not designed to reflect this. Research has shown that modelling uncertainty can improve the quality of interaction with these systems. Machine learning offers a rich set of tools to make probabilistic inferences in uncertain systems - this is the focus of my thesis work. In particular, I'm interested in making inferences at the sensor level and propagating uncertainty forward appropriately to applications. In this paper I describe a probabilistic model for touch interaction, and discuss how I intend to use the uncertainty in this model to improve typing accuracy on a soft keyboard. The model described here lays the groundwork for a rich framework for interaction in the presence of uncertainty, incorporating data from multiple sensors to make more accurate inferences about the goals of users, and allowing systems to adapt smoothly and appropriately to their context of use.",1,25.0
UIST,78c74cafc0c9e84d28ebb743f67bcecad402b5b1,UIST,2011,Searching for software learning resources using application context,"Michael D. Ekstrand, Wei Li, Tovi Grossman, Justin Matejka, George W. Fitzmaurice","2386667, 2121690, 3313809, 2578065, 1703735","Users of complex software applications frequently need to consult documentation, tutorials, and support resources to learn how to use the software and further their understand-ing of its capabilities. Existing online help systems provide limited context awareness through ""what's this?"" and simi-lar techniques. We examine the possibility of making more use of the user's current context in a particular application to provide useful help resources. We provide an analysis and taxonomy of various aspects of application context and how they may be used in retrieving software help artifacts with web browsers, present the design of a context-aware augmented web search system, and describe a prototype implementation and initial user study of this system. We conclude with a discussion of open issues and an agenda for further research.",18,65.2380952381
UIST,c46bd9dd8cfd22586716484ac2097b7dc288acb6,UIST,2016,ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces,"Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A. Paradiso, Hiroshi Ishii","2333636, 2103349, 2770912, 4798651, 1749649","This paper presents ChainFORM: a linear, modular, actuated hardware system as a novel type of shape changing interface. Using rich sensing and actuation capability, this modular hardware system allows users to construct and customize a wide range of interactive applications. Inspired by modular and serpentine robotics, our prototype comprises identical modules that connect in a chain. Modules are equipped with rich input and output capability: touch detection on multiple surfaces, angular detection, visual output, and motor actuation. Each module includes a servo motor wrapped with a flexible circuit board with an embedded microcontroller.
 Leveraging the modular functionality, we introduce novel interaction capability with shape changing interfaces, such as rearranging the shape/configuration and attaching to passive objects and bodies. To demonstrate the capability and interaction design space of ChainFORM, we implemented a variety of applications for both computer interfaces and hands-on prototyping tools.",0,44.6540880503
UIST,727cb39480a2666b1c16b1a58e00cd9cde6d4f20,UIST,2003,Tactile interfaces for small touch screens,"Ivan Poupyrev, Shigeaki Maruyama","1736819, 2814991","We present the design, implementation, and informal evaluation of tactile interfaces for small touch screens used in mobile devices. We embedded a tactile apparatus in a Sony PDA touch screen and enhanced its basic GUI elements with tactile feedback. Instead of <i>observing</i> the response of interface controls, users can <i>feel</i> it with their fingers as they press the screen. In informal evaluations, tactile feedback was greeted with enthusiasm. We believe that tactile feedback will become the next step in touch screen interface design and a standard feature of future mobile devices.",98,75.0
UIST,97bd196a036b0d054f44295600be2a0db77eea1e,UIST,2016,Changing the Appearance of Physical Interfaces Through Controlled Transparency,"David Lindlbauer, Jörg Müller, Marc Alexa","2287283, 1798163, 1751554","We present physical interfaces that change their appearance through controlled transparency. These <i>transparency-controlled physical interfaces</i> are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.",0,44.6540880503
UIST,3b59122199f37f1bd8090dafffde14d2c4bb184b,UIST,2002,Ambient touch: designing tactile interfaces for handheld devices,"Ivan Poupyrev, Shigeaki Maruyama, Jun Rekimoto","1736819, 2814991, 1685962","This paper investigates the sense of touch as a channel for communicating with miniature handheld devices. We embedded a PDA with a TouchEngine<sup>TM</sup> --- a thin, miniature lower-power tactile actuator that we have designed specifically to use in mobile interfaces (Figure 1). Unlike previous tactile actuators, the TouchEngine is a universal tactile display that can produce a wide variety of tactile feelings from simple clicks to complex vibrotactile patterns. Using the TouchEngine, we began exploring the design space of interactive tactile feedback for handheld computers. Here, we investigated only a subset of this space: using touch as the ambient, background channel of interaction. We proposed a general approach to design such tactile interfaces and described several implemented prototypes. Finally, our user studies demonstrated 22% faster task completion when we enhanced handheld tilting interfaces with tactile feedback.",140,95.8333333333
UIST,a33cac1c4a73874fd8703dc0430738a7f4e4c79b,UIST,2013,Pixel-based reverse engineering of graphical interfaces,Morgan Dixon,1743901,"My dissertation proposes a vision in which anybody can modify any interface of any application. Realizing this vision is difficult because of the rigidity and fragmentation of current interfaces. Specifically, rigidity makes it difficult or impossible for a designer to modify or customize existing interfaces. Fragmentation results from the fact that people generally use many different applications built with a variety of toolkits. Each is implemented differently, so it is difficult to consistently add new functionality. As a result, researchers are often limited to demonstrating new ideas in small testbeds, and practitioners often find it difficult to adopt and deploy ideas from the literature. In my dissertation, I propose transcending the rigidity and fragmentation of modern interfaces by building upon their single largest commonality: that they ultimately consist of pixels painted to a display. Building from this universal representation, I propose pixel-based interpretation to enable modification of interfaces without their source code and independent of their underlying toolkit implementation.",0,10.5504587156
UIST,cf783643f2287bb506ce17c1551cb82584b35f68,UIST,2014,Slack-scroll: sharing sliding operations among scrolling and other GUI functions,"Eunhye Youn, Geehyuk Lee","3036795, 1717371","Sliding is one of the basic touchscreen operations, but is mainly used for scrolling in mobile touchscreen GUIs. As a way to share sliding operations among scrolling and other GUI functions, we propose Slack-Scroll. We implemented two application scenarios of Slack-Scroll, and asserted their feasibility in a user study. All participants could accept and adapt well to the new techniques enabled by Slack-Scroll.",0,12.015503876
UIST,bc803c0f8fb92a15798429ffcc912600e8d30ebf,UIST,2014,Programming by manipulation for layout,"Thibaud Hottelier, Rastislav Bodík, Kimiko Ryokai","1832541, 1991345, 2455971","We present Programming by Manipulation, a new programming methodology for specifying the layout of data visualizations, targeted at non-programmers. We address the two central sources of bugs that arise when programming with constraints: ambiguities and conflicts (inconsistencies). We rule out conflicts by design and exploit ambiguity to explore possible layout designs. Our users design layouts by highlighting undesirable aspects of a current design, effectively breaking spurious constraints and introducing ambiguity by giving some elements freedom to move or resize. Subsequently, the tool indicates how the ambiguity can be removed, by computing how the free elements can be fixed with available constraints. To support this workflow, our tool computes the ambiguity and summarizes it visually. We evaluate our work with two user-studies demonstrating that both non-programmers and programmers can effectively use our prototype. Our results suggest that our tool is 5-times more productive than direct programming with constraints.",8,75.9689922481
UIST,a323743efba8597bb08dbf6576099f63b005e87a,UIST,2015,"LineFORM: Actuated Curve Interfaces for Display, Interaction, and Constraint","Ken Nakagaki, Sean Follmer, Hiroshi Ishii","2333636, 2770912, 1749649","In this paper we explore the design space of actuated curve interfaces, a novel class of shape changing-interfaces. Physical curves have several interesting characteristics from the perspective of interaction design: they have a variety of inherent affordances; they can easily represent abstract data; and they can act as constraints, boundaries, or borderlines. By utilizing such aspects of lines and curves, together with the added capability of shape-change, new possibilities for display, interaction and body constraint are possible. In order to investigate these possibilities we have implemented two actuated curve interfaces at different scales. LineFORM, our implementation, inspired by serpentine robotics, is comprised of a series chain of 1DOF servo motors with integrated sensors for direct manipulation. To motivate this work we present various applications such as shape changing cords, mobiles, body constraints, and data manipulation tools.",5,80.701754386
UIST,080b1c3b721c4e3094d3036d59e08c0c422d5f43,UIST,2008,Tapping and rubbing: exploring new dimensions of tactile feedback with voice coil motors,"Kevin A. Li, Patrick Baudisch, William G. Griswold, James D. Hollan","2463991, 1729393, 6980007, 1698170","Tactile feedback allows devices to communicate with users when visual and auditory feedback are inappropriate. Unfortunately, current vibrotactile feedback is abstract and not related to the content of the message. This often clash-es with the nature of the message, for example, when sending a comforting message.
 We propose addressing this by extending the repertoire of haptic notifications. By moving an actuator perpendicular to the user's skin, our prototype device can tap the user. Moving the actuator parallel to the user's skin induces rub-bing. Unlike traditional vibrotactile feedback, tapping and rubbing convey a distinct emotional message, similar to those induced by human-human touch.
 To enable these techniques we built a device we call soundTouch. It translates audio wave files into lateral motion using a voice coil motor found in computer hard drives. SoundTouch can produce motion from below 1Hz to above 10kHz with high precision and fidelity.
 We present the results of two exploratory studies. We found that participants were able to distinguish a range of taps and rubs. Our findings also indicate that tapping and rubbing are perceived as being similar to touch interactions exchanged by humans.",14,31.4285714286
UIST,8e3391ae184b36be991a3a512aa5eea472a70daf,UIST,2006,Brain-computer interaction,José del R. Millán,1716694,"The promise of Brain-Computer Interfaces (BCI) technology is to augment human capabilities by enabling people to interact with a computer through a conscious and spontaneous modulation of their brainwaves after a short training period. Indeed, by analyzing brain electrical activity online, several groups have designed brain-actuated systems that provide alternative channels for communication, entertainment and control. Thus, a person can write messages using a virtual keyboard on a computer screen and also browse the internet. Alternatively, subjects can operate simple computer games, or brain games, and interact with educational software. Researchers have also been able to train monkeys to move a computer cursor to desired targets and also to control a robot arm. Work with humans has shown that it is possible for them to move a cursor and even to drive a mobile robot between rooms in a house model. In this talk I will review the field of BCI, with a focus on non-invasive systems based on electroencephalogram (EEG) signals. I will also describe three brain-actuated applications we have developed: a virtual keyboard, a brain game, and a mobile robot (emulating a motorized wheelchair). Finally, we discuss current research directions we are pursuing in order to improve the performance and robustness of our BCI system, especially for real-time control of brain-actuated robots.",0,2.5
UIST,7581468f81b0df3ceaafacb3a581f8e23a70061c,UIST,2016,MagTacS: Delivering Tactile Sensation over an Object,"Hyung-Sik Kim, Seong-Young Gim, Woo-Ram Kim, Mi-Hyun Choi, Seungmoon Choi, Soon-Cheol Chung","3274367, 3492776, 7891448, 8486443, 1718126, 2345586","A system that can deliver tactile sensation despite an object existing between an actuator and human was developed. This system composed of a control part, power part, output part, and coil. The control part controls the overall system using a microcontroller. The power part generates electric current to create a magnetic field. The output part delivers high energies to the coil. The coil generates a time-varying magnetic field to induce current flow within the body. Through the tactile sensation recognition test, delivery of tactile sensation was confirmed in the air even an object existed between actuator and human skin.",0,44.6540880503
UIST,216777da15b1b309d3e16d24586b197923450859,UIST,2012,Follow-me!: conducting a virtual concert,"Seungju Han, Jung-Bae Kim, James D. K. Kim","2423429, 3074617, 1680170","In this paper, we present a real-time continuous gesture recognition system for conducting a virtual concert. Our systems allow the user control over beat, by conducting four different beat-pattern gestures; tempo, by making faster or slower gestures; volume, by making larger or smaller gestures; and instrument emphasis, by directing the gestures towards specific areas of the orchestra on a large display. A recognition accuracy of up to 95% could be achieved for the conducting gestures (beat, tempo, and volume).",2,33.8235294118
UIST,7e40823e2254575ae5aed23d5b497b9ef0ccd6ff,UIST,2014,Video lens: rapid playback and exploration of large video collections and associated metadata,"Justin Matejka, Tovi Grossman, George W. Fitzmaurice","2578065, 3313809, 1703735","We present Video Lens, a framework which allows users to visualize and interactively explore large collections of videos and associated metadata. The primary goal of the framework is to let users quickly find relevant sections within the videos and play them back in rapid succession. The individual UI elements are linked and highly interactive, supporting a faceted search paradigm and encouraging exploration of the data set. We demonstrate the capabilities and specific scenarios of Video Lens within the domain of professional baseball videos. A user study with 12 participants indicates that Video Lens efficiently supports a diverse range of powerful yet desirable video query tasks, while a series of interviews with professionals in the field demonstrates the framework's benefits and future potential.",3,49.2248062016
UIST,0d55ce3fef1afc5b95973cc794c72256ea37bd0c,UIST,2008,Towards more paper-like input: flexible input devices for foldable interaction styles,"David T. Gallant, Andrew G. Seniuk, Roel Vertegaal","2592810, 2573865, 1687608","This paper presents Foldable User Interfaces (FUI), a combination of a 3D GUI with windows imbued with the physics of paper, and Foldable Input Devices (FIDs). FIDs are sheets of paper that allow realistic transformations of graphical sheets in the FUI. Foldable input devices are made out of construction paper augmented with IR reflectors, and tracked by computer vision. Window sheets can be picked up and flexed with simple movements and deformations of the FID. FIDs allow a diverse lexicon of one-handed and two-handed interaction techniques, including folding, bending, flipping and stacking. We show how these can be used to ease the creation of simple 3D models, but also for tasks such as page navigation.",36,68.5714285714
UIST,b7c055a39d863292a5cbdcebd18e03c30b67ce2b,UIST,2016,A 3D Printer for Interactive Electromagnetic Devices,"Huaishu Peng, François Guimbretière, James McCann, Scott E. Hudson","1817818, 2539134, 2400939, 1749296","We introduce a new form of low-cost 3D printer to print interactive electromechanical objects with wound in place coils. At the heart of this printer is a mechanism for depositing wire within a five degree of freedom (5DOF) fused deposition modeling (FDM) 3D printer. Copper wire can be used with this mechanism to form coils which induce magnetic fields as a current is passed through them. Soft iron wire can additionally be used to form components with high magnetic permeability which are thus able to shape and direct these magnetic fields to where they are needed. When fabricated with structural plastic elements, this allows simple but complete custom electromagnetic devices to be 3D printed. As examples, we demonstrate the fabrication of a solenoid actuator for the arm of a Lucky Cat figurine, a 6-pole motor stepper stator, a reluctance motor rotor and a Ferrofluid display. In addition, we show how printed coils which generate small currents in response to user actions can be used as input sensors in interactive devices.",0,44.6540880503
UIST,bcc84f40dd21ad60957dc7077171ed72ccc722e3,UIST,2015,Joint 5D Pen Input for Light Field Displays,"James Tompkin, Samuel Muff, James McCann, Hanspeter Pfister, Jan Kautz, Marc Alexa, Wojciech Matusik","1854493, 3071646, 2400939, 1701371, 1690538, 1751554, 1752521","Light field displays allow viewers to see view-dependent 3D content as if looking through a window; however, existing work on light field display interaction is limited. Yet, they have the potential to parallel 2D pen and touch screen systems, which present a joint input and display surface for natural interaction. We propose a 4D display and interaction space using a dual-purpose lenslet array, which combines light field display and light field pen sensing, and allows us to estimate the 3D position and 2D orientation of the pen. This method is simple, fast (150Hz), with position accuracy of 2-3mm and precision of 0.2-0.6mm from 0-350mm away from the lenslet array, and orientation accuracy of 2 degrees and precision of 0.2-0.3 degrees within a 45 degree field of view. Further, we 3D print the lenslet array with embedded baffles to reduce out-of-bounds cross-talk, and use an optical relay to allow interaction behind the focal plane. We demonstrate our joint display/sensing system with interactive light field painting.",1,42.1052631579
UIST,0a5537a8a5925e11604a46cde076bf81bb12667c,UIST,2015,Webstrates: Shareable Dynamic Media,"Clemens Nylandsted Klokmose, James R. Eagan, Siemen Baader, Wendy E. Mackay, Michel Beaudouin-Lafon","3027683, 2152856, 2200151, 1732917, 1682346","We revisit Alan Kay's early vision of dynamic media that blurs the distinction between documents and applications. We introduce <i>shareable</i> dynamic media that are <i>malleable</i> by users, who may appropriate them in idiosyncratic ways; shareable among users, who collaborate on multiple aspects of the media; and <i>distributable</i> across diverse devices and platforms. We present <i>Webstrates</i>, an environment for exploring shareable dynamic media. <i>Webstrates</i> augment web technology with real-time sharing. They turn web pages into substrates, i.e. software entities that act as applications or documents depending upon use. We illustrate <i>Webstrates</i> with two implemented case studies: users collaboratively author an article with functionally and visually different editors that they can personalize and extend at run-time; and they orchestrate its presentation and audience participation with multiple devices. We demonstrate the simplicity and generative power of <i>Webstrates</i> with three additional prototypes and evaluate it from a systems perspective.",8,90.350877193
UIST,73162ffb39fec419ad2902c3d2910504adde4955,UIST,2012,Medical operating documents: dynamic checklists improve crisis attention,Leslie Wu,2524633,"The attentional aspects of crisis computing - supporting highly trained teams as they respond to real-life emergencies - have been underexplored in the user interface community. My research investigates the development of interactive software systems that support crisis teams, with an eye towards intelligently managing attention. In this paper, I briefly describe MDOCS, a Medical operating DOCuments System built for time-critical interaction. MDOCS is a multi-user, multi-surface software system that implements dynamic checklists and interactive cognitive aids written to support medical crisis teams. I present the results of a study that evaluates the deployment of MDOCS in a realistic, mannequin-based medical simulator used by anesthesiologists. I propose controlled laboratory experiments that evaluate the feasibility and effectiveness of our design principles and attentional interaction techniques.",0,9.80392156863
UIST,aef1dc28e6ba776a0e9d5460906c234e38c13b61,UIST,2012,Point and share: from paper to whiteboard,"Misha Sra, Austin Lee, Sheng-Ying Pao, Gonglue Jiang, Hiroshii Ishii","3024298, 3014598, 3319261, 2846455, 2773059","Traditional writing instruments have the potential to enable new forms of interactions and collaboration though digital enhancement. This work specifically enables the user to utilize pen and paper as input mechanisms for content to be displayed on a shared interactive whiteboard. We introduce a pen cap with an infrared led, an actuator and a switch. Pointing the pen cap at the whiteboard allows users to select and position a ""canvas"" on the whiteboard to display handwritten text while the actuator enables resizing the canvas and the text. It is conceivable that anything one can write on paper anywhere, could be displayed on an interactive whiteboard.",0,9.80392156863
UIST,81f2b992e1addacb9b7b3c57ae1b75d4239795f7,UIST,2012,UnderScore: musical underlays for audio stories,"Steve Rubin, Floraine Berthouzoz, Gautham J. Mysore, Wilmot Li, Maneesh Agrawala","3280503, 2842099, 1781063, 2812691, 1820412","Audio producers often use <i>musical underlays</i> to emphasize key moments in spoken content and give listeners time to reflect on what was said. Yet, creating such underlays is time-consuming as producers must carefully (1) mark an emphasis point in the speech (2) select music with the appropriate style, (3) align the music with the emphasis point, and (4) adjust dynamics to produce a harmonious composition. We present UnderScore, a set of semi-automated tools designed to facilitate the creation of such underlays. The producer simply marks an emphasis point in the speech and selects a music track. UnderScore automatically refines, aligns and adjusts the speech and music to generate a high-quality underlay. UnderScore allows producers to focus on the high-level design of the underlay; they can quickly try out a variety of music and test different points of emphasis in the story. Amateur producers, who may lack the time or skills necessary to author underlays, can quickly add music to their stories. An informal evaluation of UnderScore suggests that it can produce high-quality underlays for a variety of examples while significantly reducing the time and effort required of radio producers.",5,49.0196078431
UIST,2c608564071e5bcfea9ca0fb3706e253fc0c3c9f,UIST,2011,"IP-QAT: in-product questions, answers, & tips","Justin Matejka, Tovi Grossman, George W. Fitzmaurice","2578065, 3313809, 1703735","We present IP-QAT, a new community-based question and answer system for software users. Unlike most community forums, IP-QAT is integrated into the actual software application, allowing users to easily post questions, answers and tips without having to leave the application. Our in-product implementation is context-aware and shows relevant posts based on a user's recent activity. It is also designed with minimal transaction costs to encourage users to easily post, include annotated images and file attachments, as well as tag their posts with relevant UI components. We describe a robust cloud-based system implementation, which allowed us to release IP-QAT to 37 users for a 2 week field study. Our study showed that IP-QAT increased user contributions, and subjectively, users found our system more useful and easier to use, in comparison to the existing commercial discussion board.",12,51.9047619048
UIST,100cd3dff08f3a0cd387f5cb9a941c3a732760c5,UIST,2011,Sketch-sketch revolution: an engaging tutorial system for guided sketching and application learning,"Jennifer Fernquist, Tovi Grossman, George W. Fitzmaurice","2214955, 3313809, 1703735","We describe Sketch-Sketch Revolution, a new tutorial system that allows any user to experience the success of drawing content previously created by an expert artist. Sketch-Sketch Revolution not only guides users through the application user interface, it also provides assistance with the actual sketching. In addition, the system offers an authoring tool that enables artists to create content and then automatically generates a tutorial from their recorded workflow history. Sketch-Sketch Revolution is a unique hybrid tutorial system that combines in-product, content-centric and reactive tutorial methods to provide an engaging learning experience. A qualitative user study showed that our system successfully taught users how to interact with a drawing application user interface, gave users confidence they could recreate expert content, and was uniformly considered useful and easy to use.",32,78.5714285714
UIST,4e0feadf81de30bffa53f502cfd77bd2293d0670,UIST,2011,TwitApp: in-product micro-blogging for design sharing,"Wei Li, Tovi Grossman, Justin Matejka, George W. Fitzmaurice","2121690, 3313809, 2578065, 1703735","We describe TwitApp, an enhanced micro-blogging system integrated within AutoCAD for design sharing. TwitApp integrates rich content and still keeps the sharing transaction cost low. In TwitApp, tweets are organized by their project, and users can follow or unfollow each individual project. We introduce the concept of automatic tweet drafting and other novel features such as enhanced real-time search and integrated live video streaming. The TwitApp system leverages the existing Twitter micro-blogging system. We also contribute a study which provides insights on these concepts and associated designs, and demonstrates potential user excitement of such tools.",1,15.7142857143
UIST,250ed4d45c5e280843d501bb2872b06ff24ccc72,UIST,2010,"Chronicle: capture, exploration, and playback of document workflow histories","Tovi Grossman, Justin Matejka, George W. Fitzmaurice","3313809, 2578065, 1703735","We describe Chronicle, a new system that allows users to explore document workflow histories. Chronicle captures the entire video history of a graphical document, and provides links between the content and the relevant areas of the history. Users can indicate specific content of interest, and see the workflows, tools, and settings needed to reproduce the associated results, or to better understand how it was constructed to allow for informed modification. Thus, by storing the rich information regarding the document's history workflow, Chronicle makes any working document a potentially powerful learning tool. We outline some of the challenges surrounding the development of such a system, and then describe our implementation within an image editing application. A qualitative user study produced extremely encouraging results, as users unanimously found the system both useful and easy to use.",80,93.023255814
UIST,1fe7498cc9be790acae031253cabb2cb28c41aac,UIST,2009,CommunityCommands: command recommendations for software applications,"Justin Matejka, Wei Li, Tovi Grossman, George W. Fitzmaurice","2578065, 2121690, 3313809, 1703735","We explore the use of modern recommender system technology to address the problem of learning software applications. Before describing our new command recommender system, we first define relevant design considerations. We then discuss a 3 month user study we conducted with professional users to evaluate our algorithms which generated customized recommendations for each user. Analysis shows that our item-based collaborative filtering algorithm generates 2.1 times as many good suggestions as existing techniques. In addition we present a prototype user interface to ambiently present command recommendations to users, which has received promising initial user feedback.",50,74.2857142857
UIST,7c4a28e08f8b345e0b9e3c61589e4d9eeffbb596,UIST,2013,ViziCal: accurate energy expenditure prediction for playing exergames,"Miran Kim, Jeff Angermann, George Bebis, Eelke Folmer","2589791, 2845507, 1808451, 3267671","In recent years, exercise games have been criticized for not being able to engage their players into levels of physical activity that are high enough to yield health benefits. A major challenge in the design of exergames, however, is that it is difficult to assess the amount of physical activity an exergame yields due to limitations of existing techniques to assess energy expenditure of exergaming activities. With recent advances in commercial depth sensing technology to accurately track players' motions in 3D, we present a technique called Vizical that uses a non-linear regression approach to accurately predict energy expenditure in real-time. Vizical may allow for creating exergames that can report energy expenditure while playing, and whose intensity can be adjusted in real-time to stimulate larger health benefits.",1,27.0642201835
UIST,a0d438355679ed92f3b4302a7a4588df01b78b30,UIST,2012,Directed social queries with transparent user models,"Saiph Savage, Angus Graeme Forbes, Rodrigo Savage, Tobias Höllerer, Norma Elva Chávez-Rodríguez","2121569, 7574995, 2772935, 1743721, 2668133","The friend list of many social network users can be very large. This creates challenges when users seek to direct their social interactions to friends that share a particular interest. We present a self-organizing online tool that by incorporating ideas from user modeling and data visualization allows a person to quickly identify which friends best match a social query, enabling precise and efficient directed social interactions. To cover the different modalities in which our tool might be used, we introduce two different interactive visualizations. One view enables a human-in-the-loop approach for result analysis and verification, and, in a second view, location, social affiliations and ""personality"" data is incorporated, allowing the user to quickly consider different social and spatial factors when directing social queries. We report on a qualitative analysis, which indicates that transparency leads to an increased effectiveness of the system. This work contributes a novel method for exploring online friends.",0,9.80392156863
UIST,3943277f69663502f5d7e41457cdabe52d358d37,UIST,2013,Content-based tools for editing audio stories,"Steve Rubin, Floraine Berthouzoz, Gautham J. Mysore, Wilmot Li, Maneesh Agrawala","3280503, 2842099, 1781063, 2812691, 1820412","Audio stories are an engaging form of communication that combine speech and music into compelling narratives. Existing audio editing tools force story producers to manipulate speech and music tracks via tedious, low-level waveform editing. In contrast, we present a set of tools that analyze the audio content of the speech and music and thereby allow producers to work at much higher level. Our tools address several challenges in creating audio stories, including (1) navigating and editing speech, (2) selecting appropriate music for the score, and (3) editing the music to complement the speech. Key features include a transcript-based speech editing tool that automatically propagates edits in the transcript text to the corresponding speech track; a music browser that supports searching based on emotion, tempo, key, or timbral similarity to other songs; and music retargeting tools that make it easy to combine sections of music with the speech. We have used our tools to create audio stories from a variety of raw speech sources, including scripted narratives, interviews and political speeches. Informal feedback from first-time users suggests that our tools are easy to learn and greatly facilitate the process of editing raw footage into a final story.",16,79.8165137615
UIST,224b1a250b0a23ad673fbb05738279c368c023ae,UIST,2009,Disappearing mobile devices,"Tao Ni, Patrick Baudisch","2975479, 1729393","In this paper, we extrapolate the evolution of mobile devices in one specific direction, namely miniaturization. While we maintain the concept of a device that people are aware of and interact with intentionally, we envision that this concept can become small enough to allow invisible integration into arbitrary surfaces or human skin, and thus truly ubiquitous use. This outcome assumed, we investigate what technology would be most likely to provide the basis for these devices, what abilities such devices can be expected to have, and whether or not devices that size can still allow for meaningful interaction. We survey candidate technologies, drill down on gesture-based interaction, and demonstrate how it can be adapted to the desired form factors. While the resulting devices offer only the bare minimum in feedback and only the most basic interactions, we demonstrate that simple applications remain possible. We complete our exploration with two studies in which we investigate the affordance of these devices more concretely, namely marking and text entry using a gesture alphabet.",47,65.7142857143
UIST,456e009925182913bfb778d8cfc3474aed3415c5,UIST,1991,Buttons as first class objects on an X desktop,"George G. Robertson, Austin Henderson, Stuart K. Card","1699184, 7973424, 1720492","A high-level user interface toolkit, called XButtons, has been developed to support on-screen buttons as first class objects on an X window system desktop, With the toolkit, buttons can be built that connect user interactions with procedures specified as arbitrary Unix Shell scripts. As first class desktop objects, these buttons encapsulate appearance and behavior that is user tailorable. They are persistent objects and may store state relevant to the task they perform. They can also be mailed to other users electronically. In addition to being first class desktop objects, XButtons are gesture-based with multiple actions. They support other interaction styles, like the drag and drop metaphor, in addition to simple button click actions. They also may be concurrently shared among users, with changes reflected to all users of the shared buttons. This paper describes the goals of XButtons and the history of button development that led to XButtons, It also describes XButtons from the user's point of view. Finally, it discusses some implementation issues encountered in building XBut-tons on top of the X window system. 1 Introduction Physical buttons have been around since the first electrical devices were built. They are so common that we never think about them; push a button and some action will take place. On-screen buttons in one form or another have been around since the mid-1960's, Their appeal as a human computer interaction technique is obvious; arbitrary actions can be invoked by a simple Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. interaction with a display object that looks pressable, and the style of interaction is familiar to everyone. It is no surprise that many computer systems use on-screen buttons as part of their interface. On the other hand, very few systems provide buttons that stand on their own ('%rst class objects "") or that allow an end-user to create and adapt buttons for their own needs. This kind of user tailorable button is what this paper focuses on. In this paper, we define a class of on-screen buttons that are easy for a user to …",9,39.1304347826
UIST,34511a0924064f12e028adf37b8e4702d69205c9,UIST,2016,Flying User Interface,Pramod Verma,3913855,"This paper describes a special type of drone called ""Flying User Interface"", comprised of a robotic projector-camera system, an onboard digital computer connected with the Internet, sensors, and a hardware interface capable of sticking to any surface such as wall, ceilings, etc. Computer further consists of other subsystems, devices, and sensors such as accelerometer, compass, gyroscope, flashlight, etc. Drone flies from one place to another, detects a surface, and attaches itself to it. After a successful attachment, the device stops all its rotators; it then projects or augments images, information, and user interfaces on nearby surfaces and walls. User interface may contain applications, information about object being augmented and information from Internet. User can interact with user-interface using commands and gestures such as hand, body, feet, voice, etc.",0,44.6540880503
UIST,0d50a88a1fed349d71dbf590bde6cbfbd984c8e3,UIST,2012,Dynamic tactile guidance for visual search tasks,"Ville Lehtinen, Antti Oulasvirta, Antti Salovaara, Petteri Nurmi","2124672, 2663734, 3266610, 2583077","Visual search in large real-world scenes is both time consuming and frustrating, because the search becomes serial when items are visually similar. Tactile guidance techniques can facilitate search by allowing visual attention to focus on a subregion of the scene. We present a technique for <i>dynamic tactile cueing</i> that couples hand position with a scene position and uses tactile feedback to guide the hand actively toward the target. We demonstrate substantial improvements in task performance over a baseline of visual search only, when the scene's complexity increases. Analyzing task performance, we demonstrate that the effect of visual complexity can be practically eliminated through improved spatial precision of the guidance.",14,64.2156862745
UIST,a8ae4676285a6bb06263e231b94b59c4d73fb4de,UIST,2013,Fiberio: a touchscreen that senses fingerprints,"Christian Holz, Patrick Baudisch","2794828, 1729393","We present Fiberio, a rear-projected multitouch table that identifies users biometrically based on their fingerprints during each touch interaction. Fiberio accomplishes this using a new type of screen material: a large fiber optic plate. The plate diffuses light on transmission, thereby allowing it to act as projection surface. At the same time, the plate reflects light specularly, which produces the contrast required for fingerprint sensing. In addition to offering all the functionality known from traditional diffused illumination systems, Fiberio is the first interactive tabletop system that authenticates users during touch interaction-unobtrusively and securely using the biometric features of fingerprints, which eliminates the need for users to carry any identification tokens.",26,90.3669724771
UIST,0b6a411f5b0058a7f17623fac567513bceb03cc2,UIST,2012,Interactive construction: interactive fabrication of functional mechanical devices,"Stefanie Müller, Pedro Lopes, Patrick Baudisch","4942906, 2816776, 1729393","Personal fabrication tools, such as laser cutters and 3D printers allow users to create precise objects quickly. However, working through a CAD system removes users from the workpiece. Recent interactive fabrication tools reintroduce this directness, but at the expense of precision.
 In this paper, we introduce <i>constructable</i>, an interactive drafting table that produces precise physical output in every step. Users interact by drafting directly on the workpiece using a hand-held laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting the workpiece using a fast high-powered laser cutter.
 <i>Constructable</i> achieves precision through tool-specific constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, rather than using a screen or projection. We demonstrate how constructable allows creating simple but functional devices, including a simple gearbox, that cannot be created with traditional interactive fabrication tools.",48,95.0980392157
UIST,c46e48b531b5e1ed3a74c6d56cc1fca687f36fb3,UIST,2013,Enabling an ecosystem of personal behavioral data,Jason Wiese,2926492,"Almost every computational system a person interacts with keeps a detailed log of that person's behavior. The possibility of this data promises a breadth of new service opportunities for improving people's lives through deep personalization, tools to manage aspects of their personal wellbeing, and services that support identity construction. However, the way that this data is collected and managed today introduces several challenges that severely limit the utility of this rich data.
 This thesis maps out a computational ecosystem for personal behavioral data through the design, implementation, and evaluation of Phenom, a web service that factors out common activities in making inferences from personal behavioral data. The primary benefits of Phenom include: a structured process for aggregating and representing user data; support for developing models based on personal behavioral data; and a unified API for accessing inferences made by models within Phenom. To evaluate Phenom for ease of use and versatility, an external set of developers will create example applications with it.",0,10.5504587156
UIST,2ecf9567c6c435c6a57ab3a3b33f2c82d32f75bb,UIST,2014,Prefab layers and prefab annotations: extensible pixel-based interpretation of graphical interfaces,"Morgan Dixon, Alexander Nied, James Fogarty","1743901, 2470411, 1738171","Pixel-based methods have the potential to fundamentally change how we build graphical interfaces, but remain difficult to implement. We introduce a new toolkit for pixel based enhancements, focused on two areas of support. Prefab Layers helps developers write interpretation logic that can be composed, reused, and shared to manage the multi-faceted nature of pixel-based interpretation. Prefab Annotations supports robustly annotating interface elements with metadata needed to enable runtime enhancements. Together, these help developers overcome subtle but critical dependencies between code and data. We validate our toolkit with (1) demonstrative applications and (2) a lab study that compares how developers build an enhancement using our toolkit versus state of the art methods. Our toolkit addresses core challenges faced by developers when building pixel based enhancements, potentially opening up pixel based systems to broader adoption.",4,57.3643410853
UIST,108b3c510691dfe5d618d72fe41b7331aedb4307,UIST,2011,Modular and deformable touch-sensitive surfaces based on time domain reflectometry,"Raphael Wimmer, Patrick Baudisch","3231762, 1729393","Time domain reflectometry, a technique originally used in diagnosing cable faults, can also locate where a cable is being touched. In this paper, we explore how to extend time domain reflectometry in order to touch-enable thin, modular, and deformable surfaces and devices. We demonstrate how to use this approach to make smart clothing and to rapid prototype touch-sensitive objects of arbitrary shape. To accomplish this, we extend time domain reflectometry in three ways: (1) Thin: We demonstrate how to run time domain reflectometry on a single wire. This allows us to touch-enable thin metal objects, such as guitar strings. (2) Modularity: We present a two-pin connector system that allows users to daisy chain touch-sensitive segments. We illustrate these enhancements with 13 prototypes and a series of performance measurements. (3) Deformability: We create deformable touch devices by mounting stretch-able wire patterns onto elastic tape and meshes. We present selected performance measurements.",30,75.7142857143
UIST,3b642a390f530f82fe2749714517dfc2ee06718a,UIST,2013,PacCAM: material capture and interactive 2D packing for efficient material usage on CNC cutting machines,"Daniel Saakes, Thomas Cambazard, Jun Mitani, Takeo Igarashi","2691594, 2577384, 2618827, 1717356","The availability of low-cost digital fabrication devices enables new groups of users to participate in the design and fabrication of things. However, software to assist in the transition from design to actual fabrication is currently overlooked. In this paper, we introduce PacCAM, a system for packing 2D parts within a given source material for fabrication using 2D cutting machines. Our solution combines computer vision to capture the source material shape with a user interface that incorporates 2D rigid body simulation and snapping. A user study demonstrated that participants could make layouts faster with our system compared with using traditional drafting tools. PacCAM caters to a variety of 2D fabrication applications and can contribute to the reduction of material waste.",4,51.8348623853
UIST,3468b5b468b83ac04726628d49d55aa60437279f,UIST,2015,RFlow: User Interaction Beyond Walls,"Hisham Bedri, Otkrist Gupta, Andrew Temme, Micha Feigin, Gregory L. Charvat, Ramesh Raskar","2977909, 2332192, 2062494, 2063782, 2701825, 1717566","Current user-interaction with optical gesture tracking technologies suffer from occlusions, limiting the functionality to direct line-of-sight. We introduce RFlow, a compact, medium-range interface based on Radio Frequency (RF) that enables camera-free tracking of the position of a moving hand through drywall and other occluders. Our system uses Time of Flight (TOF) RF sensors and speed-based segmentation to localize the hand of a single user with 5cm accuracy (as measured to the closest ground-truth point), enabling an interface which is not restricted to a training set.",0,16.2280701754
UIST,1cd150579ed52a9e2bb6f8683e6dc63cf49e71b0,UIST,2013,PneUI: pneumatically actuated soft composite materials for shape changing interfaces,"Lining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva, Hiroshi Ishii","2912448, 2392339, 3163859, 2770912, 3332772, 1749649","This paper presents PneUI, an enabling technology to build shape-changing interfaces through pneumatically-actuated soft composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. This is enabled by the composites' multi-layer structures with different mechanical or electrical properties. The shape changing states are computationally controllable through pneumatics and pre-defined structure. We explore the design space of PneUI through four applications: height changing tangible phicons, a shape changing mobile, a transformable tablet case and a shape shifting lamp.",47,99.0825688073
UIST,515cdf3242c425820b4c5e116b8cd062d608aca3,UIST,2015,Looking through the Eye of the Mouse: A Simple Method for Measuring End-to-end Latency using an Optical Mouse,"Géry Casiez, Stéphane Conversy, Matthieu Falce, Stéphane Huot, Nicolas Roussel","3051289, 1783132, 2478399, 1809689, 1728921","We present a simple method for measuring end-to-end latency in graphical user interfaces. The method works with most optical mice and allows accurate and real time latency measures up to 5 times per second. In addition, the technique allows easy insertion of probes at different places in the system I.e. mouse events listeners - to investigate the sources of latency. After presenting the measurement method and our methodology, we detail the measures we performed on different systems, toolkits and applications. Results show that latency is affected by the operating system and system load. Substantial differences are found between C++/GLUT and C++/Qt or Java/Swing implementations, as well as between web browsers.",2,60.5263157895
UIST,7b3ae35b08b35079f4c74232691187ee3c2e7fda,UIST,2011,"deForm: an interactive malleable surface for capturing 2.5D arbitrary objects, tools and touch","Sean Follmer, Micah K. Johnson, Edward H. Adelson, Hiroshi Ishii","2770912, 1744375, 1788148, 1749649","We introduce a novel input device, deForm, that supports 2.5D touch gestures, tangible tools, and arbitrary objects through real-time structured light scanning of a malleable surface of interaction. DeForm captures high-resolution surface deformations and 2D grey-scale textures of a gel surface through a three-phase structured light 3D scanner. This technique can be combined with IR projection to allow for invisible capture, providing the opportunity for co-located visual feedback on the deformable surface. We describe methods for tracking fingers, whole hand gestures, and arbitrary tangible tools. We outline a method for physically encoding fiducial marker information in the height map of tangible tools. In addition, we describe a novel method for distinguishing between human touch and tangible tools, through capacitive sensing on top of the input surface. Finally we motivate our device through a number of sample applications.",23,69.0476190476
UIST,0a7fdb84a43e76eda9f50c075c17ae93eb2b6707,UIST,2012,Learning design patterns with bayesian grammar induction,"Jerry O. Talton, Lingfeng Yang, Ranjitha Kumar, Maxine Lim, Noah D. Goodman, Radomír Mech","2220493, 2406176, 1816562, 2701217, 1945655, 2008027","Design patterns have proven useful in many creative fields, providing content creators with archetypal, reusable guidelines to leverage in projects. Creating such patterns, however, is a time-consuming, manual process, typically relegated to a few experts in any given domain. In this paper, we describe an algorithmic method for learning design patterns directly from data using techniques from natural language processing and structured concept learning. Given a set of labeled, hierarchical designs as input, we induce a probabilistic formal grammar over these exemplars. Once learned, this grammar encodes a set of generative rules for the class of designs, which can be sampled to synthesize novel artifacts. We demonstrate the method on geometric models and Web pages, and discuss how the learned patterns can drive new interaction mechanisms for content creators.",24,81.3725490196
UIST,e76a7103f6753b00c6d3fec705029cd16078c63e,UIST,2016,OctaRing: Examining Pressure-Sensitive Multi-Touch Input on a Finger Ring Device,"Hyunchul Lim, Jungmin Chung, Changhoon Oh, SoHyun Park, Bongwon Suh","2970583, 3492407, 2674712, 2315128, 3155504","In this paper, we introduce OctaRing, an octagon-shaped finger ring device that facilitates pressure-sensitive multi- touch gestures. To explore the feasibility of its prototype, we conducted an experiment and investigated users' sensorimotor skills in exerting different levels of pressure on the ring with more than one finger. The results of the experiment indicate that users are comfortable with the two-finger touch configuration with two levels of pressure. Based on this result, future work will explore novel gestures involving a finger ring device.",0,44.6540880503
UIST,5a9c6c1355f72d76a21389630b9f07dfd4b77ffa,UIST,2008,Lineogrammer: creating diagrams by drawing,"Robert C. Zeleznik, Andrew Bragdon, Chu-Chi Liu, Andrew S. Forsberg","1713625, 1729297, 2181852, 1792958","We present the design of Lineogrammer, a diagram-drawing system motivated by the immediacy and fluidity of pencil-drawing. We attempted for Lineogrammer to feel like a modeless diagramming ""medium"" in which stylus input is immediately interpreted as a command, text label or a drawing element, and drawing elements snap to or sculpt from existing elements. An inferred dual representation allows geometric diagram elements, no matter how they were entered, to be manipulated at granularities ranging from vertices to lines to shapes. We also integrate lightweight tools, based on rulers and construction lines, for controlling higher-level diagram attributes, such as symmetry and alignment. We include preliminary usability observations to help identify areas of strength and weakness with this approach.",28,55.7142857143
UIST,03b92455480625371f521e68c6a29235a2faadab,UIST,2010,Hands-on math: a page-based multi-touch and pen desktop for technical work and problem solving,"Robert C. Zeleznik, Andrew Bragdon, Ferdi Adeputra, Hsu-Sheng Ko","1713625, 1729297, 1818315, 2118720","Students, scientists and engineers have to choose between the flexible, free-form input of pencil and paper and the computational power of Computer Algebra Systems (CAS) when solving mathematical problems. Hands-On Math is a multi-touch and pen-based system which attempts to unify these approaches by providing virtual paper that is enhanced to recognize mathematical notations as a means of providing in <i>situ</i> access to CAS functionality. Pages can be created and organized on a large pannable desktop, and mathematical expressions can be computed, graphed and manipulated using a set of uni- and bi-manual interactions which facilitate rapid exploration by eliminating tedious and error prone transcription tasks. Analysis of a qualitative pilot evaluation indicates the potential of our approach and highlights usability issues with the novel techniques used.",34,84.8837209302
UIST,36ab37d450a900ceeaa092cd7ae4fc3a01b2eafc,UIST,2013,Imaginary reality gaming: ball games without a ball,"Patrick Baudisch, Henning Pohl, Stefanie Reinicke, Emilia Wittmers, Patrick Lühne, Marius Knaust, Sven Köhler, Patrick Schmidt, Christian Holz","1729393, 2242459, 1813895, 2508895, 1906774, 1966318, 3027101, 2256967, 2794828","We present imaginary reality games, i.e., games that mimic the respective real world sport, such as basketball or soccer, except that there is no visible ball. The ball is virtual and players learn about its position only from watching each other act and a small amount of occasional auditory feed-back, e.g., when a person is receiving the ball. Imaginary reality games maintain many of the properties of physical sports, such as unencumbered play, physical exertion, and immediate social interaction between players. At the same time, they allow introducing game elements from video games, such as power-ups, non-realistic physics, and player balancing. Most importantly, they create a new game dynamic around the notion of the invisible ball. To allow players to successfully interact with the invisible ball, we have created a physics engine that evaluates all plausible ball trajectories in parallel, allowing the game engine to select the trajectory that leads to the most enjoyable game play while still favoring skillful play.",13,74.3119266055
UIST,e484fccad4cc69e8528f1d805f6b646e0d640083,UIST,2015,Printem: Instant Printed Circuit Boards with Standard Office Printers & Inks,"Varun Perumal, Daniel J. Wigdor","3297031, 1961958","Printem film, a novel method for the fabrication of Printed Circuit Boards (PCBs) for small batch/prototyping use, is presented. Printem film enables a standard office inkjet or laser printer, using standard inks, to produce a PCB: the user prints a negative of the PCB onto the film, exposes it to UV or sunlight, and then tears-away the unneeded portion of the film, leaving-behind a copper PCB. PCBs produced with Printem film are as conductive as PCBs created using standard industrial methods. Herein, the composition of Printem film is described, and advantages of various materials discussed. Sample applications are also described, each of which demonstrates some unique advantage of Printem film over current prototyping methods: conductivity, flexibility, the ability to be cut with a pair of scissors, and the ability to be mounted to a rigid backplane.
    NOTE: publication of full-text held until November 9, 2015.",2,60.5263157895
UIST,a7f54e286ac15283ae70b9358722d1f33578735d,UIST,2014,WirePrint: 3D printed previews for fast prototyping,"Stefanie Müller, Sangha Im, Serafima Gurevich, Alexander Teibrich, Lisa Pfisterer, François Guimbretière, Patrick Baudisch","4942906, 3121051, 2495689, 2184501, 3160012, 2539134, 1729393","Even though considered a rapid prototyping tool, 3D printing is so slow that a reasonably sized object requires printing overnight. This slows designers down to a single iteration per day. In this paper, we propose to instead print low-fidelity wireframe previews in the early stages of the design process. Wireframe previews are 3D prints in which surfaces have been replaced with a wireframe mesh. Since wireframe previews are to scale and represent the overall shape of the 3D object, they allow users to quickly verify key aspects of their 3D design, such as the ergonomic fit. To maximize the speed-up, we instruct 3D printers to extrude filament not layer-by-layer, but directly in 3D-space, allowing them to create the edges of the wireframe model directly one stroke at a time. This allows us to achieve speed-ups of up to a factor of 10 compared to traditional layer-based printing. We demonstrate how to achieve wireframe previews on standard FDM 3D printers, such as the PrintrBot or the Kossel mini. Users only need to install the WirePrint software, making our approach applicable to many 3D printers already in use today. Finally, wireframe previews use only a fraction of material required for a regular print, making it even more affordable to iterate.",15,88.7596899225
UIST,15940c3cf0526cb7904981b5e2a2000a96e06548,UIST,2015,Patching Physical Objects,"Alexander Teibrich, Stefanie Müller, François Guimbretière, Robert Kovacs, Stefan Neubert, Patrick Baudisch","2184501, 4942906, 2539134, 2672289, 2899869, 1729393","Personal fabrication is currently a one-way process: Once an object has been fabricated with a 3D printer, it cannot be changed anymore; any change requires printing a new version from scratch. The problem is that this approach ignores the nature of design <i>iteration</i>, i.e. that in subsequent iterations large parts of an object stay the same and only small parts change. This makes fabricating from scratch feel unnecessary and wasteful.
 In this paper, we propose a different approach: instead of re-printing the entire object from scratch, we suggest patching the existing object to reflect the next design iteration. We built a system on top of a 3D printer that accomplishes this: Users mount the <i>existing</i> object into the 3D printer, then load both the original and the modified 3D model into our software, which in turn calculates how to patch the object. After identifying which parts to remove and what to add, our system locates the existing object in the printer using the system's built-in 3D scanner. After calibrating the orientation, a mill first removes the outdated geometry, then a print head prints the new geometry in place.
 Since only a fraction of the entire object is refabricated, our approach reduces material consumption and plastic waste (for our example objects by 82% and 93% respectively).",10,96.9298245614
UIST,3c28c313d4eb76c7348ad5dbaebfc8600a55b2de,UIST,2016,Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display,"Evan Strasnick, Sean Follmer","3492590, 2770912","Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.",0,44.6540880503
UIST,65555a852eff770439b0ecec7b265d137c8afc2a,UIST,2000,Page detection using embedded tags,"Maribeth Back, Jonathan Cohen","2131565, 5054536","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT We describe a robust working prototype of a system for accurate page-ID detection from bound paper books. Our method uses a new RFID technology to recognize book page location. A thin flexible transponder tag with a unique ID is embedded in the paper of each page, and a tag reader is affixed to the binding of the back of the book. As the pages turn, the tag reader notices which tags are within its read range and which have moved out of its range (which is about four inches). The human interacts with the book naturally, and is not required to perform any actions for page detection that are not usual in book interaction. The page-detection data can be used to enhance the experience of the book, or to enable the book as a controller for another system. One such system, an interactive museum exhibit, is briefly described.",13,16.0
UIST,0f8b8e2dd7febcaeec78d7bc0220d339b10e6b01,UIST,2016,Initial Trials of ofxEpilog: From Real Time Operation to Dynamic Focus of Epilog Laser Cutter,"Mitsuhito Ando, Chisaki Murakami, Takayuki Ito, Kazuhiro Jo","3396281, 3493211, 1679044, 3093077","This paper describes ofxEpilog which enable people to control a laser cutter of Epilog in real time. ofxEpilog is an add on of open Frameworks, an open source C++ toolkit for creative coding. With the add on, people could directly send their image object to a laser cutter through Ethernet. By alternating the generation and transmission of the command of cutting, the add on could sequentially control a laser cutter in real time. This paper introduces our initial trials of ofxEpilog with a real time operation (A), dynamic focus (z-axis) control with a given 3D object (B), and a scanned 3D object (C). Technical limitations and our upcoming challenges are also discussed.",0,44.6540880503
UIST,f2cfbdd7e0e64c548e5c0431945dc400ba663862,UIST,2016,Wrap & Sense: Grasp Capture by a Band Sensor,"Natsuki Miyata, Takehiro Honoki, Yusuke Maeda, Yui Endo, Mitsunori Tada, Yuta Sugiura","1769049, 3493216, 2505698, 2320329, 1779121, 1799242","This paper proposes a bare hand grasp observation system named Wrap &#38; Sense. We built a band type sensing equipment composed of infrared distance sensors placed in an array. The sensor band is attached to a target object with all sensors directed along the object surface and detects the hand side edge with respect to the object. Assuming type of grasp as 'power grasp', the whole hand posture can be determined according to the 3D shape of the object. Three types of application are shown as proof-of-concept.",0,44.6540880503
UIST,44d73552b7588ac91287e98aa061fb8d8f1923f9,UIST,2013,Obake: interactions on a 2.5D elastic display,"Dhairya Dand, Robert Hemsley","1742280, 3295527","In this poster we present an interaction language for the manipulation of an elastic deformable 2.5D display. We discuss a range of gestures to interact and directly deform the surface. To demonstrate these affordances and the associated interactions, we present a scenario of a topographic data viewer using this prototype system.",1,27.0642201835
UIST,7ecc7bdabcd91e7b41e13d35aa4a4a5fd90cbd5c,UIST,2015,Responsive Facilitation of Experiential Learning Through Access to Attentional State,Scott Greenwald,2686192,"The planned thesis presents a vision of the future of learning, where learners explore environments, physical and virtual, in a curiosity-driven or intrinsically motivated way, and receive contextual information from a companion facilitator or teacher. Learners are instrumented with sensors that convey their cognitive and attentional state to the companion, who can then accurately judge what is interesting or relevant, and when is a good moment to jump in. I provide a broad definition of the possible types of sensor input as well as the modalities of intervention, and then present a specific proof-of-concept system that uses gaze behavior as a means of communication between the learner and a human companion.",0,16.2280701754
UIST,e9f2b54b0b74777e16d542660005ef11c89b1357,UIST,2011,HaCHIStick: simulating haptic sensation on tablet pc for musical instruments application,"Taku Hachisu, Michi Sato, Shogo Fukushima, Hiroyuki Kajimoto","3242743, 2494645, 1844978, 1776927","In this paper, we propose a novel stick-type interface, the ""HaCHIStick,"" for musical performance on a tablet PC. The HaCHIStick is composed of a stick with an embedded vibrotactile actuator, a visual display, and an elastic sheet on the display. By combining the kinesthetic sensation induced by striking the elastic sheet with vibrotactile sensation, the system provides natural haptic cues that enable the user to feel what they strike with the stick, such as steel or wood. This haptic interaction would enrich the user's experience when playing the instruments. The interface is regarded as a type of haptic augmented reality (AR) system, with a relatively simple setup.",3,31.4285714286
UIST,b1027d324bb3f4ac1e274b6bdc07da2a92175ead,UIST,2015,Juggling the Effects of Latency: Software Approaches to Minimizing Latency in Dynamic Projector-Camera Systems,"Jarrod Knibbe, Hrvoje Benko, Andrew D. Wilson","1849493, 2704133, 1767449","Projector-camera (pro-cam) systems afford a wide range of interactive possibilities, combining both natural and mixed-reality 3D interaction. However, the latency inherent within these systems can cause the projection to ?slip? from any moving target, so pro-cam systems have typically shied away from truly dynamic scenarios. We explore software-only techniques to reduce latency; considering the best achievable results with widely adopted commodity devices (e.g. 30Hz depth cameras and 60Hz projectors). We achieve 50% projection alignment on objects in free flight (a 34% improvement) and 69% alignment on dynamic human movement (a 40% improvement).",3,71.0526315789
UIST,e694789006076522363f4fb4995295f6868df8aa,UIST,2011,AdaptableGIMP: designing a socially-adaptable interface,"Benjamin J. Lafreniere, Andrea Bunt, Matthew Lount, Filip Krynicki, Michael A. Terry","2813134, 1727047, 2118384, 3305163, 1740154","We introduce the concept of a socially-adaptable interface, an interface that provides instant access to task-specific interface customizations created, edited, and documented by the application's user community. We demonstrate this concept in AdaptableGIMP, a modified version of the GIMP image editor that we have developed.",12,51.9047619048
UIST,5e5451105177e429fca7c31c2c0d0fa62ee306ab,UIST,2009,A screen-space formulation for 2D and 3D direct manipulation,"Jason L. Reisman, Philip L. Davidson, Jefferson Y. Han","1818710, 2362379, 2183554","Rotate-Scale-Translate (RST) interactions have become the <i>de facto</i> standard when interacting with two-dimensional (2D) contexts in single-touch and multi-touch environments. Because the use of RST has thus far focused almost entirely on 2D, there are not yet standard techniques for extending these principles into three dimensions. In this paper we describe a screen-space method which fully captures the semantics of the traditional 2D RST multi-touch interaction, but also allows us to extend these same principles into three-dimensional (3D) interaction. Just like RST allows users to directly manipulate 2D contexts with two or more points, our method allows the user to directly manipulate 3D objects with three or more points. We show some novel interactions, which take perspective into account and are thus not available in orthographic environments. Furthermore, we identify key ambiguities and unexpected behaviors that arise when performing direct manipulation in 3D and offer solutions to mitigate the difficulties each presents. Finally, we show how to extend our method to meet application-specific control objectives, as well as show our method working in some example environments.",84,88.5714285714
UIST,1d4bfd7075182a7665fe08bc9554e9e1c5327374,UIST,2008,Extending 2D object arrangement with pressure-sensitive layering cues,"Philip L. Davidson, Jefferson Y. Han","2362379, 2183554","We demonstrate a pressure-sensitive depth sorting technique that extends standard two-dimensional (2D) manipulation techniques, particularly those used with multi-touch or multi-point controls. We combine this layering operation with a page-folding metaphor for more fluid interaction in applications requiring 2D sorting and layout.",28,55.7142857143
UIST,1f93c57ecaafb9c75ee983a21787bd9314015294,UIST,2012,Crowd-based recognition of web interaction patterns,"Walter S. Lasecki, Tessa A. Lau, Grant He, Jeffrey P. Bigham","2598433, 1800706, 2061943, 1744846","Web automation often involves users describing complex tasks to a system, with directives generally limited to low-level constituent actions like ""click the search button."" This level of description is unnatural and makes it difficult to generalize the task across websites. In this paper, we propose a system for automatically recognizing higher-level <i>interaction patterns</i> from user's completion of tasks, such as ""searching for cat videos"" or ""replying to a post"". We present PatFinder, a system that identifies these patterns using the input of crowd workers. We validate the system by generating data for 10 tasks, having 62 crowd workers label them, and automatically extracting 14 interaction patterns. Our results show that the number of patterns grows sublinearly with the number of tasks, suggesting that a small finite set of patterns may suffice to describe the vast majority of tasks on the web.",3,40.1960784314
UIST,f55899e49ff08b3b92f7eb829615d687a28b8156,UIST,2015,TurkDeck: Physical Virtual Reality Based on People,"Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven Köhler, Patrick Schmidt, Robert Kovacs, Johannes Jasper, Jonas Kemper, Patrick Baudisch","2763041, 2927226, 2330493, 3027101, 2256967, 2672289, 2100179, 2027961, 1729393","TurkDeck is an immersive virtual reality system that reproduces not only what users see and hear, but also what users feel. TurkDeck produces the haptic sensation using props, i.e., when users touch or manipulate an object in the virtual world, they simultaneously also touch or manipulate a corresponding object in the physical world. Unlike previous work on prop-based virtual reality, however, TurkDeck allows creating arbitrarily large virtual worlds in finite space and using a finite set of physical props. The key idea behind TurkDeck is that it creates these physical representations on the fly by making a group of human workers present and operate the props only when and where the user can actually reach them. TurkDeck manages these so-called ""human actuators"" by displaying visual instructions that tell the human actuators when and where to place props and how to actuate them. We demonstrate TurkDeck at the example of an immersive 300m2 experience in 25m2 physical space. We show how to simulate a wide range of physical objects and effects, including walls, doors, ledges, steps, beams, switches, stompers, portals, zip lines, and wind. In a user study, participants rated the realism/immersion of TurkDeck higher than a traditional prop-less baseline condition (4.9 vs. 3.6 on 7 item Likert).",8,90.350877193
UIST,e878b4e36b896c78730c8c16948d8085503f3bf5,UIST,2010,Enabling beyond-surface interactions for interactive surface with an invisible projection,"Li-Wei Chan, Hsiang-Tao Wu, HuiShan Kao, Ju-Chun Ko, Home-Ru Lin, Mike Y. Chen, Jane Yung-jen Hsu, Yi-Ping Hung","1682665, 1775992, 2289658, 3243124, 3054075, 2335746, 1717095, 7312257","This paper presents a programmable infrared (IR) technique that utilizes invisible, programmable markers to support interaction beyond the surface of a diffused-illumination (DI) multi-touch system. We combine an IR projector and a standard color projector to simultaneously project visible content and invisible markers. Mobile devices outfitted with IR cameras can compute their 3D positions based on the markers perceived. Markers are selectively turned off to support multi-touch and direct on-surface tangible input. The proposed techniques enable a collaborative multi-display multi-touch tabletop system. We also present three interactive tools: <i>i-m</i>-View, <i>i-m</i>-Lamp, and <i>i-m</i>-Flashlight, which consist of a mobile tablet and projectors that users can freely interact with beyond the main display surface. Early user feedback shows that these interactive devices, combined with a large interactive display, allow more intuitive navigation and are reportedly enjoyable to use.",12,66.8604651163
UIST,0d26ce41c7fd09c486b1420b3d401aae5f0f7274,UIST,2005,Low-cost multi-touch sensing through frustrated total internal reflection,Jefferson Y. Han,2183554,"This paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on <i>frustrated total internal reflection</i>. We review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions.",521,100.0
UIST,27b39f5c81a9dbf877f52659b7f597f8d888ff22,UIST,2015,Impacto: Simulating Physical Impact by Combining Tactile Stimulation with Electrical Muscle Stimulation,"Pedro Lopes, Alexandra Ion, Patrick Baudisch","2816776, 1906663, 1729393","We present impacto, a device designed to render the haptic sensation of hitting or being hit in virtual reality. The key idea that allows the small and light impacto device to simulate a strong hit is that it decomposes the stimulus: it renders the tactile aspect of being hit by tapping the skin using a solenoid; it adds impact to the hit by thrusting the user's arm backwards using electrical muscle stimulation. The device is self-contained, wireless, and small enough for wearable use, thus leaves the user unencumbered and able to walk around freely in a virtual environment. The device is of generic shape, allowing it to also be worn on legs, so as to enhance the experience of kicking, or merged into props, such as a baseball bat. We demonstrate how to assemble multiple impacto units into a simple haptic suit. Participants of our study rated impact simulated using impacto's combination of solenoid hit and electrical muscle stimulation as more realistic than either technique in isolation.",9,94.298245614
UIST,8ed0643fd2090975d0da95d159bb675c01b4bdc4,UIST,2013,Inkjet-printed conductive patterns for physical manipulation of audio signals,"Nan-Wei Gong, Amit Zoran, Joseph A. Paradiso","1690147, 2866829, 4798651","In this demo paper, we present the realization of a completely aesthetically driven conductive image as a multi-modal music controller. Combining two emerging technologies - rapid prototyping with an off-the-shelf inkjet printer using conductive ink and parametric graphic design, we are able to create an interactive surface that is thin, flat, and flexible. This sensate surface can be conformally wrapped around a simple curved surface, and unlike touch screens, can accommodate complex structures and shapes such as holes on a surface. We present the design and manufacturing flow and discuss the technology behind this multi-modal sensing design. Our work seeks to offer a new dimension of designing sonic interaction with graphic tools, playing and learning music from a visual perspective and performing with expressive physical manipulation.",3,45.871559633
UIST,ba75abbcc4d7b346b8c8c3ce3cb0b4cb76f016f5,UIST,2013,Shape changing device for notification,"Kazuki Kobayashi, Seiji Yamada","8557863, 1679243","In this paper, we describe a notification method with peripheral cognition technology that uses a human cognitive characteristic. The method achieves notification without interrupting users' primary tasks. We developed a shape changing device that change its shape to notify the arrival of information. Such behavior enables a user to easily find and accept notifications without interruption when their attention on the primary task decreases. The result of an experiment showed that the successful notification rate was 45.5%.",1,27.0642201835
UIST,9e015c05aff067d24288b00cd37ded9155050f55,UIST,2016,Friend*Chip: A Bracelet with Digital Pet for Socially Inclusive Games for Children,"Eleuda Nuñez, Francesco Visentin, Kenji Suzuki","2096779, 2639834, 4471601","Learning in groups have different potential benefits for children. They have the opportunity to solve problems together, to share experiences and to develop social skills. However, from teachers point of view, creating a safe and inclusive positive environment for children is not an simple task since each child has differences that represent a challenge for implementing effectively group dynamics. The focus of this work is the design of a system that motivates children to approach to others and create opportunities of social interaction. The system creates a fun and enjoyable situation that is always supervised by the teacher, who can monitor and change the group dynamics at any moment during the activity.",0,44.6540880503
UIST,f59ce4bb8dd7b7b58fcb0465ea2f59c5e2cd549b,UIST,2016,Sidetap & Slingshot Gestures on Unmodified Smartwatches,"Hui-Shyong Yeo, Juyoung Lee, Andrea Bianchi, Aaron Quigley","2602737, 2726779, 4189910, 3378059","We present a technique for detecting gestures on the edge of an unmodified smartwatch. We demonstrate two exemplary gestures, i) Sidetap -- tapping on any side and ii) Slingshot -- pressing on the edge and then releasing quickly. Our technique is lightweight, as it relies on measuring the data from the internal Inertial measurement unit (IMU) only. With these two gestures, we expand the input expressiveness of a smartwatch, allowing users to use intuitive gestures with natural tactile feedback, e.g., for the rapid navigation of a long list of items with a tap, or act as shortcut commands to launch applications. It can also allow for eyes-free interaction or subtle interaction where visual attention is not available.",0,44.6540880503
UIST,cbcb6ea03e319f8ef91de7f2c3968e10f51f5fd9,UIST,1991,A unidraw-based user interface builder,"John M. Vlissides, Steven Tang","1781083, 1957975","Ibuild is a user interface builder that lets a user manipulate simulations of toolkit objects rather than actual toolkit objects. Ibuild is built with Unidraw, a framework for building graphical editors that is part of the InterViews toolkit. Urridraw makes the simulation-based approach attractive. Simulating toolkit objects in Unidraw makes it easier to support editing facilities that are common in other kinds of graphical editors, and it keeps the builder insulated from a particular toolkit implementation. Ibuild supports direct manipulation analogs of InterViews' composition mechanisms , which simplify the specification of an interface's layout and resize semantics. Ibuild also leverages the C++ inheritance mechanism to decouple builder-generated code from the rest of the application. And while current user interface builders SOP at the widget level, ibuild incorporates Unidraw abstractions to simplify the implementation of graphical editors.",18,47.8260869565
UIST,afb54d5e3be44c61d8b8863b575f8470c0e7136d,UIST,2011,Animating from markup code to rendered documents and vice versa,"Pierre Dragicevic, Stéphane Huot, Fanny Chevalier","3297322, 1809689, 2840251","We present a quick preview technique that smoothly transitions between document markup code and its visual rendering. This technique allows users to regularly check the code they are editing in-place, without leaving the text editor. This method can complement classical preview windows by offering rapid overviews of code-to-document mappings and leaving more screen real-estate. We discuss the design and implementation of our technique.",1,15.7142857143
UIST,cc3cd425c692d12f49e7529954fbbed305b36d14,UIST,2007,Smart bookmarks: automatic retroactive macro recording on the web,"Darris Hupp, Rob Miller","3123506, 1723785","We present a new web automation system that allows users to create a smart bookmark, consisting of a starting URL plus a script of commands that returns to a particular web page or state of a web application. A smart bookmark can be requested for any page, and the necessary commands are automatically extracted from the user's interaction history. Unlike other web macro recorders, which require the user to start recording before navigating to the desired page, smart bookmarks are generated retroactively, after the user has already reached a page, and the starting point of the macro is found automatically. Smart bookmarks have a rich graphical visualization that combines textual commands, web page screenshots, and animations to explain what the bookmark does. A bookmark's script consists of keyword commands, interpreted without strict reliance on syntax, allowing bookmarks to be easily edited and shared.",27,50.0
UIST,66eba41403b1894f8b78eceeb104de8bfe5e0f83,UIST,2005,A1: end-user programming for web-based system administration,"Eser Kandogan, Eben M. Haber, Rob Barrett, Allen Cypher, Paul P. Maglio, Haixia Zhao","1781317, 1757259, 1958217, 1809071, 2387444, 5072245","System administrators work with many different tools to manage and fix complex hardware and software infrastructure in a rapidly paced work environment. Through extensive field studies, we observed that they often build and share custom tools for specific tasks that are not supported by vendor tools. Recent trends toward web-based management consoles offer many advantages but put an extra burden on system administrators, as customization requires web programming, which is beyond the skills of many system administrators. To meet their needs, we developed A1, a spreadsheet-based environment with a task-specific system-administration language for quickly creating small tools or migrating existing scripts to run as web portlets. Using A1, system administrators can build spreadsheets to access remote and heterogeneous systems, gather and integrate status data, and orchestrate control of disparate systems in a uniform way. A preliminary user study showed that in just a few hours, system administrators can learn to use A1 to build relatively complex tools from scratch.",18,29.0322580645
UIST,5c8062b6f795a76140f74107c04a7770a8f3616a,UIST,2016,Physiological Signal-Driven Virtual Reality in Social Spaces,Yun Suen Pai,3427883,"Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.",0,44.6540880503
UIST,aecfe976ec39df09ccd5e4925028a9e3d3e16c40,UIST,2013,AIRREAL: tactile interactive experiences in free air,"Rajinder Sodhi, Matthew Glisson, Ivan Poupyrev","1924499, 2200814, 1736819","AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation.",0,10.5504587156
UIST,dee81f24932b82cd95dfd9b0f6953dc44ce98027,UIST,2012,Histomages: fully synchronized views for image editing,"Fanny Chevalier, Pierre Dragicevic, Christophe Hurter","2840251, 3297322, 2433007","We present Histomages, a new interaction model for image editing that considers color histograms as spatial rearrangements of image pixels. Users can select pixels on image histograms as they would select image regions and directly manipulate them to adjust their colors. Histomages are also affected by other image tools such as paintbrushes. We explore some possibilities offered by this interaction model, and discuss the four key principles behind it as well as their implications for the design of feature-rich software in general.",7,54.9019607843
UIST,16d63c2d27cffd78377cfe8e163439e5a01aa9ac,UIST,2005,Sensing and visualizing spatial relations of mobile devices,"Gerd Kortuem, Christian Kray, Hans-Werner Gellersen","1682658, 1782482, 4919595","Location information can be used to enhance interaction with mobile devices. While many location systems require instrumentation of the environment, we present a system that allows devices to measure their spatial relations in a true peer-to-peer fashion. The system is based on custom sensor hardware implemented as USB dongle, and computes spatial relations in real-time. In extension of this system we propose a set of spatialized widgets for incorporation of spatial relations in the user interface. The use of these widgets is illustrated in a number of applications, showing how spatial relations can be employed to support and streamline interaction with mobile devices.",55,64.5161290323
UIST,2d368d92746c400b2f2f03f7bb817d1c9b1a4d14,UIST,2010,Exploring pen and paper interaction with high-resolution wall displays,"Nadir Weibel, Anne Marie Piper, James D. Hollan","1752711, 2373998, 1698170","We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays. While the size and resolution of such displays allow visualization of data sets of a scale not previously possible, mechanisms for interacting with wall displays remain challenging. HIPerPaper enables multiple concurrent users to select, move, scale, and rotate objects on a high-dimension wall display.",0,9.3023255814
UIST,675e799b2b7a8038b64c074a9967681057b17e47,UIST,2011,Imaginary phone: learning imaginary interfaces by transferring spatial memory from a familiar device,"Sean Gustafson, Christian Holz, Patrick Baudisch","1793660, 2794828, 1729393","We propose a method for learning how to use an imaginary interface (i.e., a spatial non-visual interface) that we call ""transfer learning"". By using a physical device (e.g. an iPhone) a user inadvertently learns the interface and can then transfer that knowledge to an imaginary interface. We illustrate this concept with our Imaginary Phone prototype. With it users interact by mimicking the use of a physical iPhone by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing on the hand is tracked using a depth camera and touch events are sent wirelessly to an actual iPhone, where they invoke the corresponding actions. Our prototype allows the user to perform everyday task such as picking up a phone call or launching the timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that frees users from the necessity of retrieving the actual physical device. We present two user studies that validate the three assumptions underlying the transfer learning method. (1) Users build up spatial memory automatically while using a physical device: participants knew the correct location of 68% of their own iPhone home screen apps by heart. (2) Spatial memory transfers from a physical to an imaginary inter-face: participants recalled 61% of their home screen apps when recalling app location on the palm of their hand. (3) Palm interaction is precise enough to operate a typical mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their palm-sufficiently large to operate any iPhone standard widget.",52,90.0
UIST,48b91ef80de616cc2fffbc7a95c7c13f0c57009a,UIST,2010,HIPerPaper: introducing pen and paper interfaces for ultra-scale wall displays,"Nadir Weibel, Anne Marie Piper, James D. Hollan","1752711, 2373998, 1698170","While recent advances in graphics, display, and computer hardware support ultra-scale visualizations of a tremendous amount of data sets, mechanisms for interacting with this information on large high-resolution wall displays are still under investigation. Different issues in terms of user interface, ergonomics, multi-user interaction, and system flexibility arise while facing ultra-scale wall displays and none of the introduced approaches fully address them. We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with the HIPerSpace wall, a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays.",3,42.4418604651
UIST,a53d2ee2cca8d8db75853521819f437bb506a1ee,UIST,2010,Imaginary interfaces: spatial interaction with empty hands and without visual feedback,"Sean Gustafson, Daniel Bierwirth, Patrick Baudisch","1793660, 2306179, 1729393","Screen-less wearable devices allow for the smallest form factor and thus the maximum mobility. However, current screen-less devices only support buttons and gestures. Pointing is not supported because users have nothing to point at. However, we challenge the notion that spatial interaction requires a screen and propose a method for bringing spatial interaction to screen-less devices.
 We present <i>Imaginary Interfaces</i>, screen-less devices that allow users to perform spatial interaction with empty hands and without visual feedback. Unlike projection-based solutions, such as Sixth Sense, all visual ""feedback"" takes place in the user's imagination. Users define the origin of an imaginary space by forming an L-shaped coordinate cross with their non-dominant hand. Users then point and draw with their dominant hand in the resulting space.
 With three user studies we investigate the question: To what extent can users interact spatially with a user interface that exists only in their imagination? Participants created simple drawings, annotated existing drawings, and pointed at locations described in imaginary space. Our findings suggest that users' visual short-term memory can, in part, replace the feedback conventionally displayed on a screen.",103,95.3488372093
UIST,eb4229758b659a3a7041f8f3def402a53f64320a,UIST,2016,Transparent Reality: Using Eye Gaze Focus Depth as Interaction Modality,"Yun Suen Pai, Benjamin Outram, Noriyasu Vontin, Kai Kunze","3427883, 3428012, 3428550, 3055122","We present a novel, eye gaze based interaction technique, using focus depth as an input modality for virtual reality (VR) applications. We also show custom hardware prototype implementation. Comparing the focus depth based interaction to a scroll wheel interface, we find no statistically significant difference in performance (the focus depth works slightly better) and a subjective preference of the users in a user study with 10 participants playing a simple VR game. This indicates that it is a suitable interface modality that should be further explored. Finally, we give some application scenarios and guidelines for using focus depth interactions in VR applications.",0,44.6540880503
UIST,8d18b722a5313444e3b9f9e5e9a3d2c1d3f1360c,UIST,2012,Programming with everybody: tightening the copy-modify-publish feedback loop,"Thomas Lieber, Rob Miller","2065874, 1723785","People write more code than they ever share online. They also copy and tweak code more often than they contribute their modifications back to the public. These situations can lead to widespread duplication of effort. However, the copy-modify-publish feedback loop which could solve the problem is inhibited by the effort required to publish code online. In this paper we present our preliminary, ongoing effort to create Ditty, a programming environment that attacks the problem by sharing changes immediately, making all code public by default. Ditty tracks the changes users make to code they find and exposes the modified versions alongside the original so that commonly-used derivatives can eventually become canonical. Our work will examine mechanical and social methods to consolidate global effort on common code snippets, and the effects of designing a programming interface that inspires a feeling of the whole world programming together.",1,25.0
UIST,2eebd5aa7f989c469e980f94df504b780e66df95,UIST,2006,"Reflective physical prototyping through integrated design, test, and analysis","Björn Hartmann, Scott R. Klemmer, Michael S. Bernstein, Leith Abdulla, Brandon Burr, Avi Robinson-Mosher, Jennifer Gee","4020023, 1728167, 3047089, 1910920, 3261262, 2982297, 8447484","Prototyping is the pivotal activity that structures innovation, collaboration, and creativity in design. Prototypes embody design hypotheses and enable designers to test them. Framin design as a thinking-by-doing activity foregrounds <i>iteration</i> as a central concern. This paper presents d.tools, a toolkit that embodies an iterative-design-centered approach to prototyping information appliances. This work offers contributions in three areas. First, d.tools introduces a <i>statechart-based visual design tool</i> that provides a low threshold for early-stage prototyping, extensible through code for higher-fidelity prototypes. Second, our research introduces <i>three important types of hardware extensibility</i> - at the hardware-to-PC interface, the intra-hardware communication level, and the circuit level. Third, d.tools integrates <i>design, test, and analysis</i> of information appliances. We have evaluated d.tools through three studies: a laboratory study with thirteen participants; rebuilding prototypes of existing and emerging devices; and by observing seven student teams who built prototypes with d.tools.",139,100.0
UIST,ad95aca097716edb06d84d2e5b0cf1e353438995,UIST,2016,A Rapid Prototyping Approach to Synthetic Data Generation for Improved 2D Gesture Recognition,"Eugene M. Taranta, Mehran Maghoumi, Corey R. Pittman, Joseph J. LaViola","3122624, 3250084, 3491891, 1726039","Training gesture recognizers with synthetic data generated from real gestures is a well known and powerful technique that can significantly improve recognition accuracy. In this paper we introduce a novel technique called <i>gesture path stochastic resampling</i> (GPSR) that is computationally efficient, has minimal coding overhead, and yet despite its simplicity is able to achieve higher accuracy than competitive, state-of-the-art approaches. GPSR generates synthetic samples by lengthening and shortening gesture subpaths within a given sample to produce realistic variations of the input via a process of nonuniform resampling. As such, GPSR is an appropriate rapid prototyping technique where ease of use, understandability, and efficiency are key. Further, through an extensive evaluation, we show that accuracy significantly improves when gesture recognizers are trained with GPSR synthetic samples. In some cases, mean recognition errors are reduced by more than 70%, and in most cases, GPSR outperforms two other evaluated state-of-the-art methods.",0,44.6540880503
UIST,7dca0f56046ece7e8b7c7ae2f371891567ac0b8b,UIST,2015,SmartTokens: Embedding Motion and Grip Sensing in Small Tangible Objects,"Mathieu Le Goc, Pierre Dragicevic, Samuel Huron, Jeremy Boy, Jean-Daniel Fekete","2685181, 3297322, 1813823, 2443687, 1721432","SmartTokens are small-sized tangible tokens that can sense multiple types of motion, multiple types of touch/grip, and send input events wirelessly as state-machine transitions. By providing an open platform for embedding basic sensing capabilities within small form-factors, SmartTokens extend the design space of tangible user interfaces. We describe the design and implementation of SmartTokens and illustrate how they can be used in practice by introducing a novel TUI design for event notification and personal task management.",1,42.1052631579
UIST,541c6d712ee4421e4fb4ce24b4cd7f9bb3ee33a6,UIST,2010,Multitoe: high-precision interaction with back-projected floors based on high-resolution multi-touch input,"Thomas Augsten, Konstantin Kaefer, René Meusel, Caroline Fetzer, Dorian Kanitz, Thomas Stoff, Torsten Becker, Christian Holz, Patrick Baudisch","2929820, 2004728, 3141336, 2314032, 2075477, 2774786, 2662057, 2794828, 1729393","Tabletop applications cannot display more than a few dozen on-screen objects. The reason is their limited size: tables cannot become larger than arm's length without giving up direct touch. We propose creating direct touch surfaces that are orders of magnitude larger. We approach this challenge by integrating high-resolution multitouch input into a back-projected floor. As the same time, we maintain the purpose and interaction concepts of tabletop computers, namely direct manipulation.
 We base our hardware design on frustrated total internal reflection. Its ability to sense per-pixel pressure allows the floor to locate and analyze users' soles. We demonstrate how this allows the floor to recognize foot postures and identify users. These two functions form the basis of our system. They allow the floor to ignore users unless they interact explicitly, identify and track users based on their shoes, enable high-precision interaction, invoke menus, track heads, and allow users to control high-degree of freedom interactions using their feet. While we base our designs on a series of simple user studies, the primary contribution on this paper is in the engineering domain.",39,87.2093023256
UIST,87deaca9d1821ce9f4758cd4e022bc7575309ed1,UIST,2004,Combining crossing-based and paper-based interaction paradigms for dragging and dropping between overlapping windows,Pierre Dragicevic,3297322,"Despite novel interaction techniques proposed for virtual desktops, common yet challenging tasks remain to be investigated. Dragging and dropping between overlapping windows is one of them. The fold-and-drop technique presented here offers a natural and efficient way of performing those tasks. We show how this technique successfully builds upon several interaction paradigms previously described, while shedding new light on them.",35,44.7368421053
UIST,9f476813db335534a92a0c3ab1a7390975d71e58,UIST,1991,Comparing the programming demands of single-user and multi-user applications,John F. Patterson,1907528,"Synchronous multiuser applications are designed to support two or more simultaneous users. The RENDEZVOUSTM1 system is an infrastructure for building such multiuser applications. Several multiuser applications, such as a tic-tac-toe game. a multiuser CardTable application, and a multiuser whiteboard have been or are being constructed with the RENDEZVOUS system. We argue that there are at least three dimensions of programming complexity that are differentially affected by the programming of multiuser applications as compared to the programming of single-user applications. The first, concurrency, addresses the need to cope with parallel activities. The second dimension, abstraction, addresses the need to separate the user-int~rface from an underlying application abstraction, The thmd dimension, roles, addresses the need to differentially characterize users and customize the user-interface appropriately. Certainly, single-user applications often deal with these complexities; we argue that multiuser applications cannot avoid them.",40,67.3913043478
UIST,fb72320bbc907303f57cb6b41e0dbebca3232306,UIST,2012,Development of a non-contact tongue-motion acquisition system,"Masato Miyauchi, Takashi Kimura, Takuya Nojima","2784515, 3120616, 1797797","We present a new tongue detection system called SITA, which comprises only a Kinect device and conventional laptop computer. In contrast with other tongue-based devices, the SITA system does not require the subject to wear a device. This avoids the issue of oral hygiene and removes the risk of swallowing a device inserted in the mouth. In this paper, we introduce the SITA system and an application. To evaluate the system, a user test was conducted. The results indicate that the system could detect the tongue position in real time. Moreover, there are possibilities of training the tongue with this system.",1,25.0
UIST,5fec63334c99755cf0ff8e1edc67b206cdb2343a,UIST,2013,A tongue training system for children with down syndrome,"Masato Miyauchi, Takashi Kimura, Takuya Nojima","2784515, 3120616, 1797797","Children with Down syndrome have a variety of symptoms including speech and swallowing disorders. To improve these symptoms, tongue training is thought to be beneficial. However, inducing children with Down syndrome to do such training is not easy because tongue training can be an unpleasant experience for children. In addition, with no supporting technology for such training, teachers and families around such children must make efforts to induce them to undergo the training. In this research, we develop an interactive tongue training system especially for children with Down syndrome using SITA (Simple Interface for Tongue motion Acquisition) system. In this paper, we describe in detail our preliminary evaluations of SITA, and present the results of user tests.",3,45.871559633
UIST,b8971c007282d98a19e5fa213e794d30f35aa7bc,UIST,2016,RadarCat: Radar Categorization for Input & Interaction,"Hui-Shyong Yeo, Gergely Flamich, Patrick Schrempf, David Harris-Birtill, Aaron Quigley","2602737, 3492156, 3491595, 3492006, 3378059","In RadarCat we present a small, versatile radar-based system for material and object classification which enables new forms of everyday proximate interaction with digital devices. We demonstrate that we can train and classify different types of materials and objects which we can then recognize in real time. Based on established research designs, we report on the results of three studies, first with 26 materials (including complex composite objects), next with 16 transparent materials (with different thickness and varying dyes) and finally 10 body parts from 6 participants. Both leave one-out and 10-fold cross-validation demonstrate that our approach of classification of radar signals using random forest classifier is robust and accurate. We further demonstrate four working examples including a physical object dictionary, painting and photo editing application, body shortcuts and automatic refill based on RadarCat. We conclude with a discussion of our results, limitations and outline future directions.",1,92.7672955975
UIST,2e9729e481a8e9b43e96b0bc39026cc7095b495d,UIST,2013,Physink: sketching physical behavior,"Jeremy Scott, Randall Davis","1861338, 1735802","Describing device behavior is a common task that is currently not well supported by general animation or CAD software. We present PhysInk, a system that enables users to demonstrate 2D behavior by sketching and directly manipulating objects on a physics-enabled stage. Unlike previous tools that simply capture the user's animation, PhysInk captures an understanding of the behavior in a timeline. This enables useful capabilities such as causality-aware editing and finding physically-correct equivalent behavior. We envision PhysInk being used as a physics teacher's sketchpad or a WYSIWYG tool for game designers.",1,27.0642201835
UIST,0430fee3edb94c609dc4e6b668cb8245505da12d,UIST,2004,SketchREAD: a multi-domain sketch recognition engine,"Christine Alvarado, Randall Davis","1836654, 1735802","We present SketchREAD, a multi-domain sketch recognition engine capable of recognizing freely hand-drawn diagrammatic sketches. Current computer sketch recognition systems are difficult to construct, and either are fragile or accomplish robustness by severely limiting the designer's drawing freedom. Our system can be applied to a variety of domains by providing structural descriptions of the shapes in that domain; no training data or programming is necessary. Robustness to the ambiguity and uncertainty inherent in complex, freely-drawn sketches is achieved through the use of context. The system uses context to guide the search for possible interpretations and uses a novel form of dynamically constructed Bayesian networks to evaluate these interpretations. This process allows the system to recover from low-level recognition errors (e.g., a line misclassified as an arc) that would otherwise result in domain level recognition errors. We evaluated Sketch-READ on real sketches in two domains--family trees and circuit diagrams--and found that in both domains the use of context to reclassify low-level shapes significantly reduced recognition error over a baseline system that did not reinterpret low-level classifications. We also discuss the system's potential role in sketch based user interfaces.",126,96.0526315789
UIST,155c8907493334bc5657e1367c9ef3c1943aa824,UIST,2016,Depth Based Shadow Pointing Interface for Public Displays,"Jun Shingu, Patrick Chiu, Sven G. Kratz, Jim Vaughan, Don Kimber","2565333, 2895008, 3103005, 2133761, 2178004","We propose a robust pointing detection with virtual shadow representation for interacting with a public display. Using a depth camera, our shadow is generated by a model with an angled virtual sun light and detects the nearest point as a pointer. The position of the shadow becomes higher when user walks closer, which conveys the notion of correct distance to control the pointer and offers accessibility to the higher area of the display.",0,44.6540880503
UIST,cc695b47f06c1fab24d3aeb1c623f05fcd382ac3,UIST,2016,WristWhirl: One-handed Continuous Smartwatch Input using Wrist Gestures,"Jun Gong, Xing-Dong Yang, Pourang Irani","5801252, 1791070, 1773923","We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist's bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a $1 gesture recognizer with an accuracy of 93.8% and by three human inspectors with an accuracy of 85%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.",0,44.6540880503
UIST,1ca941f5daa78d146cd48eeb0a2c7f5f40099775,UIST,2003,TalkBack: a conversational answering machine,"Vidya Lakshmipathy, Chris Schmandt, Natalia Marmasse","2098674, 1729321, 2584860","Current asynchronous voice messaging interfaces, like voicemail, fail to take advantage of our conversational skills. TalkBack restores conversational turn-taking to voicemail retrieval by dividing voice messages into smaller sections based on the most significant silent and filled pauses and pausing after each to record a response. The responses are composed into a reply, alternating with snippets of the original message for context. TalkBack is built into a digital picture frame; the recipient touches a picture of the caller to hear each segment of the message in turn. The minimal interface models synchronous interaction and facilitates asynchronous voice messaging. TalkBack can also present a voice-annotated slide show which it receives over the Internet.",9,6.25
UIST,35bf49252dcda18c026212f60aebf2957613be16,UIST,2010,What interfaces mean: a history and sociology of computer windows,Louis-Jean Teitelbaum,7663639,"This poster presents a cursory look at the history of windows in Graphical User Interfaces. It examines the controversy between tiling and overlapping window managers and explains that controversy's sociological importance: windows are control devices, enabling their users to manage their activity and attention. It then explores a few possible reasons for the relative disappearance of windowing in recent computing devices. It concludes with a recapitulative typology.",0,9.3023255814
UIST,8f4fbec6053b15603e7ba750fde132bbefd81b5e,UIST,2012,DataPlay: interactive tweaking and example-driven correction of graphical database queries,"Azza Abouzeid, Joseph M. Hellerstein, Abraham Silberschatz","2827658, 1695576, 1688159","Writing complex queries in SQL is a challenge for users. Prior work has developed several techniques to ease query specification but none of these techniques are applicable to a particularly difficult class of queries: <i>quantified queries</i>. Our hypothesis is that users prefer to specify quantified queries interactively by <i>trial-and-error</i>. We identify two impediments to this form of interactive trial-and-error query specification in SQL: (i) changing quantifiers often requires global syntactical query restructuring, and (ii) the absence of <i>non-answers</i> from SQL's results makes verifying query correctness difficult. We remedy these issues with <i>DataPlay</i>, a query tool with an underlying graphical query language, a unique data model and a graphical interface. DataPlay provides two interaction features that support trial-and-error query specification. First, DataPlay allows users to <i>directly manipulate</i> a graphical query by changing quantifiers and modifying dependencies between constraints. Users receive real-time feedback in the form of updated answers and non-answers. Second, DataPlay can <i>auto-correct</i> a user's query, based on user feedback about which tuples to keep or drop from the answers and non-answers. We evaluated the effectiveness of each interaction feature with a user study and we found that direct query manipulation is more effective than auto-correction for simple queries but auto-correction is more effective than direct query manipulation for more complex queries.",19,75.4901960784
UIST,20bf1e162ad94c131fc885f9ea3e38c3f8fe425a,UIST,2013,Augmenting the input space of portable displays using add-on hall-sensor grid,Rong-Hao Liang,1705512,"Since handheld and wearable displays are highly mobile, various applications are enabled to enrich our daily life. In addition to displaying high-fidelity information, these devices also support natural and effective user interactions by exploiting the capability of various embedded sensors. Nonetheless, the set of built-in sensors has limitations. Add-on sensor technologies, therefore, are needed. This work chooses to exploit magnetism as an additional channel of user input. The author first explains the reasons of developing the add-on magnetic field sensing technology based on neodymium magnets and the analog Hall-sensor grid. Then, the augmented input space is showcased through two instances. 1) For handheld displays, the sensor extends the object tracking capability to the near-surface 3D space by simply attaching it to the back of devices. 2) For wearable displays, the sensor enables private and rich-haptic 2D input by wearing it on user's fingernails. Limitations and possible research directions of this approach are highlighted in the end of paper.",4,51.8348623853
UIST,6f8142145064c2ae961e898629b21c3b05add2a4,UIST,2011,Injured person information management during second triage,"Yuki Takahashi, Hiroaki Kojima, Ken-ichi Okada","2529056, 2535975, 1695617","In a large-scale disaster in which many persons are injured at the same time, triage has been introduced. Triage is a method that temporarily delays the treatment of people with mild to moderate injuries and symptoms and gives priority to those in a critical condition. In the process of multiple triage, more specific information is needed in the second triage compared to the first to accurately prioritize the persons injuries and state. To solve this problem we proposed and constructed a touch-based interface for managing information inserted during second triage. A touch-based tablet interface is introduced to specify wound areas and gestures for wound types. The information is shared wirelessly with all emergency personnel, giving medics shared, data-centric, visibility of overall triage status for the first time. The evaluation experiment shows that this proposed system enables to reduce input errors, speed up injured person care, and facilitate information sharing between medics efficiently. As a result, we believe that many more injured persons can and will be saved.",0,6.19047619048
UIST,6fa10d56b406acb816f894bea437be9025a9ec39,UIST,2010,SqueezeBlock: using virtual springs in mobile devices for eyes-free interaction,"Sidhant Gupta, Tim Campbell, Jeffrey R. Hightower, Shwetak N. Patel","2856837, 2083320, 2725872, 1701358","Haptic feedback provides an additional interaction channel when auditory and visual feedback may not be appropriate. We present a novel haptic feedback system that changes its elasticity to convey information for eyes-free interaction. SqueezeBlock is an electro-mechanical system that can realize a virtual spring having a programmatically controlled spring constant. It also allows for additional haptic modalities by altering the Hooke's Law linear-elastic force- displacement equation, such as non-linear springs, size changes, and spring length (range of motion) variations. This ability to program arbitrarily spring constants also allows for ""click"" and button-like feedback. We present several potential applications along with results from a study showing how well participants can distinguish between several levels of stiffness, size, and range of motion. We conclude with implications for interaction design.",11,63.3720930233
UIST,da4f3034c16175ea86c12b839ad23346a03a066e,UIST,2010,ImpAct: enabling direct touch and manipulation for surface computing,"Anusha Indrajith Withana, Makoto Kondo, Kakehi Gota, Yasutoshi Makino, Maki Sugimoto, Masahiko Inami","3209849, 3188369, 2562353, 1712395, 1792570, 1684930","This paper explores direct touch and manipulation techniques for surface computing platforms using a special force feedback stylus named ImpAct(Immersive Haptic Augmentation for Direct Touch). Proposed haptic stylus can change its length when it is pushed against a display surface. Correspondingly, a virtual stem is rendered inside the display area so that user perceives the stylus immersed through to the digital space below the screen. We propose ImpAct as a tool to probe and manipulate digital objects in the shallow region beneath display surface. ImpAct creates a direct touch interface by providing kinesthetic haptic sensations along with continuous visual contact to digital objects below the screen surface.",2,33.1395348837
UIST,9e9809b285ba769fb3a92257d926709422555c0e,UIST,2012,User interface toolkit mechanisms for securing interface elements,"Franziska Roesner, James Fogarty, Tadayoshi Kohno","3268360, 1738171, 1769675","User interface toolkit research has traditionally assumed that developers have full control of an interface. This assumption is challenged by the mashup nature of many modern interfaces, in which different portions of a single interface are implemented by multiple, potentially mutually distrusting developers (e.g., an Android application embedding a third-party advertisement). We propose considering security as a primary goal for user interface toolkits. We motivate the need for security at this level by examining today's mashup scenarios, in which security and interface flexibility are not simultaneously achieved. We describe a security-aware user interface toolkit architecture that secures interface elements while providing developers with the flexibility and expressivity traditionally desired in a user interface toolkit. By challenging trust assumptions inherent in existing approaches, this architecture effectively addresses important interface-level security concerns.",8,55.8823529412
UIST,a2b4fd6ba9c8a4fe6b78724d4721632667b87159,UIST,2005,Predictive interaction using the delphian desktop,"Takeshi Asano, Ehud Sharlin, Yoshifumi Kitamura, Kazuki Takashima, Fumio Kishino","1991596, 1800680, 1690228, 1725740, 3289052","This paper details the design and evaluation of the <i>Delphian Desktop</i>, a mechanism for online spatial prediction of cursor movements in a Windows-Icons-Menus-Pointers (WIMP) environment. Interaction with WIMP-based interfaces often becomes a spatially challenging task when the physical interaction mediators are the common mouse and a high resolution, physically large display screen. These spatial challenges are especially evident in overly crowded Windows desktops. The <i>Delphian Desktop</i> integrates simple yet effective predictive spatial tracking and selection paradigms into ordinary WIMP environments in order to simplify and ease pointing tasks. Predictions are calculated by tracking cursor movements and estimating spatial intentions using a computationally inexpensive online algorithm based on estimating the movement direction and peak velocity. In testing the <i>Delphian Desktop</i> effectively shortened pointing time to faraway icons, and reduced the overall physical distance the mouse (and user hand) had to mechanically traverse.",55,64.5161290323
UIST,4bf4e8b0435e5a6df994650fc77aaa7afe773337,UIST,2016,ExtendedHand on Wheelchair,"Yuki Asai, Yuta Ueda, Ryuichi Enomoto, Daisuke Iwai, Kosuke Sato","2307500, 1882645, 3492651, 1752105, 1808503","In this paper, we present a novel welfare system which utilizes a spatial augmented reality technique. Hand is a crucial component in human-human communication. For example, we can intuitively indicate an object or place by reaching and pointing it to nearby partners. Unfortunately, for wheelchair users, such communication is often limited because their reaching ranges are narrow, and moving their bodies to the target is tiresome. To solve this issue, we propose a novel wheelchair system on which a battery-powered mobile projector is mounted. A user manipulates the projected virtual hand as an extension of the real one using a touch panel equipped on an armrest of the wheelchair. We implement our proposed system and demonstrate the effectiveness.",0,44.6540880503
UIST,2730b3ea67c77a93ea84595be4d5dd11dd807882,UIST,2010,Tweeting halo: clothing that tweets,"Wai Shan Ng, Ehud Sharlin","2955767, 1800680","People often like to express their unique personalities, interests, and opinions. This poster explores new ways that allow a user to express her feelings in both physical and virtual settings. With our <i>Tweeting Halo</i>, we demonstrate how a wearable lightweight projector can be used for self-expression very much like a hairstyle, makeup or a T-shirt imprint. Our current prototype allows a user to post a message physically above their head and virtually on Twitter at the same time. We also explore simple ways that will allow physical followers of the <i>Tweeting Halo</i> user to easily become virtual followers by simply taking a snapshot of her projected tweet with a mobile device such as a camera phone. In this extended abstract we present our current prototype, and the results of a design critique we performed using it.",2,33.1395348837
UIST,858f5b67ac75dad6d9979c976eaeedddc50abb74,UIST,2016,"Habitsourcing: Sensing the Environment through Immersive, Habit-Building Experiences","Katherine Lin, Henry Spindell, Scott Cambo, Yongsung Kim, Haoqi Zhang","4544768, 3492147, 3492113, 2480638, 3162562","Citizen science and communitysensing applications allow everyday citizens to collect data about the physical world to benefit science and society. Yet despite successes, current approaches are still limited by the number of domain-interested volunteers who are willing and able to contribute useful data. In this paper we introduce <i>habitsourcing</i>, an alternative approach that harnesses the habit-building practices of millions of people to collect environmental data. To support the design and development of habitsourcing apps, we present (1) interaction techniques and design principles for <i>sensing through actuation</i>, a method for acquiring sensing data from cued interactions; and (2) <i>ExperienceKit</i>, an iOS library that makes it easy for developers to build and test habitsourcing applications. In two experiments, we show that our two proof-of-concept apps, <i>ZenWalk</i> and <i>Zombies Interactive</i>, compare favorably to their non-data collecting counterparts, and that we can effectively extract environmental data using simple detection techniques.",0,44.6540880503
UIST,0fac038d7e6ea3ae8243a933e79ab76c490eb885,UIST,2013,Improving structured data entry on mobile devices,"Kerry Shih-Ping Chang, Brad A. Myers, Gene M. Cahill, Soumya Simanta, Edwin J. Morris, Grace A. Lewis","1750695, 1707801, 2529527, 1689046, 1727318, 1723579","Structure makes data more useful, but also makes data entry more cumbersome. Studies have found that this is especially true on mobile devices, as mobile users often reject structured personal information management tools because the structure is too restrictive and makes entering data slower. To overcome these problems, we introduce a new data entry technique that lets users create customized structured data in an unstructured manner. We use a novel notepad-like editing interface with built-in data detectors that allow users to specify structured data implicitly and reuse the structures when desired. To minimize the amount of typing, it provides intelligent, context-sensitive autocomplete suggestions using personal and public databases that contain candidate information to be entered. We implemented these mechanisms in an example application called Listpad. Our evaluation shows that people using Listpad create customized structured data 16% faster than using a conventional mobile database tool. The speed further increases to 42% when the fields can be autocompleted.",3,45.871559633
UIST,c97cf0b85662d16bbb6d126dadb3443bfaa5f9fc,UIST,2007,Socially augmenting employee profiles with people-tagging,"Stephen Farrell, Tessa A. Lau, Stefan Nusser, Eric Wilcox, Michael J. Muller","1773767, 1800706, 1772974, 8493409, 1693243","Employee directories play a valuable role in helping people find others to collaborate with, solve a problem, or provide needed expertise. Serving this role successfully requires accurate and up-to-date user profiles, yet few users take the time to maintain them. In this paper, we present a system that enables users to tag other users with key words that are displayed on their profiles. We discuss how people-tagging is a form of social bookmarking that enables people to organize their contacts into groups, annotate them with terms supporting future recall, and search for people by topic area. In addition, we show that people-tagging has a valuable side benefit: it enables the community to collectively maintain each others' interest and expertise profiles. Our user studies suggest that people tag other people as a form of contact management and that the tags they have been given are accurate descriptions of their interests and expertise. Moreover, none of the people interviewed reported offensive or inappropriate tags. Based on our results, we believe that peopletagging will become an important tool for relationship management in an organization.",58,75.0
UIST,ee82cf3ba0e36161c22bb47037095e0bdb247d2f,UIST,2013,The nudging technique: input method without fine-grained pointing by pushing a segment,"Shota Yamanaka, Homei Miyashita","2735982, 1796760","The Nudging Technique is a new manipulation paradigm for GUIs. With traditional techniques, the user sometimes has to perform a fine-grained operation (e.g., pointing at the edge of a window to resize). When the user makes a mistake in the pointing, problems may arise such as an accidental switching of the foreground window. The nudging technique relieves the user from the fine pointing before dragging; the user just moves the cursor to a target then pushes it. Visual and acoustic feedbacks also help the user's operation. We describe two application examples: window resizing and spreadsheet cell resizing systems.",0,10.5504587156
UIST,1be06072af9c6ed19dacaef9f273fb907a7c8d33,UIST,2014,Vibkinesis: notification by direct tap and 'dying message' using vibronic movement controllable smartphones,"Shota Yamanaka, Homei Miyashita","2735982, 1796760","We propose Vibkinesis, a smartphone that can control its angle and directions of movement and rotation. By separately controlling the vibration motors attached to it, the smartphone can move on a table in the direction it chooses. Vibkinesis can inform a user of a message received when the user is away from the smartphone by changing its orientation, e.g., the smartphone has rotated 90&#176; to the left before the user returns to the smartphone. With this capability, Vibkinesis can notify the user of a message even if the battery is discharged. We also extend the sensing area of Vibkinesis by using an omni-directional lens so that the smartphone tracks the surrounding objects. This allows Vibkinesis to tap the user's hand. These novel interactions expand the mobile device's movement area, notification channels, and notification time span.",4,57.3643410853
UIST,7c20fb10832002e6aab6c0209d6b474da79d564c,UIST,2005,Mediating photo collage authoring,"Nicholas Diakopoulos, Irfan A. Essa","2943892, 1714295",The medium of collage supports the visualization of meaningful event summaries using photographs. It can however be rather tedious to author a collage from a large collection of photographs. In this work we present an approach that supports efficient construction of a collage by assisting the user with an automatic layout procedure that can be controlled at a high level. Our layout method utilizes a pre-designed template which consists of cells for photos and annotations applied to these cells. The layout is then filled by matching the metadata of photos to the annotations in the cells using an optimization algorithm. The user exercises flexibility in the authoring process by (a) maintaining high-level control through the types of constraints applied and (b) leveraging visual emphases supported by the layout algorithm. The user can of course provide fine-grained control of the final collage through direct manipulation. Off-loading the tedium of collage construction to a user controlled yet automated process clears the way for rapidly generating different views of the same album and could also support the increased sharing of digital photos in the form of compact collages.,24,40.3225806452
UIST,0e9eebdfaf18cd072649e05a8059a15e5801d395,UIST,2015,"Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming",Philip J. Guo,2251384,"One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.",5,80.701754386
UIST,1aec84b5619f22354b344677e8c526825e5bae97,UIST,2014,OverCode: visualizing variation in student solutions to programming problems at scale,"Elena L. Glassman, Jeremy Scott, Rishabh Singh, Philip J. Guo, Rob Miller","2459941, 1861338, 3274303, 2251384, 1723785","In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets teachers further filter and cluster solutions based on different criteria. We evaluated OverCode against a nonclustering baseline in a within-subjects study with 24 teaching assistants and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is relevant to more students' solutions.",19,91.8604651163
UIST,9866694d56e51703c4ecec9e2980689a48163d51,UIST,2006,Videotater: an approach for pen-based digital video segmentation and tagging,"Nicholas Diakopoulos, Irfan A. Essa","2943892, 1714295","The continuous growth of media databases necessitates development of novel visualization and interaction techniques to support management of these collections. We present <i>Videotater</i>, an experimental tool for a Tablet PC that supports the efficient and intuitive navigation, selection, segmentation, and tagging of video. Our veridical representation immediately signals to the user where appropriate segment boundaries should be placed and allows for rapid review and refinement of manually or automatically generated segments. Finally, we explore a distribution of modalities in the interface by using multiple timeline representations, pressure sensing, and a tag painting/erasing metaphor with the pen.",17,22.5
UIST,112158fa14b493548c271cbfea420062a251242f,UIST,2014,AirPincher: a handheld device for recognizing delicate mid-air hand gestures,"Kyeongeun Seo, Hyeonjoong Cho","3131371, 2420548","We propose AirPincher, a handheld device for recognizing delicate mid-air hand gestures. AirPincher is designed to overcome disadvantages of the two kinds of existing hand gesture-aware techniques such as wearable sensor-based and external vision-based. The wearable sensor-based techniques cause cumbersomeness of wearing sensors every time and the external vision-based techniques incur performance dependence on distance between a user and a remote display. AirPincher allows a user to hold the device in one hand and to generate several delicate mid-air finger gestures. The gestures are captured by several sensors proximately embedded into AirPincher. These features help AirPincher avoid the aforementioned disadvantages of the existing techniques. It allows several delicate finger gestures, for example, rubbing a thumb against a middle finger, swiping with a thumb on an index finger, pinching with a thumb and an index finger, etc. Due to the inherent haptic feedback of these gestures, AirPincher eventually supports the eyes-free interaction. To validate AirPincher's feasibility, we implemented two use cases, i.e., controlling a pointing cursor and moving a virtual 3D object on the remote screen.",0,12.015503876
UIST,745f761bc0d716894ce38e858337b2b042201544,UIST,2014,Data-driven interaction techniques for improving navigation of educational videos,"Juho Kim, Philip J. Guo, Carrie J. Cai, Shang-Wen Li, Krzysztof Z. Gajos, Rob Miller","1800981, 2251384, 2657025, 2530311, 1770992, 1723785","With an unprecedented scale of learners watching educational videos on online platforms such as MOOCs and YouTube, there is an opportunity to incorporate data generated from their interactions into the design of novel video interaction techniques. Interaction data has the potential to help not only instructors to improve their videos, but also to enrich the learning experience of educational video watchers. This paper explores the design space of data-driven interaction techniques for educational video navigation. We introduce a set of techniques that augment existing video interface widgets, including: a 2D video timeline with an embedded visualization of collective navigation traces; dynamic and non-linear timeline scrubbing; data-enhanced transcript search and keyword summary; automatic display of relevant still frames next to the video; and a visual summary representing points with high learner activity. To evaluate the feasibility of the techniques, we ran a laboratory user study with simulated learning tasks. Participants rated watching lecture videos with interaction data to be efficient and useful in completing the tasks. However, no significant differences were found in task performance, suggesting that interaction data may not always align with moment-by-moment information needs during the tasks.",21,94.1860465116
UIST,b11df1b644d016b75e3fcec4ddfd883fc4d474a0,UIST,2014,PrintScreen: fabricating highly customizable thin-film touch-displays,"Simon Olberding, Michael Wessely, Jürgen Steimle","3186111, 2886214, 1790324","PrintScreen is an enabling technology for digital fabrication of customized flexible displays using thin-film electroluminescence (TFEL). It enables inexpensive and rapid fabrication of highly customized displays in low volume, in a simple lab environment, print shop or even at home. We show how to print ultra-thin (120 &#181;m) segmented and passive matrix displays in greyscale or multi-color on a variety of deformable and rigid substrate materials, including PET film, office paper, leather, metal, stone, and wood. The displays can have custom, unconventional 2D shapes and can be bent, rolled and folded to create 3D shapes. We contribute a systematic overview of graphical display primitives for customized displays and show how to integrate them with static print and printed electronics. Furthermore, we contribute a sensing framework, which leverages the display itself for touch sensing. To demonstrate the wide applicability of PrintScreen, we present application examples from ubiquitous, mobile and wearable computing.",21,94.1860465116
UIST,9036adfbeccd96b78b3341b7e9e5327922ad2670,UIST,2015,Improving Virtual Keyboards When All Finger Positions Are Known,"Daewoong Choi, Hyeonjoong Cho, Joono Cheong","2229526, 2420548, 2301965","Current virtual keyboards are known to be slower and less convenient than physical QWERTY keyboards because they simply imitate the traditional QWERTY keyboards on touchscreens. In order to improve virtual keyboards, we consider two reasonable assumptions based on the observation of skilled typists. First, the keys are already assigned to each finger for typing. Based on this assumption, we suggest restricting each finger to entering pre-allocated keys only. Second, non-touching fingers move in correlation with the touching finger because of the intrinsic structure of human hands. To verify of our assumptions, we conducted two experiments with skilled typists. In the first experiment, we statistically verified the second assumption. We then suggest a novel virtual keyboard using our observations. In the second experiment, we show that our suggested keyboard outperforms existing virtual keyboards.",0,16.2280701754
UIST,c1144e226847442f4cdfe53191afd58ed41b6140,UIST,2009,Contact area interaction with sliding widgets,Tomer Moscovich,2966785,"We show how to design touchscreen widgets that respond to a finger's contact area. In standard touchscreen systems a finger often appears to touch several screen objects, but the system responds as though only a single pixel is touched. In contact area interaction all objects under the finger respond to the touch. Users activate control widgets by sliding a movable element, as though flipping a switch. These Sliding Widgets resolve selection ambiguity and provide designers with a rich vocabulary of self-disclosing interaction mechanism. We showcase the design of several types of Sliding Widgets, and report study results showing that the simplest of these widgets, the Sliding Button, performs on-par with medium-sized pushbuttons and offers greater accuracy for small-sized buttons.",32,54.2857142857
UIST,2001775776a2310edeb7ad5f6bc29623700fc211,UIST,2013,MenuOptimizer: interactive optimization of menu systems,"Gilles Bailly, Antti Oulasvirta, Timo Kötzing, Sabrina Hoppe","1693915, 2663734, 1751539, 2856697","Menu systems are challenging to design because design spaces are immense, and several human factors affect user behavior. This paper contributes to the design of menus with the goal of interactively assisting designers with an optimizer in the loop. To reach this goal, 1) we extend a predictive model of user performance to account for expectations as to item groupings; 2) we adapt an ant colony optimizer that has been proven efficient for this class of problems; and 3) we present MenuOptimizer, a set of inter-actions integrated into a real interface design tool (QtDesigner). MenuOptimizer supports designers' abilities to cope with uncertainty and recognize good solutions. It allows designers to delegate combinatorial problems to the optimizer, which should solve them quickly enough without disrupting the design process. We show evidence that satisfactory menu designs can be produced for complex problems in minutes.",13,74.3119266055
UIST,1b497d32e028e61b2fe084b51f3060769e4d1498,UIST,2016,"Aesthetic Electronics: Designing, Sketching, and Fabricating Circuits through Digital Exploration","Joanne Lo, César Torres, Isabel Yang, Jasper O'Leary, Danny Kaufman, Wilmot Li, Mira Dontcheva, Eric Paulos","8551648, 4769628, 2521884, 3409825, 3492074, 2812691, 2875493, 2749792","As interactive electronics become increasingly intimate and personal, the design of circuitry is correspondingly developing a more playful and creative aesthetic. Circuit sketching and design is a multidimensional activity which combines the arts, crafts, and engineering broadening participation of electronic creation to include makers of diverse backgrounds. In order to support this design ecology, we present Ellustrate, a digital design tool that enables the functional and aesthetic design of electronic circuits with multiple conductive and dielectric materials. Ellustrate guides users through the fabrication and debugging process, easing the task of practical circuit creation while supporting designers' aesthetic decisions throughout the circuit authoring workflow. In a formal user study, we demonstrate how Ellustrate enables a new electronic design conversation that combines electronics, materials, and visual aesthetic concerns.",0,44.6540880503
UIST,63b76f438e2e1e74390d73bf3338ed154b92ae7d,UIST,1991,A model for input and output of multilingual text in a windowing environment,"Yutaka Kataoka, Masato Morisaki, Hiroshi Kuribayashi, Hiroyoshi Ohara","2185813, 2384790, 8628331, 2229613","A multilingual Input/Output (1/0) system has been designed based on topological studies of writing conventions of major world languages. Designed as a lay-ered structure, it unifies common features of writing conventions and is intended to ease international and local function alities. The input module of the intern a-tzonalizatzon layer converts phonograms to ideograms. The corresponding output module displays position-independent characters and position-dependent characters. The localization layer positions highly language-specific functions outside the structure. These functions are integrated as tables and servers to add new languages without the necessity of compilation. The I/O system interactively generates both stateful and stateless code sets. Beyond the boundaries of the PO SIX locale model, the system generates ISO 2022, IS O/DIS 10646, and Compound Text, defined for the interchange encoding format in .X11 protocols, for basic polyglot text processing. Possessing the capability of generating multilingual code sets, this I/O Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direGt commercial advantage, the AC?vl copyright notloe and the title of the publication and its date appear, and notice ia given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. system clearly shows that code sets should be selected by applications with purposes beyond the selection of one element from a set of localization. Likewise, func-tionality and functions relatinq to text manipulation in an operating applications. A subset of this window system supplying basic system should be determined by such 1/0 system was implemented in the X as a basic X11R5 I/O capability by code set generation and string manipulation to avoid interference from operating systems. To ensure the possibility of polyglot string manipulation , the I/O system clearly should be implemented separately from the operating system with its limitations .",1,6.52173913043
UIST,b658e5c0c5b9fc21519872b5d297ab6c0539bd85,UIST,2012,Speaking with the crowd,"Walter S. Lasecki, Rachel Wesley, Anand Kulkarni, Jeffrey P. Bigham","2598433, 5982068, 5618063, 1744846","Automated systems are not yet able to engage in a robust dialogue with users due the complexity and ambiguity of natural language. However, humans can easily converse with one another and maintain a shared history of past interactions. In this paper, we introduce Chorus, a system that enables real-time, two-way natural language conversation between an end user and a crowd acting as a single agent. Chorus is capable of maintaining a consistent, on-topic conversation with end users across multiple sessions, despite constituent individuals perpetually joining and leaving the crowd. This is enabled by using a curated shared dialogue history.
 Even though crowd members are constantly providing input, we present users with a stream of dialogue that appears to be from a single conversational partner. Experiments demonstrate that dialogue with Chorus displays elements of conversational memory and interaction consistency. Workers were able to answer 84.6% of user queries correctly, demonstrating that crowd-powered communication interfaces can serve as a robust means of interacting with software systems.",2,33.8235294118
UIST,cc1b11debe50be088e92c4431947e4e36dc50e84,UIST,2015,FoveAR: Combining an Optically See-Through Near-Eye Display with Projector-Based Spatial Augmented Reality,"Hrvoje Benko, Eyal Ofek, Feng Zheng, Andrew D. Wilson","2704133, 1735652, 1724559, 1767449","Optically see-through (OST) augmented reality glasses can overlay spatially-registered computer-generated content onto the real world. However, current optical designs and weight considerations limit their diagonal field of view to less than 40 degrees, making it difficult to create a sense of immersion or give the viewer an overview of the augmented reality space. We combine OST glasses with a projection-based spatial augmented reality display to achieve a novel display hybrid, called FoveAR, capable of greater than 100 degrees field of view, view dependent graphics, extended brightness and color, as well as interesting combinations of public and personal data display. We contribute details of our prototype implementation and an analysis of the interactive design space that our system enables. We also contribute four prototype experiences showcasing the capabilities of FoveAR as well as preliminary user feedback providing insights for enhancing future FoveAR experiences.",2,60.5263157895
UIST,0333746cbe16d657505549930019774cd064f406,UIST,2014,Powering interactive intelligent systems with the crowd,Walter S. Lasecki,2598433,"Creating intelligent systems that are able to recognize a user's behavior, understand unrestricted spoken natural language, complete complex tasks, and respond fluently could change the way computers are used in daily life. But fully-automated intelligent systems are a far-off goal -- currently, machines struggle in many real-world settings because problems can be almost entirely unconstrained and can vary greatly between instances. Human computation has been shown to be effective in many of these settings, but is traditionally applied in an offline, batch-processing fashion. My work focuses on a new model of continuous, real-time crowdsourcing that enables interactive crowd-powered systems.",0,12.015503876
UIST,0e4c4df0474c52fa0f20c46ebbfc7be1c2bbc37c,UIST,2015,LegionTools: A Toolkit + UI for Recruiting and Routing Crowds to Synchronous Real-Time Tasks,"Mitchell Gordon, Jeffrey P. Bigham, Walter S. Lasecki","2216996, 1744846, 2598433","We introduce LegionTools, a toolkit and interface for managing large, synchronous crowds of online workers for experiments. This poster contributes the design and implementation of a state-of-the-art crowd management tool, along with a publicly-available, open-source toolkit that future system builders can use to coordinate synchronous crowds of online workers for their systems and studies.
 We describe the toolkit itself, along with the underlying design rationale, in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project. We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems, including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of. While the version of LegionTools discussed here focuses on Amazon's Mechanical Turk platform, it can be easily extended to other platforms as APIs become available.",2,60.5263157895
UIST,e8f44171fdb253382c0fd3bb27e9dbddec9bbe01,UIST,2013,MagGetz: customizable passive tangible controllers on and around conventional mobile devices,"Sungjae Hwang, Myungwook Ahn, KwangYun Wohn","7547522, 2363417, 1969742","This paper proposes user-customizable passive control widgets, called MagGetz, which enable tangible interaction on and around mobile devices without requiring power or wireless connections. This is achieved by tracking and ana-lyzing the magnetic field generated by controllers attached on and around the device through a single magnetometer, which is commonly integrated in smartphones today. The proposed method provides users with a broader interaction area, customizable input layouts, richer physical clues, and higher input expressiveness without the need for hardware modifications. We have presented a software toolkit and several applications using MagGetz.",12,71.1009174312
UIST,60e3afca4e2d97931c025fbb6547e7d6a8464611,UIST,2014,"Sensory triptych: here, near, out there","Eric Paulos, Chris Myers, Rundong Tian, Paxton Paulos","2749792, 5833077, 3148721, 3298540","Sensory Triptych is a set of exploratory, interactive sensors designed for children that invite ""new ways of seeing"" our world from the perspective of the here (the earth, air, and water around us), near (things just out of sight), and out there (orbiting satellites and space junk) using familiar and novel interfaces, affordances, and narratives. We present a series of novel physical design prototypes that reframe sensing technologies for children that foster an early adoption of technology usage for exploring, understanding, communicating, sharing, and changing our world. Finally, we discuss how such designs expand the potential opportunities and landscapes for our future interactive systems and experiences within the UIST community.",3,49.2248062016
UIST,b3b45b319b88015e8976aaffaf8ca4405b04db23,UIST,2015,"MoveableMaker: Facilitating the Design, Generation, and Assembly of Moveable Papercraft","Michelle Annett, Tovi Grossman, Daniel J. Wigdor, George W. Fitzmaurice","2870099, 3313809, 1961958, 1703735","In this work, we explore moveables, i.e., interactive papercraft that harness user interaction to generate visual effects. First, we present a survey of children's books that captured the state of the art of moveables. The results of this survey were synthesized into a moveable taxonomy and informed <i>MoveableMaker</i>, a new tool to assist users in designing, generating, and assembling moveable papercraft. MoveableMaker supports the creation and customization of a number of moveable effects and employs moveable-specific features including animated tooltips, automatic instruction generation, constraint-based rendering, techniques to reduce material waste, and so on. To understand how MoveableMaker encourages creativity and enhances the workflow when creating moveables, a series of exploratory workshops were conducted. The results of these explorations, including the content participants created and their impressions, are discussed, along with avenues for future research involving moveables.",0,16.2280701754
UIST,083c128519eb3592a99e71c5ad24a5c24185006c,UIST,2009,The rise of the expert amateur: DIY culture and citizen science,Eric Paulos,2749792,"We are at an important technological inflection point. Most of our computing systems have been designed and built by professionally trained experts (i.e. us--computer scientists, engineers, and designers) for use in specific domains and to solve explicit problems. Artifacts often called ""user manuals"" traditionally prescribed the appropriate usage of these tools and implied an acceptable etiquette for interaction and experience. A fringe group of individuals usually labeled ""hackers"" or ""nerds"" have challenged this producer-consumer model of technology by hacking novel hardware and software features to ""improve"" our research and products while a similar creative group of technicians called ""artists"" have re-directed the techniques, tools, and tenets of accepted technological usage away from their typical manifestations in practicality and product. Over time the technological artifacts of these fringe groups and the support for their rhetoric have gained them a foothold into computing culture and eroded the established power discontinuities within the practice of computing research. We now expect our computing tools to be driven by an architecture of open participation and democracy that encourages users to add value to their tools and applications as they use them. Similarly, the bar for enabling the design of novel, personal computing systems and ""hardware remixes"" has fallen to the point where many non-experts and novices are readily embracing and creating fascinating and ingenious computing artifacts outside of our official and traditionally sanctioned academic research communities.
 But how have we as ""expert"" practitioners been influencing this discussion? By constructing a practice around the design and development of technology for task based and problem solving applications, we have unintentionally established such work as the status quo for the human computing experience. We have failed in our duty to open up alternate forums for technology to express itself and touch our lives beyond productivity and efficiency. Blinded by our quest for ""smart technologies"" we have forgotten to contemplate the design of technologies to inspire us to be smarter, more curious, and more inquisitive. We owe it to ourselves to rethink the impact we desire to have on this historic moment in computing culture. We must choose to participate in and perhaps lead a dialogue that heralds an expansive new acceptable practice of designing to enable participation by experts and non-experts alike. We are in the milieu of the rise of the ""expert amateur"".
 We must change our mantra: ""not just usability but usefulness and relevancy to our world, its citizens, and our environment"".
 We must design for the world and what matters.
 This means discussing our computing research alongside new keywords such as the economy, the environment, activism, poverty, healthcare, famine, homelessness, literacy, religion, and politics.
 This talk will explore the design territory and potential opportunities for all of us to collaborate and benefit as a society from this cultural movement.",7,12.8571428571
UIST,131b3f3dca845119ae0ce53ffb186bc78453d77b,UIST,2013,NailSense: fingertip force as a new input modality,"Sungjae Hwang, Dongchul Kim, Sang-Won Leigh, KwangYun Wohn","7547522, 5570760, 2123620, 1969742","In this paper, we propose a new interaction technique, called NailSense, which allows users to control a mobile device by hovering and slightly bending/extending fingers behind the device. NailSense provides basic interactions equivalent to that of touchscreen interactions; 2-D locations and binary states (i.e., touch or released) are tracked and used for input, but without any need of touching on the screen. The proposed technique tracks the user's fingertip in real-time and triggers event on color change in the fingernail area. It works with conventional smartphone cameras, which means no additional hardware is needed for its utilization. This novel technique allows users to use mobile devices without occlusion which was a crucial problem in touchscreens, also promising extended interaction space in the air, on desktop, or in everywhere. This new interaction technique is tested with example applications: a drawing app and a web browser.",2,37.6146788991
UIST,053b0fb5ebaceae3ea6abfd62d955fd00e7ad4f1,UIST,2012,Capacitive fingerprinting: exploring user differentiation by sensing electrical properties of the human body,"Chris Harrison, Munehiko Sato, Ivan Poupyrev","1730920, 2131343, 1736819","At present, touchscreens can differentiate multiple points of contact, but not who is touching the device. In this work, we consider how the electrical properties of humans and their attire can be used to support user differentiation on touchscreens. We propose a novel sensing approach based on Swept Frequency Capacitive Sensing, which measures the impedance of a user to the environment (i.e., ground) across a range of AC frequencies. Different people have different bone densities and muscle mass, wear different footwear, and so on. This, in turn, yields different impedance profiles, which allows for touch events, including multitouch gestures, to be attributed to a particular user. This has many interesting implications for interactive design. We describe and evaluate our sensing approach, demonstrating that the technique has considerable promise. We also discuss limitations, how these might be overcome, and next steps.",19,75.4901960784
UIST,87f7b48a61739540ac46326c8e79e6349773fdaf,UIST,2003,PreSense: interaction techniques for finger sensing input devices,"Jun Rekimoto, Takaaki Ishizawa, Carsten Schwesig, Haruo Oba","1685962, 2430508, 2270984, 2370803","Although graphical user interfaces started as imitations of the physical world, many interaction techniques have since been invented that are not available in the real world. This paper focuses on one of these ""previewing"", and how a sensory enhanced input device called ""PreSense Keypad"" can provide a preview for users before they actually execute the commands. Preview important in the real world because it is often not possible to undo an action. This previewable feature helps users to see what will occur next. It is also helpful when the command assignment of the keypad dynamically changes, such as for universal commanders. We present several interaction techniques based on this input device, including menu and map browsing systems and a text input system. We also discuss finger gesture recognition for the PreSense Keypad.",38,43.75
UIST,1eb751cd151710fa064e7caec91191ae9142b018,UIST,2001,Novel interaction techniques for overlapping windows,Michel Beaudouin-Lafon,1682346,"This note presents several techniques to improve window management with overlapping windows: tabbed windows, turning and peeling back windows, and snapping and zipping windows.",66,56.6666666667
UIST,ccfe59d3f8310b76aace66b4d9765e10b3021e97,UIST,2006,Interactive environment-aware display bubbles,"Daniel Cotting, Markus H. Gross","2225147, 1743207","We present a novel display metaphor which extends traditional tabletop projections in collaborative environments by introducing freeform, environment-aware display representations and a matching set of interaction schemes. For that purpose, we map personalized widgets or ordinary computer applications that have been designed for a conventional, rectangular layout into space-efficient bubbles whose warping is performed with a potential-based physics approach. With a set of interaction operators based on laser pointer tracking, these freeform displays can be transformed and elastically deformed using focus and context visualization techniques. We also provide operations for intuitive instantiation of bubbles, cloning, cut &amp; pasting, deletion and grouping in an interactive way, and we allow for user-drawn annotations and text entry using a projected keyboard. Additionally, an optional environment-aware adaptivity of the displays is achieved by imperceptible, realtime scanning of the projection geometry. Subsequently, collision-responses of the bubbles with non-optimal surface parts are computed in a rigid body simulation. The extraction of the projection surface properties runs concurrently with the main application of the system. Our approach is entirely based on off the-shelf, low-cost hardware including DLP-projectors and FireWire cameras.",25,37.5
UIST,cc4eaa5c99037966f141ec25dfb1f9462e4d764f,UIST,2014,"Tohme: detecting curb ramps in google street view using crowdsourcing, computer vision, and machine learning","Kotaro Hara, Jin Sun, Robert Moore, David Jacobs, Jon Froehlich","2172745, 3650050, 7346680, 4731186, 1752433","Building on recent prior work that combines Google Street View (GSV) and crowdsourcing to remotely collect information on physical world accessibility, we present the first 'smart' system, Tohme, that combines machine learning, computer vision (CV), and custom crowd interfaces to find curb ramps remotely in GSV scenes. Tohme consists of two workflows, a human labeling pipeline and a CV pipeline with human verification, which are scheduled dynamically based on predicted performance. Using 1,086 GSV scenes (street intersections) from four North American cities and data from 403 crowd workers, we show that Tohme performs similarly in detecting curb ramps compared to a manual labeling approach alone (F- measure: 84% vs. 86% baseline) but at a 13% reduction in time cost. Our work contributes the first CV-based curb ramp detection system, a custom machine-learning based workflow controller, a validation of GSV as a viable curb ramp data source, and a detailed examination of why curb ramp detection is a hard problem along with steps forward.",7,72.0930232558
UIST,1f75f30d5ca62904d35e5dab0dc1bd11af6ec540,UIST,2015,"Tomo: Wearable, Low-Cost Electrical Impedance Tomography for Hand Gesture Recognition","Yang Zhang, Chris Harrison","4449904, 1730920","We present <i>Tomo</i>, a wearable, low-cost system using Electrical Impedance Tomography (EIT) to recover the interior impedance geometry of a user's arm. This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user's skin. Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband, which can monitor and classify gestures in real-time. We conducted a user study that evaluated two gesture sets, one focused on gross hand gestures and another using thumb-to-finger pinches. Our wrist location achieved 97% and 87% accuracies on these gesture sets respectively, while our arm location achieved 93% and 81%. We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens.",13,99.1228070175
UIST,8919b3cc684bd0708ed4346afc3b22bea3a38312,UIST,1991,The DigitalDesk calculator: tangible manipulation on a desk top display,Pierre Wellner,2751729,"(In this article, the term "" embodied vir-tuality "" is used instead of "" computerized reality "") [Xero91] Xerox paper product manager's office, private communication, 26 June 1991. 6/7 electronic document. It may be possible to implement this with a single slot: why have a separate indoor and out-door? In Figure 4, the solid arrows indicate movements of documents by hand. This desktop could replace a conventional workstation, but some users may want to remain backward-compatible with the workstation style of interaction. The two can be integrated by allowing the user to drag electronic documents into the workstation by hand, and out of the workstation by mouse. The mode of use thus encouraged by this system is for users to continuously move documents back and forth between the physical and electronic media, and to work on a document in both places, depending on which medium seems better suited to the particular task. Ideally, the user should hardly be aware of where the document is. The physical and electronic desktops complement and enhance each other. Conclusion Today's electronic desktop is a kind of "" virtual reality, "" quite separate from the physical desk of the user. A "" computerized reality "" approach to HCI, on the other hand, seeks to add computer functionality to physical objects and environments while allowing people to continue interacting with them as before. The DigitalDesk applies this approach to the desk, and attempts to merge the physical world of paper, pens and tape with the electronic world of the workstation. The DigitalDesk Calculator is an example that illustrates how merging these two worlds can improve the usability of a very simple and well-established application. The style of interaction enabled by a digital desk is different than mouse-based "" direct manipulation, "" because users manipulate both electronic and physical objects by touching them with their fingers. This style of "" tactile manipulation "" makes possible new interaction techniques that offer important advantages over currently used techniques, but it does not preclude Scanner Printer electronic DigitalDesk Figure 4. Moving documents through the desktop paper document document them. Tactile manipulation on a digital desk brings the desk-top back to the desk top, and it offers advantages over conventional workstations that, in some settings, could render them obsolete.",156,100.0
UIST,1fca8fc06fa94c3d9fa4ca64ef98a08e8e22c0f8,UIST,2010,Bringing the field into the lab: supporting capture and replay of contextual data for the design of context-aware applications,"Mark W. Newman, Mark S. Ackerman, Jungwoo Kim, Atul Prakash, Zhenan Hong, Jacob Mandel, Tao Dong","4590190, 1797833, 2868118, 1704708, 2113913, 3086204, 1775174","When designing context-aware applications, it is difficult to for designers in the studio or lab to envision the contextual conditions that will be encountered at runtime. Designers need a tool that can create/re-create naturalistic contextual states and transitions, so that they can evaluate an application under expected contexts. We have designed and developed RePlay: a system for capturing and playing back sensor traces representing scenarios of use. RePlay contributes to research on ubicomp design tools by embodying a structured approach to the capture and playback of contextual data. In particular, RePlay supports: capturing naturalistic data through <i>Capture Probes</i>, encapsulating scenarios of use through <i>Episodes</i>, and supporting exploratory manipulation of scenarios through <i>Transforms</i>. Our experiences using RePlay in internal design projects illustrate its potential benefits for ubicomp design.",5,54.0697674419
UIST,1d9395d5ca007997331a682e081bca727b9a04e1,UIST,2016,A Novel Real Time Monitor System of 3D Printing Layers for Better Slicing Parameter Setting,"Yu-Kai Chiu, Hao-Yu Chang, Wan-ling Yang, Yu-Hsuan Huang, Ouhyoung Ming","3427747, 3428113, 2063213, 6695834, 3413976","We proposed a novel real time monitor system of 3D printer with dual cameras, which capture and reconstruct the printed result layer by layer. With the reconstructed image, we can apply computer vision technique to evaluate the difference with the ideal path generate by G-code. The difference gives us clues to classify which might be the possible factor of the result. Hence we can produce advice to user for better slicing parameter settings. We believe that this system can give helps to beginner or users of 3D printer that struggle in parameter settings in the future.",0,44.6540880503
UIST,47d9f2de92068771399c212d81d3f0f891f66852,UIST,2015,"3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers, and Bristles","Gierad Laput, Xiang 'Anthony' Chen, Chris Harrison","1727999, 2028468, 1730920","We introduce a technique for furbricating 3D printed hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles. We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands. We provide several examples of output, demonstrating the immediate feasibility of our approach using a low cost, commodity printer. Overall, this technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware.",2,60.5263157895
UIST,66354fd5586ce17859526207069383c4dd441da1,UIST,2013,DDMixer2.5D: drag and drop to mix 2.5D video objects,"Tatsuya Kurihara, Makoto Okabe, Rikio Onai","7462700, 3102663, 2086194","We propose a 2.5D video editing system called DDMixer2.5D. 2.5D video contains not only color channels but also a depth channel, which can be recorded easily using recently available depth sensors, such as Microsoft Kinect. Our system employs this depth channel to allow a user to quickly and easily edit video objects by using simple drag-and-drop gestures. For example, a user can copy a video object of a dancing figure from video to video simply by dragging and dropping using finger on the touch screen of a mobile phone handset. In addition, the user can drag to adjust the 3D position in the new video so that contact between foot and floor is preserved and the size of the body is automatically adjusted according to the depth. DDMixer2.5D has other useful functions required for practical use, including object removal, editing 3D camera path, creating of anaglyph 3D video, as well as a timeline interface.",1,27.0642201835
UIST,11a63c43fe3c2f972f27282b9f49ea755930b6c5,UIST,2014,Scalable methods to collect and visualize sidewalk accessibility data for people with mobility impairments,Kotaro Hara,2172745,"Poorly maintained sidewalks pose considerable accessibility challenges for mobility impaired persons; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, I introduce four threads of research that I will conduct for my Ph.D. thesis aimed at creating new methods and tools to provide unprecedented levels of information on the accessibility of streets and sidewalk.",0,12.015503876
UIST,8f45510dbebd97c041a313e95ef55dcf7dff5226,UIST,2016,Peripersonal Space in Virtual Reality: Navigating 3D Space with Different Perspectives,"Jooyeon Lee, Manri Cheon, Seong-Eun Moon, Jong-Seok Lee","3174854, 2958510, 2914869, 5722542","We introduce the concept of ""peripersonal space"" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment.",0,44.6540880503
UIST,20601047ec615e2827dff35a8faf41478d00d726,UIST,2012,RevMiner: an extractive interface for navigating reviews on a smartphone,"Jeff Huang, Oren Etzioni, Luke S. Zettlemoyer, Kevin Clark, Christian Lee","3404131, 1741101, 1982950, 2714423, 2146697","Smartphones are convenient, but their small screens make searching, clicking, and reading awkward. Thus, perusing product reviews on a smartphone is difficult. In response, we introduce <i>RevMiner</i> - a novel smartphone interface that utilizes Natural Language Processing techniques to analyze and navigate reviews. <i>RevMiner</i> was run over 300K Yelp restaurant reviews extracting attribute-value pairs, where attributes represent restaurant attributes such as <i>sushi</i> and <i>service</i>, and values represent opinions about the attributes such as <i>fresh</i> or <i>fast</i>. These pairs were aggregated and used to: 1) answer queries such as ""cheap Indian food"", 2) concisely present information about each restaurant, and 3) identify similar restaurants. Our user studies demonstrate that on a smartphone, participants preferred <i>RevMiner</i>'s interface to tag clouds and color bars, and that they preferred <i>RevMiner</i>'s results to Yelp's, particularly for conjunctive queries (e.g., ""great food and huge portions""). Demonstrations of <i>RevMiner</i> are available at revminer.com.",20,78.9215686275
UIST,1be76b002eb7d48e254c39b76e7c6bcf5d076b87,UIST,2001,Phidgets: easy development of physical interfaces through physical widgets,"Saul Greenberg, Chester Fitchett","1696942, 1813098","Physical widgets or <i>phidgets</i> are to physical user interfaces what widgets are to graphical user interfaces. Similar to widgets, phidgets abstract and package input and output devices: they hide implementation and construction details, they expose functionality through a well-defined API, and they have an (optional) on-screen interactive interface for displaying and controlling device state. Unlike widgets, phidgets also require: a connection manager to track how devices appear on-line; a way to link a software phidget with its physical counterpart; and a simulation mode to allow the programmer to develop, debug and test a physical interface even when no physical device is present. Our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces.",375,96.6666666667
UIST,93b5acd0b3120765889255c9e32151e41976280a,UIST,2011,Detecting shape deformation of soft objects using directional photoreflectivity measurement,"Yuta Sugiura, Kakehi Gota, Anusha Indrajith Withana, Calista Lee, Daisuke Sakamoto, Maki Sugimoto, Masahiko Inami, Takeo Igarashi","1799242, 2562353, 3209849, 2464708, 1762551, 1792570, 1684930, 1717356","We present the FuwaFuwa sensor module, a round, hand-size, wireless device for measuring the shape deformations of soft objects such as cushions and plush toys. It can be embedded in typical soft objects in the household without complex installation procedures and without spoiling the softness of the object because it requires no physical connection. Six LEDs in the module emit IR light in six orthogonal directions, and six corresponding photosensors measure the reflected light energy. One can easily convert almost any soft object into a touch-input device that can detect both touch position and surface displacement by embedding multiple FuwaFuwa sensor modules in the object. A variety of example applications illustrate the utility of the FuwaFuwa sensor module. An evaluation of the proposed deformation measurement technique confirms its effectiveness.",15,60.0
UIST,d4a5eef07507a95bb60825238524649c40bac822,UIST,2010,Surfboard: keyboard with microphone as a low-cost interactive surface,"Jun Kato, Daisuke Sakamoto, Takeo Igarashi","3200463, 1762551, 1717356","We introduce a technique to detect simple gestures of ""surfing"" (moving a hand horizontally) on a standard keyboard by analyzing recorded sounds in real-time with a microphone attached close to the keyboard. This technique allows the user to maintain a focus on the screen while surfing on the keyboard. Since this technique uses a standard keyboard without any modification, the user can take full advantage of the input functionality and tactile quality of his favorite keyboard supplemented with our interface.",1,22.0930232558
UIST,6b5bc505f616af18309ef86dfb7b90648c4e18dc,UIST,2013,FingerPad: private and subtle interaction using fingertips,"Li-Wei Chan, Rong-Hao Liang, Ming-Chang Tsai, Kai-Yin Cheng, Chao-Huai Su, Mike Y. Chen, Wen-Huang Cheng, Bing-Yu Chen","1682665, 1705512, 1894546, 2921999, 2838988, 2335746, 1711298, 1733344","We present FingerPad, a nail-mounted device that turns the tip of the index finger into a touchpad, allowing private and subtle interaction while on the move. FingerPad enables touch input using magnetic tracking, by adding a Hall sensor grid on the index fingernail, and a magnet on the thumbnail. Since it permits input through the pinch gesture, FingerPad is suitable for private use because the movements of the fingers in a pinch are subtle and are naturally hidden by the hand. Functionally, FingerPad resembles a touchpad, and also allows for eyes-free use. Additionally, since the necessary devices are attached to the nails, FingerPad preserves natural haptic feedback without affecting the native function of the fingertips. Through user study, we analyze the three design factors, namely posture, commitment method and target size, to assess the design of the FingerPad. Though the results show some trade-off among the factors, generally participants achieve 93% accuracy for very small targets (1.2mm-width) in the seated condition, and 92% accuracy for 2.5mm-width targets in the walking condition.",38,97.247706422
UIST,1badc5bb4a8bdf36a8dc47991a1de3ae1422205a,UIST,2012,GaussSense: attachable stylus sensing using magnetic sensor grid,"Rong-Hao Liang, Kai-Yin Cheng, Chao-Huai Su, Chien-Ting Weng, Bing-Yu Chen, De-Nian Yang","1705512, 2921999, 2838988, 2665165, 1733344, 1731140","This work presents <i>GaussSense</i>, which is a back-of-device sensing technique for enabling input on an arbitrary surface using stylus by exploiting magnetism. A 2mm-thick Hall sensor grid is developed to sense magnets that are embedded in the stylus. Our system can sense the magnetic field that is emitted from the stylus when it is within 2cm of any non-ferromagnetic surface. Attaching the sensor behind an arbitrary thin surface enables the stylus input to be recognized by analyzing the distribution of the applied magnetic field. Attaching the sensor grid to the back of a touchscreen device and incorporating magnets into the corresponding stylus enable the system 1) to distinguish touch events that are caused by a finger from those caused by the stylus, 2) to sense the tilt angle of the stylus and the pressure with which it is applied, and 3) to detect where the stylus hovers over the screen. A pilot study reveals that people were satisfied with the novel sketching experiences based on this system.",25,83.3333333333
UIST,555282c096f4937aca2e881c4c7c739e63b9d56c,UIST,2011,Pub - point upon body: exploring eyes-free interaction and methods on an arm,"Shu-Yang Lin, Chao-Huai Su, Kai-Yin Cheng, Rong-Hao Liang, Tzu-Hao Kuo, Bing-Yu Chen","4386459, 2838988, 2921999, 1705512, 2487918, 1733344","This paper presents a novel interaction system, PUB (Point Upon Body), to explore eyes-free interaction in a personal space by allowing users tapping on their own arms to be provided with haptic feedback from their skin. Two user studies determine how users can interact precisely with their forearms and how users behave when operating in their arm space. According to those results, normal users can divide their arm space at most into 6 points between their wrists and elbows with iterative practice. Experimental results also indicate that the divided pattern of each user is unique from that of other ones. Based on the design principles from the observations, an interaction system, PUB, is designed to demonstrate how interaction design benefits from those findings. Two scenarios, remote display control and mobile device control, are demonstrated through the UltraSonic device attached on the users' wrists to detect their tapped positions.",23,69.0476190476
UIST,a2b0f39ec46fd0cd283bcacd5108d0d3706cbdea,UIST,2011,The jabberwocky programming environment for structured social computing,"Salman Ahmad, Alexis Battle, Zahan Malkani, Sepandar D. Kamvar","2317437, 2523256, 3034000, 2833700","We present Jabberwocky, a social computing stack that consists of three components: a human and machine resource management system called Dormouse, a parallel programming framework for human and machine computation called ManReduce, and a high-level programming language on top of ManReduce called Dog. Dormouse is designed to enable cross-platform programming languages for social computation, so, for example, programs written for Mechanical Turk can also run on other crowdsourcing platforms. Dormouse also enables a programmer to easily combine crowdsourcing platforms or create new ones. Further, machines and people are both first-class citizens in Dormouse, allowing for natural parallelization and control flows for a broad range of data-intensive applications. And finally and importantly, Dormouse includes notions of real identity, heterogeneity, and social structure. We show that the unique properties of Dormouse enable elegant programming models for complex and useful problems, and we propose two such frameworks. ManReduce is a framework for combining human and machine computation into an intuitive parallel data flow that goes beyond existing frameworks in several important ways, such as enabling functions on arbitrary communication graphs between human and machine clusters. And Dog is a high-level procedural language written on top of ManReduce that focuses on expressivity and reuse. We explore two applications written in Dog: bootstrapping product recommendations without purchase data, and expert labeling of medical images.",59,91.4285714286
UIST,7b4675148ccc373ff2a206abfb1d35374b79ae17,UIST,2013,The dog programming language,"Salman Ahmad, Sepandar D. Kamvar","2317437, 2833700","Today, most popular software applications are deployed in the cloud, interact with many users, and run on multiple platforms from Web browsers to mobile operating systems. While these applications confer a number of benefits to their users, building them brings many challenges: manually managing state between asynchronous user actions, creating and maintaining separate code bases for each desired client platform and gracefully scaling to handle a large number of concurrent users. Dog is a new programming language that provides a solution to these challenges and others through a unique runtime model that allows developers to model scalable cross-client applications as an imperative control-flow --- simplifying many development tasks. In this paper we describe the key features of Dog and show its utility through several applications that are difficult and time-consuming to write in existing languages, but are simple and easily written in Dog in a few lines of code.",2,37.6146788991
UIST,164452a6d60b691365aa2b9f807f273e1c88be17,UIST,2014,Crowd-powered parameter analysis for visual design exploration,"Yuki Koyama, Daisuke Sakamoto, Takeo Igarashi","2196816, 1762551, 1717356","Parameter tweaking is one of the fundamental tasks in the editing of visual digital contents, such as correcting photo color or executing blendshape facial expression control. A problem with parameter tweaking is that it often requires much time and effort to explore a high-dimensional parameter space. We present a new technique to analyze such high-dimensional parameter space to obtain a distribution of human preference. Our method uses crowdsourcing to gather pairwise comparisons between various parameter sets. As a result of analysis, the user obtains a goodness function that computes the goodness value of a given parameter set. This goodness function enables two interfaces for exploration: Smart Suggestion, which provides suggestions of preferable parameter sets, and VisOpt Slider, which interactively visualizes the distribution of goodness values on sliders and gently optimizes slider values while the user is editing. We created four applications with different design parameter spaces. As a result, the system could facilitate the user's design exploration.",5,63.1782945736
UIST,f1b8369ebaf2480fa5f939581a9114b7b0b1145f,UIST,2003,"Rhythm modeling, visualizations and applications","James Begole, John C. Tang, Rosco Hill","1791489, 1808267, 2305602","People use their awareness of others' temporal patterns to plan work activities and communication. This paper presents algorithms for programatically detecting and modeling temporal patterns from a record of online presence data. We describe analytic and end-user visualizations of rhythmic patterns and the tradeoffs between them. We conducted a design study that explored the accuracy of the derived rhythm models compared to user perceptions, user preference among the visualization alternatives, and users' privacy preferences. We also present a prototype application based on the rhythm model that detects when a person is ""away"" for an extended period and predicts their return. We discuss the implications of this technology on the design of computer-mediated communication.",73,58.3333333333
UIST,83f39b71705f3b33502f551497bf2c9dc096cdbf,UIST,2015,Gaze-Shifting: Direct-Indirect Input with Pen and Touch Modulated by Gaze,"Ken Pfeuffer, Jason Alexander, Ming Ki Chong, Yanxia Zhang, Hans-Werner Gellersen","3171800, 2928764, 2577932, 5411713, 4919595","Modalities such as pen and touch are associated with direct input but can also be used for indirect input. We propose to combine the two modes for direct-indirect input modulated by gaze. We introduce gaze-shifting as a novel mechanism for switching the input mode based on the alignment of manual input and the user's visual attention. Input in the user's area of attention results in direct manipulation whereas input offset from the user's gaze is redirected to the visual target. The technique is generic and can be used in the same manner with different input modalities. We show how gaze-shifting enables novel direct-indirect techniques with pen, touch, and combinations of pen and touch input.",7,87.2807017544
UIST,ae9a54333d07df994f84f2d570fd6c6c8e5f60c3,UIST,2012,"An interface agent for non-visual, accessible web automation",Yury Puzis,2620033,"The Web is far less usable and accessible for the users with visual impairments than it is for the sighted people. Web automation has the potential to bridge the divide between the ways visually impaired people and sighted people access the Web, and enable visually impaired users to breeze through Web browsing tasks that beforehand were slow, hard, or even impossible to achieve. Typical automation interfaces require that the user record a macro, a useful sequence of browsing steps, so that these steps can be re-played in the future. In this paper, I present a high-level overview of an approach that enables users to find quickly relevant information on the webpage, and automate browsing without recording macros. This approach is potentially useful both for visually impaired, and sighted users.",1,25.0
UIST,e93defd7df5237f454e7e960978adeb678069689,UIST,2014,Trampoline: a double-sided elastic touch device for creating reliefs,"Jaehyun Han, Jiseong Gu, Geehyuk Lee","7181851, 2042461, 1717371","Although reliefs are frequently used to add patterns to product surfaces, there is a lack of interaction techniques to model reliefs on the surface of virtual objects. We adopted the repouss&#233; and chasing artwork techniques in an alternative interaction technique to model relief on virtual surfaces. To support this interaction technique, we developed the double-sided touchpad Trampoline that can detect the position and force of a finger touch on both sides. Additionally, Trampoline provides users with elastic feedback, as its surface consists of a stretchable fabric. We implemented a relief application with this device and the developed interaction technique. An informal user study showed that the proposed system can be a promising solution to create reliefs.",1,31.007751938
UIST,f74d55c8dadfed5a945379b1a1a79a14ebd3b2d2,UIST,2014,Third surface: an augmented world wide web for the physical world,"Valentin Heun, Kenneth Friedman, Andrew Mendez, Benjamin Reynolds, Kevin Wong, Pattie Maes","3186528, 2120259, 3031035, 2253713, 7551119, 1701876","The ubiquitous use of Augmented Reality (AR) applications is dependent on an easy way of authoring and using content. Present systems depend on specific authoring tools or content delivery systems that provide a limited amount of freedom and content ownership to the author compared to the possibilities of the World Wide Web (WWW). Third Surface is a system that allows the user to publish and use WWW content saved on personal HTTP servers for augmented reality applications in the physical environment. The contribution of this work is a system that allows a web developer to post location-based augmented reality content and AR marker on one's own HTTP server. A Global Location Service (GLS) provides a browsing application with location-based URLs that link the browsing application to content, AR markers, and data for the right positioning of content in the augmented reality interface. The Third Surface has three advantages compared to other concepts. It is globally scalable able to millions of users. The interactive possibilities for developers and users are the same as for the WWW. The developers are in charge of their own content distribution.",1,31.007751938
UIST,2fd321ca02b89c93cc57f14bf4b2b0912dbd3893,UIST,2013,BodyAvatar: creating freeform 3D avatars using first-person body gestures,"Yupeng Zhang, Teng Han, Zhimin Ren, Nobuyuki Umetani, Xin Tong, Yang Liu, Takaaki Shiratori, Xiang Cao","3092404, 3049649, 2961841, 2065148, 1743927, 1742731, 2463857, 7299595","BodyAvatar is a Kinect-based interactive system that allows users without professional skills to create freeform 3D avatars using body gestures. Unlike existing gesture-based 3D modeling tools, BodyAvatar centers around a first-person ""you're the avatar"" metaphor, where the user treats their own body as a physical proxy of the virtual avatar. Based on an intuitive body-centric mapping, the user performs gestures to their own body as if wanting to modify it, which in turn results in corresponding modifications to the avatar. BodyAvatar provides an intuitive, immersive, and playful creation experience for the user. We present a formative study that leads to the design of BodyAvatar, the system's interactions and underlying algorithms, and results from initial user trials.",10,68.3486238532
UIST,2fafd8b528727f4966cff89a1d5aaa0ec37231b7,UIST,2015,PERCs: Persistently Trackable Tangibles on Capacitive Multi-Touch Displays,"Simon Voelker, Christian Cherek, Jan Thar, Thorsten Karrer, Christian Thoresen, Kjell Ivar Øvergård, Jan O. Borchers","2766971, 3358763, 2088123, 3301123, 2906470, 1679262, 1692837","Tangible objects on capacitive multi-touch surfaces are usually only detected while the user is touching them. When the user lets go of such a tangible, the system cannot distinguish whether the user just released the tangible, or picked it up and removed it from the surface. We introduce PERCs, persistent capacitive tangibles that ""know"" whether they are currently on a capacitive touch surface or not. This is achieved by adding a small field sensor to the tangible to detect the touch screen's own, weak electromagnetic touch detection probing signal. Thus, unlike previous designs, PERCs do not get filtered out over time by the adaptive signal filters of the touch screen. We provide a technical overview of the theory be- hind PERCs and our prototype construction, and we evaluate detection rates, timing performance, and positional and angular accuracy for PERCs on a variety of unmodified, commercially available multi-touch devices.Through their affordable circuitry and high accuracy, PERCs open up the potential for a variety of new applications that use tangibles on today's ubiquitous multi-touch devices.",2,60.5263157895
UIST,441b99e1faeb289fb2f8b077bf859baf72d879b6,UIST,2007,Enabling efficient orienteering behavior in webmail clients,"Stefan Nusser, Julian A. Cerruti, Eric Wilcox, Steve B. Cousins, Jerald Schoudt, Sergio Sancho","1772974, 2802890, 8493409, 1705038, 2586995, 2055293","Webmail clients provide millions of end users with convenient and ubiquitous access to electronic mail - the most successful collaboration tool ever. Web email clients are also the platform of choice for recent innovations on electronic mail and for integration of related information services into email. In the enterprise, however, webmail applications have been relegated to being a supplemental tool for mail access from home or while on the road. In this paper, we draw on recent research in the area of electronic mail to understand usage models and performance requirements for enterprise email applications. We then present an innovative architecture for a webmail client. By leveraging recent advances in web browser technology, we show that webmail clients can offer performance and responsiveness that rivals a desktop application while still retaining all the advantages of a browser based client.",3,9.72222222222
UIST,6a9f90b1207d84a5b42d6512d3e3693d4e578220,UIST,2014,Push-push: a two-point touchscreen operation utilizing the pressed state and the hover state,"Jaehyun Han, Sunggeun Ahn, Geehyuk Lee","7181851, 2170752, 1717371","A drag operation is used for many two-point functions in mouse-based graphical user interfaces (GUIs), but its usage in touchscreen GUIs is limited because it is mainly used for scrolling. We propose Push-Push as a second two-point touchscreen operation that is not in conflict with a drag operation. We implemented three application scenarios and showed how Push-Push can be used effectively for other two-point functions while overlapping drag operations are used for scrolling.",1,31.007751938
UIST,27c168bda05eef831697b89d4ce5b337251a2496,UIST,2011,OmniTouch: wearable multitouch interaction everywhere,"Chris Harrison, Hrvoje Benko, Andrew D. Wilson","1730920, 2704133, 1767449","OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are ""clicked"" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.",196,98.0952380952
UIST,45904b307e36c5d52ada7f8ab190dede65079f85,UIST,2015,Fix and Slide: Caret Navigation with Movable Background,"Kenji Suzuki, Kazumasa Okabe, Ryuuki Sakamoto, Daisuke Sakamoto","4471601, 2607064, 1767579, 1762551","We present a concept of using a movable background to navigate a caret on small mobile devices. The standard approach to selecting text on mobile devices is to directly touch the location on the text that a user wants to select. This is problematic because the user's finger hides the area to select. Our concept is to use a movable background to navigate the caret. Users place a caret by tapping on the screen and then move the background by touching and dragging. In this method, the caret is fixed on the screen and the user drags the background text to navigate the caret. We compared our technique with the iPhone's default UI and found that even though participants were using our technique for the first time, average task completion time was not different or even faster than Default UI in the case of the small font size and got a significantly higher usability score than Default UI.",1,42.1052631579
UIST,706000bec95a51ba4549ab8ab8de966c3d8bcdb6,UIST,2000,The metropolis keyboard - an exploration of quantitative techniques for virtual keyboard design,"Shumin Zhai, Michael A. Hunter, Barton A. Smith","1748079, 1681409, 2152839","Text entry user interfaces have been a bottleneck of non-traditional computing devices. One of the promising methods is the virtual keyboard on touch screens. Various layouts have been manually designed to replace the dominant QWERTY layout. This paper presents two computerized quantitative design techniques to search for the optimal virtual keyboard. The first technique simulated the dynamics of a keyboard with "" digraph springs "" between keys, which produced a "" Hooke's "" keyboard with 41.6 wpm performance. The second technique used a Metropolis random walk algorithm guided by a "" Fitts energy "" objective function, which produced a "" Metropolis "" keyboard with 43.1 wpm performance. The paper also models and evaluates the perfo rmance of four existing keyboard layouts. We corrected erroneous estimates in the literature and predicted the performance of QWERTY, CHUBON, FITALY, OPTI to be in the neighborhood of 30, 33, 36 and 38 wpm respectively. Our best design was 40% faster than QWERTY and 10% faster than OPTI, illustrating the advantage of quantitative user interface design techniques based on models of human performance over traditional trial and error designs guided by heuristics.",111,84.0
UIST,6bf63a7c3aa05b3b651640b7e23d3821cabcc2e8,UIST,2004,DART: a toolkit for rapid design exploration of augmented reality experiences,"Blair MacIntyre, Maribeth Gandy Coleman, Steven Dow, Jay David Bolter","1768774, 1874459, 5319364, 2393402","In this paper [MacIntyre et al 2004]. we describe The Designer's Augmented Reality Toolkit (DART). DART is built on top of Macromedia Director, a widely used multimedia development environment. We summarize the most significant problems faced by designers working with AR in the real world, and discuss how DART addresses them. Most of DART is implemented in an interpreted scripting language, and can be modified by designers to suit their needs. Our work focuses on supporting early design activities, especially a rapid transition from storyboards to working experience, so that the experiential part of a design can be tested early and often. DART allows designers to specify complex relationships between the physical and virtual worlds, and supports 3D animatic actors (informal, sketch-based content) in addition to more polished content. Designers can capture and replay synchronized video and sensor data, allowing them to work off-site and to test specific parts of their experience more effectively.",103,89.4736842105
UIST,3a366df8b2360a222663d569f2b082559712f5e6,UIST,2014,Ethereal: a toolkit for spatially adaptive augmented reality content,"Gheric Speiginer, Blair MacIntyre","2865355, 1768774","In this poster, we describe a framework and toolkit (Ethereal) for creating spatially adaptive content based on complex spatial and visual metrics in augmented reality, and demonstrate our approach with an illustrative example.",0,12.015503876
UIST,290e00bfdb19cdba7b5f2563d6935a5c4670ec9d,UIST,2014,"Designer's augmented reality toolkit, ten years later: implications for new media authoring tools","Maribeth Gandy Coleman, Blair MacIntyre","1874459, 1768774","The Designer's Augmented Reality Toolkit (DART) was an augmented (AR) and mixed reality (MR) authoring tool targeted at new media designers. It was released in 2003 and was heavily used by a diverse population of creators for the next several years [28]. Ten years later, we approached a group of users to collect reflections on their use of DART, the artifacts they produced, their subsequent AR/MR authoring, their thoughts on the challenges of AR/MR authoring in general, and the state of modern tools. In this paper we present the findings from in-depth interviews with these DART developers and other AR experts. Their reflections provide insights on how to successfully engage non-technologists with new media and the challenges they face during authoring, the unique requirements of new media authoring, and how modern tools are still not meeting the needs of this type of author, highlighting where additional research is needed.",0,12.015503876
UIST,73495937721f9a6ec74f9c5e250015c30b8c92db,UIST,2013,Exploring back-of-device interaction,Mohammad Faizuddin Mohd Noor,1909216,"Back of device interaction is gaining popularity as an alternative input modality in mobile devices, however it is still unclear how the back of device is related to other interactions. My research explores the relationship between hand grip from the back of the device and other interactions. In order to investigate this relationship, I will use touch target application to study hand grip patterns, then analyse the correlation that exists between touch target and hand grip. Finally I will explore the possibilities offered when the relationship between the touch target and hand grip is established in a quantifiable way.",2,37.6146788991
UIST,556fc5cd4d01cf452ded4022dc4798ba40884576,UIST,2001,From desktop to phonetop: a UI for web interaction on very small devices,"Jonathan Trevor, David M. Hilbert, Bill N. Schilit, Tzu Khiau Koh","1696250, 1702600, 1711856, 1689688","While it is generally accepted that new Internet terminals should leverage the installed base of Web content and services, the differences between desktop computers and very small devices makes this challenging. Indeed, the browser interaction model has evolved on desktop computers having a unique combination of user interface (large display, keyboard, pointing device), hardware, and networking capabilities. In contrast, Internet enabled cell phones, typically with 3-10 lines of text, sacrifice usability as Web terminals in favor of portability and other functions. Based on our earlier experiences building and using a Web browser for small devices we propose a new UI that splits apart the integrated activities of link following and reading into separate modes: <i>navigating</i> to; and <i>acting</i> on web content. This interaction technique for very small devices is both simpler for navigating and allows users to do more than just read. The M-Links system incorporates modal browsing interaction and addresses a number of associated problems. We have built our system with an emphasis on simplicity and user extensibility and describe the design, implementation and evolution of the user interface.",33,43.3333333333
UIST,8ed5e5a395d4ce70d49a6386077c15ff6bba41e8,UIST,2006,Phosphor: explaining transitions in the user interface using afterglow effects,"Patrick Baudisch, Desney S. Tan, Maxime Collomb, Daniel C. Robbins, Ken Hinckley, Maneesh Agrawala, Shengdong Zhao, Gonzalo Ramos","1729393, 1719056, 1852891, 1770003, 1738072, 1820412, 2645457, 1910455","Sometimes users fail to notice a change that just took place on their display. For example, the user may have accidentally deleted an icon or a remote collaborator may have changed settings in a control panel. Animated transitions can help, but they force users to wait for the animation to complete. This can be cumbersome, especially in situations where users did not need an explanation. We propose a different approach. Phosphor objects show the outcome of their transition instantly; at the same time they explain their change in retrospect. Manipulating a phosphor slider, for example, leaves an afterglow that illustrates how the knob moved. The parallelism of instant outcome and explanation supports both types of users. Users who already understood the transition can continue interacting without delay, while those who are inexperienced or may have been distracted can take time to view the effects at their own pace. We present a framework of transition designs for widgets, icons, and objects in drawing programs. We evaluate phosphor objects in two user studies and report significant performance benefits for phosphor objects.",58,81.25
UIST,18d44b5245893d631b648b4ffb1431e67f710420,UIST,2006,Soap: a pointing device that works in mid-air,"Patrick Baudisch, Mike Sinclair, Andrew Wilson","1729393, 1722648, 1792265","Soap is a pointing device based on hardware found in a mouse, yet works in mid-air. Soap consists of an optical sensor device moving freely inside a hull made of fabric. As the user applies pressure from the outside, the optical sensor moves independent from the hull. The optical sensor perceives this relative motion and reports it as position input. Soap offers many of the benefits of optical mice, such as high-accuracy sensing. We describe the design of a soap prototype and report our experiences with four application scenarios, including a wall display, Windows Media Center, slide presentation, and interactive video games.",22,35.0
UIST,49a10e298f5f7fc9b3fbea2573b31d1a8842bd7f,UIST,2012,FlexAura: a flexible near-surface range sensor,"Shenwei Liu, François Guimbretière","2657522, 2539134","The availability of flexible capacitive sensors that can be fitted around mice, smartphones, and pens carries great potential in leveraging grasp as a new interaction modality. Unfortunately, most capacitive sensors only track interaction directly on the surface, making it harder to differentiate among grips and constraining user movements. We present a new optical range sensor design based on high power infrared LEDs and photo-transistors, which can be fabricat-ed on a flexible PCB and wrapped around a wide variety of graspable objects including pens, mice, smartphones, and slates. Our sensor offers a native resolution of 10 dpi with a sensing range of up to 30mm (1.2"""") and sampling speed of 50Hz. Based on our prototype wrapped around the barrel of a pen, we present a summary of the characteristics of the sensor and describe the sensor output in several typical pen grips. Our design is versatile enough to apply not only to pens but to a wide variety of graspable objects including smartphones and slates.",12,60.2941176471
UIST,0cc8f80e317873d75bde21d956dcb81c7705296f,UIST,2004,Collapse-to-zoom: viewing web pages on small screen devices by interactively removing irrelevant content,"Patrick Baudisch, Xing Xie, Chong Wang, Wei-Ying Ma","1729393, 1687677, 1782199, 1705244","Overview visualizations for small-screen web browsers were designed to provide users with visual context and to allow them to rapidly zoom in on tiles of relevant content. Given that content in the overview is reduced, however, users are often unable to tell which tiles hold the relevant material, which can force them to adopt a time-consuming hunt-and-peck strategy. Collapse-to-zoom addresses this issue by offering an alternative exploration strategy. In addition to allowing users to zoom into relevant areas, collapse-to-zoom allows users to collapse areas deemed irrelevant, such as columns containing menus, archive material, or advertising. Collapsing content causes all remaining content to expand in size causing it to reveal more detail, which increases the user's chance of identifying relevant content. Collapse-to-zoom navigation is based on a hybrid between a marquee selection tool and a marking menu, called marquee menu. It offers four commands for collapsing content areas at different granularities and to switch to a full-size reading view of what is left of the page.",61,63.1578947368
UIST,0770a2b19ceca2cc4f8e1d7398ce3d119bc96b48,UIST,2016,Metamaterial Mechanisms,"Alexandra Ion, Johannes Frohnhofen, Ludwig Wall, Robert Kovacs, Mirela Alistar, Jack Lindsay, Pedro Lopes, Hsiang-Ting Chen, Patrick Baudisch","1906663, 1710690, 1967682, 2672289, 3088955, 3157184, 2816776, 3327016, 1729393","Recently, researchers started to engineer not only the outer shape of objects, but also their <i>internal microstructure</i>. Such objects, typically based on 3D cell grids, are also known as metamaterials. Metamaterials have been used, for example, to create materials with soft and hard regions.
 So far, metamaterials were understood as materials-we want to think of them as <i>machines</i>. We demonstrate metamaterial objects that perform a mechanical function. Such <i>metamaterial mechanisms</i> consist of a single block of material the cells of which play together in a well-defined way in order to achieve macroscopic movement. Our metamaterial door latch, for example, transforms the rotary movement of its handle into a linear motion of the latch. Our metamaterial Jansen walker consists of a single block of cells-that can walk. The key element behind our metamaterial mechanisms is a specialized type of cell, the only ability of which is to shear.
 In order to allow users to create metamaterial mechanisms efficiently we implemented a specialized 3D editor. It allows users to place different types of cells, including the shear cell, thereby allowing users to add mechanical functionality to their objects. To help users verify their designs during editing, our editor allows users to apply forces and simulates how the object deforms in response.",0,44.6540880503
UIST,835b8d5300598a29c6ee0a66b0720c5e395e689d,UIST,2000,PicturePiper: using a re-configurable pipeline to find images on the Web,"Adam M. Fass, Eric A. Bier, Eytan Adar","3130429, 1734223, 2630700","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT In this paper, we discuss a re-configurable pipeline architecture that is ideally suited for applications in which a user is interactively managing a stream of data. Currently, document service buses allow stand-alone document services (translation, printing, etc.) to be combined for batch processing. Our architecture allows services to be composed and re-configured on the fly in order to support interactive applications. To motivate the need for such an architecture we address the problem of finding and organizing images on the World Wide Web. The resulting tool, PicturePiper, provides a mechanism for allowing users access to images on the web related to a topic of interest.",4,4.0
UIST,b51c0ccd471bdaa138e8afe1cc201c1ebb7a51e1,UIST,2001,Focus plus context screens: combining display technology with visualization techniques,"Patrick Baudisch, Nathaniel Good, Paul Stewart","1729393, 6201430, 2106184","Computer users working with large visual documents, such as large layouts, blueprints, or maps perform tasks that require them to simultaneously access overview information while working on details. To avoid the need for zooming, users currently have to choose between using a sufficiently large screen or applying appropriate visualization techniques. Currently available hi-res ""wall-size"" screens, however, are cost-intensive, space-intensive, or both. Visualization techniques allow the user to more efficiently use the given screen space, but in exchange they either require the user to switch between multiple views or they introduce distortion.In this paper, we present a novel approach to simultaneously display focus and context information. <i>Focus plus context screens</i> consist of a hi-res display and a larger low-res display. Image content is displayed such that the scaling of the display content is preserved, while its resolution may vary according to which display region it is displayed in. Focus plus context screens are applicable to practically all tasks that currently use overviews or fisheye views, but unlike these visualization techniques, focus plus context screens provide a single, non-distorted view. We present a prototype that seamlessly integrates an LCD with a projection screen and demonstrate four applications that we have adapted so far.",120,73.3333333333
UIST,2376106eb10f26c0545c078d256d3921a32a080d,UIST,2015,Protopiper: Physically Sketching Room-Sized Objects at Actual Scale,"Harshit Agrawal, Udayan Umapathi, Robert Kovacs, Johannes Frohnhofen, Hsiang-Ting Chen, Stefanie Müller, Patrick Baudisch","2071315, 2754729, 2672289, 1710690, 3327016, 4942906, 1729393","Physical sketching of 3D wireframe models, using a hand-held plastic extruder, allows users to explore the design space of 3D models efficiently. Unfortunately, the scale of these devices limits users' design explorations to small-scale objects. We present protopiper, a computer aided, hand-held fabrication device, that allows users to sketch room-sized objects at actual scale. The key idea behind protopiper is that it forms adhesive tape into tubes as its main building material, rather than extruded plastic or photopolymer lines. Since the resulting tubes are hollow they offer excellent strength-to-weight ratio, thus scale well to large structures. Since the tape is pre-coated with adhesive it allows connecting tubes quickly, unlike extruded plastic that would require heating and cooling in the kilowatt range. We demonstrate protopiper's use through several demo objects, ranging from more constructive objects, such as furniture, to more decorative objects, such as statues. In our exploratory user study, 16 participants created objects based on their own ideas. They rated the device as being ""useful for creative exploration"", ""its ability to sketch at actual scale helped judge fit"", and ""fun to use.""",5,80.701754386
UIST,bcfb07e20267e228fa28f01ee0db2a693bee0995,UIST,2015,Push-Push: A Drag-like Operation Overlapped with a Page Transition Operation on Touch Interfaces,"Jaehyun Han, Geehyuk Lee","7181851, 1717371","A page transition operation on touch interfaces is a common and frequent subtask when one conducts a drag-like operation such as selecting text and dragging an icon. Traditional page transition gestures such as scrolling and flicking gestures, however, cannot be conducted while conducting the drag-like operation since they have a confliction. We proposed Push-Push that is a new drag-like operation not in conflict with page transition operations. Thus, page transition operations could be conducted while performing Push-Push. To design Push-Push, we utilized the hover and pressed states as additional input states of touch interfaces. The results from two experiments showed that Push-Push has an advantage on increasing performance and qualitative opinions of users while reducing the subjective overload.",1,42.1052631579
UIST,14d7eb5f94d2f18235c28f62e9159d3ccbf61086,UIST,2006,In-stroke word completion,"Jacob O. Wobbrock, Brad A. Myers, Duen Horng Chau","1796045, 1707801, 1793506","We present the design and implementation of a word-level stroking system called <i>Fisch</i>, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on <i>in-stroke word completion</i>, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of ""record speed"" with the stylus version resulted in 50-60 wpm with no uncorrected errors.",12,10.0
UIST,6714de13a5d28e6e9c6ef8428eba9839334f152e,UIST,2006,Huddle: automatically generating interfaces for systems of multiple connected appliances,"Jeffrey Nichols, Brandon Rothrock, Duen Horng Chau, Brad A. Myers","1687909, 1904850, 1793506, 1707801","Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.",33,46.25
UIST,35d784874614fbac62f279a79e3560570caca002,UIST,2010,Interacting with live preview frames: in-picture cues for a digital camera interface,Steven R. Gomez,1853207,"We present a new interaction paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues in front of the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera's image processor to control the lens or other settings. We design and analyze vision-based camera interactions, including focus and zoom controls, and argue that the vision-based paradigm offers a new level of photographer control needed for the next generation of digital cameras.",1,22.0930232558
UIST,3e57aee97aff017c71efd76b52e8374f8c943910,UIST,2009,A reconfigurable ferromagnetic input device,"Jonathan Hook, Stuart Taylor, Alex Butler, Nicolas Villar, Shahram Izadi","1685911, 1683873, 1684414, 1707115, 1699068","We present a novel hardware device based on ferromagnetic sensing, capable of detecting the presence, position and deformation of any ferrous object placed on or near its surface. These objects can include ball bearings, magnets, iron filings, and soft malleable bladders filled with ferrofluid. Our technology can be used to build reconfigurable input devices -- where the physical form of the input device can be assembled using combinations of such ferrous objects. This allows users to rapidly construct new forms of input device, such as a trackball-style device based on a single large ball bearing, tangible mixers based on a collection of sliders and buttons with ferrous components, and multi-touch malleable surfaces using a ferrofluid bladder. We discuss the implementation of our technology, its strengths and limitations, and potential application scenarios.",21,38.5714285714
UIST,cb8764a11268aa5a642876637f33c9db8bf79333,UIST,2016,OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping,"Zhengqing Li, Shio Miyafuji, Toshiki Sato, Hideki Koike","3493119, 2784800, 3269647, 1684942","Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time.
 In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.",0,44.6540880503
UIST,11d0df79b4b95dcb4318101590390378e42dd5e4,UIST,2010,Kinetic tiles: modular construction units for interactive kinetic surfaces,"Hyunjung Kim, Woohun Lee","3355092, 1714095","We propose and demonstrate Kinetic Tiles, modular con-struction units for <i>Interactive Kinetic Surfaces</i> (IKSs). We aimed to design Kinetic Tiles to be accessible and available so that users can construct IKSs easily and rapidly. The components of Kinetic Tiles are inexpensive and easily available. In addition, the use of magnetic force enables the separation of the surface material and actuators so that users only interact with the tile modules as if constructing a tile mosaic. Kinetic Tiles can be utilized as a new design and architectural material that allows the surfaces of everyday objects and spaces to convey ambient and pleasurable kinetic expressions.",2,33.1395348837
UIST,000aff6e61f62abefd78be594d428779f58e763b,UIST,2005,Informal prototyping of continuous graphical interactions by demonstration,"Yang Li, James A. Landay","1678662, 1708404","We introduce Monet, a sketch-based tool for prototyping continuous graphical interactions by demonstration. In Monet, designers can prototype continuous widgets and their states of interest using examples. They can also demonstrate compound behaviors involving multiple widgets by direct manipulation. Monet allows continuous interactions to be easily integrated with event-based, discrete interactions. Continuous widgets can be embedded into storyboards and their states can condition or trigger storyboard transitions. Monet achieves these features without using any domain specific knowledge or assuming any application semantics.",13,22.5806451613
UIST,bdb11f75d45a6510dac66c10c4be12e06b580100,UIST,2007,"Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes","Jacob O. Wobbrock, Andrew D. Wilson, Yang Li","1796045, 1767449, 1678662","Although mobile, tablet, large display, and tabletop computers increasingly present opportunities for using pen, finger, and wand gestures in user interfaces, implementing gesture recognition largely has been the privilege of pattern matching experts, not user interface prototypers. Although some user interface libraries and toolkits offer gesture recognizers, such infrastructure is often unavailable in design-oriented environments like Flash, scripting environments like JavaScript, or brand new off-desktop prototyping environments. To enable novice programmers to incorporate gestures into their UI prototypes, we present a ""$1 recognizer"" that is easy, cheap, and usable almost anywhere in about 100 lines of code. In a study comparing our $1 recognizer, Dynamic Time Warping, and the Rubine classifier on user-supplied gestures, we found that $1 obtains over 97% accuracy with only 1 loaded template and 99% accuracy with 3+ loaded templates. These results were nearly identical to DTW and superior to Rubine. In addition, we found that medium-speed gestures, in which users balanced speed and accuracy, were recognized better than slow or fast gestures for all three recognizers. We also discuss the effect that the number of templates or training examples has on recognition, the score falloff along recognizers' N-best lists, and results for individual gestures. We include detailed pseudocode of the $1 recognizer to aid development, inspection, extension, and testing.",317,100.0
UIST,46cdd0e9cd509f8935529ece010acc26aeb69afc,UIST,2010,DoubleFlip: a motion gesture delimiter for interaction,"Jaime Ruiz, Yang Li","1831956, 1678662","In order to use motion gestures with mobile devices it is imperative that the device be able to distinguish between input motion and everyday motion. In this abstract we present DoubleFlip, a unique motion gesture designed to act as an input delimiter for mobile motion gestures. We demonstrate that the DoubleFlip gesture is extremely resistant to false positive conditions, while still achieving high recognition accuracy. Since DoubleFlip is easy to perform and less likely to be accidentally invoked, it provides an always-active input event for mobile interaction.",1,22.0930232558
UIST,a86fc0b12bb17c410fb8f234be9b6d279adbb675,UIST,2010,Gesture search: a tool for fast mobile data access,Yang Li,1678662,"Modern mobile phones can store a large amount of data, such as contacts, applications and music. However, it is difficult to access specific data items via existing mobile user interfaces. In this paper, we present Gesture Search, a tool that allows a user to quickly access various data items on a mobile phone by drawing gestures on its touch screen. Gesture Search contributes a unique way of combining gesture-based interaction and search for fast mobile data access. It also demonstrates a novel approach for coupling gestures with standard GUI interaction. A real world deployment with mobile phone users showed that Gesture Search enabled fast, easy access to mobile data in their day-to-day lives. Gesture Search has been released to public and is currently in use by hundreds of thousands of mobile users. It was rated positively by users, with a mean of 4.5 out of 5 for over 5000 ratings.",32,82.5581395349
UIST,a8fae334d7f50d6cb5aaa52420c38ca3d2834616,UIST,2013,CrowdLearner: rapidly creating mobile recognizers using crowdsourcing,"Shahriyar Amini, Yang Li","2831620, 1678662","Mobile applications can offer improved user experience through the use of novel modalities and user context. However, these new input dimensions often require recognition-based techniques, with which mobile app developers or designers may not be familiar. Furthermore, the recruiting, data collection and labeling, necessary for using these techniques, are usually time-consuming and expensive. We present CrowdLearner, a framework based on crowdsourcing to automatically generate recognizers using mobile sensor input such as accelerometer or touchscreen readings. CrowdLearner allows a developer to easily create a recognition task, distribute it to the crowd, and monitor its progress as more data becomes available. We deployed CrowdLearner to a crowd of 72 mobile users over a period of 2.5 weeks. We evaluated the system by experimenting with 6 recognition tasks concerning motion gestures, touchscreen gestures, and activity recognition. The experimental results indicated that CrowdLearner enables a developer to quickly acquire a usable recognizer for their specific application by spending a moderate amount of money, often less than $10, in a short period of time, often in the order of 2 hours. Our exploration also revealed challenges and provided insights into the design of future crowdsourcing systems for machine learning tasks.",6,59.1743119266
UIST,c800f487ff8866ec45f0a4433fb7a45356bfd81e,UIST,2013,Open project: a lightweight framework for remote sharing of mobile applications,"Matei Negulescu, Yang Li","2577452, 1678662","The form factor of mobile devices remains small while their computing power grows at an accelerated rate. Prior work has explored expanding the output space by leveraging free displays in the environment. However, existing solutions often do not scale. In this paper we discuss Open Project, an end-to-end framework that allows a user to ""project"" a native mobile application onto a display using a phone camera, leveraging interaction spaces ranging from a PC monitor to a public wall-sized display. Any display becomes projectable instantaneously by simply accessing the lightweight Open Project server via a web browser. By distributing computation load onto each projecting mobile device, our framework easily scales for hosting many projection sessions and devices simultaneously. Our performance experiments and user studies indicated that Open Project supported a variety of useful collaborative, sharing scenarios and performed reliably in diverse settings.",3,45.871559633
UIST,84c0e80e004c0bfe0a9c8167b3ee807c7f7e2435,UIST,2014,Detecting tapping motion on the side of mobile devices by probabilistically combining hand postures,"William McGrath, Yang Li","3342564, 1678662","We contribute a novel method for detecting finger taps on the different sides of a smartphone, using the built-in motion sensors of the device. In particular, we discuss new features and algorithms that infer side taps by probabilistically combining estimates of tap location and the hand pose--the hand holding the device. Based on a dataset collected from 9 participants, our method achieved 97.3% precision and 98.4% recall on tap event detection against ambient motion. For detecting single-tap locations, our method outperformed an approach that uses inferred hand postures deterministically by 3% and an approach that does not use hand posture inference by 17%. For inferring the location of two consecutive side taps from the same direction, our method outperformed the two baseline approaches by 6% and 17% respectively. We discuss our insights into designing the detection algorithm and the implication on side tap-based interaction behaviors.",7,72.0930232558
UIST,77e5c8262de23473315d8ed616a7ae9d5da0af98,UIST,2004,Topiary: a tool for prototyping location-enhanced applications,"Yang Li, Jason I. Hong, James A. Landay","1678662, 1689960, 1708404","Location-enhanced applications use the location of people, places, and things to augment or streamline interaction. Location-enhanced applications are just starting to emerge in several different domains, and many people believe that this type of application will experience tremendous growth in the near future. However, it currently requires a high level of technical expertise to build location-enhanced applications, making it hard to iterate on designs. To address this problem we introduce Topiary, a tool for rapidly prototyping location-enhanced applications. Topiary lets designers create a map that models the location of people, places, and things; use this active map to demonstrate scenarios depicting location contexts; use these scenarios in creating storyboards that describe interaction sequences; and then run these storyboards on mobile devices, with a wizard updating the location of people and things on a separate device. We performed an informal evaluation with seven researchers and interface designers and found that they reacted positively to the concept.",105,92.1052631579
UIST,b87db05a5d9a1998a7dddefab4effddc3ff929a0,UIST,2014,Reflection: enabling event prediction as an on-device service for mobile interaction,Yang Li,1678662,"By knowing which upcoming action a user might perform, a mobile application can optimize its user interface for accomplishing the task. However, it is technically challenging for developers to implement event prediction in their own application. We created Reflection, an on-device service that answers queries from a mobile application regarding which actions the user is likely to perform at a given time. Any application can register itself and communicate with Reflection via a simple API. Reflection continuously learns a prediction model for each application based on its evolving event history. It employs a novel method for prediction by 1) combining multiple well-designed predictors with an online learning method, and 2) capturing event patterns not only within but also across registered applications--only possible as an infrastructure solution. We evaluated Reflection with two sets of large-scale, in situ mobile event logs, which showed our infrastructure approach is feasible.",1,31.007751938
UIST,f1de3403a60386d4aa75709dd6639c26fc545364,UIST,2016,Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation,"Xiang 'Anthony' Chen, Yang Li","2028468, 1678662","To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users' unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline-before the system sees any user-defined gestures and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone's built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems.",0,44.6540880503
UIST,ece728700c2f844a4c6c6924c6ce7e2c11975a2c,UIST,2004,C-blink: a hue-difference-based light signal marker for large screen interaction via any mobile terminal,"Kento Miyaoku, Suguru Higashino, Yoshinobu Tonomura","2068437, 1713187, 2000148","To enable common mobile terminals to interact with contents shown on large screens, we propose ""C-Blink"", a new light signal marker method that uses the color liquid-crystal display of a mobile terminal as a visible light source. We overcome the performance limitations of such displays by developing a hue-difference-blink technique. In combination with a screen-side sensor, we describe a system that detects and receives light signal markers sent by cell phone displays. Evaluations of a prototype system confirm that C-Blink performs well under common indoor lighting. The C-Blink program can be installed in any mobile terminal that has a color display, and the installation costs are small. C-Blink is a very useful way of enabling ubiquitous large screens to become interfaces for mobile terminals.",37,50.0
UIST,73d7b1614b2122b651686b709172cd569d01d5a6,UIST,2014,Paper3D: bringing casual 3D modeling to a multi-touch interface,"Patrick Paczkowski, Julie Dorsey, Holly E. Rushmeier, Min H. Kim","1818289, 1775220, 1690595, 7591291","A 3D modeling system that provides all-inclusive functionality is generally too demanding for a casual 3D modeler to learn. In recent years, there has been a shift towards developing more approachable systems, with easy-to-learn, intuitive interfaces. However, most modeling systems still employ mouse and keyboard interfaces, despite the ubiquity of tablet devices, and the benefits of multi-touch interfaces applied to 3D modeling. In this paper, we introduce an alternative 3D modeling paradigm for creating developable surfaces, inspired by traditional papercrafting, and implemented as a system designed from the start for a multi-touch tablet. We demonstrate the process of assembling complex 3D scenes from a collection of simpler models, in turn shaped through operations applied to sheets of virtual paper. The modeling and assembling operations mimic familiar, real-world operations performed on paper, allowing users to quickly learn our system with very little guidance. We outline key design decisions made throughout the development process, based on feedback obtained through collaboration with target users. Finally, we include a range of models created in our system.",6,67.8294573643
UIST,7bcee80a5526acd7e51c7b4eb5820fcdeec717c2,UIST,2016,IdeaHound: Improving Large-scale Collaborative Ideation with Crowd-Powered Real-time Semantic Modeling,"Pao Siangliulue, Joel Chan, Steven Dow, Krzysztof Z. Gajos","3061428, 2939123, 5319364, 1770992","Prior work on creativity support tools demonstrates how a computational semantic model of a solution space can enable interventions that substantially improve the number, quality and diversity of ideas. However, automated semantic modeling often falls short when people contribute short text snippets or sketches. Innovation platforms can employ humans to provide semantic judgments to construct a semantic model, but this relies on external workers completing a large number of tedious micro tasks. This requirement threatens both accuracy (external workers may lack expertise and context to make accurate semantic judgments) and scalability (external workers are costly). In this paper, we introduce IdeaHound, an ideation system that seamlessly integrates the task of defining semantic relationships among ideas into the primary task of idea generation. The system combines implicit human actions with machine learning to create a computational semantic model of the emerging solution space. The integrated nature of these judgments allows IDEAHOUND to leverage the expertise and efforts of participants who are already motivated to contribute to idea generation, overcoming the issues of scalability inherent to existing approaches. Our results show that participants were equally willing to use (and just as productive using) IDEAHOUND compared to a conventional platform that did not require organizing ideas. Our integrated crowdsourcing approach also creates a more accurate semantic model than an existing crowdsourced approach (performed by external crowds). We demonstrate how this model enables helpful creative interventions: providing diverse inspirational examples, providing similar ideas for a given idea and providing a visual overview of the solution space.",0,44.6540880503
UIST,1b984234d64e19f50cb455d7f45493177ae2a682,UIST,2007,"Robust, low-cost, non-intrusive sensing and recognition of seated postures","Bilge Mutlu, Andreas Krause, Jodi Forlizzi, Carlos Guestrin, Jessica K. Hodgins","2662943, 3421686, 1767344, 1730156, 1788773","In this paper, we present a methodology for recognizing seatedpostures using data from pressure sensors installed on a chair.Information about seated postures could be used to help avoidadverse effects of sitting for long periods of time or to predictseated activities for a human-computer interface. Our system designdisplays accurate near-real-time classification performance on datafrom subjects on which the posture recognition system was nottrained by using a set of carefully designed, subject-invariantsignal features. By using a near-optimal sensor placement strategy,we keep the number of required sensors low thereby reducing costand computational complexity. We evaluated the performance of ourtechnology using a series of empirical methods including (1)cross-validation (classification accuracy of 87% for ten posturesusing data from 31 sensors), and (2) a physical deployment of oursystem (78% classification accuracy using data from 19sensors).",25,41.6666666667
UIST,0d08f5be6ac8508394d4c027b5926ed389645c34,UIST,2009,User guided audio selection from complex sound mixtures,Paris Smaragdis,1718742,"In this paper we present a novel interface for selecting sounds in audio mixtures. Traditional interfaces in audio editors provide a graphical representation of sounds which is either a waveform, or some variation of a time/frequency transform. Although with these representations a user might be able to visually identify elements of sounds in a mixture, they do not facilitate object-specific editing (e.g. selecting only the voice of a singer in a song). This interface uses audio guidance from a user in order to select a target sound within a mixture. The user is asked to vocalize (or otherwise sonically represent) the desired target sound, and an automatic process identifies and isolates the elements of the mixture that best relate to the user's input. This way of pointing to specific parts of an audio stream allows a user to perform audio selections which would have been infeasible otherwise.",6,8.57142857143
UIST,b7a78445d1b5a3249bf38aadc22034f823b7515d,UIST,2010,Anywhere touchtyping: text input on arbitrary surface using depth sensing,"Adiyan Mujibiya, Takashi Miyaki, Jun Rekimoto","2845050, 2279502, 1685962","In this paper, touch typing enabled virtual keyboard system using depth sensing on arbitrary surface is proposed. Keystroke event detection is conducted using 3-dimensional hand appearance database matching combined with fingertip's surface touch sensing. Our prototype system acquired hand posture depth map by implementing phase shift algorithm for Digital Light Processor (DLP) fringe projection on arbitrary flat surface. The system robustly detects hand postures on the sensible surface with no requirement of hand position alignment on virtual keyboard frame. The keystroke feedback is the physical touch to the surface, thus no specific hardware must be worn. The system works real-time in average of 20 frames per second.",2,33.1395348837
UIST,1382f8214cc174b01a9c3ebaf27f5c1116a02dc6,UIST,2011,TactileTape: low-cost touch sensing on curved surfaces,"David Holman, Roel Vertegaal","1765555, 1687608","TactileTape is a one-dimensional touch sensor that looks and behaves like regular tape. It can be constructed from everyday materials (a pencil, tin foil, and shelf liner) and senses single-touch input on curved and deformable surfaces. It is used as a roll of touch sensitive material from which designers cut pieces to quickly add touch sensitive strips to physical prototypes. TactileTape is low-cost, easy to interface, and, unlike current non-planar touch solutions [2,7,11], it is better adapted for the rapid exploration and iteration in the early design stage.",15,60.0
UIST,3a9b37bee39cebd43362d15c2ae0d4bc9699dbec,UIST,2016,Private Webmail 2.0: Simple and Easy-to-Use Secure Email,"Scott Ruoti, Jeff Andersen, Travis Hendershot, Daniel Zappala, Kent E. Seamons","2554572, 2860061, 3045701, 1963419, 1774940","Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.",2,98.427672956
UIST,8a2e4e616709999c9ebe3822cd82becb43f407c5,UIST,2016,Analysis of Sequential Tasks in Use Context of Mobile Apps,"Jihye Lee, Sangwon Lee","2465435, 2607628","Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from automated task service, ""If This Then That (IFTTT)"", and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-applications. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.",0,44.6540880503
UIST,8897bce7e12648af30d2d5960408943834c1feff,UIST,2011,"MoodMusic: a method for cooperative, generative music playlist creation","Jared S. Bauer, Alex Jansen, Jesse Cirimele","1820096, 2692208, 1818711","Music is a major element of social gatherings. However, creating playlists that suit everyone's tastes and the mood of the group can require a large amount of manual effort. In this paper, we present MoodMusic, a method to dynamically generate contextually appropriate music playlists for groups of people. MoodMusic uses speaker pitch and intensity in the conversation to determine the current 'mood'. MoodMusic then queries the online music libraries of the speakers to choose songs appropriate for that mood. This allows groups to listen to music appropriate for their current mood without managing playlists. This work contributes a novel method for dynamically creating music playlists for groups based on their music preferences and current mood.",1,15.7142857143
UIST,72d30ec346de4fdcc91466c0a989e551b64b4382,UIST,1991,EmbeddedButtons: documents as user interfaces,Eric A. Bier,1734223,"Recent electronic document editors and hypertext systems allow users to create customized user interfaces by adding user-pressable buttons to on-screen documents. Positioning these buttons is easy because users are already tlamlliar with the use of document editors. Unfortunately, the resulting user interfaces often ex!st only in stand-alone document systems, making it hard to integrate them with other applications. Furthermore, because buttons are usually treated as special document objects, they cannot take advan~age of document editor formatting and layout capabilities to create then-appearance This paper describes the EmbeddedButtons architecture, which makes ~asy to integrate buttons into documents and to use the resulting documents for a variety of user interface types. EmbeddedButtons allows arbitrary document elements to behave as buttons. Documents can be linked to application windows to serve as application control panels. Buttons can store and display application state to serve as mode indicators. New button classes, editors, and ap-phcations can be added dynamically",5,19.5652173913
UIST,2f4bec9d9c5b0c2daf38bce5c499480839f7b9d0,UIST,2015,"SceneSkim: Searching and Browsing Movies Using Synchronized Captions, Scripts and Plot Summaries","Amy Pavel, Dan B. Goldman, Björn Hartmann, Maneesh Agrawala","3099614, 1976171, 4020023, 1820412","Searching for scenes in movies is a time-consuming but crucial task for film studies scholars, film professionals, and new media artists. In pilot interviews we have found that such users search for a wide variety of clips---e.g., actions, props, dialogue phrases, character performances, locations---and they return to particular scenes they have seen in the past. Today, these users find relevant clips by watching the entire movie, scrubbing the video timeline, or navigating via DVD chapter menus. Increasingly, users can also index films through transcripts---however, dialogue often lacks visual context, character names, and high level event descriptions. We introduce SceneSkim, a tool for searching and browsing movies using synchronized captions, scripts and plot summaries. Our interface integrates information from such sources to allow expressive search at several levels of granularity: Captions provide access to accurate dialogue, scripts describe shot-by-shot actions and settings, and plot summaries contain high-level event descriptions. We propose new algorithms for finding word-level caption to script alignments, parsing text scripts, and aligning plot summaries to scripts. Film studies graduate students evaluating SceneSkim expressed enthusiasm about the usability of the proposed system for their research and teaching.",4,75.4385964912
UIST,820c670a8ef3acde43956b84354553cc8f54c03a,UIST,2016,Designing a Non-contact Wearable Tactile Display Using Airflows,"Jaeyeon Lee, Geehyuk Lee","8152662, 1717371","Traditional wearable tactile displays transfer tactile stimulations through a firm contact between the stimulator and the skin. We conjecture that a firm contact may not be always possible and acceptable. Therefore, we explored the concept of a non-contact wearable tactile display using an airflow, which can transfer information without a firm contact. To secure an empirical ground for the design of a wearable airflow display, we conducted a series of psychophysical experiments to estimate the intensity thresholds, duration thresholds, and distance thresholds of airflow perception on various body locations, and report the resulting empirical data in this paper. We then built a 4-point airflow display, compared its performance with that of a vibrotactile display, and could show that the two tactile displays are comparable in information transfer performance. User feedback was also positive and revealed many unique expressions describing airflow-based tactile experiences. Lastly, we demonstrate the feasibility of an airflow-based wearable tactile display with a prototype using micro-fans.",0,44.6540880503
UIST,f14d9a48786126f255823d8eec18f3263a04c653,UIST,2012,Homework: putting interaction into the infrastructure,"Richard Mortier, Tom Rodden, Peter Tolmie, Tom Lodge, Robert Spencer, Andy Crabtree, Joseph S. Sventek, Alexandros Koliousis","1679929, 1680844, 1834641, 2339608, 8461891, 1713085, 1769765, 2097479",This paper presents a user driven redesign of the domestic network infrastructure that draws upon a series of ethnographic studies of home networks. We present an infrastructure based around a purpose built access point that has modified the handling of protocols and services to reflect the interactive needs of the home. The developed infrastructure offers a novel measurement framework that allows a broad range of infrastructure information to be easily captured and made available to interactive applications. This is complemented by a diverse set of novel interactive control mechanisms and interfaces for the underlying infrastructure. We also briefly reflect on the technical and user issues arising from deployments.,16,68.6274509804
UIST,2e1481e7a02db3ee8afde7db3be878c15f96933e,UIST,2012,What art can tell us about the brain,Margaret S. Livingstone,2953737,"Artists have been doing experiments on vision longer than neurobiologists. Some major works of art have provided insights as to how we see; some of these insights are so fundamental that they can be understood in terms of the underlying neurobiology. For example, artists have long realized that color and luminance can play independent roles in visual perception. Picasso said, ""Colors are only symbols. Reality is to be found in luminance alone."" This observation has a parallel in the functional subdivision of our visual systems, where color and luminance are processed by the newer, primate-specific What system, and the older, colorblind, Where (or How) system. Many techniques developed over the centuries by artists can be understood in terms of the parallel organization of our visual systems. I will explore how the segregation of color and luminance processing are the basis for why some Impressionist paintings seem to shimmer, why some op art paintings seem to move, some principles of Matisse's use of color, and how the Impressionists painted ""air"". Central and peripheral vision are distinct, and I will show how the differences in resolution across our visual field make the Mona Lisa's smile elusive, and produce a dynamic illusion in Pointillist paintings, Chuck Close paintings, and photomosaics. I will explore how artists have intuited important features about how our brains extract relevant information about faces and objects, and I will discuss why learning disabilities may be associated with artistic talent.",1,25.0
UIST,52073f263ed8bef4792c68e3eb74f5f1d21810ee,UIST,2014,Cheaper by the dozen: group annotation of 3D data,"Aleksey Boyko, Thomas A. Funkhouser","2195057, 1807080","This paper proposes a group annotation approach to interactive semantic labeling of data and demonstrates the idea in a system for labeling objects in 3D LiDAR scans of a city. In this approach, the system selects a group of objects, predicts a semantic label for it, and highlights it in an interactive display. In response, the user either confirms the predicted label, provides a different label, or indicates that no single label can be assigned to all objects in the group. This sequence of interactions repeats until a label has been confirmed for every object in the data set. The main advantage of this approach is that it provides faster interactive labeling rates than alternative approaches, especially in cases where all labels must be explicitly confirmed by a person. The main challenge is to provide an algorithm that selects groups with many objects all of the same label type arranged in patterns that are quick to recognize, which requires models for predicting object labels and for estimating times for people to recognize objects in groups. We address these challenges by defining an objective function that models the estimated time required to process all unlabeled objects and approximation algorithms to minimize it. Results of user studies suggest that group annotation can be used to label objects in LiDAR scans of cities significantly faster than one-by-one annotation with active learning.",3,49.2248062016
UIST,f9f0d2015499346688e4255032f68ee77748b4e7,UIST,2015,Gaze vs. Mouse: A Fast and Accurate Gaze-Only Click Alternative,"Christof Lutteroth, Abdul Moiz Penkar, Gerald Weber","2990849, 2760162, 3123537","Eye gaze tracking is a promising input method which is gradually finding its way into the mainstream. An obvious question to arise is whether it can be used for point-and-click tasks, as an alternative for mouse or touch. Pointing with gaze is both fast and natural, although its accuracy is limited. There are still technical challenges with gaze tracking, as well as inherent physiological limitations. Furthermore, providing an alternative to clicking is challenging.
 We are considering use cases where input based purely on gaze is desired, and the click targets are discrete user interface (UI) elements which are too small to be reliably resolved by gaze alone, e.g., links in hypertext. We present Actigaze, a new gaze-only click alternative which is fast and accurate for this scenario. A clickable user interface element is selected by dwelling on one of a set of confirm buttons, based on two main design contributions: First, the confirm buttons stay on fixed positions with easily distinguishable visual identifiers such as colors, enabling procedural learning of the confirm button position. Secondly, UI elements are associated with confirm buttons through the visual identifiers in a way which minimizes the likelihood of inadvertent clicks. We evaluate two variants of the proposed click alternative, comparing them against the mouse and another gaze-only click alternative.",2,60.5263157895
UIST,0738b8b6ddb89c961c71a94bedef1badaac63298,UIST,2016,Smart Headlight: An Application of Projector-Camera Vision,Takeo Kanade,7642093,"A projector manipulates outgoing light rays, while a camera records incoming ones. Combining these optically inverse devices, especially in a coaxial manner, creates the possibility of a new computer-vision technology. The ""Smart Headlight,"" currently under development at Carnegie Mellon's Robotics Institute, is one example: a device that can ""erase"" raindrops or snowflakes from a driver's sight, allowing for continuous use of the ""high beams"" mode while not causing glare against oncoming drivers, and enhance the appearance of important objects, such as pedestrians. In that sense, it constitutes a ""genuine"" augmented reality, manipulating the reality for how it appears to a viewer, rather than merely overlaying objects on the image of the reality. This talk will present the state of the Smart Headlight project and discuss further possible applications of projector-camera systems.",0,44.6540880503
UIST,e9db31f804a3ec8cabf4be0fa7e50d9e1c45bbb3,UIST,2006,The design and evaluation of selection techniques for 3D volumetric displays,"Tovi Grossman, Ravin Balakrishnan","3313809, 1748870","Volumetric displays, which display imagery in true 3D space, are a promising platform for the display and manipulation of 3D data. To fully leverage their capabilities, appropriate user interfaces and interaction techniques must be designed. In this paper, we explore 3D selection techniques for volumetric displays. In a first experiment, we find a ray cursor to be superior to a 3D point cursor in a single target environment. To address the difficulties associated with dense target environments we design four new ray cursor techniques which provide disambiguation mechanisms for multiple intersected targets. Our techniques showed varied success in a second, dense target experiment. One of the new techniques, the <i>depth ray</i>, performed particularly well, significantly reducing movement time, error rate, and input device footprint in comparison to the 3D point cursor.",54,77.5
UIST,6fa153107a9ab717ad9c33aa07a81e4d8883ee53,UIST,2011,Pause-and-play: automatically linking screencast video tutorials with applications,"Suporn Pongnumkul, Mira Dontcheva, Wilmot Li, Jue Wang, Lubomir D. Bourdev, Shai Avidan, Michael F. Cohen","2537143, 2875493, 2812691, 1718812, 1769383, 2740179, 1694613","Video tutorials provide a convenient means for novices to learn new software applications. Unfortunately, staying in sync with a video while trying to use the target application at the same time requires users to repeatedly switch from the application to the video to pause or scrub backwards to replay missed steps. We present Pause-and-Play, a system that helps users work along with existing video tutorials. Pause-and-Play detects important events in the video and links them with corresponding events in the target application as the user tries to replicate the depicted procedure. This linking allows our system to automatically pause and play the video to stay in sync with the user. Pause-and-Play also supports convenient video navigation controls that are accessible from within the target application and allow the user to easily replay portions of the video without switching focus out of the application. Finally, since our system uses computer vision to detect events in existing videos and leverages application scripting APIs to obtain real time usage traces, our approach is largely independent of the specific target application and does not require access or modifications to application source code. We have implemented Pause-and-Play for two target applications, Google SketchUp and Adobe Photoshop, and we report on a user study that shows our system improves the user experience of working with video tutorials.",46,86.6666666667
UIST,90cf26018577f8c12f57022b47cf542a5e489c6e,UIST,2010,Content-aware dynamic timeline for video browsing,"Suporn Pongnumkul, Jue Wang, Gonzalo Ramos, Michael F. Cohen","2537143, 1718812, 1910455, 1694613","When browsing a long video using a traditional timeline slider control, its effectiveness and precision degrade as a video's length grows. When browsing videos with more frames than pixels in the slider, aside from some frames being inaccessible, scrolling actions cause sudden jumps in a video's continuity as well as video frames to flash by too fast for one to assess the content. We propose a <i>content-aware dynamic timeline</i> control that is designed to overcome these limitations. Our timeline control decouples video speed and playback speed, and leverages video content analysis to allow salient shots to be presented at an intelligible speed. Our control also takes advantage of previous work on elastic sliders, which allows us to produce an accurate navigation control.",16,73.8372093023
UIST,e55cd535474726ed8862a525dda6e66b55fc94ed,UIST,2015,Zensei: Augmenting Objects with Effortless User Recognition Capabilities through Bioimpedance Sensing,"Munehiko Sato, Rohan S. Puri, Alex Olwal, Deepak Chandra, Ivan Poupyrev, Ramesh Raskar","2131343, 2851570, 2375159, 1693136, 1736819, 1717566","As interactions with everyday handheld devices and objects become increasingly common, a more seamless and effortless identification and personalization technique will be essential to an uninterrupted user experience. In this paper, we present Zensei, a user identification and customization system using human body bioimpedance sensing through multiple electrodes embedded into everyday objects. Zensei provides for an uninterrupted user-device personalization experience that is difficult to forge because it uses both the unique physiological and behavioral characteristics of the user. We demonstrate our measurement system in three exemplary device configurations that showcase different levels of constraint via environment-based, whole-body-based, and handheld-based identification scenarios. We evaluated Zensei's classification accuracy among 12 subjects on each configuration over 22 days of collected data and report our promising results.",0,16.2280701754
UIST,5bfd5ac61ab1ff5a9c814de92134a3d83bc62504,UIST,2016,Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms,"Snehalkumar S. Gaikwad, Durim Morina, Adam Ginzberg, Catherine A. Mullings, Shirish Goyal, Dilrukshi Gamage, Christopher Diemert, Mathias Burton, Sharon Zhou, Mark E. Whiting, Karolina R. Ziulkoski, Alipta Ballav, Aaron Gilbee, Senadhipathige S. Niranga, Vibhor Sehgal, Jasmine Lin, Leonardy Kristianto, Angela Richmond-Fuller, Jeff Regino, Nalin Chhibber, Dinesh Majeti, Sachin Sharma, Kamila Mananova, Dinesh Dhakal, William Dai, Victoria Purynova, Samarth Sandeep, Varshine Chandrakanthan, Tejas Sarma, Sekandar Matin, Ahmed Nasser, Rohit Nistala, Alexander Stolzoff, Kristy Milland, Vinayak Mathur, Rajan Vaish, Michael S. Bernstein","3491922, 3312398, 2785736, 3491509, 2733639, 2046734, 2898183, 3491996, 3396987, 8169988, 2741088, 3492016, 3492092, 3492165, 2059248, 3491544, 3491604, 1977110, 2483230, 3492114, 2159092, 4331276, 3491663, 3492032, 3264799, 3492210, 3492034, 3491637, 2623259, 3492138, 3361788, 2029908, 3491631, 1844994, 3492205, 1927646, 3047089","Paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. These inflated scores, which make high-quality work and workers difficult to find, stem from social pressure to avoid giving negative feedback. We introduce Boomerang, a reputation system for crowdsourcing platforms that elicits more accurate feedback by rebounding the consequences of feedback directly back onto the person who gave it. With Boomerang, requesters find that their highly-rated workers gain earliest access to their future tasks, and workers find tasks from their highly-rated requesters at the top of their task feed. Field experiments verify that Boomerang causes both workers and requesters to provide feedback that is more closely aligned with their private opinions. Inspired by a game-theoretic notion of incentive-compatibility, Boomerang opens opportunities for interaction design to incentivize honest reporting over strategic dishonesty.",2,98.427672956
UIST,102771ee6f39c056851c1c5f36421ed300e27adc,UIST,2008,Creating map-based storyboards for browsing tour videos,"Suporn Pongnumkul, Jue Wang, Michael F. Cohen","2537143, 1718812, 1694613","Watching a long unedited video is usually a boring experience. In this paper we examine a particular subset of videos, tour videos, in which the video is captured by walking about with a running camera with the goal of conveying the essence of some place. We present a system that makes the process of sharing and watching a long tour video easier, less boring, and more informative. To achieve this, we augment the tour video with a map-based storyboard, where the tour path is reconstructed, and coherent shots at different locations are directly visualized on the map. This allows the viewer to navigate the video in the joint location-time space. To create such a storyboard we employ an automatic pre-processing component to parse the video into coherent shots, and an authoring tool to enable the user to tie the shots with landmarks on the map. The browser-based viewing tool allows users to navigate the video in a variety of creative modes with a rich set of controls, giving each viewer a unique, personal viewing experience. Informal evaluation shows that our approach works well for tour videos compared with conventional media players.",16,34.2857142857
UIST,33262b1135b8b30805b42419a11170c9a5c9ef24,UIST,2000,"FlowMenu: combining command, text, and data entry","François Guimbretière, Terry Winograd","2539134, 1699245","We present a new kind of marking menu that was developed for use with a pen device on display surfaces such as large, high resolution, wall-mounted displays. It integrates capabilities of previously separate mechanisms such as marking menus and Quikwriting, and facilitates the entry of multiple commands. While using this menu, the pen never has to leave the active surface so that consecutive menu selections, data entry (text and parameters) and direct manipulation tasks can be integrated fluidly.",103,76.0
UIST,3e88d6f867ec3d1c0347b20698d2e82117485219,UIST,2000,Suede: a Wizard of Oz prototyping tool for speech user interfaces,"Scott R. Klemmer, Anoop K. Sinha, Jack Chen, James A. Landay, Nadeem Aboobaker, Annie Wang","1728167, 3269069, 1965344, 1708404, 3211254, 8665710","Speech-based user interfaces are growing in popularity. Unfortunately, the technology expertise required to build speech UIs precludes many individuals from participating in the speech interface design process. Furthermore, the time and knowledge costs of building even simple speech systems make it difficult for designers to iteratively design speech UIs. SUEDE, the speech interface prototyping tool we describe in this paper, allows designers to rapidly create prompt/response speech interfaces. It offers an electronically supported Wizard of Oz (WOz) technique that captures test data, allowing designers to analyze the interface after testing. This informal tool enables speech user interface designers, even non-experts, to quickly create, test, and analyze speech user interface prototypes.",109,80.0
UIST,1858d55d1a2aa9e213466b8eb6cf843c1191bcb7,UIST,2015,Daemo: A Self-Governed Crowdsourcing Marketplace,"Snehal Gaikwad, Durim Morina, Rohit Nistala, Megha Agarwal, Alison Cossette, Radhika Bhanu, Saiph Savage, Vishwajeet Narwal, Karan Rajpal, Jeff Regino, Aditi Mithal, Adam Ginzberg, Aditi Nath, Karolina R. Ziulkoski, Trygve Cossette, Dilrukshi Gamage, Angela Richmond-Fuller, Ryo Suzuki, Jeerel Herrejón, Kevin Le, Claudia Flores-Saviaga, Haritha Thilakarathne, Kajal Gupta, William Dai, Ankita Sastry, Shirish Goyal, Thejan Rajapakshe, Niki Abolhassani, Angela Xie, Abigail Reyes, Surabhi Ingle, Verónica Jaramillo, Martin Godínez, Walter Ángel, Carlos Toxtli, Juan Flores, Asmita Gupta, Vineet Sethia, Diana Padilla, Kristy Milland, Kristiono Setyadi, Nuwan Wajirasena, Muthitha Batagoda, Rolando Cruz, James Damon, Divya Nekkanti, Tejas Sarma, Mohamed Saleh, Gabriela Gongora-Svartzman, Soroosh Bateni, Gema Toledo Barrera, Alex Peña, Ryan Compton, Deen Aariff, Luis Palacios, Manuela Paula Ritter, Nisha K. K., Alan Kay, Jana Uhrmeister, Srivalli Nistala, Milad Esfahani, Elsa Bakiu, Christopher Diemert, Luca Matsumoto, Manik Singh, Krupa Patel, Ranjay Krishna, Geza Kovacs, Rajan Vaish, Michael S. Bernstein","2671016, 3312398, 2029908, 2468733, 2911800, 2088201, 2121569, 2383244, 2475469, 2483230, 3051137, 2785736, 1959334, 2741088, 3145461, 2046734, 1977110, 3335444, 2582743, 1921270, 2626271, 2723332, 2149537, 3264799, 1870559, 2733639, 1918200, 3294798, 2717172, 2541522, 3138428, 3154666, 2765671, 2122419, 2602469, 6640127, 7821809, 2441054, 2853171, 1844994, 3099569, 3235009, 1970009, 1899380, 6261673, 2802356, 2623259, 1704562, 2638444, 3112609, 2311568, 2816810, 2230514, 2704162, 3229629, 2454841, 1908205, 3772297, 2164373, 3200142, 1893973, 3260009, 2898183, 3247054, 2100978, 7210550, 2580593, 1776950, 1927646, 3047089","Crowdsourcing marketplaces provide opportunities for autonomous and collaborative professional work as well as social engagement. However, in these marketplaces, workers feel disrespected due to unreasonable rejections and low payments, whereas requesters do not trust the results they receive. The lack of trust and uneven distribution of power among workers and requesters have raised serious concerns about sustainability of these marketplaces. To address the challenges of trust and power, this paper introduces Daemo, a self-governed crowdsourcing marketplace. We propose a <i>prototype task</i> to improve the work quality and <i>open-governance model</i> to achieve equitable representation. We envisage Daemo will enable workers to build sustainable careers and provide requesters with timely, quality labor for their businesses.",4,75.4385964912
UIST,c217372c9c4b93830aa099aa85f9ae96075f12c7,UIST,2016,VidCrit: Video-based Asynchronous Video Review,"Amy Pavel, Dan B. Goldman, Björn Hartmann, Maneesh Agrawala","3099614, 1976171, 4020023, 1820412","Video production is a collaborative process in which stakeholders regularly review drafts of the edited video to indicate problems and offer suggestions for improvement. Although practitioners prefer in-person feedback, most reviews are conducted asynchronously via email due to scheduling and location constraints. The use of this impoverished medium is challenging for both providers and consumers of feedback. We introduce VidCrit, a system for providing asynchronous feedback on drafts of edited video that incorporates favorable qualities of an in-person review. This system consists of two separate interfaces: (1) A feedback <i>recording interface</i> captures reviewers' spoken comments, mouse interactions, hand gestures and other physical reactions. (2) A feedback <i>viewing interface</i> transcribes and segments the recorded review into topical comments so that the video author can browse the review by either text or timelines. Our system features novel methods to automatically segment a long review session into topical text comments, and to label such comments with additional contextual information. We interviewed practitioners to inform a set of design guidelines for giving and receiving feedback, and based our system's design on these guidelines. Video reviewers using our system preferred our feedback recording interface over email for providing feedback due to the reduction in time and effort. In a fixed amount of time, reviewers provided 10.9 (&#963;=5.09) more local comments than when using text. All video authors rated our feedback viewing interface preferable to receiving feedback via e-mail.",0,44.6540880503
UIST,7185626c5e12ca24054279862663577506282e92,UIST,2010,Supporting self-expression for informal communication,Lisa G. Cowan,1974552,"Mobile phones are becoming the central tools for communicating and can help us keep in touch with friends and family on-the-go. However, they can also place high demands on attention and constrain interaction. My research concerns how to design communication mechanisms that mitigate these problems to support self-expression for informal communication on mobile phones. I will study how people communicate with camera-phone photos, paper-based sketches, and projected information and how this communication impacts social practices.",0,9.3023255814
UIST,021b29a501be07d4ad3769e3c12bc983d8f20521,UIST,2003,Stylus input and editing without prior selection of mode,"Eric Saund, Edward Lank","1763321, 1788496","This paper offers a solution to the <i>mode</i> problem in computer sketch/notetaking programs. Conventionally, the user must specify the intended ""draw"" or ""command"" mode prior to performing a stroke. This necessity has proven to be a barrier to the usability of pen/stylus systems. We offer a novel <i>Inferred-Mode</i> interaction protocol that avoids the mode hassles of conventional sketch systems. The system infers the user's intent, if possible, from the properties of the pen trajectory and the context of the trajectory. If the intent is ambiguous, the user is offered a choice mediator in the form of a pop-up button. To maximize the fluidity of drawing, the user is entitled to ignore the mediator and continue drawing. We present decision logic for the inferred mode protocol, and discuss subtleties learned in the course of its development. We also present results of initial user trials validating the usability of this interaction design.",49,50.0
UIST,7dac7d51ceb130112ee29e1762f17214db849b26,UIST,1991,An event-object recovery model for object-oriented user interfaces,"Haiying Wang, Mark Green","3702604, 3081643","An important aspect of interactive systems is the provision of a recovery facility that allows the user to reverse the effects of his interactions with the system. Due to differences between object-oriented and non-object-oriented methodologies, user recovery approaches used for non-object-oriented software are not suitable for object-oriented software. This paper presents an event-object user recovery model for the construction of recovery facilities in object-oriented user interfaces. Our approach divides traditional history/command lists into per-object lists which fit well with object-oriented structure. Unique features of this framework are the hierarchical structure of the local recovery objects that reflect the application structure, its simple semantics, and its ease of implementation, which greatly reduces the effort required by the interface builder to incorporate it into existing object-oriented user interface structures. We introduce this framework by describing the event-object model, defining the protocol used by the local facilities to perform user recovery, and presenting examples of how the framework is used. 1 Introduction Interactive systems, such as graphic editors, animation systems, and program development systems, usually permit a user to construct and modify objects (e.g. graphic objects and programs) in real-time. Despite the considerable attention given to usability in user interface design, users are fallible and often change their minds during interaction with these systems. So it is very important that these systems provide recovery facilities allowing the user to cancel the effects of un-wanted interactions and reverse the effects of these interactions on the object state. Such a facility not only enhances the usability of the system, but also encour-Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ages users to explore unfamiliar, yet powerful aspects of the system. Recovery features in interactive systems have been studied for a long time in several contexts. Interlisp [Tei75], COPE [Arc84], and PECAN [Rei84] are systems that have recovery facilities and US&R [Vit84] is a general recovery framework for interactive systems. Yang [Yan88] gives a comprehensive formal discussion of recovery features in interactive system. While these recovery facilities seem to take different approaches to implementing recovery, all adopt …",5,19.5652173913
UIST,5f825082d0848c55fb2d86b24bed9e87b7cf18e0,UIST,2003,A molecular architecture for creating advanced GUIs,Eric Lecolinet,1794025,"This paper presents a new GUI architecture for creating advanced interfaces. This model is based on a limited set of general principles that improve flexibility and provide capabilities for implementing information visualization techniques such as magic lenses, transparent tools or semantic zooming. This architecture also makes it possible to create multiple views and application-sharing systems (by sharing views on multiple computer screens) in a simple and uniform way and to handle bimanual interaction and multiple pointers. An experimental toolkit called <i>Ubit</i> was implemented to test the feasibility of this approach. It is based on a pseudo-declarative C++ API that tries to simplify GUI programming by providing a higher level of abstraction.",25,29.1666666667
UIST,2de72cad472ea1a4304f9a09fcac990bc00849ad,UIST,1991,MMM: a user interface architecture for shared editors on a single screen,"Eric A. Bier, Steve Freeman","1734223, 3237444","There is a growing interest in software applications that allow several users to simultaneously interact with computer applications either in the same room or at a distance. Much early work focused on sharing exnting single-user applications across a network. The Multi-Device MultiUser Multi-Editor (MMM) project is developing a user interface and software architecture to support a new generation of editors specifically designed to be used by groups, including groups who share a single screen. Each user has his or her own modes, style settings, msertlon points, and feedback. Screen space is conserved by reducing the size and number of on-screen tools. The editors use per-user data structures to respond to multiuser input.",97,86.9565217391
UIST,2dc2861d11369480d4d32ab78b2cd68cf7f9fbbd,UIST,2008,Edge-respecting brushes,"Dan R. Olsen, Mitchell K. Harris","1733794, 2262189","Digital paint is one of the more successful interactive applications of computing. Brushes that apply various effects to an image have been central to this success. Current painting techniques ignore the underlying image. By considering that image we can help the user paint more effectively. There are algorithms that assist in selecting regions to paint including flood fill, intelligent scissors and graph cut. Selected regions and the algorithms to create them introduce conceptual layers between the user and the painting task. We propose a series of ""edge-respecting brushes"" that spread paint or other effects according to the edges and texture of the image being modified. This restores the simple painting metaphor while providing assistance in working with the shapes already in the image. Our most successful fill brush algorithm uses competing least-cost-paths to identify what should be selected and what should not.",7,11.4285714286
UIST,4627f3ae1eab377768a13a31e4a40820843acc92,UIST,2008,Interactive viscosity,Dan R. Olsen,1733794,"When the Macintosh first made graphical user interfaces popular the notion of each person having their own computer was novel. Today's technology landscape is characterized by multiple computers per person many with far more capacity than that original Mac. The world of input devices, display devices and interactive techniques is far richer than those Macintosh days. Despite all of this diversity in possible interactions very few of these integrate well with each other. The monolithic isolated user interface architecture that characterized the Macintosh still dominates a great deal of today's personal computing. This talk will explore how possible ways to change that architecture so that information, interaction and communication flows more smoothly among our devices and those of our associates.",0,4.28571428571
UIST,b570d0db551d850a97f0704fd0e5299016eaa1bb,UIST,2004,Achieving higher magnification in context,"M. Sheelagh T. Carpendale, John Ligh, Eric Pattison","8238284, 3148531, 2715136","The difficulty of accessing information details while preserving context has generated many different focus-in-context techniques. A common limitation of focus-in-context techniques is their ability to work well at high magnification. We present a set of improvements that will make high magnification in context more feasible. We demonstrate new distortion functions that effectively integrate high magnification within its context. Finally, we show how lenses can be used on top of other lenses, effectively multiplying their magnification power in the same manner that a magnifying glass applied on top of another causes multiplicative magnification. The combined effect is to change feasible detail-in-context magnification factors from less than 8 to more than 40.",41,57.8947368421
UIST,5f5b438ca7d46de645e2dde9941e62151dea833e,UIST,2013,Transmogrification: causal manipulation of visualizations,"John Brosz, Miguel A. Nacenta, Richard Pusch, M. Sheelagh T. Carpendale, Christophe Hurter","2773625, 2930601, 2387952, 8238284, 2433007","A transmogrifier is a novel interface that enables quick, on-the-fly graphic transformations. A region of a graphic can be specified by a shape and transformed into a destination shape with real-time, visual feedback. Both origin and destination shapes can be circles, quadrilaterals or arbitrary shapes defined through touch. Transmogrifiers are flexible, fast and simple to create and invite use in casual InfoVis scenarios, opening the door to alternative ways of exploring and displaying existing visualizations (e.g., rectifying routes or rivers in maps), and enabling free-form prototyping of new visualizations (e.g., lenses).",13,74.3119266055
UIST,21998926f65853c7f9b9440269cd27c017f6944f,UIST,2014,Traceband: locating missing items by visual remembrance,"Farshid Tavakolizadeh, Jiawei Gu, Bahador Saket","2644400, 1767095, 3288410","Finding missing items has always been troublesome. To tackle the hassle, several systems have been suggested; yet they are inflexible due to excessive setup time, operational cost, and effectiveness. We present Traceband; a lightweight and portable bracelet, which keeps track of every targeted commonly used object that a user interacts with. Users can find the location of missing items via a web-based software portal.",1,31.007751938
UIST,2ad067e186c275116faaff1ad26858f8132a336a,UIST,2014,Graffiti fur: turning your carpet into a computer display,"Yuta Sugiura, Koki Toda, Takayuki Hoshi, Yoichi Kamiyama, Takeo Igarashi, Masahiko Inami","1799242, 2412803, 1789842, 8102219, 1717356, 1684930","We devised a display technology that utilizes the phenomenon whereby the shading properties of fur change as the fibers are raised or flattened. One can erase drawings by first flattening the fibers by sweeping the surface by hand in the fiber's growth direction, and then draw lines by raising the fibers by moving the finger in the opposite direction. These material properties can be found in various items such as carpets in our living environments. We have developed three different devices to draw patterns on a ""fur display"" utilizing this phenomenon: a roller device, a pen device and pressure projection device. Our technology can turn ordinary objects in our environment into rewritable displays without requiring or creating any non-reversible modifications to them. In addition, it can be used to present large-scale image without glare, and the images it creates require no running costs to maintain.",3,49.2248062016
UIST,76391458245726befa4defdeceb833f5d3f2f3d7,UIST,2014,Interactive exploration and selection in volumetric datasets with color tunneling,"Christophe Hurter, A. Russel Taylor, M. Sheelagh T. Carpendale, Alexandru Telea","2433007, 3001498, 8238284, 1686665","Interactive data exploration and manipulation are often hindered by dataset sizes. For 3D data, this is aggravated by occlusion, important adjacencies, and entangled patterns. Such challenges make visual interaction via common filtering techniques hard. We describe a set of real-time multi-dimensional data deformation techniques that aim to help users to easily select, analyze, and eliminate spatial and data patterns. Our techniques allow animation between view configurations, semantic filtering and view deformation. Any data subset can be selected at any step along the animation. Data can be filtered and deformed to reduce occlusion and ease complex data selections. Our techniques are simple to learn and implement, flexible, and real-time interactive with datasets of tens of millions of data points. We demonstrate our techniques on three domain areas: 2D image segmentation and manipulation, 3D medical volume exploration, and astrophysical exploration.",0,12.015503876
UIST,18bbdb4c3b071b6568859493d0fe9e4f80fa8b0a,UIST,2010,HyperSource: bridging the gap between source and code-related web sites,"Björn Hartmann, Mark Dhillon","4020023, 1802139","Programmers frequently use the Web while writing code: they search for libraries, code examples, tutorials, and documentation. This link between code and visited Web pages remains implicit today. Connecting source code and browsing histories might help programmers maintain con-text, reduce the cost of Web page re-retrieval, and enhance understanding when code is shared. This note introduces HyperSource, an IDE augmentation that associates browsing histories with source code edits. HyperSource comprises a browser extension that logs visited pages; an IDE that tracks user activity and maps pages to code edits; a source document model that tracks visited pages at a character level; and a user interface that enables interaction with these histories. We discuss relevance heuristics and privacy issues inherent in this approach. Informal log analyses and user feedback suggest that our annotation model is promising for code editing and might also apply to other document authoring tasks after refinement.",15,72.0930232558
UIST,4f1e6e84c0be84a1107fac073234fe70c61c0df2,UIST,2013,Surface haptic interactions with a TPad tablet,"Joe Mullenbach, Craig D. Shultz, Anne Marie Piper, Michael A. Peshkin, J. Edward Colgate","3152982, 1922059, 2373998, 1789986, 1700106","A TPad Tablet is a tablet computer with a variable friction touchscreen. It can create the perception of force, shape, and texture on a fingertip, enabling unique and novel haptic interactions on a flat touchscreen surface. We have created an affordable and easy to use variable friction device and have made it available through the open-hardware TPad Tablet Project. We present this device as a potential research platform as well as demonstrate two applications: remote touch communication and rapid haptic sketching.",5,55.9633027523
UIST,4c7d15d0cae8ab45066dce399ff6fd4b635a5d54,UIST,2001,Guided gesture support in the paper PDA,"Daniel Avrahami, Scott E. Hudson, Thomas P. Moran, Brian D. Williams","2667384, 1749296, 2909984, 1959099","Ordinary paper offers properties of readability, fluidity, flexibility, cost, and portability that current electronic devices are often hard pressed to match. In fact, a lofty goal for many interactive systems is to be ""as easy to use as pencil and paper"". However, the static nature of paper does not support a number of capabilities, such as search and hyperlinking that an electronic device can provide. The <i>Paper PDA</i> project explores ways in which hybrid paper electronic interfaces can bring some of the capabilities of the electronic medium to interactions occurring on real paper. Key to this effort is the invention of on-paper interaction techniques which retain the flexibility and fluidity of normal pen and paper, but which are structured enough to allow robust interpretation and processing in the digital world. This paper considers the design of a class of simple printed templates that allow users to make common marks in a fluid fashion, and allow additional gestures to be invented by the users to meet their needs, but at the same time encourages marks that are quite easy to recognize.",11,20.0
UIST,02b0a6e5c91ea187893385b70a43378d0de8fa67,UIST,2015,Capture-Time Feedback for Recording Scripted Narration,"Steve Rubin, Floraine Berthouzoz, Gautham J. Mysore, Maneesh Agrawala","3280503, 2842099, 1781063, 1820412","Well-performed audio narrations are a hallmark of captivating podcasts, explainer videos, radio stories, and movie trailers. To record these narrations, professional voiceover actors follow guidelines that describe how to use low-level vocal components---volume, pitch, timbre, and tempo---to deliver performances that emphasize important words while maintaining variety, flow and diction. Yet, these techniques are not well-known outside the professional voiceover community, especially among hobbyist producers looking to create their own narrations. We present Narration Coach, an interface that assists novice users in recording scripted narrations. As a user records her narration, our system synchronizes the takes to her script, provides text feedback about how well she is meeting the expert voiceover guidelines, and resynthesizes her recordings to help her hear how she can speak better.",2,60.5263157895
UIST,3c79c967c2cb2e5e69f4b20688d0102a3bb28be3,UIST,2014,Glance: rapidly coding behavioral video with the crowd,"Walter S. Lasecki, Mitchell Gordon, Danai Koutra, Malte F. Jung, Steven Dow, Jeffrey P. Bigham","2598433, 2216996, 2479152, 2360284, 5319364, 1744846","Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions. In this paper, we present Glance, a tool that allows researchers to rapidly query, sample, and analyze large video datasets for behavioral events that are hard to detect automatically. Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data. Glance provides analysts with rapid responses when initially exploring a dataset, and reliable codings when refining an analysis. Our experiments show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously, and can get initial feedback to analysts in under 10 seconds for most clips. We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data, and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers. Glance's rapid responses to natural language queries, feedback regarding question ambiguity and anomalies in the data, and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data - opening up new possibilities for naturally exploring video data.",30,98.4496124031
UIST,3e202588b345019f645f3af6ac41a86cbddab450,UIST,2016,QuickCut: An Interactive Tool for Editing Narrated Video,"Anh Truong, Floraine Berthouzoz, Wilmot Li, Maneesh Agrawala","6684681, 2842099, 2812691, 1820412","We present QuickCut, an interactive video editing tool designed to help authors efficiently edit narrated video. QuickCut takes an audio recording of the narration voiceover and a collection of raw video footage as input. Users then review the raw footage and provide spoken annotations describing the relevant actions and objects in the scene. QuickCut time-aligns a transcript of the annotations with the raw footage and a transcript of the narration to the voiceover. These aligned transcripts enable authors to quickly match story events in the narration with semantically relevant video segments and form alignment constraints between them. Given a set of such constraints, QuickCut applies dynamic programming optimization to choose frame-level cut points between the video segments while maintaining alignments with the narration and adhering to low-level film editing guidelines. We demonstrate QuickCut's effectiveness by using it to generate a variety of short (less than 2 minutes) narrated videos. Each result required between 14 and 52 minutes of user time to edit (i.e. between 8 and 31 minutes for each minute of output video), which is far less than typical authoring times with existing video editing workflows.",1,92.7672955975
UIST,ec7dca4695dd88ea4cba097e354cd83f5836b0e0,UIST,2016,Designing a Haptic Feedback System for Hearing-Impaired to Experience Tap Dance,"Mina Shibasaki, Yoichi Kamiyama, Kouta Minamizawa","2395671, 8102219, 1711743","In this study, we have designed a system to enable hearing-impaired to enjoy the performance of tap dancers. This system transfers the haptic sensation of tap dancing from the stage to the audience and helps hearing-impaired people enjoy the vibration of the taps even if they cannot hear the sound. We organized an event to verify the effectiveness of the system. To do this, we collaborated with a tap dance unit and science museum. We found that our system succeeded in helping the tap dancers share the fun and enjoyment of dance with the audience comprising people with hearing disabilities.",0,44.6540880503
UIST,ba91477e35153327dab6cdccbeaadb823d3f1940,UIST,2016,floatio: Floating Tangible User Interface Based on Animacy Perception,"Toshiya Yui, Tomoko Hashida","2149381, 1807316","In this study, we propose floatio: a floating tangible user interface that makes it easy to create a perception of animacy (lifelike movement). It has been pointed out that there are three requirements that make animacy more likely to be perceived: interactivity, irregularities, and automatic movement resisting the force of gravity. Based on these requirements, floatio provides a tangible user interface where a polystyrene ball resembling a pixel is suspended in a stream of air where it can be positioned passively by the user, or autonomously by the system itself. To implement floatio, we developed three mechanisms: a floating field mechanism, a pointer input/output mechanism and a hand-over mechanism. We also measured the precision of the pointer input/output and hand-over mechanisms.",0,44.6540880503
UIST,216541c66f597260e43cb8b0f0907526fe0a99ea,UIST,2015,Foobaz: Variable Name Feedback for Student Code at Scale,"Elena L. Glassman, Lyla Fischer, Jeremy Scott, Rob Miller","2459941, 2873138, 1861338, 1723785","Current traditional feedback methods, such as hand-grading student code for substance and style, are labor intensive and do not scale. We created a user interface that addresses feedback at scale for a particular and important aspect of code quality: variable names. We built this user interface on top of an existing back-end that distinguishes variables by their behavior in the program. Therefore our interface not only allows teachers to comment on poor variable names, they can comment on names that mislead the reader about the variable's role in the program. We ran two user studies in which 10 teachers and 6 students created and received feedback, respectively. The interface helped teachers give personalized variable name feedback on thousands of student solutions from an edX introductory programming MOOC. In the second study, students composed solutions to the same programming assignments and immediately received personalized quizzes composed by teachers in the previous user study.",1,42.1052631579
UIST,5626c8a9a361e6f5e20719d04599f3bc6c95d071,UIST,2011,STIMTAC: a tactile input device with programmable friction,"Michel Amberg, Frédéric Giraud, Betty Lemaire-Semail, Paolo Olivo, Géry Casiez, Nicolas Roussel","3290647, 2226969, 3205996, 2380080, 3051289, 1728921","We present the STIMTAC, a touchpad device that supports friction reduction. Contrary to traditional vibrotactile approaches, the STIMTAC provides information passively, acting as a texture display. It does not transfer energy to the user but modifies how energy is dissipated within the contact area by a user-initiated friction process. We report on the iterative process that led to the current hardware design and briefly describe the software framework that we are developing to illustrate its potential.",14,56.6666666667
UIST,f6f8109136954743f819b1c778689cbe30723e3d,UIST,2016,JOLED: A Mid-air Display based on Electrostatic Rotation of Levitated Janus Objects,"Deepak Ranjan Sahoo, Takuto Nakamura, Asier Marzo, Themis Omirou, Michihiro Asakawa, Sriram Subramanian","2548351, 2891348, 2043267, 3186291, 2843787, 1702794","We present JOLED, a mid-air display for interactive physical visualization using Janus objects as physical voxels. The Janus objects have special surfaces that have two or more asymmetric physical properties at different areas. In JOLED, they are levitated in mid-air and controllably rotated to reveal their different physical properties. We made voxels by coating the hemispheres of expanded polystyrene beads with different materials, and applied a thin patch of titanium dioxide to induce electrostatic charge on them. Transparent indium tin oxide electrodes are used around the levitation volume to create a tailored electric field to control the orientation of the voxels. We propose a novel method to control the angular position of individual voxels in a grid using electrostatic rotation and their 3D position using acoustic levitation. We present a display in which voxels can be flipped independently, and two mid-air physical games with a voxel as the playable character that moves in 3D across other physical structures and rotates to reflect its status in the games. We demonstrate a voxel update speed of 37.8 ms/flip, which is video-rate.",0,44.6540880503
UIST,2536bf533421f8889f37eba9f6adad4f77f4c866,UIST,2015,Tiltcasting: 3D Interaction on Large Displays using a Mobile Device,"Krzysztof Pietroszek, James R. Wallace, Edward Lank","2408097, 2092309, 1788496","We develop and formally evaluate a metaphor for smartphone interaction with 3D environments: Tiltcasting. Under the Tiltcasting metaphor, users interact within a rotatable 2D plane that is ""cast"" from their phone's interactive display into 3D space. Through an empirical validation, we show that Tiltcasting supports efficient pointing, interaction with occluded objects, disambiguation between nearby objects, and object selection and manipulation in fully addressable 3D space. Our technique out-performs existing target agnostic pointing implementations, and approaches the performance of physical pointing with an off-the-shelf smartphone.",2,60.5263157895
UIST,ba88ad315de49c00db20150cab16c668ec6bd3a6,UIST,2002,Query-by-critique: spoken language access to large lists,"Dan R. Olsen, Jon R. Peachey","1733794, 2168728","Spoken language interfaces provide highly mobile, small form-factor, hands-free, eyes-free interaction with information. Uniform access to large lists of information using spoken interfaces is highly desirable, but problematic due to inherent limitations of speech. A speech widget for lists of attributed objects is described that provides for approximate queries to retrieve desired items. User tests demonstrate that this is an effective technique for accessing information using speech.",2,6.25
UIST,9ba6fe1940b6993943fc6ac6bcf05059046aa6ca,UIST,2005,Preference elicitation for interface optimization,"Krzysztof Z. Gajos, Daniel S. Weld","1770992, 1780531","Decision-theoretic optimization is becoming a popular tool in the user interface community, but creating accurate cost (or utility) functions has become a bottleneck --- in most cases the numerous parameters of these functions are chosen manually, which is a tedious and error-prone process. This paper describes ARNAULD, a general interactive tool for eliciting user preferences concerning concrete outcomes and using this feedback to automatically learn a factored cost function. We empirically evaluate our machine learning algorithm and two automatic query generation approaches and report on an informal user study.",59,74.1935483871
UIST,11e782491c747b8ed01583766af9c15a1f55ec36,UIST,2009,Interactive viscosity,"Stephen L. Macknik, Susana Martinez-Conde","1969116, 7650679","Your conscious experience is not a function of the world, it is a function of the neural networks of your brain. Therefore, the biology of the brain and consciousness is as fundamental to understanding the universe, as we know it, as the high-energy physics of subatomic particles. This is especially true for the study of sensory and cognitive illusions, since they represent effects that clearly stand out as not representing the real world. That is, since illusions don't match reality we can know that by studying illusions we are studying exactly what the brain is actually doing, and not just what we think the brain should be doing. Your brain does a staggering amount of pragmatic self-dealing guesswork and outright confabulation in order to construct the highly imperfect mental simulation of reality known as ""consciousness."" This is not to say that objective reality isn't ""out there"" in a very real sense--but no one lives there. No one's ever even been there for a visit. Ironically, the fact that consciousness feels like a solid, robust, fact-rich transcript of reality is just one of the countless illusions your brain creates for itself.
 Illusions are not errors of the brain. Far from it. Illusions arise from processes that are critical to our survival. Our brains have developed illusory processes so that we may experience the world in a ready-to-consume manner. Remove the machinery of illusion, and you unwind the entire tapestry of human awareness. Illusions are those perceptual experiences that do not match the physical reality. They are therefore exquisite tools with which to analyze the neural correlates of human perception and consciousness. Neuroscientists have long known that they can only be sure of where they stand, in terms of correlating neural responses to awareness, when they correlate the awareness of an illusion to the brain's response, specifically because of the illusions' mismatch with reality. The study of illusions is therefore of critical importance to the understanding of the basic mechanisms of sensory perception and conscious awareness.
 If you've ever seen a good magician perform, you know how thrilling it is to watch the impossible happening before your eyes. The laws of physics, probability, psychology and common sense--the four trusty compass points in your mental map of reality--are suddenly turned into liabilities. Objects and people appear, vanish, levitate, transpose, transform, and with all your smarts you can't imagine how it's being done. Magicians are the premier artists of attention and awareness, and they manipulate our cognition like clay on a potter's wheel.
 And the mechanisms underlying magic perception have implications for our daily lives. The magical arts work because humans have hardwired processes of attention and awareness that are hackable. By understanding how magicians hack our brains, we can better understand how we work.",0,2.85714285714
UIST,044f06c01c0076802f80d4a8813bceba8477c8cc,UIST,2007,Automatically generating user interfaces adapted to users' motor and vision capabilities,"Krzysztof Z. Gajos, Jacob O. Wobbrock, Daniel S. Weld","1770992, 1796045, 1780531","Most of today's GUIs are designed for the typical, able-bodied user; atypical users are, for the most part, left to adapt as best they can, perhaps using specialized assistive technologies as an aid. In this paper, we present an alternative approach: SUPPLE++ automatically generates interfaces which are tailored to an individual's motor capabilities and can be easily adjusted to accommodate varying vision capabilities. SUPPLE++ models users. motor capabilities based on a onetime motor performance test and uses this model in an optimization process, generating a personalized interface. A preliminary study indicates that while there is still room for improvement, SUPPLE++ allowed one user to complete tasks that she could not perform using a standard interface, while for the remaining users it resulted in an average time savings of 20%, ranging from an slowdown of 3% to a speedup of 43%.",54,72.2222222222
UIST,2d496f4c54713ff6b5b6cbbb3f3a7ebf1428b4ff,UIST,2003,Dynamo: a public interactive surface supporting the cooperative sharing and exchange of media,"Shahram Izadi, Harry Brignull, Tom Rodden, Yvonne Rogers, Mia Underwood","1699068, 2967611, 1680844, 1685816, 2388334","In this paper we propose a novel way of supporting occasional meetings that take place in unfamiliar public places, which promotes lightweight, visible and fluid collaboration. Our central idea is that the sharing and exchange of information occurs across public surfaces that users can easily access and interact with. To this end, we designed and implemented Dynamo, a communal multi-user interactive surface. The surface supports the cooperative sharing and exchange of a wide range of media that can be brought to the surface by users that are remote from their familiar organizational settings.",158,91.6666666667
UIST,a15f78310f71e5706b34bcce213c1e650bb7bb01,UIST,2014,Interacting with massive numbers of student solutions,Elena L. Glassman,2459941,"When teaching programming or hardware design, it is pedagogically valuable for students to generate examples of functions, circuits, or system designs. Teachers can be overwhelmed by these types of student submissions when running large residential or recently released massive online courses. The underlying distribution of student solutions submitted in response to a particular assignment may be complex, but the newly available volume of student solutions represents a denser sampling of that distribution. Working with large datasets of students' solutions, I am building systems with user interfaces that allow teachers to explore the variety of their students' correct and incorrect solutions. Forum posts, grading rubrics, and automatic graders can be based on student solution data, and turn massive engineering and computer science classrooms into useful insight and feedback for teachers. In the development process, I hope to describe essential design principles for such systems.",1,31.007751938
UIST,339262510f63cf87887871642109834f1554c3b2,UIST,2010,PETALS: a visual interface for landmine detection,"Lahiru G. Jayatilaka, Luca F. Bertuccelli, James Staszewski, Krzysztof Z. Gajos","2587775, 3276069, 2902995, 1770992","Post-conflict landmines have serious humanitarian repercussions: landmines cost lives, limbs and land. The primary method used to locate these buried devices relies on the inherently dangerous and difficult task of a human listening to audio feedback from a metal detector. Researchers have previously hypothesized that expert operators respond to these challenges by building mental patterns with metal detectors through the identification of object-dependent spatially distributed metallic fields. This paper presents the preliminary stages of a novel interface - Pattern Enhancement Tool for Assisting Landmine Sensing (PETALS) - that aims to assist with building and visualizing these patterns, rather than relying on memory alone. Simulated demining experiments show that the experimental interface decreases classification error from 23% to 5% and reduces localization error by 54%, demonstrating the potential for PETALS to improve novice deminer safety and efficiency.",0,9.3023255814
UIST,1bcd883ed748899196bde3d84d32afb585645e42,UIST,2015,Using Personal Devices to Facilitate Multi-user Interaction with Large Display Walls,Ulrich von Zadow,2457344,"Large display walls and personal devices such as Smartphones have complementary characteristics. While large displays are well-suited to multi-user interaction (potentially with complex data), they are inherently public and generally cannot present an interface adapted to the individual user. However, effective multi-user interaction in many cases depends on the ability to tailor the interface, to interact without interfering with others, and to access and possibly share private data. The combination with personal devices facilitates exactly this. Multi-device interaction concepts enable data transfer and include moving parts of UIs to the personal device. In addition, hand-held devices can be used to present personal views to the user. Our work will focus on using personal devices for true multi-user interaction with interactive display walls. It will cover appropriate interaction techniques as well as the technical foundation and will be validated with corresponding application cases.",0,16.2280701754
UIST,75931f1c24c3596cd89dc12e93582c2377424a5e,UIST,2001,A framework for unifying presentation space,"M. Sheelagh T. Carpendale, Catherine Montagnese","8238284, 7362906","Making effective use of the available display space has long been a fundamental issue in user interface design. We live in a time of rapid advances in available CPU power and memory. However, the common sizes of our computational display spaces have only minimally increased or in some cases, such as hand held devices, actually decreased. In addition, the size and scope of the information spaces we wish to explore are also expanding. Representing vast amounts of information on our relatively small screens has become increasingly problematic and has been associated with problems in navigation, interpretation and recognition. User interface research has proposed several differing presentation approaches to address these problems. These methods create displays that vary considerably, visually and algorithmically. We present a unified framework that provides a way of relating seemingly distinct methods, facilitating the inclusion of more than one presentation method in a single interface. Furthermore, it supports extrapolation between the presentation methods it describes. Of particular interest are the presentation possibilities that exist in the ranges between various distortion presentations, magnified insets and detail-in-context presentations, and between detail-in-context presentations and a full-zooming environment. This unified framework offers a geometric presentation library in which presentation variations are available independently of the mode of graphic representation. The intention is to promote the ease of exploration and experimentation into the use of varied presentation combinations.",93,66.6666666667
UIST,c01192c7d4b57bc5406d8c2c0e0411af4521ea8a,UIST,2007,Evaluating user interface systems research,Dan R. Olsen,1733794,"The development of user interface systems has languished with the stability of desktop computing. Future systems, however, that are off-the-desktop, nomadic or physical in nature will involve new devices and new software systems for creating interactive applications. Simple usability testing is not adequate for evaluating complex systems. The problems with evaluating systems work are explored and a set of criteria for evaluating new UI systems work is presented.",71,86.1111111111
UIST,0ac62d0f438606dcd794358abecf6b9a9636ab1a,UIST,2011,Platemate: crowdsourcing nutritional analysis from food photographs,"Jon Noronha, Eric Hysen, Haoqi Zhang, Krzysztof Z. Gajos","2714595, 3023163, 3162562, 1770992","We introduce PlateMate, a system that allows users to take photos of their meals and receive estimates of food intake and composition. Accurate awareness of this information can help people monitor their progress towards dieting goals, but current methods for food logging via self-reporting, expert observation, or algorithmic analysis are time-consuming, expensive, or inaccurate. PlateMate crowdsources nutritional analysis from photographs using Amazon Mechanical Turk, automatically coordinating untrained workers to estimate a meal's calories, fat, carbohydrates, and protein. We present the Management framework for crowdsourcing complex tasks, which supports PlateMate's nutrition analysis workflow. Results of our evaluations show that PlateMate is nearly as accurate as a trained dietitian and easier to use for most users than traditional self-reporting.",80,94.2857142857
UIST,27f32908fed9c4a0fdbbbce799fb48655a05c11e,UIST,2015,Effective Interactions for Personalizing Spatial Visualizations of Collections,"Kenneth C. Arnold, Krzysztof Z. Gajos","3170193, 1770992","Interactive spatial visualizations powered by machine learning will help us explore and understand large collections in meaningful ways, but little is yet known about the design space of interactions. We ran a pilot user study to compare two different interaction techniques: a ""grouping?"" interaction adapted from interactive clustering, and an existing ""positioning?"" interaction. We identified three important dimensions of the interaction design space that inform future design of more intuitive and expressive interactions.",0,16.2280701754
UIST,d6ca0e4fa23cca3fe05cb0f2cd10e9c633d0cdfa,UIST,2016,On Suggesting Phrases vs. Predicting Words for Mobile Text Composition,"Kenneth C. Arnold, Krzysztof Z. Gajos, Adam Tauman Kalai","3170193, 1770992, 2186481","A system capable of suggesting multi-word phrases while someone is writing could supply ideas about content and phrasing and allow those ideas to be inserted efficiently. Meanwhile, statistical language modeling has provided various approaches to predicting phrases that users type. We introduce a simple extension to the familiar mobile keyboard suggestion interface that presents phrase suggestions that can be accepted by a repeated-tap gesture. In an extended composition task, we found that phrases were interpreted as suggestions that affected the content of what participants wrote more than conventional single-word suggestions, which were interpreted as predictions. We highlight a design challenge: how can a phrase suggestion system make valuable suggestions rather than just accurate predictions'",0,44.6540880503
UIST,0ab904c7cd7a9ce53a5a6b583bd923e7b0b5af10,UIST,2011,A tongue input device for creating conversations,"Ronit Slyper, Jill Fain Lehman, Jodi Forlizzi, Jessica K. Hodgins","2061386, 2142730, 1767344, 1788773","We present a new tongue input device, the tongue joystick, for use by an actor inside an articulated-head character costume. Using our device, the actor can maneuver through a dialogue tree, selecting clips of prerecorded audio to hold a conversation in the voice of the character. The device is constructed of silicone sewn with conductive thread, a unique method for creating rugged, soft, low-actuation force devices. This method has application for entertainment and assistive technology. We compare our device against other portable mouth input devices, showing it to be the fastest and most accurate in tasks mimicking our target application. Finally, we show early results of an actor inside an articulated-head costume using the tongue joystick to interact with a child.",7,41.9047619048
UIST,0542c7a8800d1f3b0314191ca82b85fbfac4dcf0,UIST,2004,ScreenCrayons: annotating anything,"Dan R. Olsen, Trent Taufer, Jerry Alan Fails","1733794, 2303997, 3141061","ScreenCrayons is a system for collecting annotations on any type of document or visual information from any application. The basis for the system is a screen capture upon which the user can highlight the relevant portions of the image. The user can define any number of topics for organizing notes. Each topic is associated with a highlighting ""crayon."" In addition the user can supply annotations in digital ink or text. Algorithms are described that summarize captured images based on the highlight strokes so as to provide overviews of many annotations as well as being able to ""zoom in"" on particular information about a given note and the context of that note.",34,40.7894736842
UIST,0ed33701bd172eeadf023fd83c220533e43dd0f6,UIST,2010,Cosaliency: where people look when comparing images,"David E. Jacobs, Dan B. Goldman, Eli Shechtman","3049679, 1976171, 2177801","Image triage is a common task in digital photography. Determining which photos are worth processing for sharing with friends and family and which should be deleted to make room for new ones can be a challenge, especially on a device with a small screen like a mobile phone or camera. In this work we explore the importance of local structure changes?e.g. human pose, appearance changes, object orientation, etc.?to the photographic triage task. We perform a user study in which subjects are asked to mark regions of image pairs most useful in making triage decisions. From this data, we train a model for image saliency in the context of other images that we call <i>cosaliency</i>. This allows us to create <i>collection-aware</i> crops that can augment the information provided by existing thumbnailing techniques for the image triage task.",13,69.7674418605
UIST,807b9c73800f59ac191c0a43c242ef79ba5ec253,UIST,2011,Proactive wrangling: mixed-initiative end-user programming of data transformation scripts,"Philip J. Guo, Sean Kandel, Joseph M. Hellerstein, Jeffrey Heer","2251384, 7786415, 1695576, 1803140","Analysts regularly wrangle data into a form suitable for computational tools through a tedious process that delays more substantive analysis. While interactive tools can assist data transformation, analysts must still conceptualize the desired output state, formulate a transformation strategy, and specify complex transforms. We present a model to proactively suggest data transforms which map input data to a relational format expected by analysis tools. To guide search through the space of transforms, we propose a metric that scores tables according to type homogeneity, sparsity and the presence of delimiters. When compared to ""ideal"" hand-crafted transformations, our model suggests over half of the needed steps; in these cases the top-ranked suggestion is preferred 77% of the time. User study results indicate that suggestions produced by our model can assist analysts' transformation tasks, but that users do not always value proactive assistance, instead preferring to maintain the initiative. We discuss some implications of these results for mixed-initiative interfaces.",14,56.6666666667
UIST,b15bdd249dd7aed82c7a77d57006a77392ae2bc5,UIST,2016,Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography,"Yang Zhang, Robert Xiao, Chris Harrison","4449904, 1681726, 1730920","Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.",0,44.6540880503
UIST,0c0e978730ed230767b4d1b2b5a20dd941bb0d49,UIST,2007,Gui --- phooey!: the case for text input,"Max Van Kleek, Michael S. Bernstein, David R. Karger, Monica M. C. Schraefel","2341082, 3047089, 1743286, 2284695","Information cannot be found if it is not recorded. Existing rich graphical application approaches interfere with user input in many ways, forcing complex interactions to enter simple information, requiring complex cognition to decide where the data should be stored, and limiting the kind of information that can be entered to what can fit into specific applications' data models. Freeform text entry suffers from none of these limitations but produces data that is hard to retrieve or visualize. We describe the design and implementation of Jourknow, a system that aims to bridge these two modalities, supporting lightweight text entry and weightless context capture that produces enough structure to support rich interactive presentation and retrieval of the arbitrary information entered.",23,36.1111111111
UIST,307e373c964da154472cf24ac8af56af76ff5755,UIST,2014,SideSwipe: detecting in-air gestures around mobile devices using actual GSM signal,"Chen Zhao, Ke-Yu Chen, Md Tanvir Islam Aumi, Shwetak N. Patel, Matthew S. Reynolds","1963231, 3233043, 2855795, 1701358, 2428393","Current smartphone inputs are limited to physical buttons, touchscreens, cameras or built-in sensors. These approaches either require a dedicated surface or line-of-sight for interaction. We introduce SideSwipe, a novel system that enables in-air gestures both above and around a mobile device. Our system leverages the actual GSM signal to detect hand gestures around the device. We developed an algorithm to convert the discrete and bursty GSM pulses to a continuous wave that can be used for gesture recognition. Specifically, when a user waves their hand near the phone, the hand movement disturbs the signal propagation between the phone's transmitter and added receiving antennas. Our system captures this variation and uses it for gesture recognition. To evaluate our system, we conduct a study with 10 participants and present robust gesture recognition with an average accuracy of 87.2% across 14 hand gestures.",8,75.9689922481
UIST,2d73e27db335eab39f9a8fa90714d9286bbca6ce,UIST,2012,SlickFeel: sliding and clicking haptic feedback on a touchscreen,"Xiaowei Dai, Jiawei Gu, Xiang Cao, J. Edward Colgate, Hong Z. Tan","7529033, 1767095, 7299595, 1700106, 1698913","We present SlickFeel, a single haptic display setup that can deliver two distinct types of feedback to a finger on a touchscreen during typical operations of sliding and clicking. Sliding feedback enables the sliding finger to feel interactive objects on a touchscreen through variations in friction. Clicking feedback provides a key-click sensation for confirming a key or button click. Two scenarios have been developed to demonstrate the utility of the two haptic effects. In the first, simple button-click scenario, a user feels the positions of four buttons on a touchscreen by sliding a finger over them and feels a simulated key-click signal by pressing on any of the buttons. In the second scenario, the advantage of haptic feedback is demonstrated in a haptically-enhanced thumb-typing scenario. A user enters text on a touchscreen with two thumbs without having to monitor the thumbs' locations on the screen. By integrating SlickFeel with a Kindle Fire tablet, we show that it can be used with existing mobile touchscreen devices.",2,33.8235294118
UIST,1e5f33a4843fe609aedd060c50278d0d62e0adc8,UIST,2013,Attribit: content creation with semantic attributes,"Siddhartha Chaudhuri, Evangelos Kalogerakis, Stephen Giguere, Thomas A. Funkhouser","7218171, 2808670, 1837707, 1807080","We present AttribIt, an approach for people to create visual content using relative semantic attributes expressed in linguistic terms. During an off-line processing step, AttribIt learns semantic attributes for design components that reflect the high-level intent people may have for creating content in a domain (e.g. adjectives such as ""dangerous"", ""scary"" or ""strong"") and ranks them according to the strength of each learned attribute. Then, during an interactive design session, a person can explore different combinations of visual components using commands based on relative attributes (e.g. ""make this part more dangerous""). Novel designs are assembled in real-time as the strengths of selected attributes are varied, enabling rapid, in-situ exploration of candidate designs. We applied this approach to 3D modeling and web design. Experiments suggest this interface is an effective alternative for novices performing tasks with high-level design goals.",23,85.3211009174
UIST,78f1084aa75b00deb3357e5b73e21bb9c3750fe7,UIST,2016,ERICA: Interaction Mining Mobile Apps,"Biplab Deka, Zifeng Huang, Ranjitha Kumar","2653698, 7945682, 1816562","Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present <i>interaction mining</i> -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user <i>interaction trace</i>. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common <i>user flows</i>. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as <i>searching</i> or <i>composing</i>. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.",1,92.7672955975
UIST,90a30d9707f357cfca99b7848922cc950c31d49b,UIST,2013,Sauron: embedded single-camera sensing of printed physical user interfaces,"Valkyrie Savage, Colin Chang, Björn Hartmann","2973941, 2110106, 4020023","3D printers enable designers and makers to rapidly produce physical models of future products. Today these physical prototypes are mostly passive. Our research goal is to enable users to turn models produced on commodity 3D printers into interactive objects with a minimum of required assembly or instrumentation. We present Sauron, an embedded machine vision-based system for sensing human input on physical controls like buttons, sliders, and joysticks. With Sauron, designers attach a single camera with integrated ring light to a printed prototype. This camera observes the interior portions of input components to determine their state. In many prototypes, input components may be occluded or outside the viewing frustum of a single camera. We introduce algorithms that generate internal geometry and calculate mirror placements to redirect input motion into the visible camera area. To investigate the space of designs that can be built with Sauron along with its limitations, we built prototype devices, evaluated the suitability of existing models for vision sensing, and performed an informal study with three CAD users. While our approach imposes some constraints on device design, results suggest that it is expressive and accessible enough to enable constructing a useful variety of devices.",31,93.5779816514
UIST,1812f444b31f23a496da8452252b6f67358ffe74,UIST,2013,Good vibrations: an evaluation of vibrotactile impedance matching for low power wearable applications,"Jack Lindsay, Iris Jiang, Eric C. Larson, Richard J. Adams, Shwetak N. Patel, Blake Hannaford","3157184, 2148756, 2340684, 2960004, 1701358, 2036485","Vibrotactile devices suffer from poor energy efficiency, arising from a mismatch between the device and the impedance of the human skin. This results in over-sized actuators and excessive power consumption, and prevents development of more sophisticated, miniaturized and low-power mobile tactile devices. In this paper, we present the experimental evaluation of a vibrotactile system designed to match the impedance of the skin to the impedance of the actuator. This system is able to quadruple the motion of the skin without increasing power consumption, and produce sensations equivalent to a standard system while consuming 1/2 of the power. By greatly reducing the size and power constraints of vibrotactile actuators, this technology offers a means to realize more sophisticated, smaller haptic devices for the user interface community.",0,10.5504587156
UIST,52a386eb776569f579988bc78e0b2b3c5625645f,UIST,2014,A series of tubes: adding interactivity to 3D prints using internal pipes,"Valkyrie Savage, Ryan M. Schmidt, Tovi Grossman, George W. Fitzmaurice, Björn Hartmann","2973941, 2959976, 3313809, 1703735, 4020023","3D printers offer extraordinary flexibility for prototyping the shape and mechanical function of objects. We investigate how 3D models can be modified to facilitate the creation of interactive objects that offer dynamic input and output. We introduce a general technique for supporting the rapid prototyping of interactivity by removing interior material from 3D models to form internal pipes. We describe this new design space of pipes for interaction design, where variables include openings, path constraints, topologies, and inserted media. We then present PipeDream, a tool for routing such pipes through the interior of 3D models, integrated within a 3D modeling program. We use two distinct routing algorithms. The first has users define pipes' terminals, and uses path routing and physics-based simulation to minimize pipe bending energy, allowing easy insertion of media post-print. The second allows users to supply a desired internal shape to which we fit a pipe route: for this we describe a graph-routing algorithm. We present several prototypes created using our tool to show its flexibility and potential.",19,91.8604651163
UIST,119bfd8b14e3dd31e364bceea9827e8f969b0dcb,UIST,2013,uTrack: 3D input using two magnetic sensors,"Ke-Yu Chen, Kent Lyons, Sean White, Shwetak N. Patel","3233043, 2073793, 5047258, 1701358","While much progress has been made in wearable computing in recent years, input techniques remain a key challenge. In this paper, we introduce uTrack, a technique to convert the thumb and fingers into a 3D input system using magnetic field (MF) sensing. A user wears a pair of magnetometers on the back of their fingers and a permanent magnet affixed to the back of the thumb. By moving the thumb across the fingers, we obtain a continuous input stream that can be used for 3D pointing. Specifically, our novel algorithm calculates the magnet's 3D position and tilt angle directly from the sensor readings. We evaluated uTrack as an input device, showing an average tracking accuracy of 4.84 mm in 3D space - sufficient for subtle interaction. We also demonstrate a real-time prototype and example applications allowing users to interact with the computer using 3D finger input.",35,95.4128440367
UIST,2f5780ca50bc21e6190ab45fff32a388ec585624,UIST,2012,Midas: fabricating custom capacitive touch sensors to prototype interactive objects,"Valkyrie Savage, Xiaohan Zhang, Björn Hartmann","2973941, 2682457, 4020023","An increasing number of consumer products include user interfaces that rely on touch input. While digital fabrication techniques such as 3D printing make it easier to prototype the shape of custom devices, adding interactivity to such prototypes remains a challenge for many designers. We introduce Midas, a software and hardware toolkit to support the design, fabrication, and programming of flexible capacitive touch sensors for interactive objects. With Midas, designers first define the desired shape, layout, and type of touch sensitive areas, as well as routing obstacles, in a sensor editor. From this high-level specification, Midas automatically generates layout files with appropriate sensor pads and routed connections. These files are then used to fabricate sensors using digital fabrication processes, e.g., vinyl cutters and conductive ink printers. Using step-by-step assembly instructions generated by Midas, designers connect these sensors to the Midas microcontroller, which detects touch events. Once the prototype is assembled, designers can define interactivity for their sensors: Midas supports both record-and-replay actions for controlling existing local applications and WebSocket-based event output for controlling novel or remote applications. In a first-use study with three participants, users successfully prototyped media players. We also demonstrate how Midas can be used to create a number of touch-sensitive interfaces.",51,96.0784313725
UIST,18bb77c622e63e286abd4a0a9b006abb0dabbc53,UIST,2012,GripSense: using built-in sensors to detect hand posture and pressure on commodity mobile phones,"Mayank Goel, Jacob O. Wobbrock, Shwetak N. Patel","1916388, 1796045, 1701358","We introduce GripSense, a system that leverages mobile device touchscreens and their built-in inertial sensors and vibration motor to infer hand postures including one- or two-handed interaction, use of thumb or index finger, or use on a table. GripSense also senses the amount of pres-sure a user exerts on the touchscreen despite a lack of direct pressure sensors by inferring from gyroscope readings when the vibration motor is ""pulsed."" In a controlled study with 10 participants, GripSense accurately differentiated device usage on a table vs. in hand with 99.67% accuracy and when in hand, it inferred hand postures with 84.26% accuracy. In addition, GripSense distinguished three levels of pressure with 95.1% accuracy. A usability analysis of GripSense was conducted in three custom applications and showed that pressure input and hand-posture sensing can be useful in a number of scenarios.",47,93.6274509804
UIST,1c0e66d39d38642b85752d69a181afbe1626ae5d,UIST,2007,Blui: low-cost localized blowable user interfaces,"Shwetak N. Patel, Gregory D. Abowd","1701358, 1732524","We describe a unique form of hands-free interaction that can be implemented on most commodity computing platforms. Our approach supports blowing at a laptop or computer screen to directly control certain interactive applications. Localization estimates are produced in real-time to determine where on the screen the person is blowing. Our approach relies solely on a single microphone, such as those already embedded in a standard laptop or one placed near a computer monitor, which makes our approach very cost-effective and easy-to-deploy. We show example interaction techniques that leverage this approach.",19,25.0
UIST,7bf88b45afb9400a347574ae3420b0db15fd6674,UIST,2016,Stretchis: Fabricating Highly Stretchable User Interfaces,"Michael Wessely, Theophanis Tsandilas, Wendy E. Mackay","2886214, 2110778, 1732917","Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. We present a novel and inexpensive fabrication method for creating <i>Stretchis</i>, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a <i>Stretchi</i> and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin (&#8776; 200&#956;<i>m</i>), flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.",0,44.6540880503
UIST,4bc08650539c166673e5a2d6535cb896fa5c1224,UIST,2004,A gesture-based authentication scheme for untrusted public terminals,"Shwetak N. Patel, Jeffrey S. Pierce, Gregory D. Abowd","1701358, 1783827, 1732524","Powerful mobile devices with minimal I/O capabilities increase the likelihood that we will want to annex these devices to I/O resources we encounter in the local environment. This opportunistic annexing will require authentication. We present a sensor-based authentication mechanism for mobile devices that relies on physical possession instead of knowledge to setup the initial connection to a public terminal. Our solution provides a simple mechanism for shaking a device to authenticate with the public infrastructure, making few assumptions about the surrounding infrastructure while also maintaining a reasonable level of security.",69,73.6842105263
UIST,07aa37b27e4fea3a46307bc41ff212e127c199d7,UIST,2010,Using temporal video annotation as a navigational aid for video browsing,"Stefanie Müller, Gregor Miller, Sidney Fels","4942906, 1781538, 1749457","Video is a complex information space that requires advanced navigational aids for effective browsing. The increasing number of temporal video annotations offers new opportunities to provide video navigation according to a user's needs. We present a novel video browsing interface called TAV (Temporal Annotation Viewing) that provides the user with a visual overview of temporal video annotations. TAV enables the user to quickly determine the general content of a video, the location of scenes of interest and the type of annotations that are displayed while watching the video. An ongoing user study will evaluate our novel approach.",2,33.1395348837
UIST,1cfa915f1f86da52c92aff8d60cced0924b6a344,UIST,2000,Sensing techniques for mobile interaction,"Ken Hinckley, Jeffrey S. Pierce, Mike Sinclair, Eric Horvitz","1738072, 1783827, 1722648, 1688884","We describe sensing techniques motivated by unique aspects of human-computer interaction with handheld devices in mobile settings. Special features of mobile interaction include changing orientation and position, changing venues, the use of computing as auxiliary to ongoing, real-world activities like talking to a colleague, and the general intimacy of use for such devices. We introduce and integrate a set of sensors into a handheld device, and demonstrate several new functionalities engendered by the sensors, such as recording memos when the device is held like a cell phone, switching between portrait and landscape display modes by holding the device in the desired orientation, automatically powering up the device when the user picks it up the device to start using it, and scrolling the display using tilt. We present an informal experiment, initial usability testing results, and user reactions to these techniques.",317,100.0
UIST,d952c7ec5e0aa5725d0750e5c8f08f3d02b6c85a,UIST,2016,Haptic Learning of Semaphoric Finger Gestures,"Aakar Gupta, Antony Irudayaraj, Vimal Chandran, Goutham Palaniappan, Khai N. Truong, Ravin Balakrishnan","2020345, 3491611, 1827508, 3492203, 1752847, 1748870","Haptic learning of gesture shortcuts has never been explored. In this paper, we investigate haptic learning of a freehand semaphoric finger tap gesture shortcut set using haptic rings. We conduct a two-day study of 30 participants where we couple haptic stimuli with visual and audio stimuli, and compare their learning performance with wholly visual learning. The results indicate that with <30 minutes of learning, haptic learning of finger tap semaphoric gestures is comparable to visual learning and maintains its recall on the second day.",0,44.6540880503
UIST,0fdc89cc7462a55020198340c979f62cab2b22a9,UIST,2001,Toward more sensitive mobile phones,"Ken Hinckley, Eric Horvitz","1738072, 1688884","Although cell phones are extremely useful, they can be annoying and distracting to owners and others nearby. We describe sensing techniques intended to help make mobile phones more polite and less distracting. For example, our phone's ringing quiets as soon as the user responds to an incoming call, and the ring mutes if the user glances at the caller ID and decides not to answer. We also eliminate the need to press a TALK button to answer an incoming call by recognizing if the user picks up the phone and listens to it.",31,40.0
UIST,37602e8f07ab2c754c70b11e1bf819994f9fb740,UIST,2008,Taskposé: exploring fluid boundaries in an associative window visualization,"Michael S. Bernstein, Jeff Shrager, Terry Winograd","3047089, 3319902, 1699245","Window management research has aimed to leverage users' tasks to organize the growing number of open windows in a useful manner. This research has largely assumed task classifications to be binary -- either a window is in a task, or not -- and context-independent. We suggest that the continual evolution of tasks can invalidate this approach and instead propose a fuzzy association model in which windows are related to one another by varying degrees. Task groupings are an emergent property of our approach. To support the association model, we introduce the WindowRank algorithm and its use in determining window association. We then describe Taskpos&#233;, a prototype window switch visualization embodying these ideas, and report on a week-long user study of the system.",13,28.5714285714
UIST,6879653aa0df0f02a08fd70d0f6f2533b9822710,UIST,2011,onNote: playing printed music scores as a musical instrument,"Yusuke Yamamoto, Hideaki Uchiyama, Yasuaki Kakehi","2495258, 2025248, 1755682","This paper presents a novel musical performance system named onNote that directly utilizes printed music scores as a musical instrument. This system can make users believe that sound is indeed embedded on the music notes in the scores. The users can play music simply by placing, moving and touching the scores under a desk lamp equipped with a camera and a small projector. By varying the movement, the users can control the playing sound and the tempo of the music. To develop this system, we propose an image processing based framework for retrieving music from a music database by capturing printed music scores. From a captured image, we identify the scores by matching them with the reference music scores, and compute the position and pose of the scores with respect to the camera. By using this framework, we can develop novel types of musical interactions.",2,24.2857142857
UIST,a1daf66a3ac73a33c3ccb78f92e9c4bae78975f5,UIST,2007,QuME: a mechanism to support expertise finding in online help-seeking communities,"Jun Zhang, Mark S. Ackerman, Lada A. Adamic, Kevin Kyung Nam","2108372, 1797833, 1778398, 3041616","Help-seeking communities have been playing an increasingly critical role in the way people seek and share information. However, traditional help-seeking mechanisms of these online communities have some limitations. In this paper, we describe an expertise-finding mechanism that attempts to alleviate the limitations caused by not knowing users' expertise levels. As a result of using social network data from the online community, this mechanism can automatically infer expertise level. This allows, for example, a question list to be personalized to the user's expertise level as well as to keyword similarity. We believe this expertise location mechanism will facilitate the development of next generation help-seeking communities.",34,63.8888888889
UIST,0e5816708031a226b58e2338c12a9d82b4b71b32,UIST,2011,Maintaining shared mental models in anesthesia crisis care with nurse tablet input and large-screen displays,"Leslie Wu, Jesse Cirimele, Stuart Card, Scott R. Klemmer, Larry Chu, Kyle Harrison","2524633, 1818711, 4729853, 1728167, 5435839, 1865805","In an effort to reduce medical errors, doctors are beginning to embrace cognitive aids, such as paper-based checklists. We describe the early stage design process of an interactive cognitive aid for crisis care teams. This process included collaboration with anesthesia professors in the school of medicine and observation of medical students practicing in simulated scenarios. Based on these insights, we identify opportunities to employ large-screen displays and coordinated tablets to support team performance. We also propose a system design for interactive cognitive aids intended to encourage a shared mental model amongst crisis care staff.",4,34.2857142857
UIST,6ded9f16223ec0a89f745c500cc159e837cc9960,UIST,2012,Low-cost audience polling using computer vision,"Andrew Cross, Edward Cutrell, William Thies","4556348, 1722375, 1718457","Electronic response systems known as ""clickers"" have demonstrated educational benefits in well-resourced classrooms, but remain out-of-reach for most schools due to their prohibitive cost. We propose a new, low-cost technique that utilizes computer vision for real-time polling of a classroom. Our approach allows teachers to ask a multiple-choice question. Students respond by holding up a qCard: a sheet of paper that contains a printed code, similar to a QR code, encoding their student IDs. Students indicate their answers (A, B, C or D) by holding the card in one of four orientations. Using a laptop and an off-the-shelf webcam, our software automatically recognizes and aggregates the students' responses and displays them to the teacher. We built this system and performed initial trials in secondary schools in Bangalore, India. In a 25-student classroom, our system offers 99.8% recognition accuracy, captures 97% of responses within 10 seconds, and costs 15 times less than existing electronic solutions.",6,52.4509803922
UIST,4632128edc7a5d6bd461deadaaa21fded068b107,UIST,2001,Cursive: a novel interaction technique for controlling expressive avatar gesture,"Francesca Barrientos, John F. Canny","2337239, 1729041","We are developing an interaction technique for rich nonverbal communication through an avatar. By writing a single letter on a pen tablet device, a user can express their ideas or intentions, non-verbally, using their avatar body. Our system solves the difficult problem of controlling the movements of a highly articulated, 3D avatar model using a common input device within the context of an office environment. We believe that <i>writing</i> is a richly expressive and natural means for controlling expressive avatar gesture.",1,6.66666666667
UIST,6e67a8318f28e5f3cae69b63fb2f8078e8c02f57,UIST,2011,The 1line keyboard: a QWERTY layout in a single line,"Frank Chun Yat Li, Richard T. Guy, Koji Yatani, Khai N. Truong","3182550, 1962931, 3260704, 1752847","Current soft QWERTY keyboards often consume a large portion of the screen space on portable touchscreens. This space consumption can diminish the overall user experi-ence on these devices. In this paper, we present the 1Line keyboard, a soft QWERTY keyboard that is 140 pixels tall (in landscape mode) and 40% of the height of the native iPad QWERTY keyboard. Our keyboard condenses the three rows of keys in the normal QWERTY layout into a single line with eight keys. The sizing of the eight keys is based on users' mental layout of a QWERTY keyboard on an iPad. The system disambiguates the word the user types based on the sequence of keys pressed. The user can use flick gestures to perform backspace and enter, and tap on the bezel below the keyboard to input a space. Through an evaluation, we show that participants are able to quickly learn how to use the 1Line keyboard and type at a rate of over 30 WPM after just five 20-minute typing sessions. Using a keystroke level model, we predict the peak expert text entry rate with the 1Line keyboard to be 66--68 WPM.",39,83.8095238095
UIST,70bcefe4ba8def2d7e4e16dd08c06dc3a006b381,UIST,2016,Meta: Enabling Programming Languages to Learn from the Crowd,"Ethan Fast, Michael S. Bernstein","2660071, 3047089","Collectively authored programming resources such as Q&#38;A sites and open-source libraries provide a limited window into how programs are constructed, debugged, and run. To address these limitations, we introduce <i>Meta</i>: a language extension for Python that allows programmers to share functions and track how they are used by a crowd of other programmers. Meta functions are shareable via URL and instrumented to record runtime data. Combining thousands of Meta functions with their collective runtime data, we demonstrate tools including an optimizer that replaces your function with a more efficient version written by someone else, an auto-patcher that saves your program from crashing by finding equivalent functions in the community, and a proactive linter that warns you when a function fails elsewhere in the community. We find that professional programmers are able to use Meta for complex tasks (creating new Meta functions that, for example, cross-validate a logistic regression), and that Meta is able to find 44 optimizations (for a 1.45 times average speedup) and 5 bug fixes across the crowd.",1,92.7672955975
UIST,048c8b72d7a11d9bcd5d39926ee0876c8fba0480,UIST,2014,"Riding the plane: bimanual, desktop 3D manipulation","Jinbo Feng, Zachary Wartell","8242334, 2656827","A bimanual 7 Degree of Freedom (DOF) manipulation technique based on a hybrid 3D cursor driven by the combination of mouse and trackball is presented. This technique allows the user to move the cursor to the target location in 3D scene by following a conceived straight or curved path. In the pilot study, participants could learn the technique in a short time and perform the docking task steadily without physical fatigue.",0,12.015503876
UIST,345e8d5eb06460ea9ed0b93d7ee099d7989bec79,UIST,2009,The web page as a WYSIWYG end-user customizable database-backed information management application,"David R. Karger, Scott Ostler, Ryan Lee","1743286, 2203174, 5671354","Dido is an application (and application development environment) in a web page. It is a single web page containing rich structured data, an AJAXy interactive visualizer/editor for that data, and a ""metaeditor"" for WYSIWYG editing of the visualizer/editor. Historically, users have been limited to the data schemas, visualizations, and interactions offered by a small number of heavyweight applications. In contrast, Dido encourages and enables the end user to edit (not code) in his or her web browser a distinct ephemeral interaction ""wrapper"" for each data collection that is specifically suited to its intended use. Dido's <i>active document</i> metaphor has been explored before but we show how, given today's web infrastructure, it can be deployed in a small self-contained HTML document without touching a web client or server.",14,28.5714285714
UIST,201b39a371ebab414d72d62e567fe57ed975569e,UIST,2013,Crowd-scale interactive formal reasoning and analytics,"Ethan Fast, Colleen Lee, Alexander Aiken, Michael S. Bernstein, Daphne Koller, Eric Smith","2660071, 2273303, 1695674, 3047089, 1736370, 8302957","Large online courses often assign problems that are easy to grade because they have a fixed set of solutions (such as multiple choice), but grading and guiding students is more difficult in problem domains that have an unbounded number of correct answers. One such domain is derivations: sequences of logical steps commonly used in assignments for technical, mathematical and scientific subjects. We present DeduceIt, a system for creating, grading, and analyzing derivation assignments in any formal domain. DeduceIt supports assignments in any logical formalism, provides students with incremental feedback, and aggregates student paths through each proof to produce instructor analytics. DeduceIt benefits from checking thousands of derivations on the web: it introduces a proof cache, a novel data structure which leverages a crowd of students to decrease the cost of checking derivations and providing real-time, constructive feedback. We evaluate DeduceIt with 990 students in an online compilers course, finding students take advantage of its incremental feedback and instructors benefit from its structured insights into course topics. Our work suggests that automated reasoning can extend online assignments and large-scale education to many new domains.",13,74.3119266055
UIST,9fe3634237db30bc6ad050d4c8348fb95f6f0084,UIST,2011,Crowds in two seconds: enabling realtime crowd-powered interfaces,"Michael S. Bernstein, Joel Brandt, Rob Miller, David R. Karger","3047089, 1702922, 1723785, 1743286","Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting.",157,97.1428571429
UIST,1112be854d3c52f80395aa5b8847943305eb7cf0,UIST,2014,Video text retouch: retouching text in videos with direct manipulation,"Laurent Denoue, Scott Carter, Matthew Cooper","1788661, 1804229, 4268667","Video Text Retouch is a technique for retouching textual content found in many online videos such as screencasts, recorded presentations and many online e-learning videos. Viewed through our special, HTML5-based player, users can edit in real-time the textual content of the video frames, such as correcting typos or inserting new words between existing characters. Edits are overlaid and tracked at the desired position for as long as the original video content remains similar. We describe the interaction techniques, image processing algorithms and give implementation details of the system.",0,12.015503876
UIST,1480dab32d9676b8a3db7946eccd7c8fc172273d,UIST,2015,Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects,"Martin Schmitz, Mohammadreza Khalilbeigi, Matthias Balwierz, Roman Lissermann, Max Mühlhäuser, Jürgen Steimle","7419430, 3028523, 1889139, 2432512, 1725964, 1790324","3D printing is widely used to physically prototype the look and feel of 3D objects. Interaction possibilities of these prototypes, however, are often limited to mechanical parts or post-assembled electronics. In this paper, we present Capricate, a fabrication pipeline that enables users to easily design and 3D print highly customized objects that feature embedded capacitive multi-touch sensing. The object is printed in a single pass using a commodity multi-material 3D printer. To enable touch input on a wide variety of 3D printable surfaces, we contribute two techniques for designing and printing embedded sensors of custom shape. The fabrication pipeline is technically validated by a series of experiments and practically validated by a set of example applications. They demonstrate the wide applicability of Capricate for interactive objects.",8,90.350877193
UIST,dae2c1768c40e24ae331543877b48e9dfbb36471,UIST,2010,The enhancement of hearing using a combination of sound and skin sensation to the pinna,"Kanako Aou, Asuka Ishii, Masahiro Furukawa, Shogo Fukushima, Hiroyuki Kajimoto","3026782, 2166994, 1848375, 1844978, 1776927","Recent development in sound technologies has enabled the realistic replay of real-life sounds. Thanks to these technologies, we can experience a virtual real sound environment. However, there are other types of sound technologies that enhance reality, such as acoustic filters, sound effects, and background music. They are quite effective if carefully prepared, but they also alter the sound itself. Consequently, sound is simultaneously used to reconstruct realistic environments and to enhance emotions, which are actually incompatible functions.
 With this background, we focused on using tactile modality to enhance emotions and propose a method that enhances the sound experience by a combination of sound and skin sensation to the pinna (earlobe). In this paper, we evaluate the effectiveness of this method.",1,22.0930232558
UIST,795e7e18d38e6161ed304c10160b07fd32c2cb7a,UIST,2015,Makers' Marks: Physical Markup for Designing and Fabricating Functional Objects,"Valkyrie Savage, Sean Follmer, Jingyi Li, Björn Hartmann","2973941, 2770912, 2231629, 4020023","To fabricate functional objects, designers create assemblies combining existing parts (e.g., mechanical hinges, electronic components) with custom-designed geometry (e.g., enclosures). Modeling complex assemblies is outside the reach of the growing number of novice ``makers' with access to digital fabrication tools. We aim to allow makers to design and 3D print functional mechanical and electronic assemblies. Based on a formative exploration, we created Makers' Marks, a system based on physically authoring assemblies with sculpting materials and annotation stickers. Makers physically sculpt the shape of an object and attach stickers to place existing parts or high-level features (such as parting lines). Our tool extracts the 3D pose of these annotations from a scan of the design, then synthesizes the geometry needed to support integrating desired parts using a library of clearance and mounting constraints. The resulting designs can then be easily 3D printed and assembled. Our approach enables easy creation of complex objects such as TUIs, and leverages physical materials for tangible manipulation and understanding scale. We validate our tool through several design examples: a custom game controller, an animated toy figure, a friendly baby monitor, and a hinged box with integrated alarm.",7,87.2807017544
UIST,1bd0e1086694f0b8fc144b6ab5b3123fff565779,UIST,2010,Crowd-powered interfaces,Michael S. Bernstein,3047089,"We investigate <i>crowd-powered interfaces</i>: interfaces that embed human activity to support high-level conceptual activities such as writing, editing and question-answering. For example, a crowd-ppowered interface using paid crowd workers can compute a series of textual cuts and edits to a paragraph, then provide the user with an interface to condense his or her writing. We map out the design space of interfaces that depend on outsourced, friendsourced, and data mined resources, and report on designs for each of these. We discuss technical and motivational challenges inherent in human-powered interfaces.",3,42.4418604651
UIST,a01125ffd36af7bfbb2e49dcc1fd067be03e4857,UIST,2010,Eddi: interactive topic-based browsing of social status streams,"Michael S. Bernstein, Bongwon Suh, Lichan Hong, Jilin Chen, Sanjay Kairam, Ed Huai-hsin Chi","3047089, 3155504, 2217278, 5401495, 2572092, 1730922","Twitter streams are on overload: active users receive hundreds of items per day, and existing interfaces force us to march through a chronologically-ordered morass to find tweets of interest. We present an approach to organizing a user's own feed into coherently clustered trending topics for more directed exploration. Our Twitter client, called Eddi, groups tweets in a user's feed into topics mentioned explicitly or implicitly, which users can then browse for items of interest. To implement this topic clustering, we have developed a novel algorithm for discovering topics in short status updates powered by linguistic syntactic transformation and callouts to a search engine. An algorithm evaluation reveals that search engine callouts outperform other approaches when they employ simple syntactic transformation and backoff strategies. Active Twitter users evaluated Eddi and found it to be a more efficient and enjoyable way to browse an overwhelming status update feed than the standard chronological interface.",84,94.1860465116
UIST,28827b00da3fff36f036dcdbc457695e4f5b631a,UIST,2010,Animated paper: a moving prototyping platform,"Naoya Koizumi, Kentaro Yasu, Angela Liu, Maki Sugimoto, Masahiko Inami","2493346, 3257746, 4594557, 1792570, 1684930","We have developed a novel prototyping method that utilizes animated paper, a versatile platform created from paper and shape memory alloy (SMA), which is easy to control using a range of different energy sources from sunlight to lasers. We have further designed a laser point tracking system to improve the precision of the wireless control system by embedding retro-reflective material on the paper to act as light markers. It is possible to change the movement of paper prototypes by varying where to mount the SMA or how to heat it, creating a wide range of applications.",4,48.2558139535
UIST,7e57683791668b67d596537ef8dd649df91f3721,UIST,2012,DejaVu: integrated support for developing interactive camera-based programs,"Jun Kato, Sean McDirmid, Xiang Cao","3200463, 1701601, 7299595","The increasing popularity of interactive camera-based programs highlights the inadequacies of conventional IDEs in developing these programs given their distinctive attributes and workflows. We present DejaVu, an IDE enhancement that eases the development of these programs by enabling programmers to visually and continuously monitor program data in consistency with the frame-based pipeline of computer-vision programs; and to easily record, review, and reprocess temporal data to iteratively improve the processing of non-reproducible camera input. DejaVu was positively received by three experienced programmers of interactive camera-based programs in our preliminary user trial.",19,75.4901960784
UIST,7d046e73f7e70935c33af9e32ea72905ed959d2e,UIST,2007,SearchTogether: an interface for collaborative web search,"Meredith Ringel Morris, Eric Horvitz","1741255, 1688884","Studies of search habits reveal that people engage in many search tasks involving collaboration with others, such as travel planning, organizing social events, or working on a homework assignment. However, current Web search tools are designed for a single user, working alone. We introduce SearchTogether, a prototype that enables groups of remote users to synchronously or asynchronously collaborate when searching the Web. We describe an example usage scenario, and discuss the ways SearchTogether facilitates collaboration by supporting awareness, division of labor, and persistence. We then discuss the findings of our evaluation of SearchTogether, analyzing which aspects of its design enabled successful collaboration among study participants.",177,97.2222222222
UIST,020b51ae6076969cba671c5cb20fe54d7e877380,UIST,2016,Data-driven Mobile App Design,Biplab Deka,2653698,"Design is becoming a key differentiating factor for successful apps in today's crowded app marketplaces. This thesis describes how data-driven approaches can enable useful tools for mobile app design. It presents <i>interaction mining</i> -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. It develops two approaches for interaction mining existing Android apps and presents three applications enabled by the resultant data: a search engine for user flows, lightweight usability testing at scale, and automated generation of mobile UIs.",0,44.6540880503
UIST,0d892d3befcc0a584425131ab17794a96f0d0809,UIST,2016,Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting,"Joseph Chee Chang, Nathan Hahn, Aniket Kittur","3071893, 2577665, 1717650","Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use. However, saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use. Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user. In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster, we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context to support identifying and saving information in an intentionally uncertain way on mobile devices. In a two part user study we find that this approach reduced selection time and was preferred by participants over the default system text selection method.",0,44.6540880503
UIST,d7088b8102edc27106a0ad8cee9bc6c4845edfe9,UIST,2008,OctoPocus: a dynamic guide for learning gesture-based command sets,"Olivier Bau, Wendy E. Mackay","2931896, 1732917","We describe OctoPocus, an example of a dynamic guide that combines on-screen feedforward and feedback to help users learn, execute and remember gesture sets. OctoPocus can be applied to a wide range of single-stroke gestures and recognition algorithms and helps users progress smoothly from novice to expert performance. We provide an analysis of the design space and describe the results of two experi-ments that show that OctoPocus is significantly faster and improves learning of arbitrary gestures, compared to con-ventional Help menus. It can also be adapted to a mark-based gesture set, significantly improving input time compared to a two-level, four-item Hierarchical Marking menu.",92,91.4285714286
UIST,8b9b177f3542274a4caefff236d0ef37c8517219,UIST,2016,Multi-Device Storyboards for Cinematic Narratives in VR,"Rorik Henrikson, Bruno Rodrigues De Araújo, Fanny Chevalier, Karan Singh, Ravin Balakrishnan","3000941, 5367447, 2840251, 1682205, 1748870","Virtual Reality (VR) narratives have the unprecedented potential to connect with an audience through <i>presence</i>, placing viewers within the narrative. The onset of consumer VR has resulted in an explosion of interest in immersive storytelling. Planning narratives for VR, however, is a grand challenge due to its unique affordances, its evolving cinematic vocabulary, and most importantly the lack of supporting tools to explore the creative process in VR.
 In this paper, we distill key considerations with the planning process for VR stories, collected through a formative study conducted with film industry professionals. Based on these insights we propose a workflow, specific to the needs of professionals creating storyboards for VR film, and present a multi-device (tablet and head-mounted display) storyboard tool supporting this workflow. We discuss our design and report on feedback received from interviews following demonstration of our tool to VR film professionals.",0,44.6540880503
UIST,59931bff9421b4ef60384b696e4897911ac97fed,UIST,2014,WristFlex: low-power gesture input with wrist-worn pressure sensors,"Artem Dementyev, Joseph A. Paradiso","2103349, 4798651","In this paper we present WristFlex, an always-available on-body gestural interface. Using an array of force sensitive resistors (FSRs) worn around the wrist, the interface can distinguish subtle finger pinch gestures with high accuracy (&gt;80 %) and speed. The system is trained to classify gestures from subtle tendon movements on the wrist. We demonstrate that WristFlex is a complete system that works wirelessly in real-time. The system is simple and light-weight in terms of power consumption and computational overhead. WristFlex's sensor power consumption is 60.7 uW, allowing the prototype to potentially last more then a week on a small lithium polymer battery. Also, WristFlex is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. We perform user studies to evaluate the accuracy, speed, and repeatability. We demonstrate that the number of gestures can be extended with orientation data from an accelerometer. We conclude by showing example applications.",24,96.8992248062
UIST,82d1935345af8ccc3ce14e7dcc4e25eaa20fdb9c,UIST,2011,Using graphical representation of user interfaces as visual references,Tsung-Hsiang Chang,2337269,"Many user interfaces use indirect references to identify specific objects and devices. My thesis investigates using graphical representations of user interfaces (i.e. screenshots) as direct visual references to support various kinds of applications. Sikuli Script enables users to programmatically control GUIs without the support from the underlying applications. Sikuli Test lets GUI developers and testers create test scripts without coding. Deep Shot introduces a framework and interaction techniques to migrate work states across heterogeneous devices in one action, taking a picture. In addition to these pure pixel-based systems, PAX associates the pixel representation with the internal structures and metadata of the user interface. Based on these building blocks, we propose to develop a visual history system that enables users to search and browse what they have seen on their computer screens. We outline some interesting use cases and discuss the challenges in this ongoing work.",2,24.2857142857
UIST,99520e561932af79130e852d96e1111dc7ebc3d6,UIST,2016,HoloFlex: A Flexible Light-Field Smartphone with a Microlens Array and a P-OLED Touchscreen,"Daniel Gotsch, Xujing Zhang, Juan Pablo Carrascal, Roel Vertegaal","3397130, 2618185, 2883811, 1687608","We present HoloFlex, a 3D flexible smartphone featuring a light-field display consisting of a high-resolution P-OLED display and an array of 16,640 microlenses. HoloFlex allows mobile users to interact with 3D images featuring natural visual cues such as motion parallax and stereoscopy without glasses or head tracking. Its flexibility allows the use of bend input for interacting with 3D objects along the z axis. Images are rendered into 12-pixel wide circular blocks-pinhole views of the 3D scene-which enable ~80 unique viewports at an effective resolution of 160 &#215; 104. The microlens array distributes each pixel from the display in a direction that preserves the angular information of light rays in the 3D scene. We present a preliminary study evaluating the effect of bend input vs. a vertical touch screen slider on 3D docking performance. Results indicate that bend input significantly improves movement time in this task. We also present 3D applications including a 3D editor, a 3D <i>Angry Birds</i> game and a 3D teleconferencing system that utilize bend input.",0,44.6540880503
UIST,aabfed0fc87b3db4770faaa7f35de24b1b9e060b,UIST,2009,Activity analysis enabling real-time video communication on mobile phones for deaf users,"Neva Cherniavsky, Jaehong Chon, Jacob O. Wobbrock, Richard E. Ladner, Eve A. Riskin","1877079, 2908072, 1796045, 1762656, 1788775","We describe our system called MobileASL for real-time video communication on the current U.S. mobile phone network. The goal of MobileASL is to enable Deaf people to communicate with Sign Language over mobile phones by compressing and transmitting sign language video in real-time on an off-the-shelf mobile phone, which has a weak processor, uses limited bandwidth, and has little battery capacity. We develop several H.264-compliant algorithms to save system resources while maintaining ASL intelligibility by focusing on the important segments of the video. We employ a dynamic skin-based region-of-interest (ROI) that encodes the skin at higher quality at the expense of the rest of the video. We also automatically recognize periods of signing versus not signing and raise and lower the frame rate accordingly, a technique we call variable frame rate (VFR).
 We show that our variable frame rate technique results in a 47% gain in battery life on the phone, corresponding to an extra 68 minutes of talk time. We also evaluate our system in a user study. Participants fluent in ASL engage in unconstrained conversations over mobile phones in a laboratory setting. We find that the ROI increases intelligibility and decreases guessing. VFR increases the need for signs to be repeated and the number of conversational breakdowns, but does not affect the users' perception of adopting the technology. These results show that our sign language sensitive algorithms can save considerable resources without sacrificing intelligibility.",8,17.1428571429
UIST,fe930379455d80922d1db2fa8dcd5bdc928a2448,UIST,2016,Energy-Brushes: Interactive Tools for Illustrating Stylized Elemental Dynamics,"Jun Xing, Rubaiat Habib Kazi, Tovi Grossman, Li-Yi Wei, Jos Stam, George W. Fitzmaurice","8262577, 2820401, 3313809, 2420851, 1720764, 1703735","Dynamic effects such as waves, splashes, fire, smoke, and explosions are an integral part of stylized animations. However, such dynamics are challenging to produce, as manually sketching key-frames requires significant effort and artistic expertise while physical simulation tools lack sufficient expressiveness and user control. We present an interactive interface for designing these <i>elemental dynamics</i> for animated illustrations. Users draw with coarse-scale <i>energy brushes</i> which serve as control gestures to drive detailed <i>flow particles</i> which represent local velocity fields. These fields can convey both realistic and artistic effects based on user specification. This painting metaphor for creating elemental dynamics simplifies the process, providing artistic control, and preserves the fluidity of sketching. Our system is fast, stable, and intuitive. An initial user evaluation shows that even novice users with no prior animation experience can create intriguing dynamics using our system.",0,44.6540880503
UIST,a34ab1be842001afef6f2d022a38c20f4a0763c5,UIST,2013,DigiTaps: eyes-free number entry on touchscreens with minimal audio feedback,"Shiri Azenkot, Cynthia L. Bennett, Richard E. Ladner","3283573, 2803724, 1762656","Eyes-free input usually relies on audio feedback that can be difficult to hear in noisy environments. We present DigiTaps, an eyes-free number entry method for touchscreen devices that requires little auditory attention. To enter a digit, users tap or swipe anywhere on the screen with one, two, or three fingers. The 10 digits are encoded by combinations of these gestures that relate to the digits' semantics. For example, the digit 2 is input with a 2-finger tap. We conducted a longitudinal evaluation with 16 people and found that DigiTaps with no audio feedback was faster but less accurate than with audio feedback after every input. Throughout the study, participants entered numbers with no audio feedback at an average rate of 0.87 characters per second, with an uncorrected error rate of 5.63%.",5,55.9633027523
UIST,452e8a706d0efd4737912ae64401adff42ab6216,UIST,2015,BitDrones: Towards Levitating Programmable Matter Using Interactive 3D Quadcopter Displays,"Calvin Rubens, Sean Braley, Antonio Gomes, Daniel Goc, Xujing Zhang, Juan Pablo Carrascal, Roel Vertegaal","3169420, 2090804, 7318622, 1741277, 2618185, 2883811, 1687608","In this paper, we present BitDrones, a platform for the construction of interactive 3D displays that utilize nano quadcopters as self-levitating tangible building blocks. Our prototype is a first step towards supporting interactive mid-air, tangible experiences with physical interaction techniques through multiple building blocks capable of physically representing interactive 3D data.",1,42.1052631579
UIST,03fb8e58efe1e27948740edf7b98adcef37c7560,UIST,2011,Elasticurves: exploiting stroke dynamics and inertia for the real-time neatening of sketched 2D curves,"Yannick Thiel, Karan Singh, Ravin Balakrishnan","2503196, 1682205, 1748870","Elasticurves present a novel approach to neaten sketches in real-time, resulting in curves that combine smoothness with user-intended detail. Inspired by natural variations in stroke speed when drawing quickly or with precision, we exploit stroke dynamics to distinguish intentional fine detail from stroke noise. Combining inertia and stroke dynamics, elasticurves can be imagined as the trace of a pen attached to the user by an oscillation-free elastic band. Sketched quickly, the elasticurve spatially lags behind the stroke, smoothing over stroke detail, but catches up and matches the input stroke at slower speeds. Connectors, such as lines or circular-arcs link the evolving elasticurve to the next input point, growing the curve by a responsiveness fraction along the connector. Responsiveness is calibrated, to reflect drawing skill or device noise. Elasticurves are theoretically sound and robust to variations in stroke sampling. Practically, they neaten digital strokes in real-time while retaining the modeless and visceral feel of pen on paper.",6,38.5714285714
UIST,86e624ef8b09d436d7099f3d6e2389f893bf5a7c,UIST,2012,Adding structured data in unstructured web chat conversation,"Min Wu, Arin Bhowmick, Joseph Goldberg","1751973, 2175557, 3589827","Web chat is becoming the primary customer contact channel in customer relationship management (CRM), and Question/Answer/Lookup (QAL) is the dominant communication pattern in CRM agent-to-customer chat. Text-based web chat for QAL has two main usability problems. Chat transcripts between agents and customers are not tightly integrated into agent-side applications, requiring customer service agents to re-enter customer typed data. Also, sensitive information posted in chat sessions in plain text raises security concerns. The addition of web form widgets to web chat not only solves both of these problems but also adds new usability benefits to QAL. Forms can be defined beforehand or, more flexibly, dynamically composed. Two preliminary user studies were conducted. An agent-side study showed that adding inline forms to web chat decreased overall QAL completion time by 47 percent and increased QAL accuracy by removing all potential human errors. A customer-side study showed that web chat with inline forms is intuitive to customers.",1,25.0
UIST,0c5af957b988a8ee89b1ae8bf010da7b9e99fcb7,UIST,2007,The re: search engine: simultaneous support for finding and re-finding,Jaime Teevan,1691357,"Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query ""breast cancer treatments"", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten.",26,47.2222222222
UIST,4f5c168c46074ee8cc5d8e2fca81ab01ca3f4d9e,UIST,2005,Physical embodiments for mobile communication agents,"Stefan Marti, Chris Schmandt","2807271, 1729321","This paper describes a physically embodied and animated user interface to an interactive call handling agent, consisting of a small wireless animatronic device in the form of a squirrel, bunny, or parrot. A software tool creates movement primitives, composes these primitives into complex behaviors, and triggers these behaviors dynamically at state changes in the conversational agent's finite state machine. Gaze and gestural cues from the animatronics alert both the user and co-located third parties of incoming phone calls, and data suggests that such alerting is less intrusive than conventional telephones.",28,48.3870967742
UIST,a9bfc8a76d831fbcc711647ecbe359d4ce0dd320,UIST,2015,Codo: Fundraising with Conditional Donations,"Juan Felipe Beltran, Aysha Siddique, Azza Abouzeid, Jay Chen","2210179, 1985913, 2827658, 1683194","Crowdfunding websites like Kickstarter and Indiegogo offer project organizers the ability to market, fund, and build a community around their campaign. While offering support and flexibility for organizers, crowdfunding sites provide very little control to donors. In this paper, we investigate the idea of empowering donors by allowing them to specify conditions for their crowdfunding contributions. We introduce a crowdfunding system, Codo, that allows donors to specify conditional donations. Codo allow donors to contribute to a campaign but hold off on their contribution until certain specific conditions are met (e.g. specific members or groups contribute a certain amount). We begin with a micro study to assess several specific conditional donations based on their comprehensibility and usage likelihood. Based on this study, we formalize conditional donations into a general grammar that captures a broad set of useful conditions. We demonstrate the feasibility of resolving conditions in our grammar by elegantly transforming conditional donations into a system of linear inequalities that are efficiently resolved using off-the-shelf linear program solvers. Finally, we designed a user-friendly crowdfunding interface that supports conditional donations for an actual fund raising campaign and assess the potential of conditional donations through this campaign. We find preliminary evidence that roughly 1 in 3 donors make conditional donations and that conditional donors donate more compared to direct donors.",0,16.2280701754
UIST,bb05196035d8d2e0d5dabb6ef1e8dfdff7031845,UIST,2003,A widget framework for augmented interaction in SCAPE,"Leonard D. Brown, Hong Hua, Chunyu Gao","2008384, 1804619, 2398760","We have previously developed a collaborative infrastructure called SCAPE - an acronym for Stereoscopic Collaboration in Augmented and Projective Environments - that integrates the traditionally separate paradigms of virtual and augmented reality. In this paper, we extend SCAPE by formalizing its underlying mathematical framework and detailing three augmented Widgets constructed via this framework: CoCylinder, Magnifier, and CoCube. These devices promote intuitive ways of selecting, examining, and sharing synthetic objects, and retrieving associated documentary text. Finally we present a testbed application to showcase SCAPE's capabilities for interaction in large, augmented virtual environments.",89,70.8333333333
UIST,1bc4cbf6363560de444168b1802417b8579c8f4f,UIST,2015,SensorTape: Modular and Programmable 3D-Aware Dense Sensor Network on a Tape,"Artem Dementyev, Hsin-Liu Cindy Kao, Joseph A. Paradiso","2103349, 2649998, 4798651","SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics substrate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. Also, we made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low deployment effort required by the user. We showed how SensorTape could be produced at scale using current technologies and we made a 2.3-meter long prototype.",4,75.4385964912
UIST,04bad8cfdde678573c4c96d65cb298dda1159dcf,UIST,2015,Improving Automated Email Tagging with Implicit Feedback,"Mohammad S. Sorower, Michael Slater, Thomas G. Dietterich","2261204, 7281359, 1699720","Tagging email is an important tactic for managing information overload. Machine learning methods can help the user with this task by predicting tags for incoming email messages. The natural user interface displays the predicted tags on the email message, and the user doesn't need to do anything unless those predictions are wrong (in which case, the user can delete the incorrect tags and add the missing tags). From a machine learning perspective, this means that the learning algorithm never receives confirmation that its predictions are correct---it only receives feedback when it makes a mistake. This can lead to slower learning, particularly when the predictions were not very confident, and hence, the learning algorithm would benefit from positive feedback. One could assume that if the user never changes any tag, then the predictions are correct, but users sometimes forget to correct the tags, presumably because they are focused on the content of the email messages and fail to notice incorrect and missing tags. The aim of this paper is to determine whether implicit feedback can provide useful additional training examples to the email prediction subsystem of TaskTracer, known as EP2 (Email Predictor 2). Our hypothesis is that the more time a user spends working on an email message, the more likely it is that the user will notice tag errors and correct them. If no corrections are made, then perhaps it is safe for the learning system to treat the predicted tags as being correct and train accordingly. This paper proposes three algorithms (and two baselines) for incorporating implicit feedback into the EP2 tag predictor. These algorithms are then evaluated using email interaction and tag correction events collected from 14 user-study participants as they performed email-directed tasks while using TaskTracer EP2. The results show that implicit feedback produces important increases in training feedback, and hence, significant reductions in subsequent prediction errors despite the fact that the implicit feedback is not perfect. We conclude that implicit feedback mechanisms can provide a useful performance boost for email tagging systems.",0,16.2280701754
UIST,5989283d52ab678ef45eff5e83d518ecd7f43ec1,UIST,2016,FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality,"Jan Gugenheimer, David Dobbelstein, Christian Winkler, Gabriel Haas, Enrico Rukzio","3186723, 2638354, 1700240, 7545521, 2021950","We present <i>FaceTouch</i>, a novel interaction concept for mobile Virtual Reality (VR) head-mounted displays (HMDs) that leverages the backside as a touch-sensitive surface. With <i>FaceTouch</i>, the user can point at and select virtual content inside their field-of-view by touching the corresponding location at the backside of the HMD utilizing their sense of proprioception. This allows for rich interaction (e.g. gestures) in mobile and nomadic scenarios without having to carry additional accessories (e.g. a gamepad). We built a prototype of <i>FaceTouch</i> and conducted two user studies. In the first study we measured the precision of <i>FaceTouch</i> in <i>a display-fixed</i> target selection task using three different selection techniques showing a low error rate of 2% indicate the viability for everyday usage. To asses the impact of different mounting positions on the user performance we conducted a second study. We compared three mounting positions of the touchpad (<i>face, hand</i> and <i>side</i>) showing that mounting the touchpad at the back of the HMD resulted in a significantly lower error rate, lower selection time and higher usability. Finally, we present interaction techniques and three example applications that explore the <i>FaceTouch</i> design space.",2,98.427672956
UIST,8ab05cfda057351f969ba84be0746cbe197aeadc,UIST,2016,GyroVR: Simulating Inertia in Virtual Reality using Head Worn Flywheels,"Jan Gugenheimer, Dennis Wolf, Eythor R. Eiriksson, Pattie Maes, Enrico Rukzio","3186723, 2072716, 3491586, 1701876, 2021950","We present GyroVR, head worn flywheels designed to render inertia in Virtual Reality (VR. Motions such as flying, diving or floating in outer space generate kinesthetic forces onto our body which impede movement and are currently not represented in VR. We simulate those kinesthetic forces by attaching flywheels to the users head, leveraging the gyroscopic effect of resistance when changing the spinning axis of rotation. GyroVR is an ungrounded, wireless and self contained device allowing the user to freely move inside the virtual environment. The generic shape allows to attach it to different positions on the users body. We evaluated the impact of GyroVR onto different mounting positions on the head (back and front) in terms of immersion, enjoyment and simulator sickness. Our results show, that attaching GyroVR onto the users head (front of the Head Mounted Display (HMD)) resulted in the highest level of immersion and enjoyment and therefore can be built into future VR HMDs, enabling kinesthetic forces in VR.",2,98.427672956
UIST,080a89a81b42957462234c7b217a493fe61fc528,UIST,2013,The skweezee system: enabling the design and the programming of squeeze interactions,"Karen Vanderloock, Vero Vanden Abeele, Johan A. K. Suykens, Luc Geurts","2205311, 8474018, 1744439, 3050475","The Skweezee System is an easy, flexible and open system for designing and developing squeeze-based, gestural interactions. It consists of Skweezees, which are soft objects, filled with conductive padding, that can be deformed or squeezed by applying pressure. These objects contain a number of electrodes that are dispersed over the shape. The electrodes sense the shape shifting of the conductive filling by measuring the changing resistance between every possible pair of electrodes. In addition, the Skweezee System contains user-friendly software that allows end-users to define and to record their own squeeze gestures. These gestures are distinguished using a Support Vector Machine (SVM) classifier. In this paper we introduce the concept and the underlying technology of the Skweezee System and we demonstrate the robustness of the SVM based classifier via two experimental user studies. The results of these studies demonstrate accuracies of 81% (8 gestures, user-defined) to 97% (3 gestures, user-defined), with an accuracy of 90% for 7 pre-defined gestures.",13,74.3119266055
UIST,0d1dc855e3b84ec9ffafd3e7ad51244c88b7de0c,UIST,2015,HapticPrint: Designing Feel Aesthetics for Digital Fabrication,"César Torres, Tim Campbell, Neil Kumar, Eric Paulos","4769628, 2083320, 7795824, 2749792","Digital fabrication has enabled massive creativity in hobbyist communities and professional product design. These emerging technologies excel at realizing an arbitrary shape or form; however these objects are often rigid and lack the feel desired by designers. We aim to enable physical haptic design in passive 3D printed objects. This paper identifies two core areas for extending physical design into digital fabrication: designing the external and internal haptic characteristics of an object. We present HapticPrint as a pair of design tools to easily modify the feel of a 3D model. Our external tool maps textures and UI elements onto arbitrary shapes, and our internal tool modifies the internal geometry of models for novel compliance and weight characteristics. We demonstrate the value of HapticPrint with a range of applications that expand the aesthetics of feel, usability, and interactivity in 3D artifacts.",2,60.5263157895
UIST,45cd9c9214f15fc7802dbdccb6b4a8f84f19dc41,UIST,2016,"Beyond Snapping: Persistent, Tweakable Alignment and Distribution with StickyLines","Marianela Ciolfi Felice, Nolwenn Maudet, Wendy E. Mackay, Michel Beaudouin-Lafon","3349060, 2045942, 1732917, 1682346","Aligning and distributing graphical objects is a common, but cumbersome task. In a preliminary study (six graphic designers, six non-designers), we identified three key problems with current tools: lack of persistence, unpredictability of results, and inability to 'tweak' the layout. We created <i>StickyLines</i>, a tool that treats guidelines as first-class objects: Users can create precise, predictable and persistent interactive alignment and distribution relationships, and 'tweaked' positions can be maintained for subsequent interactions. We ran a [2x2] within-participant experiment to compare <i>StickyLines</i> with standard commands, with two levels of layout difficulty. <i>StickyLines</i> performed 40% faster and required 49% fewer actions than traditional alignment and distribution commands for complex layouts. In study three, six professional designers quickly adopted <i>StickyLines</i> and identified novel uses, including creating complex compound guidelines and using them for both spatial and semantic grouping.",0,44.6540880503
UIST,f7a984766cbc3eca22f16149a84182fc73bac2be,UIST,1991,XJp system: an internationalized language interface for the X Window system,"Masato Morisaki, Etsuo Kawada, Hiroshi Kuribayashi, Seiji Kuwari, Masahiko Narita","2384790, 2987785, 8628331, 3152315, 1906401","This paper discusses the internationalization of the X Window System developed by the MIT X consortium. The main purpose is to enable X Window System Release 4 (X11R4) and earlier versions to support Asian languages, primarily Japanese, Chinese, and Korean. Unlike English and other European-based languages, Asian languages involve idiographic character manipulation. X Window System XIlR4 and earlier versions can output such ideograms when the corresponding fonts are provided, but they have no corresponding input feature. Asian language input thus involves more than one keystroke to input a single ideogram, e.g., Japanese-language input uses romaji-kana-kanji conversion. This paper proposes an ideogram input architecture on the X Window System and discusses the interfaces between conversion systems Window oriented and X application programs. Like the X System, our input-conversion feature is to a distributed network environment. Permission to copy without fee all or part of this matarial is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.",1,6.52173913043
UIST,3f18eee626e2ed5fe4ba52b3f2ad1fff35ed48d7,UIST,2016,Expressive Keyboards: Enriching Gesture-Typing on Mobile Devices,"Jessalyn Alvina, Joseph Malloch, Wendy E. Mackay","2721543, 2460042, 1732917","Gesture-typing is an efficient, easy-to-learn, and errortolerant technique for entering text on software keyboards. Our goal is to ""recycle"" users' otherwise-unused gesture variation to create rich output under the users' control, without sacrificing accuracy. Experiment 1 reveals a high level of existing gesture variation, even for accurate text, and shows that users can consciously vary their gestures under different conditions. We designed an <i>Expressive Keyboard</i> for a smart phone which maps input gesture features identified in Experiment 1 to a continuous output parameter space, i.e. RGB color. Experiment 2 shows that users can consciously modify their gestures, while retaining accuracy, to generate specific colors as they gesture-type. Users are more successful when they focus on output characteristics (such as red) rather than input characteristics (such as curviness). We designed an app with a dynamic font engine that continuously interpolates between several typefaces, as well as controlling weight and random variation. Experiment 3 shows that, in the context of a more ecologically-valid conversation task, users enjoy generating multiple forms of rich output. We conclude with suggestions for how the <i>Expressive Keyboard</i> approach can enhance a wide variety of gesture recognition applications.",0,44.6540880503
UIST,a86ac5c2a5c1b1774a2aa0a0b37f0b312a63642e,UIST,2013,Cobi: a community-informed conference scheduling tool,"Juho Kim, Haoqi Zhang, Paul André, Lydia B. Chilton, Wendy E. Mackay, Michel Beaudouin-Lafon, Rob Miller, Steven Dow","1800981, 3162562, 2411694, 3314354, 1732917, 1682346, 1723785, 5319364","Effectively planning a large multi-track conference requires an understanding of the preferences and constraints of organizers, authors, and attendees. Traditionally, the onus of scheduling the program falls on a few dedicated organizers. Resolving conflicts becomes difficult due to the size and complexity of the schedule and the lack of insight into community members' needs and desires. Cobi presents an alternative approach to conference scheduling that engages the entire community in the planning process. Cobi comprises (a) communitysourcing applications that collect preferences, constraints, and affinity data from community members, and (b) a visual scheduling interface that combines communitysourced data and constraint-solving to enable organizers to make informed improvements to the schedule. This paper describes Cobi's scheduling tool and reports on a live deployment for planning CHI 2013, where organizers considered input from 645 authors and resolved 168 scheduling conflicts. Results show the value of integrating community input with an intelligent user interface to solve complex planning tasks.",9,65.1376146789
UIST,2d2f8863cb2d0088cd20e942afd5e23f037e96f7,UIST,2011,Cracking the cocoa nut: user interface programming at runtime,"James R. Eagan, Michel Beaudouin-Lafon, Wendy E. Mackay","2152856, 1682346, 1732917","This article introduces <i>runtime toolkit overloading</i>, a novel approach to help third-party developers modify the interaction and behavior of existing software applications without access to their underlying source code. We describe the abstractions provided by this approach as well as the mechanisms for implementing them in existing environments. We describe Scotty, a prototype implementation for Mac OS X Cocoa that enables developers to modify existing applications at runtime, and we demonstrate a collection of interaction and functional transformations on existing off-the-shelf applications. We show how Scotty helps a developer make sense of unfamiliar software, even without access to its source code. We further discuss what features of future environments would facilitate this kind of runtime software development.",16,62.380952381
UIST,7506c0256ecbec0587d0a2e9d8b81d2daa7b083a,UIST,2004,"An explanation-based, visual debugger for one-way constraints","Bradley T. Vander Zanden, David K. Baker, Jing Jin","2255161, 2756390, 1691569","This paper describes a domain-specific debugger for one-way constraint solvers. The debugger makes use of several new techniques. First, the debugger displays only a portion of the dataflow graph, called a &#60;i>constraint slice&#60;/i>, that is directly related to an incorrect variable. This technique helps the debugger scale to a system containing thousands of constraints. Second, the debugger presents a visual representation of the solver's data structures and uses color encodings to highlight changes to the data structures. Finally, the debugger allows the user to point to a variable that has an unexpected value and ask the debugger to suggest reasons for the unexpected value. The debugger makes use of information gathered during the constraint satisfaction process to generate plausible suggestions. Informal testing has shown that the explanatory capability and the color coding of the constraint solver's data structures are particularly useful in locating bugs in constraint code.",3,10.5263157895
UIST,518759fd7dc5f653102f8db9887f639921aa94eb,UIST,2006,Mnemonic rendering: an image-based approach for exposing hidden changes in dynamic displays,"Anastasia Bezerianos, Pierre Dragicevic, Ravin Balakrishnan","1717129, 3297322, 1748870","Managing large amounts of dynamic visual information involves understanding changes happening out of the user's sight. In this paper, we show how current software does not adequately support users in this task, and motivate the need for a more general approach. We propose an image-based storage, visualization, and implicit interaction paradigm called mnemonic rendering that provides better support for handling visual changes. Once implemented on a system, mnemonic rendering techniques can benefit all applications. We explore its rich design space and discuss its expected benefits as well as limitations based on feedback from users of a small-screen and a wall-size prototype.",37,57.5
UIST,3e7d72a1ad522cd0752b7d42b726fdc5492187e0,UIST,2011,Execution control for crowdsourcing,"Daniel S. Weld, Mausam, Peng Dai","1780531, 2674444, 2666496","Crowdsourcing marketplaces enable a wide range of applications, but constructing any new application is challenging - usually requiring a complex, self-managing workflow in order to guarantee quality results. We report on the CLOWDER project, which uses machine learning to continually refine models of worker performance and task difficulty. We present decision-theoretic optimization techniques that can select the best parameters for a range of workflows. Initial experiments show our optimized workflows are significantly more economical than with manually set parameters.",0,6.19047619048
UIST,c42a631e8b4d2931a9f6db627c36972a2c065b35,UIST,2007,Assieme: finding and leveraging implicit references in a web search interface for programmers,"Raphael Hoffmann, James Fogarty, Daniel S. Weld","3100993, 1738171, 1780531","Programmers regularly use search as part of the development process, attempting to identify an appropriate API for a problem, seeking more information about an API, and seeking samples that show how to use an API. However, neither general-purpose search engines nor existing code search engines currently fit their needs, in large part because the information programmers need is distributed across many pages. We present Assieme, a Web search interface that effectively supports common programming search tasks by combining information from Web-accessible Java Archive (JAR) files, API documentation, and pages that include explanatory text and sample code. Assieme uses a novel approach to finding and resolving implicit references to Java packages, types, and members within sample code on the Web. In a study of programmers performing searches related to common programming tasks, we show that programmers obtain better solutions, using fewer queries, in the same amount of time spent using a general Web search interface.",70,83.3333333333
UIST,4bf957625ba35492f91abc5d067048264b044987,UIST,2003,"A fast, interactive 3D paper-flier metaphor for digital bulletin boards","Laurent Denoue, Les Nelson, Elizabeth F. Churchill","1788661, 2004203, 1801572","We describe a novel interface for presenting interactive content on public digital bulletin boards. Inspired by paper fliers on physical bulletin boards, posted content is displayed using 3D virtual fliers attached to a virtual corkboard by virtual pushpins. Fliers appear in different orientations, creating an attractive, informal look, and have autonomous behaviors like fluttering in the wind. Passers-by can rotate, move and fold fliers; they can also interact with fliers' live content. Flier content is streamed from a server and represented by the system on large screen displays using a real-time cloth simulation algorithm. We describe our prototype, and offer the results of an initial evaluative user study.",9,6.25
UIST,7536e6d1afe412ecd25e122c777862e96834410c,UIST,2015,Improving Haptic Feedback on Wearable Devices through Accelerometer Measurements,"Jeffrey R. Blum, Ilja Frissen, Jeremy R. Cooperstock","2526202, 7580031, 2242019","Many variables have been shown to impact whether a vibration stimulus will be perceived. We present a user study that takes into account not only previously investigated predictors such as vibration intensity and duration along with the age of the person receiving the stimulus, but also the amount of motion, as measured by an accelerometer, at the site of vibration immediately preceding the stimulus. This is a more specific measure than in previous studies showing an effect on perception due to gross conditions such as walking. We show that a logistic regression model including prior acceleration is significantly better at predicting vibration perception than a model including only vibration intensity, duration and participant age. In addition to the overall regression, we discuss individual participant differences and measures of classification performance for real-world applications. Our expectation is that haptic interface designers will be able to use such results to design better vibrations that are perceivable under the user's current activity conditions, without being annoyingly loud or jarring, eventually approaching ``perceptually equivalent' feedback independent of motion.",0,16.2280701754
UIST,50fb5e2f0c2fe8679c218ff88d4906e5a0812d34,UIST,2012,"Sketch-editing games: human-machine communication, game theory and applications","Andre Ribeiro, Takeo Igarashi","2348933, 1717356","We study uncertainty in graphical-based interaction (with special attention to sketches). We argue that a comprehensive model for the problem must include the interaction participants (and their current beliefs), their possible actions and their past sketches. It's yet unclear how to frame and solve the former problem, considering all the latter elements. We suggest framing the problem as a game and solving it with a game-theoretical solution, which leads to a framework for the design of new two-way, sketch-based user interfaces. In special, we use the framework to design a game that can progressively learn visual models of objects from user sketches, and use the models in real-world interactions. Instead of an abstract visual criterion, players in this game learn models to optimize interaction (the game's duration). This two-way sketching game addresses problems essential in emerging interfaces (such as learning and how to deal with interpretation errors). We review possible applications in robotic sketch-to-command, hand gesture recognition, media authoring and visual search, and evaluate two. Evaluations demonstrate how players improve performance with repeated play, and the influence of interaction aspects on learning.",0,9.80392156863
UIST,51930218d82ed53dd7f1c485a6d6c2dc428512c6,UIST,2015,Adding Body Motion and Intonation to Instant Messaging with Animation,"Weston Gaylord, Vivian Hare, Ashley Ngu","2250629, 2541206, 3062575","Digital text communication (DTC) has transformed the way people communicate. Static typographical cues like emoticons, punctuation, letter case, and word lengthening (ie. Hellooo?) are regularly employed to convey intonation and affect. However, DTC platforms like instant messaging still suffer from a lack of nonverbal communication cues. This paper introduces an Animated Text Instant Messenger (ATIM), which uses text animations to add another distinct layer of cues to existing plaintext. ATIM builds upon previous research by using kinetic typography in communication. This paper describes the design principles and features of ATIM and discusses how animated text can add more nuanced communication cues of intonation and body motion.",1,42.1052631579
UIST,1c0312b4260cbe22f889123cef122cb0e8f06bc0,UIST,2006,A direct texture placement and editing interface,"Yotam I. Gingold, Philip L. Davidson, Jefferson Y. Han, Denis Zorin","1697606, 2362379, 2183554, 1798055","The creation of most models used in computer animation and computer games requires the assignment of texture coordinates, texture painting, and texture editing. We present a novel approach for texture placement and editing based on direct manipulation of textures on the surface. Compared to conventional tools for surface texturing, our system combines <i>UV</i>-coordinate specification and texture editing into one seamless process, reducing the need for careful initial design of parameterization and providing a natural interface for working with textures directly on 3D surfaces.A combination of efficient techniques for interactive constrained parameterization and advanced input devices makes it possible to realize a set of natural interaction paradigms. The texture is regarded as a piece of stretchable material, which the user can position and deform on the surface, selecting arbitrary sets of constraints and mapping texture points to the surface; in addition, the multi-touch input makes it possible to specify natural handles for texture manipulation using point constraints associated with different fingers. Pressure can be used as a direct interface for texture combination operations. The 3D position of the object and its texture can be manipulated simultaneously using two-hand input.",17,22.5
UIST,eeed120fb2d2aa73f1fc972f0862c7f82476bb23,UIST,2011,NaviRadar: a novel tactile information display for pedestrian navigation,"Sonja Rümelin, Enrico Rukzio, Robert Hardy","3159098, 2021950, 2400058","We introduce NaviRadar: an interaction technique for mobile phones that uses a radar metaphor in order to communicate the user's correct direction for crossings along a desired route. A radar sweep rotates clockwise and tactile feedback is provided where each sweep distinctly conveys the user's current direction and the direction in which the user must travel. In a first study, we evaluated the overall concept and tested five different tactile patterns to communicate the two different directions via a single tactor. The results show that people are able to easily understand the NaviRadar concept and can identify the correct direction with a mean deviation of 37&#176; out of the full 360&#176; provided. A second study shows that NaviRadar achieves similar results in terms of perceived usability and navigation performance when compared with spoken instructions. By using only tactile feedback, NaviRadar provides distinct advantages over current systems. In particular, no visual attention is required to navigate; thus, it can be spent on providing greater awareness of one's surroundings. Moreover, the lack of audio attention enables it to be used in noisy environments or this attention can be better spent on talking with others during navigation.",17,63.8095238095
UIST,b1f7185c790ec090b82286cc557a009857c85532,UIST,2007,Graphstract: minimal graphical help for computers,"Jeff Huang, Michael Twidale","1787345, 1772877","We explore the use of abstracted screenshots as part of a new help interface. Graphstract, an implementation of a graphical help system, extends the ideas of textually oriented Minimal Manuals to the use of screenshots, allowing multiple small graphical elements to be shown in a limited space. This allows a user to get an overview of a complex sequential task as a whole. The ideas have been developed by three iterations of prototyping and evaluation. A user study shows that Graphstract helps users perform tasks faster on some but not all tasks. Due to their graphical nature, it is possible to construct Graphstracts automatically from pre-recorded interactions. A second study shows that automated capture and replay is a low-cost method for authoring Graphstracts, and the resultant help is as understandable as manually constructed help.",12,18.0555555556
UIST,3514465369b211b09b3aaf8d2b1d25b2b65800f8,UIST,2009,Changing how people view changes on the web,"Jaime Teevan, Susan T. Dumais, Daniel J. Liebling, Richard L. Hughes","1691357, 1728602, 1724850, 3021161","The Web is a dynamic information environment. Web content changes regularly and people revisit Web pages frequently. But the tools used to access the Web, including browsers and search engines, do little to explicitly support these dynamics. In this paper we present <i>DiffIE</i>, a browser plug-in that makes content change explicit in a simple and lightweight manner. DiffIE caches the pages a person visits and highlights how those pages have changed when the person returns to them. We describe how we built a stable, reliable, and usable system, including how we created compact, privacy-preserving page representations to support fast difference detection. Via a longitudinal user study, we explore how DiffIE changed the way people dealt with changing content. We find that much of its benefit came not from exposing expected change, but rather from drawing attention to unexpected change and helping people build a richer understanding of the Web content they frequent.",14,28.5714285714
UIST,a3e857aff09230452c6fd837ec3b7a362441d189,UIST,2016,Asymmetric Design Approach and Collision Avoidance Techniques For Room-scale Multiplayer Virtual Reality,Misha Sra,3024298,"Recent advances in consumer virtual reality (VR) technology have made it easy to accurately capture users' motions over room-sized areas allowing natural locomotion for navigation in VR. While this helps create a stronger match between proprioceptive information from human body movements for enhancing immersion and reducing motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space which for most users is quite small. Within this space the potential for colliding with physical objects around the play area is high. Additionally, only limited haptic feedback is available. In this paper, I focus on the problem of variations in the size and shape of each user's tracked physical space for multiplayer interactions. As part of the constrained physical space problem, I also present an automated system for steering the user away from play area boundaries using Galvanic Vestibular Stimulation (GVS). In my thesis, I will build techniques to enable the system to intelligently apply redirection and GVS-based steering as users explore virtual environments of arbitrary sizes.",0,44.6540880503
UIST,a1c516a2e844e5141c34ea56b9fae33c29966d20,UIST,2016,M.Sketch: Prototyping Tool for Linkage-Based Mechanism Design,"Han-Jong Kim, Yunwoo Jeong, Ju-Whan Kim, Tek-Jin Nam","2255353, 3396053, 2584081, 1696044","We present M.Sketch, a prototyping tool to support non-experts to design and build linkage-based mechanism prototype. It enables users to draw and simulate arbitrary mechanisms as well as to make physical prototype for testing actual movement. Mix of bottom-up and top-down sketching approaches, real-time movement visualization, and functions for digital fabrication can make the users to design the desired mechanism easily and effectively. M.Sketch can be used to design customized products with kinetic movement, such as interactive robot, toys, and sculptures.",0,44.6540880503
UIST,874edf678187887cc0d158f188a956b4f3100a59,UIST,2015,"GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel","Viktor Miruchna, Robert Walter, David Lindlbauer, Maren Lehmann, Regine von Klitzing, Jörg Müller","2415914, 6503339, 2287283, 2822086, 7139048, 1798163","We present <i>GelTouch</i>, a gel-based layer that can selectively transition between <i>soft</i> and <i>stiff</i> to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. <i>GelTouch</i> consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).",5,80.701754386
UIST,b8944d154765793442295bb1c913fe2cc692c949,UIST,2011,"Sex, food, and words: the hidden meanings behind everyday language",Daniel Jurafsky,1746807,"Language is a subtle and powerful tool for communication. But the words we use also provide a rich mine of information for the social scientist. The history of words like ""ketchup"", ""ceviche"", or ""dessert"" tells us about the relationships between the superpowers who dominated the globe 500 or 1000 years ago. The words on the back of potato chip packages can demonstrate popular attitudes toward social class. And the names we give ice cream flavors may be an evolutionary reflex of the attempt by early mammals to appear larger than their competitors. The language of dating is just as informative as the language of food. In experiments with speed dating, work in our lab shows that we can detect flirtation or other stances in men and women on dates, just by looking at linguistic features like their pitch, their use of negative words like ""can't"" or ""don't"", or how often they use hedges like ""sort of"" or ""kind of"". The language of these two popular topics of conversation, food and dating, can teach us a lot about history, culture, and psychology.",0,6.19047619048
UIST,ec77fc88acd5941c23b12a6e8f7910c8dafe5d2d,UIST,2015,"Investigating the ""Wisdom of Crowds"" at Scale","Alok Shankar Mysore, Vikas S. Yaligar, Imanol Arrieta Ibarra, Camelia Simoiu, Sharad Goel, Ramesh Arvind, Chiraag Sumanth, Arvind Srikantan, Bhargav HS, Mayank Pahadia, Tushar Dobha, Atif Ahmed, Mani Shankar, Himani Agarwal, Rajat Agarwal, Sai Anirudh-Kondaveeti, Shashank Arun-Gokhale, Aayush Attri, Arpita Chandra, Yogitha Chilukur, Sharath Dharmaji, Deepak Garg, Naman Gupta, Paras Gupta, Glincy Mary Jacob, Siddharth Jain, Shashank Joshi, Tarun Khajuria, Sameeksha Khillan, Sandeep Konam, Praveen Kumar-Kolla, Sahil Loomba, Rachit Madan, Akshansh Maharaja, Vidit Mathur, Bharat Munshi, Mohammed Nawazish, Venkata Neehar-Kurukunda, Venkat Nirmal-Gavarraju, Sonali Parashar, Harsh Parikh, Avinash Paritala, Amit Patil, Rahul Phatak, Mandar Pradhan, Abhilasha Ravichander, Krishna Sangeeth, Sreecharan Sankaranarayanan, Vibhor Sehgal, Ashrith Sheshan, Suprajha Shibiraj, Aditya Singh, Anjali Singh, Prashant Sinha, Pushkin Soni, Bipin Thomas, Kasyap Varma-Dattada, Sukanya Venkataraman, Pulkit Verma, Ishan Yelurwar","2728942, 2935033, 3238071, 2906955, 3184636, 2923604, 3308088, 3259829, 3185803, 2389809, 3217750, 4034369, 2758232, 8740758, 8622347, 2592688, 1976240, 2882028, 2291932, 3000157, 3221863, 2789786, 7639960, 2250013, 2481755, 1724271, 7505308, 2550452, 2542223, 2249518, 3103380, 3241534, 2919854, 2837482, 2077652, 2131130, 2259104, 3142695, 2239062, 2730165, 8458233, 2768840, 7189237, 2070697, 2638316, 3023068, 2031265, 3069162, 2059248, 3334066, 2542777, 2461059, 6318868, 2934670, 3051926, 2411040, 3162265, 2577158, 3061581, 2802542","In a variety of problem domains, it has been observed that the aggregate opinions of groups are often more accurate than those of the constituent individuals, a phenomenon that has been termed the ""wisdom of the crowd."" Yet, perhaps surprisingly, there is still little consensus on how generally the phenomenon holds, how best to aggregate crowd judgements, and how social influence affects estimates. We investigate these questions by taking a meta wisdom of crowds approach. With a distributed team of over 100 student researchers across 17 institutions in the United States and India, we develop a large-scale online experiment to systematically study the wisdom of crowds effect for 1,000 different tasks in 50 subject domains. These tasks involve various types of knowledge (e.g., explicit knowledge, tacit knowledge, and prediction), question formats (e.g., multiple choice and point estimation), and inputs (e.g., text, audio, and video). To examine the effect of social influence, participants are randomly assigned to one of three different experiment conditions in which they see varying degrees of information on the responses of others. In this ongoing project, we are now preparing to recruit participants via Amazon?s Mechanical Turk.",0,16.2280701754
UIST,fbcfe71916d10d4483013083b81e9def0b77ff00,UIST,2015,Kinetic Blocks: Actuated Constructive Assembly for Interaction and Display,"Philipp Schoessler, Daniel Windham, Daniel Leithinger, Sean Follmer, Hiroshi Ishii","1868968, 2205684, 3136322, 2770912, 1749649","Pin-based shape displays not only give physical form to digital information, they have the inherent ability to accurately move and manipulate objects placed on top of them. In this paper we focus on such object manipulation: we present ideas and techniques that use the underlying shape change to give kinetic ability to otherwise inanimate objects. First, we describe the shape display's ability to assemble, disassemble, and reassemble structures from simple passive building blocks through stacking, scaffolding, and catapulting. A technical evaluation demonstrates the reliability of the presented techniques. Second, we introduce special kinematic blocks that are actuated and sensed through the underlying pins. These blocks translate vertical pin movements into other degrees of freedom like rotation or horizontal movement. This interplay of the shape display with objects on its surface allows us to render otherwise inaccessible forms, like overhangs, and enables richer input and output.",0,16.2280701754
UIST,8beb797b1ef771dcb4f23488cc69cf5b56fd2548,UIST,2016,"Gushed Diffusers: Fast-moving, Floating, and Lightweight Midair Display","Ippei Suzuki, Shuntarou Yoshimitsu, Keisuke Kawahara, Nobutaka Ito, Atsushi Shinoda, Akira Ishii, Takatoshi Yoshida, Yoichi Ochiai","3428384, 3492744, 3460025, 1798203, 7270650, 1727896, 3461775, 1734002","We present a novel method for fast-moving aerial imaging using aerosol-based fog screens. Conventional systems of aerial imaging cannot move fast because they need large and heavy setup. In this study, we propose to add new tradeoffs between limited display time and payloads. This system employ aerosol distribution from off-the-shelf spray as a fog screen that can resist the wind, and have high portability. As application examples, we present wearable application and aerial imaging on objects with high speed movements such as a drone, a radio-controlled model car, and performers. We believe that our study contribute to the exploration of new application areas for fog displays and expand expressions of entertainments and interactivity.",0,44.6540880503
UIST,86ba6269bd8c2f2904f3e5c2ced263036b5b55bc,UIST,2001,A modular geometric constraint solver for user interface applications,Hiroshi Hosobe,1710423,"Constraints have been playing an important role in the user interface field since its infancy. A prime use of constraints in this field is to automatically maintain geometric layouts of graphical objects. To facilitate the construction of constraint-based user interface applications, researchers have proposed various constraint satisfaction methods and constraint solvers. Most previous research has focused on either local propagation or linear constraints, excluding more general nonlinear ones. However, nonlinear geometric constraints are practically useful to various user interfaces, e.g., drawing editors and information visualization systems. In this paper, we propose a novel constraint solver called Chorus, which realizes various powerful nonlinear geometric constraints such as Euclidean geometric, non-overlapping, and graph layout constraints. A key feature of Chorus is its module mechanism that allows users to define new kinds of geometric constraints. Also, Chorus supports ""soft"" constraints with hierarchical strengths or preferences (i.e., constraint hierarchies). We describe its framework, algorithm, implementation, and experimental results.",19,28.3333333333
UIST,3bfa16c615449107b4f751cd8daf05b58c2c68f3,UIST,2008,Attribute gates,"Ahmed Kharrufa, Patrick Olivier","2445830, 1707234","Attribute gates are a new user interface element designed to address the problem of concurrently setting attributes and moving objects between territories on a digital tabletop. Motivated by the notion of task levels in activity theory, and crossing interfaces, attribute gates allow users to operationalize multiple subtasks in one smooth movement. We present two configurations of attribute gates; (1) grid gates which spatially distribute attribute values in a regular grid, and require users to draw trajectories through the attributes; (2) polar gates which distribute attribute values on segments of concentric rings, and require users to align segments when setting attribute combinations. The layout of both configurations was optimised based on targeting and steering laws derived from Fitts' Law. A study compared the use of attribute gates with traditional contextual menus. Users of attribute gates demonstrated both increased performance and higher mutual awareness.",0,4.28571428571
UIST,b769e93d182442c810fac03cb1da6548429377bb,UIST,2013,StickEar: making everyday objects respond to sound,"Kian Peen Yeo, Suranga Nanayakkara, Shanaka Ransiri","2349609, 1744107, 2681009","This paper presents StickEar, a system consisting of a network of distributed 'Sticker-like' sound-based sensor nodes to propose a means of enabling sound-based interactions on everyday objects. StickEar encapsulates wireless sensor network technology into a form factor that is intuitive to reuse and redeploy. Each StickEar sensor node consists of a miniature sized microphone and speaker to provide sound-based input/output capabilities. We provide a discussion of interaction design space and hardware design space of StickEar that cuts across domains such as remote sound monitoring, remote triggering of sound, autonomous response to sound events, and controlling of digital devices using sound. We implemented three applications to demonstrate the unique interaction capabilities of StickEar.",4,51.8348623853
UIST,93fc7b32619852406d9dfcf907ff0dcbf4347dae,UIST,2011,Fit your hand: personalized user interface considering physical attributes of mobile device users,"Hosub Lee, Young Sang Choi","2176177, 1707390","We present a mobile user interface which dynamically reformulates the layout based on the touch input pattern of users. By analyzing the touch input, it infers users' physical characteristics such as handedness, finger length, or usage habits, thereby calculates the optimal touch area for the user. The user interface is gradually adapted to each user by automatically rearranging graphic objects such as application icons to the most easy-to-touch positions. To compute the optimal touch area, we designed software architecture and implemented an Android application which analyzes touch input and determines the touch frequency in specific screen areas, the handedness and hand size of users. As proof of concept, this research prototype shows acceptable performance and accuracy. To decide which items should be placed in the optimal touch area, we plan to integrate our machine learning algorithm which prioritizes applications according to the context of users into the proposed system.",1,15.7142857143
UIST,22bf2e76ccf9a1771dea65ef60305ac5abff12be,UIST,2013,inFORM: dynamic physical affordances and constraints through shape and object actuation,"Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, Hiroshi Ishii","2770912, 3136322, 2375159, 2046594, 1749649","Past research on shape displays has primarily focused on rendering content and user interface elements through shape output, with less emphasis on dynamically changing UIs. We propose utilizing shape displays in three different ways to mediate interaction: to facilitate by providing dynamic physical affordances through shape change, to restrict by guiding users with dynamic physical constraints, and to manipulate by actuating physical objects. We outline potential interaction techniques and introduce Dynamic Physical Affordances and Constraints with our inFORM system, built on top of a state-of-the-art shape display, which provides for variable stiffness rendering and real-time user input through direct touch and tangible interaction. A set of motivating examples demonstrates how dynamic affordances, constraints and object actuation can create novel interaction possibilities.",75,100.0
UIST,5a9a27bbfeb24030d320ba0ef3b6296e552c9c99,UIST,1991,Primitives for programming multi-user interfaces,"Prasun Dewan, Rajiv Choudhary","1749237, 1700086","We have designed a set of primitives for programming multiuser interfaces by extending a set of existing high-level primitives for programming single-user interfaces. These primitives support both collaboration-transparent and collaboration-aware multiuser programs and allow existing single-user programs to be incrementally changed to corresponding multiuser programs, The collaboration-aware primitives include primitives for tailoring the input and output to a user, authenticating users, executing code in a user's environment and querying and setting properties of it, and tailoring the user interface coupling. We have identified several application-independent user groups that arise in a collaborative setting and allow the original single-user calls to be targeted at these groups. In addition , we provide primitives for defining application-specific groups. Our preliminary experience with these primitives shows that they can be used to easily implement collabora-tive tasks of a wide range of applications including message systems, multiuser editors, computer conferencing systems, and coordination systems. In this paper, we motivate, describe, and illustrate these primitives, discuss how primitives similar to them can be offered by a variety of user interface tools, and point out future directions for work.",25,56.5217391304
UIST,d824745884672dd919eba55eaa93607fdefca8c4,UIST,2001,Tools for expressive text-to-speech markup,"Erik Blankinship, Richard Beckwith","2563349, 8100788","This paper describes handicapped accessible text-to-speech markup software developed for poetry and performance. Most text-to-speech software allows the user to select a voice, but provides no control over performance parameters such as rate, volume, and pitch. For users with vocal disabilities, the default ""computer voice"" is often dreaded since it provides no personalization. Evolving standards exist for text-to-speech markup (Sable, Java Speech Markup Language, Spoken Text Markup Language), but few tools exist for non-experts to modify documents using these prosody options [1, 5]. Furthermore, we could find fewer tools allowing for straightforward live performance using a synthesized voice [3]. Thus we created an easy to learn text-to-speech markup tool that requires little training to use.",3,10.0
UIST,21cf0f84eb385be4ee63f5987134709d6130afbe,UIST,2011,D.tour: Style-based Exploration of Design Example Galleries,"Daniel Ritchie, Ankita Arvind Kejriwal, Scott R. Klemmer","3106018, 1984324, 1728167","In design, people often seek examples for inspiration. However, current example-finding practices suffer many drawbacks: templates present designs without a usage context; search engines can only examine the text on a page. This paper introduces exploratory techniques for finding relevant and inspiring design examples. These novel techniques include searching by stylistic similarity to a known example design and searching by stylistic keyword. These interactions are manifest in d.tour, a style-based design exploration tool. d.tour presents a curated database of Web pages as an explorable design gallery. It extracts and analyzes design features of these pages, allowing it to process style-based queries and recommend designs to the user. d.tour's gallery interface decreases the gulfs of execution and evaluation for design example-finding.",9,44.2857142857
UIST,61c1c9655b0603e9d8f5bd07cc6a46ed2d5f08ac,UIST,2006,Projector-guided painting,"Matthew Flagg, James M. Rehg","2990157, 1692956","This paper presents a novel interactive system for guiding artists to paint using traditional media and tools. The enabling technology is a multi-projector display capable of controlling the appearance of an artist's canvas. This display-on-canvas guides the artist to construct the painting as a series of layers. Our process model for painting is based on classical techniques and was designed to address three main issues which are challenging to novices: (1) positioning and sizing elements on the canvas, (2) executing the brushstrokes to achieve a desired texture and (3) mixing pigments to make a target color. These challenges are addressed through a set of interaction modes. Preview and color selection modes enable the artist to focus on the current target layer by highlighting the areas of the canvas to be painted. Orientation mode displays brushstroke guidelines for the creation of desired brush texture. Color mixing mode guides the artist through the color mixing process with a user interface similar to a color wheel. These interaction modes allow a novice artist to focus on a series of manageable subtasks in executing a complex painting. Our system covers the gamut of the painting process from overall composition down to detailed brushwork. We present the results from a user study which quantify the benefit that our system can provide to a novice painter.",36,55.0
UIST,8feb891b1b211310a35289e95e381844dbec6eea,UIST,2011,"Redprint: integrating API specific ""instant example"" and ""instant documentation"" display interface in IDEs","Anant P. Bhardwaj, Dave Luciano, Scott R. Klemmer","3202553, 3172252, 1728167","Software libraries for most of the modern programming languages are numerous, large and complex. Remembering the syntax and usage of APIs is a difficult task for not just novices but also expert programmers. IDEs (Integrated Development Environment) provide capabilities like autocomplete and intellisense to assist programmers; however, programmers still need to visit search engines like Google to find API (Application Program Interface) documentation and samples. This paper evaluates Redprint - a browser based development environment for PHP that integrates API specific ""Instant Example"" and ""Instant Documentation"" display interfaces. A comparative laboratory study shows that integrating API specific ""Instant Example"" and ""Instant Documentation"" display interfaces into a development environment significantly reduces the cost of searching and thus significantly reduces the time to develop software.",1,15.7142857143
UIST,5a680a5257c27ca5f175c3edf9ccf7f71fde711d,UIST,2013,Visualizing web browsing history with barcode chart,"Borui Wang, Ningxia Zhang, Jianfeng Hu, Zheng Shen","2203187, 2137969, 6138044, 2149936","Inspired by the DNA art, we introduce a data visualization technique called barcode chart, which uses color-illuminated stripes that resemble barcodes to visualize temporal data. Barcode chart excels at demonstrating high-level patterns in highly segmented temporal data, while retaining details in the data through interaction. We demonstrate Yogurt, a browser extension that implements barcode chart to visualize online browsing history. We conducted a user study and analyzed the effectiveness of using barcode chart for Yogurt in comparison with other applications. We conclude that barcode chart satisfies the need of visualizing the high-density and high-fragmentation nature of temporal data in Yogurt, and helps reveal online distraction and other web browsing patterns.",0,10.5504587156
UIST,2a7cbde35c52034d3bd56dc45313e113dc1e152d,UIST,2014,"Physical telepresence: shape capture and display for embodied, computer-mediated remote collaboration","Daniel Leithinger, Sean Follmer, Alex Olwal, Hiroshi Ishii","3136322, 2770912, 2375159, 1749649","We propose a new approach to Physical Telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects, and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system.",17,89.9224806202
UIST,cd9ee5144a27487cad39f13bf8ff8a34db5d3c09,UIST,2007,Capturing the user's attention: insights from the study of human vision,Jeremy M. Wolfe,2171976,"An effective user interface is a cooperative interaction between humans and their technology. For that interaction to work, it needs to recognize the limitations and exploit the strengths of both parties. In this talk, I will concentrate on the human side of the equation. What do we know about human visual perceptual abilities that might have an impact on the design of user interfaces? The world presents us with more information than we can process. Just try to read this abstract and the next piece of prose at the same time. We cope with this problem by using attentional mechanisms to select a subset of the input for further processing. An inter-face might be designed to .capture. attention, in order to induce a human to interact with it. Once the human is using an interface, that interface should .guide. the user.s atten-tion in an intelligent manner. In recent decades, many of the rules of attentional capture and guidance have been worked out in the laboratory. I will illustrate some of the basic principles. For example: Do some colors grab attention better than others? Are faces special? When and why do people fail to .see. things that are right in front of their eyes.",0,4.16666666667
UIST,8d5c8c2fc544c70b91f03ce60d9aaf55cfe86973,UIST,2010,Eden: supporting home network management through interactive visual tools,"Jeonghwa Yang, W. Keith Edwards, David Haslem","2052026, 3023379, 2194963","As networking moves into the home, home users are increasingly being faced with complex network management chores. Previous research, however, has demonstrated the difficulty many users have in managing their networks. This difficulty is compounded by the fact that advanced network management tools - such as those developed for the enterprise - are generally too complex for home users, do not support the common tasks they face, and are not a good fit for the technical peculiarities of the home. This paper presents <i>Eden</i>, an interactive, direct manipulation home network management system aimed at end users. Eden supports a range of common tasks, and provides a simple conceptual model that can help users understand key aspects of networking better. The system leverages a novel home network router that acts as a ""dropin"" replacement for users' current router. We demonstrate that Eden not only improves the user experience of networking, but also aids users in forming workable conceptual models of how the network works.",19,76.7441860465
UIST,e4269505347daa544f2eb740c70b23750b1efdeb,UIST,2014,Going to the dogs: towards an interactive touchscreen interface for working dogs,"Clint Zeagler, Scott M. Gilliland, Larry Freil, Thad Starner, Melody Moore Jackson","2521246, 2099103, 2368488, 1738894, 3454635","Computer-mediated interaction for working dogs is an important new domain for interaction research. In domestic settings, touchscreens could provide a way for dogs to communicate critical information to humans. In this paper we explore how a dog might interact with a touchscreen interface. We observe dogs' touchscreen interactions and record difficulties against what is expected of humans' touchscreen interactions. We also solve hardware issues through screen adaptations and projection styles to make a touchscreen usable for a canine's nose touch interactions. We also compare our canine touch data to humans' touch data on the same system. Our goal is to understand the affordances needed to make touchscreen interfaces usable for canines and help the future design of touchscreen interfaces for assistive dogs in the home.",10,80.2325581395
UIST,5c9cb1b56f06e4f273c2795045050667e6b48cdc,UIST,2001,Empirical measurements of intrabody communication performance under varied physical configurations,"Kurt Partridge, Bradley Dahlquist, Alireza Veiseh, Annie Cain, Ann Foreman, Joseph Goldberg, Gaetano Borriello","1721265, 2651974, 2625148, 3310061, 2366477, 3589827, 1735801","Intrabody communication (IBC) is a wireless communications technology that uses a person's body as the transmission medium for imperceptible electrical signals. Because communication is limited to the vicinity of a person's body, ambiguities arising from communication between personal devices and environmental devices when multiple people are present can, in theory, be solved simply. Intrabody communication also potentially allows data to be transferred when a person touches an IBC-enabled device. We have designed and constructed an intrabody communication system, modeled after Zimmerman's original design, and extended it to operate up to 38.4Kbps and to calculate signal strength. In this paper, we present quantitative measurements of data error rates and signal strength while varying hand distance to transceiver plate, electrode location on the body, touch plate size and shape, and several other factors. We find that plate size and shape have only minor effects, but that the distance to plate and the coupling mechanism significantly effect signal strength. We also find that portable devices, with poor ground coupling, suffer more significant signal attenuation. Our goal is to promote design guidelines for this technology and identify the best contexts for its effective deployment.",19,28.3333333333
UIST,d432f41a0144dc78122c32f810bbbbc8c7f362be,UIST,2012,Bimanual gesture keyboard,"Xiaojun Bi, Ciprian Chelba, Tom Ouyang, Kurt Partridge, Shumin Zhai","1682293, 1802969, 7905267, 1721265, 1748079","Gesture keyboards represent an increasingly popular way to input text on mobile devices today. However, current gesture keyboards are exclusively unimanual. To take advantage of the capability of modern multi-touch screens, we created a novel bimanual gesture text entry system, extending the gesture keyboard paradigm from one finger to multiple fingers. To address the complexity of recognizing bimanual gesture, we designed and implemented two related interaction methods, finger-release and space-required, both based on a new multi-stroke gesture recognition algorithm. A formal experiment showed that bimanual gesture behaviors were easy to learn. They improved comfort and reduced the physical demand relative to unimanual gestures on tablets. The results indicated that these new gesture keyboards were valuable complements to unimanual gesture and regular typing keyboards.",12,60.2941176471
UIST,32a52bd33eccce34f9f0b942cc7f3914bd17d493,UIST,2006,Interacting with dynamically defined information spaces using a handheld projector and a pen,"Xiang Cao, Ravin Balakrishnan","7299595, 1748870","The recent trend towards miniaturization of projection technology indicates that handheld devices will soon have the ability to project information onto any surface, thus enabling interfaces that are not possible with current handhelds. We explore the design space of dynamically defining and interacting with multiple virtual information spaces embedded in a physical environment using a handheld projector and a passive pen tracked in 3D. We develop techniques for defining and interacting with these spaces, and explore usage scenarios.",82,95.0
UIST,101f6808a0244f725979e6e651bbef2f86410c1d,UIST,2007,Multi-user interaction using handheld projectors,"Xiang Cao, Clifton Forlines, Ravin Balakrishnan","7299595, 1694854, 1748870","Recent research on handheld projector interaction has expanded the display and interaction space of handheld devices by projecting information onto the physical environment around the user, but has mainly focused on single-user scenarios. We extend this prior single-user research to co-located multi-user interaction using multiple handheld projectors. We present a set of interaction techniques for supporting co-located collaboration with multiple handheld projectors, and discuss application scenarios enabled by them.",84,91.6666666667
UIST,8ef06752a2d8d59627672f9a005457d390ab9aaa,UIST,2014,A text entry technique for wrist-worn watches with tiny touchscreens,"Hyeonjoong Cho, Miso Kim, Kyeongeun Seo","2420548, 7590918, 3131371","We consider a text entry technique for wrist-worn watches with inch-scale touchscreens. Most of the watches which are commercially available, for example, Galaxy Gear, Omate, etc., have around 1.5-inch touchscreens that is too small for the shrinked Qwerty keyboard. Moreover, the virtual button-based techniques determine input-letters by distinguishing touched locations on touchscreens which continuously demands a user to carefully touch certain locations. Thus, they are not suitable to tiny-touchscreen devices in mobile environment. Instead, the proposed text entry technique allows a user to touch almost anywhere on the touchscreen for text entry by determining input-letters based on drag direction regardless of touched location. We implemented the proposed method on a commercial watch with 1.54-inch touchscreen for validating its feasibility.",3,49.2248062016
UIST,2dddff4da3830f68e66b704cc24e3ce9cd8b2fc0,UIST,2011,"PicoPet: ""Real World"" digital pet on a handheld projector","Yuhang Zhao, Chao Xue, Xiang Cao, Yuanchun Shi","2582568, 7503199, 7299595, 1732440","We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player's daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.",2,24.2857142857
UIST,59d6f320547dd1607e8ec56bd93be5f8dc2af3af,UIST,2014,World-stabilized annotations and virtual scene navigation for remote collaboration,"Steffen Gauglitz, Benjamin Nuernberger, Matthew Turk, Tobias Höllerer","1940938, 2089559, 1752714, 1743721","We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines.",18,90.6976744186
UIST,3c14188389ff7b3502e9c319be735c6b7f6de24c,UIST,2015,Gunslinger: Subtle Arms-down Mid-air Interaction,"Mingyu Liu, Mathieu Nancel, Daniel Vogel","3007208, 1793712, 3076153","We describe Gunslinger, a mid-air interaction technique using barehand postures and gestures. Unlike past work, we explore a relaxed arms-down position with both hands interacting at the sides of the body. It features ""hand-cursor"" feedback to communicate recognized hand posture, command mode and tracking quality; and a simple, but flexible hand posture recognizer. Although Gunslinger is suitable for many usage contexts, we focus on integrating mid-air gestures with large display touch input. We show how the Gunslinger form factor enables an interaction language that is equivalent, coherent, and compatible with large display touch input. A four-part study evaluates Midas Touch, posture recognition feedback, pointing and clicking, and general usability.",2,60.5263157895
UIST,8944f027355ad0484e25c514cad1777fd49e1d2d,UIST,2016,Reconstruction of Scene from Multiple Sketches,"Ayuri Tomohiro, Yasuyuki Sumi","2073856, 1717358","This paper discusses the feasibility of extension of expressive style with multiple 3D sketches drawn by a sketching tool that enables its users to draw and paint on 3D structured surfaces. Users of our proposed system take a picture of target objects and sketch with reference to the taken picture. They can not only sketch on the pictures but can also change their viewpoint of the sketched environment, since the system captures 3D structure by using a depth sensor as well as RGB data. Trial usage of the system shows that our users can rapidly extract their target objects/space and extend their ideas by taking pictures and drawing/painting on them. This paper presents examples of system usage, and discusses the feasibility of extension of sketches.",0,44.6540880503
UIST,36e89be7f9a13d0edc5f10a48b8a9089718ebb96,UIST,2012,SnipMatch: using source code context to enhance snippet retrieval and parameterization,"Doug Wightman, Zi Ye, Joel Brandt, Roel Vertegaal","3289511, 3886305, 1702922, 1687608","Programmers routinely use source code snippets to increase their productivity. However, locating and adapting code snippets to the current context still takes time: for example, variables must be renamed, and dependencies included. We believe that when programmers decide to invest time in creating a new code snippet from scratch, <i>they would also be willing to spend additional effort to make that code snippet configurable and easy to integrate</i>. To explore this insight, we built <i>SnipMatch</i>, a plug-in for the Eclipse IDE. SnipMatch introduces a simple markup that allows snippet authors to specify search patterns and integration instructions. SnipMatch leverages this information, in conjunction with current code context, to improve snippet search and parameterization. For example, when a search query includes local variables, SnipMatch suggests compatible snippets, and automatically adapts them by substituting in these variables. In the lab, we observed that participants integrated snippets faster when using SnipMatch than when using standard Eclipse. Findings from a public deployment to 93 programmers suggest that SnipMatch has become integrated into the work practices of real users.",9,57.3529411765
UIST,c2fedcde308c85f738b3f28ebbd2d97393f4ed8f,UIST,2016,Optical Marionette: Graphical Manipulation of Human's Walking Direction,"Akira Ishii, Ippei Suzuki, Shinji Sakamoto, Keita Kanai, Kazuki Takazawa, Hiraku Doi, Yoichi Ochiai","1727896, 3428384, 1801441, 3427871, 3427909, 8051862, 1734002","We present a novel manipulation method that subconsciously changes the walking direction of users via visual processing on a head mounted display (HMD). Unlike existing navigation systems that require users to recognize information and then follow directions as two separate, conscious processes, the proposed method guides users without them needing to pay attention to the information provided by the navigation system and also allows them to be graphically manipulated by controllers. In the proposed system, users perceive the real world by means of stereo images provided by a stereo camera and the HMD. Specifically, while walking, the navigation system provides users with real-time feedback by processing the images they have just perceived and giving them visual stimuli. This study examined two image-processing methods for manipulation of human's walking direction: moving stripe pattern and changing focal region. Experimental results indicate that the changing focal region method most effectively leads walkers as it changes their walking path by approximately 200 mm/m on average.",0,44.6540880503
UIST,ddc7b93b0891dd70b23b4686ac9a3c32eb71d890,UIST,2016,The UIST Video Browser: Creating Shareable Playlists of Video Previews,"Carla F. Griggio, Nam Giang, Germán Leiva, Wendy E. Mackay","2065577, 1783360, 2990046, 1732917","We introduce the UIST Video Browser which provides a rapid overview of the UIST 30-second video previews, based on the conference schedule. Attendees can see an overview of upcoming talks, search by topic, and create personalized, shareable video playlists that capture the most interesting or relevant papers.",0,44.6540880503
UIST,2d27ef004022d772b9b2533db60f112589c0a4ce,UIST,2005,Automation and customization of rendered web pages,"Michael Bolin, Matthew Webber, Philip Rha, Thomas D. Wilson, Rob Miller","2267213, 8373850, 3138491, 1766823, 1723785","On the desktop, an application can expect to control its user interface down to the last pixel, but on the World Wide Web, a content provider has no control over how the client will view the page, once delivered to the browser. This creates an opportunity for end-users who want to automate and customize their web experiences, but the growing complexity of web pages and standards prevents most users from realizing this opportunity. We describe Chickenfoot, a programming system embedded in the Firefox web browser, which enables end-users to automate, customize, and integrate web applications without examining their source code. One way Chickenfoot addresses this goal is a novel technique for identifying page components by keyword pattern matching. We motivate this technique by studying how users name web page components, and present a heuristic keyword matching algorithm that identifies the desired component from the user's name.",136,90.3225806452
UIST,03ccb08b4efef014eb1bce40b276c3d1bbcb784b,UIST,2010,"Towards a unified framework for modeling, dispatching, and interpreting uncertain input",Julia Schwarz,2251233,"Many new input technologies (such as touch and voice) hold the promise of more natural user interfaces. However, many of these technologies create inputs with some uncertainty. Unfortunately, conventional infrastructure lacks a method for easily handling uncertainty, and as a result input produced by these technologies is often converted to conventional events as quickly as possible, leading to a stunted interactive experience. Our ongoing work aims to design a unified framework for modeling uncertain input and dispatching it to interactors. This should allow developers to easily create interactors which can interpret uncertain input, give the user appropriate feedback, and accurately resolve any ambiguity. This abstract presents an overview of the design of a framework for handling input with uncertainty and describes topics we hope to pursue in future work. We also give an example of how we built highly accurate touch buttons using our framework. For examples of what interactors can be built and a more detailed description of our framework we refer the reader to [8].",0,9.3023255814
UIST,dd7bab9f0cb7e310facdc206c86a7feddf9a50c1,UIST,2012,A user-specific machine learning approach for improving touch accuracy on mobile devices,"Daryl Weir, Simon Rogers, Roderick Murray-Smith, Markus Löchtefeld","1966315, 1877643, 1723152, 1808720","We present a flexible Machine Learning approach for learning user-specific touch input models to increase touch accuracy on mobile devices. The model is based on flexible, non-parametric Gaussian Process regression and is learned using recorded touch inputs. We demonstrate that significant touch accuracy improvements can be obtained when either raw sensor data is used as an input or when the device's reported touch location is used as an input, with the latter marginally outperforming the former. We show that learned offset functions are highly nonlinear and user-specific and that user-specific models outperform models trained on data pooled from several users. Crucially, significant performance improvements can be obtained with a small (&#8776;200) number of training examples, easily obtained for a particular user through a calibration game or from keyboard entry data.",17,71.0784313725
UIST,1ec9523049e7de2e7b04567eac84395cfbef557b,UIST,2005,Zoom-and-pick: facilitating visual zooming and precision pointing with interactive handheld projectors,"Clifton Forlines, Ravin Balakrishnan, Paul A. Beardsley, Jeroen van Baar, Ramesh Raskar","1694854, 1748870, 1777539, 1751880, 1717566","Designing interfaces for interactive handheld projectors is an exiting new area of research that is currently limited by two problems: hand jitter resulting in poor input control, and possible reduction of image resolution due to the needs of image stabilization and warping algorithms. We present the design and evaluation of a new interaction technique, called <i>zoom-and-pick</i>, that addresses both problems by allowing the user to fluidly zoom in on areas of interest and make accurate target selections. Subtle design features of <i>zoom-and-pick</i> enable pixel-accurate pointing, which is not possible in most freehand interaction techniques. Our evaluation results indicate that <i>zoom-and-pick</i> is significantly more accurate than the standard pointing technique described in our previous work.",24,40.3225806452
UIST,77fa7868a105034d84c1cb74d9a073cc86209879,UIST,2016,Music Composition with Recommendation,"Junki Kikuchi, Hidekatsu Yanagi, Yoshiaki Mima","3492874, 2119772, 2956401","Creating a piece of music requires deep knowledge of composition, and is time-consuming even for experts. Algorithmic composition systems can generate pieces in an existing style. However, these systems are not interactive. Therefore, it is difficult for them to express the user's intention. We propose a system that recommends a continuation melody in accordance with a melody expressed by the user. Recommendation uses the style of the piece of the composer, thus users give the system a piece of the style in which they want to compose. With this system, users can compose pieces tailored to their needs, and composers can get assistance with composition.",0,44.6540880503
UIST,cbdcf2af972c10ea26285a7d0b4cc44c9abb189e,UIST,2013,Visimu: a game for music color label collection,"Borui Wang, Jingshu Chen","2203187, 2505765","Based on previous studies of the associations between color and music, we introduce a scalable way of using colors to label songs and a visualization of music archives that facilitates music exploration. We present Visimu, an online game that attracted users to generate 926 color labels for 102 songs, with over 75% of the songs having color labels reaching high consensus in the Lab color space. We implemented a music archive visualization using the color labels generated by Visimu, and conducted an experiment to show that labeling music by color is more effective than text tags when the user is looking for songs of a particular mood or use scenario. Our results showed that Visimu is effective to produce meaningful color labels for music mood classification, and such approach enables a wide range of applications for music visualization and discovery.",1,27.0642201835
UIST,0d68070eed2e622ffeeef6c6c834fabe80f25f33,UIST,2001,Parallel bargrams for consumer-based information exploration and choice,"Kent Wittenburg, Tom Lanning, Michael Heinrichs, Michael Stanton","3242147, 2305161, 1990177, 2887496","In this paper we introduce multidimensional visualization and interaction techniques that are an extension to related work in parallel histograms and dynamic querying. Bargrams are, in effect, histograms whose bars have been tipped over and lined up end-to-end. We discuss affordances of parallel bargrams in the context of systems that support consumer-based information exploration and choice based on the attributes of the items in the choice set. Our tool called EZChooser has enabled a number of prototypes in such domains as Internet shopping, investment decisions, college choice, and so on, and a limited version has been deployed for car shopping. Evaluations of the techniques include an experiment indicating that trained users prefer EZChooser over static tables for choice tasks among sets of 50 items with 7-9 attributes.",42,46.6666666667
UIST,1990162a1c17fa9a236620e5c730c4e212eb0394,UIST,2010,Connected environments,Natalie Jeremijenko,1853127,"Can new interfaces contribute to social and environmental improvement? For all the care, wit and brilliance that UIST innovations can contribute, can they actually make things better - better in the sense of public good - not merely lead to easier to use or more efficient consumer goods? This talk will explore the impact of interface technology on society and the environment, and examine engineered systems that invite participation, document change over time, and suggest alternative courses of action that are ethical and sustainable, drawing on examples from a diverse series of experimental designs and site-specific work Natalie has created throughout her career.",0,9.3023255814
UIST,827573a74c097a0f83e79454e750d094d415b436,UIST,2015,BackHand: Sensing Hand Gestures via Back of the Hand,"Jhe-Wei Lin, Chiuan Wang, Yi Yao Huang, Kuan-Ting Chou, Hsuan-Yu Chen, Wei-Luan Tseng, Mike Y. Chen","8188668, 2503613, 2131280, 2445817, 2610243, 2693173, 2335746","In this paper, we explore using the back of hands for sensing hand gestures, which interferes less than glove-based approaches and provides better recognition than sensing at wrists and forearms. Our prototype, BackHand, uses an array of strain gauge sensors affixed to the back of hands, and applies machine learning techniques to recognize a variety of hand gestures. We conducted a user study with 10 participants to better understand gesture recognition accuracy and the effects of sensing locations. Results showed that sensor reading patterns differ significantly across users, but are consistent for the same user. The leave-one-user-out accuracy is low at an average of 27.4%, but reaches 95.8% average accuracy for 16 popular hand gestures when personalized for each participant. The most promising location spans the 1/8~1/4 area between the metacarpophalangeal joints (MCP, the knuckles between the hand and fingers) and the head of ulna (tip of the wrist).",2,60.5263157895
UIST,ff3f4019d9a8caf07a1289abe590d14fc5d5c253,UIST,2016,Expanding the Field-of-View of Head-Mounted Displays with Peripheral Blurred Images,"Wataru Yamada, Hiroyuki Manabe","1696877, 2962182","Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn't need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.",0,44.6540880503
UIST,37719872f524b0a0b585cbd47a8e12bb4e4a3427,UIST,2002,User interfaces when and where they are needed: an infrastructure for recombinant computing,"Mark W. Newman, Shahram Izadi, W. Keith Edwards, Jana Z. Sedivy, Trevor F. Smith","4590190, 1699068, 3023379, 3329659, 1862263","Users in ubiquitous computing environments need to be able to make serendipitous use of resources that they did not anticipate and of which they have no prior knowledge. The Speakeasy recombinant computing framework is designed to support such ad hoc use of resources on a network. In addition to other facilities, the framework provides an infrastructure through which device and service user interfaces can be made available to users on multiple platforms. The framework enables UIs to be provided for connections involving multiple entities, allows these UIs to be delivered asynchronously, and allows them to be injected by any party participating in a connection.",34,45.8333333333
UIST,4533914c05b5003927aaa98a868102a1c610c60c,UIST,2015,Enhanced Motion Robustness from ToF-based Depth Sensing Cameras,"Wataru Yamada, Hiroyuki Manabe, Hiroshi Inamura","1696877, 2962182, 1893266","Depth sensing cameras that can acquire RGB and depth information are being widely used. They can expand and enhance various camera-based applications and are cheap but strong tools for computer human interaction. RGB and depth sensing cameras have quite different key parameters, such as exposure time. We focus on the differences in their motion robustness; the RGB camera has relatively long exposure times while those of ToF (Time-of-flight) based depth sensing camera are relatively short. An experiment on visual tag reading, one typical application, shows that depth sensing cameras can robustly decode moving tags. The proposed technique will yield robust tag reading, indoor localization, and color image stabilization while walking and jogging or even glancing momentarily without requiring any special additional devices.",0,16.2280701754
UIST,4de5ef0f29ee116ee735d2380b77926e4c0b3c53,UIST,2014,Tag system with low-powered tag and depth sensing camera,"Hiroyuki Manabe, Wataru Yamada, Hiroshi Inamura","2962182, 1696877, 1893266","A tag system is proposed that offers a practical approach to ubiquitous computing. It provides small and low-power tags that are easy to distribute; does not need a special device to read the tags (in the future), thus enabling their use anytime, anywhere; and has a wide reading range in angle and distance that extends the design space of tag-based applications. The tag consists of a kind of liquid crystal (LC) and a retroreflector, and it sends its ID by switching the LC. A depth sensing camera that emits infrared (IR) is used as the tag reader; we assume that it will be part of the user's everyday devices, such as a smartphone. Experiments were conducted to confirm its potential, and a regular IR camera was also tested for comparison. The results show that the tag system has a wide readable range in terms of both distance (up to 8m) and viewing angle offset. Several applications were also developed to explore the design space. Finally, limitations of the current setup and possible improvements are discussed.",1,31.007751938
UIST,0e41853242ac915440ef4962638d063aa8e0ac1f,UIST,2013,SeeSS: seeing what i broke - visualizing change impact of cascading style sheets (css),"Hsiang-Sheng Liang, Kuan-Hung Kuo, Po-Wei Lee, Yu-Chien Chan, Yu-Chin Lin, Mike Y. Chen","2905335, 3237647, 3764535, 2659145, 7892701, 2335746","Cascading Style Sheet (CSS) is a fundamental web language for describing the presentation of web pages. CSS rules are often reused across multiple parts of a page and across multiple pages throughout a site to reduce repetition and to provide a consistent look and feel. When a CSS rule is modified, developers currently have to manually track and visually inspect all possible parts of the site that may be impacted by that change. We present SeeSS, a system that automatically tracks CSS change impact across a site and enables developers to easily visualize all of them. The impacted page fragments are sorted by severity and the differences before and after the change are highlighted using animation.",10,68.3486238532
UIST,359a217e3d022f081be64dfc499213beb1fc4b0b,UIST,2010,"D-Macs: building multi-device user interfaces by demonstrating, sharing and replaying design actions","Jan Meskens, Kris Luyten, Karin Coninx","3084250, 1681624, 1695519","Multi-device user interface design mostly implies creating suitable interface for each targeted device, using a diverse set of design tools and toolkits. This is a time consuming activity, concerning a lot of repetitive design actions without support for reusing this effort in later designs. In this paper, we propose D-Macs: a design tool that allows designers to <i>record</i> their design actions across devices, to <i>share</i> these actions with other designers and to <i>replay</i> their own design actions and those of others. D-Macs lowers the burden in multi-device user interface design and can reduce the necessity for manually repeating design actions.",4,48.2558139535
UIST,92a56026adccffb6486b575141cfd194f03be3e9,UIST,2016,WithYou: An Interactive Shadowing Coach with Speech Recognition,"Xinlei Zhang, Takashi Miyaki, Jun Rekimoto","1934695, 2279502, 1685962","Speech shadowing, in which the subject listens to native narration sound and tries to repeat it immediately while listening, is a proven way of practicing speaking skills when learning foreign languages. However, since the narration is independent of user's speech, the playback cannot make an adjustment when the learner fails to catch up, and this makes shadowing difficult. We propose WithYou, a system based on Automated Speech Recognition (ASR) that is able to adjust narration playback during a live shadowing speech. WithYou compares the student's live speech with the narration playback to detect shadowing mistakes. In addition, WithYou is able to handle pauses and recognize repetitive phrases in shadowing practice. A user study shows that practicing shadowing with WithYou is easier and more effective compared with conventional methods.",0,44.6540880503
UIST,fea4ae34b4a7086d1b36e17aed86ea6d24eb6eb4,UIST,2001,PhotoMesa: a zoomable image browser using quantum treemaps and bubblemaps,Benjamin B. Bederson,1799187,"PhotoMesa is a zoomable image browser that uses a novel treemap algorithm to present large numbers of images grouped by directory, or other available metadata. It uses a new interaction technique for zoomable user interfaces designed for novices and family use that makes it straightforward to navigate through the space of images, and impossible to get lost.PhotoMesa groups images using one of two new algorithms that lay out groups of objects in a 2D space-filling manner. <i>Quantum treemaps</i> are designed for laying out images or other objects of indivisible (quantum) size. They are a variation on existing treemap algorithms in that they guarantee that every generated rectangle will have a width and height that are an integral multiple of an input object size. <i>Bubblemaps</i> also fill space with groups of quantum-sized objects, but generate non-rectangular blobs, and utilize space more efficiently.",240,93.3333333333
UIST,3363580bc4e1e5f4c5047edfb3f00183558523ec,UIST,2000,Fisheye menus,Benjamin B. Bederson,1799187,"We introduce "" fisheye menus "" whichh apply traditional fisheyegraphical visualizationn techniques tolinearmenus. This provides for ann efficient mechanism to select items from longg menus, which are becomingg more common as menus are usedd to select data items in, for example, e-commerce applications. Fisheye menus dynamically change the size of menu items to provide a focus area around the mouse pointer. This makes it possible to",80,68.0
UIST,8a86985f9172245557b144d49ba305047d57c193,UIST,2011,Clip-on gadgets: expanding multi-touch interaction area with unpowered tactile controls,"Neng-Hao Yu, Sung-Sheng Tsai, I-Chun Hsiao, Dian-Je Tsai, Meng-Han Lee, Mike Y. Chen, Yi-Ping Hung","2436503, 2710704, 3438729, 2031840, 2564995, 2335746, 7312257","Virtual keyboards and controls, commonly used on mobile multi-touch devices, occlude content of interest and do not provide tactile feedback. Clip-on Gadgets solve these issues by extending the interaction area of multi-touch devices with physical controllers. Clip-on Gadgets use only conductive materials to map user input on the controllers to touch points on the edges of screens; therefore, they are battery-free, lightweight, and low-cost. In addition, they can be used in combination with multi-touch gestures. We present several hardware designs and a software toolkit, which enable users to simply attach Clip-on Gadgets to an edge of a device and start interacting with it.",18,65.2380952381
UIST,71955648962b1b4de059f65941de6d42b61aac21,UIST,2010,Enabling tangible interaction on capacitive touch panels,"Neng-Hao Yu, Li-Wei Chan, Lung-Pan Cheng, Mike Y. Chen, Yi-Ping Hung","2436503, 1682665, 2763041, 2335746, 7312257","We propose two approaches to sense tangible objects on capacitive touch screens, which are used in off-the-shelf multi-touch devices such as Apple iPad, iPhone, and 3M's multi-touch displays. We seek for the approaches that do not require modifications to the panels: <i>spatial tag</i> and <i>frequency tag</i>. Spatial tag is similar to fiducial tag used by tangible tabletop surface interaction, and uses multi-point, geometric patterns to encode object IDs. Frequency tag simulates high-frequency touches in the time domain to encode object IDs, using modulation circuits embedded inside tangible objects to simulate high-speed touches in varying frequency. We will show several demo applications. The first combines simultaneous tangible + touch input system. This explores how tangible inputs (e.g., pen, easer, etc.) and some simple gestures work together on capacitive touch panels.",2,33.1395348837
UIST,8490656ff98f82b2d54b3307a5e4a4c118f07362,UIST,2015,Spotlights: Facilitating Skim Reading with Attention-Optimized Highlights,"Byungjoo Lee, Antti Oulasvirta","2428695, 2663734","This demo presents Spotlights, a technique to facilitate skim reading, or the activity of rapidly comprehending long documents such as webpages or PDFs. Users mainly use continuous rate-based scrolling to skim. However, visual attention fails when scrolling rapidly due to excessive number of objects and brief exposure per object. Spotlights supports continuous scrolling at high speeds. It selects a small number of objects and raises them to transparent overlays (spotlights) in the viewer. Spotlights stay static for a prolonged time and then fade away. The technical contribution is novel method for ?brokering? user?s attentional resources in a way that guarantees sufficient attentional resources for some objects, even at very high scrolling rates. It facilitates visual attention by (1) decreasing the number of objects competing for divided attention and (2) by ensuring sufficient processing time per object.",0,16.2280701754
UIST,feb46cb68941e047cfe26a84e9399be1b5e48431,UIST,2001,Pop through mouse button interactions,"Robert C. Zeleznik, Timothy S. Miller, Andrew S. Forsberg","1713625, 1684620, 1792958","We present a range of novel interactions enabled by a simple modification in the design of a computer mouse. By converting each mouse button to <i>pop through</i> tactile push-buttons, similar to the focus/shutter-release buttons used in many cameras, users can feel, and the computer can sense, two distinct ""clicks"" corresponding to pressing lightly and pressing firmly to pop through. Despite the prototypical status of our hardware and software implementations, our current pop through mouse interactions are compelling and warrant further investigation. In particular, we demonstrate that pop through buttons not only yield an additional button activation state that is composable with, or even preferable to, techniques such as double-clicking, but also can endow a qualitatively novel user experience when meaningfully and consistently applied. We propose a number of software guidelines that may provide a consistent, systemic benefit; for example, light pressure may invoke default interaction (short menu), and firm pressure may supply more detail (long menu).",25,33.3333333333
UIST,8d5c821c496e95353a3dcb85996f6a3ad8eb5d22,UIST,2014,M-gesture: geometric gesture authoring framework for multi-device gestures using wearable devices,"Ju-Whan Kim, Tek-Jin Nam","2584081, 1696044","Wearable devices and mobile devices have great potential to detect various body motions as they are attached to different body parts. We present M-Gesture, a geometric gesture authoring framework using multiple wearable devices. We implemented physical metaphor, geometric gesture language, and continuity in spatial layout for easy and clear gesture authoring. M-Gesture demonstrates the use of geometric notation as an intuitive gesture language.",0,12.015503876
UIST,82fd25fc1543a51ecc7beb5727b9cea2a2317d8b,UIST,2006,Personalizing routes,"Kayur Patel, Mike Y. Chen, Ian E. Smith, James A. Landay","2760803, 2335746, 3093905, 1708404","Navigation services (e.g., in-car navigation systems and online mapping sites) compute routes between two locations to help users navigate. However, these routes may direct users along an unfamiliar path when a familiar path exists, or, conversely, may include redundant information that the user already knows. These overly complicated directions increase the cognitive load of the user, which may lead to a dangerous driving environment. Since the level of detail is user specific and depends on their familiarity with a region, routes need to be personalized. We have developed a system, called MyRoute, that reduces route complexity by creating user specific routes based on a priori knowledge of familiar routes and landmarks. MyRoute works by <i>compressing</i> well known steps into a single contextualized step and <i>rerouting</i> users along familiar routes.",21,31.25
UIST,12c046fea22bf316333fd3cbdc7614bfccd24548,UIST,2010,QWIC: performance heuristics for large scale exploratory user interfaces,"Daniel A. Smith, Joe Lambert, Monica M. C. Schraefel, David Bretherton","2649789, 1930302, 2284695, 3253366","Faceted browsers offer an effective way to explore relationships and build new knowledge across data sets. So far, web-based faceted browsers have been hampered by limited feature performance and scale. QWIC, Quick Web Interface Control, describes a set of design heuristics to address performance speed both at the interface and the backend to operate on large-scale sources.",0,9.3023255814
UIST,87e6745631edb4b47ca70ab3b7ecee29adba715d,UIST,2016,AquaCAVE: Augmented Swimming Environment with Immersive Surround-Screen Virtual Reality,"Shogo Yamashita, Xinlei Zhang, Jun Rekimoto","2816867, 1934695, 1685962","AquaCAVE is a system for enhancing the swimming experience. Although swimming is considered to be one of the best exercises to maintain our health, swimming in a pool is normally monotonous; thus, maintaining its motivation is sometimes difficult. AquaCAVE is a computer-augmented swimming pool with rear-projection acrylic walls that surround a swimmer, providing a CAVE-like immersive stereoscopic projection environment. The swimmer wears goggles with liquid-crystal display (LCD) shutter glasses, and cameras installed in the pool tracks swimmer's head position. Swimmers can be immersed into synthetic scenes such as coral reefs, outer space, or any other computer generated environments. The system can also provide swimming training with projections such as record lines and swimming forms as 3D virtual characters in the 3D space.",1,92.7672955975
UIST,def252b76817b3aa4bb3046053924d524545115e,UIST,2016,CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits,"Chiuan Wang, Hsuan-Ming Yeh, Bryan Wang, Te-Yen Wu, Hsin-Ruey Tsai, Rong-Hao Liang, Yi-Ping Hung, Mike Y. Chen","2503613, 3492221, 3492171, 1983696, 3148028, 1705512, 7312257, 2335746","For makers and developers, circuit prototyping is an integral part of building electronic projects. Currently, it is common to build circuits based on breadboard schematics that are available on various maker and DIY websites. Some breadboard schematics are used as is without modification, and some are modified and extended to fit specific needs. In such cases, diagrams and schematics merely serve as blueprints and visual instructions, but users still must physically wire the breadboard connections, which can be time-consuming and error-prone. We present CircuitStack, a system that combines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit construction. This hybrid system enables circuit reconfigurability, component reusability, and high efficiency at the early stage of prototyping development.",0,44.6540880503
UIST,2052f781f8cbaff9cf781b539a107066f2de82c9,UIST,2002,FLANNEL: adding computation to electronic mail during transmission,"Victoria Bellotti, Nicolas Ducheneaut, Mark Howard, Christine Neuwirth, Ian Smith, Trevor F. Smith","1690707, 1697237, 4542193, 1790517, 1735225, 1862263","In this paper, we describe FLANNEL, an architecture for adding computational capabilities to email. FLANNEL allows email to be modified by an application while in transit between sender and receiver. This modification is done without modification to the endpoints---mail clients---at either end. This paper also describes interaction techniques that we have developed to allow senders of email to quickly and easily select computations to be performed by FLANNEL. Through, our experience, we explain the properties that applications must have in order to be successful in the context of FLANNEL.",2,6.25
UIST,8ffcce766bd5fd69a1c8c5b1b224ef84b2452998,UIST,2015,MagPad: A Near Surface Augmented Reading System for Physical Paper and Smartphone Coupling,"Ding Xu, Ali Momeni, Eric Brockmeyer","3171253, 3157441, 1715064","In this paper, we present a novel near surface augmented reading system that brings digital content to physical papers. Our system allows a collocated mobile phone to provide augmented content based on its position on top of paper. Our system utilizes built-in magnetometer of a smartphone together with six constantly spinning magnets that generate designed patterns of magnetic flux, to detect 2D location of phone and render dynamic interactive content on the smartphone screen. The proposed technique could be implemented on most of mobile platforms without external sensing hardware.",0,16.2280701754
UIST,0e5322f9b4540b8e367e4772638095da562759a1,UIST,2012,Jamming user interfaces: programmable particle stiffness and sensing for malleable and shape-changing devices,"Sean Follmer, Daniel Leithinger, Alex Olwal, Nadia Cheng, Hiroshi Ishii","2770912, 3136322, 2375159, 1770405, 1749649","Malleable and organic user interfaces have the potential to enable radically new forms of interactions and expressiveness through flexible, free-form and computationally controlled shapes and displays. This work, specifically focuses on particle jamming as a simple, effective method for flexible, shape-changing user interfaces where programmatic control of material stiffness enables haptic feedback, deformation, tunable affordances and control gain. We introduce a compact, low-power pneumatic jamming system suitable for mobile devices, and a new hydraulic-based technique with fast, silent actuation and optical shape sensing. We enable jamming structures to sense input and function as interaction devices through two contributed methods for high-resolution shape sensing using: 1) index-matched particles and fluids, and 2) capacitive and electric field sensing. We explore the design space of malleable and organic user interfaces enabled by jamming through four motivational prototypes that highlight jamming's potential in HCI, including applications for tabletops, tablets and for portable shape-changing mobile devices.",47,93.6274509804
UIST,00ce00d9218f28642e1f510750f1f24ebb14f665,UIST,2014,FlatFitFab: interactive modeling with planar sections,"James McCrae, Nobuyuki Umetani, Karan Singh","2256139, 2065148, 1682205","We present a comprehensive system to author planar section structures, common in art and engineering. A study on how planar section assemblies are imagined and drawn guide our design principles: planar sections are best drawn in-situ, with little foreshortening, orthogonal to intersecting planar sections, exhibiting regularities between planes and contours. We capture these principles with a novel drawing workflow where a single fluid user stroke specifies a 3D plane and its contour in relation to existing planar sections. Regularity is supported by defining a vocabulary of procedural operations for intersecting planar sections. We exploit planar structure properties to provide real-time visual feedback on physically simulated stresses, and geometric verification that the structure is stable, connected and can be assembled. This feedback is validated by real-world fabrication and testing. As evaluation, we report on over 50 subjects who all used our system with minimal instruction to create unique models.",3,49.2248062016
UIST,7e212ae39c1671d00593bbcbaa454d6c447be4bf,UIST,2011,Direct and gestural interaction with relief: a 2.5D shape display,"Daniel Leithinger, David Lakatos, Anthony DeVincenzi, Matthew Blackshaw, Hiroshi Ishii","3136322, 1876042, 3127538, 1905534, 1749649","Actuated shape output provides novel opportunities for experiencing, creating and manipulating 3D content in the physical world. While various shape displays have been proposed, a common approach utilizes an array of linear actuators to form 2.5D surfaces. Through identifying a set of common interactions for viewing and manipulating content on shape displays, we argue why input modalities beyond direct touch are required. The combination of freehand gestures and direct touch provides additional degrees of freedom and resolves input ambiguities, while keeping the locus of interaction on the shape output. To demonstrate the proposed combination of input modalities and explore applications for 2.5D shape displays, two example scenarios are implemented on a prototype system.",40,84.7619047619
UIST,0bd5e2977e860a25a0dee3c2abf1e0e711e519d0,UIST,2014,GaussStones: shielded magnetic tangibles for multi-token interactions on portable displays,"Rong-Hao Liang, Han-Chih Kuo, Li-Wei Chan, De-Nian Yang, Bing-Yu Chen","1705512, 1736008, 1682665, 1731140, 1733344","This work presents GaussStones, a system of shielded magnetic tangibles design for supporting multi-token interactions on portable displays. Unlike prior works in sensing magnetic tangibles on portable displays, the proposed tangible design applies magnetic shielding by using an inexpensive galvanized steel case, which eliminates interference between magnetic tangibles. An analog Hall-sensor grid can recognize the identity of each shielded magnetic unit since each unit generates a magnetic field with a specific intensity distribution and/or polarization. Combining multiple units as a knob further allows for resolving additional identities and their orientations. Enabling these features improves support for applications involving multiple tokens. Thus, using prevalent portable displays provides generic platforms for tangible interaction design.",7,72.0930232558
UIST,0c4c6365eb0268d8e5ccd0266228cf787b7e9cb3,UIST,2005,Bimanual and unimanual image alignment: an evaluation of mouse-based techniques,"Celine Latulipe, Craig S. Kaplan, Charles L. A. Clarke","1683753, 2345263, 1751287","We present an evaluation of three mouse-based techniques for aligning digital images. We investigate the physical image alignment task and discuss the implications for interacting with virtual images. In a formal evaluation we show that a symmetric bimanual technique outperforms an asymmetric bimanual technique which in turn outperforms a unimanual technique. We show that even after mode switching times are removed, the symmetric technique outperforms the single mouse technique. Subjects also exhibited more parallel interaction using the symmetric technique than when using the asymmetric technique.",41,53.2258064516
UIST,560fa0a951c156b828403423aed955e114fca12e,UIST,2016,Sparkle: Towards Haptic Hover-Feedback with Electric Arcs,"Daniel Spelmezan, Deepak Ranjan Sahoo, Sriram Subramanian","2264859, 2548351, 1702794","We demonstrate a method for stimulating the fingertip with touchable electric arcs above a hover sensing input device. We built a hardware platform using a high-voltage resonant transformer for which we control the electric discharge to create in-air haptic feedback up to 4 mm in height, and combined this technology with infrared proximity sensing. Our method is a first step towards supporting novel in-air haptic experiences for hover input that does not require the user to wear haptic feedback stimulators.",0,44.6540880503
UIST,25c06febb6bc3363cb96a18361cb49f37f960ae2,UIST,2006,Multi-layer interaction for digital tables,"Sriram Subramanian, Dzmitry Aliakseyeu, Andrés Lucero","1702794, 1742400, 7366744","Interaction on digital tables has been restricted to a single layer on the table's active work-surface. We extend the design space of digital tables to include multiple layers of interaction. We leverage 3D position information of a pointing device to support interaction in the space above the active work-surface by creating multiple layers with drift-correction in which the user can interact with an application. We also illustrate through a point-design that designers can use multiple-layers to create a rich and clutter free application. A subjective evaluation showed that users liked the interaction techniques and found that, because of the drift correction we use, they could control the pointer when working in any layer.",35,51.25
UIST,a6854cf3fbefc54d9aa74daeba975c0bdbe7811a,UIST,2011,Cloudtop: a workspace for the cloud,"Hubert Pham, Justin Mazzola Paluska, Rob Miller, Steve Ward","2095906, 1737421, 1723785, 3190843","Even as users rely more on the web for their computing needs, they continue to depend on a desktop-like area for quick access to in-use resources. The traditional desktop is file-centric and prone to clutter, making it suboptimal for use in a web-dominated world. This paper introduces Cloudtop, a browser plugin that offers a lightweight workplace for temporary items, optimized around the idea that its contents originate from and will ultimately return to the web. Cloudtop improves upon the desktop by 1) implementing a simple, time-based notebook metaphor for managing clutter, 2) capturing and bundling extensible metadata for web resources, and 3) providing a platform for greater interface uniformity across sites.",0,6.19047619048
UIST,0ef0039df0f12c0e75c71aad71caef0f26ad8cf4,UIST,2012,Clui: a platform for handles to rich objects,"Hubert Pham, Justin Mazzola Paluska, Rob Miller, Steve Ward","2095906, 1737421, 1723785, 3190843","On the desktop, users are accustomed to having visible handles to objects that they want to organize, share, or manipulate. Web applications today feature many classes of such objects, like flight itineraries, products for sale, people, recipes, and businesses, but there are no interoperable handles for high-level semantic objects that users can grab. This paper proposes Clui, a platform for exploring a new data type, called a Webit, that provides uniform handles to rich objects. Clui uses plugins to 1) create Webits on existing pages by extracting semantic data from those pages, and 2) augmenting existing sites with drag and drop targets that accept and interpret Webits. Users drag and drop Webits between sites to transfer data, auto-fill search forms, map associated locations, or share Webits with others. Clui enables experimentation with handles to semantic objects and the standards that underlie them.",3,40.1960784314
UIST,dcbfbf57514b29b6a2f3cf0a103060ed43cf13e5,UIST,2013,A cluster information navigate method by gaze tracking,"Dawei Cheng, Danqiong Li, Liang Fang","2476347, 2674538, 1802551","According to the rapid growth of data volume, it's increasingly complicated to present and navigate large amount of data in a convenient method on mobile devices with a small screen. To address this challenge, we present a new method which displays cluster information in a hierarchy pattern and interact with them by eyes' movement captured by the front camera of mobile devices. The key of this system is providing users a new interacting method to navigate and select data quickly by eyes without any additional equipment.",0,10.5504587156
UIST,09fe91c27d40776e41aba0e4aeb5a041d7b71daa,UIST,2012,sleepyWhispers: sharing goodnights within distant relationships,"Daniel Gooch, Leon Adam Watts","1718976, 2802623","There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of a prototype device intended to allow distant lovers to share goodnight messages. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that voice remains an under-utilised media when designing interactive technologies for long-distant couples. Through exploring the results of a 2-month case study we present some of the unique challenges that using voice entails.",1,25.0
UIST,29be892f61c2f02e72266fbe6194ba123260e288,UIST,2011,MUSE: reviving memories using email archives,"Sudheendra Hangal, Monica S. Lam, Jeffrey Heer","2589414, 1711151, 1803140","Email archives silently record our actions and thoughts over the years, forming a passively acquired and detailed life-log that contains rich material for reminiscing on our lives. However, exploratory browsing of archives containing thousands of messages is tedious without effective ways to guide the user towards interesting events and messages. We present Muse (Memories USing Email), a system that combines data mining techniques and an interactive interface to help users browse a long-term email archive. Muse analyzes the contents of the archive and generates a set of cues that help to spark users' memories: communication activity with inferred social groups, a summary of recurring named entities, occurrence of sentimental words, and image attachments. These cues serve as salient entry points into a browsing interface that enables faceted navigation and rapid skimming of email messages. In our user studies, we found that users generally enjoyed browsing their archives with Muse, and extracted a range of benefits, from summarizing work progress to renewing friendships and making serendipitous discoveries.",24,70.9523809524
UIST,0781fd69dd08ed17d03b8e111e9a0fdf56c71a53,UIST,2012,"YourGloves, hothands and hotmits: devices to hold hands at a distance","Daniel Gooch, Leon Adam Watts","1718976, 2802623","There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of hand-holding prototypes. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that the combined evocative power of unique co-created physical representations of the absent other can be used by separated lovers to generate powerful and positive experiences, in turn sustaining romantic connections at a distance.",10,58.8235294118
UIST,c982b915322333970cdc650b7cdb8d98d04afbc6,UIST,2016,3D Printed Physical Interfaces that can Extend Touch Devices,"Kunihiro Kato, Homei Miyashita","2211261, 1796760","We propose a method to create a physical interface that allows touch input to be transferred from an external surface attached to a touch panel. Our technique prints a grid having multiple conductive points using an FDM-based 3D printer. When the user touches the conductive points, touch input is generated. This allows the user to control the touch input at arbitrary locations on an XY plane. By arranging the structure of the conductive wiring inside a physical object, a variety of interfaces can be realized.",0,44.6540880503
UIST,b02d41e730a69a64c2d5221dc16bd5e99c2958f6,UIST,2015,Creating a Mobile Head-mounted Display with Proprietary Controllers for Interactive Virtual Reality Content,"Kunihiro Kato, Homei Miyashita","2211261, 1796760","A method to create a mobile head-mounted display (HMD) a proprietary controller for interactive virtual reality (VR) content is proposed. The proposed method uses an interface cartridge printed with a conductive pattern. This allows the user to operate a smartphone by touching on the face of the mobile HMD. In addition, the user can easily create a mobile HMD and interface cartridge using a laser cutter and inkjet printer. Changing the form of the conductive pattern allows the user to create a variety of controllers. The proposed method can realize an environment that can deliver a variety of interactions with VR content.",0,16.2280701754
UIST,1675053183f5faaadcbfb2a0f2e4bd838e28f868,UIST,2014,Extension sticker: a method for transferring external touch input using a striped pattern sticker,"Kunihiro Kato, Homei Miyashita","2211261, 1796760","A method for transferring external touch input is proposed by partially attaching a sticker to a touch-panel display. The touch input area can be extended by printing striped patterns using a conductive ink and attaching them to overlap with a portion of a touch-panel display. Even if the user does not touch the touch panel directly, a touch event can be generated by touching the stripes at an arbitrary point corresponding to the touched area. Thus, continuous touch input can be generated, such as a scrolling operation without interruption. This method can be applied to a variety of devices including PCs, smartphones, and wearable devices. In this paper, we present several different examples of applications, including a method for extending control areas outside of the touch panel, such as the side or back of a smartphone.",1,31.007751938
UIST,cca4f8323d8765c7517b14e9d7ec4ee3330268ce,UIST,2015,MetaSpace: Full-body Tracking for Immersive Multiperson Virtual Reality,"Misha Sra, Chris Schmandt","3024298, 1729321","Most current virtual reality (VR) interactions are mediated by hand-held input devices or hand gestures and they usually display only a partial representation of the user in the synthetic environment. We believe, representing the user as a full avatar that is controlled by natural movements of the person in the real world will lead to a greater sense of presence in VR. Possible applications exist in various domains such as entertainment, therapy, travel, real estate, education, social interaction and professional assistance. In this demo, we present MetaSpace, a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user's body is represented by an avatar that is dynamically controlled by their body movements. We achieve this by tracking each user's body with a Kinect device such that their physical movements are mirrored in the virtual world. Users can see their own avatar and the other person's avatar allowing them to perceive and act intuitively in the virtual environment.",2,60.5263157895
UIST,5ac041afbc600258905777eec8f5f0284d510ae6,UIST,2015,WearWrite: Orchestrating the Crowd to Complete Complex Tasks from Wearables,"Michael Nebeling, Anhong Guo, Alexandra To, Steven Dow, Jaime Teevan, Jeffrey P. Bigham","2262595, 2582404, 2097573, 5319364, 1691357, 1744846","Smartwatches are becoming increasingly powerful, but limited input makes completing complex tasks impractical. Our WearWrite system introduces a new paradigm for enabling a watch user to contribute to complex tasks, not through new hardware or input methods, but by directing a crowd to work on their behalf from their wearable device. WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch, while crowd workers actually write the document on more powerful devices. We used this approach to write three academic papers, and found it was effective at producing reasonable drafts.",1,42.1052631579
UIST,5949116e0c551cb9bf2ee45343ac493d7d86319f,UIST,2012,Real-time captioning by groups of non-experts,"Walter S. Lasecki, Christopher D. Miller, Adam Sadilek, Andrew Abumoussa, Donato Borrello, Raja S. Kushalnagar, Jeffrey P. Bigham","2598433, 2598184, 1743087, 7940323, 3169100, 1683550, 1744846","Real-time captioning provides deaf and hard of hearing people immediate access to spoken language and enables participation in dialogue with others. Low latency is critical because it allows speech to be paired with relevant visual cues. Currently, the only reliable source of real-time captions are expensive stenographers who must be recruited in advance and who are trained to use specialized keyboards. Automatic speech recognition (ASR) is less expensive and available on-demand, but its low accuracy, high noise sensitivity, and need for training beforehand render it unusable in real-world situations. In this paper, we introduce a new approach in which groups of non-expert captionists (people who can hear and type) collectively caption speech in real-time on-demand. We present Legion:Scribe, an end-to-end system that allows deaf people to request captions at any time. We introduce an algorithm for merging partial captions into a single output stream in real-time, and a captioning interface designed to encourage coverage of the entire audio stream. Evaluation with 20 local participants and 18 crowd workers shows that non-experts can provide an effective solution for captioning, accurately covering an average of 93.2% of an audio stream with only 10 workers and an average per-word latency of 2.9 seconds. More generally, our model in which multiple workers contribute partial inputs that are automatically merged in real-time may be extended to allow dynamic groups to surpass constituent individuals (even experts) on a variety of human performance tasks.",96,99.0196078431
UIST,3782ef9818702e450b15a776e2075fbf4ab6e0e6,UIST,2016,Telescope: Fine-Tuned Discovery of Interactive Web UI Feature Implementation,"Joshua Hibschman, Haoqi Zhang","2015651, 3162562","Professional websites contain rich interactive features that developers can learn from, yet understanding their implementation remains a challenge due to the nature of unfamiliar code. Existing tools provide affordances to analyze source code, but feature-rich websites reveal tens of thousands of lines of code and can easily overwhelm the user. We thus present <i>Telescope</i>, a platform for discovering how JavaScript and HTML support a website interaction. Telescope helps users understand unfamiliar website code through a composite view they control by adjusting JavaScript detail, scoping the runtime timeline, and triggering relational links between JS, HTML, and website components. To support these affordances on the open web, Telescope instruments the JavaScript in a website without request intercepts using a novel <i>sleight-of-hand</i> technique, then watches for traces emitted from the website. In a case study across seven popular websites, Telescope helped identify less than 150 lines of front-end code out of tens of thousands that accurately describe the desired interaction in six of the sites. In an exploratory user study, we observed users identifying difficult programming concepts by developing strategies to analyze relatively small amounts of unfamiliar website source code with Telescope.",0,44.6540880503
UIST,25426f1ad13da4da5b030eec125c6e11b7e1be19,UIST,2016,Ballumiere: Real-Time Tracking and Projection for High-Speed Moving Balls,"Shio Miyafuji, Masato Sugasaki, Hideki Koike","2784800, 3493198, 1684942","Projection onto moving objects has a serious slipping problem due to delay between tracking and projection. We propose a new method to overcome the delay problem, and we succeed in increasing the accuracy of projection. We present Ballumiere as a demo for projection to volleyballs and juggling balls.",0,44.6540880503
UIST,6c6e030eccf4184756888a4314176e23e60e40bc,UIST,2011,Artisanship training using wearable egocentric display,"Atsushi Hiyama, Yusuke Doyama, Mariko Miyashita, Eikan Ebuchi, Masazumi Seki, Michitaka Hirose","1760454, 2214008, 1835382, 3281697, 3341799, 1709460","In recent years, most of traditional artisanship is declining because of aging skilled artisan and fewer successors. Therefore, methods for digital archiving of such traditional artisanship are needed. We have constructed a wearable skill-training interface that displays egocentric visual and audio information and muscle activities of an artisan. We used acceleration data of an instrument associated with the usage of the tools for evaluating the effect of proposed wearable display system. This paper introduces the concept and development of wearable egocentric display. Then briefly reports the application results in Kamisuki, Japanese traditional papermaking.",0,6.19047619048
UIST,12035ab9a62de581cd0c3af7aec76698f216120f,UIST,2015,Explaining Visual Changes in Web Interfaces,"Brian Burg, Andrew Jensen Ko, Michael D. Ernst","1687998, 3314595, 6698059","Web developers often want to repurpose interactive behaviors from third-party web pages, but struggle to locate the specific source code that implements the behavior. This task is challenging because developers must find and connect all of the non-local interactions between event-based JavaScript code, declarative CSS styles, and web page content that combine to express the behavior.
 The Scry tool embodies a new approach to locating the code that implements interactive behaviors. A developer selects a page element; whenever the element changes, Scry captures the rendering engine's inputs (DOM, CSS) and outputs (screenshot) for the element. For any two captured element states, Scry can compute how the states differ and which lines of JavaScript code were responsible. Using Scry, a developer can locate an interactive behavior's implementation by picking two output states; Scry indicates the JavaScript code directly responsible for their differences.",2,60.5263157895
UIST,50fd5c6ceee00a98023f696e851f90d608e691f3,UIST,2000,Fluid sketches: continuous recognition and morphing of simple hand-drawn shapes,"James Arvo, Kevin L. Novins","1739434, 2169548","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT We describe a new sketching interface in which shape recognition and morphing are tightly coupled. Raw input strokes are continuously morphed into ideal geometric shapes, even before the pen is lifted. By means of smooth and continual shape transformations the user is apprised of recognition progress and the appearance of the final shape, yet always retains a sense of control over the process. At each time t the system uses the trajectory traced out thus far by the pen coupled with the current appearance of the time-varying shape to classify the sketch as one of several pre-defined basic shapes. The recognition operation is performed using shape-specific fits based on least-squares or relaxation, which are continuously updated as the user draws. We describe the time-dependent transformation of the sketch, beginning with the raw pen trajectory, using a family of first-order ordinary differential equations that depend on time and the current shape of the sketch. Using this formalism, we describe several possible behaviors that result by varying the relative significance of new and old portions of a stroke, changing the "" viscosity "" of the morph, and enforcing different end conditions. A preliminary user study suggests that the new interface is particularly effective for rapidly constructing diagrams consisting of simple shapes.",61,56.0
UIST,56010df889d80a80e95eed9a3ab3600a395516ef,UIST,2006,WindowScape: a task oriented window manager,Craig S. Tashman,2437287,"We propose WindowScape, a window manager that uses a photograph metaphor for lightweight, <i>post hoc</i> task management. This is the first task management windowing model to provide intuitive accessibility while allowing windows to exist simultaneously in multiple tasks. WindowScape exploits users' spatial and visual memories by providing a stable thumbnail layout in which to search for windows. A function is provided to let users search the window space while maintaining a largely consistent screen image to minimize distractions. A novel keyboard interaction technique is also presented.",15,15.0
UIST,0a3269714b8975a0359ae3f610617cbf6e2927c3,UIST,2004,The IBar: a perspective-based camera widget,"Karan Singh, Cindy Grimm, Nisha Sudarsanam","1682205, 1805157, 1770824","We present a new screen space widget, the IBar, for effective camera control in 3D graphics environments. The IBar provides a compelling interface for controlling scene perspective based on the artistic concept of vanishing points. Various handles on the widget manipulate multiple camera parameters simultaneously to create a single perceived projection change. For example, changing just the perspective distortion is accomplished by simultaneously decreasing the camera's distance to the scene while increasing focal length. We demonstrate that the IBar is easier to learn for novice users and improves their understanding of camera perspective.",17,23.6842105263
UIST,8488a1f4fa734332ea9ddfdb985ec06291e1c6eb,UIST,2009,Using fNIRS brain sensing in realistic HCI settings: experiments and guidelines,"Erin Treacy Solovey, Audrey Girouard, Krysta Chauncey, Leanne M. Hirshfield, Angelo Sassaroli, Feng Zheng, Sergio Fantini, Robert J. K. Jacob","1751638, 2320954, 3273840, 3019090, 2607178, 1724559, 1731407, 1723792","Because functional near-infrared spectroscopy (fNIRS) eases many of the restrictions of other brain sensors, it has potential to open up new possibilities for HCI research. From our experience using fNIRS technology for HCI, we identify several considerations and provide guidelines for using fNIRS in realistic HCI laboratory settings. We empirically examine whether typical human behavior (e.g. head and facial movement) or computer interaction (e.g. keyboard and mouse usage) interfere with brain measurement using fNIRS. Based on the results of our study, we establish which physical behaviors inherent in computer usage interfere with accurate fNIRS sensing of cognitive state information, which can be corrected in data analysis, and which are acceptable. With these findings, we hope to facilitate further adoption of fNIRS brain sensing technology in HCI research.",48,68.5714285714
UIST,e8f5672373ce9aa58864a4501a05c2b17b775afd,UIST,2013,Interactive record/replay for web application debugging,"Brian Burg, Richard Bailey, Andrew Jensen Ko, Michael D. Ernst","1687998, 8125272, 3314595, 6698059","During debugging, a developer must repeatedly and manually reproduce faulty behavior in order to inspect different facets of the program's execution. Existing tools for reproducing such behaviors prevent the use of debugging aids such as breakpoints and logging, and are not designed for interactive, random-access exploration of recorded behavior. This paper presents Timelapse, a tool for quickly recording, reproducing, and debugging interactive behaviors in web applications. Developers can use Timelapse to browse, visualize, and seek within recorded program executions while simultaneously using familiar debugging tools such as breakpoints and logging. Testers and end-users can use Timelapse to demonstrate failures in situ and share recorded behaviors with developers, improving bug report quality by obviating the need for detailed reproduction steps. Timelapse is built on Dolos, a novel record/replay infrastructure that ensures deterministic execution by capturing and reusing program inputs both from the user and from external sources such as the network. Dolos introduces negligible overhead and does not interfere with breakpoints and logging. In a small user evaluation, participants used Timelapse to accelerate existing reproduction activities, but were not significantly faster or more successful in completing the larger tasks at hand. Together, the Dolos infrastructure and Timelapse developer tool support systematic bug reporting and debugging practices.",34,94.495412844
UIST,5d21fa032ab2a53eb26b6abd3a9ba627814c5bb9,UIST,2004,Physical user interfaces: what they are and how to build them,Saul Greenberg,1696942,"Physical user interfaces are special purpose devices that can be situated in a real-world setting. Unlike general purpose computers, they are typically designed for particular contexts and uses. In this survey, I present an introductory tour of this new interface genre. First, I will summarize what they are by describing several design niches for these devices: ubiquitous computing, tangible media, foreground and ambient devices, collaborative devices, roomware, and physical controls. Examples will be plentiful, and will range from the playful, to the artistic, and to the serious. Second, I will introduce technologies that are suitable for software professionals who wish to prototype these physical user interfaces. The commercially available Phidgets (www.phidgets.com) are used as a case study of what is available and what can be done with them.",1,5.26315789474
UIST,c3bf89c7154a97b4d6119e7c7bc149269f05a9c8,UIST,2002,Customizable physical interfaces for interacting with conventional applications,"Saul Greenberg, Michael Boyle","1696942, 1834358","When using today's productivity applications, people rely heavily on graphical controls (GUI widgets) as the way to invoke application functions and to obtain feedback. Yet we all know that certain controls can be difficult or tedious to find and use. As an alternative, a <i>customizable physical interface</i> lets an end-user easily bind a modest number of physical controls to similar graphical counterparts. The user can then use the physical control to invoke the corresponding graphical control's function, or to display its graphical state in a physical form. To show how customizable physical interfaces work, we present examples that illustrate how our combined <i>phidgets</i><sup>&#174;</sup> and <i>widget tap</i> packages are used to link existing application widgets to physical controls. While promising, our implementation prompts a number of issues relevant to others pursuing interface customization.",45,58.3333333333
UIST,68f6c9a89905f5765f541646d4e3202316bafbfc,UIST,2013,Foreign manga reader: learn grammar and pronunciation while reading comics,"Geza Kovacs, Rob Miller","1776950, 1723785","Foreign-language comics are potentially an enjoyable way to learn foreign languages. However, the difficulty of reading authentic material makes them inaccessible to novice learners. We present the Foreign Manga Reader, a system that helps readers comprehend foreign-language written materials and learn multiple aspects of the language. Specifically, it generates a sentence-structure visualization to help learners understand the grammar, pronounces dialogs to improve listening comprehension and pronunciation, and translates dialogs, phrases, and words to teach vocabulary. Learners can use the system to match their experience level, giving novices access to dialog-level translations and pronunciations, and more advanced learners with access to information at the level of phrases and individual words. The annotations are automatically generated, and can be used with arbitrary written materials in several languages. A preliminary study suggests that learners find our system useful for understanding and learning from authentic foreign-language material.",1,27.0642201835
UIST,2bc4b67b7eb8d44dad99419fec0ae23547310474,UIST,2015,Remot-IO: a System for Reaching into the Environment of a Remote Collaborator,"Xavier Benavides, Judith Amores, Pattie Maes","2838190, 2233123, 1701876","In this paper we present Remot-IO, a system for mobile collaboration and remote assistance around Internet connected devices. The system uses two Head Mounted Displays, cameras and depth sensors to enable a remote expert to be immersed in a local user's point of view and control devices in that user?s environment. The remote expert can provide guidance through the use of hand gestures that appear in real-time in the local user?s field of view as superimposed 3D hands. In addition, the remote expert is able to operate devices in the novice?s environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote user alike. Moreover, the user can visualize, interact and modify properties of sound waves in real time by using intuitive hand gestures.",0,16.2280701754
UIST,40a4bbd725cd4da36d19f6a95d069f53c5f3d3a0,UIST,2011,Access overlays: improving non-visual access to large touch screens for blind users,"Shaun K. Kane, Meredith Ringel Morris, Annuska Z. Perkins, Daniel J. Wigdor, Richard E. Ladner, Jacob O. Wobbrock","2893996, 1741255, 1996570, 1961958, 1762656, 1796045","Many touch screens remain inaccessible to blind users, and those approaches to providing access that do exist offer minimal support for interacting with large touch screens or spatial data. In this paper, we introduce a set of three software-based access overlays intended to improve the accessibility of large touch screen interfaces, specifically interactive tabletops. Our access overlays are called edge projection, neighborhood browsing, and touch-and-speak. In a user study, 14 blind users compared access overlays to an implementation of Apple's VoiceOver screen reader. Our results show that two of our techniques were faster than VoiceOver, that participants correctly answered more questions about the screen's layout using our techniques, and that participants overwhelmingly preferred our techniques. We developed several applications demonstrating the use of access overlays, including an accessible map kiosk and an accessible board game.",36,82.8571428571
UIST,1cd7b67febbf62feb614daa4f52bde8d3b4a462e,UIST,2005,PlayAnywhere: a compact interactive tabletop projection-vision system,Andrew D. Wilson,1767449,"We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.",199,96.7741935484
UIST,143b946922bf1ab3ec4e1f2d694ae89b723a463b,UIST,2000,TopicShop: enhanced support for evaluating and organizing collections of Web sites,"Brian Amento, Loren G. Terveen, William C. Hill, Deborah Hix","1730851, 7456007, 1720167, 1715243","TopicShop is an interface that helps users evaluate and organize collections of web sites. The main interface components are site profiles, which contain information that helps users select high-quality items, and a work area, which offers thumbnail images, annotation, and lightweight grouping techniques to help users organize selected sites. The two components are linked to allow task integration. Previous work [2] demonstrated that subjects who used TopicShop were able to select significantly more high-quality sites, in less time and with less effort. We report here on studies that confirm and extend these results. We also show that TopicShop subjects spent just half the time organizing sites, yet still created more groups and more annotations, and agreed more in how they grouped sites. Finally, TopicShop subjects tightly integrated the tasks of evaluating and organizing sites.",17,24.0
UIST,63d9d0b454b8ee56e201b8ff65a8105879d8d115,UIST,2007,Rethinking the progress bar,"Chris Harrison, Brian Amento, Stacey Kuznetsov, Robert Bell","1730920, 1730851, 2563615, 5017912","Progress bars are prevalent in modern user interfaces. Typically, a linear function is employed such that the progress of the bar is directly proportional to how much work has been completed. However, numerous factors cause progress bars to proceed at non-linear rates. Additionally, humans perceive time in a non-linear way. This paper explores the impact of various progress bar behaviors on user perception of process duration. The results are used to suggest several design considerations that can make progress bars appear faster and ultimately improve users' computing experience.",29,55.5555555556
UIST,3bad297643c71cbd6bb20104130dfbfd67c670e0,UIST,2000,Illusions of infinity: feedback for infinite worlds,"George W. Furnas, Xiaolong Zhang","2937904, 1799079","Sensory feedback for user actions in arbitrarily large information words can exhaust the limited dynamic range of human sensation. Two well-known illusions, one optical and one auditory, can be used to give arbitrarily large ranges of feedback.",7,12.0
UIST,50e22666167640c84170a585ac6f7f1a28974851,UIST,2011,A design space analysis of availability-sharing systems,"Juan David Hincapié-Ramos, Stephen Voida, Gloria Mark","1797275, 1804450, 1695284","Workplace collaboration often requires interruptions, which can happen at inopportune times. Designing a successful availability-sharing system requires finding the right balance to optimize the benefits and reduce costs for both the interrupter and interruptee. In this paper, we examine the design space of availability-sharing systems and identify six relevant design dimensions: abstraction, presentation, information delivery, symmetry, obtrusiveness and temporal gradient. We describe these dimensions in terms of the tensions between interrupters and interruptees revealed in previous studies of workplace collaboration and deployments of awareness systems. As a demonstration of the utility of our design space, we introduce InterruptMe, a novel availability-sharing system that represents a previously unexplored point in the design space and that balances the tensions between interrupters and interruptees. InterruptMe differs from previous systems in that it displays availability information only when needed by monitoring implicit inputs from the system's users, implements a traceable asymmetry structure, and introduces the notion of per-communications channel availability.",6,38.5714285714
UIST,87901fdebf38cd7265c31333370c4d3fc3bc1bbd,UIST,2016,AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing,"Junhan Zhou, Yang Zhang, Gierad Laput, Chris Harrison","3396837, 4449904, 1727999, 1730920","Existing smartwatches rely on touchscreens for display and input, which inevitably leads to finger occlusion and confines interactivity to a small area. In this work, we introduce AuraSense, which enables rich, around-device, smartwatch interactions using electric field sensing as an adapted device. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above or in close proximity to watches, and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities, suggesting that AuraSense can be low latency and robust across users and environments.",0,44.6540880503
UIST,06ac6751e1a4f3dfc12bcc18132476fde2736001,UIST,2011,6D hands: markerless hand-tracking for computer aided design,"Robert Y. Wang, Sylvain Paris, Jovan Popovic","1692433, 1720990, 1731389","Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.",69,93.3333333333
UIST,0dcdffb798733a275f4046d9cb53253f97971504,UIST,2008,Re-framing the desktop interface around the activities of knowledge work,"Stephen Voida, Elizabeth D. Mynatt, W. Keith Edwards","1804450, 1752751, 3023379","The venerable desktop metaphor is beginning to show signs of strain in supporting modern knowledge work. In this paper, we examine how the desktop metaphor can be re-framed, shifting the focus away from a low-level (and increasingly obsolete) focus on documents and applications to an interface based upon the creation of and interaction with manually declared, semantically meaningful activities. We begin by unpacking some of the foundational assumptions of desktop interface design, describe an activity-based model for organizing the desktop interface based on theories of cognition and observations of real-world practice, and identify a series of high-level system requirements for interfaces that use activity as their primary organizing principle. Based on these requirements, we present the novel interface design of the Giornata system, a prototype activity-based desktop interface, and share initial findings from a longitudinal deployment of the Giornata system in a real-world setting.",19,41.4285714286
UIST,12540e76907f0613a60f0e87859417b83a467bfb,UIST,2010,Mixture model based label association techniques for web accessibility,"Muhammad Asiful Islam, Yevgen Borodin, I. V. Ramakrishnan","3020758, 2133004, 1762197","An important aspect of making the Web accessible to blind users is ensuring that all important web page elements such as links, clickable buttons, and form fields have explicitly assigned labels. Properly labeled content is then correctly read out by screen readers, a dominant assistive technology used by blind users. In particular, improperly labeled form fields can critically impede online transactions such as shopping, paying bills, etc. with screen readers. Very often labels are not associated with form fields or are missing altogether, making form filling a challenge for blind users. Algorithms for associating a form element with one of several candidate labels in its vicinity must cope with the variability of the element's features including label's location relative to the element, distance to the element, etc. Probabilistic models provide a natural machinery to reason with such uncertainties. In this paper we present a <i>Finite Mixture Model (FMM</i>) formulation of the label association problem. The variability of feature values are captured in the FMM by a mixture of random variables that are drawn from parameterized distributions. Then, the most likely label to be paired with a form element is computed by maximizing the log-likelihood of the feature data using the Expectation-Maximization algorithm. We also adapt the FMM approach for two related problems: assigning labels (from an external Knowledge Base) to form elements that have no candidate labels in their vicinity and for quickly identifying clickable elements such as add-to-cart, checkout, etc., used in online transactions even when these elements do not have textual captions (e.g., image buttons w/o alternative text). We provide a quantitative evaluation of our techniques, as well as a user study with two blind subjects who used an aural web browser implementing our approach.",4,48.2558139535
UIST,2370006bd6289b693e9854882d68806c6e4c9407,UIST,2002,Augmenting shared personal calendars,"Joe Tullio, Jeremy Goecks, Elizabeth D. Mynatt, David H. Nguyen","3126077, 2718156, 1752751, 1767024","In this paper, we describe Augur, a groupware calendar system to support personal calendaring practices, informal workplace communication, and the socio-technical evolution of the calendar system within a workgroup. Successful design and deployment of groupware calendar systems have been shown to depend on several converging, interacting perspectives. We describe calendar-based work practices as viewed from these perspectives, and present the Augur system in support of them. Augur allows users to retain the flexibility of personal calendars by anticipating and compensating for inaccurate calendar entries and idiosyncratic event names. We employ predictive user models of event attendance, intelligent processing of calendar text, and discovery of shared events to drive novel calendar visualizations that facilitate interpersonal communication. In addition, we visualize calendar access to support privacy management and long-term evolution of the calendar system.",34,45.8333333333
UIST,1a14fa8db325c850265ffa04044a0062cfdd5f3c,UIST,2013,DemoCut: generating concise instructional videos for physical demonstrations,"Pei-Yu Chi, Joyce Liu, Jason Linder, Mira Dontcheva, Wilmot Li, Björn Hartmann","2442421, 2149988, 1884110, 2875493, 2812691, 4020023","Amateur instructional videos often show a single uninterrupted take of a recorded demonstration without any edits. While easy to produce, such videos are often too long as they include unnecessary or repetitive actions as well as mistakes. We introduce DemoCut, a semi-automatic video editing system that improves the quality of amateur instructional videos for physical tasks. DemoCut asks users to mark key moments in a recorded demonstration using a set of marker types derived from our formative study. Based on these markers, the system uses audio and video analysis to automatically organize the video into meaningful segments and apply appropriate video editing effects. To understand the effectiveness of DemoCut, we report a technical evaluation of seven video tutorials created with DemoCut. In a separate user evaluation, all eight participants successfully created a complete tutorial with a variety of video editing effects using our system.",14,77.5229357798
UIST,7dbbf5564e254c6f4027043368f2df22d38ea25e,UIST,2010,Creating collections with automatic suggestions and example-based refinement,"Adrian Secord, Holger Winnemöller, Wilmot Li, Mira Dontcheva","2821128, 2168852, 2812691, 2875493","To create collections, like music playlists from personal media libraries, users today typically do one of two things. They either manually select items one-by-one, which can be time consuming, or they use an example-based recommendation system to automatically generate a collection. While such automatic engines are convenient, they offer the user limited control over how items are selected. Based on prior research and our own observations of existing practices, we propose a semi-automatic interface for creating collections that combines automatic suggestions with manual refinement tools. Our system includes a keyword query interface for specifying high-level collection preferences (e.g., ""some rock, no Madonna, lots of U2,"") as well as three example-based collection refinement techniques: 1) a <i>suggestion widget</i> for adding new items in-place in the context of the collection; 2) a mechanism for exploring alternatives for one or more collection items; and 3) a two-pane linked interface that helps users browse their libraries based on any selected collection item. We demonstrate our approach with two applications. <i>SongSelect</i> helps users create music playlists, and <i>PhotoSelect</i> helps users select photos for sharing. Initial user feedback is positive and confirms the need for semi-automated tools that give users control over automatically created collections.",2,33.1395348837
UIST,cb09f737c91f93141071a8c923e7a021780ffa94,UIST,2006,"Multi-user, multi-display interaction with a single-user, single-display geospatial application","Clifton Forlines, Alan Esenther, Chia Shen, Daniel J. Wigdor, Kathy Ryall","1694854, 2802986, 1697008, 1961958, 1712855","In this paper, we discuss our adaptation of a single-display, single-user commercial application for use in a multi-device, multi-user environment. We wrap Google Earth, a popular geospatial application, in a manner that allows for synchronized coordinated views among multiple instances running on different machines in the same co-located environment. The environment includes a touch-sensitive tabletop display, three vertical wall displays, and a TabletPC. A set of interaction techniques that allow a group to manage and exploit this collection of devices is presented.",43,68.75
UIST,b49d1d1024c91a752b288872344d790fcfc6e812,UIST,2004,Olfactory display,Joseph Kaye,1725629,"The last twenty years have seen enormous leaps forward in computers' abilities to generate sound and video. What happens when computers can produce scents on demand? In this talk, I present three approaches to this question. I first look at human olfactory processing: what is our olfactory bandwidth, and what are the limitations of our sense of smell? I then explore the use of scent to accompany other media, from historical examples like Sense-o-Rama and Aromarama, to more recent work including firefighter training systems, augmented gaming, and food and beverage applications. Finally, I look at the possibilities of olfactory output as an ambient display medium. I conclude with an overview of current computer-controlled olfactory output devices: off the shelf solutions for incorporating scent into user interface applications.",1,5.26315789474
UIST,5ead24d7541f08e55f43c153963101124d4460b5,UIST,2011,Gesture keyboard requiring only one camera,"Taichi Murase, Atsunori Moteki, Noriaki Ozawa, Nobuyuki Hara, Takehiro Nakai, Katsuhito Fujimoto","3052077, 2130818, 2445387, 2482162, 1884147, 2646558","In this paper, we propose a novel gesture-based virtual keyboard (Gesture Keyboard) of QWERTY key layout requiring only one camera. Gesture Keyboard tracks the user's fingers and recognizes gestures as the input, and each virtual key of it follows a corresponding finger. Therefore, it is possible to input characters at the user's preferred hand position even if displacing hands during inputting. Because Gesture Keyboard requires only one camera to obtain sensor information, keyboard-less devices can feature it easily.",5,35.7142857143
UIST,10a36561e69ba91b3f55187d46eff0316d4d8d9c,UIST,2002,"Side views: persistent, on-demand previews for open-ended tasks","Michael A. Terry, Elizabeth D. Mynatt","1740154, 1752751","We introduce Side Views, a user interface mechanism that provides on-demand, persistent, and dynamic previews of commands. Side Views are designed to explicitly support the practices and needs of expert users engaged in openended tasks. In this paper, we summarize results from field studies of expert users that motivated this work, then discuss the design of Side Views in detail. We show how Side Views' design affords their use as tools for clarifying, comparing, and contrasting commands; generating alternative visualizations; experimenting without modifying the original data (i.e., ""what-if"" tools); and as tools that support the serendipitous discovery of viable alternatives. We then convey lessons learned from implementing Side Views in two sample applications, a rich text editor and an image manipulation application. These contributions include a discussion of how to implement Side Views for commands with parameters, for commands that require direct user input (such as mouse strokes for a paint program), and for computationally-intensive commands.",54,66.6666666667
UIST,200b399c39cfe64d6d620c34f9c2c02de121380c,UIST,2000,A temporal model for multi-level undo and redo,"W. Keith Edwards, Takeo Igarashi, Anthony LaMarca, Elizabeth D. Mynatt","3023379, 1717356, 7871341, 1752751","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. divergent and convergent timelines as a way to support collaboration, and provides facilities for conflict resolution. Even in systems without such rich models of history, time is often an explicit—and directly manipulable—part of the user interface, and user experience. Systems such as Time-Machine Computing [15] and Lifestreams [6] are exemplars of this trend. All of these systems rely on an explicit model of history, which can be scanned to support search or "" navigation "" over a timeline, and all allow their timelines to be "" traversed "" to move the application's state to other points in its history. However, as powerful as these applications are, their timeline representations are for the most part exceedingly simple. They typically support only linear, not branching timelines (GINA and Timewarp are exceptions, however); the "" nodes "" in a timeline must represent atomic operations with side effects that are well understood at the time the application is created; and, typically, the timeline of the entire application must be navigated or traversed as a whole—it is impossible to have a portion of the timeline exist in a "" bubble "" that can be manipulated separately. While we don't commonly encounter such rich models of time in our day to day experience, they can be extremely useful nonetheless. Divergent timelines, for example, can be employed to allow users to interact with different but related versions of an artifact, and then reconcile those differences later. The ability to expand the representation of time in ways that better accommodate side effects can make applications easier to write. And being able to separate the history of one nested artifact from the history of the application as a whole can allow users to work locally on a document, project source code, et cetera, and still integrate their changes globally. This paper presents an expansion of the most traditional representation of timelines, which is based on the command object idiom. The research here makes two contributions. First it extends this …",26,36.0
UIST,196679763a659ef6296a7664927e880cb46ad8be,UIST,2006,Translating keyword commands into executable code,"Greg Little, Rob Miller","1715840, 1723785","Modern applications provide interfaces for scripting, but many users do not know how to write script commands. However, many users are familiar with the idea of entering keywords into a web search engine. Hence, if a user is familiar with the vocabulary of an application domain, we anticipate that they could write a set of keywords expressing a command in that domain. For instance, in the web browsing domain, a user might enter &lt;B&gt;click search button&lt;/B&gt;. We call expressions of this form <i>keyword commands</i>, and we present a novel approach for translating keyword commands directly into executable code. Our prototype of this system in the web browsing domain translates &lt;B&gt;click search button&lt;/B&gt; into the Chickenfoot code &lt;B&gt;click(findButton(""search""))&lt;/B&gt;. This code is then executed in the context of a web browser to carry out the effect. We also present an implementation of this system in the domain of Microsoft Word. A user study revealed that subjects could use keyword commands to successfully complete 90% of the web browsing tasks in our study without instructions or training. Conversely, we would expect users to complete close to 0% of the tasks if they had to guess the underlying JavaScript commands with no instructions or training.",45,72.5
UIST,dcb1895aa3e6cdda1ed07cf21531acbeaa80ba96,UIST,2011,Designing for effective end-user interaction with machine learning,Saleema Amershi,1719124,"End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond na&#239;ve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.",7,41.9047619048
UIST,0f2ab8e03a32a9e2c5dbd6bdfe28d74fb59eb7eb,UIST,2010,Gestalt: integrated support for implementation and analysis in machine learning,"Kayur Patel, Naomi Bancroft, Steven M. Drucker, James Fogarty, Andrew Jensen Ko, James A. Landay","2760803, 1949091, 2311676, 1738171, 3314595, 1708404","We present Gestalt, a development environment designed to support the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to <i>implement</i> a classification pipeline, <i>analyze</i> data as it moves through that pipeline, and <i>easily transition</i> between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the machine learning process.",34,84.8837209302
UIST,1066e9c048ee1fb540404d3afb2a9a0d8009dd5e,UIST,2009,Bonfire: a nomadic system for hybrid laptop-tabletop interaction,"Shaun K. Kane, Daniel Avrahami, Jacob O. Wobbrock, Beverly L. Harrison, Adam D. Rea, Matthai Philipose, Anthony LaMarca","2893996, 2667384, 1796045, 1768833, 2881919, 3041721, 7871341","We present Bonfire, a self-contained mobile computing system that uses two laptop-mounted laser micro-projectors to project an interactive display space to either side of a laptop keyboard. Coupled with each micro-projector is a camera to enable hand gesture tracking, object recognition, and information transfer within the projected space. Thus, Bonfire is neither a pure laptop system nor a pure tabletop system, but an integration of the two into one new nomadic computing platform. This integration (1) enables observing the periphery and responding appropriately, e.g., to the casual placement of objects within its field of view, (2) enables integration between physical and digital objects via computer vision, (3) provides a horizontal surface in tandem with the usual vertical laptop display, allowing direct pointing and gestures, and (4) enlarges the input/output space to enrich existing applications. We describe Bonfire's architecture, and offer scenarios that highlight Bonfire's advantages. We also include lessons learned and insights for further development and use.",68,84.2857142857
UIST,1dba0d497101ece09393c72a20dcb3aeb8b39e1c,UIST,2010,PhoneTouch: a technique for direct phone interaction on surfaces,"Dominik Schmidt, Fadi Chehimi, Enrico Rukzio, Hans-Werner Gellersen","1722031, 2734179, 2021950, 4919595","PhoneTouch is a novel technique for integration of mobile phones and interactive surfaces. The technique enables use of phones to select targets on the surface by direct touch, facilitating for instance pick&amp;drop-style transfer of objects between phone and surface. The technique is based on separate detection of phone touch events by the surface, which determines location of the touch, and by the phone, which contributes device identity. Our current implementation uses the phones' internal microphones and the table's camera system for touch detection.",55,90.6976744186
UIST,f3f86ef3ab0aaf27fcb9986588b357c5a7c30563,UIST,2016,RealPen: Providing Realism in Handwriting Tasks on Touch Surfaces using Auditory-Tactile Feedback,"Youngjun Cho, Andrea Bianchi, Nicolai Marquardt, Nadia Bianchi-Berthouze","2796715, 4189910, 3328262, 1690885","We present RealPen, an augmented stylus for capacitive tablet screens that recreates the physical sensation of writing on paper with a pencil, ball-point pen or marker pen. The aim is to create a more engaging experience when writing on touch surfaces, such as screens of tablet computers. This is achieved by regenerating the friction-induced oscillation and sound of a real writing tool in contact with paper. To generate realistic tactile feedback, our algorithm analyzes the frequency spectrum of the friction oscillation generated when writing with traditional tools, extracts principal frequencies, and uses the actuator's frequency response profile for an adjustment weighting function. We enhance the realism by providing the sound feedback aligned with the writing pressure and speed. Furthermore, we investigated the effects of superposition and fluctuation of several frequencies on human tactile perception, evaluated the performance of RealPen, and characterized users' perception and preference of each feedback type.",0,44.6540880503
UIST,4354e022e9afeda60829579142d6062d2913a29f,UIST,2001,Support for multitasking and background awareness using interactive peripheral displays,"Blair MacIntyre, Elizabeth D. Mynatt, Stephen Voida, Klaus Marius Hansen, Joe Tullio, Gregory M. Corso","1768774, 1752751, 1804450, 1781815, 3126077, 1863709","In this paper, we describe Kimura, an augmented office environment to support common multitasking practices. Previous systems, such as Rooms, limit users by constraining the interaction to the desktop monitor. In Kimura, we leverage interactive projected peripheral displays to support the perusal, manipulation and awareness of background activities. Furthermore, each activity is represented by a montage comprised of images from current and past interaction on the desktop. These montages help remind the user of past actions, and serve as a springboard for ambient context-aware reminders and notifications.",131,76.6666666667
UIST,4ef9839d135bf6dadc308fa21e3e0e9214165d16,UIST,2010,IODisk: disk-type i/o interface for browsing digital contents,"Koji Tsukada, Keisuke Kambara","1748862, 2339278","We propose a disk-type I/O interface, IODisk, which helps users browse various digital contents intuitively in their living environment. IODisk mainly consists of a forcefeedback mechanism integrated in the rotation axis of a disk. Users can control the playing speed/direction contents (e.g., videos or picture slideshows) in proportion to the rotational speed/direction of the disk. We developed a prototype system and some applications.",1,22.0930232558
UIST,f738291face6688087275887b8f1a8da6e5676f4,UIST,2016,Object-Oriented Interaction: Enabling Direct Physical Manipulation of Abstract Content via Objectification,Haijun Xia,1786964,"Touch input promises intuitive interactions with digital content as it employs our experience of manipulating physical objects: digital content can be rotated, scaled, and translated using direct manipulation gestures. However, the reliance on analog also confines the scope of direct physical manipulation: the physical world provides no mechanism to interact with digital abstract content. As such, applications on touchscreen devices either only include limited functionalities or fallback on the traditional form-filling paradigm, which is tedious, slow, and error prone for touch input. My research focuses on designing a new UI framework to enable complex functionalities on touch screen devices by expanding direct physical manipulation to abstract content via objectification. I present two research projects, objectification of attributes and selection, which demonstrate considerable promises.",0,44.6540880503
UIST,121ebdef949c54c15a65820ac55d1fd49effa8b4,UIST,2015,NanoStylus: Enhancing Input on Ultra-Small Displays with a Finger-Mounted Stylus,"Haijun Xia, Tovi Grossman, George W. Fitzmaurice","1786964, 3313809, 1703735","Due to their limited input area, ultra-small devices, such as smartwatches, are even more prone to occlusion or the fat finger problem, than their larger counterparts, such as smart phones, tablets, and tabletop displays. We present NanoStylus -- a finger-mounted fine-tip stylus that enables fast and accurate pointing on a smartwatch with almost no occlusion. The NanoStylus is built from the circuitry of an active capacitive stylus, and mounted within a custom 3D-printed thimble-shaped housing unit. A sensor strip is mounted on each side of the device to enable additional gestures. A user study shows that NanoStylus reduces error rate by 80%, compared to traditional touch interaction and by 45%, compared to a traditional stylus. This high precision pointing capability, coupled with the implemented gesture sensing, gives us the opportunity to explore a rich set of interactive applications on a smartwatch form factor.",11,98.2456140351
UIST,0cfce8fa4f2cb4b937d87b64d56cb3f354cb0e33,UIST,2009,Augmenting interactive tables with mice & keyboards,"Björn Hartmann, Meredith Ringel Morris, Hrvoje Benko, Andrew D. Wilson","4020023, 1741255, 2704133, 1767449","This note examines the role traditional input devices can play in surface computing. Mice and keyboards can enhance tabletop technologies since they support high fidelity input, facilitate interaction with distant objects, and serve as a proxy for user identity and position. Interactive tabletops, in turn, can enhance the functionality of traditional input devices: they provide spatial sensing, augment devices with co-located visual content, and support connections among a plurality of devices. We introduce eight interaction techniques for a table with mice and keyboards, and we discuss the design space of such interactions.",30,51.4285714286
UIST,eb07e7af792f210c2b033a2e0bc2ecdf68365503,UIST,2010,"Combining multiple depth cameras and projectors for interactions on, above and between surfaces","Andrew D. Wilson, Hrvoje Benko","1767449, 2704133","Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may ""pick up"" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and ""drop"" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.",159,97.6744186047
UIST,da09e12a0a5114107bbc7fbf8bdf0bf68d10b2f9,UIST,2012,CrowdScape: interactively visualizing user behavior and output,"Jeffrey M. Rzeszotarski, Aniket Kittur","2494495, 1717650","Crowdsourcing has become a powerful paradigm for accomplishing work quickly and at scale, but involves significant challenges in quality control. Researchers have developed algorithmic quality control approaches based on either worker outputs (such as gold standards or worker agreement) or worker behavior (such as task fingerprinting), but each approach has serious limitations, especially for complex or creative work. Human evaluation addresses these limitations but does not scale well with increasing numbers of workers. We present CrowdScape, a system that supports the human evaluation of complex crowd work through interactive visualization and mixed initiative machine learning. The system combines information about worker behavior with worker outputs, helping users to better understand and harness the crowd. We describe the system and discuss its utility through grounded case studies. We explore other contexts where CrowdScape's visualizations might be useful, such as in user studies.",24,81.3725490196
UIST,fb3fd0ddc209c9dfe39feff6632605cd91751b06,UIST,2013,H-Studio: an authoring tool for adding haptic and motion effects to audiovisual content,"Fabien Danieau, Jérémie Bernon, Julien Fleureau, Philippe Guillotel, Nicolas Mollet, Marc Christie, Anatole Lécuyer","1834970, 3255924, 3196853, 1871505, 1862487, 1701717, 1693899","Haptic and motion effects have been widely used for virtual reality applications in order to provide a physical feedback from the virtual world. Such feedback was recently studied to improve the user experience in audiovisual entertainment applications. But the creation of haptic and motion effects is a main issue and requires dedicated editing tool. This paper describes a user-friendly authoring tool to create and synchronize such effects with audiovisual content. More precisely we focus on the edition of motion effects. Authoring is simplified thanks to a dedicated graphical user interface, allowing either to import external data or to synthesize effects thanks to a force-feedback device. Another key feature of this editor is the playback function which enables to preview the motion effect. Hence this new tool allows non expert users to create immersive haptic-audiovisual experiences.",0,10.5504587156
UIST,15a091e135b17a27c1c56661e537a8d4a4edbb5d,UIST,2011,Dynamic ambient lighting for mobile devices,"Qian Qin, Michael Rohs, Sven G. Kratz","8542766, 1876551, 3103005","The information a small mobile device can show via its display has been always limited by its size. In large information spaces, relevant information, such as important locations on a map can get clipped when a user starts zooming and panning. Dynamic ambient lighting allows mobile devices to visualize off-screen objects by illuminating the background without compromising valuable display space. The lighted spots can be used to show the direction and distance of such objects by varying the spot's position and intensity. Dynamic ambient lighting also provides a new way of displaying the state of a mobile device. Illumination is provided by a prototype rear of device shell which contains LEDs and requires the device to be placed on a surface, such as a table or desk.",12,51.9047619048
UIST,126ca5896957907b1d20cf8b210f44d192891565,UIST,2015,Candid Interaction: Revealing Hidden Mobile and Wearable Computing Activities,"Barrett Ens, Tovi Grossman, Fraser Anderson, Justin Matejka, George W. Fitzmaurice","2896885, 3313809, 3118963, 2578065, 1703735","The growth of mobile and wearable technologies has made it often difficult to understand what people in our surroundings are doing with their technology. In this paper, we introduce the concept of candid interaction: techniques for providing awareness about our mobile and wearable device usage to others in the vicinity. We motivate and ground this exploration through a survey on current attitudes toward device usage during interpersonal encounters. We then explore a design space for candid interaction through seven prototypes that leverage a wide range of technological enhancements, such as Augmented Reality, shape memory muscle wire, and wearable projection. Preliminary user feedback of our prototypes highlights the trade-offs between the benefits of sharing device activity and the need to protect user privacy.",1,42.1052631579
UIST,b4efc50dd31c7d4fd47f656390d4d85710012d89,UIST,2013,YouMove: enhancing movement training with an augmented reality mirror,"Fraser Anderson, Tovi Grossman, Justin Matejka, George W. Fitzmaurice","3118963, 3313809, 2578065, 1703735","YouMove is a novel system that allows users to record and learn physical movement sequences. The recording system is designed to be simple, allowing anyone to create and share training content. The training system uses recorded data to train the user using a large-scale augmented reality mirror. The system trains the user through a series of stages that gradually reduce the user's reliance on guidance and feedback. This paper discusses the design and implementation of YouMove and its interactive mirror. We also present a user study in which YouMove was shown to improve learning and short-term retention by a factor of 2 compared to a traditional video demonstration.",30,92.2018348624
UIST,48ca535feaebd597a59eb15d07336bef54f46f95,UIST,2008,Sphere: multi-touch interactions on a spherical display,"Hrvoje Benko, Andrew D. Wilson, Ravin Balakrishnan","2704133, 1767449, 1748870","Sphere is a multi-user, multi-touch-sensitive spherical display in which an infrared camera used for touch sensing shares the same optical path with the projector used for the display. This novel configuration permits: (1) the enclosure of both the projection and the sensing mechanism in the base of the device, and (2) easy 360-degree access for multiple users, with a high degree of interactivity without shadowing or occlusion. In addition to the hardware and software solution, we present a set of multi-touch interaction techniques and interface concepts that facilitate collaborative interactions around Sphere. We designed four spherical application concepts and report on several important observations of collaborative activity from our initial Sphere installation in three high-traffic locations.",67,82.8571428571
UIST,5396b3c133ffdfcb9ff151ca5e1af6483549450e,UIST,2016,SleepCoacher: A Personalized Automated Self-Experimentation System for Sleep Recommendations,"Nediyana Daskalova, Danaë Metaxa-Kakavouli, Adrienne Tran, Nicole Nugent, Julie Boergers, John McGeary, Jeff Huang","2277777, 3491735, 3491714, 3491601, 3492242, 7667105, 3404131","We present SleepCoacher, an integrated system implementing a framework for effective self-experiments. SleepCoacher automates the cycle of single-case experiments by collecting raw mobile sensor data and generating personalized, data-driven sleep recommendations based on a collection of template recommendations created with input from clinicians. The system guides users through iterative short experiments to test the effect of recommendations on their sleep. We evaluate SleepCoacher in two studies, measuring the effect of recommendations on the frequency of awakenings, self-reported restfulness, and sleep onset latency, concluding that it is effective: participant sleep improves as adherence with SleepCoacher's recommendations and experiment schedule increases. This approach presents computationally-enhanced interventions leveraging the capacity of a closed feedback loop system, offering a method for scaling guided single-case experiments in real time.",0,44.6540880503
UIST,a4a265591bd6cba2d29776cd47ac6d3cce3e5f5e,UIST,2011,TouchString: a flexible linear multi-touch sensor for prototyping a freeform multi-touch surface,"Jiseong Gu, Geehyuk Lee","2042461, 1717371","We propose the concept of prototyping a multi-touch surface of an arbitrary form using a flexible linear multi-touch sensor that we call TouchString. We defined the conceptual structure of a TouchString, and implemented an example prototype of a TouchString. We verified the feasibility of the concept by demonstrating a few basic application scenarios using the prototype.",2,24.2857142857
UIST,f944c656fe7abc331af05f70b23e2191c6c4d534,UIST,2013,Asymmetric cores for low power user interface systems,"Jaeyeon Kihm, François Guimbretière","1831764, 2539134","In recent years, advances in hardware design have lead to significant improvements in the battery life of everyday information appliances. In particular, application processors increasingly include low power ""helper"" cores dedicated to simpler tasks. Using a custom board design, Guimbreti&#232;re et al. [2], demonstrated that such helper cores can also be used to execute simple user interface tasks. We revisit their approach by implementing a similar system on an off-the-shelf application processor (TI OMAP4), and demonstrate that, in many cases, the gains reported by Guimbreti&#232;re et al. [2], can be achieved by simply having the helper core dispatch input events. This new approach can be implemented by merely changing the toolkit infrastructure, thus greatly simplifying deployment",0,10.5504587156
UIST,15855042df73027079b4cd17d3930440c8e124d2,UIST,2016,Facial Expression Mapping inside Head Mounted Display by Embedded Optical Sensors,"Katsuhiro Suzuki, Fumihiko Nakamura, Jiu Otsuka, Katsutoshi Masai, Yuta Itoh, Yuta Sugiura, Maki Sugimoto","2301571, 1859613, 3462186, 1998122, 2792579, 1799242, 1792570","Head Mounted Display (HMD) provides an immersive ex-perience in virtual environments for various purposes such as for games and communication. However, it is difficult to capture facial expression in a HMD-based virtual environ-ment because the upper half of user's face is covered up by the HMD. In this paper, we propose a facial expression mapping technology between user and a virtual avatar using embedded optical sensors and machine learning. The distance between each sensor and surface of the face is measured by the optical sensors that are attached inside the HMD. Our system learns the sensor values of each facial expression by neural network and creates a classifier to estimate the current facial expression.",0,44.6540880503
UIST,794fca0c9fc41187380b67d5c2dfae90a9636e72,UIST,2010,MudPad: localized tactile feedback on touch surfaces,"Yvonne Jansen, Thorsten Karrer, Jan O. Borchers","1809140, 3301123, 1692837","We present MudPad, a system that is capable of localized active haptic feedback on multitouch surfaces. An array of electromagnets locally actuates a tablet-sized overlay containing magnetorheological (MR) fluid. The reaction time of the fluid is fast enough for realtime feedback ranging from static levels of surface softness to a broad set of dynamically changeable textures. As each area can be addressed individually, the entire visual interface can be enriched with a multi-touch haptic layer that conveys semantic information as the appropriate counterpart to multi-touch input.",6,57.5581395349
UIST,73c5bfb92d8e2fce423319a1554d42b0be508204,UIST,2001,Conducting a realistic electronic orchestra,"Jan O. Borchers, Wolfgang Samminger, Max Mühlhäuser","1692837, 3309454, 1725964","<i>Personal Orchestra</i> is the first system to let users conduct an actual audio and video recording of an orchestra, using an infrared baton to control tempo, volume, and instrument sections. A gesture recognition algorithm interprets user input, and a novel high-fidelity playback algorithm renders audio and video data at variable speed without time-stretching artifacts. The system is installed as a public exhibit in the <sc>HOUSE OF MUSIC VIENNA</sc>.",6,13.3333333333
UIST,cfd73f3b2b505150a23ccdfa8ef4391b8ad8448b,UIST,2011,Toucheo: multitouch and stereo combined in a seamless workspace,"Martin Hachet, Benoit Bossavit, Aurélie Cohé, Jean-Baptiste de la Rivière","2281511, 2922994, 1966967, 8682110","We propose a new system that efficiently combines direct multitouch interaction with co-located 3D stereoscopic visualization. In our approach, users benefit from well-known 2D metaphors and widgets displayed on a monoscopic touchscreen, while visualizing occlusion-free 3D objects floating above the surface at an optically correct distance. Technically, a horizontal semi-transparent mirror is used to reflect 3D images produced by a stereoscopic screen, while the user's hand as well as a multitouch screen located below this mirror remain visible. By registering the 3D virtual space and the physical space, we produce a rich and unified workspace where users benefit simultaneously from the advantages of both direct and indirect interaction, and from 2D and 3D visualizations. A pilot usability study shows that this combination of technology provides a good user experience.",32,78.5714285714
UIST,1a892b8f0abf8f1ea559cc94470cf9d0ccc07526,UIST,2016,ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers,"Gierad Laput, Robert Xiao, Chris Harrison","1727999, 1681726, 1730920","Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Their position on the wrist makes them ideal for capturing <i>bio-acoustic</i> signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch's existing accelerometer to 4 kHz. Using this new source of high-fidelity data, we uncovered a wide range of applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-aware functionality. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body. Overall, our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices.",1,92.7672955975
UIST,3ae93360db042e8c77f7d2b5fb49fefd4f331352,UIST,2013,Surround-see: enabling peripheral vision on smartphones during active use,"Xing-Dong Yang, Khalad Hasan, Neil D. B. Bruce, Pourang Irani","1791070, 2342261, 2866780, 1773923","Mobile devices are endowed with significant sensing capabilities. However, their ability to 'see' their surroundings, during active use, is limited. We present Surround-See, a self-contained smartphone equipped with an omni-directional camera that enables peripheral vision around the device to augment daily mobile tasks. Surround-See provides mobile devices with a field-of-view collinear to the device screen. This capability facilitates novel mobile tasks such as, pointing at objects in the environment to interact with content, operating the mobile device at a physical distance and allowing the device to detect user activity, even when the user is not holding it. We describe Surround-See's architecture, and demonstrate applications that exploit peripheral 'seeing' capabilities during active use of a mobile device. Users confirm the value of embedding peripheral vision capabilities on mobile devices and offer insights for novel usage methods.",17,81.6513761468
UIST,d3611bf540f526b8e012def4e23517567aba32d5,UIST,2007,Shadow reaching: a new perspective on interaction for large displays,"Garth Shoemaker, Anthony Tang, Kellogg S. Booth","3061182, 3151023, 1800617","We introduce Shadow Reaching, an interaction technique that makes use of a perspective projection applied to a shadow representation of a user. The technique was designed to facilitate manipulation over large distances and enhance understanding in collaborative settings. We describe three prototype implementations that illustrate the technique, examining the advantages of using shadows as an interaction metaphor to support single users and groups of collaborating users. Using these prototypes as a design probe, we discuss how the three components of the technique (sensing, modeling, and rendering) can be accomplished with real (physical) or computed (virtual) shadows, and the benefits and drawbacks of each approach.",60,80.5555555556
UIST,a7f2053dc42b395481b192618511c549ea2cbbdb,UIST,2013,UltraHaptics: multi-point mid-air haptic feedback for touch surfaces,"Tom Carter, Sue Ann Seah, Benjamin Long, Bruce Drinkwater, Sriram Subramanian","2484657, 8546038, 2826191, 1982947, 1702794","We introduce UltraHaptics, a system designed to provide multi-point haptic feedback above an interactive surface. UltraHaptics employs focused ultrasound to project discrete points of haptic feedback through the display and directly on to users' unadorned hands. We investigate the desirable properties of an acoustically transparent display and demonstrate that the system is capable of creating multiple localised points of feedback in mid-air. Through psychophysical experiments we show that feedback points with different tactile properties can be identified at smaller separations. We also show that users are able to distinguish between different vibration frequencies of non-contact points with training. Finally, we explore a number of exciting new interaction possibilities that UltraHaptics provides.",36,96.3302752294
UIST,6023773da64a1187fc7c1339c8bc870e49c341af,UIST,2016,Representing Gaze Direction in Video Communication Using Eye-Shaped Display,"Mai Otsuki, Taiki Kawano, Keita Maruyama, Hideaki Kuzuoka, Yusuke Suzuki","1683502, 2877739, 3493282, 1696686, 2498412","A long-standing challenge of video-mediated communication systems is to correctly represent a remote participant's gaze direction in local environments. To address this problem, we developed a video communication system using an ""eye-shaped display."" This display is made of an artificial ulexite (TV rock) that is cut into a hemispherical shape, enabling the light from the bottom surface to be projected onto the curved surface. By displaying a simulated iris onto the eye-shaped display, we theorize that our system can represent the gaze direction as accurately as a real human eye.",0,44.6540880503
UIST,99d2d86297456921622b1abeeea9455a1c9eef94,UIST,2014,Zero-latency tapping: using hover information to predict touch locations and eliminate touchdown latency,"Haijun Xia, Ricardo Jota, Benjamin McCanny, Zhe Yu, Clifton Forlines, Karan Singh, Daniel J. Wigdor","1786964, 1766253, 1769379, 1765855, 1694854, 1682205, 1961958","A method of reducing the perceived latency of touch input by employing a model to predict touch events before the finger reaches the touch surface is proposed. A corpus of 3D finger movement data was collected, and used to develop a model capable of three granularities at different phases of movement: initial direction, final touch location, time of touchdown. The model is validated for target distances &gt;= 25.5cm, and demonstrated to have a mean accuracy of 1.05cm 128ms before the user touches the screen. Preference study of different levels of latency reveals a strong preference for unperceived latency touchdown feedback. A form of 'soft' feedback, as well as other uses for this prediction to improve performance, is proposed.",5,63.1782945736
UIST,f135a397dd4d9d541eca433646f841f7234d5e8e,UIST,2016,Gaze and Touch Interaction on Tablets,"Ken Pfeuffer, Hans-Werner Gellersen","3171800, 4919595","We explore how gaze can support touch interaction on tablets. When holding the device, the free thumb is normally limited in reach, but can provide an opportunity for indirect touch input. Here we propose gaze and touch input, where touches redirect to the gaze target. This provides whole-screen reachability while only using a single hand for both holding and input. We present a user study comparing this technique to direct-touch, showing that users are slightly slower but can utilise one-handed use with less physical effort. To enable interaction with small targets, we introduce CursorShift, a method that uses gaze to provide users temporal control over cursors during direct-touch interactions. Taken together, users can employ three techniques on tablets: direct-touch, gaze and touch, and cursor input. In three applications, we explore how these techniques can coexist in the same UI and demonstrate how tablet tasks can be performed with thumb-only input of the holding hand, and with it describe novel interaction techniques for gaze based tablet interaction.",1,92.7672955975
UIST,6ec95ce28d4c24aa4cf13d9a002517ba49dfd63b,UIST,2014,Gaze-touch: combining gaze with multi-touch for interaction on the same surface,"Ken Pfeuffer, Jason Alexander, Ming Ki Chong, Hans-Werner Gellersen","3171800, 2928764, 2577932, 4919595","Gaze has the potential to complement multi-touch for interaction on the same surface. We present gaze-touch, a technique that combines the two modalities based on the principle of 'gaze selects, touch manipulates'. Gaze is used to select a target, and coupled with multi-touch gestures that the user can perform anywhere on the surface. Gaze-touch enables users to manipulate any target from the same touch position, for whole-surface reachability and rapid context switching. Conversely, gaze-touch enables manipulation of the same target from any touch position on the surface, for example to avoid occlusion. Gaze-touch is designed to complement direct-touch as the default interaction on multi-touch surfaces. We provide a design space analysis of the properties of gaze-touch versus direct-touch, and present four applications that explore how gaze-touch can be used alongside direct-touch. The applications demonstrate use cases for interchangeable, complementary and alternative use of the two modes of interaction, and introduce novel techniques arising from the combination of gaze-touch and conventional multi-touch.",14,86.8217054264
UIST,4ac651852e0909590ceb788223b14721d582034c,UIST,2016,Nomadic Virtual Reality: Exploring New Interaction Concepts for Mobile Virtual Reality Head-Mounted Displays,Jan Gugenheimer,3186723,"Technical progress and miniaturization enables virtual reality (VR) head-mounted displays (HMDs) now to be solely operated using a smartphone as a display, processing unit and sensor unit. These mobile VR HMDs (e.g. Samsung GearVR) allow for a whole new interaction scenario, where users can bring their HMD with them wherever they want and immerse themselves anytime at any place (nomadic VR). However, most of the early research on interaction with VR HMDs focused around stationary setups. My research revolves around enabling new forms of interaction for these nomadic VR scenarios. In my research I choose a user-centered design approach where I build research prototypes to solve potential problems of nomadic VR and evaluate those prototypes in user studies. I am going to present three prototypes revolving around current challenges of nomadic VR (input and feedback).",0,44.6540880503
UIST,eeb84a62fc8012bd9ca1706959d0f7ae590e3697,UIST,2011,Poke: emotional touch delivery through an inflatable surface over interpersonal mobile communications,"Youngwoo Park, Sungjae Hwang, Tek-Jin Nam","8108899, 7547522, 1696044","In this paper we present Poke - a soft and human-like re-mote touch technique through an inflatable surface. We aimed to design it for delivering more emotional and plea-sant touches over interpersonal mobile communications. Poke enables to touch human's skin with an inflatable sur-face according to the other user's finger pressures and hand gestures during a phone call. It delivers different kinds of pokes and other affective touches with its inflating patterns (strengths and repetitions) and vibrations from the top of the inflatable surface. The paper also suggests affective touches such as weak/hard poke, poke and then shake, poke back and pat which can be exchanged during typical phone calls.",3,31.4285714286
UIST,57efe8e562bec93d7223dd640554ef7750feec8c,UIST,2006,Robust computer vision-based detection of pinching for one and two-handed gesture input,Andrew D. Wilson,1767449,We present a computer vision technique to detect when the user brings their thumb and forefinger together (a <i>pinch</i> gesture) for close-range and relatively controlled viewing circumstances. The technique avoids complex and fragile hand tracking algorithms by detecting the hole formed when the thumb and forefinger are touching; this hole is found by simple analysis of the connected components of the background segmented against the hand. Our Thumb and Fore-Finger Interface (TAFFI) demonstrates the technique for cursor control as well as map navigation using one and two-handed interactions.,69,92.5
UIST,7d4cea82201bb2ef45b44ae4ef6368266bad44a1,UIST,2013,FlexStroke: a jamming brush tip simulating multiple painting tools on digital platform,"Xin Liu, Haijun Xia, Jiawei Gu","1749705, 1786964, 1767095","We propose a new system to enable the real painting experience on digital platform and extend it to multi-strokes for different painting needs. In this paper, we describe how the FlexStroke is used as Chinese brush, oil brush and crayon with changes of its jamming tip. This tip has different levels of stiffness based on its jamming structure. Visual simulations on PixelSense jointly enhance the intuitive painting process with highly realistic display results.",0,10.5504587156
UIST,1b764ca26d8af71fffab82d860cf7a70f85f8461,UIST,2010,Development of the motion-controllable ball,"Takashi Ichikawa, Takuya Nojima","1763819, 1797797","In this report, we propose a novel ball type interactive interface device. Balls are one of the most important pieces of equipment used for entertainment and sports. Their motion guides a player's response in terms of, for example, a feint or similar movement. Many kinds of breaking ball throws have been developed for various sports(e.g. baseball). However, acquiring the skill to appropriately react to these breaking balls is often hard to achieve and requires long-term training. Many researchers focus on the ball itself and have developed interactive balls with visual and acoustic feedbacks. However, these balls do not have the ability for motion control. In this paper, we introduce a ball-type motion control interface device. It is composed of a ball and an air-pressure tank to change its vector using gas ejection. We conducted an experiment that measures the ball's flight path while subjected to gas ejection and the results showed that the prototype system had enough power to change the ball's vector while flying",4,48.2558139535
UIST,1c62176899f8d3e23bd4652df0c2ed888f520ae5,UIST,2006,Viz: a visual analysis suite for explaining local search behavior,"Steven Halim, Roland H. C. Yap, Hoong Chuin Lau","1848182, 1713932, 1809379","<i>NP</i>-hard combinatorial optimization problems are common in real life. Due to their intractability, local search algorithms are often used to solve such problems. Since these algorithms are heuristic-based, it is hard to understand how to improve or tune them. We propose an interactive visualization tool, VIZ, meant for understanding the behavior of local search. VIZ uses animation of abstract search trajectories with other visualizations which are also animated in a VCR-like fashion to graphically playback the algorithm behavior. It combines generic visualizations applicable on arbitrary algorithms with algorithm and problem specific visualizations. We use a variety of techniques such as alpha blending to reduce visual clutter and to smooth animation, highlights and shading, automatically generated index points for playback, and visual comparison of two algorithms. The use of multiple viewpoints can be an effective way of understanding search behavior and highlight algorithm behavior which might otherwise be hidden.",14,12.5
UIST,5a7e3a5dc022b0b44f4c23cf2b0c6eedc90f091e,UIST,2000,A semi-automatic approach to home video editing,"Andreas Girgensohn, John S. Boreczky, Patrick Chiu, John Doherty, Jonathan Foote, Gene Golovchinsky, Shingo Uchihashi, Lynn Wilcox","2195286, 2719487, 2895008, 3881379, 1797460, 2724097, 1713254, 1691319","Hitchcock is a system that allows users to easily create custom videos from raw video shot with a standard video camera. In contrast to other video editing systems, Hitchcock uses automatic analysis to determine the suitability of portions of the raw video. Unsuitable video typically has fast or erratic camera motion. Hitchcock first analyzes video to identify the type and amount of camera motion: fast pan, slow zoom, etc. Based on this analysis, a numerical "" unsuit-ability "" score is computed for each frame of the video. Combined with standard editing rules, this score is used to identify clips for inclusion in the final video and to select their start and end points. To create a custom video, the user drags keyframes corresponding to the desired clips into a storyboard. Users can lengthen or shorten the clip without specifying the start and end frames explicitly. Clip lengths are balanced automatically using a spring-based algorithm.",76,64.0
UIST,45280b62221d718a978cd082f705f478dfd95b74,UIST,2012,Closing the loop between intentions and actions,Ankit Gupta,2972945,"In this document, I propose systems that aim to minimize the gap between intentions and the corresponding actions under different scenarios. The gap exists because of many reasons like subjective mapping between the two, lack of resources to implement the action, or inherent noise in the physical processes. The proposed system observes the action and infers the intention behind it. The system then generates a refined action using the inference. The inferred intention and the refined action are then provided as feedback to the user who can then perform corrective actions or choose the refined action as it is as the desired result. I demonstrate the design and implementation of such systems through five projects - Image Deblurring, Tracking Block Model Assembly, Animating with Physical Proxies, What Affects Handwriting and Spying on the Writer.",0,9.80392156863
UIST,818c0e8ef92070e7c4cb4f616f9d28625002936f,UIST,2013,Detecting student frustration based on handwriting behavior,"Hiroki Asai, Hayato Yamana","2984232, 1749490","Detecting states of frustration among students engaged in learning activities is critical to the success of teaching assistance tools. We examine the relationship between a student's pen activity and his/her state of frustration while solving handwritten problems. Based on a user study involving mathematics problems, we found that our detection method was able to detect student frustration with a precision of 87% and a recall of 90%. We also identified several particularly discriminative features, including writing stroke number, erased stroke number, pen activity time, and air stroke speed.",1,27.0642201835
UIST,618479781040d70f34f850cf0988d6f297861372,UIST,2011,Yelling in the hall: using sidetone to address a problem with mobile remote presence systems,"Andreas Paepcke, Bianca Soto, Leila Takayama, Frank Koenig, Blaise Gassend","1750481, 2317381, 1753156, 1923898, 1946570","In our field deployments of mobile remote presence (MRP) systems in offices, we observed that remote operators of MRPs often unintentionally spoke too loudly. This disrupted their local co-workers, who happened to be within earshot of the MRP system. To address this issue, we prototyped and empirically evaluated the effect of sidetone to help operators self regulate their speaking loudness. Sidetone is the intentional, attenuated feedback of speakers' voices to their ears while they are using a telecommunication device. In a 3-level (no sidetone vs. low sidetone vs. high sidetone) within- participants pair of experiments, people interacted with a confederate through an MRP system. The first experiment involved MRP operators using headsets with boom microphones (N=20). The second experiment involved MRP operators using loudspeakers and desktop microphones (N=14). While we detected the effects of the sidetone manipulation in our audio-visual context, the effect was attenuated in comparison to earlier audio-only studies. We hypothesize that the strong visual component of our MRP system interferes with the sidetone effect. We also found that engaging in more social tasks (e.g., a getting-to-know-you activity) and more intellectually demanding tasks (e.g., a creativity exercise) influenced how loudly people spoke. This suggests that testing such sidetone effects in the typical read-aloud setting is insufficient for generalizing to more interactive, communication tasks. We conclude that MRP application support must reach beyond the time honored audio-only technologies to solve the problem of excessive speaker loudness.",2,24.2857142857
UIST,9c291595894e0b14f0a023319091c4e4c207ee94,UIST,2013,"BitWear: a platform for small, connected, interactive devices","Kent Lyons, David H. Nguyen, Shigeyuki Seko, Sean White, Daniel Ashbrook, Halley Profita","2073793, 1767024, 3070877, 5047258, 2071682, 2141959","We describe BitWear, a platform for prototyping small, wireless, interactive devices. BitWear incorporates hardware, wireless connectivity, and a cloud component to enable collections of connected devices. We are using this platform to create, explore, and experiment with a multitude of wearable and deployable physical forms and interactions.",3,45.871559633
UIST,3a01fa01db39779e296a005d2695f3a23df9652c,UIST,2008,Search Vox: leveraging multimodal refinement and partial knowledge for mobile voice search,"Tim Paek, Bo Thiesson, Yun-Cheng Ju, Bongshin Lee","3049377, 2868274, 1740054, 1710078","Internet usage on mobile devices continues to grow as users seek anytime, anywhere access to information. Because users frequently search for businesses, directory assistance has been the focus of many voice search applications utilizing speech as the primary input modality. Unfortunately, mobile settings often contain noise which degrades performance. As such, we present Search Vox, a mobile search interface that not only facilitates touch and text refinement whenever speech fails, but also allows users to assist the recognizer via text hints. Search Vox can also take advantage of any partial knowledge users may have about the business listing by letting them express their uncertainty in an intuitive way using verbal wildcards. In simulation experiments conducted on real voice search data, leveraging multimodal refinement resulted in a 28% relative reduction in error rate. Providing text hints along with the spoken utterance resulted in even greater relative reduction, with dramatic gains in recovery for each additional character.",8,15.7142857143
UIST,51950c7610c80cf8b9e0739890130cdc9cd287ad,UIST,2014,Loupe: a handheld near-eye display,"Kent Lyons, Seung Wook Kim, Shigeyuki Seko, David H. Nguyen, Audrey Desjardins, Mélodie Vidal, David Dobbelstein, Jeremy Rubin","2073793, 6557465, 3070877, 1767024, 3106993, 2424540, 2638354, 3169477","Loupe is a novel interactive device with a near-eye virtual display similar to head-up display glasses that retains a handheld form factor. We present our hardware implementation and discuss our user interface that leverages Loupe's unique combination of properties. In particular, we present our input capabilities, spatial metaphor, opportunities for using the round aspect of Loupe, and our use of focal depth. We demonstrate how those capabilities come together in an example application designed to allow quick access to information feeds.",2,41.4728682171
UIST,247ef39043504bd91d0f858d4e202fcf4c564e88,UIST,2006,Pen-top feedback for paper-based interfaces,"Chunyuan Liao, François Guimbretière, Corinna E. Löckenhoff","2686363, 2539134, 2538621","Current paper-based interfaces such as PapierCraft, provide very little feedback and this limits the scope of possible interactions. So far, there has been little systematic exploration of the structure, constraints, and contingencies of feedback-mechanisms in paper-based interaction systems for paper-only environments. We identify three levels of feedback: discovery feedback (e.g., to aid with menu learning), status-indication feedback (e.g., for error detection), and task feedback (e.g., to aid in a search task). Using three modalities (visual, tactile, and auditory) which can be easily implemented on a pen-sized computer, we introduce a conceptual matrix to guide systematic research on pen-top feedback for paper-based interfaces. Using this matrix, we implemented a multimodal pen prototype demonstrating the potential of our approach. We conducted an experiment that confirmed the efficacy of our design in helping users discover a new interface and identify and correct their errors.",27,42.5
UIST,211c589f03fa03580b513500f5c6aad4dfaf94a9,UIST,2003,Rapid serial visual presentation techniques for consumer digital video devices,"Kent Wittenburg, Clifton Forlines, Tom Lanning, Alan Esenther, Shigeo Harada, Taizo Miyachi","3242147, 1694854, 2305161, 2802986, 2446045, 2834941","In this paper we propose a new model for a class of rapid serial visual presentation (RSVP) interfaces [16] in the context of consumer video devices. The basic spatial layout ""explodes"" a sequence of image frames into a 3D trail in order to provide more context for a spatial/temporal presentation. As the user plays forward or back, the trail advances or recedes while the image in the foreground focus position is replaced. The design is able to incorporate a variety of methods for analyzing or highlighting images in the trail. Our hypotheses are that users can navigate more quickly and precisely to points of interest when compared to conventional consumer-based browsing, channel flipping, or fast-forwarding techniques. We report on an experiment testing our hypotheses in which we found that subjects were more accurate but not faster in browsing to a target of interest in recorded television content with a TV remote.",23,25.0
UIST,36a607d93637475a3675ee55ee7a1c2eb6d4376a,UIST,2015,DataTone: Managing Ambiguity in Natural Language Interfaces for Data Visualization,"Tong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, Karrie Karahalios","1703670, 2875493, 2630700, 1995841, 1680270","Answering questions with data is a difficult and time-consuming process. Visual dashboards and templates make it easy to get started, but asking more sophisticated questions often requires learning a tool designed for expert analysts. Natural language interaction allows users to ask questions directly in complex programs without having to learn how to use an interface. However, natural language is often ambiguous. In this work we propose a mixed-initiative approach to managing ambiguity in natural language interfaces for data visualization. We model ambiguity throughout the process of turning a natural language query into a visualization and use algorithmic disambiguation coupled with interactive <i>ambiguity widgets</i>. These widgets allow the user to resolve ambiguities by surfacing system decisions at the point where the ambiguity matters. Corrections are stored as constraints and influence subsequent queries. We have implemented these ideas in a system, DataTone. In a comparative study, we find that DataTone is easy to learn and lets users ask questions without worrying about syntax and proper question form.",4,75.4385964912
UIST,cb9b1d0f29d74feb63571b6d4341feac4fb036cf,UIST,2014,PortraitSketch: face sketching assistance for novices,"Jun Xie, Aaron Hertzmann, Wilmot Li, Holger Winnemöller","1786096, 1747779, 2812691, 2168852","We present PortraitSketch, an interactive drawing system that helps novices create pleasing, recognizable face sketches without requiring prior artistic training. As the user traces over a source portrait photograph, PortraitSketch automatically adjusts the geometry and stroke parameters (thickness, opacity, etc.) to improve the aesthetic quality of the sketch. We present algorithms for adjusting both outlines and shading strokes based on important features of the underlying source image. In contrast to automatic stylization systems, PortraitSketch is designed to encourage a sense of ownership and accomplishment in the user. To this end, all adjustments are performed in real-time, and the user ends up directly drawing all strokes on the canvas. The findings from our user study suggest that users prefer drawing with some automatic assistance, thereby producing better drawings, and that assistance does not decrease the perceived level of involvement in the creative process.",11,82.9457364341
UIST,0fd8a2b1a759dfe17ecb3926574025709ba1c69d,UIST,2016,Promoting Natural Interactions Through Embedded Input Using Novel Sensing Techniques,Sang Ho Yoon,6566004,"From mobile devices to interactive objects, various input methods are provided using built-in motion and capacitive touch sensors. These inputs are offered in effective and efficient manner where users can operate interface quickly and easily. However, they do not fully explore the input space supported by human's natural motion behavior. As a solution, my work focuses on promoting natural interaction through hand-driven embedded input powered by multimodal and magnetic sensing techniques. In my previous works, embedded inputs were implemented in the form of smart textile, stylus, and ring supporting from mobile devices to everyday objects. Throughout the paper, I will briefly go over implemented systems along with evaluated results and potential applications. Future research direction is highlighted at the end of the paper.",0,44.6540880503
UIST,5bc9cd5b54d9781f3f388673a7102e579b5a176c,UIST,2014,ParaFrustum: visualization techniques for guiding a user to a constrained set of viewing positions and orientations,"Mengu Sukan, Carmine Elvezio, Ohan Oda, Steven K. Feiner, Barbara Tversky","2977638, 2030511, 3147807, 1809403, 1743610","Many tasks in real or virtual environments require users to view a target object or location from one of a set of strategic viewpoints to see it in context, avoid occlusions, or view it at an appropriate angle or distance. We introduce ParaFrustum, a geometric construct that represents this set of strategic viewpoints and viewing directions. ParaFrustum is inspired by the look-from and look-at points of a computer graphics camera specification, which precisely delineate a location for the camera and a direction in which it looks. We generalize this approach by defining a ParaFrustum in terms of a look-from volume and a look-at volume, which establish constraints on a range of acceptable locations for the user's eyes and a range of acceptable angles in which the user's head can be oriented. Providing tolerance in the allowable viewing positions and directions avoids burdening the user with the need to assume a tightly constrained 6DoF pose when it is not required by the task. We describe two visualization techniques for virtual or augmented reality that guide a user to assume one of the poses defined by a ParaFrustum, and present the results of a user study measuring the performance of these techniques. The study shows that the constraints of a tightly constrained ParaFrustum (e.g., approximating a conventional camera frustum) require significantly more time to satisfy than those of a loosely constrained one. The study also reveals interesting differences in participant trajectories in response to the two techniques.",3,49.2248062016
UIST,200e91184be890f898cf74cc9c65f6391605aa9a,UIST,2010,UIMarks: quick graphical interaction with specific targets,"Olivier Chapuis, Nicolas Roussel","3342979, 1728921","This paper reports on the design and evaluation of UIMarks, a system that lets users specify on-screen targets and associated actions by means of a graphical marking language. UIMarks supplements traditional pointing by providing an alternative mode in which users can quickly activate these marks. Associated actions can range from basic pointing facilitation to complex sequences possibly involving user interaction: one can leave a mark on a palette to make it more reachable, but the mark can also be configured to wait for a click and then automatically move the pointer back to its original location, for example. The system has been implemented on two different platforms, Metisse and OS X. We compared it to traditional pointing on a set of elementary and composite tasks in an abstract setting. Although pure pointing was not improved, the programmable automation supported by the system proved very effective.",7,59.3023255814
UIST,42739714a5a2947f4ec2b33aa760ad37eaf6910e,UIST,2005,Metisse is not a 3D desktop!,"Olivier Chapuis, Nicolas Roussel","3342979, 1728921","Twenty years after the general adoption of overlapping windows and the desktop metaphor, modern window systems differ mainly in minor details such as window decorations or mouse and keyboard bindings. While a number of innovative window management techniques have been proposed, few of them have been evaluated and fewer have made their way into real systems. We believe that one reason for this is that most of the proposed techniques have been designed using a low fidelity approach and were never made properly available. In this paper, we present Metisse, a fully functional window system specifically created to facilitate the design, the implementation and the evaluation of innovative window management techniques. We describe the architecture of the system, some of its implementation details and present several examples that illustrate its potential.",41,53.2258064516
UIST,376d0d971d1ce9b4e33564e3c987d18503b98fab,UIST,2011,Recognizing currency bills using a mobile phone: an assistive aid for the visually impaired,"Nektarios Paisios, Alex Rubinsteyn, Vrutti Vyas, Lakshminarayanan Subramanian","3138681, 1717086, 2627131, 1710917","Despite the rapidly increasing use of credit cards and other electronic forms of payment, cash is still widely used for everyday transactions due to its convenience, perceived security and anonymity. However, the visually impaired might have a hard time telling each paper bill apart, since, for example, all dollar bills have the exact same size and, in general, currency bills around the world are not distinguishable by any tactile markings. We propose the use of a broadly available tool, the camera of a smart-phone, and an adaptation of the SIFT algorithm to recognize partial and even distorted images of paper bills. Our algorithm improves memory efficiency and the speed of SIFT key-point classification by using a k-means clustering approach. Our results show that our system can be used in real-world scenarios to recognize unknown bills with a high accuracy.",0,6.19047619048
UIST,90a67bc508e86c6cf20b14955820d6fa0be5614b,UIST,2015,Virtual Replicas for Remote Assistance in Virtual and Augmented Reality,"Ohan Oda, Carmine Elvezio, Mengu Sukan, Steven K. Feiner, Barbara Tversky","3147807, 2030511, 2977638, 1809403, 1743610","In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.",5,80.701754386
UIST,63d8bf25c8909d5b718f0a6e9122ccf25512f6fa,UIST,2015,Procedural Modeling Using Autoencoder Networks,"Mehmet Ersin Yümer, Paul Asente, Radomír Mech, Levent Burak Kara","2396667, 2934421, 2008027, 1808848","Procedural modeling systems allow users to create high quality content through parametric, conditional or stochastic rule sets. While such approaches create an abstraction layer by freeing the user from direct geometry editing, the nonlinear nature and the high number of parameters associated with such design spaces result in arduous modeling experiences for non-expert users. We propose a method to enable intuitive exploration of such high dimensional procedural modeling spaces within a lower dimensional space learned through autoencoder network training. Our method automatically generates a representative training dataset from the procedural modeling rule set based on shape similarity features. We then leverage the samples in this dataset to train an autoencoder neural network, while also structuring the learned lower dimensional space for continuous exploration with respect to shape features. We demonstrate the efficacy our method with user studies where designers create content with more than 10-fold faster speeds using our system compared to the classic procedural modeling interface.",3,71.0526315789
UIST,9787cf05946511022876a9a7e9dd21849b254f43,UIST,2013,SenSkin: adapting skin as a soft interface,"Masayasu Ogata, Yuta Sugiura, Yasutoshi Makino, Masahiko Inami, Michita Imai","3176640, 1799242, 1712395, 1684930, 1752970","We present a sensing technology and input method that uses skin deformation estimated through a thin band-type device attached to the human body, the appearance of which seems socially acceptable in daily life. An input interface usually requires feedback. SenSkin provides tactile feedback that enables users to know which part of the skin they are touching in order to issue commands. The user, having found an acceptable area before beginning the input operation, can continue to input commands without receiving explicit feedback. We developed an experimental device with two armbands to sense three-dimensional pressure applied to the skin. Sensing tangential force on uncovered skin without haptic obstacles has not previously been achieved. SenSkin is also novel in that quantitative tangential force applied to the skin, such as that of the forearm or fingers, is measured. An infrared (IR) reflective sensor is used since its durability and inexpensiveness make it suitable for everyday human sensing purposes. The multiple sensors located on the two armbands allow the tangential and normal force applied to the skin dimension to be sensed. The input command is learned and recognized using a Support Vector Machine (SVM). Finally, we show an application in which this input method is implemented.",24,87.1559633028
UIST,15a680b23e34c28ffb21098a40e5becf440c4645,UIST,2010,ARmonica: a collaborative sonic environment,"Mengu Sukan, Ohan Oda, Xiang Shi, Manuel Entrena, Shrenik Sadalgi, Jie Qi, Steven K. Feiner","2977638, 3147807, 8714285, 2467639, 2206379, 4130567, 1809403","ARmonica is a 3D audiovisual augmented reality environment in which players can position and edit virtual bars that play sounds when struck by virtual balls launched under the influence of physics. Players experience ARmonica through head-tracked head-worn displays and tracked hand-held ultramobile personal computers, and interact through tracked Wii remotes and touch-screen taps. The goal is for players to collaborate in the creation and editing of an evolving sonic environment. Research challenges include supporting walk-up usability without sacrificing deeper functionality.",1,22.0930232558
UIST,869156d56cf1ce9c9c9cbc20aa83b6e6f320566d,UIST,2012,iRing: intelligent ring using infrared reflection,"Masayasu Ogata, Yuta Sugiura, Hirotaka Osawa, Michita Imai","3176640, 1799242, 1787301, 1752970","We present the iRing, an intelligent input ring device developed for measuring finger gestures and external input. iRing recognizes rotation, finger bending, and external force via an infrared (IR) reflection sensor that leverages skin characteristics such as reflectance and softness. Furthermore, iRing allows using a push and stroke input method, which is popular in touch displays. The ring design has potential to be used as a wearable controller because its accessory shape is socially acceptable, easy to install, and safe, and iRing does not require extra devices. We present examples of iRing applications and discuss its validity as an inexpensive wearable interface and as a human sensing device.",24,81.3725490196
UIST,0b51c5b5ad9de91cc6ef2d42e784d0a8223f2ace,UIST,2016,Phones on Wheels: Exploring Interaction for Smartphones with Kinetic Capabilities,"Takefumi Hiraki, Koya Narumi, Koji Yatani, Yoshihiro Kawahara","3105106, 2257685, 3260704, 1773545","This paper introduces novel interaction and applications using smartphones with kinetic capabilities. We develop an accessory module with robot wheels for a smartphone. With this module, the smartphone can move in a linear direction or rotate with sufficient power. The module also includes rotary encoders, allowing us to use the wheels as an input modality. We demonstrate a series of novel mobile interaction for mobile devices with kinetic capabilities through three applications.",0,44.6540880503
UIST,129be707c4422e11f7abf8d0481ca13839c35a63,UIST,2010,Sensing foot gestures from the pocket,"Jeremy Scott, David Dearman, Koji Yatani, Khai N. Truong","1861338, 1762768, 3260704, 1752847","Visually demanding interfaces on a mobile phone can diminish the user experience by monopolizing the user's attention when they are focusing on another task and impede accessibility for visually impaired users. Because mobile devices are often located in pockets when users are mobile, explicit foot movements can be defined as eyes-and-hands-free input gestures for interacting with the device. In this work, we study the human capability associated with performing foot-based interactions which involve lifting and rotation of the foot when pivoting on the toe and heel. Building upon these results, we then developed a system to learn and recognize foot gestures using a single commodity mobile phone placed in the user's pocket or in a holster on their hip. Our system uses acceleration data recorded by a built-in accelerometer on the mobile device and a machine learning approach to recognizing gestures. Through a lab study, we demonstrate that our system can classify ten different foot gestures at approximately 86% accuracy.",29,81.3953488372
UIST,8049f961d6b3f6c44f8b48ea9035db4dd9ce2a2a,UIST,2004,Visual tracking of bare fingers for interactive surfaces,"Julien Letessier, François Bérard","8340429, 2116935","Visual tracking of bare fingers allows more direct manipulation of digital objects, multiple simultaneous users interacting with their two hands, and permits the interaction on large surfaces, using only commodity hardware. After presenting related work, we detail our implementation. Its design is based on our modeling of two classes of algorithms that are key to the tracker: Image Differencing Segmentation (IDS) and Fast Rejection Filters (FRF). We introduce a new chromatic distance for IDS and a FRF that is independent to finger rotation. The system runs at full frame rate (25 Hz) with an average total system latency of 80 ms, independently of the number of tracked fingers. When used in a controlled environment such as a meeting room, its robustness is satisfying for everyday use.",72,76.3157894737
UIST,3868fc6a82247c4150948b43389998bf794c96f6,UIST,2009,SemFeel: a user interface with semantic tactile feedback for mobile touch-screen devices,"Koji Yatani, Khai N. Truong","3260704, 1752847","One of the challenges with using mobile touch-screen devices is that they do not provide tactile feedback to the user. Thus, the user is required to look at the screen to interact with these devices. In this paper, we present SemFeel, a tactile feedback system which informs the user about the presence of an object where she touches on the screen and can offer additional semantic information about that item. Through multiple vibration motors that we attached to the backside of a mobile touch-screen device, SemFeel can generate different patterns of vibration, such as ones that flow from right to left or from top to bottom, to help the user interact with a mobile device. Through two user studies, we show that users can distinguish ten different patterns, including linear patterns and a circular pattern, at approximately 90% accuracy, and that SemFeel supports accurate eyes-free interactions.",53,77.1428571429
UIST,24e85fc840aea95aa82a9e4de45f3c68089b3695,UIST,2016,VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World,"Anhong Guo, Xiang 'Anthony' Chen, Haoran Qi, Samuel White, Suman Ghosh, Chieko Asakawa, Jeffrey P. Bigham","2582404, 2028468, 3491517, 4707811, 7359689, 1851536, 1744846","The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. We introduce <i>VizLens</i> - an accessible mobile application and supporting backend that can robustly and interactively help blind people use nearly any interface they encounter. VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier. The VizLens application helps users recapture the interface in the field of the camera, and uses computer vision to interactively describe the part of the interface beneath their finger (updating 8 times per second). We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind participants, and our crowdsourcing labeling workflow was fast (8 minutes), accurate (99.7%), and cheap ($1.15). We then explore extensions of VizLens that allow it to <i>(i)</i> adapt to state changes in dynamic interfaces, <i>(ii)</i> combine crowd labeling with OCR technology to handle dynamic displays, and <i>(iii)</i> benefit from head-mounted cameras. VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision, and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone.",1,92.7672955975
UIST,27d378f1741eaaa414a0d26f28fd8fd08899c1b2,UIST,2002,Manipulating structured information in a visual workspace,"Hao-wei Hsieh, Frank M. Shipman","1822063, 1749811","This paper describes the VITE system, a visual workspace that supports two-way mapping for projecting structured information to a two-dimensional workspace and updating the structured information based on user interactions in the workspace. This is related to information visualization, but reflecting visual edits in the structured data requires a two-way mapping from data to visualization and from visualization to data. VITE provides users with an interface for designing two-way mappings. Mappings are reusable on different datasets and may be switched within a task. An evaluation of VITE was conducted to study how people use two-way mapping and how two-way mapping can help in problem solving tasks. The results show that users could quickly design visual mappings to help their problem-solving tasks. Users developed more sophisticated strategies for visual problem-solving over time.",15,20.8333333333
UIST,64a80a5775e58ef55f42c107a098bcc7052366d2,UIST,2010,Enabling social interactions through real-time sketch-based communication,"Nadir Weibel, Lisa G. Cowan, Laura R. Pina, William G. Griswold, James D. Hollan","1752711, 1974552, 1873428, 6980007, 1698170","We present UbiSketch, a tool for ubiquitous real-time sketch-based communication. We describe the UbiSketch system, which enables people to create doodles, drawings, and notes with digital pens and paper and publish them quickly and easily via their mobile phones to social communication channels, such as Facebook, Twitter, and email. The natural paper-based social interaction enabled by UbiSketch has the potential to enrich current mobile communication practices.",3,42.4418604651
UIST,8d94fbadb8fbc5d8641b5b7f4c48385fda49d3f5,UIST,2015,Self-Calibrating Head-Mounted Eye Trackers Using Egocentric Visual Saliency,"Yusuke Sugano, Andreas Bulling","1751242, 3194727","Head-mounted eye tracking has significant potential for gaze-based applications such as life logging, mental health monitoring, or the quantified self. A neglected challenge for the long-term recordings required by these applications is that drift in the initial person-specific eye tracker calibration, for example caused by physical activity, can severely impact gaze estimation accuracy and thus system performance and user experience. We first analyse calibration drift on a new dataset of natural gaze data recorded using synchronised video-based and Electrooculography-based eye trackers of 20 users performing everyday activities in a mobile setting. Based on this analysis we present a method to automatically self-calibrate head-mounted eye trackers based on a computational model of bottom-up visual saliency. Through evaluations on the dataset we show that our method 1) is effective in reducing calibration drift in calibrated eye trackers and 2) given sufficient data, can achieve gaze estimation accuracy competitive with that of a calibrated eye tracker, without any manual calibration.",5,80.701754386
UIST,7f7a05e9f68bbb23432328aed2b60972ec7c2a7f,UIST,2012,BallCam!: dynamic view synthesis from spinning cameras,"Kris Makoto Kitani, Kodai Horita, Hideki Koike","2067961, 2870864, 1684942","We are interested in generating novel video sequences from a ball's point of view for sports domains. Despite the challenge of extreme camera motion, we show that we can leverage the periodicity of spinning cameras to generate a stabilized ball point-of-view video. We present preliminary results of image stabilization and view synthesis from a single camera being hurled in the air at 600 RPM.",4,45.5882352941
UIST,14d51ad24d199340db5dd7fdeedcfa1b3d0f132d,UIST,2012,Pressages: augmenting phone calls with non-verbal messages,"Eve E. Hoggan, Craig D. Stewart, Laura Haverinen, Giulio Jacucci, Vuokko Lantz","1720793, 1709841, 2010869, 1741764, 3231701","ForcePhone is a mobile synchronous haptic communication system. During phone calls, users can squeeze the side of the device and the pressure level is mapped to vibrations on the recipient's device. The pressure/vibrotactile messages supported by ForcePhone are called <i>pressages</i>. Using a lab-based study and a small field study, this paper addresses the following questions: how can haptic interpersonal communication be integrated into a standard mobile device? What is the most appropriate feedback design for pressages? What types of non-verbal cues can be represented by pressages? Do users make use of pressages during their conversations? The results of this research indicate that such a system has value as a communication channel in real-world settings with users expressing greetings, presence and emotions through pressages.",19,75.4901960784
UIST,d29f2774e22c23ad1f6bad464f335ad3b3ec8f7c,UIST,2012,Carpus: a non-intrusive user identification technique for interactive surfaces,"Raf Ramakers, Davy Vanacken, Kris Luyten, Karin Coninx, Johannes Schöning","1950213, 3202396, 1681624, 1695519, 2070910","Interactive surfaces have great potential for co-located collaboration because of their ability to track multiple inputs simultaneously. However, the multi-user experience on these devices could be enriched significantly if touch points could be associated with a particular user. Existing approaches to user identification are intrusive, require users to stay in a fixed position, or suffer from poor accuracy. We present a non-intrusive, high-accuracy technique for mapping touches to their corresponding user in a collaborative environment. By mounting a high-resolution camera above the interactive surface, we are able to identify touches reliably without any extra instrumentation, and users are able to move around the surface at will. Our technique, which leverages the back of users' hands as identifiers, supports walk-up-and-use situations in which multiple people interact on a shared surface.",17,71.0784313725
UIST,3857d1647c6f542a9e5318e3502daf61d53cf688,UIST,2015,"Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection","Joshua Hibschman, Haoqi Zhang","2015651, 3162562","Professional websites with complex UI features provide real world examples for developers to learn from. Yet despite the availability of source code, it is still difficult to understand how these features are implemented. Existing tools such as the Chrome Developer Tools and Firebug offer debugging and inspection, but reverse engineering is still a time consuming task. We thus present Unravel, an extension of the Chrome Developer Tools for quickly tracking and visualizing HTML changes, JavaScript method calls, and JavaScript libraries. Unravel injects an observation agent into websites to monitor DOM interactions in real-time without functional interference or external dependencies. To manage potentially large observations of events, the Unravel UI provides affordances to reduce, sort, and scope observations. Testing Unravel with 13 web developers on 5 large-scale websites, we found a 53% decrease in time to discovering the first key source behind a UI feature and a 32% decrease in time to understanding how to fully recreate a feature.",3,71.0526315789
UIST,54504e382f258e5c724b50bdcda16b9256de1e33,UIST,2009,PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity,"Toshiki Sato, Haruko Mamiya, Hideki Koike, Kentaro Fukuchi","3269647, 2602082, 1684942, 2200842","PhotoelasticTouch is a novel tabletop system designed to intuitively facilitate touch-based interaction via real objects made from transparent elastic material. The system utilizes vision-based recognition techniques and the photoelastic properties of the transparent rubber to recognize deformed regions of the elastic material. Our system works with elastic materials over a wide variety of shapes and does not require any explicit visual markers. Compared to traditional interactive surfaces, our 2.5 dimensional interface system enables direct touch interaction and soft tactile feedback. In this paper we present our force sensing technique using photoelasticity and describe the implementation of our prototype system. We also present three practical applications of PhotoelasticTouch, a force-sensitive touch panel, a tangible face application, and a paint application.",33,57.1428571429
UIST,5d16e9df65d250c8b9c725a4c2d6f35cf6c811aa,UIST,2006,"Camera phone based motion sensing: interaction techniques, applications and performance study","Jingtao Wang, Shumin Zhai, John F. Canny","7899190, 1748079, 1729041","This paper presents <i>TinyMotion</i>, a pure software approach for detecting a mobile phone user's hand movement in real time by analyzing image sequences captured by the built-in camera. We present the design and implementation of <i>TinyMotion</i> and several interactive applications based on <i>TinyMotion</i>. Through both an informal evaluation and a formal 17-participant user study, we found that 1. <i>TinyMotion</i> can detect camera movement reliably under most background and illumination conditions. 2. Target acquisition tasks based on <i>TinyMotion</i> follow Fitts' law and Fitts law parameters can be used for <i>TinyMotion</i> based pointing performance measurement. 3. The users can use Vision TiltText, a <i>TinyMotion</i> enabled input method, to enter sentences faster than MultiTap with a few minutes of practicing. 4. Using camera phone as a handwriting capture device and performing large vocabulary, multilingual real time handwriting recognition on the cell phone are feasible. 5. <i>TinyMotion</i> based gaming is enjoyable and immediately available for the current generation camera phones. We also report user experiences and problems with <i>TinyMotion</i> based interaction as resources for future design and development of mobile interfaces.",89,97.5
UIST,8d1a57d92d708601348f2ad89d94f42f9446dfc2,UIST,2006,SwingStates: adding state machines to the swing toolkit,"Caroline Appert, Michel Beaudouin-Lafon","1802574, 1682346","This article describes <i>SwingStates</i>, a library that adds state machines to the Java Swing user interface toolkit. Unlike traditional approaches, which use callbacks or listeners to define interaction, state machines provide a powerful control structure and localize all of the interaction code in one place. <i>SwingStates</i> takes advantage of Java's inner classes, providing programmers with a natural syntax and making it easier to follow and debug the resulting code. <i>SwingStates</i> tightly integrates state machines, the Java language and the Swing toolkit. It reduces the potential for an explosion of states by allowing multiple state machines to work together. We show how to use <i>SwingStates</i> to add new interaction techniques to existing Swing widgets, to program a powerful new Canvas widget and to control high-level dialogues.",26,40.0
UIST,4b5e260bd31643360b2b9a0e153d1fbbc1205e53,UIST,2002,Dynamic approximation of complex graphical constraints by linear constraints,"Nathan Hurst, Kim Marriott, Peter Moulder","1755527, 1768695, 1794214","Current constraint solving techniques for interactive graphical applications cannot satisfactorily handle constraints such as non-overlap, or containment within non-convex shapes or shapes with smooth edges. We present a generic new technique for efficiently handling such kinds of constraints based on trust regions and linear arithmetic constraint solving. Our approach is to model these more complex constraints by a dynamically changing conjunction of linear constraints. At each stage, these give a local approximation to the complex constraints. During direct manipulation, linear constraints in the current local approximation can become <i>active</i> indicating that the current solution is on the boundary of the trust region for the approximation. The associated complex constraint is notified and it may choose to modify the current linear approximation. Empirical evaluation demonstrates that it is possible to (re-)solve systems of linear constraints that are dynamically approximating complex constraints such as non-overlap sufficiently quickly to support direct manipulation in interactive graphical applications.",7,16.6666666667
UIST,2759283a541d9d888acac251f2fb40fac638d191,UIST,2011,MARBLS: a visual environment for building clinical alert rules,"Dave Krebs, Alexander Conrad, Milos Hauskrecht, Jingtao Wang","2112743, 3137081, 1731761, 7899190","Physicians and nurses usually rely on hospital information systems (HIS) for detecting a variety of adverse clinical conditions and reminding repetitive treatments. However, the acquisition of alert rules expected by HIS from experts remains a challenging, error-prone, and time-consuming process. In this work, we present MARBLS (Medical Alert Rule BuiLding System) - a visual environment to facilitate the design and definition of clinical alert rules. MARBLS enables a two-way, synchronized visual rule workspace and visual query explorer. Monitoring rules can be built by manipulating block components in the rule workspace, by querying and generalizing region of interests in the visual query explorer via direct manipulations, or a combination of both. Informal testing with doctors has shown positive feedback.",2,24.2857142857
UIST,260208677f557e14753975b7db9686906d296088,UIST,2001,Haptic techniques for media control,"Scott S. Snibbe, Karon E. MacLean, Robert Shaw, Jayne Roderick, Bill Verplank, Mark Scheeff","3163335, 5658688, 2882560, 2623963, 3344074, 2565834","We introduce a set of techniques for haptically manipulating digital media such as video, audio, voicemail and computer graphics, utilizing virtual mediating dynamic models based on intuitive physical metaphors. For example, a video sequence can be modeled by linking its motion to a heavy spinning virtual wheel: the user browses by grasping a physical force-feedback knob and engaging the virtual wheel through a simulated clutch to spin or brake it, while feeling the passage of individual frames. These systems were implemented on a collection of single axis actuated displays (knobs and sliders), equipped with orthogonal force sensing to enhance their expressive potential. We demonstrate how continuous interaction through a haptically actuated device rather than discrete button and key presses can produce simple yet powerful tools that leverage physical intuition.",53,53.3333333333
UIST,16d2c3bdd7ca3030308b152d899ecf5e33909345,UIST,2009,Enabling always-available input with muscle-computer interfaces,"T. Scott Saponas, Desney S. Tan, Dan Morris, Ravin Balakrishnan, Jim Turner, James A. Landay","1766388, 1719056, 1779342, 1748870, 2794378, 1708404","Previous work has demonstrated the viability of applying offline analysis to interpret forearm electromyography (EMG) and classify finger gestures on a physical surface. We extend those results to bring us closer to using muscle-computer interfaces for always-available input in real-world applications. We leverage existing taxonomies of natural human grips to develop a gesture set covering interaction in free space even when hands are busy with other objects. We present a system that classifies these gestures in real-time and we introduce a bi-manual paradigm that enables use in interactive systems. We report experimental results demonstrating four-finger classification accuracies averaging 79% for pinching, 85% while holding a travel mug, and 88% when carrying a weighted bag. We further show generalizability across different arm postures and explore the tradeoffs of providing real-time visual feedback.",103,91.4285714286
UIST,26727a6a158a7827ab087505a0740f6955a352c2,UIST,2011,Calibration games: making calibration tasks enjoyable by adding motivating game elements,"David R. Flatla, Carl Gutwin, Lennart E. Nacke, Scott Bateman, Regan L. Mandryk","3247725, 1693768, 1953205, 1770919, 1788745","Interactive systems often require calibration to ensure that input and output are optimally configured. Without calibration, user performance can degrade (e.g., if an input device is not adjusted for the user's abilities), errors can increase (e.g., if color spaces are not matched), and some interactions may not be possible (e.g., use of an eye tracker). The value of calibration is often lost, however, because many calibration processes are tedious and unenjoyable, and many users avoid them altogether. To address this problem, we propose calibration games that gather calibration data in an engaging and entertaining manner. To facilitate the creation of calibration games, we present design guidelines that map common types of calibration to core tasks, and then to well-known game mechanics. To evaluate the approach, we developed three calibration games and compared them to standard procedures. Users found the game versions significantly more enjoyable than regular calibration procedures, without compromising the quality of the data. Calibration games are a novel way to motivate users to carry out calibrations, thereby improving the performance and accuracy of many human-computer systems.",47,87.619047619
UIST,b5dfbd00a582346b5d4fd03d35978e39295380c8,UIST,2016,LaserStroke: Mid-air Tactile Experiences on Contours Using Indirect Laser Radiation,"Hojin Lee, Hojun Cha, Junsuk Park, Seungmoon Choi, Hyung-Sik Kim, Soon-Cheol Chung","4881091, 3492273, 3058055, 1718126, 3274367, 2345586","This demonstration presents a novel form of mid-air tactile display, <i>LaserStroke</i>, that makes use of a laser irradiated on the elastic medium attached to the skin. LaserStroke extends a laser device with an orientation control platform and a magnetic tracker so that it can elicit tapping and stroking sensations to a user's palm from a distance. LaserStroke offers unique tactile experiences while a user freely moves his/her hand in midair.",0,44.6540880503
UIST,cfcb2b8f13e69489a8f7973fb66eb35ceca06ceb,UIST,2015,Tactile Animation by Direct Manipulation of Grid Displays,"Oliver S. Schneider, Ali Israr, Karon E. MacLean","1865182, 1769549, 5658688","Chairs, wearables, and handhelds have become popular sites for spatial tactile display. Visual animators, already expert in using time and space to portray motion, could readily transfer their skills to produce rich haptic sensations if given the right tools. We introduce the <i>tactile animation object</i>, a directly manipulated phantom tactile sensation. This abstraction has two key benefits: 1) efficient, creative, iterative control of spatiotemporal sensations, and 2) the potential to support a variety of tactile grids, including sparse displays. We present Mango, an editing tool for animators, including its rendering pipeline and perceptually-optimized interpolation algorithm for sparse vibrotactile grids. In our evaluation, professional animators found it easy to create a variety of vibrotactile patterns, with both experts and novices preferring the tactile animation object over controlling actuators individually.",5,80.701754386
UIST,4abe19ec7287801b20d8a9d3d9b6fe79348984df,UIST,2013,A touchless passive infrared gesture sensor,"Piotr Wojtczuk, T. David Binnie, Alistair Armitage, Tim Chamberlain, Carsten Giebeler","2049877, 2410168, 2208911, 2214654, 3276106","A sensing device for a touchless, hand gesture, user interface based on an inexpensive passive infrared pyroelectric detector array is presented. The 2 x 2 element sensor responds to changing infrared radiation generated by hand movement over the array. The sensing range is from a few millimetres to tens of centimetres. The low power consumption (< 50 &#956;W) enables the sensor's use in mobile devices and in low energy applications. Detection rates of 77% have been demonstrated using a prototype system that differentiates the four main hand motion trajectories -- up, down, left and right. This device allows greater non-contact control capability without an increase in size, cost or power consumption over existing on/off devices.",0,10.5504587156
UIST,8dedf22141c38aa84942d5b3f04afa4ba4bb9cf8,UIST,2010,A support to multi-devices web application,"Xavier Le Pallec, Raphaël Marvie, José Rouillard, Jean-Claude Tarby","1735785, 2974556, 1717701, 1717873","Programming an application which uses interactive devices located on different terminals is not easy. Programming such applications with standard Web technologies (HTTP, Javascript, Web browser) is even more difficult. However, Web applications have interesting properties like running on very different terminals, the lack of a specific installation step, the ability to evolve the application code at runtime. Our demonstration presents a support for designing multi-devices Web applications. After introducing the context of this work, we briefly describe some problems related to the design of multi-devices web application. Then, we present the toolkit we have implemented to help the development of applications based upon distant interactive devices.",2,33.1395348837
UIST,168f144380732938a905f3a7d09841dcf3deb27f,UIST,2016,Computational Design Driven by Aesthetic Preference,Yuki Koyama,2196816,"Tweaking design parameters is one of the most fundamental tasks in many design domains. In this paper, we describe three computational design methods for parameter tweaking tasks in which <i>aesthetic preference</i>---how aesthetically preferable the design looks---is used as a criterion to be maximized. The first method estimates a preference distribution in the target parameter space using crowdsourced human computation. The estimated preference distribution is then used in a design interface to facilitate interactive design exploration. The second method also estimates a preference distribution and uses it in an interface, but the distribution is estimated using the editing history of the target user. In contrast to these two methods, the third method automatically finds the best parameter that maximizes aesthetic preference, without requiring the user of this method to manually tweak parameters. This is enabled by implementing optimization algorithms using crowdsourced human computation. We validated these methods mainly in the scenario of photo color enhancement where parameters, such as brightness and contrast, need to be tweaked.",0,44.6540880503
UIST,dc89cc19816d9c14378c97b98f7d84b01b75b52a,UIST,2016,Eviza: A Natural Language Interface for Visual Analysis,"Vidya Setlur, Sarah E. Battersby, Melanie Tory, Rich Gossweiler, Angel X. Chang","1689165, 2015823, 2441204, 2513783, 3317599","Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query <i>dialog</i> with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches.
 The result of an interaction is a change to the view (<i>e.g.,</i> filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.",1,92.7672955975
UIST,93ad5a6a982d46855c947e7e99b21a92007f605b,UIST,2011,Tracking indoor location and motion for navigational assistance,"Nektarios Paisios, Alex Rubinsteyn, Lakshminarayanan Subramanian, Matt Tierney, Vrutti Vyas","3138681, 1717086, 1710917, 1769341, 2627131","Visually impaired people have a harder time remembering their way around complex unfamiliar buildings, whilst obtaining the help of a sighted guide is not always possible or desirable. By sensing the users location and motion, however, mobile phone software can provide navigational assistance in such situations, obviating the need of human guides. We present a simple to operate and highly usable mobile navigational guide that uses Wi-Fi and accelerometer sensors to help the user repeat paths that were already walked once.",1,15.7142857143
UIST,71a1cf751714413e4c11cacae9e64c32b2979a30,UIST,2016,Digital Gastronomy: Methods & Recipes for Hybrid Cooking,"Moran Mizrahi, Amos Golan, Ariel Bezaleli Mizrahi, Rotem Gruber, Alexander Zoonder Lachnise, Amit Zoran","3365083, 2619770, 3492151, 3492001, 3491499, 2866829","Several recent projects have introduced digital machines to the kitchen, yet their impact on culinary culture is limited. We envision a culture of Digital Gastronomy that enhances traditional cooking with new interactive capabilities, rather than replacing the chef with an autonomous machine. Thus, we deploy existing digital fabrication instruments in traditional kitchen and integrate them into cooking via hybrid recipes. This concept merges manual and digital procedures, and imports parametric design tools into cooking, allowing the chef to personalize the tastes, flavors, structures and aesthetics of dishes. In this paper we present our hybrid kitchen and the new cooking methodology, illustrated by detailed recipes with degrees of freedom that can be set digitally prior to cooking. Lastly, we discuss future work and conclude with thoughts on the future of hybrid gastronomy.",0,44.6540880503
UIST,99e8f2447754d4d7680e335467c9523cc045b105,UIST,2015,FoldMecha: Design for Linkage-Based Paper Toys,"Hyunjoo Oh, Mark D. Gross, Michael Eisenberg","3036652, 1700028, 1749923","We present <i>FoldMecha</i>, a computational tool to help non-experts design and build paper mechanical toys. By customizing templates a user can experiment with basic mechanisms, design their own model, print and cut out a folding net to construct the toy. We used the tool to build two kinds of paper automata models: walkers and flowers.",2,60.5263157895
UIST,29abc1ecca69ffec56990aa4c9c7e2712c3b1dc7,UIST,2011,Instrumenting the crowd: using implicit behavioral measures to predict task performance,"Jeffrey M. Rzeszotarski, Aniket Kittur","2494495, 1717650","Detecting and correcting low quality submissions in crowdsourcing tasks is an important challenge. Prior work has primarily focused on worker outcomes or reputation, using approaches such as agreement across workers or with a gold standard to evaluate quality. We propose an alternative and complementary technique that focuses on the way workers work rather than the products they produce. Our technique captures behavioral traces from online crowd workers and uses them to predict outcome measures such quality, errors, and the likelihood of cheating. We evaluate the effectiveness of the approach across three contexts including classification, generation, and comprehension tasks. The results indicate that we can build predictive models of task performance based on behavioral traces alone, and that these models generalize to related tasks. Finally, we discuss limitations and extensions of the approach.",52,90.0
UIST,1b75c493e089077209f6bafecf5f07115789415c,UIST,2012,Highly deformable interactive 3D surface display,"Noriyuki Aihara, Toshiki Sato, Hideki Koike","2180188, 3269647, 1684942","In this research, we focused on the flexibility limitation of a display material as one of the main causes for height con-straints in deformable surfaces. We propose a method that does not only utilize the material flexibility but also allows for increased variations of shapes and their corresponding interaction possibilities. Using this method, our proposed display design can then support additional expansion via protrusion of an air-pressure-controlled moldable display surface using a residual cloth-excess method and a fixed airbag mount.",4,45.5882352941
UIST,10b38e1103226acf35373082b22369fcac2d06ec,UIST,2003,Fluid interaction techniques for the control and annotation of digital video,"Gonzalo Ramos, Ravin Balakrishnan","1910455, 1748870","We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called <i>LEAN</i> that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called <i>twist-lens</i>, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen's pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.",87,66.6666666667
UIST,862c64c73e002a96e6daa158a3b3457147bd0863,UIST,2013,Panopticon: a parallel video overview system,"Daniel Jackson, James Nicholson, Gerrit Stoeckigt, Rebecca Wrobel, Anja Thieme, Patrick Olivier","3902081, 3222439, 3298346, 2450237, 7370481, 1707234","Panopticon is a video surrogate system that displays multiple sub-sequences in parallel to present a rapid overview of the entire sequence to the user. A novel, precisely animated arrangement slides thumbnails to provide a consistent spatiotemporal layout while allowing any sub-sequence of the original video to be watched without interruption. Furthermore, this output can be generated offline as a highly efficient repeated animation loop, making it suitable for resource-constrained environments, such as web-based interaction. Two versions of Panopticon were evaluated using three different types of video footage with the aim of determining the usability of the proposed system. Results demonstrated an advantage over another surrogate with surveillance footage in terms of search times and this advantage was further improved with Panopticon 2. Eye tracking data suggests that Panopticon's advantage stems from the animated timeline that users heavily rely on.",10,68.3486238532
UIST,f93b1f0d014bd781335005b69c3fd350598b889c,UIST,2016,"Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects","Xiang 'Anthony' Chen, Jeeeun Kim, Jennifer Mankoff, Tovi Grossman, Stelian Coros, Scott E. Hudson","2028468, 7950141, 3055754, 3313809, 1783776, 1749296","Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user, such as adding a bigger pull to a zipper or a larger grip for a pen. The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations. However, while 3D printers are now affordable enough for even home use, the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience.
 In this paper, we describe Reprise--a design tool for specifying, generating, customizing and fitting adaptations onto existing household objects. Reprise allows users to express at a high level what type of action is applied to an object. Based on this high level specification, Reprise automatically generates adaptations. Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences, such as increasing the tightness for gripping, enhancing torque for rotation, or making a larger base for stability. Finally, Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects.
 To validate our approach, we used Reprise to replicate 15 existing adaptation examples, each of which represents a specific category in a design space distilled from an analysis of over 3000 cases found in the literature and online communities. We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies.",2,98.427672956
UIST,96807124d7162756f8bed60e9bd75d40d56771c4,UIST,2014,Air+touch: interweaving touch & in-air gestures,"Xiang 'Anthony' Chen, Julia Schwarz, Chris Harrison, Jennifer Mankoff, Scott E. Hudson","2028468, 2251233, 1730920, 3055754, 1749296","We present Air+Touch, a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events. For example, a user can draw a circle in the air and tap to trigger a context menu, do a finger 'high jump' between two touches to select a region of text, or drag and in-air 'pigtail' to copy text to the clipboard. Through an observational study, we devised a basic taxonomy of Air+Touch interactions, based on whether the in-air component occurs before, between or after touches. To illustrate the potential of our approach, we built four applications that showcase seven exemplar Air+Touch interactions we created.",14,86.8217054264
UIST,6337bb70c393627d8f49da4681b763b3ddb09d7d,UIST,2011,"Monte carlo methods for managing interactive state, action and feedback under uncertainty","Julia Schwarz, Jennifer Mankoff, Scott E. Hudson","2251233, 3055754, 1749296","Current input handling systems provide effective techniques for modeling, tracking, interpreting, and acting on user input. However, new interaction technologies violate the standard assumption that input is certain. Touch, speech recognition, gestural input, and sensors for context often produce uncertain estimates of user inputs. Current systems tend to remove uncertainty early on. However, information available in the user interface and application can help to resolve uncertainty more appropriately for the end user. This paper presents a set of techniques for tracking the state of interactive objects in the presence of uncertain inputs. These techniques use a Monte Carlo approach to maintain a probabilistically accurate description of the user interface that can be used to make informed choices about actions. Samples are used to approximate the distribution of possible inputs, possible interactor states that result from inputs, and possible actions (callbacks and feedback) interactors may execute. Because each sample is certain, the developer can specify most of the behavior of interactors in a familiar, non-probabilistic fashion. This approach retains all the advantages of maintaining information about uncertainty while minimizing the need for the developer to work in probabilistic terms. We present a working implementation of our framework and illustrate the power of these techniques within a paint program that includes three different kinds of uncertain input.",19,66.6666666667
UIST,19cfe0af02b2bf11a0b1b6ad3ef32d1d2dc44954,UIST,2010,A framework for robust and flexible handling of inputs with uncertainty,"Julia Schwarz, Scott E. Hudson, Jennifer Mankoff, Andrew D. Wilson","2251233, 1749296, 3055754, 1767449","New input technologies (such as touch), recognition based input (such as pen gestures) and next-generation interactions (such as inexact interaction) all hold the promise of more natural user interfaces. However, these techniques all create inputs with some uncertainty. Unfortunately, conventional infrastructure lacks a method for easily handling uncertainty, and as a result input produced by these technologies is often converted to conventional events as quickly as possible, leading to a stunted interactive experience. We present a framework for handling input with uncertainty in a systematic, extensible, and easy to manipulate fashion. To illustrate this framework, we present several traditional interactors which have been extended to provide feedback about uncertain inputs and to allow for the possibility that in the end that input will be judged wrong (or end up going to a different interactor). Our six demonstrations include tiny buttons that are manipulable using touch input, a text box that can handle multiple interpretations of spoken input, a scrollbar that can respond to inexactly placed input, and buttons which are easier to click for people with motor impairments. Our framework supports all of these interactions by carrying uncertainty forward all the way through selection of possible target interactors, interpretation by interactors, generation of (uncertain) candidate actions to take, and a mediation process that decides (in a lazy fashion) which actions should become final.",23,77.9069767442
UIST,e045b2da7927caae66d8ccc8c80570fce938e900,UIST,2007,Dirty desktops: using a patina of magnetic mouse dust to make common interactor targets easier to select,"Amy Hurst, Jennifer Mankoff, Anind K. Dey, Scott E. Hudson","1759649, 3055754, 1703700, 1749296","A common task in graphical user interfaces is controlling onscreen elements using a pointer. Current adaptive pointing techniques require applications to be built using accessibility libraries that reveal information about interactive targets, and most do not handle path/menu navigation. We present a pseudo-haptic technique that is OS and application independent, and can handle both dragging and clicking. We do this by associating a small force with each past click or drag. When a user frequently clicks in the same general area (e.g., on a button), the patina of past clicks naturally creates a pseudo-haptic magnetic field with an effect similar to that ofsnapping or sticky icons. Our contribution is a bottom-up approach to make targets easier to select without requiring prior knowledge of them.",20,27.7777777778
UIST,68f482ad0b9c77cfc22b3da3b82a89225c0908c5,UIST,2000,A programming model for active documents,"Paul Dourish, W. Keith Edwards, Jon Howell, Anthony LaMarca, John Lamping, Karin Petersen, Michael Salisbury, Douglas B. Terry, James D. Thornton","1762952, 3023379, 2268930, 7871341, 2905809, 5186930, 2634335, 1680763, 2473625","Traditionally, designers organize software system as active end-points (e.g. applications) linked by passive infrastruc-tures (e.g. networks). Increasingly, however, networks and infrastructures are becoming active components that contribute directly to application behavior. Amongst the various problems that this presents is the question of how such active infrastructures should be programmed. We have been developing an active document management system called Placeless Documents. Its programming model is organized in terms of properties that actively contribute to the functionality and behavior of the documents to which they are attached. This paper discusses active properties and their use as a programming model for active infrastructures. We have found that active properties enable the creation of persistent, autonomous active entities in document systems, independent of specific repositories and applications, but present challenges for managing problems of composition.",15,20.0
UIST,e1cd774ef8b2f7ff469d95b7b940952eb623c8c3,UIST,2016,Dynamic Authoring of Audio with Linked Scripts,"Hijung Shin, Wilmot Li, Frédo Durand","2596714, 2812691, 1728125","Speech recordings are central to modern media from podcasts to audio books to e-lectures and voice-overs. Authoring these recordings involves an iterative back and forth process between script writing/editing and audio recording/editing. Yet, most existing tools treat the script and the audio separately, making the back and forth workflow very tedious. We present <i>Voice Script</i>, an interface to support a dynamic workflow for script writing and audio recording/editing. Our system integrates the script with the audio such that, as the user writes the script or records speech, edits to the script are translated to the audio and vice versa. Through informal user studies, we demonstrate that our interface greatly facilitates the audio authoring process in various scenarios.",0,44.6540880503
UIST,d78cd2423c500159bb158305e9a6a5fd5d6b643a,UIST,2006,"Rapid construction of functioning physical interfaces from cardboard, thumbtacks, tin foil and masking tape","Scott E. Hudson, Jennifer Mankoff","1749296, 3055754","Rapid, early, but rough system prototypes are becoming a standard and valued part of the user interface design process. Pen, paper, and tools like Flash&#8482; and Director&#8482; are well suited to creating such prototypes. However, in the case of physical forms with embedded technology, there is a lack of tools for developing rapid, early prototypes. Instead, the process tends to be fragmented into prototypes exploring forms that <i>look like</i> the intended product or explorations of functioning interactions that <i>work like</i> the intended product - bringing these aspects together into full design concepts only later in the design process. To help alleviate this problem, we present a simple tool for very rapidly creating functioning, rough physical prototypes early in the design process - supporting what amounts to interactive physical sketching. Our tool allows a designer to combine exploration of form and interactive function, using objects constructed from materials such as thumbtacks, foil, cardboard and masking tape, enhanced with a small electronic sensor board. By means of a simple and fluid tool for delivering events to ""screen clippings,"" these physical sketches can then be easily connected to any existing (or new) program running on a PC to provide real or Wizard of Oz supported functionality.",47,75.0
UIST,2ea61f2b8ecc3ba9b35f050bfd6f2b00bddff188,UIST,2004,An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces,"Michelle X. Zhou, Vikram Aggarwal","1705742, 3070134","We are building a multimedia conversation system to facilitate information seeking in large and complex data spaces. To provide tailored responses to diverse user queries introduced during a conversation, we automate the generation of a system response. Here we focus on the problem of determining the data content of a response. Specifically, we develop an optimization-based approach to content selection. Compared to existing rule-based or plan-based approaches, our work offers three unique contributions. First, our approach provides a general framework that effectively addresses content selection for various interaction situations by balancing a comprehensive set of constraints (e.g., content quality and quantity constraints). Second, our method is easily extensible, since it uses feature-based metrics to systematically model selection constraints. Third, our method improves selection results by incorporating content organization and media allocation effects, which otherwise are treated separately. Preliminary studies show that our method can handle most of the user situations identified in a Wizard-of-Oz study, and achieves results similar to those produced by human designers.",15,21.0526315789
UIST,8ed66b0e9753618d13b28b9943845f3b5d5b8751,UIST,2004,A toolkit for managing user attention in peripheral displays,"Tara Matthews, Anind K. Dey, Jennifer Mankoff, Scott Carter, Tye Rattenbury","2774046, 1703700, 3055754, 1804229, 3332698","Traditionally, computer interfaces have been confined to conventional displays and focused activities. However, as displays become embedded throughout our environment and daily lives, increasing numbers of them must operate on the periphery of our attention. &#60;i>Peripheral displays&#60;/i> can allow a person to be aware of information while she is attending to some other primary task or activity. We present the Peripheral Displays Toolkit (PTK), a toolkit that provides structured support for managing user attention in the development of peripheral displays. Our goal is to enable designers to explore different approaches to managing user attention. The PTK supports three issues specific to conveying information on the periphery of human attention. These issues are &#60;i>abstraction&#60;/i> of raw input, rules for assigning &#60;i>notification levels&#60;/i> to input, and &#60;i>transitions&#60;/i> for updating a display when input arrives. Our contribution is the investigation of issues specific to attention in peripheral display design and a toolkit that encapsulates support for these issues. We describe our toolkit architecture and present five sample peripheral displays demonstrating our toolkit's capabilities.",83,84.2105263158
UIST,92f90fe7da5d5067f296950f1fbe50014a3d427f,UIST,2002,Distributed mediation of ambiguous context in aware environments,"Anind K. Dey, Jennifer Mankoff, Gregory D. Abowd, Scott Carter","1703700, 3055754, 1732524, 1804229","Many context-aware services make the assumption that the context they use is completely accurate. However, in reality, both sensed and interpreted context is often ambiguous. A challenge facing the development of realistic and deployable context-aware services, therefore, is the ability to handle ambiguous context. In this paper, we describe an architecture that supports the building of context-aware services that assume context is ambiguous and allows for mediation of ambiguity by mobile users in aware environments. We illustrate the use of our architecture and evaluate it through three example context-aware services, a word predictor system, an In/Out Board, and a reminder tool.",64,70.8333333333
UIST,998fb2d0b9ed0648f22338bf691f6ca815cff32d,UIST,2012,Synchrum: a tangible interface for rhythmic collaboration,"Foad Hamidi, Melanie Baljko, Alexander Moakler, Assaf Gadot","2829268, 1943448, 3148811, 3028891","<i>Synchrum</i> is a tangible interface, inspired by the Tibetan prayer wheel, for audience participation and collaboration during digital performance. It engages audience members in effortful interaction, where they have to rotate the device in accord with a given rotation speed. We used synchrum in a video installation and report our observations.",0,9.80392156863
UIST,e118b95747963137fb88582d05e7a6ee5f7a87cb,UIST,2013,Sensor design and interaction techniques for gestural input to smart glasses and mobile devices,Andrea Colaco,4716385,"Touchscreen interfaces for small display devices have several limitations: the act of touching the screen occludes the display, interface elements like keyboards consume precious display real estate, and even simple tasks like document navigation - which the user performs effortlessly using a mouse and keyboard - require repeated actions like pinch-and-zoom with touch input. More recently, smart glasses with limited or no touch input are starting to emerge commercially. However, the primary input to these systems has been voice.
 In this paper, we explore the space around the device as a means of touchless gestural input to devices with small or no displays. Capturing gestural input in the surrounding volume requires sensing the human hand. To achieve gestural input we have built Mime [3] -- a compact, low-power 3D sensor for short-range gestural control of small display devices. Our sensor is based on a novel signal processing pipeline and is built using standard off-the-shelf components. Using Mime we demonstrated a variety of application scenarios including 3D spatial input using close-range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions. In my thesis, I will continue to extend sensor capabilities to support new interaction styles.",2,37.6146788991
UIST,f48c3ce6e0f415cde1469ef68d61f1afddaa0015,UIST,2012,"For novices playing music together, adding structural constraints leads to better music and may improve user experience","Luke Dahl, Sébastien Robaszkiewicz","1772442, 3068383","We investigate the effects of adding structure to musical interactions for novices. A simple instrument allows control of three musical parameters: pitch, timbre, and note density. Two users can play at once, and their actions are visible on a public display. We asked pairs of users to perform duets under two interaction conditions: <i>unstructured</i>, where users are free to play what they like, and <i>structured</i>, where users are directed to different areas of the musical parameter space by time-varying constraints indicated on the display. A control group played two duets without structure, while an experimental group played one duet with structure and a second without. By crowd-sourcing the ranking of recorded duets we find that structure leads to musically better results. A post experiment survey showed that the experimental group had a better experience during the second unstructured duet than during the structured.",0,9.80392156863
UIST,610434e224ccf7bb9c8f39a93d30f51b315517c7,UIST,2014,Hover Pad: interacting with autonomous and self-actuated displays in space,"Julian Seifert, Sebastian Boring, Christian Winkler, Florian Schaub, Fabian Schwab, Steffen Herrdum, Fabian Maier, Daniel Mayer, Enrico Rukzio","2890971, 1741219, 1700240, 1794574, 2525587, 3011625, 2650237, 2960417, 2021950","Handheld displays enable flexible spatial exploration of information spaces -- users can physically navigate through three-dimensional space to access information at specific locations. Having users constantly hold the display, however, has several limitations: (1) inaccuracies due to natural hand tremors; (2) fatigue over time; and (3) limited exploration within arm's reach. We investigate autonomous, self-actuated displays that can freely move and hold their position and orientation in space without users having to hold them at all times. We illustrate various stages of such a display's autonomy ranging from manual to fully autonomous, which -- depending on the tasks -- facilitate the interaction. Further, we discuss possible motion control mechanisms for these displays and present several interaction techniques enabled by such displays. Our Hover Pad toolkit enables exploring five degrees of freedom of self-actuated and autonomous displays and the developed control and interaction techniques. We illustrate the utility of our toolkit with five prototype applications, such as a volumetric medical data explorer.",6,67.8294573643
UIST,da1b85caa894166a17c77ce38d4e062fb28318e7,UIST,2005,Supporting interspecies social awareness: using peripheral displays for distributed pack awareness,"Demi Mankoff, Anind K. Dey, Jennifer Mankoff, Ken Mankoff","2237466, 1703700, 3055754, 2033152","In interspecies households, it is common for the non <i>homo sapien</i> members to be isolated and ignored for many hours each day when humans are out of the house or working. For pack animals, such as canines, information about a pack member's extended pack interactions (outside of the nuclear household) could help to mitigate this social isolation. We have developed a Pack Activity Watch System: Allowing Broad Interspecies Love In Telecommunication with Internet-Enabled Sociability (PAWSABILITIES) for helping to support remote awareness of social activities. Our work focuses on canine companions, and includes, pawticipatory design, <i>lab</i>radory tests, and <i>canid</i> camera monitoring.",23,35.4838709677
UIST,97a843de5d1b884ac7907f75bbd2866e522c741a,UIST,2016,Authoring Illustrations of Human Movements by Iterative Physical Demonstration,"Pei-Yu Chi, Daniel Vogel, Mira Dontcheva, Wilmot Li, Björn Hartmann","2442421, 1913655, 2875493, 2812691, 4020023","Illustrations of human movements are used to communicate ideas and convey instructions in many domains, but creating them is time-consuming and requires skill. We introduce DemoDraw, a multi-modal approach to generate these illustrations as the user physically demonstrates the movements. In a Demonstration Interface, DemoDraw segments speech and 3D joint motion into a sequence of motion segments, each characterized by a key pose and salient joint trajectories. Based on this sequence, a series of illustrations is automatically generated using a stylistically rendered 3D avatar annotated with arrows to convey movements. During demonstration, the user can navigate using speech and amend or re-perform motions if needed. Once a suitable sequence of steps has been created, a Refinement Interface enables fine control of visualization parameters. In a three-part evaluation, we validate the effectiveness of the generated illustrations and the usability of DemoDraw. Our results show 4 to 7-step illustrations can be created in 5 or 10 minutes on average.",0,44.6540880503
UIST,b7c521b99240a47af65813747f9102de6f4a8255,UIST,2012,Sawtooth planar waves for haptic feedback,Joseph Kaye,1725629,"Current touchscreen technology does not provide adequate haptic feedback to the user. Mostly haptic feedback solutions for touchscreens involve either a) deforming the surface layers screen itself or b) placing actuators under the screen to vibrate it. This means that we have only limited control over where on the screen the feedback feels like it is coming from, and that we are limited to feedback that feels like movement up and down, orthogonal to the screen. In this work I demonstrate a novel technique for haptic feedback: sawtooth planar waves. In a series of paper Canny &#38; Reznick showed that sawtooth planar waves could be used for object manipulation. Here that technique is applied to haptic feedback. By varying the input waves, from 1 one to 4 actuators, it is possible to provide feelings of motion in any planar direction to a finger at one point on the screen while providing a different sensation, or none at all, to fingers placed at several other points on the screen.",1,25.0
UIST,3a56992f2f7849b3b3d959f3442cd3913d17ad71,UIST,2016,A Tangible Interface to Realize Touch Operations on the Face of a Physical Object,"Saraha Ueno, Kunihiro Kato, Homei Miyashita","3492821, 2211261, 1796760","In this paper, we describe a tangible interface that can realize touch operations on a physical object. We printed physical objects that have conductive striped patterns using a multi-material 3D printer. The ExtensionSticker technique allows the user to operate capacitive touch-panel devices by tapping, scrolling, and swiping the physical object. By shaping the structure of conductive wiring inside a physical object, a variety of interfaces can be realized. We examined the conditions for using our proposed method on touch-panel devices.",0,44.6540880503
UIST,4a2d856bf893f15c68c2f3664ae6790c722d3ed6,UIST,2005,Zliding: fluid zooming and sliding for high precision parameter manipulation,"Gonzalo Ramos, Ravin Balakrishnan","1910455, 1748870","High precision parameter manipulation tasks typically require adjustment of the scale of manipulation in addition to the parameter itself. This paper introduces the notion of Zoom Sliding, or Zliding, for fluid integrated manipulation of scale (zooming) via pressure input while parameter manipulation within that scale is achieved via x-y cursor movement (sliding). We also present the Zlider (Figure 1), a widget that instantiates the Zliding concept. We experimentally evaluate three different input techniques for use with the Zlider in conjunction with a stylus for x-y cursor positioning, in a high accuracy zoom and select task. Our results marginally favor the stylus with integrated isometric pressure sensing tip over bimanual techniques which separate zooming and sliding controls over the two hands. We discuss the implications of our results and present further designs that make use of Zliding.",68,79.0322580645
UIST,0b5a7d52224ab301b7c3d458d8392e8006984d6d,UIST,2013,WebNexter: dynamic guided tours for screen readers,"Prathik Gadde, Davide Bolchini","1805501, 1780806","Recent research has shown that screen-reader users can find information on a website almost twice as fast if they bypass indexes and just navigate the content pages of a collection linearly (in a guided-tour fashion). Yet manually building a guided tour for each existing index requires significant resources from web developers, especially for very large web applications. To address this problem, we introduce WebNexter, a web browser extension that automatically generates guided tours from the indexes present in the page a screen-reader user is currently visiting. WebNexter is manifest in a Google Chrome extension that implements screen-reader accessible, dynamic construction of guided tours from a very large, eCommerce website prototype. Our goal is to develop WebNexter extensions for multiple browsers that will work on any website; this will relieve developers from the burden of designing guided tours while greatly accelerating the screen-reader navigation experience during fact-finding.",1,27.0642201835
UIST,341530b6e9f08d696520bedef585da57f960f574,UIST,2014,LiveSphere: immersive experience sharing with 360 degrees head-mounted cameras,"Shunichi Kasahara, Shohei Nagai, Jun Rekimoto","2622335, 2892489, 1685962","Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose LiveSphere, immersive experience sharing system with wearable camera headgear that provide 360 degrees spherical images of the user's surrounding environment. LiveSphere system performs spherical video stabilization and transmits it to other users, so that they are enable to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the LiveSphere system implementation, stabilization and viewing experience.",4,57.3643410853
UIST,5894d31be860eb0f1bec24aa48a456462585cace,UIST,2011,RhythmLink: securely pairing I/O-constrained devices by tapping,"Felix Xiaozhu Lin, Daniel Ashbrook, Sean White","1774176, 2071682, 5047258","We present RhythmLink, a system that improves the wireless pairing user experience. Users can link devices such as phones and headsets together by tapping a known rhythm on each device. In contrast to current solutions, RhythmLink does not require user interaction with the host device during the pairing process; and it only requires binary input on the peripheral, making it appropriate for small devices with minimal physical affordances. We describe the challenges in enabling this user experience and our solution, an algorithm that allows two devices to compare imprecisely-entered tap sequences while maintaining the secrecy of those sequences. We also discuss our prototype implementation of RhythmLink and review the results of initial user tests.",6,38.5714285714
UIST,020f76e207dc023fea856987add3b7fee7f9c44a,UIST,2012,Designing for low-latency direct-touch input,"Albert Ng, G. Julian Lepinski, Daniel J. Wigdor, Steven Sanders, Paul Dietz","3253272, 3002110, 1961958, 2797538, 5943041","Software designed for direct-touch interfaces often utilize a metaphor of direct physical manipulation of pseudo ""real-world"" objects. However, current touch systems typically take 50-200ms to update the display in response to a physi-cal touch action. Utilizing a high performance touch de-monstrator, subjects were able to experience touch latencies ranging from current levels down to about 1ms. Our tests show that users greatly prefer lower latencies, and noticea-ble improvement continued well below 10ms. This level of performance is difficult to achieve in commercial compu-ting systems using current technologies. As an alternative, we propose a hybrid system that provides low-fidelity visu-al feedback immediately, followed by high-fidelity visuals at standard levels of latency.",38,91.6666666667
UIST,077f162b35738dc548318cb06b298d734154e85d,UIST,2004,Simple vs. compound mark hierarchical marking menus,"Shengdong Zhao, Ravin Balakrishnan","2645457, 1748870","We present a variant of hierarchical marking menus where items are selected using a series of inflection-free simple marks, rather than the single ""zig-zag"" compound mark used in the traditional design. Theoretical analysis indicates that this simple mark approach has the potential to significantly increase the number of items in a marking menu that can be selected efficiently and accurately. A user experiment is presented that compares the simple and compound mark techniques. Results show that the simple mark technique allows for significantly more accurate and faster menu selections overall, but most importantly also in menus with a large number of items where performance of the compound mark technique is particularly poor. The simple mark technique also requires significantly less physical input space to perform the selections, making it particularly suitable for small footprint pen-based input devices. Visual design alternatives are also discussed.",67,71.0526315789
UIST,544ac3904d04e5d277fafa9b2b743dcd2b35d83e,UIST,1991,Stylus user interfaces for manipulating text,"David Goldberg, Aaron Goodisman","8409816, 2686642","This paper is concerned with pen-based (also called stylus-based) computers. Two of the key questions for such computers are how to interface to handwriting recognition algorithms, and whether there are interfaces that can effect ively exploit the differences between a stylus and a key-board/mouse. We describe prototypes that explore each of these questions. Our text entry tool is designed around the idea that handwriting recognition algorithms will always be error prone, and has a different flavor from existing systems. Our prototype editor goes beyond the usual gesture editors used with styli and is based on the idea of leaving the markups visible. 1 Introduction Common experience with keyboards and pens suggest that keyboards are optimized for text entry, and pens for drawing. With the appearance of small, portable pen-based computers such as the GridPad, whose primary input device is a stylus (i.e. an electronic pen) used directly on a display, we are faced with the problem of how to use a stylus to manipulate text. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permiaaion of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. aspects of this problem. First, what is a good user interface to a handwriting recognize? There have been many papers on handwriting recognition (see [11] for a survey), but very few on user interfaces to recognizes (the most completely described interface is PenPoint [3]). Our system is based on the philosophy that recognizes will always make errors, and that the interface must be designed from the beginning to accommodate that fact. This results in a subst an-tially different interface from PenPoint (and was developed before the publication of [3]). The second problem is this: can we find an int er-face that exploits the differences between a stylus and a mouse/keyboard? If we cannot, then there is a real question as to whether pen-based computers are viable, since text entry is slower and more error prone with a stylus than with a keyboard, and since it is feasible to build a notebook sized pen-based computer with a keyboard and thumb operated trackball with similar functionality to a mouse (such …",40,67.3913043478
UIST,07ebc80b442f4e8d324731a1496fe01adfb05b2f,UIST,2011,Composition for conductor and audience: new uses for mobile devices in the concert hall,"Charles Roberts, Tobias Höllerer","6114171, 1743721","Composition for Conductor and Audience is an audience interaction piece first performed for an audience of over seventy-five people in June of 2011. The audience becomes the orchestra in this composition as they control different musical variables using the touchscreen surfaces on their personal mobile devices. To the authors' knowledge this is the first concert piece for bi-directional networked interactivity on audience-owned mobile devices to ever be performed. Audience members participated using the iOS / Android application 'Control', a generic solution for creating touchscreen interfaces written by the first author. Over twenty members of the audience participated in the performance, using their personal devices to match gestures made by the conductor with corresponding gestures on their mobile devices.",10,46.6666666667
UIST,7f0313d96425f800acd0f6227f05c0a5632a1e02,UIST,2015,Capacitive Blocks: A Block System that Connects the Physical with the Virtual using Changes of Capacitance,"Arika Yoshida, Buntarou Shizuki, Jiro Tanaka","2015617, 1765222, 1726327","We propose a block-stacking system based on capacitance. The system, called Capacitive Blocks, allows users to build 3D models in a virtual space by stacking physical blocks. The construction of the block-stacking system is simple, and fundamental components including physical blocks can be made with a 3D printer. The block is a capacitor that consists of two layers made of conductive plastic filament and between them a layer made of non-conductive plastic filament. In this paper, we present a prototype of this block-stacking system and the mechanism that detects the height of blocks (i.e., the number of stacked blocks) by measuring the capacitance of the stacked blocks, which changes in accordance with the number of stacked blocks.",1,42.1052631579
UIST,69c3d7c885da475fd96b5b2b6ca7f1b84cdc57d8,UIST,2015,Orbits: Gaze Interaction for Smart Watches using Smooth Pursuit Eye Movements,"Augusto Esteves, Eduardo Velloso, Andreas Bulling, Hans-Werner Gellersen","1757267, 2520424, 3194727, 4919595","We introduce Orbits, a novel gaze interaction technique that enables hands-free input on smart watches. The technique relies on moving controls to leverage the smooth pursuit movements of the eyes and detect whether and at which control the user is looking at. In Orbits, controls include targets that move in a circular trajectory in the face of the watch, and can be selected by following the desired one for a small amount of time. We conducted two user studies to assess the technique's recognition and robustness, which demonstrated how Orbits is robust against false positives triggered by natural eye movements and how it presents a hands-free, high accuracy way of interacting with smart watches using off-the-shelf devices. Finally, we developed three example interfaces built with Orbits: a music player, a notifications face plate and a missed call menu. Despite relying on moving controls -- very unusual in current HCI interfaces -- these were generally well received by participants in a third and final study.",17,100.0
UIST,0ca6ba6ef10fccde0e69f8aaac45f5291456abb4,UIST,2003,Automatic thumbnail cropping and its effectiveness,"Bongwon Suh, Haibin Ling, Benjamin B. Bederson, David W. Jacobs","3155504, 1805398, 1799187, 1771485","Thumbnail images provide users of image retrieval and browsing systems with a method for quickly scanning large numbers of images. Recognizing the objects in an image is important in many retrieval tasks, but thumbnails generated by shrinking the original image often render objects illegible. We study the ability of computer vision systems to detect key components of images so that automated cropping, prior to shrinking, can render objects more recognizable. We evaluate automatic cropping techniques 1) based on a general method that detects salient portions of images, and 2) based on automatic face detection. Our user study shows that these methods result in small thumbnails that are substantially more recognizable and easier to find in the context of visual search.",183,95.8333333333
UIST,a68cb3bd374a587585482ffa9c39588f5d6aab66,UIST,2016,TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device,"Sang Ho Yoon, Yunbo Zhang, Ke Huo, Karthik Ramani","6566004, 8700575, 1989590, 1766368","We present <i>TRing</i>, a finger-worn input device which provides instant and customizable interactions. <i>TRing</i> offers a novel method for making plain objects interactive using an embedded magnet and a finger-worn device. With a particle filter integrated magnetic sensing technique, we compute the fingertip's position relative to the embedded magnet. We also offer a magnet placement algorithm that guides the magnet installation location based upon the user's interface customization. By simply inserting or attaching a small magnet, we bring interactivity to both fabricated and existing objects. In our evaluations, <i>TRing</i> shows an average tracking error of 8.6 mm in 3D space and a 2D targeting error of 4.96 mm, which are sufficient for implementing average-sized conventional controls such as buttons and sliders. A user study validates the input performance with <i>TRing</i> on a targeting task (92% accuracy within 45 mm distance) and a cursor control task (91% accuracy for a 10 mm target). Furthermore, we show examples that highlight the interaction capability of our approach.",0,44.6540880503
UIST,d3d63d2db8f49c5ae659e3c7e083c7dada8508e9,UIST,2016,An Input Switching Interface Using Carbon Copy Metaphor,"Kaori Ikematsu, Itiro Siio","2645997, 1709079","This paper proposes a novel input technique that aims to switch between relative and absolute coordinates input methods seamlessly based on the ""carbon copy"" metaphor. We display a small workspace (``carbon copy area'') on a computer screen that corresponds one-to-one with the handy trackpad. The user can input hand-written characters or images using absolute coordinates input on this virtual carbon copy paper and move it anywhere using relative coordinates. Our technique allows a user to call both absolute and relative coordinates input methods and use them appropriately with arbitrary timing. Several advantages can be obtained by combining these methods. We developed a desktop application software to utilize this technique in a real GUI environment based on a user's evaluation.",0,44.6540880503
UIST,456052f7136b650c42347a7d0ce65e00a6047cab,UIST,2015,TMotion: Embedded 3D Mobile Input using Magnetic Sensing Technique,"Sang Ho Yoon, Ke Huo, Karthik Ramani","6566004, 1989590, 1766368","We present TMotion, a self-contained 3D input that enables spatial interactions around mobile using a magnetic sensing technique. Using a single magnetometer from the mobile device, we can track the 3D position of the permanent magnet embedded in the prototype along with an inertial measurement unit. By numerically solving non-linear magnetic field equations with known orientation from inertial measurement unit (IMU), we attain a tracking rate greater than 30Hz based solely on the mobile device computation. We describe the working principle of TMotion and example applications illustrating its capability.",4,75.4385964912
UIST,e44795c86a531e611b4b80d89533a1ad70c38cc8,UIST,2016,Virtual Sweet: Simulating Sweet Sensation Using Thermal Stimulation on the Tip of the Tongue,"Nimesha Ranasinghe, Ellen Yi-Luen Do","1722792, 1689168","Being a pleasurable sensation, sweetness is recognized as the most preferred sensation among the five primary taste sensations. In this paper, we present a novel method to virtually simulate the sensation of sweetness by applying thermal stimulation to the tip of the human tongue. To digitally simulate the sensation of sweetness, the system delivers rapid heating and cooling stimuli to the tongue via a 2x2 grid of Peltier elements. To achieve distinct, controlled, and synchronized temperature variations in the stimuli, a control module is used to regulate each of the Peltier elements. Results from our preliminary experiments suggest that the participants were able to perceive mild sweetness on the tip of their tongue while using the proposed system.",0,44.6540880503
UIST,f72a4ecefb8672ecbde2dc4f3abf3e6a196387f2,UIST,2014,Digital flavor interface,"Nimesha Ranasinghe, Gajan Suthokumar, Kuan-Yi Lee, Ellen Yi-Luen Do","1722792, 2304561, 2403479, 1689168","This demo presents a unique technology to enable digital simulation of flavors. The Digital Flavor Interface, a digital control system, is developed to stimulate taste (using electrical and thermal stimulation methodologies on the human tongue) and smell (using a controlled scent emitting mechanism) senses simultaneously, thus simulating different virtual flavors. A preliminary user experiment was conducted to investigate the effectiveness of this approach with five distinct flavor stimuli. The experimental results suggested that the users' were effectively able to identify different flavors such as minty, spicy, and lemon flavor. In summary, our work demonstrates a novel controllable digital flavor instrument, which may be utilized in interactive computer systems for rendering virtual flavors.",2,41.4728682171
UIST,0ec5d483b6da0bb1eae7ab7c72f2a24124a8610e,UIST,2016,The Toastboard: Ubiquitous Instrumentation and Automated Checking of Breadboarded Circuits,"Daniel Drew, Julie L. Newcomb, William McGrath, Filip Maksimovic, David Mellis, Björn Hartmann","3491918, 3492163, 3342564, 3491609, 2643525, 4020023","The recent proliferation of easy to use electronic components and toolkits has introduced a large number of novices to designing and building electronic projects. Nevertheless, debugging circuits remains a difficult and time-consuming task. This paper presents a novel debugging tool for electronic design projects, the Toastboard, that aims to reduce debugging time by improving upon the standard paradigm of point-wise circuit measurements. Ubiquitous instrumentation allows for immediate visualization of an entire breadboard's state, meaning users can diagnose problems based on a wealth of data instead of having to form a single hypothesis and plan before taking a measurement. Basic connectivity information is displayed visually on the circuit itself and quantitative data is displayed on the accompanying web interface. Software-based testing functions further lower the expertise threshold for efficient debugging by diagnosing classes of circuit errors automatically. In an informal study, participants found the detailed, pervasive, and context-rich data from our tool helpful and potentially time-saving.",0,44.6540880503
UIST,374f48396dde9bb6f3287250034a2f5e890fec31,UIST,2003,Paper augmented digital documents,François Guimbretière,2539134,"<i>Paper Augmented Digital Documents</i> (PADDs) are digital documents that can be manipulated either on a computer screen or on paper. PADDs, and the infrastructure supporting them, can be seen as a bridge between the digital and the paper worlds. As digital documents, PADDs are easy to edit, distribute and archive; as paper documents, PADDs are easy to navigate, annotate and well accepted in social settings. The chimeric nature of PADDs make them well suited for many tasks such as proofreading, editing, and annotation of large format document like blueprints.We are presenting an architecture which supports the seamless manipulation of PADDs using today's technologies and reports on the lessons we learned while implementing the first PADD system.",82,62.5
UIST,51b1bbbb2834dc5a156a46b2a06c5c2fbcb5b33c,UIST,2012,Magic finger: always-available input through finger instrumentation,"Xing-Dong Yang, Tovi Grossman, Daniel J. Wigdor, George W. Fitzmaurice","1791070, 3313809, 1961958, 1703735","We present Magic Finger, a small device worn on the fingertip, which supports always-available input. Magic Finger inverts the typical relationship between the finger and an interactive surface: with Magic Finger, we instrument the user's finger itself, rather than the surface it is touching. Magic Finger senses touch through an optical mouse sensor, enabling any surface to act as a touch screen. Magic Finger also senses texture through a micro RGB camera, allowing contextual actions to be carried out based on the particular surface being touched. A technical evaluation shows that Magic Finger can accurately sense 22 textures with an accuracy of 98.9%. We explore the interaction design space enabled by Magic Finger, and implement a number of novel interaction techniques that leverage its unique capabilities.",38,91.6666666667
UIST,4243aaf971ec65e9907cb39f999fb134d0e27462,UIST,2002,TiltType: accelerometer-supported text entry for very small devices,"Kurt Partridge, Saurav Chatterjee, Vibha Sazawal, Gaetano Borriello, Roy Want","1721265, 1726894, 1722797, 1735801, 1802351","TiltType is a novel text entry technique for mobile devices. To enter a character, the user tilts the device and presses one or more buttons. The character chosen depends on the button pressed, the direction of tilt, and the angle of tilt. TiltType consumes minimal power and requires little board space, making it appropriate for wristwatch-sized devices. But because controlled tilting of one's forearm is fatiguing, a wristwatch using this technique must be easily removable from its wriststrap. Applications include two-way paging, text entry for watch computers, web browsing, numeric entry for calculator watches, and existing applications for PDAs.",86,79.1666666667
UIST,a82a10b8c59f330910bdd0e0391c46e2eb89d8d6,UIST,2014,Microtask programming: building software with a crowd,"Thomas D. LaToza, W. Ben Towne, Christian M. Adriano, André van der Hoek","1683595, 2596326, 3142756, 1739859","Microtask crowdsourcing organizes complex work into workflows, decomposing large tasks into small, relatively independent microtasks. Applied to software development, this model might increase participation in open source software development by lowering the barriers to contribu-tion and dramatically decrease time to market by increasing the parallelism in development work. To explore this idea, we have developed an approach to decomposing programming work into microtasks. Work is coordinated through tracking changes to a graph of artifacts, generating appropriate microtasks and propagating change notifications to artifacts with dependencies. We have implemented our approach in CrowdCode, a cloud IDE for crowd development. To evaluate the feasibility of microtask programming, we performed a small study and found that a small crowd of 12 workers was able to successfully write 480 lines of code and 61 unit tests in 14.25 person-hours of time.",22,95.3488372093
UIST,0ad1e3a97282ef0d9ce751dfb9a69a30db514f23,UIST,2005,Interacting with large displays from a distance with vision-tracked multi-finger gestural input,"Shahzad Malik, Abhishek Ranjan, Ravin Balakrishnan","3224978, 3040246, 1748870","We explore the idea of using vision-based hand tracking over a constrained tabletop surface area to perform multi-finger and whole-hand gestural interactions with large displays from a distance. We develop bimanual techniques to support a variety of asymmetric and symmetric interactions, including fast targeting and navigation to all parts of a large display from the comfort of a desk and chair, as well as techniques that exploit the ability of the vision-based hand tracking system to provide multi-finger identification and full 2D hand segmentation.",95,87.0967741935
UIST,57d59e26ee78c3ea04db09e49af093b7cca7f9f7,UIST,2001,Outlier finding: focusing user attention on possible errors,"Rob Miller, Brad A. Myers","1723785, 1707801","When users handle large amounts of data, errors are hard to notice. <i>Outlier finding</i> is a new way to reduce errors by directing the user's attention to inconsistent data which may indicate errors. We have implemented an outlier finder for text, which can detect both unusual matches and unusual mismatches to a text pattern. When integrated into the user interface of a PBD text editor and tested in a user study, outlier finding substantially reduced errors.",51,50.0
UIST,1910dec4d637bccb2efafe685196cbaec27771b5,UIST,1991,Separating application code from toolkits: eliminating the spaghetti of call-backs,Brad A. Myers,1707801,"Conventional toolkits today require the programmer to attach call-back procedures to most buttons, scroll bars, menu items, and other widgets in the interface. These procedures are called by the system when the user operates the widget in order to notify the application of the user's actions. Unfortunately, real interfaces contain hundreds or thousands of widgets, and therefore many call-back procedures, most of which perform trivial tasks, resulting in a maintenance nightmare. This paper describes a system that allows the majority of these procedures to be eliminated. The user interface designer can specify by demonstration many of the desired actions and connections among the widgets, so call-backs are only needed for the most significant application actions. In addition, the call-backs that remain are completely insulated from the widgets, so that the application code is better separated from the user interface.",91,82.6086956522
UIST,30406c88f2eacc3d51bd5e426afdb5c1602865ab,UIST,1991,The importance of pointer variables in constraint models,"Bradley T. Vander Zanden, Brad A. Myers, Dario A. Giuse, Pedro A. Szekely","2255161, 1707801, 1762395, 2628881","Graphical tools are increasingly using constraints to specify the graphical layout and behavior of many parts of an application. However, conventional constraints directly encode the objects they reference, and thus cannot provide support for the dynamic rttntime creation and manipulation of application objects. This paper discusses an extension to current constraint models that allows constraints to indirectly reference objects through pointer variables. Pointer variables permit programmers to create the constraint equivalent of procedures in traditional programming languages. This procedural abstraction allows constraints to model a wide array of dynamic application behavior, simplifies the implementation of structured object and demonstrational systems, and improves the storage and efficiency of highly interactive, graphical applications. It also promotes a simpler, more effective style of programming than conventional constraints. Constraints that use pointer variables are powerful enough to allow a comprehensive user interface toolkit to be built for the first time on top of a constraint system.",2,13.0434782609
UIST,5b15f9fb9b85af1b040353327c22879208ae9b16,UIST,2015,"EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects","Gierad Laput, Chouchang Yang, Robert Xiao, Alanson P. Sample, Chris Harrison","1727999, 1735064, 1681726, 1749219, 1730920","Most everyday electrical and electromechanical objects emit small amounts of electromagnetic (EM) noise during regular operation. When a user makes physical contact with such an object, this EM signal propagates through the user, owing to the conductivity of the human body. By modifying a small, low-cost, software-defined radio, we can detect and classify these signals in real-time, enabling robust on-touch object detection. Unlike prior work, our approach requires no instrumentation of objects or the environment; our sensor is self-contained and can be worn unobtrusively on the body. We call our technique <i>EM-Sense</i> and built a proof-of-concept smartwatch implementation. Our studies show that discrimination between dozens of objects is feasible, independent of wearer, time and local environment.",9,94.298245614
UIST,0fb747c73965d4d8dfe6d2f7f1e4f2bc53f57b4c,UIST,2013,Skillometers: reflective widgets that motivate and help users to improve performance,"Sylvain Malacria, Joey Scarr, Andy Cockburn, Carl Gutwin, Tovi Grossman","1702364, 2946824, 1814003, 1693768, 3313809","Applications typically provide ways for expert users to increase their performance, such as keyboard shortcuts or customization, but these facilities are frequently ignored. To help address this problem, we introduce skillometers -- lightweight displays that visualize the benefits available through practicing, adopting a better technique, or switching to a faster mode of interaction. We present a general framework for skillometer design, then discuss the design and implementation of a real-world skillometer intended to increase hotkey use. A controlled experiment shows that our skillometer successfully encourages earlier and faster learning of hotkeys. Finally, we discuss general lessons for future development and deployment of skillometers.",8,63.3027522936
UIST,1ed6f97ae3ad8a5f5966a89a6e4516fbd1970836,UIST,2009,ARC-Pad: absolute+relative cursor positioning for large displays with a mobile touchscreen,"David C. McCallum, Pourang Irani","1800082, 1773923","We introduce ARC-Pad (Absolute+Relative Cursor pad), a novel technique for interacting with large displays using a mobile phone's touchscreen. In ARC-Pad we combine ab-solute and relative cursor positioning. Tapping with ARC-Pad causes the cursor to jump to the corresponding location on the screen, providing rapid movement across large distances. For fine position control, users can also clutch using relative mode. Unlike prior hybrid cursor positioning techniques, ARC-Pad does not require an explicit switch between relative and absolute modes. We compared ARC-Pad with the relative positioning commonly found on touchpads. Users were given a target acquisition task on a large display, and results showed that they were faster with ARC-Pad, without sacrificing accuracy. Users welcomed the benefits associated with ARC-Pad.",37,60.0
UIST,66450348072786024fbd0447bc8f638f63ec2709,UIST,2006,Quiet interfaces that help students think,"Sharon L. Oviatt, Alexander M. Arthur, Julia Cohen","2807460, 1887790, 7714317","As technical as we have become, modern computing has not permeated many important areas of our lives, including mathematics education which still involves pencil and paper. In the present study, twenty high school geometry students varying in ability from low to high participated in a comparative assessment of math problem solving using existing pencil and paper work practice (PP), and three different interfaces: an Anoto-based digital stylus and paper interface (DP), pen tablet interface (PT), and graphical tablet interface (GT). Cognitive Load Theory correctly predicted that as interfaces departed more from familiar work practice (GT &gt; PT &gt; DP), students would experience greater cognitive load such that performance would deteriorate in speed, attentional focus, meta-cognitive control, correctness of problem solutions, and memory. In addition, low-performing students experienced elevated cognitive load, with the more challenging interfaces (GT, PT) disrupting their performance disproportionately more than higher performers. The present results indicate that Cognitive Load Theory provides a coherent and powerful basis for predicting the rank ordering of users' performance by type of interface. In the future, new interfaces for areas like education and mobile computing could benefit from designs that minimize users' load so performance is more adequately supported.",42,65.0
UIST,7f07f20259715aff3069933247f38627adbf4488,UIST,2014,SikuliBot: automating physical interface using images,"Jeeeun Kim, Mike Kasper, Tom Yeh, Nikolaus Correll","7950141, 2945751, 1704158, 2886493","We present SikuliBot, an image-based approach to automating user interface. SikuliBot extends the visual programming concept of Sikuli Script[2] from the graphical UIs to the real world of physical UIs, such as mobile devices' touch-screens and hardware buttons. The key to our approach is using a physical robot to see an interface, identify a target, and perform an action on the target using the robot's actuators. We demonstrate working examples on MakerBot 3D printer that could move a stylus to perform multi-touch gestures on touchscreen to automate tasks such as swipe-to unlock, playing a virtual piano, and playing the Angry Bird game. A wide range of automation possibilities are made viable using a simple scripting language based on images of UI components. The benefits of our approach are: generalizability, instrumentation-free, and high-level programming abstraction.",0,12.015503876
UIST,8c208aee489553f707795b5cb418fcef4f552e14,UIST,2004,"Clip, connect, clone: combining application elements to build custom interfaces for information access","Jun Fujima, Aran Lunzer, Kasper Hornbæk, Yuzuru Tanaka","2490602, 3306182, 1679367, 1720508","Many applications provide a form-like interface for requesting information: the user fills in some fields, submits the form, and the application presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a trial-and-error search, (2) results from one application are to be used as inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs.",63,65.7894736842
UIST,a9ec471d9e716e4fbeec878fe3d059a5b9666da3,UIST,2016,CodeMend: Assisting Interactive Programming with Bimodal Embedding,"Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, Eytan Adar","2479660, 7189956, 2953859, 2875493, 2630700","Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring <i>found code</i> to their task. To address this problem, we present <i>CodeMend</i>, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change. We demonstrate the utility and accuracy of CodeMend through lab and simulation studies.",0,44.6540880503
UIST,80cc443f2232819df8affe391d828c935e423168,UIST,2012,PhantomPen: virtualization of pen head for digital drawing free from pen occlusion & visual parallax,"David Lee, KyoungHee Son, Joon Hyub Lee, Seok-Hyung Bae","2344788, 1961230, 2690215, 1715434","We present PhantomPen, a direct pen input device whose pen head is virtualized onto the tablet display surface and visually connected to a graspable pen barrel in order to achieve digital drawing free from pen occlusion and visual parallax. As the pen barrel approaches the display, the virtual pen head smoothly appears as if the rendered virtual pen head and the physical pen barrel are in unity. The virtual pen head provides visual feedback by changing its virtual form according to pen type, color, and thickness while the physical pen tip, hidden in the user's sight, provides tactile feedback. Three experiments were carefully designed based on an analysis of drawings by design professionals and observations of design drawing classes. With these experiments that simulate natural drawing we proved significant performance advantages of PhantomPen. PhantomPen was at least as usable as the normal stylus in basic line drawing, and was 17 % faster in focus region drawing (26% faster in extreme focus region drawing). PhantomPen also reduced error rate by 40 % in a typical drawing setup where users have to manage a complex combination of pen and stroke properties.",3,40.1960784314
UIST,064c639d76fc3b769cd87d2b3a9cd76a3112d5d9,UIST,2014,"CommandSpace: modeling the relationships between tasks, descriptions and features","Eytan Adar, Mira Dontcheva, Gierad Laput","2630700, 2875493, 1727999","Users often describe what they want to accomplish with an application in a language that is very different from the application's domain language. To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. A high dimensional vector space representation can model the relationships between user tasks, system commands, and natural language descriptions and supports mapping operations, such as identifying likely system commands given natural language queries and identifying user tasks given a trace of user operations. We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We build and evaluate several applications enabled by our model showing the power and flexibility of this approach.",10,80.2325581395
UIST,01e5f27514f7c97e1180e46e969d82c1653e1373,UIST,2010,Enhanced area cursors: reducing fine pointing demands for people with motor impairments,"Leah Findlater, Alex Jansen, Kristen Shinohara, Morgan Dixon, Peter Kamb, Joshua Rakita, Jacob O. Wobbrock","1689620, 2692208, 2078369, 1743901, 2925462, 3027489, 1796045","Computer users with motor impairments face major challenges with conventional mouse pointing. These challenges are mostly due to fine pointing corrections at the final stages of target acquisition. To reduce the need for correction-phase pointing and to lessen the effects of small target size on acquisition difficulty, we introduce four enhanced area cursors, two of which rely on magnification and two of which use goal crossing. In a study with motor-impaired and able-bodied users, we compared the new designs to the point and Bubble cursors, the latter of which had not been evaluated for users with motor impairments. Two enhanced area cursors, the <i>Visual-Motor-Magnifier</i> and <i>Click-and-Cross</i>, were the most successful new designs for users with motor impairments, reducing selection time for small targets by 19%, corrective submovements by 45%, and error rate by up to 82% compared to the point cursor. Although the Bubble cursor also improved performance, participants with motor impairments unanimously preferred the enhanced area cursors.",28,80.2325581395
UIST,3eef0f716cbe1384df065c68dd9341c8da8707cc,UIST,2014,Structured handoffs in expert crowdsourcing improve communication and work output,"Alex Embiricos, Negar Rahmati, Nicole Zhu, Michael S. Bernstein","1997163, 2374500, 2754112, 3047089","Expert crowdsourcing allows specialized, remote teams to complete projects, often large and involving multiple stages. Its execution is complicated due to communication difficulties between remote workers. This paper investigates whether structured handoff methods, from one worker to the next, improve final product quality by helping the workers understand the input of their tasks and reduce overall integration cost. We investigate this question through 1) a ""live"" handoff method where the next worker shadows the former via screen sharing technology and 2) a ""recorded"" handoff, where workers summarize work done for the next, via a screen capture and narration. We confirm the need for a handoff process. We conclude that structured handoffs result in higher quality work, improved satisfaction (especially for workers with creative tasks), improved communication of non-obvious instructions, and increased adherence to the original intent of the project.",0,12.015503876
UIST,9e43b6711bfe468c2d0b77f669ab405c2f00203f,UIST,2011,Advanced interaction with mobile projection interfaces,Markus Löchtefeld,1808720,"Through the increasing miniaturization of projection units the integration of such units in everyday-life objects is now possible. Even though these so called pico-projectors are already getting integrated into mobile devices like phones or digital cameras, comparably little research has been conducted to empower these devices to their full capabilities. I outline my previous and current work towards an interface design and a privacy framework that will facilitate mobile projection devices to be part in people's everyday-life. In particular my work is divided into two directions, on the one hand the development of a single-user scenario interface and on the other hand a framework to cope with privacy issues. This will allow the deeper exploitation of the capabilities of mobile projection units for a variety of everyday tasks.",0,6.19047619048
UIST,daf81707da5d02a3fe0e1f646fa84a69d8289785,UIST,2014,Tactile cue presentation for vocabulary learning with keyboard,"Daichi Ogawa, Sakiko Ikeno, Ryuta Okazaki, Taku Hachisu, Hiroyuki Kajimoto","2267661, 2469950, 2812591, 3242743, 1776927","This paper presents the results of a pilot experiment observ-ing the effect of tactile cues on vocabulary learning. Con-sidering that we generally memorize words by associating them with various cues, we designed a tactile cue presentation device that aids vocabulary learning by applying vibra-tions to the finger that is associated with the next key to press when typing on a keyboard. Experiments comparing tactile and visual cues indicated that tactile cues can signifi-cantly improve long-term retention of vocabulary after one week.",1,31.007751938
UIST,1c0a3ceafb5c816198867911fc0c883d2c69e2bc,UIST,2014,Nishanchi: CAD for hand-fabrication,"Pragun Goyal, Joseph A. Paradiso, Pattie Maes","2301932, 4798651, 1701876","We present Nishanchi, a position and orientation aware handheld inkjet printer which can be used to transfer the reference marks from CAD to the workpiece for use in manual fabrication workflows. Nishanchi also has a digitizing tip that can be used to input features about the workpiece to a computer model. By allowing for this two-way exchange of information from CAD to a nonconcormal workpiece, we believe that Nishanchi might help make inclusion of CAD in manual fabrication workflows more seamless.",0,12.015503876
UIST,096a000ef74ea2abc5f8403329d6d91dae20802f,UIST,2003,EdgeWrite: a stylus-based text entry method designed for high accuracy and stability of motion,"Jacob O. Wobbrock, Brad A. Myers, John A. Kembel","1796045, 1707801, 2469390","EdgeWrite is a new unistroke text entry method for handheld devices designed to provide high accuracy and stability of motion for people with motor impairments. It is also effective for able-bodied people. An EdgeWrite user enters text by traversing the edges and diagonals of a square hole imposed over the usual text input area. Gesture recognition is accomplished not through pattern recognition but through the sequence of corners that are hit. This means that the full stroke path is unimportant and recognition is highly deterministic, enabling better accuracy than other gestural alphabets such as Graffiti. A study of able-bodied users showed subjects with no prior experience were 18% more accurate during text entry with Edge Write than with Graffiti (<i>p</i>&gt;.05), with no significant difference in speed. A study of 4 subjects with motor impairments revealed that some of them were unable to do Graffiti, but all of them could do Edge Write. Those who could do both methods had dramatically better accuracy with Edge Write.",150,87.5
UIST,74f3fd11b2a5d614f5ad33a93c5281cff769185c,UIST,2016,Semi-Automated SVG Programming via Direct Manipulation,"Brian Hempel, Ravi Chugh","3409809, 1910682","Direct manipulation interfaces provide intuitive and interactive features to a broad range of users, but they often exhibit two limitations: the built-in features cannot possibly cover all use cases, and the internal representation of the content is not readily exposed. We believe that if direct manipulation interfaces were to (a) use general-purpose programs as the representation format, and (b) expose those programs to the user, then experts could customize these systems in powerful new ways and non-experts could enjoy some of the benefits of programmable systems.
 In recent work, we presented a prototype SVG editor called Sketch-n-Sketch that offered a step towards this vision. In that system, the user wrote a program in a general-purpose lambda-calculus to generate a graphic design and could then directly manipulate the output to indirectly change design parameters (i.e. constant literals) in the program in real-time during the manipulation. Unfortunately, the burden of programming the desired relationships rested entirely on the user.
 In this paper, we design and implement new features for Sketch-n-Sketch that assist in the programming process itself. Like typical direct manipulation systems, our extended Sketch-n-Sketch now provides GUI-based tools for drawing shapes, relating shapes to each other, and grouping shapes together. Unlike typical systems, however, each tool carries out the user's intention by transforming their general-purpose program. This novel, semi-automated programming workflow allows the user to rapidly create high-level, reusable abstractions in the program while at the same time retaining direct manipulation capabilities. In future work, our approach may be extended with more graphic design features or realized for other application domains.",0,44.6540880503
UIST,49e6d4cf4130109b73b7377c5c2732e80299ba49,UIST,2015,AirFlip-Undo: Quick Undo using a Double Crossing In-Air Gesture in Hover Zone,"Keigo Shima, Ryosuke Takada, Kazusa Onishi, Takuya Adachi, Buntarou Shizuki, Jiro Tanaka","2800213, 2857821, 1956963, 4784188, 1765222, 1726327","In this work, we use AirFlip to undo text input on mobile touchscreen devices. AirFlip involves a quick double crossing in-air gesture in the boundary surfaces of hover zone of devices that have hover sensing capability. To evaluate the effectiveness of undoing text input with AirFlip, we implemented two QWERTY soft keyboards (AirFlip keyboard and Typical keyboard). With these keyboards, we conducted a user study to investigate the users? workload and to collect subjective opinions. The results show that there is no significant difference in workload between keyboards.",0,16.2280701754
UIST,67b0d525f5b05a8a481bbb93e112172ea8c8d01c,UIST,2015,Corona: Positioning Adjacent Device with Asymmetric Bluetooth Low Energy RSSI Distributions,"Haojian Jin, Cheng Xu, Kent Lyons","2340572, 4062000, 2073793","We introduce Corona, a novel spatial sensing technique that implicitly locates adjacent mobile devices in the same plane by examining asymmetric Bluetooth Low Energy RSSI distributions. The underlying phenomenon is that the off-center BLE antenna and asymmetric radio frequency topology create a characteristic Bluetooth RSSI distribution around the device. By comparing the real-time RSSI readings against a RSSI distribution model, each device can derive the relative position of the other adjacent device. Our experiments using an iPhone and iPad Mini show that Corona yields position estimation at 50% accuracy within a 2cm range, or 85% for the best two candidates. We developed an application to combine Corona with accelerometer readings to mitigate ambiguity and enable cross-device interactions on adjacent devices.",1,42.1052631579
UIST,307694fcfe5a0094601c6cac3bd4991a5edd9284,UIST,2016,Immersive Scuba Diving Simulator Using Virtual Reality,"Dhruv Jain, Misha Sra, Jingru Guo, Rodrigo Marques, Raymond Wu, Justin Chiu, Chris Schmandt","1960730, 3024298, 3396882, 8557569, 1684661, 3172816, 1729321","We present Amphibian, a simulator to experience scuba diving virtually in a terrestrial setting. While existing diving simulators mostly focus on visual and aural displays, Amphibian simulates a wider variety of sensations experienced underwater. Users rest their torso on a motion platform to feel buoyancy. Their outstretched arms and legs are placed in a suspended harness to simulate drag as they swim. An Oculus Rift head-mounted display (HMD) and a pair of headphones delineate the visual and auditory ocean scene. Additional senses simulated in Amphibian are breath motion, temperature changes, and tactile feedback through various sensors. Twelve experienced divers compared Amphibian to real-life scuba diving. We analyzed the system factors that influenced the users' sense of <i>being there</i> while using our simulator. We present future UI improvements for enhancing immersion in VR diving simulators.",0,44.6540880503
UIST,46a3638ffa24c55b7c914b6397f49effdd118f4d,UIST,2014,Reaching targets on discomfort region using tilting gesture,"Youli Chang, Sehi L'Yi, Jinwook Seo","2463431, 3219582, 1705834","We present three novel methods to facilitate one hand targeting at discomfort regions on a mobile touch screen using tilting gestures; TiltSlide, TiltReduction, and TiltCursor. We conducted a controlled user study to evaluate them in terms of their performance and user preferences by comparing them with other related methods, i.e. ThumbSpace, Edge Triggered with Extendible Cursor (ETEC), and Direct Touch (directly touching with a thumb). All three methods showed better performance than ThumbSpace in terms of speed and accuracy. Moreover, TiltReduction led users to require less thumb/grip movement than Direct Touch while showing comparable performance in speed and accuracy.",2,41.4728682171
UIST,88a3175f50a285f3234145bcda2746b36446867c,UIST,2015,SHOCam: A 3D Orbiting Algorithm,"Michael Ortega-Binderberger, Wolfgang Stuerzlinger, Douglas Scheurich","2163619, 3342964, 2070139","In this paper we describe a new orbiting algorithm, called SHOCam, which enables simple, safe and visually attractive control of a camera moving around 3D objects. Compared with existing methods, SHOCam provides a more consistent mapping between the user's interaction and the path of the camera by substantially reducing variability in both camera motion and look direction. Also, we present a new orbiting method that prevents the camera from penetrating object(s), making the visual feedback -- and with it the user experience -- more pleasing and also less error prone. Finally, we present new solutions for orbiting around multiple objects and multi-scale environments.",1,42.1052631579
UIST,56a0c81cfe0d82debe68d6de462d2d34740494db,UIST,2014,HaptoMime: mid-air haptic interaction with a floating virtual screen,"Yasuaki Monnai, Keisuke Hasegawa, Masahiro Fujiwara, Kazuma Yoshino, Seki Inoue, Hiroyuki Shinoda","1800083, 1782349, 2382476, 1819657, 1724700, 1715925","We present HaptoMime, a mid-air interaction system that allows users to touch a floating virtual screen with hands-free tactile feedback. Floating images formed by tailored light beams are inherently lacking in tactile feedback. Here we propose a method to superpose hands-free tactile feedback on such a floating image using ultrasound. By tracking a fingertip with an electronically steerable ultrasonic beam, the fingertip encounters a mechanical force consistent with the floating image. We demonstrate and characterize the proposed transmission scheme and discuss promising applications with an emphasis that it helps us 'pantomime' in mid-air.",5,63.1782945736
UIST,517ce439e6bdc7c301d45c1925950fe22f97f319,UIST,1991,Issues in combining marking and direct manipulation techniques,"Gordon Kurtenbach, William Buxton","1708940, 6037251","We use the term ""marking interactions"" to describe interactions where the pointing device leaves an ""ink trail"" on the display similar to writing with a pen. An example of an interface that uses marking interactions is a prototype spread-sheet described in Wolf, Rhyne & Ellozy (1989). Motivating our work is the view that marking interactions can be combined with direct manipulation interfaces to produce easier to use and expressive interfaces. This view has also been expressed by Wolf and Rhyne (1987). The advantages of marking interfaces can be used to support the weaknesses of direct manipulation and vice versa. The PenPoint system by the Go Corporation is an example of a system which supports both marking interactions and direct manipulation (Carr, 1991). The direct manipulation paradigm has been effective in helping designers create easy to use mouse and keyboard based interfaces. The development of flat display surfaces and transparent tablets are now making possible interfaces where a user can write directly on the screen using a special stylus. The intention of these types of interfaces is to exploit user's existing handwriting, markup and drawing skills while also providing the benefits of direct manipulation. This paper reports on a test bed program which we are using for exploring hand-marking types of interactions and their integration with direct manipulation interactions. 1.1 GEdit In order to explore this hybrid marking/direct manipulation design space we have created a program called GEdit. GEdit 1 is a prototype graphical editor that permits a user to create and manipulate three types of objects (squares, circles and triangles) using shorthand and proofreader style markings. Using hand-drawn symbols, a user can add, delete, move and copy these objects. Objects can be manipulated either individually or in groups. Groups are specified with hand-drawn circling symbols.",117,91.3043478261
UIST,f32a142ab0ff2c108fc047b3ace9a7dddff4a217,UIST,2016,CloakingNote: A Novel Desktop Interface for Subtle Writing Using Decoy Texts,"Sehi L'Yi, Kyle Koh, Jaemin Jo, Bo Hyoung Kim, Jinwook Seo","3219582, 1834207, 1939362, 2713391, 1705834","We present <i>CloakingNote</i>, a novel desktop interface for subtle writing. The main idea of <i>CloakingNote</i> is to misdirect observers' attention away from a real text by using a prominent decoy text. To assess the subtlety of <i>CloakingNote</i>, we conducted a subtlety test while varying the contrast ratio between the real text and its background. Our results demonstrated that the real text as well as the interface itself were subtle even when participants were aware that a writer might be engaged in suspicious activities. We also evaluated the feasibility of <i>CloakingNote</i> through a performance test and categorized the users' layout strategies.",0,44.6540880503
UIST,783b76869d44a69b25871cde0041be59d4ed92f7,UIST,2013,BoardLab: PCB as an interface to EDA software,"Pragun Goyal, Harshit Agrawal, Joseph A. Paradiso, Pattie Maes","2301932, 2071315, 4798651, 1701876","The tools used to work with Printed Circuit Boards (PCBs), for example soldering iron, multi-meter and oscilloscope involve working directly with the board and the board components. However, the Electronic Design Automation (EDA) software used to query a PCB's design data requires using a keyboard and a mouse. These different interfaces make it difficult to connect both kinds of operations in a workflow. Further, the measurements made by tools like a multi-meter have to be understood in the context of the schematics of the board manually. We propose a solution to reduce the cognitive load of this disconnect by introducing a handheld probe that allows for direct interactions with the PCB for just-in-time information on board schematics, component datasheets and source code. The probe also doubles up as a voltmeter and annotates the schematics of the board with voltage measurements.",2,37.6146788991
UIST,b1aa6b1e7ad9e32d5604d602d44fc20954c320eb,UIST,2012,Cliplets: juxtaposing still and dynamic imagery,"Neel Joshi, Sisil Mehta, Steven M. Drucker, Eric J. Stollnitz, Hugues Hoppe, Matthew Uyttendaele, Michael F. Cohen","2641664, 2859162, 2311676, 3011420, 1688461, 2262291, 1694613","We explore creating """"cliplets"""", a form of visual media that juxtaposes still image and video segments, both spatially and temporally, to expressively abstract a moment. Much as in """"cinemagraphs"""", the tension between static and dynamic elements in a cliplet reinforces both aspects, strongly focusing the viewer's attention. Creating this type of imagery is challenging without professional tools and training. We develop a set of idioms, essentially spatiotemporal mappings, that characterize cliplet elements, and use these idioms in an interactive system to quickly compose a cliplet from ordinary handheld video. One difficulty is to avoid artifacts in the cliplet composition without resorting to extensive manual input. We address this with automatic alignment, looping optimization and feathering, simultaneous matting and compositing, and Laplacian blending. A key user-interface challenge is to provide affordances to define the parameters of the mappings from input time to output time while maintaining a focus on the cliplet being created. We demonstrate the creation of a variety of cliplet types. We also report on informal feedback as well as a more structured survey of users.",20,78.9215686275
UIST,de5d3776a5747ab15781900edc0c4b6e9fae7164,UIST,2004,Bridging the gap from theory to practice: the path toward innovation in human-computer interaction,Mary Czerwinski,1702712,"How do we break away from existing tools and techniques in HCI and truly innovate in a way that benefits the next generation of computer users? Today, too many of our technological designs and inventions are ""one off"" point designs, not building on or contributing to a theoretical foundation of understanding around human perception, cognition, social behavior and physical movement. Of course, these point designs can be successful in and of themselves, so why bother with theory and models? In order to mature as a field in a way that benefits users, it can be argued that we need to work more closely together and with an awareness of multiple disciplines, including not just the computer science and engineering arenas, but also psychology, sociology, and any field of human behavior. Of course, this could be a daunting task-how do we know that important improvements in user interface design can be obtained?
 I will present a series of examples of what I consider to be significant contributions to the field of HCI, each based on a multidisciplinarian, theory-driven approach. I hope to challenge the audience to creatively consider ways that their own work could be more theoretically motivated, and what it might take for more of us to move forward in that direction.",1,5.26315789474
UIST,337d18e3c1d7338e061368adc6d95b028f8a493c,UIST,2014,InterState: a language and environment for expressing interface behavior,"Stephen Oney, Brad A. Myers, Joel Brandt","2953859, 1707801, 1702922","InterState is a new programming language and environment that addresses the challenges of writing and reusing user interface code. InterState represents interactive behaviors clearly and concisely using a combination of novel forms of state machines and constraints. It also introduces new language features that allow programmers to easily modularize and reuse behaviors. InterState uses a new visual notation that allows programmers to better understand and navigate their code. InterState also includes a live editor that immediately updates the running application in response to changes in the editor and vice versa to help programmers understand the state of their program. Finally, InterState can interface with code and widgets written in other languages, for example to create a user interface in InterState that communicates with a database. We evaluated the understandability of InterState's programming primitives in a comparative laboratory study. We found that participants were twice as fast at understanding and modifying GUI components when they were implemented with InterState than when they were implemented in a conventional textual event-callback style. We evaluated InterState's scalability with a series of benchmarks and example applications and found that it can scale to implement complex behaviors involving thousands of objects and constraints.",2,41.4728682171
UIST,c9db3d2aa9f42263c23d8654c22bad94afa97dbb,UIST,2012,ConstraintJS: programming interactive behaviors for the web by integrating constraints and states,"Stephen Oney, Brad A. Myers, Joel Brandt","2953859, 1707801, 1702922","Interactive behaviors in GUIs are often described in terms of states, transitions, and constraints, where the constraints only hold in certain states. These constraints maintain relationships among objects, control the graphical layout, and link the user interface to an underlying data model. However, no existing Web implementation technology provides direct support for all of these, so the code for maintaining constraints and tracking state may end up spread across multiple languages and libraries. In this paper we describe ConstraintJS, a system that integrates constraints and finite-state machines (FSMs) with Web languages. A key role for the FSMs is to enable and disable constraints based on the interface's current mode, making it possible to write constraints that <i>sometimes</i> hold. We illustrate that constraints combined with FSMs can be a clearer way of defining many interactive behaviors with a series of examples.",14,64.2156862745
UIST,6c3bfa48b01e764d194be69da8ad003d887b4ab8,UIST,2005,Citrus: a language and toolkit for simplifying the creation of structured editors for code and data,"Andrew Jensen Ko, Brad A. Myers","3314595, 1707801","Direct-manipulation editors for structured data are increasingly common. While such editors can greatly simplify the creation of structured data, there are few tools to simplify the creation of the editors themselves. This paper presents Citrus, a new programming language and user interface toolkit designed for this purpose. Citrus offers language-level support for constraints, restrictions and change notifications on primitive and aggregate data, mechanisms for automatically creating, removing, and reusing views as data changes, a library of widgets, layouts and behaviors for defining interactive views, and two comprehensive interactive editors as an interface to the language and toolkit itself. Together, these features support the creation of editors for a large class of data and code.",9,19.3548387097
UIST,02cfe5cca617695f7098baa141a49376dc8edf71,UIST,2004,Citrine: providing intelligent copy-and-paste,"Jeffrey Stylos, Brad A. Myers, Andrew Faulring","2766620, 1707801, 2287103","We present Citrine, a system that extends the widespread copy-and-paste interaction technique with intelligent transformations, making it useful in more situations. Citrine uses text parsing to find the structure in copied text and allows users to paste the structured information, which might have many pieces, in a single paste operation. For example, using Citrine, a user can copy the text of a meeting request and add it to the Outlook calendar with a single paste. In applications such as Excel, users can teach Citrine by example how to copy and paste data by showing it which fields go into which columns, and can use this to copy or paste many items at a time in a user-defined manner. Citrine can be used with a wide variety of applications and types of data and can be easily extended to work with more. It currently includes parsers that recognize contact information, calendar appointments and bibliographic citations. It works with Internet Explorer, Outlook, Excel, Palm Desktop, EndNote and other applications. Citrine is available to download on the internet.",43,60.5263157895
UIST,bba75b7fe4a06e7d0fd875e14717457858e953bd,UIST,2013,Multi-perspective multi-layer interaction on mobile device,"Maryam Khademi, Mingming Fan, Hossein Mousavi Hondori, Cristina V. Lopes","3004980, 1868611, 2432120, 8265496","We propose a novel multi-perspective multi-layer interaction using a mobile device, which provides an immersive experience of 3D navigation through an object. The mobile device serves as a window, through which the user can observe the object in detail from various perspectives by orienting the device differently. Various layers of the object can also be shown while users move the device away and toward themselves. Our approach is real-time, completely mobile (running on Android) and does not depend on external sensor/displays (e.g., camera and projector).",0,10.5504587156
UIST,782ebc83e8820ebb33bc4948cfb01d528c1eec26,UIST,2016,Resolving Spatial Variation And Allowing Spectator Participation In Multiplayer VR,"Misha Sra, Dhruv Jain, Arthur Pitzer Caetano, Andrés A. Calvo, Erwin Hilton, Chris Schmandt","3024298, 1960730, 3492853, 2955647, 3493196, 1729321","Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.",0,44.6540880503
UIST,629c40e2e896551ec7cbaedd2a94ad155b4a95ff,UIST,2012,Tutorial-based interfaces for cloud-enabled applications,"Gierad Laput, Eytan Adar, Mira Dontcheva, Wilmot Li","1727999, 2630700, 2875493, 2812691","Powerful image editing software like Adobe Photoshop and GIMP have complex interfaces that can be hard to master. To help users perform image editing tasks, we introduce <i>tutorial-based applications</i> (tapps) that retain the step-by-step structure and descriptive text of tutorials but can also automatically apply tutorial steps to new images. Thus, tapps can be used to batch process many images automatically, similar to traditional macros. Tapps also support interactive exploration of parameters, automatic variations, and direct manipulation (e.g., selection, brushing). Another key feature of tapps is that they execute on remote instances of Photoshop, which allows users to edit their images on any Web-enabled device. We demonstrate a working prototype system called TappCloud for creating, managing and using tapps. Initial user feedback indicates support for both the interactive features of tapps and their ability to automate image editing. We conclude with a discussion of approaches and challenges of pushing monolithic direct-manipulation GUIs to the cloud.",15,66.6666666667
UIST,031b25d462246a4752648191f4f3390af21dcf4c,UIST,2008,Zoetrope: interacting with the ephemeral web,"Eytan Adar, Mira Dontcheva, James Fogarty, Daniel S. Weld","2630700, 2875493, 1738171, 1780531","The Web is ephemeral. Pages change frequently, and it is nearly impossible to find data or follow a link after the underlying page evolves. We present Zoetrope, a system that enables interaction with the historicalWeb (pages, links, and embedded data) that would otherwise be lost to time. Using a number of novel interactions, the temporal Web can be manipulated, queried, and analyzed from the context of familar pages. Zoetrope is based on a set of operators for manipulating content streams. We describe these primitives and the associated indexing strategies for handling temporal Web data. They form the basis of Zoetrope and enable our construction of new temporal interactions and visualizations.",45,77.1428571429
UIST,04328981d6187598af89705f91a3df76967497f3,UIST,2016,Study on Control Method of Virtual Food Texture by Electrical Muscle Stimulation,"Arinobu Niijima, Takefumi Ogawa","1899586, 2256487","We propose Electric Food Texture System, which can present virtual food texture such as hardness and elasticity by electrical muscle stimulation (EMS) to the masseter muscle. In our previous study, we investigated the feasibility to detect user's bite with a photoreflector and that to construct database of food texture with electromyography sensors. In this paper, we investigated the feasibility to control virtual food texture by EMS. We conducted an experiment to reveal the relationship of the parameters of EMS and those of virtual food texture. The experimental results show that the higher strength of EMS is, the harder virtual food texture is, and the longer duration of EMS is, the more elastic virtual food texture is.",0,44.6540880503
UIST,202a7c6e32a7c59974651548997f000f4a2bb2b2,UIST,2015,A Study on Grasp Recognition Independent of Users' Situations Using Built-in Sensors of Smartphones,"Chanho Park, Takefumi Ogawa","2650226, 2256487","There are many hand postures of smartphone according to the users? situations. In order to support appropriate inter-face, it is important to know user?s hand posture. To recognize grasp postures, which is not depend on users? situations, we consider using smartphone?s touchscreen and their built-in gyroscope and accelerometer and use support vector machine (SVM). In order to evaluate our system, we described the result of the experiments when users are using the devices in the room and on the train. We knew that our system could be feasible for personal use only system by improving the information from the accelerometer. We also collected users? data when users are sitting in the room. Results showed that grasp recognition accuracy under 5 and 4 hand postures were 87.7%, 92.4% respectively when training and testing on 6 users.",0,16.2280701754
UIST,0a73fa6ca1c8f5ce8c2e0615ff84914beb53634e,UIST,2016,Fitter: A System for Easily Printing Objects that Fit Real Objects,"Yoh Akiyama, Homei Miyashita","2890244, 1796760","When printing both self-making and existing 3D models, users often create models to fit to a real object within it. Fitting models to the size of a real object is a delicate problem. To address it, we present a concept to capture the size of a real object, create or modify a model that conforms to the captured image, and print the model on the spot. We create a 3D printer to realize this concept by installing a touch panel display in the build plate system. In this paper, we focus on creating containers that fit accessories. We create containers for a pair of scissors, a smart watch, a drone, a pair of glasses, and a pen holder.",0,44.6540880503
UIST,3eacef58f18fc9ab395e6f4224ec91336ee4b2a4,UIST,2014,Projectron mapping: the exercise and extension of augmented workspaces for learning electronic modeling through projection mapping,"Yoh Akiyama, Homei Miyashita","2890244, 1796760","There has been research using software simulations to support the learning of electronic modeling by beginners. There have also been systems to extend workspaces and support electronic modeling on tabletop interfaces. However, in the case of software-based circuit operation, as it is not possible to operate the actual elements, the feeling of actually moving the elements is lacking. For this reason, we are proposing a system that extends the sense of reality in software simulators through the use of projection mapping. This will make it possible to actually give the impression of moving the elements by using a software simulator, and to achieve both high speed and a sense of reality through trial and error.",1,31.007751938
UIST,8dce4d6b6ee6bd9acefd655e7160ce45611d9034,UIST,2015,Methods of 3D Printing Micro-pillar Structures on Surfaces,"Jifei Ou, Chin-Yi Cheng, Liang Zhou, Gershon Dublon, Hiroshi Ishii","3163859, 7152249, 4298828, 2527803, 1749649","This work presents a method of 3D printing hair-like structures on both flat and curved surfaces. It allows a user to design and fabricate hair geometry that is smaller than 100 micron. We built a software platform to let one quickly define a hair's angle, thickness, density, and height. The ability to fabricate customized hair-like structures expands the library of 3D-printable shape. We then present several applications to show how the 3D-printed hair can be used for designing toy objects.",0,16.2280701754
UIST,6a2b1d81ab0248d5db83abaee2c2dbce625e0d96,UIST,2016,Estimating Contact Force of Fingertip and Providing Tactile Feedback Simultaneously,"Nobuhiro Funato, Kentaro Takemura","3493028, 3298569","This study proposes a method for estimating the contact force of the fingertip by inputting vibrations actively. The use of active bone-conducted sound sensing has been limited to estimating the joint angle of the elbow and the finger. We applied it to the method for estimating the contact force of the fingertip. Unlike related works, it is not necessary to mount the device on a fingertip, and tactile feedback is enabled using tangible vibrations.",0,44.6540880503
UIST,588aa7a777cbe70740e702891f35afc154579d12,UIST,2015,Haptic-enabled Active Bone-Conducted Sound Sensing,"Yuya Okawa, Kentaro Takemura","2501558, 3298569","In this study, we propose active bone-conducted sound sens- ing for estimating a joint angle of a finger and simultaneous use as a haptic interface. For estimating the joint angle, an unnoticeable vibration is input to the finger, and a perceptible vibration is additionally input to the finger for providing hap- tic feedback. The joint angle is estimated by switching the estimation model depending on the haptic feedback and the average error of the estimation is within about seven degree.",2,60.5263157895
UIST,80c2d9219a5110200bc142e0911f3ec53f061d7d,UIST,2015,ReForm: Integrating Physical and Digital Design through Bidirectional Fabrication,"Christian Weichel, John Hardy, Jason Alexander, Hans-Werner Gellersen","3111625, 3346238, 2928764, 4919595","Digital fabrication machines such as 3D printers and laser-cutters allow users to produce physical objects based on virtual models. The creation process is currently unidirectional: once an object is fabricated it is separated from its originating virtual model. Consequently, users are tied into digital modeling tools, the virtual design must be completed before fabrication, and once fabricated, re-shaping the physical object no longer influences the digital model. To provide a more flexible design process that allows objects to iteratively evolve through both digital and physical input, we introduce <i>bidirectional fabrication</i>. To demonstrate the concept, we built <i>ReForm</i>, a system that integrates digital modeling with shape input, shape output, annotation for machine commands, and visual output. By continually synchronizing the physical object and digital model it supports object versioning to allow physical changes to be undone. Through application examples, we demonstrate the benefits of ReForm to the digital fabrication process.",3,71.0526315789
UIST,87239d32be1e297c64a3d095002e5416f9f4f9d1,UIST,2011,Associating the visual representation of user interfaces with their internal structures and metadata,"Tsung-Hsiang Chang, Tom Yeh, Rob Miller","2337269, 1704158, 1723785","Pixel-based methods are emerging as a new and promising way to develop new interaction techniques on top of existing user interfaces. However, in order to maintain platform independence, other available low-level information about GUI widgets, such as accessibility metadata, was neglected intentionally. In this paper, we present a hybrid framework, PAX, which associates the visual representation of user interfaces (i.e. the pixels) and their internal hierarchical metadata (i.e. the content, role, and value). We identify challenges to building such a framework. We also develop and evaluate two new algorithms for detecting text at arbitrary places on the screen, and for segmenting a text image into individual word blobs. Finally, we validate our framework in implementations of three applications. We enhance an existing pixel-based system, Sikuli Script, and preserve the readability of its script code at the same time. Further, we create two novel applications, Screen Search and Screen Copy, to demonstrate how PAX can be applied to development of desktop-level interactive systems.",12,51.9047619048
UIST,5c571c9699afcc8a9090b2af2dbebb2755672f83,UIST,2010,Intimacy versus privacy,Marvin Minsky,1847175,"When you talk to a person, it's safe to assume that you both share large bodies of ""common sense knowledge."" But when you converse with a programmed computer, neither of you is likely to know much about what the other one knows.
 Indeed, in some respects this is desirable - as when we're concerned with our privacy. We don't want strangers to know our most personal goals, or all the resources that we may control.
 However, when we turn to our computers for help, we'll want that relationship to change - because now it is in our interest for those systems to understand our aims and goals, as well as our fears and phobias. Indeed, the extents to which those processes ""know us as individuals"".
 Issues like these will always arise whenever we need a new interface - and as one of my teachers wrote long ago, ""The hope is that, in not too many years, human brains and computing machines will be coupled together very tightly, and that the resulting partnership will think as no human brain has ever thought and process data in a way not approached by the information-handling machines we know today.""<sup>1</sup>
 Indeed, the '60s and '70s saw substantial advancestowars this but it seems to me that then progress slowed down. If so, perhaps this was partly because the AI community moved from semantic and heuristic methods towards more formal (but less flexible) statistical schemes. So nowI'd like to see more researchers remedy this by developing systems that use more commonsense knowledge.",0,9.3023255814
UIST,9638842da9f51dae49f537051aeaf909fea96ff1,UIST,2014,Laevo: a temporal desktop interface for integrated knowledge work,"Steven Jeuris, Steven Houben, Jakob E. Bardram","2692365, 1770335, 7475687","Prior studies show that knowledge work is characterized by highly interlinked practices, including task, file and window management. However, existing personal information management tools primarily focus on a limited subset of knowledge work, forcing users to perform additional manual configuration work to integrate the different tools they use. In order to understand tool usage, we review literature on how users' activities are created and evolve over time as part of knowledge worker practices. From this we derive the activity life cycle, a conceptual framework describing the different states and transitions of an activity. The life cycle is used to inform the design of Laevo, a temporal activity-centric desktop interface for personal knowledge work. Laevo allows users to structure work within dedicated workspaces, managed on a timeline. Through a centralized notification system which doubles as a to-do list, incoming interruptions can be handled. Our field study indicates how highlighting the temporal nature of activities results in lightweight scalable activity management, while making users more aware about their ongoing and planned work.",1,31.007751938
UIST,15d42cdbfd0014123e8cd2eb52d538b5f2a80c5d,UIST,2014,Trainer: a motion-based interactive game for balance rehabilitation training,"Guanyun Wang, Ye Tao, Dian Yu, Chuan Cao, Hongyu Chen, Cheng Yao","1960053, 3200464, 2388443, 8674804, 6523968, 7590222","In physiotherapy, the traditional approach of using fixed aids to train patients to keep their balance is often ineffective, due to the tendency of people to lose interest in the training or to lose confidence in their ability to finish the training. A Trainer system is proposed on traditional physiotherapy treatment methods to allow patients to play qualified and immersive games with a mobile aid. Using RF localization and self-balancing technology, the system allows patients to control a vehicle with their sense of balance. This platform provides a series of game feedback interface which involves part-body motion in sitting manipulation therapy to make the rehabilitation more flexible and more effective. This paper reports the designing and the control of the Trainer, the experimental evaluations of the performance of system, as well as an exploration of the future work in detail. Our work is intended to improve the patient experience of the physiotherapy rehabilitation using games with instinctive ways of controlling mobile instruments.",1,31.007751938
UIST,906821e2916ca794144342259a5db762c86c2753,UIST,2014,Eugenie: gestural and tangible interaction with active tokens for bio-design,"Casey Grote, Evan Segreto, Johanna Okerlund, Robert Kincaid, Orit Shaer","2119149, 1939265, 2194499, 1804009, 1718579","We present a case study of a tangible user interface that implements novel interaction techniques for the construction of complex queries in large data sets. Our interface, Eugenie, utilizes gestural interaction with active physical tokens and a multi-touch interactive surface to aid in the collaborative design process of synthetic biological circuits. We developed new interaction techniques for navigating large hierarchical data sets and for exploring a combinatorial design space. The goal of this research is to study the effect of gestural and tangible interaction with active tokens on sense-making throughout the bio-design process.",0,12.015503876
UIST,bd097689d5755eec3f6b5358b33556d9d116410d,UIST,2015,EMG Sensor-based Two-Hand Smart Watch Interaction,"Yoonsik Yang, Seungho Chae, Jinwook Shim, Tack-Don Han","3053579, 1824042, 2040181, 2184681","These days, smart watches have drawn more attention of users, and many smart watch products have been launched (Samsung Gear series, apple watch and etc.). Since a smart watch is put on the wrist, the device should be small and unobtrusive. Because of these features, display of the smart watch is small and there is a limitation to interaction. To overcome the limitation, many studies are conducted. In this paper, we propose a two-hand interaction technique that obtains posture information of a hand using electromyography (EMG) sensor attached to the arm and to make input interaction to a smart watch different depending on each posture. EMG sensors recognize information about a user's hand posture, and the non-dominant hand is used for smart watch inputs. In this way, different function is executed depending on postures. As a result, a smart watch that has limited input methods is given a variety of interaction functions with users.",1,42.1052631579
UIST,2fcb87743b0465b98693dee58e3d2f80ac54741e,UIST,2016,Interaction Technique Using Acoustic Sensing for Different Squeak Sounds Caused by Number of Rubbing Fingers,"Ryosuke Kawakatsu, Shigeyuki Hirai","3492323, 3308299","Various studies have been conducted for developing interaction techniques in a smart house. Some of our previous studies [1, 2] focused on bathrooms and we converted an existing normal bathtub system into a user interface by using embedded sensors. A system called Bathcratch [2] detects squeak sounds by rubbing on a bathtub edge. To generate squeaks, it requires some conditions to cause the Stick-slip phenomenon. A bathtub has a smooth surface, and water can cause the phenomenon with human skins. Bathcratch uses it as an interaction technique to play DJ-scratching. Here, we extended the interaction technique using squeaks to recognize rubbing states, rubbing events including sequence, and the difference between squeaks caused by the number of fingers. This can be used in various wet environments including kitchen, washbowls in a restroom, swimming pool, and spa. This paper describes the method and its performance for identifying the number of rubbing fingers by using frequency analysis. In addition, we illustrate some smart home applications by using the proposed technique.",0,44.6540880503
UIST,c580932dfff849a983c5813991a88d42f0d4b1e3,UIST,2016,Next-Point Prediction Metrics for Perceived Spatial Errors,"Mathieu Nancel, Daniel Vogel, Bruno Rodrigues De Araújo, Ricardo Jota, Géry Casiez","1793712, 3076153, 5367447, 1766253, 3051289","Touch screens have a delay between user input and corresponding visual interface feedback, called input 'latency' (or 'lag'). Visual latency is more noticeable during continuous input actions like dragging, so methods to display feedback based on the most likely path for the next few input points have been described in research papers and patents. Designing these 'next-point prediction' methods is challenging, and there have been no standard metrics to compare different approaches. We introduce metrics to quantify the probability of 7 spatial error 'side-effects' caused by next-point prediction methods. Types of side-effects are derived using a thematic analysis of comments gathered in a 12 participants study covering drawing, dragging, and panning tasks using 5 state-of-the-art next-point predictors. Using experiment logs of actual and predicted input points, we develop quantitative metrics that correlate positively with the frequency of perceived side-effects. These metrics enable practitioners to compare next-point predictors using only input logs.",0,44.6540880503
UIST,7cecd69662651f61d78498a6c3ed9ecd711c5358,UIST,2009,Sikuli: using GUI screenshots for search and automation,"Tom Yeh, Tsung-Hsiang Chang, Rob Miller","1704158, 2337269, 1723785","We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of the element's name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.",110,94.2857142857
UIST,750a39ed63577675aebad47b6dd21cc504689dc7,UIST,1991,Smoothly integrating rule-based techniques into a direct manipulation interface builder,"Scott E. Hudson, Andrey K. Yeatts","1749296, 2947165","Work in automating the production of user interf%ce software has recently concentrated on two distinct approaches: systems that provide a direct manipulation editor for specifying user interfaces and systems that attempt to automatically generate much or all of the interface. This paper considers how a middle ground between these approaches can be constructed. It presents a technique whereby the rule-base inference methods used in many automatic generation systems can be smoothly integrated into a direct manipulation interface builder. This integration is achieved by explicitly representing the results of inference rules in the direct manipulation framework and by using semantic snapping techniques to give the user direct feedback and interactive control over the application of rules. INTRODUCTION For a number of years research in user interface management systems has sought to automate the task of producing user interfaces. Recently, two major directions have emerged from this work. One set of systems — often called interface builders — seeks to provide environments or",16,43.4782608696
UIST,69310027481e60ccd3790fb1069492e8ce759176,UIST,2010,Pinstripe: eyes-free continuous input anywhere on interactive clothing,"Thorsten Karrer, Moritz Wittenhagen, Florian Heller, Jan O. Borchers","3301123, 2236222, 2240566, 1692837","We present Pinstripe, a textile user interface element for eyes-free, continuous value input on smart garments that uses pinching and rolling a piece of cloth between your fingers. Input granularity can be controlled by the amount of cloth pinched. Pinstripe input elements are invisible, and can be included across large areas of a garment. Pinstripe thus addresses several problems previously identified in the placement and operation of textile UI elements on smart clothing.",4,48.2558139535
UIST,954ee43698dfbf86f374198a0b86f774a62baedf,UIST,2005,DTLens: multi-user tabletop spatial data exploration,"Clifton Forlines, Chia Shen","1694854, 1697008","Supporting groups of individuals exploring large maps and design diagrams on interactive tabletops is still an open research problem. Today's geospatial, mechanical engineering and CAD design applications are mostly single-user, keyboard and mouse-based desktop applications. In this paper, we present the design of and experience with DTLens, a new zoom-in-context, multi-user, two-handed, multi-lens interaction technique that enables group exploration of spatial data with multiple individual lenses on the same direct-touch interactive tabletop. DTLens provides a set of consistent interactions on lens operations, thus minimizes tool switching by users during spatial data exploration.",68,79.0322580645
UIST,84fe36b0662f383a3a686fb99953ac045e4c613a,UIST,2014,Expert crowdsourcing with flash teams,"Daniela Retelny, Sébastien Robaszkiewicz, Alexandra To, Walter S. Lasecki, Jay Patel, Negar Rahmati, Tulsee Doshi, Melissa A. Valentine, Michael S. Bernstein","2906714, 3068383, 2097573, 2598433, 1797164, 2374500, 2155007, 2043772, 3047089","We introduce flash teams, a framework for dynamically assembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accomplishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked modular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams' structures: for example, flash teams can be recombined to form larger organizations and authored automatically in response to a user's request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline intermediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring platform and runtime manager. Foundry allows users to author modular tasks, then manages teams through handoffs of intermediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including design prototyping, course development, and film animation, in half the work time of traditional self-managed teams.",37,99.2248062016
UIST,258504a98e0f147fd986f0b1ba085ff26a4586ee,UIST,2016,Foundry: Hierarchical Material Design for Multi-Material Fabrication,"Kiril Vidimce, Alexandre Kaspar, Ye Wang, Wojciech Matusik","2891782, 3035284, 1681196, 1752521","We demonstrate a new approach for designing functional material definitions for multi-material fabrication using our system called Foundry. Foundry provides an interactive and visual process for hierarchically designing spatially-varying material properties (e.g., appearance, mechanical, optical). The resulting meta-materials exhibit structure at the micro and macro level and can surpass the qualities of traditional composites. The material definitions are created by composing a set of <i>operators</i> into an <i>operator graph</i>. Each operator performs a volume decomposition operation, remaps space, or constructs and assigns a material composition. The operators are implemented using a domain-specific language for multi-material fabrication; users can easily extend the library by writing their own operators. Foundry can be used to build operator graphs that describe complex, parameterized, resolution-independent, and reusable material definitions. We also describe how to stage the evaluation of the final material definition which in conjunction with progressive refinement, allows for interactive material evaluation even for complex designs. We show sophisticated and functional parts designed with our system.",0,44.6540880503
UIST,448c50e3d01468b5b92e4d3020247af54614f3af,UIST,2016,Towards Understanding Collaboration around Interactive Surfaces: Exploring Joint Visual Attention,"Hidde van der Meulen, Petra Varsanyi, Lauren Westendorf, Andrew L. Kun, Orit Shaer","3492432, 3102391, 2360896, 1693270, 1718579","In this abstract, we present a novel method for exploring the visual behavior of multiple users engaged in a collaborative task around an interactive surface. The proposed method synchronizes input from multiple eye trackers, describes the visual behavior of individual users over time, and identifies joint attention across multiple users. We applied this method to analyze the visual behavior of four users collaborating using a large-scale multi-touch tabletop.",0,44.6540880503
UIST,141ffb7342b1e9f2c86b10bd913d1fac0c3605b6,UIST,2014,"RichReview: blending ink, speech, and gesture to support collaborative document review","Dongwook Yoon, Nicholas Chen, François Guimbretière, Abigail Sellen","2055005, 1873400, 2539134, 1693025","This paper introduces a novel document annotation system that aims to enable the kinds of rich communication that usually only occur in face-to-face meetings. Our system, RichReview, lets users create annotations on top of digital documents using three main modalities: freeform inking, voice for narration, and deictic gestures in support of voice. RichReview uses novel visual representations and time-synchronization between modalities to simplify annotation access and navigation. Moreover, RichReview's versatile support for multi-modal annotations enables users to mix and interweave different modalities in threaded conversations. A formative evaluation demonstrates early promise for the system finding support for voice, pointing, and the combination of both to be especially valuable. In addition, initial findings point to the ways in which both content and social context affect modality choice.",8,75.9689922481
UIST,025b254f164144f57486d98aef1ea8222a03d2de,UIST,2013,TextTearing: opening white space for digital ink annotation,"Dongwook Yoon, Nicholas Chen, François Guimbretière","2055005, 1873400, 2539134",Having insufficient space for making annotations is a problem that afflicts both paper and digital documents. We introduce the TextTearing technique for in situ expansion of inter-line whitespace and pair it with a lightweight interaction for margin expansion as a way to address this problem. The full system leverages the dynamism of digital documents and employs a bimanual design that combines the precision of pen with the fluidity of touch. Our evaluation found that a simpler unimanual variant of TextTearing was preferred over direct annotation and margin-only expansion. Direct annotation in naturally occurring whitespace was least preferred.,5,55.9633027523
UIST,0fb495f61357823649723a4def452e23cee1bb8c,UIST,2000,Multimodal system processing in mobile environments,Sharon L. Oviatt,2807460,"One major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology, such as a spoken language system. In recent years, the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures. In the present studies, over 2,600 utterances processed by a multimodal pen/voice system were collected during both mobile and stationary use. A new data collection infrastructure was developed, including instrumentation worn by the user while roaming, a researcher field station, and a multimodal data logger and analysis tool tailored for mobile research. Although speech recognition as a stand-alone failed more often during mobile system use, the results confirmed that a more stable multimodal architecture decreased this error rate by 19-35%. Furthermore, these findings were replicated across different types of microphone technology. In large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture, with higher levels occurring in the noisy mobile environment. Implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner.",63,60.0
UIST,bfe202fc4fc3444c7ed153e76c7daeafdb8f1f0a,UIST,2001,TSI (teething ring sound instrument): a design of the sound instrument for the baby,"Naoko Kubo, Kazuhiro Jo, Ken Matsunaga","3342983, 3093077, 2684946","In this paper, we will describe the TSI (Teething ring Sound Instrument), a new sound instrument given to babies, which consists of a teething ring, a knob, an I-CubeX Digitizer [1] and a computer which processes MIDI messages. The TSI is designed to bring music experience to baby with the movement of the babies reflex sucking motion. We provided the TSI to a baby and observed her action to the TSI and her reaction to the generated sound. This experiment showed the high potential of the TSI.",0,3.33333333333
UIST,916236ecd5851381be43b1758aac32c135995115,UIST,2012,FlowBlocks: a multi-touch ui for crowd interaction,"Florian Block, Daniel J. Wigdor, Brenda Caldwell Phillips, Michael S. Horn, Chia Shen","2171988, 1961958, 2194904, 3263438, 1697008","Multi-touch technology lends itself to collaborative crowd interaction (CI). However, common tap-operated widgets are impractical for CI, since they are susceptible to accidental touches and interference from other users. We present a novel multi-touch interface called <i>FlowBlocks</i> in which every UI action is invoked through a small sequence of user actions: dragging parametric <i>UI-Blocks</i>, and dropping them over operational <i>UI-Docks</i>. The FlowBlocks approach is advantageous for CI because it a) makes accidental touches inconsequential; and b) introduces design parameters for mutual awareness, concurrent input, and conflict management. FlowBlocks was successfully used on the floor of a busy natural history museum. We present the complete design space and describe a year-long iterative design and evaluation process which employed the Rapid Iterative Test and Evaluation (RITE) method in a museum setting.",6,52.4509803922
UIST,d0af01e9267dadca76473f1eaf9782550b166afb,UIST,2016,Watch Commander: A Gesture-based Invocation System for Rectangular Smartwatches using B2B-Swipe,"Yuki Kubo, Buntarou Shizuki, Shin Takahashi","2054058, 1765222, 2156983","We present Watch Commander, a gesture-based invocation system for rectangular smartwatches. Watch Commander allows the user to invoke functions easily and quickly by using Bezel to Bezel-Swipe (B2B-Swipe). This is because B2B-Swipe does not conflict with other swipe gestures such as flick and bezel swipe and can be performed in an eyes-free manner. Moreover, by providing GUIs that display functions assigned with B2B-Swipe, Watch Commander helps the user memorize those functions.",0,44.6540880503
UIST,569553e16266c988e00f43b9a72f8646009619f7,UIST,2016,Mining Controller Inputs to Understand Gameplay,"Brian A. Smith, Shree K. Nayar","3008842, 1750470","Today's game analytics systems are powered by event logs, which reveal information about what players are doing but offer little insight about the types of gameplay that games foster. Moreover, the concept of gameplay itself is difficult to define and quantify. In this paper, we show that analyzing players' controller inputs using probabilistic topic models allows game developers to describe the types of gameplay -- or action -- in games in a quantitative way. More specifically, developers can discover the types of action that a game fosters and the extent that each game level fosters each type of action, all in an unsupervised manner. They can use this information to verify that their levels feature the appropriate style of gameplay and to recommend levels with gameplay that is similar to levels that players like. We begin with latent Dirichlet allocation (LDA), the simplest topic model, then develop the <i>player-gameplay action (PGA) model</i> to make the same types of discoveries about gameplay in a way that is independent of each player's play style. We train a player recognition system on the PGA model's output to verify that its discoveries about gameplay are in fact independent of each player's play style. The system recognizes players with over 90% accuracy in about 20 seconds of playtime.",0,44.6540880503
UIST,59443f4964d8fa8a05a1888a828939d1752c65ff,UIST,2002,That one there! Pointing to establish device identity,"Colin Swindells, Kori Inkpen Quinn, John Dill, Melanie Tory","1794909, 2735374, 1727262, 2441204","Computing devices within current work and play environments are relatively static. As the number of 'networked' devices grows, and as people and their devices become more dynamic, situations will commonly arise where users will wish to use 'that device there' instead of navigating through traditional user interface widgets such as lists. This paper describes a process for identifying devices through a pointing gesture using custom tags and a custom stylus called the gesturePen. Implementation details for this system are provided along with qualitative and quantitative results from a formal user study. As ubiquitous computing environments become more pervasive, people will rapidly switch their focus between many computing devices. The results of our work demonstrate that our gesturePen method can improve the user experience in ubiquitous environments by facilitating significantly faster interactions between computing devices.",66,75.0
UIST,3dc376866ee84d62a6c5102c05f3e8fa3ad479ca,UIST,2000,"The architecture and implementation of CPN2000, a post-WIMP graphical application","Michel Beaudouin-Lafon, Henry Michael Lassen","1682346, 2553425","We have developed an interface for editing and simulating Coloured Petri Nets based on toolglasses, marking menus and bi-manual interaction, in order to understand how novel interaction techniques could be supported by a new generation of user interface toolkits. The architecture of CPN2000 is based on three components: the Document Structure stores all the persistent data in the system; the Display Structure represents the contents of the screen and implements rendering and hit detection algorithms; and the Input Structure uses ""instruments"" to manage interaction. The rendering engine is based on OpenGL and a number of techniques have been developed to take advantage of 3D accelerated graphics for a 2D application. Performance data show that high frame rates have been achieved with off-the-shelf hardware even with a non-optimized redisplay. This work paves the way towards a post-WIMP UI toolkit.",36,44.0
UIST,165c9d60988b9a137c4bb13b882b735117e7bed5,UIST,2010,CodeGraffiti: communication by sketching for pair programmers,"Leonhard Lichtschlag, Jan O. Borchers","2831408, 1692837","In <i>pair programming</i>, two software developers work on their code together in front of a single workstation, one typing, the other commenting. This frequently involves pointing to code on the screen, annotating it verbally, or sketching on paper or a nearby whiteboard, little of which is captured in the source code for later reference. <i>CodeGraffiti</i> lets pair programmers simultaneously write their code, and annotate it with ephemeral and persistent sketches on screen using touch or pen input. We integrated CodeGraffiti into the <i>Xcode</i> software development environment, to study how these techniques may improve the pair programming workflow.",4,48.2558139535
UIST,c78880d5dca0a8a76f0efae2c59f087fed68759b,UIST,2016,Partial Bookmarking: A Structure-independent Mechanism of Transclusion for a Portion of any Web Page,"Takehiro Nagatomo, Takahiro Tachibana, Keizo Sato, Makoto Nakashima","3492303, 3492661, 2626706, 2591101","A novel mechanism of transclusion for collecting and producing information on the Web, named partial bookmarking, is proposed. Partial bookmarking allows a user to collect portions of any web page by making it able to use for a spatial hypertext, like a web document element, without the need to duplicate its contents. Whereas the previous studies involving transclusion required pre-designed linkable objects, such as XML elements or HTML objects, partial bookmarking does not rely on any document structure. To accomplish partial bookmarking, we enhanced a conventional web browser with multiple tabs by introducing the technology of mirroring to display only a portion of a web page appropriately while factoring in potential copyright issues.",0,44.6540880503
UIST,2b8f8fef977dd975eee8448061055460f48c41b7,UIST,2011,MUST-D: multi-user see through display,"Abhijit Karnik, Walterio W. Mayol-Cuevas, Sriram Subramanian","2556545, 1731214, 1702794","In this paper we present MUST-D, a multi-user see-through display that allows users to inspect objects behind a glass panel while projecting view-dependent information on the glass to the user. MUST-D uses liquid crystal panels to implement a multi-view see-through display space in front of physical objects.",0,6.19047619048
UIST,1dd454ec90d3a3923fb9158e1bf927abd6142de3,UIST,2012,3D puppetry: a kinect-based interface for 3D animation,"Robert Held, Ankit Gupta, Brian Curless, Maneesh Agrawala","5770994, 1831230, 1810052, 1820412","We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera \\rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.",30,86.2745098039
UIST,02f872de0dc3f1d54ba68f9d751b7828f64d189c,UIST,2011,KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera,"Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard A. Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew J. Davison, Andrew W. Fitzgibbon","1699068, 6308833, 2531379, 2032716, 3016252, 1685185, 1751781, 1736330, 2998223, 1678391, 1708974","KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.",403,100.0
UIST,9543d7348120959aae0fb8c46a0ba63eb0f301ab,UIST,2015,"On Sounder Ground: CAAT, a Viable Widget for Affective Reaction Assessment","Bruno Cardoso, Osvaldo Santos, Teresa Romão","1999945, 3563963, 8150772","The reliable assessment of affective reactions to stimuli is paramount in a variety of scientific fields, including HCI (Human-Computer Interaction). Variation of emotional states over time, however, warrants the need for quick measurements of emotions. To address it, new tools for quick assessments of affective states have been developed. In this work, we explore the CAAT (Circumplex Affective Assessment Tool), an instrument with a unique design in the scope of affect assessment -- a graphical control element -- that makes it amenable to seamless integration in user interfaces. We briefly describe the CAAT and present a multi-dimensional evaluation that evidences the tool's viability. We have assessed its test-retest reliability, construct validity and quickness of use, by collecting data through an unsupervised, web-based user study. Results show high test-retest reliability, evidence the tool's construct validity and confirm its quickness of use, making it a good fit for longitudinal studies and systems requiring quick assessments of emotional reactions.",2,60.5263157895
UIST,73a222f8ddee34ce4ae7c35a25a534979bb35b3d,UIST,2000,Providing visually rich resizable images for user interface components,"Scott E. Hudson, Kenichiro Tanaka","1749296, 2946655","User interface components such as buttons, scrollbars, menus, as well as various types of containers and separators, normally need to be resizable so that they can conform to the needs of the contents within them, or the environment in which they are placed. Unfortunately, in the past, providing dynamically resizable component appearances has required writing code to draw the component. As a result, visual designers have often been cut off from the ability to create these appearances. Even when visual designers can be involved, drawing programmatically is comparatively very difficult. Hence, components created this way have not typically contained artistically rich appearances. Because of this need to write drawing code, component appearances have traditionally been quite plain, and have been controlled primarily by a few toolkit writers. This paper presents a suite of very simple techniques, along with a few composition mechanisms, that are designed to overcome this problem. These techniques allow visually rich, dynamically resizable, images to be provided using primarily conventional drawing tools (and with no programming or programming-like activities at all).",6,8.0
UIST,3ab7517a13b07a5396c21660f70a21ee06b6a124,UIST,2001,Aesthetic information collages: generating decorative displays that contain information,"James Fogarty, Jodi Forlizzi, Scott E. Hudson","1738171, 1767344, 1749296","Normally, the primary purpose of an information display is to convey information. If information displays can be aesthetically interesting, that might be an added bonus. This paper considers an experiment in reversing this imperative. It describes the <i>Kandinsky</i> system which is designed to create displays which are first aesthetically interesting, and then as an added bonus, able to convey information. The Kandinsky system works on the basis of aesthetic properties specified by an artist (in a visual form). It then explores a space of collages composed from information bearing images, using an optimization technique to find compositions which best maintain the properties of the artist's aesthetic expression.",67,60.0
UIST,17212526ae5b1c395c7611257d6a2b3a07d0d6bb,UIST,2004,Video-based document tracking: unifying your physical and electronic desktops,"Jiwon Kim, Steven M. Seitz, Maneesh Agrawala","7951354, 1679223, 1820412","This paper presents an approach for tracking paper documents on the desk over time and automatically linking them to the corresponding electronic documents using an overhead video camera. We demonstrate our system in the context of two scenarios, &#60;i>paper tracking&#60;/i> and &#60;i>photo sorting&#60;/i>. In the paper tracking scenario, the system tracks changes in the stacks of printed documents and books on the desk and builds a complete representation of the spatial structure of the desktop. When users want to find a printed document buried in the stacks, they can query the system based on appearance, keywords, or access time. The system also provides a &#60;i>remote desktop&#60;/i> interface for directly browsing the physical desktop from a remote location. In the photo sorting scenario, users sort printed photographs into physical stacks on the desk. The systemautomatically recognizes the photographs and organizes the corresponding digital photographs into separate folders according to the physical arrangement. Our framework provides a way to unify the physical and electronic desktops without the need for a specialized physical infrastructure except for a video camera.",25,26.3157894737
UIST,4adb9d798bfe55a0ce508da13b785f48094b7234,UIST,2016,Polyspector™: An Interactive Visualization Platform Optimized for Visual Analysis of Big Data,"Xinxiao Li, Akira Kuroda, Hidenori Matsuzaki","1796384, 1797851, 1750892","With the advent of the 'big data' era, there are unprecedented opportunities and challenges to explore complex and large datasets. In the paper, we introduce Polyspector, a web-based interactive visualization platform optimized for interactive visual analysis with two distinguishing features. Firstly, a visualization-specific database engine based on pixel-aware aggregation is implemented to generate views of hundreds of millions of data items within a second even with an off-the-shelf PC. Secondly, a novel deep-linking mechanism, combined with the pixel-aware aggregation, is exploited to realize interactive visual analysis interfaces such as zooming, overview + detail, context + focus etc.",0,44.6540880503
UIST,ca7740f1b90e3a34fa82d39223962d057e51f659,UIST,2016,Crowdsourced Fabrication,"Benjamin J. Lafreniere, Tovi Grossman, Fraser Anderson, Justin Matejka, Heather Kerrick, Danil Nagy, Lauren Vasey, Evan Atherton, Nicholas Beirne, Marcelo H. Coelho, Nicholas Cote, Steven Li, Andy Nogueira, Long Nguyen, Tobias Schwinn, James Stoddart, David Thomasson, Ray Wang, Thomas White, David Benjamin, Maurice Conti, Achim Menges, George W. Fitzmaurice","2813134, 3313809, 3118963, 2578065, 3459486, 3460058, 3461033, 3446656, 3491562, 3491498, 3491488, 1733107, 3492130, 6766427, 3476704, 7140677, 4447226, 3037944, 5809194, 6456800, 3491624, 1714012, 1703735","In recent years, extensive research in the HCI literature has explored interactive techniques for digital fabrication. However, little attention in this body of work has examined how to involve and guide human workers in fabricating larger-scale structures. We propose a novel model of <i>crowdsourced fabrication</i>, in which a large number of workers and volunteers are guided through the process of building a pre-designed structure. The process is facilitated by an <i>intelligent construction space</i> capable of guiding individual workers and coordinating the overall build process. More specifically, we explore the use of smartwatches, indoor location sensing, and instrumented construction materials to provide real-time guidance to workers, coordinated by a <i>foreman engine</i> that manages the overall build process. We report on a three day deployment of our system to construct a 12-tall bamboo pavilion with assistance from more than one hundred volunteer workers, and reflect on observations and feedback collected during the exhibit.",0,44.6540880503
UIST,4eb81f563f7f15f8d630284777990b4d1c75de02,UIST,2011,Breaking barriers with sound,Ge Wang,6624472,"The computer, in its many shapes and sizes, is evolving rapidly and pervading our everyday lives like never before. Mobile computing devices have become much more than simply ""mobile"", increasingly serving as personal and ""natural"" extensions of us. Therein lies immense potential to reshape the way we think and interact, and especially in how we engage one another creatively, expressively, and socially. This talk explores interaction and social design for music through the computer, told through laptop orchestras, mobile phone orchestras, an audio programming language, designing the iPhone's Ocarina, ecosystems for crowd-sourcing musical creation, and an emerging social dimension where computer, music, and people interact.",1,15.7142857143
UIST,59f14dcb01428ed20f8e3e4e0bdafae57ecdb881,UIST,2014,Content-aware kinetic scrolling for supporting web page navigation,"Juho Kim, Amy Xian Zhang, Ji Hee Kim, Rob Miller, Krzysztof Z. Gajos","1800981, 1788613, 2226300, 1723785, 1770992","Long documents are abundant on the web today, and are accessed in increasing numbers from touchscreen devices such as mobile phones and tablets. Navigating long documents with small screens can be challenging both physically and cognitively because they compel the user to scroll a great deal and to mentally filter for important content. To support navigation of long documents on touchscreen devices, we introduce content-aware kinetic scrolling, a novel scrolling technique that dynamically applies pseudo-haptic feedback in the form of friction around points of high interest within the page. This allows users to quickly find interesting content while exploring without further cluttering the limited visual space. To model degrees of interest (DOI) for a variety of existing web pages, we introduce social wear, a method for capturing DOI based on social signals that indicate collective user interest. Our preliminary evaluation shows that users pay attention to items with kinetic scrolling feedback during search, recognition, and skimming tasks.",2,41.4728682171
UIST,3da381501e7555b2f454c9e2f74830f40f90a5f1,UIST,2016,Reading and Learning Smartfonts,"Danielle Bragg, Shiri Azenkot, Adam Tauman Kalai","1803855, 3283573, 2186481","As small displays on devices like smartwatches become increasingly common, many people have difficulty reading the text on these displays. Vision conditions like presbyopia that result in blurry near vision make reading small text particularly hard. We design multiple different scripts for displaying English text, legible at small sizes even when blurry, for small screens such as smartphones and smartwatches. These ""smartfonts"" redesign visual character presentations to improve the reading experience. Like cursive, Grade 1 Braille, and ordinary fonts, they preserve orthography and spelling. They have the potential to enable people to read more text comfortably on small screens, e.g., without reading glasses. To simulate presbyopia, we blur images and evaluate their legibility using paid crowdsourcing. We also evaluate the difficulty of learning to read smartfonts and observe a learnability/legibility trade-off. Our most learnable smartfont can be read at roughly half the speed of Latin after two thousand practice sentences. It is also legible smaller than half the size of traditional Latin (i.e. ""English"") when blurry.",0,44.6540880503
UIST,250b9cb4d4744d93794b8c7bc1d872c36ec645d4,UIST,2009,Mouse 2.0: multi-touch meets the mouse,"Nicolas Villar, Shahram Izadi, Dan Rosenfeld, Hrvoje Benko, John Helmes, Jonathan Westhues, Steve Hodges, Eyal Ofek, Alex Butler, Xiang Cao, Billy Chen","1707115, 1699068, 3064582, 2704133, 1800990, 2711385, 1736330, 1735652, 1684414, 7299595, 8567761","In this paper we present novel input devices that combine the standard capabilities of a computer mouse with multi-touch sensing. Our goal is to enrich traditional pointer-based desktop interactions with touch and gestures. To chart the design space, we present five different multi-touch mouse implementations. Each explores a different touch sensing strategy, which leads to differing form-factors and hence interactive possibilities. In addition to the detailed description of hardware and software implementations of our prototypes, we discuss the relative strengths, limitations and affordances of these novel input devices as informed by the results of a preliminary user study.",49,71.4285714286
UIST,536cef6e705d14032646b46291fa4a91e25bf2f8,UIST,2016,Exploring the Design Space for Energy-Harvesting Situated Displays,"Tobias Alexander Große-Puppendahl, Steve Hodges, Nicholas Chen, John Helmes, Stuart Taylor, James Scott, Josh Fromm, David Sweeney","2044747, 1736330, 1873400, 1800990, 1683873, 5675901, 3395955, 2774999","We explore the design space of energy-neutral situated displays, which give physical presence to digital information. We investigate three central dimensions: energy sources, display technologies, and wireless communications. Based on the power implications from our analysis, we present a thin, wireless, photovoltaic-powered display that is quick and easy to deploy and capable of indefinite operation in indoor lighting conditions. The display uses a low-resolution e-paper architecture, which is 35 times more energy-efficient than smaller-sized high-resolution displays. We present a detailed analysis on power consumption, photovoltaic energy harvesting performance, and a detailed comparison to other display-driving architectures. Depending on the ambient lighting, the display can trigger an update every 1 -- 25 minutes and communicate to a PC or smartphone via Bluetooth Low-Energy.",0,44.6540880503
UIST,c11202d0086e8082713259b1d7b759761c7f9a2d,UIST,2006,From information visualization to direct manipulation: extending a generic visualization framework for the interactive editing of large datasets,Thomas Baudel,3020839,"Today's generic data management applications such as accounting, CRM or logging and tracking software, rely on form and menu based interfaces. These applications take only marginal advantage of current graphical user interfaces. This is because the data they handle does not have intrinsic visual representations upon which direct manipulation principles can be used. This article presents how we have extended an Information Visualization framework with generic data manipulation functions. These new data editing capabilities are tuned to take advantage of the characteristics of each view. They enable us to generalize the direct manipulation mechanisms to address many abstract data manipulation needs. In this article we present five uses of the features we have implemented and deduce a general workflow applicable to a variety of contexts. The workflow comprises three steps and five editing actions. The steps are: adjust view, select, and edit. The editing actions are: edit a value or group of values, clone objects, remove objects, add attributes, and remove attributes. The workflow provides complete editing access to table and hierarchical data structures using particularly terse interaction methods. It defines a general data editing model that enables powerful data manipulation tasks without requiring end-user programming or scripting.",18,27.5
UIST,de675eaf920e85c5d954a5dfed59c25970545cd6,UIST,2010,Memento: unifying content and context to aid webpage re-visitation,"Chinmay Kulkarni, Santosh Raju, Raghavendra Udupa","4924730, 2326628, 2360916","While users often revisit pages on the Web, tool support for such re-visitation is still lacking. Current tools (such as browser histories) only provide users with basic information such as the date of the last visit and title of the page visited. In this paper, we describe a system that provides users with descriptive topic-phrases that aid re-finding. Unlike prior work, our system considers both the <i>content</i> of a webpage and the <i>context</i> in which the page was visited. Preliminary evaluation of this system suggests users find this approach of combining content with context useful.",0,9.3023255814
UIST,93c955a5a321c7dd57bd23d58e92f3351805da76,UIST,2004,"The MaggLite post-WIMP toolkit: draw it, connect it and run it","Stéphane Huot, Cédric Dumas, Pierre Dragicevic, Jean-Daniel Fekete, Gérard Hégron","1809689, 2732793, 3297322, 1721432, 1682746","This article presents MaggLite, a toolkit and sketch-based interface builder allowing fast and interactive design of post-WIMP user interfaces. MaggLite improves design of advanced UIs thanks to its novel &#60;i>mixed-graph&#60;/i> architecture that dynamically combines scene-graphs with interaction-graphs. &#60;i>Scene-graphs&#60;/i> provide mechanisms to describe and produce rich graphical effects, whereas &#60;i>interaction-graphs&#60;/i> allow expressive and fine-grained description of advanced interaction techniques and behaviors such as multiple pointers management, toolglasses, bimanual interaction, gesture, and speech recognition. Both graphs can be built interactively by sketching the UI and specifying the interaction using a dataflow visual language. Communication between the two graphs is managed at runtime by components we call &#60;i>Interaction Access Points&#60;/i>. While developers can extend the toolkit by refining built-in generic mechanisms, UI designers can quickly and interactively design, prototype and test advanced user interfaces by applying the MaggLite principle: ""draw it, connect it and run it"".",26,28.9473684211
UIST,2d74221a6521f7e5cf42347d9134cc383d7a5829,UIST,2003,Perceptually-supported image editing of text and graphics,"Eric Saund, David J. Fleet, Daniel Larner, James Mahoney","1763321, 1793739, 2058659, 4737300","This paper presents a novel image editing program emphasizing easy selection and manipulation of material found in informal, casual documents such as sketches, handwritten notes, whiteboard images, screen snapshots, and scanned documents. The program, called <i>ScanScribe</i>, offers four significant advances. First, it presents a new, intuitive model for maintaining image objects and groups, along with underlying logic for updating these in the course of an editing session. Second, ScanScribe takes advantage of newly developed image processing algorithms to separate foreground markings from a white or light background, and thus can automatically render the background transparent so that image material can be rearranged without occlusion by background pixels. Third, ScanScribe introduces new interface techniques for selecting image objects with a pointing device without resorting to a palette of tool modes. Fourth, ScanScribe presents a platform for exploiting image analysis and recognition methods to make perceptually significant structure readily available to the user. As a research prototype, ScanScribe has proven useful in the work of members of our laboratory, and has been released on a limited basis for user testing and evaluation.",62,54.1666666667
UIST,296094cd3f9e12be5983e0603e2a50a08fd4bf2e,UIST,2000,Dasher - a data entry interface using continuous gestures and language models,"David J. Ward, Alan F. Blackwell, David J. C. MacKay","7525509, 1784157, 2246069","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT Existing devices for communicating information to computers are bulky, slow to use, or unreliable. Dasher is a new interface incorporating language modelling and driven by continuous two-dimensional gestures, e.g. a mouse, touchscreen, or eye-tracker. Tests have shown that this device can be used to enter text at a rate of up to 34 words per minute, compared with typical ten-finger keyboard typing of 40–60 words per minute. Although the interface is slower than a conventional keyboard , it is small and simple, and could be used on personal data assistants and by motion–impaired computer users.",151,88.0
UIST,1c7ee68b053439d09aff75261754012e54113594,UIST,2012,The FreeD: a handheld digital milling device for craft and fabrication,"Amit Zoran, Joseph A. Paradiso","2866829, 4798651","We present an approach to combine digital fabrication and craft that is focused on a new fabrication experience. The FreeD is a hand-held, digitally controlled, milling device. It is guided and monitored by a computer while still preserving gestural freedom. The computer intervenes only when the milling bit approaches the 3D model, which was designed beforehand, either by slowing down the spindle's speed or by drawing back the shaft. The rest of the time it allows complete freedom, allowing the user to manipulate and shape the work in any creative way. We believe The FreeD will enable a designer to move in between the straight boundaries of established CAD systems and the free expression of handcraft.",6,52.4509803922
UIST,c910692c113389aaec0cc34d0e349e01e83bad38,UIST,2001,Real-time audio buffering for telephone applications,"Paul H. Dietz, William S. Yerazunis","1805795, 5135329","A system that uses an ear proximity sensor to actively manage periods of distraction during telephone conversations is described. We detect when the phone is removed from the ear, record any incoming audio, and play it back when the phone is returned to the ear. By dropping silent intervals and speeding up playback with a pitch-preserving algorithm, we quickly return to real-time without the loss of information. This real-time audio buffering technique also allows us to create a user-activated, lossless instant replay function.",26,36.6666666667
UIST,36978e23f83b535c14000d3046a60221397ee5c6,UIST,2014,Integrating optical waveguides for display and sensing on pneumatic soft shape changing interfaces,"Lining Yao, Jifei Ou, Daniel Tauber, Hiroshi Ishii","2912448, 3163859, 3313285, 1749649","We introduce the design and fabrication process of integrating optical fiber into pneumatically driven soft composite shape changing interfaces. Embedded optical waveguides can provide both sensing and illumination, and add one more building block to the design of designing soft pneumatic shape changing interfaces.",0,12.015503876
UIST,042a1d7e76d8a24851451099c32ede833e540572,UIST,2011,CrowdForge: crowdsourcing complex work,"Aniket Kittur, Boris Smus, Susheel Khamkar, Robert E. Kraut","1717650, 2449135, 2818656, 1702853","Micro-task markets such as Amazon's Mechanical Turk represent a new paradigm for accomplishing work, in which employers can tap into a large population of workers around the globe to accomplish tasks in a fraction of the time and money of more traditional methods. However, such markets typically support only simple, independent tasks, such as labeling an image or judging the relevance of a search result. Here we present a general purpose framework for micro-task markets that provides a scaffolding for more complex human computation tasks which require coordination among many individuals, such as writing an article.",202,99.0476190476
UIST,59bb1b423a17bdac54d9b4412ac216618cb68c53,UIST,2013,Human-computer interaction for hybrid carving,"Amit Zoran, Roy Shilkrot, Joseph A. Paradiso","2866829, 2509354, 4798651","In this paper we explore human-computer interaction for carving, building upon our previous work with the FreeD digital sculpting device. We contribute a new tool design (FreeD V2), with a novel set of interaction techniques for the fabrication of static models: personalized tool paths, manual overriding, and physical merging of virtual models. We also present techniques for fabricating dynamic models, which may be altered directly or parametrically during fabrication. We demonstrate a semi-autonomous operation and evaluate the performance of the tool. We end by discussing synergistic cooperation between human and machine to ensure accuracy while preserving the expressiveness of manual practice.",18,82.5688073394
UIST,9010f8d0141b19b41d9a983949d71b3388b15c34,UIST,2016,Thermocons: Evaluating the Thermal Haptic Perception of the Forehead,"Roshan Lalintha Peiris, Liwei Chan, Kouta Minamizawa","2041477, 3212475, 1711743","Thermocons describes our work in progress for evaluating thermal haptic feedback on the forehead as a viable feedback modality for integration with head mounted devices. The purpose was to identify the thermal perception for simultaneous feedback at three locations of the forehead. We provided hot-only, cold-only and hot/cold-mixed thermal stimulations at these location to identify the sensitivity for accurate perception. Our evaluation with 9 participants indicated that perceiving cold-only stimulations were significantly better with an accuracy of 88%. The perception accuracy for hot-only and hot/cold-mixed stimulations were 66% and 65% respectively.",0,44.6540880503
UIST,181b1c26b2085949439d8f8d05c43452cf6fc5d9,UIST,2010,Stacksplorer: understanding dynamic program behavior,"Jan-Peter Krämer, Thorsten Karrer, Jonathan Diehl, Jan O. Borchers","1950893, 3301123, 1930500, 1692837","To thoroughly comprehend application behavior, programmers need to understand the interactions of objects at runtime. Today, these interactions are often poorly visualized in common IDEs except during debugging. Stacksplorer allows visualizing and traversing potential call stacks in an application even when it is not running by showing callers and called methods in two columns next to the code editor. The relevant information is gathered from the source code automatically.",2,33.1395348837
UIST,a45aaa3516cec03b6a93002c869ea23f6f12476d,UIST,2011,Role-based interfaces for collaborative software development,Max Goldman,2860413,"Real-time collaboration between multiple simultaneous contributors to a shared document is full of both opportunities and pitfalls, as evidenced by decades of research and industry work in computer-supported cooperative work. In the domain of software engineering, collaboration is still generally achieved either via shared use of a single computer (e.g. pair programming) or with version control (and manual pushing and pulling of changes). By examining and designing for the different roles collaborating programmers play when working synchronously together, we can build real-time collaborative programming systems that make their collaboration more effective. And beyond simple shared editing, we can provide asymmetric, role-specific interfaces on their shared task. Collabode is a web-based IDE for collaborative programming with simultaneous editors that, along with several novel models for closely-collaborative software development, explores the potential of real-time cooperative programming.",2,24.2857142857
UIST,19342ff3c6ff64944856bc2ec464913834c418e0,UIST,2014,Eyes-free text entry interface based on contact area for people with visual impairment,"Taedong Goh, Sang Woo Kim","2733399, 1766505","We developed an eyes-free text entry interface using contact area to determine pressed state for mobile device with touchscreen. The interface gives audio feedback for a touched character similar to VoiceOver of iPhone, but audio feedbacks of two simultaneous touches are considered. A desired character is entered by pressing once. Independent entry of two fingers can reduce movement distance for searching a character. Whole interaction occurs in touched states, additional tactile feedback can be augmented.",0,12.015503876
UIST,a715ae285eb99fa3df1e8fe7117fbe5b0f730b34,UIST,2011,Real-time collaborative coding in a web IDE,"Max Goldman, Greg Little, Rob Miller","2860413, 1715840, 1723785","This paper describes Collabode, a web-based Java integrated development environment designed to support close, synchronous collaboration between programmers. We examine the problem of collaborative coding in the face of program compilation errors introduced by other users which make collaboration more difficult, and describe an algorithm for error-mediated integration of program code. Concurrent editors see the text of changes made by collaborators, but the errors reported in their view are based only on their own changes. Editors may run the program at any time, using only error-free edits supplied so far, and ignoring incomplete or otherwise error-generating changes. We evaluate this algorithm and interface on recorded data from previous pilot experiments with Collabode, and via a user study with student and professional programmers. We conclude that it offers appreciable benefits over naive continuous synchronization without regard to errors and over manual version control.",25,73.3333333333
UIST,528eed628b232d2c92d7ed901fd1adb52ee30f45,UIST,2015,uniMorph: Fabricating Thin Film Composites for Shape-Changing Interfaces,"Felix Heibeck, Basheer Tome, Clark Della Silva, Hiroshi Ishii","2467221, 2780731, 3332772, 1749649","Researchers have been investigating shape-changing interfaces, however technologies for thin, reversible shape change remain complicated to fabricate. uniMorph is an enabling technology for rapid digital fabrication of customized thin-film shape-changing interfaces. By combining the thermoelectric characteristics of copper with the high thermal expansion rate of ultra-high molecular weight polyethylene, we are able to actuate the shape of flexible circuit composites directly. The shape-changing actuation is enabled by a temperature driven mechanism and reduces the complexity of fabrication for thin shape-changing interfaces. In this paper we describe how to design and fabricate thin uniMorph composites. We present composites that are actuated by either environmental temperature changes or active heating of embedded structures and provide a systematic overview of shape-changing primitives. Finally, we present different sensing techniques that leverage the existing copper structures or can be seamlessly embedded into the uniMorph composite. To demonstrate the wide applicability of uniMorph, we present several applications in ubiquitous and mobile computing.",6,85.0877192982
UIST,2edca4294ee9d6c7c9d7e13fb24185c93fecadeb,UIST,2014,Making the web easier to see with opportunistic accessibility improvement,Jeffrey P. Bigham,1744846,"Many people would find the Web easier to use if content was a little bigger, even those who already find the Web possible to use now. This paper introduces the idea of opportunistic accessibility improvement in which improvements intended to make a web page easier to access, such as magnification, are automatically applied to the extent that they can be without causing negative side effects. We explore this idea with oppaccess.js, an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects, such as horizontal scrolling or overlapping text. We validate this approach by magnifying existing web pages 1.6x on average without introducing negative side effects. We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities.",3,49.2248062016
UIST,55603f6517ba1faa0bb93f4e7bbe4c7766a6b8ea,UIST,2004,A remote control interface for large displays,"Azam Khan, George W. Fitzmaurice, Don Almeida, Nicholas Burtnyk, Gordon Kurtenbach","1715535, 1703735, 2133381, 3286011, 1708940","We describe a new widget and interaction technique, known as a ""Frisbee,"" for interacting with areas of a large display that are difficult or impossible to access directly. A frisbee is simply a portal to another part of the display. It consists of a local ""telescope"" and a remote ""target"". The remote data surrounded by the target is drawn in the telescope and interactions performed within it are applied on the remote data. In this paper we define the behavior of frisbees, show unique affordances of the widget, and discuss design characteristics. We have implemented a test application and report on an experiment that shows the benefit of using the frisbee on a large display. Our results suggest that the frisbee is preferred over walking back and forth to the local and remote spaces at a distance of 4.5 feet.",64,68.4210526316
UIST,79fc3ce064e83d498f0d120328c96e56d92c3e16,UIST,2003,Tracking menus,"George W. Fitzmaurice, Azam Khan, Robert Pieké, William Buxton, Gordon Kurtenbach","1703735, 1715535, 2269102, 6037251, 1708940","We describe a new type of graphical user interface widget, known as a ""tracking menu."" A tracking menu consists of a cluster of graphical buttons, and as with traditional menus, the cursor can be moved within the menu to select and interact with items. However, unlike traditional menus, when the cursor hits the edge of the menu, the menu moves to continue tracking the cursor. Thus, the menu always stays under the cursor and close at hand.In this paper we define the behavior of tracking menus, show unique affordances of the widget, present a variety of examples, and discuss design characteristics. We examine one tracking menu design in detail, reporting on usability studies and our experience integrating the technique into a commercial application for the Tablet PC. While user interface issues on the Tablet PC, such as preventing round trips to tool palettes with the pen, inspired tracking menus, the design also works well with a standard mouse and keyboard configuration.",36,35.4166666667
UIST,301a7c38518fecc31cd773ff1ec28fe6a13c2a8f,UIST,2016,MlioLight: Multi-Layered Image Overlay using Multiple Flashlight Devices,"Toshiki Sato, Hideki Koike","3269647, 1684942","We propose a technique that overlays natural images on the real world using the information from multiple flashlight devices. We focus on finding areas of overlapping lights in a multiple light-source scenario and overlaying multi-layered information on a real world object in these areas.In order to mix multiple images, we developed a light identification and overlapping area detection technique using rapid synchronization between high-speed cameras and multiple light devices.In this paper, we describe the concept of our system and a prototype implementation.We also describe two different applications.",0,44.6540880503
UIST,1e6384fce4baff150a7dc1ec49d6a8f67fed56d3,UIST,2002,StyleCam: interactive stylized 3D navigation using integrated spatial & temporal controls,"Nicholas Burtnyk, Azam Khan, George W. Fitzmaurice, Ravin Balakrishnan, Gordon Kurtenbach","3286011, 1715535, 1703735, 1748870, 1708940","This paper describes StyleCam, an approach for authoring 3D viewing experiences that incorporate stylistic elements that are not available in typical 3D viewers. A key aspect of StyleCam is that it allows the author to significantly tailor what the user sees and when they see it. The resulting viewing experience can approach the visual richness and pacing of highly authored visual content such as television commercials or feature films. At the same time, StyleCam allows for a satisfying level of interactivity while avoiding the problems inherent in using unconstrained camera models. The main components of StyleCam are camera surfaces which spatially constrain the viewing camera; animation clips that allow for visually appealing transitions between different camera surfaces; and a simple, unified, interaction technique that permits the user to seamlessly and continuously move between spatial-control of the camera and temporal-control of the animated transitions. Further, the user's focus of attention is always kept on the content, and not on extraneous interface widgets. In addition to describing the conceptual model of StyleCam, its current implementation, and an example authored experience, we also present the results of an evaluation involving real users.",25,29.1666666667
UIST,7eb8f78e2451f3db3c7250017dc69d63b1806c35,UIST,2013,Cross-device eye-based interaction,Jayson Turner,2235543,"Eye-tracking technology is envisaged to become part of our daily life, as its development progresses it becomes more wearable. Additionally there is a wealth of digital content around us, either close to us, on our personal devices or out-of-reach on public displays. The scope of this work aims to combine gaze with mobile input modalities to enable the transfer of content between public and close proximity personal displays. The work contributes enabling technologies, novel interaction techniques, and poses bigger questions that move toward a formalisation of this design space to develop guidelines for the development of future cross-device eye-based interaction methods.",3,45.871559633
UIST,46212bd8dee76c28ed381acf43ded048203291a2,UIST,2011,Stacksplorer: call graph navigation helps increasing code maintenance efficiency,"Thorsten Karrer, Jan-Peter Krämer, Jonathan Diehl, Björn Hartmann, Jan O. Borchers","3301123, 1950893, 1930500, 4020023, 1692837","We present Stacksplorer, a new tool to support source code navigation and comprehension. Stacksplorer computes the call graph of a given piece of code, visualizes relevant parts of it, and allows developers to interactively traverse it. This augments the traditional code editor by offering an additional layer of navigation. Stacksplorer is particularly useful to understand and edit unknown source code because branches of the call graph can be explored and backtracked easily. Visualizing the callers of a method reduces the risk of introducing unintended side effects. In a quantitative study, programmers using Stacksplorer performed three of four software maintenance tasks significantly faster and with higher success rates, and Stacksplorer received a System Usability Scale rating of 85.4 from participants.",15,60.0
UIST,1f6c1f5fb94e63f33d838939059e64e82b94ac67,UIST,2012,PICOntrol: using a handheld projector for direct control of physical devices through visible light,"Dominik Schmidt, David Molyneaux, Xiang Cao","1722031, 2032716, 7299595","Today's environments are populated with a growing number of electric devices which come in diverse form factors and provide a plethora of functions. However, rich interaction with these devices can become challenging if they need be controlled from a distance, or are too small to accommodate user interfaces on their own. In this work, we explore <i>PICOntrol</i>, a new approach using an off-the-shelf handheld pico projector for direct control of physical devices through visible light. The projected image serves a dual purpose by simultaneously presenting a visible interface to the user, and transmitting embedded control information to inexpensive sensor units integrated with the devices. To use PICOntrol, the user points the handheld projector at a target device, overlays a projected user interface on its sensor unit, and performs various GUI-style or gestural interactions. PICOntrol enables direct, visible, and rich interactions with various physical devices without requiring central infrastructure. We present our prototype implementation as well as explorations of its interaction space through various application examples.",27,85.2941176471
UIST,6413d4984f5bf60b2b875d2c817d5ec73663dceb,UIST,2010,Towards personalized surface computing,Dominik Schmidt,1722031,"With recent progress in the field of surface computing it becomes foreseeable that interactive surfaces will turn into a commodity in the future, ubiquitously integrated into our everyday environments. At the same time, we can observe a trend towards personal data and whole applications being accessible over the Internet, anytime from anywhere. We envision a future where interactive surfaces surrounding us serve as powerful portals to access these kinds of data and services. In this paper, we contribute two novel interaction techniques supporting parts of this vision: First, HandsDown, a biometric user identification approach based on hand contours and, second, PhoneTouch, a novel technique for using mobile phones in conjunction with interactive surfaces.",0,9.3023255814
UIST,74722225ccae23639ed51951b79547b2b5421ff9,UIST,2005,Supporting interaction in augmented reality in the presence of uncertain spatial knowledge,"Enylton Machado Coelho, Blair MacIntyre, Simon J. Julier","2215813, 1768774, 1751475","A significant problem encountered when building Augmented Reality (AR) systems is that all spatial knowledge about the world has uncertainty associated with it. This uncertainty manifests itself as registration errors between the graphics and the physical world, and ambiguity in user interaction. In this paper, we show how estimates of the registration error can be leveraged to support predictable selection in the presence of uncertain 3D knowledge. These ideas are demonstrated in osgAR, an extension to OpenSceneGraph with explicit support for uncertainty in the 3D transformations. The osgAR runtime propagates this uncertainty throughout the scene graph to compute robust estimates of the probable location of all entities in the system from the user's viewpoint, in real-time. We discuss the implementation of selection in osgAR, and the issues that must be addressed when creating interaction techniques in such a system.",3,9.67741935484
UIST,11fb9402cb6088658395d2007c900a174d7586a2,UIST,2016,Activity-Aware Video Stabilization for BallCam,"Ryohei Funakoshi, Vishnu Naresh Boddeti, Kris Makoto Kitani, Hideki Koike","7016017, 2232940, 2067961, 1684942","We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow.",0,44.6540880503
UIST,fb04558878bd188fd5223f5329b4ace82c6863ce,UIST,2016,aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design,"Jifei Ou, Mélina Skouras, Nikolaos Vlavianos, Felix Heibeck, Chin-Yi Cheng, Jannik Peters, Hiroshi Ishii","3163859, 2529055, 3064162, 2467221, 7152249, 3491565, 1749649","This paper presents a design, simulation, and fabrication pipeline for making transforming inflatables with various materials. We introduce a bending mechanism that creates multiple, programmable shape-changing behaviors with inextensible materials, including paper, plastics and fabrics. We developed a software tool that generates these bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. We show a range of fabrication methods, from manual sealing, to heat pressing with custom stencils and a custom heat-sealing head that can be mounted on usual 3-axis CNC machines to precisely fabricate the designed transforming material. Finally, we present three applications to show how this technology could be used for designing interactive wearables, toys, and furniture.",0,44.6540880503
UIST,91b90b143be284e9a3b2c420e05d253fc39efca4,UIST,2010,TurKit: human computation algorithms on mechanical turk,"Greg Little, Lydia B. Chilton, Max Goldman, Rob Miller","1715840, 3314354, 2860413, 1723785","Mechanical Turk (<i>MTurk</i>) provides an on-demand source of human computation. This provides a tremendous opportunity to explore algorithms which incorporate human computation as a function call. However, various systems challenges make this difficult in practice, and most uses of MTurk post large numbers of independent tasks. TurKit is a toolkit for prototyping and exploring algorithmic human computation, while maintaining a straight-forward imperative programming style. We present the crash-and-rerun programming model that makes TurKit possible, along with a variety of applications for human computation algorithms. We also present case studies of TurKit used for real experiments across different fields.",167,98.8372093023
UIST,5a397cf9ca943a529a0c75fc0ce020265ec41478,UIST,1991,A nose gesture interface device: extending virtual realities,"Tyson R. Henry, Scott E. Hudson, Andrey K. Yeatts, Brad A. Myers, Steven K. Feiner","1949625, 1749296, 2947165, 1707801, 1809403","This paper reportsl on the development of a nose-machine interface device that provides real-time gesture, position, smell and facial expression information. The DATA NOSETM2— Data AtomaTa CORNUCOPIA pNeumatic Olfactory I/O-deviSE Tactile Manipulation[Olsen86, Myers91]— allows novice users without any formal nose training to perform complex interactive tasks. 1 Hardware Design There are many different types of plastic noses commonly sold in novelty stores [Spencer Gifts]. Several different formats were tried: pig nose, elephant nose, rabbit nose, cow nose, mouse nose, duck nose, witch nose and cat nose. (Figure 1 shows six commonly available alternative nose formats.) Each proved to have serious disadvantages. For example, the pig nose was uncomfortable after several hours of use and the whiskers on the mouse 3 and rabbit noses tended to get caught in printers. Because of its simplicity, sturdy attachment mechanism , and availability, the Groucho Marx nose was cho-*This work was not sponsored at alf by the Kimberly-Clarke Corporation, but we are sure they would approve of it. 1This document was formatted by TfjX. 2DATANOSE is not a registered trademark, the 'M is just part of its name. 3 Problems with the whiskers on the mouse nose directly contradicts previous mouse results which showed that a mouse woufd be best for all tasks [Card78]. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permieaion. sen as our hardware platform. (Note that not all Grou-cho noses are the same. Subtle physical variations can cause subst ant ial differences in usability [Buxton86]). The initial DATANOSETM prototype mounted the nose to a motorcycle helmet [NASA], see Figure 2. After problems with nosebleeds, hay fever and unexplained nausea, we dispensed with the helmet platform and decided to use the traditional sturdy black rimmed glasses that come with the nose. 2 Rhino-Virtual Realities Early rhino-virtual realities relied on rapid horizontal motion—twitching—as the only interaction method [Bewitched]. The addition of modern hardware and software techniques to nose-based virtual realities 'M to use multiple inf,craction allows the DATAN OSE methods.",6,28.2608695652
UIST,d3878777f7559c57339a7ad0c4b23aadd3ee7b89,UIST,2014,Enhancing virtual immersion through tactile feedback,"Mounia Ziat, Taylor Rolison, Andrew Shirtz, Daniel Wilbern, Carrie-Anne Balcer","1761745, 2978466, 3095983, 1860177, 2641254","The lack of tangibility while interacting with virtual objects can be compensated by adding haptic and/or tactile devices or actuators to enhance the user experience. In this demonstration, we present two scenarios that consist of perceiving moving objects on the human body (insects) and feeling physical sensations of virtual thermal objects.",1,31.007751938
UIST,360d4866f9eded89e6f9084f95df8b5f7f2d2251,UIST,2011,"No more bricolage!: methods and tools to characterize, replicate and compare pointing transfer functions","Géry Casiez, Nicolas Roussel","3051289, 1728921","Transfer functions are the only pointing facilitation technique actually used in modern graphical interfaces involving the indirect control of an on-screen cursor. But despite their general use, very little is known about them. We present EchoMouse, a device we created to characterize the transfer functions of any system, and libpointing, a toolkit that we developed to replicate and compare the ones used by Windows, OS X and Xorg. We describe these functions and report on an experiment that compared the default one of the three systems. Our results show that these default functions improve performance up to 24% compared to a unitless constant CD gain. We also found significant differences between them, with the one from OS X improving performance for small target widths but reducing its performance up to 9% for larger ones compared to Windows and Xorg. These results notably suggest replacing the constant CD gain function commonly used by HCI researchers by the default function of the considered systems.",25,73.3333333333
UIST,47928f958dc8da18ddddd8643267f5fa7282d222,UIST,2016,Zooids: Building Blocks for Swarm User Interfaces,"Mathieu Le Goc, Lawrence H. Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre Dragicevic, Sean Follmer","2685181, 2668292, 3491706, 1721432, 3297322, 2770912","This paper introduces <i>swarm user interfaces</i>, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of <i>Zooids</i>, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control. We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.",0,44.6540880503
UIST,7460e0e7be6acbb15e42dbfc86571fbd32b0a004,UIST,2008,Lightweight material detection for placement-aware mobile computing,"Chris Harrison, Scott E. Hudson","1730920, 1749296","Numerous methods have been proposed that allow mobile devices to determine where they are located (e.g., home or office) and in some cases, predict what activity the user is currently engaged in (e.g., walking, sitting, or driving). While useful, this sensing currently only tells part of a much richer story. To allow devices to act most appropriately to the situation they are in, it would also be very helpful to know about their placement - for example whether they are sitting on a desk, hidden in a drawer, placed in a pocket, or held in one's hand - as different device behaviors may be called for in each of these situations. In this paper, we describe a simple, small, and inexpensive multispectral optical sensor for identifying materials in proximity to a device. This information can be used in concert with e.g., location information, to estimate, for example, that the device is ""sitting on the desk at home"", or ""in the pocket at work"". This paper discusses several potential uses of this technology, as well as results from a two-part study, which indicates that this technique can detect placement at 94.4% accuracy with real-world placement sets.",17,37.1428571429
UIST,e0714f7a9af1f8da7cf8639d486a9f666b0f404d,UIST,2008,"Scratch input: creating large, inexpensive, unpowered and mobile finger input surfaces","Chris Harrison, Scott E. Hudson","1730920, 1749296","We present Scratch Input, an acoustic-based input technique that relies on the unique sound produced when a fingernail is dragged over the surface of a textured material, such as wood, fabric, or wall paint. We employ a simple sensor that can be easily coupled with existing surfaces, such as walls and tables, turning them into large, unpowered and ad hoc finger input surfaces. Our sensor is sufficiently small that it could be incorporated into a mobile device, allowing any suitable surface on which it rests to be appropriated as a gestural input surface. Several example applications were developed to demonstrate possible interactions. We conclude with a study that shows users can perform six Scratch Input gestures at about 90% accuracy with less than five minutes of training and on wide variety of surfaces.",62,80.0
UIST,80ffd5e89473d1096e6f848f10b8929b54a4d1b5,UIST,2008,Foldable interactive displays,"Johnny C. Lee, Scott E. Hudson, Edward Tse","1803308, 1749296, 2371546","Modern computer displays tend to be in fixed size, rigid, and rectilinear rendering them insensitive to the visual area demands of an application or the desires of the user. Foldable displays offer the ability to reshape and resize the interactive surface at our convenience and even permit us to carry a very large display surface in a small volume. In this paper, we implement four interactive foldable display designs using image projection with low-cost tracking and explore display behaviors using orientation sensitivity.",39,71.4285714286
UIST,038fcd66667c5bbb99304eb8356f6d055684037e,UIST,2015,KickSoul: A Wearable System for Feet Interactions with Digital Devices,"Xavier Benavides, Chang Long Zhu, Pattie Maes, Joseph A. Paradiso","2838190, 2583070, 1701876, 4798651",In this paper we present a wearable device that maps natural feet movements into inputs for digital devices. KickSoul consists of an insole with sensors embedded that tracks movements and triggers actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze natural feet?s movements and their meaning before activating an action. This paper discusses different applications for this technology as well as the implementation of our prototype.,1,42.1052631579
UIST,4d601b4f5a29e7ec8889922e408da131cac303e1,UIST,2006,Sensing from the basement: a feasibility study of unobtrusive and low-cost home activity recognition,"James Fogarty, Carolyn Au, Scott E. Hudson","1738171, 3325465, 1749296","The home deployment of sensor-based systems offers many opportunities, particularly in the area of using sensor-based systems to support aging in place by monitoring an elder's activities of daily living. But existing approaches to home activity recognition are typically expensive, difficult to install, or intrude into the living space. This paper considers the feasibility of a new approach that ""reaches into the home"" via the existing infrastructure. Specifically, we deploy a small number of low-cost sensors at critical locations in a home's water distribution infrastructure. Based on water usage patterns, we can then infer activities in the home. To examine the feasibility of this approach, we deployed real sensors into a real home for six weeks. Among other findings, we show that a model built on microphone-based sensors that are placed away from systematic noise sources can identify 100% of clothes washer usage, 95% of dishwasher usage, 94% of showers, 88% of toilet flushes, 73% of bathroom sink activity lasting ten seconds or longer, and 81% of kitchen sink activity lasting ten seconds or longer. While there are clear limits to what activities can be detected when analyzing water usage, our new approach represents a sweet spot in the tradeoff between what information is collected at what cost.",68,88.75
UIST,d68e97ba3481e429367a1a1742e06607194000de,UIST,2005,DocWizards: a system for authoring follow-me documentation wizards,"Lawrence D. Bergman, Vittorio Castelli, Tessa A. Lau, Daniel Oblinger","3022115, 2879453, 1800706, 2104782","Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call <i>follow-me documentation wizards</i>. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.",54,58.064516129
UIST,086cd5d5f22bc41325ed03fa4127f6bee4d4d885,UIST,2009,"Abracadabra: wireless, high-precision, and unpowered finger input for very small mobile devices","Chris Harrison, Scott E. Hudson","1730920, 1749296","We present Abracadabra, a magnetically driven input technique that offers users wireless, unpowered, high fidelity finger input for mobile devices with very small screens. By extending the input area to many times the size of the device's screen, our approach is able to offer a high C-D gain, enabling fine motor control. Additionally, screen occlusion can be reduced by moving interaction off of the display and into unused space around the device. We discuss several example applications as a proof of concept. Finally, results from our user study indicate radial targets as small as 16 degrees can achieve greater than 92% selection accuracy, outperforming comparable radial, touch-based finger input.",125,100.0
UIST,a68f96df3f98d0657b5a78d3db26d2ce89179e47,UIST,2004,Using light emitting diode arrays as touch-sensitive input and output devices,Scott E. Hudson,1749296,"Light Emitting Diodes (LEDs) offer long life, low cost, efficiency, brightness, and a full range of colors. Because of these properties, they are widely used for simple displays in electronic devices. A previously characterized, but little known property of LEDs allows them to be used as photo sensors. In this paper, we show how this capability can be used to turn unmodified, off the shelf, LED arrays into touch sensitive input devices (while still remaining capable of producing output). The technique is simple and requires little or no extra hardware - in some cases operating with the same micro-controller based circuitry normally used to produce output, requiring only software changes. We will describe a simple hybrid input/output device prototype implemented with this technique, and discuss the design opportunities that this type of device opens up.",12,18.4210526316
UIST,5571ed95ba2469ed10350d0158cdafb830fe100e,UIST,2008,Iterative design and evaluation of an event architecture for pen-and-paper interfaces,"Ron B. Yeh, Andreas Paepcke, Scott R. Klemmer","2105696, 1750481, 1728167","This paper explores architectural support for interfaces combining pen, paper, and PC. We show how the event-based approach common to GUIs can apply to augmented paper, and describe additions to address paper's distinguishing characteristics. To understand the developer experience of this architecture, we deployed the toolkit to 17 student teams for six weeks. Analysis of the developers' code provided insight into the appropriateness of events for paper UIs. The usage patterns we distilled informed a second iteration of the toolkit, which introduces techniques for integrating interactive and batched input handling, coordinating interactions across devices, and debugging paper applications. The study also revealed that programmers created gesture handlers by composing simple ink measurements. This desire for informal interactions inspired us to include abstractions for recognition. This work has implications beyond paper - designers of graphical tools can examine API usage to inform iterative toolkit development.",31,65.7142857143
UIST,2ae0a7d83d016c7eb6a0de99e50779c38a011351,UIST,2002,The kinetic typography engine: an extensible system for animating expressive text,"Johnny C. Lee, Jodi Forlizzi, Scott E. Hudson","1803308, 1767344, 1749296","<i>Kinetic typography</i> --- text that uses movement or other temporal change --- has recently emerged as a new form of communication. As we hope to illustrate in this paper, kinetic typography can be seen as bringing some of the expressive power of film --- such as its ability to convey emotion, portray compelling characters, and visually direct attention --- to the strong communicative properties of text. Although kinetic typography offers substantial promise for expressive communications, it has not been widely exploited outside a few limited application areas (most notably in TV advertising). One of the reasons for this has been the lack of tools directly supporting it, and the accompanying difficulty in creating dynamic text. This paper presents a first step in remedying this situation --- an extensible and robust system for animating text in a wide variety of forms. By supporting an appropriate set of carefully factored abstractions, this <i>engine</i> provides a relatively small set of components that can be plugged together to create a wide range of different expressions. It provides new techniques for automating effects used in traditional cartoon animation, and provides specific support for typographic manipulations.",34,45.8333333333
UIST,1fba61cd67126f1d8e87be9847bb77703a087e0d,UIST,2009,Interactions in the air: adding further depth to interactive tabletops,"Otmar Hilliges, Shahram Izadi, Andrew D. Wilson, Steve Hodges, Armando Garcia-Mendoza, Andreas Butz","2531379, 1699068, 1767449, 1736330, 2387892, 8083190","Although interactive surfaces have many unique and compelling qualities, the interactions they support are by their very nature bound to the display surface. In this paper we present a technique for users to seamlessly switch between interacting on the tabletop surface to above it. Our aim is to leverage the space above the surface in combination with the regular tabletop display to allow more intuitive manipulation of digital content in three-dimensions. Our goal is to design a technique that closely resembles the ways we manipulate physical objects in the real-world; conceptually, allowing virtual objects to be 'picked up' off the tabletop surface in order to manipulate their three dimensional position or orientation. We chart the evolution of this technique, implemented on two rear projection-vision tabletops. Both use special projection screen materials to allow sensing at significant depths beyond the display. Existing and new computer vision techniques are used to sense hand gestures and postures above the tabletop, which can be used alongside more familiar multi-touch interactions. Interacting above the surface in this way opens up many interesting challenges. In particular it breaks the direct interaction metaphor that most tabletops afford. We present a novel shadow-based technique to help alleviate this issue. We discuss the strengths and limitations of our technique based on our own observations and initial user feedback, and provide various insights from comparing, and contrasting, our tabletop implementations",118,97.1428571429
UIST,39a53dd22b1f865cc4aca373937e1c28b8ed9c83,UIST,2013,Haptic props: semi-actuated tangible props for haptic interaction on the surface,"Dimitar Valkov, Andreas Mantler, Klaus H. Hinrichs","1962138, 2545415, 1685162","While multiple methods to extend the expressiveness of tangible interaction have been proposed, e. g., self-motion, stacking and transparency, providing haptic feedback to the tangible prop itself has rarely been considered. In this poster we present a semi-actuated, nano-powered, tangible prop, which is able to provide programmable friction for interaction with a tabletop setup. We have conducted a preliminary user study evaluating the users' acceptance for the device and their ability to detect changes in the programmed level of friction and received some promising results.",0,10.5504587156
UIST,672107f9b6a26305d3d724813862e4e9fb801ef6,UIST,2015,Anger-based BCI Using fNIRS Neurofeedback,"Gabor Aranyi, Fred Charles, Marc Cavazza","1786905, 1776150, 1696638","Functional near-infrared spectroscopy (fNIRS) holds increasing potential for Brain-Computer Interfaces (BCI) due to its portability, ease of application, robustness to movement artifacts, and relatively low cost. The use of fNIRS to support the development of affective BCI has received comparatively less attention, despite the role played by the prefrontal cortex in affective control, and the appropriateness of fNIRS to measure prefrontal activity. We present an active, fNIRS-based neurofeedback (NF) interface, which uses differential changes in oxygenation between the left and right sides of the dorsolateral prefrontal cortex to operationalize BCI input. The system is activated by users generating a state of anger, which has been previously linked to increased left prefrontal asymmetry. We have incorporated this NF interface into an experimental platform adapted from a virtual 3D narrative, in which users can express anger at a virtual character perceived as evil, causing the character to disappear progressively. Eleven subjects used the system and were able to successfully perform NF despite minimal training. Extensive analysis confirms that success was associated with the intent to express anger. This has positive implications for the design of affective BCI based on prefrontal asymmetry.",1,42.1052631579
UIST,3bb1956243a39d35594ffdda9ae440692390cfc3,UIST,2002,WebThumb: interaction techniques for small-screen browsers,"Jacob O. Wobbrock, Jodi Forlizzi, Scott E. Hudson, Brad A. Myers","1796045, 1767344, 1749296, 1707801","The proliferation of wireless handheld devices is placing the World Wide Web in the palms of users, but this convenience comes at a high interactive cost. The Web that came of age on the desktop is ill-suited for use on the small displays of handhelds. Today, handheld browsing often feels like browsing on a PC with a shrunken desktop. Overreliance on scrolling is a big problem in current handheld browsing. Users confined to viewing a small portion of each page often lack a sense of the overall context --- they may feel lost in a large page and be forced to remember the locations of items as those items scroll out of view. In this paper, we present a synthesis of interaction techniques to address these problems. We implemented these techniques in a prototype, WebThumb, that can browse the live Web.",51,62.5
UIST,161f19679c5f232cd4dfce2d4ca9603107ac22aa,UIST,2002,Specifying behavior and semantic meaning in an unmodified layered drawing package,"James Fogarty, Jodi Forlizzi, Scott E. Hudson","1738171, 1767344, 1749296","In order to create and use rich custom appearances, designers are often forced to introduce an unnatural gap into the design process. For example, a designer creating a skin for a music player must separately specify the appearance of the elements in the music player skin and the mapping between these visual elements and the functionality provided by the music player. This gap between appearance and semantic meaning creates a number of problems. We present a set of techniques that allows designers to use their preferred drawing tool to specify both appearance and semantic meaning. We demonstrate our techniques in an unmodified version of Adobe Photoshop&#174;, but our techniques are general and adaptable to nearly any layered drawing package.",4,12.5
UIST,04ed0b831c734181d2ec00c0db1d703775746bbf,UIST,2003,GADGET: a toolkit for optimization-based approaches to interface and display generation,"James Fogarty, Scott E. Hudson","1738171, 1749296","Recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays. Optimization-based approaches can often allow a mix of independent goals and constraints to be blended in ways that would be difficult to describe algorithmically. While optimization-based techniques appear to offer several potential advantages, further research in this area is hampered by the lack of appropriate tools. This paper presents GADGET, an experimental toolkit to support optimization for interface and display generation. GADGET provides convenient abstractions of many optimization concepts. GADGET also provides mechanisms to help programmers quickly create optimizations, including an efficient lazy evaluation framework, a powerful and configurable optimization structure, and a library of reusable components. Together these facilities provide an appropriate tool to enable exploration of a new class of interface and display generation techniques.",20,16.6666666667
UIST,5572d92d72e6d99337560dc49b2156d55e3fb78a,UIST,2010,TwinSpace: an infrastructure for cross-reality team spaces,"Derek F. Reilly, Hafez Rouzati, Chih-Sung Wu, Jee Yeon Hwang, Jeremy T. Brudvik, W. Keith Edwards","2528826, 2102401, 4126962, 1680130, 3038198, 3023379","We introduce TwinSpace, a flexible software infrastructure for combining interactive workspaces and collaborative virtual worlds. Its design is grounded in the need to support deep connectivity and flexible mappings between virtual and real spaces to effectively support collaboration. This is achieved through a robust connectivity layer linking heterogeneous collections of physical and virtual devices and services, and a centralized service to manage and control mappings between physical and virtual. In this paper we motivate and present the architecture of TwinSpace, discuss our experiences and lessons learned in building a generic framework for collaborative cross-reality, and illustrate the architecture using two implemented examples that highlight its flexibility and range, and its support for rapid prototyping.",11,63.3720930233
UIST,92980e73cd0be5429819206f98d8a9af3b9b061e,UIST,2014,bioPrint: an automatic deposition system for bacteria spore actuators,"Jifei Ou, Lining Yao, Clark Della Silva, Wen Wang, Hiroshi Ishii","3163859, 2912448, 3332772, 1721216, 1749649","We propose an automatic deposition method of bacteria spores, which deform thin soft materials under environmental humidity change. We describe the process of two-dimensional printing the spore solution as well as a design application. This research intends to contribute to the understanding of the control and pre-programming the transformation of future interfaces.",1,31.007751938
UIST,15f495dd7afb36df3d00826a2e14e1c45b750966,UIST,2011,TapSense: enhancing finger interaction on touch surfaces,"Chris Harrison, Julia Schwarz, Scott E. Hudson","1730920, 2251233, 1749296","We present TapSense, an enhancement to touch interaction that allows conventional surfaces to identify the type of object being used for input. This is achieved by segmenting and classifying sounds resulting from an object's impact. For example, the diverse anatomy of a human finger allows different parts to be recognized including the tip, pad, nail and knuckle - without having to instrument the user. This opens several new and powerful interaction opportunities for touch input, especially in mobile devices, where input is extremely constrained. Our system can also identify different sets of passive tools. We conclude with a comprehensive investigation of classification accuracy and training implications. Results show our proof-of-concept system can support sets with four input types at around 95% accuracy. Small, but useful input sets of two (e.g., pen and finger discrimination) can operate in excess of 99% accuracy.",65,92.380952381
UIST,5a580acf846726922165944f157e3a6ce16a82c9,UIST,2016,DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens,"Tomoki Shibata, Daniel Afergan, Danielle Kong, Beste F. Yuksel, I. Scott MacKenzie, Robert J. K. Jacob","1926522, 1793943, 3395856, 1943885, 1692873, 1723792","Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the ""fat-finger problem,"" wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.",0,44.6540880503
UIST,e36b944d336dfc9f3b0c8b2b65a6226c1b3771d6,UIST,2016,Holoportation: Virtual 3D Teleportation in Real-time,"Sergio Orts, Christoph Rhemann, Sean Ryan Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L. Davidson, Sameh Khamis, Mingsong Dou, Vladimir Tankovich, Charles T. Loop, Qin Cai, Philip A. Chou, Sarah Mennicken, Julien P. C. Valentin, Vivek Pradeep, Shenlong Wang, Sing Bing Kang, Pushmeet Kohli, Yuliya Lutchyn, Cem Keskin, Shahram Izadi","7540558, 2086328, 2219646, 2237334, 2371390, 2337994, 6308833, 2362379, 3144576, 3273714, 3440762, 1839367, 8123152, 1720816, 2889686, 2898574, 3273947, 1892247, 1738740, 1685185, 2751661, 7206941, 1699068","We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.",0,44.6540880503
UIST,c71cf40bb25a036363a4928f9b7bc185d2aaf7b8,UIST,2014,"Skin buttons: cheap, small, low-powered and clickable fixed-icon laser projectors","Gierad Laput, Robert Xiao, Xiang 'Anthony' Chen, Scott E. Hudson, Chris Harrison","1727999, 1681726, 2028468, 1749296, 1730920","Smartwatches are a promising new interactive platform, but their small size makes even basic actions cumbersome. Hence, there is a great need for approaches that expand the interactive envelope around smartwatches, allowing human input to escape the small physical confines of the device. We propose using tiny projectors integrated into the smartwatch to render icons on the user's skin. These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments, we show that these 'skin buttons' can have high touch accuracy and recognizability, while being low cost and power-efficient.",29,97.6744186047
UIST,10099bfe1f5326c751f86b763453f67409b39838,UIST,2012,Transparent display interaction without binocular parallax,"Joon Hyub Lee, Seok-Hyung Bae, Jinyung Jung, Hayan Choi","2690215, 1715434, 2864633, 2961374","Binocular parallax is a problem for any interaction system that has a transparent display and objects behind it. A proposed quantitative measure called Binocular Selectability Discriminant (BSD) allows UI designers to predict the ability of the user to perform selection task in their transparent display systems, in spite of binocular parallax. A proposed technique called Single-Distance Pseudo Transparency (SDPT) aims to eliminate binocular parallax for on-screen interactions that require precision. A mock-up study shows potentials and directions for future investigation.",4,45.5882352941
UIST,42e233f6b46612bf3b6b092eb9384ae05a8251f2,UIST,2012,Spatial augmented reality to enhance physical artistic creation,"Jérémy Laviole, Martin Hachet","3421303, 2281511","Spatial augmented reality (SAR) promises the integration of digital information in the real (physical) world through projection. In this doctoral symposium paper, I propose different tools to improve speed or ease the drawing by projecting photos, virtual construction lines and interactive 3D scenes. After describing the tools, I explain some future challenges to explore such as the creation of tools which helps to create drawings that are ""difficult"" to achieve for a human being, but easy to do by a computer. Furthermore, I propose some insights for the creation of digital games and programs which can take full advantages of physical drawings.",1,25.0
UIST,83f2910acbf3f022d7af803bc40f67a91d406a7e,UIST,2014,Spreadsheet driven web applications,"Edward Benson, Amy Xian Zhang, David R. Karger","8173216, 1788613, 1743286","Creating and publishing read-write-compute web applications requires programming skills beyond what most end users possess. But many end users know how to make spreadsheets that act as simple information management applications, some even with computation. We present a system for creating basic web applications using such spreadsheets in place of a server and using HTML to describe the client UI. Authors connect the two by placing spreadsheet references inside HTML attributes. Data computation is provided by spreadsheet formulas. The result is a reactive read-write-compute web page without a single line of Javascript code. Nearly all of the fifteen HTML novices we studied were able to connect HTML to spreadsheets using our method with minimal instruction. We draw conclusions from their experience and discuss future extensions to this programming model.",9,78.2945736434
UIST,4488b7b5befca3c347491ac42fca87ab020c0932,UIST,2013,PAPILLON: designing curved display surfaces with printed optics,"Eric Brockmeyer, Ivan Poupyrev, Scott E. Hudson","1715064, 1736819, 1749296","We present a technology for designing curved display surfaces that can both display information and sense two dimensions of human touch. It is based on 3D printed optics, where the surface of the display is constructed as a bundle of printed light pipes, that direct images from an arbitrary planar image source to the surface of the display. This effectively decouples the display surface and image source, allowing to iterate the design of displays without requiring changes to the complex electronics and optics of the device. In addition, the same optical elements also direct light from the surface of the display back to the image sensor allowing for touch input and proximity detection of a hand relative to the display surface. The resulting technology is effective in designing compact, efficient displays of a small size; this has been applied in the design of interactive animated eyes.",16,79.8165137615
UIST,618b079fb7dd4e6d6d7288efc3e7ad6621103b9f,UIST,2012,A guidance technique for motion tracking with a handheld camera using auditory feedback,"Keiichi Seko, Kentaro Fukuchi","1960535, 2200842","We introduce a novel guidance technique based on auditory feedback for a handheld video camera. Tracking a moving object with a handheld camera is a difficult task, especially when the camera operator follows the target, because it is difficult to see through the viewfinder at the same time as following the target. The proposed technique provides auditory feedback via a headphone, which assists the operator to keep the target in sight. Two feedback sounds are introduced: three-dimensional (3D) audio and amplitude modulation (AM)-based sonification.",2,33.8235294118
UIST,e18da7e3fbc01b576cf90022f659bb7b395859ec,UIST,2014,Using brain-computer interfaces for implicit input,Daniel Afergan,1793943,"Passive brain-computer interfaces, in which implicit input is derived from a user's changing brain activity without conscious effort from the user, may be one of the most promising applications of brain-computer interfaces because they can improve user performance without additional effort on the user's part. I seek to use physiological signals that correlate to particular brain states in order to adapt an interface while the user behaves normally. My research aims to develop strategies to adapt the interface to the user and the user's cognitive state using functional near-infrared spectroscopy (fNIRS), a non-invasive, lightweight brain-sensing technique. While passive brain-computer interfaces are currently being developed and researchers have shown their utility, there has been little effort to develop a framework or hierarchy for adaptation strategies.",1,31.007751938
UIST,0b7c3c705a9f1fa1eea80526c70f9a99c3d8d4f1,UIST,2013,"Lumitrack: low cost, high precision, high speed tracking with projected m-sequences","Robert Xiao, Chris Harrison, Karl D. D. Willis, Ivan Poupyrev, Scott E. Hudson","1681726, 1730920, 2269914, 1736819, 1749296","We present Lumitrack, a novel motion tracking technology that uses projected structured patterns and linear optical sensors. Each sensor unit is capable of recovering 2D location within the projection area, while multiple sensors can be combined for up to six degree of freedom (DOF) tracking. Our structured light approach is based on special patterns, called m-sequences, in which any consecutive sub-sequence of m bits is unique. Lumitrack can utilize both digital and static projectors, as well as scalable embedded sensing configurations. The resulting system enables high-speed, high precision, and low-cost motion tracking for a wide range of interactive applications. We detail the hardware, operation, and performance characteristics of our approach, as well as a series of example applications that highlight its immediate feasibility and utility.",7,61.4678899083
UIST,3d1583d89508900803e4ee2028d080313133eeac,UIST,2014,Brain-based target expansion,"Daniel Afergan, Tomoki Shibata, Samuel W. Hincks, Evan M. Peck, Beste F. Yuksel, Remco Chang, Robert J. K. Jacob","1793943, 1926522, 2006161, 2295942, 1943885, 1809775, 1723792","The bubble cursor is a promising cursor expansion technique, improving a user's movement time and accuracy in pointing tasks. We introduce a brain-based target expansion system, which improves the efficacy of bubble cursor by increasing the expansion of high importance targets at the optimal time based on brain measurements correlated to a particular type of multitasking. We demonstrate through controlled experiments that brain-based target expansion can deliver a graded and continuous level of assistance to a user according to their cognitive state, thereby improving task and speed-accuracy metrics, even without explicit visual changes to the system. Such an adaptation is ideal for use in complex systems to steer users toward higher priority goals during times of increased demand.",8,75.9689922481
UIST,78b7815fd895d6bb698e0920da3182be2d98b045,UIST,2012,"Acoustic barcodes: passive, durable and inexpensive notched identification tags","Chris Harrison, Robert Xiao, Scott E. Hudson","1730920, 1681726, 1749296","We present acoustic barcodes, structured patterns of physical notches that, when swiped with e.g., a fingernail, produce a complex sound that can be resolved to a binary ID. A single, inexpensive contact microphone attached to a surface or object is used to capture the waveform. We present our method for decoding sounds into IDs, which handles variations in swipe velocity and other factors. Acoustic barcodes could be used for information retrieval or to triggering interactive functions. They are passive, durable and inexpensive to produce. Further, they can be applied to a wide range of materials and objects, including plastic, wood, glass and stone. We conclude with several example applications that highlight the utility of our approach, and a user study that explores its feasibility.",16,68.6274509804
UIST,de6939447f518edf5534b1db266c514ca6845343,UIST,2012,Printed optics: 3D printing of embedded optical elements for interactive devices,"Karl D. D. Willis, Eric Brockmeyer, Scott E. Hudson, Ivan Poupyrev","2269914, 1715064, 1749296, 1736819","We present an approach to 3D printing custom optical elements for interactive devices labelled <i>Printed Optics</i>. <i>Printed Optics</i> enable sensing, display, and illumination elements to be directly embedded in the casing or mechanical structure of an interactive device. Using these elements, unique display surfaces, novel illumination techniques, custom optical sensors, and embedded optoelectronic components can be digitally fabricated for rapid, high fidelity, highly customized interactive devices. <i>Printed Optics</i> is part of our long term vision for interactive devices that are 3D printed in their entirety. In this paper we explore the possibilities for this vision afforded by fabrication of custom optical elements using today's 3D printing technology.",64,98.0392156863
UIST,7b83536c5ad86051ae3aa892e30b9044a65170c4,UIST,2011,SideBySide: ad-hoc multi-user interaction with handheld projectors,"Karl D. D. Willis, Ivan Poupyrev, Scott E. Hudson, Moshe Mahler","2269914, 1736819, 1749296, 3156160","We introduce SideBySide, a system designed for ad-hoc multi-user interaction with handheld projectors. SideBySide uses device-mounted cameras and hybrid visible/infrared light projectors to track multiple independent projected images in relation to one another. This is accomplished by projecting invisible fiducial markers in the near-infrared spectrum. Our system is completely self-contained and can be deployed as a handheld device without instrumentation of the environment. We present the design and implementation of our system including a hybrid handheld projector to project visible and infrared light, and techniques for tracking projected fiducial markers that move and overlap. We introduce a range of example applications that demonstrate the applicability of our system to real-world scenarios such as mobile content exchange, gaming, and education.",41,85.7142857143
UIST,4c3a9b88367119a1dcdb751e93b6779f5396f1f9,UIST,2011,A new angle on cheap LCDs: making positive use of optical distortion,"Chris Harrison, Scott E. Hudson","1730920, 1749296","Most LCD screens exhibit color distortions when viewed at oblique angles. Engineers have invested significant time and resources to alleviate this effect. However, the massive manufacturing base, as well as millions of in-the-wild monitors, means this effect will be common for many years to come. We take an opposite stance, embracing these optical peculiarities, and consider how they can be used in productive ways. This paper discusses how a special palette of colors can yield visual elements that are invisible when viewed straight-on, but visible at oblique angles. In essence, this allows conventional, unmodified LCD screens to output two images simultaneously - a feature normally only available in far more complex setups. We enumerate several applications that could take advantage of this ability.",6,38.5714285714
UIST,d2f452c42dbfab784c763d8c4e14133f9d441d58,UIST,2014,Matter matters: offloading machine computation to material computation for shape changing interfaces,Lining Yao,2912448,"This paper introduces material computation to offload computing from machine to material, in the process of creating shape-changing output. It contains the explanation on the mechanism of transformation, the concept of material computation, the summary and analysis of literature research within and beyond the HCI field, the interaction loop integrating material computation, and my own practice in material computation technics and applications.",1,31.007751938
UIST,288ba6476d24ce368b62d3c6601595a7db0afb9a,UIST,2014,Building implicit interfaces for wearable computers with physiological inputs: zero shutter camera and phylter,"Tomoki Shibata, Evan M. Peck, Daniel Afergan, Samuel W. Hincks, Beste F. Yuksel, Robert J. K. Jacob","1926522, 2295942, 1793943, 2006161, 1943885, 1723792","We propose implicit interfaces that use passive physiological input as additional communication channels between wearable devices and wearers. A defining characteristic of physiological input is that it is implicit and continuous, distinguishing it from conventional event-driven action on a keyboard, for example, which is explicit and discrete. By considering the fundamental differences between the two types of inputs, we introduce a core framework to support building implicit interface, such that the framework follows the three key principles: Subscription, Accumulation, and Interpretation of implicit inputs. Unlike a conventional event driven system, our framework subscribes to continuous streams of input data, accumulates the data in a buffer, and subsequently attempts to recognize patterns in the accumulated data -- upon request from the application, rather than directly in response to the input events. Finally, in order to embody the impacts of implicit interfaces in the real world, we introduce two prototype applications for Google Glass, Zero Shutter Camera triggering a camera snapshot and Phylter filtering notifications the both leverage the wearer's physiological state information.",1,31.007751938
UIST,09895f8b8e38dba2d62d742541778c97c91d97ce,UIST,2006,Enabling web browsers to augment web sites' filtering and sorting functionalities,"David Huynh, Rob Miller, David R. Karger","1762663, 1723785, 1743286","Existing augmentations of web pages are mostly small cosmetic changes (e.g., removing ads) and minor addition of third-party content (e.g., product prices from competing sites). None leverages the structured data presented in web pages. This paper describes Sifter, a web browser extension that can augment a well-structured web site with advanced filtering and sorting functionality. These added features work inside the site's own pages, preserving the site's presentational style and the user's context. Sifter contains an algorithm that scrapes structured data out of well-structured web pages while usually requiring no user intervention. We tested Sifter on real web sites and real users and found that people could use Sifter to perform sophisticated queries and high-level analyses on sizable data collections on the Web. We propose that web sites can be similarly augmented with other sophisticated data-centric functionality, giving users new benefits over the existing Web.",35,51.25
UIST,288ac160ffe60537b0db8b20a94cbb53129c24d4,UIST,2004,Navigating documents with the virtual scroll ring,"Tomer Moscovich, John F. Hughes","2966785, 2057964","We present a technique for scrolling through documents that is simple to implement and requires no special hardware. This is accomplished by simulating a hardware scroll ring--a device that maps circular finger motion into vertical scrolling. The technique performs at least as well as a mouse wheel for medium and long distances, and is preferred by users. It can be particularly useful in portable devices where screen-space and space for peripherals is at a premium.",36,47.3684210526
UIST,61f2f6c8a6eb918546a65eaabb4b3a6eec84b262,UIST,2014,Hairlytop interface: a basic tool for active interfacing,"Shuhei Umezu, Masaru Ohkubo, Yoshiharu Ooide, Takuya Nojima","2841504, 3133704, 2810245, 1797797","The Hairlytop Interface is a high scalability interface composed of hair-like units called smart hairs. The original version of the smart hair comprised a shape-memory alloy, drive circuits, and a light sensor. Simply placing the smart hair above a light display device enabled each smart hair to be bent and controlled by modulating the intensity of light from the display. Various prototypes of the Hairlytop Interface have been created to show its high flexibility in configuration. This flexibility should help users to develop their own moving interfaces.",1,31.007751938
UIST,8511b03dd36425e31478be24e486c95ec4ce2a22,UIST,2014,A rapid prototyping toolkit for touch sensitive objects using active acoustic sensing,"Makoto Ono, Buntarou Shizuki, Jiro Tanaka","2066471, 1765222, 1726327","We present a prototyping toolkit for creating touch sensitive prototypes from everyday objects without needing special skills such as code writing or designing circuits. This toolkit consists of an acoustic based touch sensor module that captures the resonant properties of objects, software modules including one that recognizes how an object is touched by using machine learning, and plugins for visual programming environments such as Scratch and Max/MSP. As a result, our toolkit enables users to easily configure the response of touches using a wide variety of visual or audio responses. We believe that our toolkit expands the creativity of a non-specialist, such as children and media artists.",0,12.015503876
UIST,1227675da06e07621ac285033d249aa404f30ec8,UIST,1991,A flexible Chinese character input scheme,Siu Chi Hsu,2209532,"A very flexible and easy-to-use scheme which possesses unique advantages over existing systems is presented in this article. The scheme is based on the partitioning of a character into parts, A character is inputted by specifying the sequence of character parts descriptions, which is then matched against the standard sequences of the characters in the character set. A character part is either described with a unique key or its stroke count. The matching algorithm allows the characters to be partitioned flexibly and in-putted in many different ways. An automatic binding mechanism offers very high adaptability to the input style of the user. The user need not remember all the key bindings before he can input Chinese and the scheme is also capable of tolerating many variations in character style and/or errors.",6,28.2608695652
UIST,cfa3f5cbf5cdeb7a37eace0f83c59442b446b3f5,UIST,2015,CyclopsRing: Enabling Whole-Hand and Context-Aware Interactions Through a Fisheye Ring,"Li-Wei Chan, Yi-Ling Chen, Chi-Hao Hsieh, Rong-Hao Liang, Bing-Yu Chen","1682665, 5014321, 2829487, 1705512, 1733344","This paper presents CyclopsRing, a ring-style fisheye imaging wearable device that can be worn on hand webbings to en- able whole-hand and context-aware interactions. Observing from a central position of the hand through a fisheye perspective, CyclopsRing sees not only the operating hand, but also the environmental contexts that involve with the hand-based interactions. Since CyclopsRing is a finger-worn device, it also allows users to fully preserve skin feedback of the hands. This paper demonstrates a proof-of-concept device, reports the performance in hand-gesture recognition using random decision forest (RDF) method, and, upon the gesture recognizer, presents a set of interaction techniques including on-finger pinch-and-slide input, in-air pinch-and-motion input, palm-writing input, and their interactions with the environ- mental contexts. The experiment obtained an 84.75% recognition rate of hand gesture input from a database of seven hand gestures collected from 15 participants. To our knowledge, CyclopsRing is the first ring-wearable device that supports whole-hand and context-aware interactions.",10,96.9298245614
UIST,16cff5d3d524df1f3331b2ff7456b017db4b289e,UIST,2016,Fluxa: Body Movements as a Social Display,"Xin Liu, Katia Vega, Jing Qian, Joseph A. Paradiso, Pattie Maes","1749705, 2343339, 3323282, 4798651, 1701876","This paper presents Fluxa, a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a wearable display to foster social interactions. It can be used to enhance existing social gestures such as hand-waving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a self-expression device that generates images while dancing. We discuss the advantages of Fluxa: a display size that could be much larger than the device itself, a semi-transparent display that allows users and others to see though it and promotes social interaction.",0,44.6540880503
UIST,251761ad69b5afb58cc76f262863ad1cae811cb8,UIST,2015,GaussStarter: Prototyping Analog Hall-Sensor Grids with Breadboards,"Rong-Hao Liang, Han-Chih Kuo, Bing-Yu Chen","1705512, 1736008, 1733344","This work presents GaussStarter, a pluggable and tileable analog Hall-sensor grid module for easy and scalable bread- board prototyping. In terms of ease-of-use, the graspable units allow users to easily plug them on or remove them from a breadboard. In terms of scalability, tiling the units on the breadboard can easily expand the sensing area. A software development kit is also provided for designing applications based on this hardware module.",1,42.1052631579
UIST,3debe724f1352c0d7f21f519433cdf1a9da2aecf,UIST,2014,Generating emotionally relevant musical scores for audio stories,"Steve Rubin, Maneesh Agrawala","3280503, 1820412","Highly-produced audio stories often include musical scores that reflect the emotions of the speech. Yet, creating effective musical scores requires deep expertise in sound production and is time-consuming even for experts. We present a system and algorithm for re-sequencing music tracks to generate emotionally relevant music scores for audio stories. The user provides a speech track and music tracks and our system gathers emotion labels on the speech through hand-labeling, crowdsourcing, and automatic methods. We develop a constraint-based dynamic programming algorithm that uses these emotion labels to generate emotionally relevant musical scores. We demonstrate the effectiveness of our algorithm by generating 20 musical scores for audio stories and showing that crowd workers rank their overall quality significantly higher than stories without music.",6,67.8294573643
UIST,61de77126cb7e2bfbf11bcf8f835bd7cca9a391e,UIST,2008,Going beyond the display: a surface technology with an electronically switchable diffuser,"Shahram Izadi, Steve Hodges, Stuart Taylor, Dan Rosenfeld, Nicolas Villar, Alex Butler, Jonathan Westhues","1699068, 1736330, 1683873, 3064582, 1707115, 1684414, 2711385","We introduce a new type of interactive surface technology based on a switchable projection screen which can be made diffuse or clear under electronic control. The screen can be continuously switched between these two states so quickly that the change is imperceptible to the human eye. It is then possible to rear-project what is perceived as a stable image onto the display surface, when the screen is in fact transparent for half the time. The clear periods may be used to project a second, different image through the display onto objects held above the surface. At the same time, a camera mounted behind the screen can see out into the environment. We explore some of the possibilities this type of screen technology affords, allowing surface computing interactions to extend 'beyond the display'. We present a single self-contained system that combines these off-screen interactions with more typical multi-touch and tangible surface interactions. We describe the technical challenges in realizing our system, with the aim of allowing others to experiment with these new forms of interactive surfaces.",79,88.5714285714
UIST,12c855549fcfbeadbd0f0406d81e5a34af48f0e0,UIST,2016,AmbioTherm: Simulating Ambient Temperatures and Wind Conditions in VR Environments,"Nimesha Ranasinghe, Pravar Jain, David Tolley, Shienny Karwita, Yilei Shi, Ellen Yi-Luen Do","1722792, 3492844, 3492593, 3493056, 1809242, 1689168","As Virtual Reality (VR) experiences become increasingly popular, simulating sensory perceptions of environmental conditions is essential for providing an immersive user experience. In this paper, we present Ambiotherm, a wearable accessory for existing Head Mounted Displays (HMD), which simulates real-world environmental conditions such as ambient temperatures and wind conditions. The system consists of a wearable accessory for the HMD and a mobile application, which generates interactive VR environments and controls the thermal and wind stimuli. The thermal stimulation module is attached to the user's neck while two fans are focused on the user's face to simulate wind conditions. We demonstrate the Ambiotherm system with two VR environments, a desert and a snowy mountain, to showcase the different types of ambient temperatures and wind conditions that can be simulated. Results from initial user experiments show that the participants perceive VR environments to be more immersive when external thermal and wind stimuli are presented as part of the VR experience.",0,44.6540880503
UIST,6d890d2f34a762c6bd6501a5be072f740c3143c2,UIST,2005,Circle & identify: interactivity-augmented object recognition for handheld devices,"Byungkon Sohn, Geehyuk Lee","1903919, 1717371","The first requirement of a ""spatial mouse"" is the ability to identify the object that it is aiming at. Among many possible technologies that can be employed for this purpose, possibly the best solution would be object recognition by machine vision. The problem, however, is that object recognition algorithms are not yet reliable enough or light enough for hand-held devices. This paper demonstrates that a simple object recognition algorithm can become a practical solution when augmented by interactivity. The user draw a circle around a target using a spatial mouse, and the mouse captures a series of camera frames. The frames can be easily stitched together to give a target image separated from the background, with which we need only additional steps of feature extraction and object classification. We present here results from two experiments with a few household objects.",0,4.83870967742
UIST,06315ad1188c2607a84c359abe6c534c60516f8b,UIST,2014,Deconstructing and restyling D3 visualizations,"Jonathan Harper, Maneesh Agrawala","2885401, 1820412","The D3 JavaScript library has become a ubiquitous tool for developing visualizations on the Web. Yet, once a D3 visualization is published online its visual style is difficult to change. We present a pair of tools for deconstructing and restyling existing D3 visualizations. Our deconstruction tool analyzes a D3 visualization to extract the data, the marks and the mappings between them. Our restyling tool lets users modify the visual attributes of the marks as well as the mappings from the data to these attributes. Together our tools allow users to easily modify D3 visualizations without examining the underlying code and we show how they can be used to deconstruct and restyle a variety of D3 visualizations.",7,72.0930232558
UIST,31c52e8c47ccff5d56f3196c7fe40e70d6b8da3f,UIST,2014,G-raffe: an elevating tangible block supporting 2.5D interaction in a tabletop computing environment,"Jungu Sim, Chang-Min Kim, Seung-Woo Nam, Tek-Jin Nam","2879819, 3067904, 7532343, 1696044","We present an elevating tangible block, G-raffe, supporting 2.5-dimensional (2.5-D) interaction in a tabletop computing environment. There is a lack of specialized interface devices for tabletop computing environments. G-raffe overcomes the limitation of conventional 2-D interactions inherited from the vertical desktop computing setting. We adopted a rollable metal tape structure to create up and down movements in a small volume of the block. This also becomes a connecting device for a mobile display to be used with the tabletop computer. We report on our design rationale as well as the results of a preliminary user study.",0,12.015503876
UIST,2a4d0fc369cb8ceb4b0139e15dfe9e8e8f8ad2c6,UIST,2013,Wheels in motion: inertia sensing in roller derby,"Craig D. Stewart, Penny Traitor, Vicki L. Hanson","1709841, 2905843, 1730685","The recent resurgence of Roller Derby has seen the game progress to an elite level with leagues becoming increasingly competitive and taking a more structured and athletic approach to training. Leagues that the authors are involved in have expressed a desire for an objective measure of basic skills and a way to monitor improvements in performance especially amongst junior skaters. This paper details the construction of an inertia-sensing platform designed to be safe to wear by skaters. We have identified a skating manoeuvre, the ""crossover"" that can be automatically detected using a simple filtering and thresholding procedure. We also report on some initial results in automatically detecting when a crossover occurs and provide details of our future work.",1,27.0642201835
UIST,3db14551d878d66d1b17c77ee717e2a71ebd387c,UIST,2014,"Video digests: a browsable, skimmable format for informational lecture videos","Amy Pavel, Colorado Reed, Björn Hartmann, Maneesh Agrawala","3099614, 1718827, 4020023, 1820412","Increasingly, authors are publishing long informational talks, lectures, and distance-learning videos online. However, it is difficult to browse and skim the content of such videos using current timeline-based video players. Video digests are a new format for informational videos that afford browsing and skimming by segmenting videos into a chapter/section structure and providing short text summaries and thumbnails for each section. Viewers can navigate by reading the summaries and clicking on sections to access the corresponding point in the video. We present a set of tools to help authors create such digests using transcript-based interactions. With our tools, authors can manually create a video digest from scratch, or they can automatically generate a digest by applying a combination of algorithmic and crowdsourcing techniques and then manually refine it as needed. Feedback from first-time users suggests that our transcript-based authoring tools and automated techniques greatly facilitate video digest creation. In an evaluative crowdsourced study we find that given a short viewing time, video digests support browsing and skimming better than timeline-based or transcript-based video players.",10,80.2325581395
UIST,01f8b06e021c7416afa6e93d078cb01b4241e0ae,UIST,2012,Proton++: a customizable declarative multitouch framework,"Kenrick Kin, Björn Hartmann, Tony DeRose, Maneesh Agrawala","7826821, 4020023, 1792251, 1820412","Proton++ is a declarative multitouch framework that allows developers to describe multitouch gestures as regular expressions of touch event symbols. It builds on the Proton framework by allowing developers to incorporate custom touch attributes directly into the gesture description. These custom attributes increase the expressivity of the gestures, while preserving the benefits of Proton: automatic gesture matching, static analysis of conflict detection, and graphical gesture creation. We demonstrate Proton++'s flexibility with several examples: a direction attribute for describing trajectory, a pinch attribute for detecting when touches move towards one another, a touch area attribute for simulating pressure, an orientation attribute for selecting menu items, and a screen location attribute for simulating hand ID. We also use screen location to simulate user ID and enable simultaneous recognition of gestures by multiple users. In addition, we show how to incorporate timing into Proton++ gestures by reporting touch events at a regular time interval. Finally, we present a user study that suggests that users are roughly four times faster at interpreting gestures written using Proton++ than those written in procedural event-handling code commonly used today.",34,88.2352941176
UIST,25dbf3362196417c63016ca421e33964e726fed6,UIST,2012,Enjoying virtual handcrafting with ToolDevice,"Ryan Arisandi, Yusuke Takami, Mai Otsuki, Asako Kimura, Fumihisa Shibata, Hideyuki Tamura","2233023, 2321608, 1683502, 1726979, 1723268, 1730627","ToolDevice is a set of devices developed to help users in spatial work such as layout design and three-dimensional (3D) modeling. It consists of three components: TweezersDevice, Knife/HammerDevice, and BrushDevice, which use hand tool metaphors to help users recognize each device's unique functions. We have developed a mixed reality (MR) 3D modeling system that imitates real-life woodworking using the TweezersDevice and the Knife/HammerDevice. In the system, users can pick up and move virtual objects with the TweezersDevice. Users can also cut and join virtual objects using the Knife/HammerDevice. By repeating these operations, users can build virtual wood models.",5,49.0196078431
UIST,3142ed568bfc357dc53c9cba9059029b130faedd,UIST,2013,Touch & activate: adding interactivity to existing objects using active acoustic sensing,"Makoto Ono, Buntarou Shizuki, Jiro Tanaka","2066471, 1765222, 1726327","In this paper, we present a novel acoustic touch sensing technique called Touch &#38; Activate. It recognizes a rich context of touches including grasp on existing objects by attaching only a vibration speaker and a piezo-electric microphone paired as a sensor. It provides easy hardware configuration for prototyping interactive objects that have touch input capability. We conducted a controlled experiment to measure the accuracy and trade-off between the accuracy and number of training rounds for our technique. From its results, per-user recognition accuracies with five touch gestures for a plastic toy as a simple example and six hand postures for the posture recognition as a complex example were 99.6% and 86.3%, respectively. Walk up user recognition accuracies for the two applications were 97.8% and 71.2%, respectively. Since the results of our experiment showed a promising accuracy for the recognition of touch gestures and hand postures, Touch &#38; Activate should be feasible for prototype interactive objects that have touch input capability.",30,92.2018348624
UIST,b52c8932bf492a825c042965ed150ce384f7ec9a,UIST,2014,Contelli: a user-controllable intelligent keyboard for watch-sized small touchscreens,"Taik Heon Rhee, Kwangmin Byeon, Hochul Shin","2883134, 2217431, 2352506","Intelligent keyboards aid fast text entry by correcting user's erroneous input, but there is a big problem that a user always has to watch and judge of their suggestion results. Contelli, a user-controllable intelligent keyboard, monitors the duration of each key-tapping, and analyzes the possibility of mis-typing only for short-tapped letters. A long-tapped letter is regarded as a precise input and excluded in the process of candidate generation from a lexicon. Using Contelli, a user may actively ""control"" the intelligent keyboards. S/he may type ordinary words quickly on watch-sized small touchscreens. Also, s/he may input a word as typed without switching off the automatic replacement or performing additional actions for the replaced result. In addition, long-tapping a part of a string reduces the number of replacement candidates, which contributes the more precise word replacement for highly erroneous input typed on small touchscreens.",0,12.015503876
UIST,0d78444ac020eb9439d51c9d12ee6f4ed57dbbdc,UIST,2011,Vermeer: direct interaction with a 360° viewable 3D display,"Alex Butler, Otmar Hilliges, Shahram Izadi, Steve Hodges, David Molyneaux, David Kim, Danny Kong","1684414, 2531379, 1699068, 1736330, 2032716, 6308833, 2310203","We present Vermeer, a novel interactive 360&#176; viewable 3D display. Like prior systems in this area, Vermeer provides viewpoint-corrected, stereoscopic 3D graphics to simultaneous users, 360&#176; around the display, without the need for eyewear or other user instrumentation. Our goal is to over-come an issue inherent in these prior systems which - typically due to moving parts - restrict interactions to outside the display volume. Our system leverages a known optical illusion to demonstrate, for the first time, how users can reach into and directly touch 3D objects inside the display volume. Vermeer is intended to be a new enabling technology for interaction, and we therefore describe our hardware implementation in full, focusing on the challenges of combining this optical configuration with an existing approach for creating a 360&#176; viewable 3D display. Initially we demonstrate direct involume interaction by sensing user input with a Kinect camera placed above the display. However, by exploiting the properties of the optical configuration, we also demonstrate novel prototypes for fully integrated input sensing alongside simultaneous display. We conclude by discussing limitations, implications for interaction, and ideas for future work.",16,62.380952381
UIST,6f1e2e2aa316c6b88168e55ea205dfeab1ede5b7,UIST,2012,Mashpoint: Browsing the Web along Structured Lines,"Igor O. Popov, Monica M. C. Schraefel, Wendy Hall, Nigel Shadbolt","2597019, 2284695, 1685385, 1705314","Large numbers of Web sites support rich data-centric features to explore and interact with data. In this paper we present mashpoint, a framework that allows distributed data-powered Web applications to linked based on similarities of the entities in their data. By linking applications in this way we allow browsing with selections of data from one application to another application. This sort of browsing allows complex queries and exploration of data to be done by average Web users using multiple applications. We additionally use this concept to surface structured information to users in Web pages. In this paper we present this concept and our initial prototype.",0,9.80392156863
UIST,61ac0e465cac59f0cc7ef3f80769db974d9fae86,UIST,2014,PoliTel: mobile remote presence system that autonomously adjusts the interpersonal distance,"Masanori Yokoyama, Masafumi Matsuda, Shinyo Muto, Naoyoshi Kanamaru","2981559, 2702771, 1911944, 2799219","Mobile Remote Presence (MRP) system that uses a smart device such as smartphone and tablet pc as video conferencing equipment is getting popular. There are varieties of smart devices, and the appearance of a smart device varies from one to another. We assumed that the appropriate interpersonal distance for an MRP system varies depending on the appearance of the smart device. To confirm our assumption, we conducted a preliminary experiment. The result of the experiment suggested that the value of the proper interpersonal distance increases as the video size increases. It is known that the task load of the remote operator of the MRP system increases if the operator is forced to manually control the MRP system to keep the interpersonal distance to the appropriate level, which adversely affects the quality of the communication through MRP. To resolve the problem, we propose PoliTel, a novel MRP system which autonomously adjusts the interpersonal distance according to the appearance of the smart device by controlling the position or video size of MRP, and allows the operator to concentrate more on the conversation with the person facing to the MRP system.",0,12.015503876
UIST,2949e518d540b499b012188c12c6a63b2a760da9,UIST,2013,Flexkit: a rapid prototyping platform for flexible displays,"David Holman, Jesse Burstyn, Ryan Brotman, Audrey Younkin, Roel Vertegaal","1765555, 2058646, 2109416, 2228510, 1687608","Commercially available development platforms for flexible displays are not designed for rapid prototyping. To create a deformable interface, one that uses a functional flexible display, designers must be familiar with embedded hardware systems and corresponding programming. We introduce Flexkit, a platform that allows designers to rapidly prototype deformable applications. With Flexkit, designers can rapidly prototype using a thin-film electrophoretic display, one that is ""Plug and Play"". To demonstrate Flexkit's ease-of-use, we present its application in PaperTab's design iteration as a case study. We further discuss how dithering can be used to increase the frame rate of electrophoretic displays from 1fps to 5fps.",2,37.6146788991
UIST,06f02199690961ba52997cde1527e714d2b3bf8f,UIST,2013,Gaze locking: passive eye contact detection for human-object interaction,"Brian A. Smith, Qi Yin, Steven K. Feiner, Shree K. Nayar","3008842, 7477615, 1809403, 1750470","Eye contact plays a crucial role in our everyday social interactions. The ability of a device to reliably detect when a person is looking at it can lead to powerful human-object interfaces. Today, most gaze-based interactive systems rely on gaze tracking technology. Unfortunately, current gaze tracking techniques require active infrared illumination, calibration, or are sensitive to distance and pose. In this work, we propose a different solution-a passive, appearance-based approach for sensing eye contact in an image. By focusing on gaze *locking* rather than gaze tracking, we exploit the special appearance of direct eye gaze, achieving a Matthews correlation coefficient (MCC) of over 0.83 at long distances (up to 18 m) and large pose variations (up to &#177;30&#176; of head yaw rotation) using a very basic classifier and without calibration. To train our detector, we also created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We demonstrate how our method facilitates human-object interaction, user analytics, image filtering, and gaze-triggered photography.",22,84.4036697248
UIST,aebb8ec710cb4b8ef7496fdb197fdabe6a053d2b,UIST,2014,Making distance matter: leveraging scale and diversity in massive online classes,Chinmay Kulkarni,4924730,"The large scale of online classes and the diversity of the students that participate in them can enable new educational systems. This massive scale and diversity can enable always-available systems that help students share diverse ideas, and inspire and learn from each other. We introduce systems for two core educational processes at scale: discussion and assessment. To date, several thousand students in a dozen online classes have used our discussion system. Controlled experiments suggest that participants in more diverse discussions perform better on tests and that discussion improves engagement. Similarly, more than 100,000 students have reviewed peer work for both summative assessment and feedback. Through these systems, we argue that to create new educational experiences at scale, pedagogical strategies and software that leverage scale and diversity must be co-developed. More broadly, we suggest the key to creating new educational experiences online lies in leveraging massive networks of peers.",0,12.015503876
UIST,7db137d45dd43f8f59819cc2cd1ee9b9324ad56f,UIST,2010,EasySnap: real-time audio feedback for blind photography,"Samuel White, Hanjie Ji, Jeffrey P. Bigham","4707811, 1737220, 1744846","This demonstration presents EasySnap, an application that enables blind and low-vision users to take high-quality photos by providing real-time audio feedback as they point their existing camera phones. Users can readily follow the audio instructions to adjust their framing, zoom level and subject lighting appropriately. Real-time feedback is achieved on current hardware using computer vision in conjunction with use patterns drawn from current blind photographers.",12,66.8604651163
UIST,d22fa50af44205d2cb18e4eb0958f7afc0bc5226,UIST,2012,E-Block: a tangible programming tool for children,"Danli Wang, Yang Zhang, Tianyuan Gu, Liang He, Hongan Wang","1686845, 4449904, 1817651, 1765426, 7643981",E-Block is a tangible programming tool for children aged 5 to 9 which gives children a preliminary understanding of programming. Children can write programs to play a maze game by placing the programming blocks in E-Block. The two stages in a general programming process: programming and running are all embodied in E-Block. We realized E-Block by wireless and infrared technology and gave it feedbacks on both screen and programming blocks. The result of a preliminary user study proved that E-Block is attractive to children and easy to learn and use.,0,9.80392156863
UIST,efdc10eb417a668ec6c1d6985025e1e824280cfe,UIST,2013,Identifying emergent behaviours from longitudinal web use,Aitor Apaolaza,2884921,"Laboratory studies present difficulties in the understanding of how usage evolves over time. Employed observations are obtrusive and not naturalistic. Our system employs a remote capture tool that provides longitudinal low-level interaction data. It is easily deployable into any Web site allowing deployments in-the-wild and is completely unobtrusive. Web application interfaces are designed assuming users' goals. Requirement specifications contain well defined use cases and scenarios that drive design and subsequent optimisations. Users' interaction patterns outside the expected ones are not considered. This results in an optimisation for a stylised user rather than a real one. A bottom-up analysis from low-level interaction data makes possible the emergence of users' tasks. Similarities among users can be found and solutions that are effective for real users can be designed. Factors such as learnability and how interface changes affect users are difficult to observe in laboratory studies. Our solution makes it possible, adding a longitudinal point of view to traditional laboratory studies. The capture tool is deployed in real world Web applications capturing in-situ data from users. These data serve to explore analysis and visualisation possibilities. We present an example of the exploration results with one Web application.",0,10.5504587156
UIST,55bfe4f401371cde3db15db0e50b551ed6abc0b6,UIST,2011,ShowMeHow: translating user interface instructions between applications,"Vidya Ramesh, Charlie Hsu, Maneesh Agrawala, Björn Hartmann","6665458, 3165274, 1820412, 4020023","Many people learn how to use complex authoring applications through tutorials. However, user interfaces for authoring tools differ between versions, platforms, and competing products, limiting the utility of tutorials. Our goal is to make tutorials more useful by enabling users to repurpose tutorials between similar applications. We introduce UI translation interfaces which enable users to locate commands in one application using the interface language of another application. Our end-user tool, ShowMeHow, demonstrates two interaction techniques to accomplish translations: 1) direct manipulation of interface facades and 2) text search for commands using the vocabulary of another application. We discuss tools needed to construct the translation maps that enable these techniques. An initial study (n=12) shows that users can locate unfamiliar commands twice as fast with interface facades. A second study showed that users can work through tutorials written for one application in another application.",11,49.0476190476
UIST,1d08ce944779b349f5b0823d21a895630eae16ba,UIST,1991,The PICASSO applications framework,"Lawrence A. Rowe, Joseph A. Konstan, Brian Christopher Smith, Steve Seitz, Chung Liu","1723155, 2478310, 1937274, 2399812, 7531278","PICASSO is a graphical user interface development system that includes an interface toolkit end an application framework. The application framework provides high-level abstractions including modal dialog boxes end non-modal frames end partels simh.r to conventional programming language procedures and co-routines. These abstractions can be used to define objects that have local variables and that can be called with parameters. PICASSO also has a constraint system that is used to bind program variables to widgets, to implement triggered behaviors, and to itn-plement multiple views of data. The system is implemented in Common Lisp using the Common Lisp Object System and the CLX interface to the X Window System.",7,34.7826086957
UIST,34280d794f1e8aae6c624b0b8131f243af911fda,UIST,2008,Inky: a sloppy command line for the web with rich visual feedback,"Rob Miller, Victoria H. Chou, Michael S. Bernstein, Greg Little, Max Van Kleek, David R. Karger, Monica M. C. Schraefel","1723785, 2067950, 3047089, 1715840, 2341082, 1743286, 2284695","We present Inky, a command line for shortcut access to common web tasks. Inky aims to capture the efficiency benefits of typed commands while mitigating their usability problems. Inky commands have little or no new syntax to learn, and the system displays rich visual feedback while the user is typing, including missing parameters and contextual information automatically clipped from the target web site. Inky is an example of a new kind of hybrid between a command line and a GUI interface. We describe the design and implementation of two prototypes of this idea, and report the results of a preliminary user study.",24,50.0
UIST,8e3c2a20abd0d0470a66a1749ffaf205cc994c39,UIST,2011,Active bone-conducted sound sensing for wearable interfaces,"Kentaro Takemura, Akihiro Ito, Jun Takamatsu, Tsukasa Ogasawara","3298569, 7146716, 2312159, 1785194","In this paper, we propose a wearable sensor system that measures an angle of an elbow and position tapped by finger using bone-conducted sound. Our system consists of two microphones and a speaker, and they are attached on forearm. A novelty of this paper is to use active sensing for measuring an angle of an elbow. In this paper, active sensing means to emit sounds to a bone, and a microphone receives the sounds reflected at the elbow. The reflection of sound depends on the angle of elbow. Since frequencies of bone-conducted sound by tapping and from the speaker are different, these proposed techniques can be used simultaneously. We confirmed the feasibility of proposed system through experiments.",10,46.6666666667
UIST,6e64943d33fda4f4f12c4557dd620e8e44d424a8,UIST,2012,Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor,"David Kim, Otmar Hilliges, Shahram Izadi, Alex Butler, Jiawen Chen, Iasonas Oikonomidis, Patrick Olivier","6308833, 2531379, 1699068, 1684414, 1967685, 1933875, 1707234","Digits is a wrist-worn sensor that recovers the full 3D pose of the user's hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is specifically designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user's wrist, but optically image the entirety of the user's hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each finger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user's hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios, including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.",127,100.0
UIST,66f7362be834ec207c7a4d79d78ed3a604a6c2e9,UIST,2011,Portico: tangible interaction on and around a tablet,"Daniel Avrahami, Jacob O. Wobbrock, Shahram Izadi","2667384, 1796045, 1699068","We present Portico, a portable system for enabling tangible interaction on and around tablet computers. Two cameras on small foldable arms are positioned above the display to recognize a variety of physical objects placed on or around the tablet. These cameras have a larger field-of-view than the screen, allowing Portico to extend interaction significantly beyond the tablet itself. Our prototype, which uses a 12"" tablet, delivers an interaction space six times the size of the tablet screen. Portico thus allows tablets to extend both their sensing capabilities and interaction space without sacrificing portability. We describe the design of our system and present a number of applications that demonstrate Portico's unique capability to track objects. We focus on a number of fun applications that demonstrate how such a device can be used as a low-cost way to create personal surface computing experiences. Finally, we discuss the challenges in supporting tangible interaction beyond the screen and describe possible mechanisms for overcoming them.",24,70.9523809524
UIST,e0aa9fbccb049872fb4806f3f8d5efece6355a9b,UIST,2014,Painting with Bob: assisted creativity for novices,"Luca Benedetti, Holger Winnemöller, Massimiliano Corsini, Roberto Scopigno","2408918, 2168852, 2761974, 7980724","Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.",7,72.0930232558
UIST,62c7ed314eeee1605280d81d77d9f169adbcdc50,UIST,2012,Elastic scroll for multi-focus interactions,"Kazuki Takashima, Kazuyuki Fujita, Yuichi Itoh, Yoshifumi Kitamura","1725740, 1725355, 1707181, 1690228","This paper proposes a novel and efficient multi-focus scroll interface that consists of a two-step operation using a con-tents distortion technique. The displayed content can be handled just like an elastic material that can be shrunk and stretched by a user's fingers. In the first operation, the us-er's dragging temporarily shows the results of the viewport transition of the scroll by elastically distorting the content. This operation allows the user to see both the newly obtained and the original focus on the viewport. Then, three types of simple gestures can be used to perform the second operation such as scrolling, restoring and zooming out to get the demanded focus (or foci).",1,25.0
UIST,97d7659e77bf45d425722a0a0e42685930f6965f,UIST,2014,StackBlock: block-shaped interface for flexible stacking,"Masahiro Ando, Yuichi Itoh, Toshiki Hosoi, Kazuki Takashima, Kosuke Nakajima, Yoshifumi Kitamura","3034840, 1707181, 2239471, 1725740, 2360645, 1690228","We propose a novel building-block interface called StackBlock that allows users to precisely construct 3D shapes by stacking blocks at arbitrary positions and angles. Infrared LEDs and phototransistors are laid in a matrix on each surface of a block to detect the areas contacted by other blocks. Contact-area information is transmitted to the bottom block by the relay of infrared communication between the stacked blocks, and then the bottom block sends all information to the host computer for recognizing the 3D shape. We implemented a prototype of StackBlock with several blocks and evaluated the accuracy and latency of 3D shape recognition. As a result, StackBlock could sufficiently perform 3D shape recognition for users' flexible stacking.",2,41.4728682171
UIST,0d690e0fa277454f38acbdd7d45cdc9ad802ed82,UIST,2016,Switch++: An Output Device of the Switches by the Finger Gestures,"Yukiko Yokomizo, Tomoya Kotegawa, Paul Haimes, Tetsuaki Baba","2952724, 3493063, 2995227, 2996142","Regarding human-machine-interfaces, switches have not changed significantly despite the machines themselves evolving constantly. In this paper, we propose a new method of operability for devices by providing multiple switches dynamically, and users choose the switch that has the functionality that they want to use. Switch++ senses the mental model of the operating sensation of switches against the user's finger gestures and changes the shape of the switch and its affordances accordingly. We design the interface based on the raw data.",0,44.6540880503
UIST,93107e0b5d64324ba71ecb1fdbc298a1c421368e,UIST,2008,"SideSight: multi-""touch"" interaction around small devices","Alex Butler, Shahram Izadi, Steve Hodges","1684414, 1699068, 1736330","Interacting with mobile devices using touch can lead to fingers occluding valuable screen real estate. For the smallest devices, the idea of using a touch-enabled display is almost wholly impractical. In this paper we investigate sensing user touch around small screens like these. We describe a prototype device with infra-red (IR) proximity sensors embedded along each side and capable of detecting the presence and position of fingers in the adjacent regions. When this device is rested on a flat surface, such as a table or desk, the user can carry out single and multi-touch gestures using the space around the device. This gives a larger input space than would otherwise be possible which may be used in conjunction with or instead of on-display touch input. Following a detailed description of our prototype, we discuss some of the interactions it affords.",107,94.2857142857
UIST,0564c581517d6ca954635451d8d87150ba889242,UIST,2008,Bringing physics to the surface,"Andrew D. Wilson, Shahram Izadi, Otmar Hilliges, Armando Garcia-Mendoza, David S. Kirk","1767449, 1699068, 2531379, 2387892, 1698434","This paper explores the intersection of emerging surface technologies, capable of sensing multiple contacts and of-ten shape information, and advanced games physics engines. We define a technique for modeling the data sensed from such surfaces as input within a physics simulation. This affords the user the ability to interact with digital objects in ways analogous to manipulation of real objects. Our technique is capable of modeling both multiple contact points and more sophisticated shape information, such as the entire hand or other physical objects, and of mapping this user input to contact forces due to friction and collisions within the physics simulation. This enables a variety of fine-grained and casual interactions, supporting finger-based, whole-hand, and tangible input. We demonstrate how our technique can be used to add real-world dynamics to interactive surfaces such as a vision-based tabletop, creating a fluid and natural experience. Our approach hides from application developers many of the complexities inherent in using physics engines, allowing the creation of applications without preprogrammed interaction behavior or gesture recognition.",136,100.0
UIST,977c9dbef168834922940fd0913a40b23b02602c,UIST,2009,Perceptual interpretation of ink annotations on line charts,"Nicholas Kong, Maneesh Agrawala","2892333, 1820412","Asynchronous collaborators often use freeform ink annotations to point to visually salient perceptual features of line charts such as peaks or humps, valleys, rising slopes and declining slopes. We present a set of techniques for interpreting such annotations to algorithmically identify the corresponding perceptual parts. Our approach is to first apply a parts-based segmentation algorithm that identifies the visually salient perceptual parts in the chart. Our system then analyzes the freeform annotations to infer the corresponding peaks, valleys or sloping segments. Once the system has identified the perceptual parts it can highlight them to draw further attention and reduce ambiguity of interpretation in asynchronous collaborative discussions.",11,20.0
UIST,03ea08cc4a0919201a6fac22f743475dfec0bf6e,UIST,2010,Madgets: actuating widgets on interactive tabletops,"Malte Weiss, Florian Schwarz, Simon Jakubowski, Jan O. Borchers","2563842, 3095047, 2343116, 1692837","We present a system for the actuation of tangible magnetic widgets (Madgets) on interactive tabletops. Our system combines electromagnetic actuation with fiber optic tracking to move and operate physical controls. The presented mechanism supports actuating complex tangibles that consist of multiple parts. A grid of optical fibers transmits marker positions past our actuation hardware to cameras below the table. We introduce a visual tracking algorithm that is able to detect objects and touches from the strongly sub-sampled video input of that grid. Six sample Madgets illustrate the capabilities of our approach, ranging from tangential movement and height actuation to inductive power transfer. Madgets combine the benefits of passive, untethered, and translucent tangibles with the ability to actuate them with multiple degrees of freedom.",52,89.5348837209
UIST,c5924b5b3c61a52b180bea0205686485a6482773,UIST,2013,Hanzi Lamp: an intelligent guide interface for Chinese character learning,"Yujie Hong, Lei Shi, Fangtian Ying","2597667, 4608232, 1848748","In recent years, an increasing number of people want to understand Chinese culture and Hanzi (Chinese characters) is a key to that. Learning Chinese characters as a second language can be quite challenging. What confuses learners is not only the meaning of Hanzi, but also the complicated writing rules since they are very different from the alphabetic ways English uses. Although many mobile applications and online learning systems provide Hanzi teaching interfaces, they are restricted to the two-dimensional screens and thus they offer little flexibility for practicing while learning. In this paper, we propose Hanzi Lamp, an intelligent guide interface which allows users to practice writing under real-time and adaptive projected guidance. Information captured by sensors has been utilized to perceive learners' behaviors and make appropriate response. We explore how we can enhance Chinese characters learning by improving the system's understanding of physical learning environment.",0,10.5504587156
UIST,3941343bf2c2ac4d6555afab0f1923250ff46a97,UIST,2003,Classroom BRIDGE: using collaborative public and desktop timelines to support activity awareness,"Craig H. Ganoe, Jacob P. Somervell, Dennis C. Neale, Philip L. Isenhour, John M. Carroll, Mary Beth Rosson, D. Scott McCrickard","1784389, 2949902, 2871459, 2042202, 1682687, 1715072, 1693753","Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.",38,43.75
UIST,3ddb58b908e6dc8e2c656138af5b6d621ad8177a,UIST,2004,Augmenting conversations using dual-purpose speech,"Kent Lyons, Christopher Skeels, Thad Starner, Cornelis M. Snoeck, Benjamin A. Wong, Daniel Ashbrook","2073793, 1890774, 1738894, 1846463, 3273492, 2071682","In this paper, we explore the concept of dual-purpose speech: speech that is socially appropriate in the context of a human-to-human conversation which also provides meaningful input to a computer. We motivate the use of dual-purpose speech and explore issues of privacy and technological challenges related to mobile speech recognition. We present three applications that utilize dual-purpose speech to assist a user in conversational tasks: the Calendar Navigator Agent, DialogTabs, and Speech Courier. The Calendar Navigator Agent navigates a user's calendar based on socially appropriate speech used while scheduling appointments. DialogTabs allows a user to postpone cognitive processing of conversational material by proving short-term capture of transient information. Finally, Speech Courier allows asynchronous delivery of relevant conversational information to a third party.",27,31.5789473684
UIST,139b0c1780556440bd1a9f9ca68a20fe0e0599da,UIST,2007,Programming by a sample: rapidly creating web applications with d.mix,"Björn Hartmann, Leslie Wu, Kevin Collins, Scott R. Klemmer","4020023, 2524633, 4532423, 1728167","Source-code examples of APIs enable developers to quickly gain a gestalt understanding of a library's functionality, and they support organically creating applications by incrementally modifying a functional starting point. As an increasing number of web sites provide APIs, significantlatent value lies in connecting the complementary representations between site and service - in essence, enabling sites themselves to be the example corpus. We introduce d.mix, a tool for creating web mashups that leverages this site-to-service correspondence. With d.mix, users browse annotated web sites and select elements to sample. d.mix's sampling mechanism generates the underlying service calls that yield those elements. This code can be edited, executed, and shared in d.mix's wiki-based hosting environment. This sampling approach leverages pre-existing web sites as example sets and supports fluid composition and modification of examples. An initial study with eight participants found d.mix to enable rapid experimentation, and suggested avenues for improving its annotation mechanism.",59,77.7777777778
UIST,7d4c8e8d4771ad4c39db9da9c0bc9a82415ea7dd,UIST,2014,Teegi: Tangible EEG Interface,"Jérémy Frey, Renaud Gervais, Stéphanie Fleck, Fabien Lotte, Martin Hachet","1741656, 2923045, 2400957, 2890785, 2281511","We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, engaging and informative way. To this end, we have designed a new system based on a unique combination of spatial augmented reality, tangible interaction and real-time neurotechnologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and informative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.",5,63.1782945736
UIST,548f6381f994cf5d7ec1a294734f235e788a45a2,UIST,2014,Through the combining glass,"Diego Martinez Plasencia, Florent Berthaut, Abhijit Karnik, Sriram Subramanian","2454780, 2174703, 2556545, 1702794","Reflective optical combiners like beam splitters and two way mirrors are used in AR to overlap digital contents on the users' hands or bodies. Augmentations are usually unidirectional, either reflecting virtual contents on the user's body (Situated Augmented Reality) or augmenting user's reflections with digital contents (AR mirrors). But many other novel possibilities remain unexplored. For example, users' hands, reflected inside a museum AR cabinet, can allow visitors to interact with the artifacts exhibited. Projecting on the user's hands as their reflection cuts through the objects can be used to reveal objects' internals. Augmentations from both sides are blended by the combiner, so they are consistently seen by any number of users, independently of their location or, even, the side of the combiner through which they are looking. This paper explores the potential of optical combiners to merge the space in front and behind them. We present this design space, identify novel augmentations/interaction opportunities and explore the design space using three prototypes.",5,63.1782945736
UIST,10e1beb2d438425bc66b78065f0bb767ffa0b962,UIST,2015,These Aren't the Commands You're Looking For: Addressing False Feedforward in Feature-Rich Software,"Benjamin J. Lafreniere, Parmit K. Chilana, Adam Fourney, Michael A. Terry","2813134, 1858682, 3318905, 1740154","The names, icons, and tooltips of commands in feature-rich software are an important source of guidance when locating and selecting amongst commands. Unfortunately, these cues can mislead users into believing that a command is appropriate for a given task, when another command would be more appropriate, resulting in wasted time and frustration. In this paper, we present <i>command disambiguation techniques</i> that inform the user of alternative commands <i>before, during</i>, and <i>after</i> an incorrect command has been executed. To inform the design of these techniques, we define categories of <i>false-feedforward errors</i> caused by misleading interface cues, and identify causes for each. Our techniques are the first designed explicitly to solve this problem in feature-rich software. A user study showed enthusiasm for the techniques, and revealed their potential to play a key role in learning of feature-rich software.",0,16.2280701754
UIST,f5579a5fb45689d0abd1f684f4ae8c2e6045952b,UIST,2016,Mobile Fabrication,"Thijs Roumen, Bastian Kruck, Tobias Dürschmid, Tobias Nack, Patrick Baudisch","2927226, 3073154, 3428509, 3491514, 1729393","We present an exploration into the future of fabrication, in particular the vision of mobile fabrication, which we define as ""personal fabrication on the go"". We explore this vision with two surveys, two simple hardware prototypes, matching custom apps that provide users with access to a solution database, custom fabrication processes we designed specifically for these devices, and a user study conducted in situ on metro trains. Our findings suggest that mobile fabrication is a compelling next direction for personal fabrication. From our experience with the prototypes we derive hardware requirements to make mobile fabrication also technically feasible.",0,44.6540880503
UIST,80c039eb8b30058601f5002d446983fe05249367,UIST,2010,Reflective haptics: haptic augmentation of GUIs through frictional actuation of stylus-based interactions,"Fabian Hemmert, Alexander Müller, Ron Jagodzinski, Götz Wintergerst, Gesche Joost","3036450, 1769469, 1826751, 3183625, 2473156","In this paper, we present a novel system for stylus-based GUI interactions: Simulated physics through actuated frictional properties of a touch screen stylus. We present a prototype that implements a series of principles which we propose for the design of frictionally augmented GUIs. It is discussed how such actuation could be a potential addition of value for stylus-controlled GUIs, through enabling prioritized content, allowing for inherent confirmation, and leveraging on manual dexterity.",1,22.0930232558
UIST,6af77222df35390294c3cb5bfd3a5c192fce719c,UIST,2016,UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time,"Emi Tamaki, Terence Chan, Ken Iwasaki","3046871, 2134024, 2530329","Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.",0,44.6540880503
UIST,f25b71b4a5e7884e1d3b9ce97ed0b727b296fef5,UIST,2016,Thickness Control Technique for Printing Tactile Sheets with Fused Deposition Modeling,"Haruki Takahashi, Homei Miyashita","3493276, 1796760","We present a printing technique that controls the thickness of objects by increasing and decreasing the amount of material extruded during printing. Using this technique, printers can dynamically control thickness and output thicker objects without a staircase effect. This technique allows users to print aesthetic pattern sheets and objects that are tactile without requiring any new hardware. This extends the capabilities of fused deposition modeling (FDM) 3D printers in a simple way. We describe a method of generating and calculating a movement path for printing tactile sheets, and demonstrate the usage and processing of example objects.",0,44.6540880503
UIST,b50ee00d8e9ce80e7aba8aa2778ee6776f5568dd,UIST,2016,Design and Evaluation of EdgeWrite Alphabets for Round Face Smartwatches,"Keiichi Ueno, Kentaro Go, Yuichiro Kinoshita","3492736, 2017116, 3154983","This study presents a project aimed at designing and evaluating a unistroke gesture set of alphanumeric characters targeting round-face smartwatches. We conducted a user study with 10 participants to generate the basic gesture design for 40 characters. For each character, we measured the preference and agreement scores and uncovered any challenges faced in designing unistroke gestures for round-face smartwatches. We developed a gesture recognizer using machine learning, which used a backpropagation mechanism to evaluate the designed gestures. Using the gesture recognizer, we collected 80,000 gesture data, and evaluated them with 5-fold cross-validation. The obtained mean recognition rate was 92.14%.",0,44.6540880503
UIST,08b308197f7168c69e10df332719cce62b084d3a,UIST,2013,Pursuit calibration: making gaze calibration less tedious and more flexible,"Ken Pfeuffer, Mélodie Vidal, Jayson Turner, Andreas Bulling, Hans-Werner Gellersen","3171800, 2424540, 2235543, 3194727, 4919595","Eye gaze is a compelling interaction modality but requires user calibration before interaction can commence. State of the art procedures require the user to fixate on a succession of calibration markers, a task that is often experienced as difficult and tedious. We present pursuit calibration, a novel approach that, unlike existing methods, is able to detect the user's attention to a calibration target. This is achieved by using moving targets, and correlation of eye movement and target trajectory, implicitly exploiting smooth pursuit eye movement. Data for calibration is then only sampled when the user is attending to the target. Because of its ability to detect user attention, pursuit calibration can be performed implicitly, which enables more flexible designs of the calibration task. We demonstrate this in application examples and user studies, and show that pursuit calibration is tolerant to interruption, can blend naturally with applications and is able to calibrate users without their awareness.",26,90.3669724771
UIST,43fe42999f13de090d7e7859c96307f1e6eb4290,UIST,2009,Mining web interactions to automatically create mash-ups,"Jeffrey P. Bigham, Ryan S. Kaminsky, Jeffrey Nichols","1744846, 2427592, 1687909","The deep web contains an order of magnitude more information than the surface web, but that information is hidden behind the web forms of a large number of web sites. Metasearch engines can help users explore this information by aggregating results from multiple resources, but previously these could only be created and maintained by programmers. In this paper, we explore the automatic creation of metasearch mash-ups by mining the web interactions of multiple web users to find relations between query forms on different web sites. We also present an implemented system called TX2 that uses those connections to search multiple deep web resources simultaneously and integrate the results in context in a single results page. TX2 illustrates the promise of constructing mash-ups automatically and the potential of mining web interactions to explore deep web resources.",4,5.71428571429
UIST,cfe826d262443e4dfa3f88c0dc6c945548e7a229,UIST,2011,"1 thumb, 4 buttons, 20 words per minute: design and evaluation of H4-writer","I. Scott MacKenzie, R. William Soukoreff, Joanna Helga","1692873, 1746261, 2086938","We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.",14,56.6666666667
UIST,293d9f8ef9ac38a441a7aab4f4933b5f61642739,UIST,2012,PiVOT: personalized view-overlays for tabletops,"Abhijit Karnik, Diego Martinez Plasencia, Walterio W. Mayol-Cuevas, Sriram Subramanian","2556545, 2454780, 1731214, 1702794","We present PiVOT, a tabletop system aimed at supporting mixed-focus collaborative tasks. Through two view-zones, PiVOT provides personalized views to individual users while presenting an unaffected and unobstructed shared view to all users. The system supports multiple personalized views which can be present at the same spatial location and yet be only visible to the users it belongs to. The system also allows the creation of personal views that can be either 2D or (auto-stereoscopic) 3D images. We first discuss the motivation and the different implementation principles required for realizing such a system, before exploring different designs able to address the seemingly opposing challenges of shared and personalized views. We then implement and evaluate a sample prototype to validate our design ideas and present a set of sample applications to demonstrate the utility of the system.",14,64.2156862745
UIST,177d46feee4c8c7822d907977d6b629ba4a2da66,UIST,2006,User interface façades: towards fully adaptable user interfaces,"Wolfgang Stuerzlinger, Olivier Chapuis, Dusty Phillips, Nicolas Roussel","3342964, 3342979, 2718392, 1728921","User interfaces are becoming more and more complex. Adaptable and adaptive interfaces have been proposed to address this issue and previous studies have shown that users prefer interfaces that they can adapt to self-adjusting ones. However, most existing systems provide users with little support for adapting their interfaces. Interface customization techniques are still very primitive and usually constricted to particular applications. In this paper, we present User Interface Fa&#231;ades, a system that provides users with simple ways to adapt, reconfigure, and re-combine existing graphical interfaces, through the use of direct manipulation techniques. The paper describes the user's view of the system, provides some technical details, and presents several examples to illustrate its potential.",68,88.75
UIST,1692fcf70d6515fc7269b765282daa954bcbe03f,UIST,2006,Using a low-cost electroencephalograph for task classification in HCI research,"Johnny Chung Lee, Desney S. Tan","1957254, 1719056","Modern brain sensing technologies provide a variety of methods for detecting specific forms of brain activity. In this paper, we present an initial step in exploring how these technologies may be used to perform task classification and applied in a relevant manner to HCI research. We describe two experiments showing successful classification between tasks using a low-cost off-the-shelf electroencephalograph (EEG) system. In the first study, we achieved a mean classification accuracy of 84.0% in subjects performing one of three cognitive tasks - rest, mental arithmetic, and mental rotation - while sitting in a controlled posture. In the second study, conducted in more ecologically valid setting for HCI research, we attained a mean classification accuracy of 92.4% using three tasks that included non-cognitive features: a relaxation task, playing a PC based game without opponents, and engaging opponents within the game. Throughout the paper, we provide lessons learned and discuss how HCI researchers may utilize these technologies in their work.",58,81.25
UIST,56d1a9eb9cefe7bd0575b37ed9cc20e11b4d87d6,UIST,2009,Optically sensing tongue gestures for computer input,"T. Scott Saponas, Daniel Kelly, Babak A. Parviz, Desney S. Tan","1766388, 5139326, 2133406, 1719056","Many patients with paralyzing injuries or medical conditions retain the use of their cranial nerves, which control the eyes, jaw, and tongue. While researchers have explored eye-tracking and speech technologies for these patients, we believe there is potential for directly sensing explicit tongue movement for controlling computers. In this paper, we describe a novel approach of using infrared optical sensors embedded within a dental retainer to sense tongue gestures. We describe an experiment showing our system effectively discriminating between four simple gestures with over 90% accuracy. In this experiment, users were also able to play the popular game Tetris with their tongues. Finally, we present lessons learned and opportunities for future work.",24,45.7142857143
UIST,1be4b2bc0e149981f8814a0d9c130fd442c48e93,UIST,2006,Comparing and managing multiple versions of slide presentations,"Steven M. Drucker, Georg Petschnigg, Maneesh Agrawala","2311676, 1964338, 1820412","Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors.",16,17.5
UIST,038580ef558ae5f51071bad1cecb67e6f0e197ef,UIST,2007,ThinSight: versatile multi-touch sensing for thin form-factor displays,"Steve Hodges, Shahram Izadi, Alex Butler, Alban Rrustemi, William Buxton","1736330, 1699068, 1684414, 2630153, 6037251","ThinSight is a novel optical sensing system, fully integrated into a thin form factor display, capable of detecting multi-ple fingers placed on or near the display surface. We describe this new hardware in detail, and demonstrate how it can be embedded behind a regular LCD, allowing sensing without degradation of display capability. With our approach, fingertips and hands are clearly identifiable through the display. The approach of optical sensing also opens up the exciting possibility for detecting other physical objects and visual markers through the display, and some initial experiments are described. We also discuss other novel capabilities of our system: interaction at a distance using IR pointing devices, and IR-based communication with other electronic devices through the display. A major advantage of ThinSight over existing camera and projector based optical systems is its compact, thin form-factor making such systems even more deployable. We therefore envisage using ThinSight to capture rich sensor data through the display which can be processed using computer vision techniques to enable both multi-touch and tangible interaction.",79,88.8888888889
UIST,61e863ca0147c724cc6c12dad313a88ab00706a1,UIST,2016,Developing fMRI-Compatible Interaction Systems through Air Pressure,"Handityo Aulia Putra, Xiangshi Ren","2798678, 1805963","We leverage the use of air pressure to expand the interaction space within fMRI (functional magnetic resonance imaging). We present three example applications that are not previously possible in conventional fMRI interaction devices: 1) pedal interface that can record continuous pressure value pressed by users, 2) wrist tactile interface that can provide various tactile patterns or stimuli, 3) adjustable resistance joystick that can provide feedback through different resistance levels. Our work shows that the use of air pressure can enable new research opportunities for fMRI researchers.",0,44.6540880503
UIST,6ef56e43260cecba24e614c58bfd433224cc2d56,UIST,2010,Tag expression: tagging with feeling,"Jesse Vig, Matthew Soukup, Shilad Sen, John Riedl","2056908, 2381847, 3342684, 8497382","In this paper we introduce <i>tag expression</i>, a novel form of preference elicitation that combines elements from tagging and rating systems. Tag expression enables users to apply <i>affect</i> to tags to indicate whether the tag describes a reason they like, dislike, or are neutral about a particular item. We present a user interface for applying affect to tags, as well as a technique for visualizing the overall community's affect. By analyzing 27,773 tag expressions from 553 users entered in a 3-month period, we empirically evaluate our design choices. We also present results of a survey of 97 users that explores users' motivations in tagging and measures user satisfaction with tag expression.",10,61.0465116279
UIST,4ef21aa591b59935d594f822c741e5217288e75d,UIST,2009,Detecting and leveraging finger orientation for interaction with direct-touch surfaces,"Feng Wang, Xiang Cao, Xiangshi Ren, Pourang Irani","1745827, 7299595, 1805963, 1773923","Current interactions on direct-touch interactive surfaces are often modeled based on properties of the input channel that are common in traditional graphical user interfaces (GUI) such as x-y coordinate information. Leveraging additional information available on the surfaces could potentially result in richer and novel interactions. In this paper we specifically explore the role of finger orientation. This property is typically ignored in touch-based interactions partly because of the ambiguity in determining it solely from the contact shape. We present a simple algorithm that unambiguously detects the directed finger orientation vector in real-time from contact information only, by considering the dynamics of the finger landing process. Results of an experimental evaluation show that our algorithm is stable and accurate. We then demonstrate how finger orientation can be leveraged to enable novel interactions and to infer higher-level information such as hand occlusion or user position. We present a set of orientation-aware interaction techniques and widgets for direct-touch surfaces.",68,84.2857142857
UIST,66e30cb10e733c818bd4511201e75c2da4276f73,UIST,2008,Is the sky pure today? AwkChecker: an assistive tool for detecting and correcting collocation errors,"Taehyun Park, Edward Lank, Pascal Poupart, Michael A. Terry","2200693, 1788496, 1807041, 1740154","Collocation preferences represent the commonly used expressions, idioms, and word pairings of a language. Because collocation preferences arise from consensus usage, rather than a set of well-defined rules, they must be learned on a case-by-case basis, making them particularly challenging for non-native speakers of a language. To assist non-native speakers with these parts of a language, we developed AwkChecker, the first end-user tool geared toward helping non-native speakers detect and correct collocation errors in their writing. As a user writes, AwkChecker automatically flags collocation errors and suggests replacement expressions that correspond more closely to consensus usage. These suggestions include example usage to help users choose the best candidate. We describe AwkChecker's interface, its novel methods for detecting collocation errors and suggesting alternatives, and an early study of its use by non-native English speakers at our institution. Collectively, these contributions advance the state of the art in writing aids for non-native speakers.",19,41.4285714286
UIST,4fc759b7f6e5610c0223e72ad7a334aaabe56230,UIST,2010,"What can internet search engines ""suggest"" about the usage and usability of popular desktop applications?","Adam Fourney, Richard Mann, Michael A. Terry","3318905, 4814403, 1740154","In this paper, we show how Internet search query logs can yield rich, ecologically valid data sets describing the common tasks and issues that people encounter when using software on a day-to-day basis. These data sets can feed directly into standard usability practices. We address challenges in collecting, filtering, and summarizing queries, and show how data can be collected at very low cost, even without direct access to raw query logs.",0,9.3023255814
UIST,8294f610696e9ca1525f86d390bb872ed2d6fce4,UIST,2006,CueTIP: a mixed-initiative interface for correcting handwriting errors,"Michael Shilman, Desney S. Tan, Patrice Y. Simard","2139615, 1719056, 3241598","With advances in pen-based computing devices, handwriting has become an increasingly popular input modality. Researchers have put considerable effort into building intelligent recognition systems that can translate handwriting to text with increasing accuracy. However, handwritten input is inherently ambiguous, and these systems will always make errors. Unfortunately, work on error recovery mechanisms has mainly focused on interface innovations that allow users to manually transform the erroneous recognition result into the intended one. In our work, we propose a mixed-initiative approach to error correction. We describe CueTIP, a novel correction interface that takes advantage of the recognizer to continually evolve its results using the additional information from user corrections. This significantly reduces the number of actions required to reach the intended result. We present a user study showing that CueTIP is more efficient and better preferred for correcting handwriting recognition errors. Grounded in the discussion of CueTIP, we also present design principles that may be applied to mixed-initiative correction interfaces in other domains.",33,46.25
UIST,fe6995ad6d3ebdeef3814cb31f5e14df63dbd226,UIST,2016,EdgeVib: Effective Alphanumeric Character Output Using a Wrist-Worn Tactile Display,"Yi-Chi Liao, Yi-Ling Chen, Jo-Yu Lo, Rong-Hao Liang, Li-Wei Chan, Bing-Yu Chen","3119590, 5014321, 3491661, 1705512, 1682665, 1733344","This paper presents <i>EdgeVib</i>, a system of spatiotemporal vibration patterns for delivering alphanumeric characters on wrist-worn vibrotactile displays. We first investigated spatiotemporal pattern delivery through a watch-back tactile display by performing a series of user studies. The results reveal that employing a 2&#215;2 vibrotactile array is more effective than employing a 3&#215;3 one, because the lower-resolution array creates clearer tactile sensations in less time consumption. We then deployed EdgeWrite patterns on a 2&#215;2 vibrotactile array to determine any difficulties of delivering alphanumerical characters, and then modified the unistroke patterns into multistroke EdgeVib ones on the basis of the findings. The results of a 24-participant user study reveal that the recognition rates of the modified multistroke patterns were significantly higher than the original unistroke ones in both alphabet (85.9% vs. 70.7%) and digits (88.6% vs. 78.5%) delivery, and a further study indicated that the techniques can be generalized to deliver two-character compound messages with recognition rates higher than 83.3%. The guidelines derived from our study can be used for designing watch-back tactile displays for alphanumeric character output.",0,44.6540880503
UIST,785da4aa393ec4154a9aae727ac01674c02d60e2,UIST,2011,Query-feature graphs: bridging user vocabulary and system functionality,"Adam Fourney, Richard Mann, Michael A. Terry","3318905, 4814403, 1740154","This paper introduces query-feature graphs, or QF-graphs. QF-graphs encode associations between high-level descriptions of user goals (articulated as natural language search queries) and the specific features of an interactive system relevant to achieving those goals. For example, a QF-graph for the GIMP graphics manipulation software links the query ""GIMP black and white"" to the commands ""desaturate"" and ""grayscale."" We demonstrate how QF-graphs can be constructed using search query logs, search engine results, web page content, and localization data from interactive systems. An analysis of QF-graphs shows that the associations produced by our approach exhibit levels of accuracy that make them eminently usable in a range of real-world applications. Finally, we present three hypothetical user interface mechanisms that illustrate the potential of QF-graphs: search-driven interaction, dynamic tooltips, and app-to-app analogy search.",21,67.619047619
UIST,681ee93e7c1f598149ff78a29e929d7460b7ae25,UIST,2012,Interactions speak louder than words: shared user models and adaptive interfaces,Kyle Montague,2414592,"Touch-screens are becoming increasingly ubiquitous. They have great appeal due to their capabilities to support new forms of human interaction, including their abilities to interpret rich gestural inputs, render flexible user interfaces and enable multi-user interactions. However, the technology creates new challenges and barriers for users with limited levels of vision and motor abilities. The PhD work described in this paper proposes a technique combining Shared User Models (SUM) and adaptive interfaces to improve the accessibility of touch-screen devices for people with low levels of vision and motor ability. SUM, built from an individual's interaction data across multiple applications and devices, is used to infer new knowledge of their abilities and characteristics, without the need for continuous calibration exercises or user configurations. This approach has been realized through the development of an open source software framework to support the creation of applications that make use of SUM to adapt interfaces that match the needs of individual users.",2,33.8235294118
UIST,abf49a60d93c142a82a16735120b14dc6a43c0b6,UIST,2013,Augmenting braille input through multitouch feedback,"Hugo Nicolau, Kyle Montague, João Guerreiro, Diogo Marques, Tiago João Vieira Guerreiro, Craig D. Stewart, Vicki L. Hanson","2334545, 2414592, 1718751, 2868723, 2742146, 1709841, 1730685","Current touch interfaces lack the rich tactile feedback that allows blind users to detect and correct errors. This is especially relevant for multitouch interactions, such as Braille input. We propose HoliBraille, a system that combines touch input and multi-point vibrotactile output on mobile devices. We believe this technology can offer several benefits to blind users; namely, convey feedback for complex multitouch gestures, improve input performance, and support inconspicuous interactions. In this paper, we present the design of our unique prototype, which allows users to receive multitouch localized vibrotactile feedback. Preliminary results on perceptual discrimination show an average of 100% and 82% accuracy for single-point and chord discrimination, respectively. Finally, we discuss a text-entry application with rich tactile feedback.",3,45.871559633
UIST,3f144de4e81a92bc804fb00ca667a55636d6c36b,UIST,2012,PICL: portable in-circuit learner,"Adam Fourney, Michael A. Terry","3318905, 1740154","This paper introduces the PICL, the portable in-circuit learner. The PICL explores the possibility of providing standalone, low-cost, programming-by-demonstration machine learning capabilities to circuit prototyping. To train the PICL, users attach a sensor to the PICL, demonstrate example input, then specify the desired output (expressed as a voltage) for the given input. The current version of the PICL provides two learning modes, binary classification and linear regression. To streamline training and also make it possible to train on highly transient signals (such as those produced by a camera flash or a hand clap), the PICL includes a number of <i>input inferencing</i> techniques. These techniques make it possible for the PICL to learn with as few as one example. The PICL's behavioural repertoire can be expanded by means of various <i>output adapters</i>, which serve to transform the output in useful ways when prototyping. Collectively, the PICL's capabilities allow users of systems such as the Arduino or littleBits electronics kit to quickly add basic sensor-based behaviour, with little or no programming required.",3,40.1960784314
UIST,41437f88cbf2e4d598fcf077c9ef6ce0120728b2,UIST,2009,Overview based example selection in end user interactive concept learning,"Saleema Amershi, James Fogarty, Ashish Kapoor, Desney S. Tan","1719124, 1738171, 7665582, 1719056","Interaction with large unstructured datasets is difficult because existing approaches, such as keyword search, are not always suited to describing concepts corresponding to the distinctions people want to make within datasets. One possible solution is to allow end users to train machine learning systems to identify desired concepts, a strategy known as <i>interactive concept learning</i>. A fundamental challenge is to design systems that preserve end user flexibility and control while also guiding them to provide examples that allow the machine learning system to effectively learn the desired concept. This paper presents our design and evaluation of four new overview based approaches to guiding example selection. We situate our explorations within CueFlik, a system examining end user interactive concept learning in Web image search. Our evaluation shows our approaches not only guide end users to select better training examples than the best performing previous design for this application, but also reduce the impact of not knowing when to stop training the system. We discuss challenges for end user interactive concept learning systems and identify opportunities for future research on the effective design of such systems.",14,28.5714285714
UIST,cf23b1f94e6a2547707ee7a134f18920afb4ddd5,UIST,2005,Role-based control of shared application views,"Lior Berry, Lyn Bartram, Kellogg S. Booth","1743860, 1693758, 1800617","Collaboration often relies on all group members having a shared view of a single-user application. A common situation is a single active presenter sharing a live view of her workstation screen with a passive audience, using simple hardware-based video signal projection onto a large screen or simple bitmap-based sharing protocols. This offers simplicity and some advantages over more sophisticated software-based replication solutions, but everyone has the exact same view of the application. This conflicts with the presenter's need to keep some information and interaction details private. It also fails to recognize the needs of the passive audience, who may struggle to follow the presentation because of verbosity, display clutter or insufficient familiarity with the application.Views that cater to the different roles of the presenter and the audience can be provided by custom solutions, but these tend to be bound to a particular application. In this paper we describe a general technique and implementation details of a prototype system that allows standardized role-specific views of existing single-user applications and permits additional customization that is application-specific with no change to the application source code. Role-based policies control manipulation and display of shared windows and image buffers produced by the application, providing semi-automated privacy protection and relaxed verbosity to meet both presenter and audience needs.",20,32.2580645161
UIST,3475c6e71ddc505e7071f5dec037adb33db2c8a4,UIST,2014,InterTwine: creating interapplication information scent to support coordinated use of software,"Adam Fourney, Benjamin J. Lafreniere, Parmit K. Chilana, Michael A. Terry","3318905, 2813134, 1858682, 1740154","Users often make continued and sustained use of online resources to complement use of a desktop application. For example, users may reference online tutorials to recall how to perform a particular task. While often used in a coordinated fashion, the browser and desktop application provide separate, independent mechanisms for helping users find and re-find task-relevant information. In this paper, we describe InterTwine, a system that links information in the web browser with relevant elements in the desktop application to create interapplication information scent. This explicit link produces a shared interapplication history to assist in re-finding information in both applications. As an example, InterTwine marks all menu items in the desktop application that are currently mentioned in the front-most web page. This paper introduces the notion of interapplication information scent, demonstrates the concept in InterTwine, and describes results from a formative study suggesting the utility of the concept.",4,57.3643410853
UIST,ab8fabc028006176fb55c8de25061a08f6be7250,UIST,2011,Embedding interface sketches in code,"James Simpson, Michael A. Terry","4526368, 1740154","This paper presents a user interface (UI) design tool, GUIIO, which uses ASCII text as its medium for rendering interface components. Like other UI design tools, GUIIO allows individuals to create and manipulate UI components as first-class objects. However, GUIIO has the advantage that its UI designs can be embedded directly within the program code itself. We implemented GUIIO as an extension to an existing development environment. As a result, developers can fluidly transition from editing code to editing the UI mock-up, with the text editor automatically switching its mode from code editing to UI editing as a function of the location of the cursor. By rendering UIs as ASCII art, GUIIO fills an important gap in the design, implementation, and revision of UIs by providing a highly portable and immediately accessible visual representation of the UI that embeds with the code itself.",1,15.7142857143
UIST,b3ba47e47c8cb6a5610200c5bd24194adac56417,UIST,2016,Phyxel: Realistic Display of Shape and Appearance using Physical Objects with High-speed Pixelated Lighting,"Takatoshi Yoshida, Yoshihiro Watanabe, Masatoshi Ishikawa","3461775, 2279584, 1734807","A computer display that is sufficiently realistic such that the difference between a presented image and a real object cannot be discerned is in high demand in a wide range of fields, such as entertainment, digital signage, and design industry. To achieve such a level of reality, it is essential to reproduce the three-dimensional (3D) shape and material appearances simultaneously; however, to date, developing a display that can satisfy both conditions has been difficult. To address this problem, we propose a system that places physical elements at desired locations to create a visual image that is perceivable by the naked eye. This configuration can be realized by exploiting characteristics of human visual perception. Humans perceive light modulation as perfectly steady light if the modulation rate is sufficiently high. Therefore, if high-speed spatially varying illumination is projected to the actuated physical elements possessing various appearances at the desired timing, a realistic visual image that can be transformed dynamically by simply modifying the lighting pattern can be obtained. We call the proposed display technology Phyxel. This paper describes the proposed configuration and required performance for Phyxel. We also demonstrate three applications: dynamic stop motion, a layered 3D display, and shape mixture.",0,44.6540880503
UIST,2b063cc6b6ae711ef3a8e923bcbd841ff2a6a78d,UIST,2001,The designers' outpost: a tangible interface for collaborative web site,"Scott R. Klemmer, Mark W. Newman, Ryan Farrell, Mark Bilezikjian, James A. Landay","1728167, 4590190, 2860196, 3151811, 1708404","In our previous studies into web design, we found that pens, paper, walls, and tables were often used for explaining, developing, and communicating ideas during the early phases of design. These wall-scale paper-based design practices inspired The Designers' Outpost, a tangible user interface that combines the affordances of paper and large physical workspaces with the advantages of electronic media to support information design. With Outpost, users collaboratively author web site information architectures on an electronic whiteboard using physical media (Post-it notes and images), structuring and annotating that information with electronic pens. This interaction is enabled by a touch-sensitive SMART Board augmented with a robust computer vision system, employing a rear-mounted video camera for capturing movement and a front-mounted high-resolution camera for capturing ink. We conducted a participatory design study with fifteen professional web designers. The study validated that Outpost supports information architecture work practice, and led to our adding support for fluid transitions to other tools.",146,86.6666666667
UIST,5bc4b5d1b3578e59170ed249e8e6c4fb042effa8,UIST,2010,The engineering of personhood,Jaron Lanier,1794116,"Any subset of reality can potentially be interpreted as a computer, so when we speak about a particular computer, we are merely speaking about a portion of reality we can understand computationally. That means that computation is only identifiable through the human experience of it. User interface is ultimately the only grounding for the abstractions of computation, in the same way that the measurement of physical phenomena provides the only legitimate basis for physics. But user interface also changes humans. As computation is perceived, the natures of self and personhood are transformed. This process, when designers are aware of it, can be understood as an emerging form of applied philosophy or even applied spirituality.",0,9.3023255814
UIST,70058cebe59b4cd76c3c7adf71de45c02bd6a979,UIST,2011,Mobile multi-display environments,Jessica R. Cauchard,3126940,"Mobile devices are increasingly being fitted with more than one display, presenting a new breed of Mobile Multi-Display Environments (MMDEs). It is however still unclear how the extra display fits within the mobile devices' ecology in terms of visualisation and interaction. My research explores the alignment between multiple displays in a mobile environment and how different alignments affect usability and the choice of a suitable interaction technique. In order to investigate those properties and adapt them to various use cases, I will build a steerable projection system to study different alignments, then analyse visual separation effects in MMDEs and finally explore the possibilities offered when the displays are overlapping.",3,31.4285714286
UIST,c08f0a66545bfa62dbf8be49a22674e1db7c35e3,UIST,2010,Gilded gait: reshaping the urban experience with augmented footsteps,Yuichiro Takeuchi,1784569,"In this paper we describe Gilded Gait, a system that changes the perceived physical texture of the ground, as felt through the soles of users' feet. Ground texture, in spite of its potential as an effective channel of peripheral information display, has so far been paid little attention in HCI research. The system is designed as a pair of insoles with embedded actuators, and utilizes vibrotactile feedback to simulate the perceptions of a range of different ground textures. The discreet, low-key nature of the interface makes it particularly suited for outdoor use, and its capacity to alter how people experience the built environment may open new possibilities in urban design.",6,57.5581395349
UIST,6245c9eac2de0a8e63ea658c8fa1486ece855d8b,UIST,2016,AggreGaze: Collective Estimation of Audience Attention on Public Displays,"Yusuke Sugano, Xucong Zhang, Andreas Bulling","1751242, 2520795, 3194727","Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present <i>AggreGaze</i>, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.",2,98.427672956
UIST,029547267853c16d6d30500b80e20d34c24c2f3b,UIST,2002,Generating remote control interfaces for complex appliances,"Jeffrey Nichols, Brad A. Myers, Michael Higgins, Joseph Hughes, Thomas K. Harris, Ronald Rosenfeld, Mathilde Pignol","1687909, 1707801, 7551883, 2432357, 2252807, 4661000, 2030457","The <i>personal universal controller</i> (PUC) is an approach for improving the interfaces to complex appliances by introducing an intermediary graphical or speech interface. A PUC engages in two-way communication with everyday appliances, first downloading a specification of the appliance's functions, and then automatically creating an interface for controlling that appliance. The specification of each appliance includes a high-level description of every function, a hierarchical grouping of those functions, and dependency information, which relates the availability of each function to the appliance's state. Dependency information makes it easier for designers to create specifications and helps the automatic interface generators produce a higher quality result. We describe the architecture that supports the PUC, and the interface generators that use our specification language to build high-quality graphical and speech interfaces.",184,100.0
UIST,1ae17e45e8b32969a3127400eb6525d2ccd93c3a,UIST,1991,Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers,"Steven K. Feiner, Ari Shamash","1809403, 3137941","While virtual worlds offer a compelling alternative to conventional interfaces, the technologies these systems currently use do not provide sufficient resolution and accuracy to support detailed work such as text editing. We describe a pragmatic approach to interface design that provides users with a large virtual world in which such high-resolution work can be performed. Our approach is based on combining heterogeneous display and interaction device technologies to produce a hybrid user interface. Display and interaction technologies that have relatively low resolution, but which cover a wide (visual and interactive) field are used to form an information surround. Display and interaction technologies that have relatively high resolution over a limited visual and interaction range are used to present concentrated information in one or more selected portions of the surround. These high-resolution fields are embedded within the low-resolution surround by choosing and coordinating complementary devices that permit the user to see and interact with both simultaneously. This allows each embedded high-resolution interface to serve as a "" sweet spot "" within which intonation may be preferentially processed, We have developed a preliminary implementation, described in this paper, that uses a Reflection Technology Private Eye display and a Polhemus sensor to provide the secondary low-resohttion surround, and a flat-panel display and mouse to provide the primary high-resolution interface.",58,78.2608695652
UIST,3859c05f1ecff2dfe56db350c4e1815b46c64078,UIST,2011,FingerFlux: near-surface haptic feedback on tabletops,"Malte Weiss, Chat Wacharamanotham, Simon Voelker, Jan O. Borchers","2563842, 2706102, 2766971, 1692837","We introduce FingerFlux, an output technique to generate near-surface haptic feedback on interactive tabletops. Our system combines electromagnetic actuation with permanent magnets attached to the user's hand. FingerFlux lets users feel the interface before touching, and can create both attracting and repelling forces. This enables applications such as reducing drifting, adding physical constraints to virtual controls, and guiding the user without visual output. We show that users can feel vibration patterns up to 35 mm above our table, and that FingerFlux can significantly reduce drifting when operating on-screen buttons without looking.",35,81.4285714286
UIST,be536b10b6fed1bc9ff7ad502fb28d666bd09088,UIST,2015,Multi-Modal Peer Discussion with RichReview on edX,"Dongwook Yoon, Piotr Mitros","2055005, 1955668","In this demo, we present RichReview, a multi-modal peer discussion system, implemented as an XBlock in the edX courseware platform. The system brings richness similar to face-to-face communication into online learning at scale. With this demonstration, we discuss the system?s scalable back-end architecture, semantic voice editing user interface, and a future research plan for the profile based group-assignment scheme.",0,16.2280701754
UIST,c2669914335f46c5019a96ed9ab8c70c48e43922,UIST,2005,Personal computing in the 21st century,Gary K. Starkweather,1802790,"Ever since the dawn of the digital computer, invention, innovation, and creativity have been a hallmark of the industry. The mainframe computer seemed for a while to be the real player with experts or at least highly trained professionals operating these large and expensive machines. Most users were allowed to see them through glass windows but ""hands on"" was a rare opportunity. In 1972, the Xerox Palo Alto Research Center (PARC), built a remarkable personal computer named the ALTO. Except for the visionaries at PARC and a few others, most people considered the personal computer a mere curiosity in this early period. Today, the personal computer has become a tool that very few imagined. What might be yet to come.While prognosticating about the future is a risky endeavor at best, perhaps we can obtain a look ahead with a straightforward review of the current status of personal computing. We will look at operating systems, application software and peripherals, however, the real goal of this talk is to see what the user interface, tools and interactions with this future computing environment might be or perhaps even should be. Will we still be using continuing variations of Doug Englebart's mouse in 2020 or might something new and much more advanced emerge? How might users seamlessly deal with terabytes of storage? How might multi-user environments be used and could multi-OS machines be an economic and generally available personal computing environment? Are there user experience issues that are critical in multi-OS environments? How might the user's display be different from today? Will tomorrow's displays be larger, have a significantly higher pixel density, be much more paper-like, etc.? Might electronic printers and their requisite paper output still be with us by 2025, for example? Will home and neighborhood network resources finally be a powerful ally of the computing environment? Many exciting opportunities and questions beg for answers and industry insight.This talk will attempt to peer into the near future to see what we might expect of the personal computing environment based on what we can extrapolate from current experience and technology directions. While the exactitude of such projections may be limited, taken as a whole, there is perhaps much that can be learned from such an exercise. Why do this? Charles Kettering, the great automotive inventor was asked why he spent so much time planning and thinking about the future. He wisely replied, ""Because I am going to spend the rest of my life there."" Thirty years ago, very few could have imagined all the wonderful things that personal computing has enabled. Perhaps we have just begun our exciting journey.",0,4.83870967742
UIST,69b1e73b670951cc96e393f3a43e1021ca82ea31,UIST,2006,"RecipeSheet: creating, combining and controlling information processors","Aran Lunzer, Kasper Hornbæk","3306182, 1679367","Many tasks require users to extract information from diverse sources, to edit or process this information locally, and to explore how the end results are affected by changes in the information or in its processing. We present the RecipeSheet, a general-purpose tool for assisting users in such tasks. The RecipeSheet lets users create information processors, called recipes, which may take input in a variety of forms such as text, Web pages, or XML, and produce results in a similar variety of forms. The processing carried out by a recipe may be specified using a macro or query language, of which we currently support Rexx, Smalltalk and XQuery, or by capturing the behaviour of a Web application or Web service. In the RecipeSheet's spreadsheet-inspired user interface, information appears in cells, with inter-cell dependencies defined by recipes rather than formulas. Users can also intervene manually to control which information flows through the dependency connections. Through a series of examples we illustrate how tasks that would be challenging in existing environments are supported by the RecipeSheet.",4,7.5
UIST,014209417fcc1b0c7727414db21cf4f062abe5ce,UIST,2015,Enriching Online Classroom Communication with Collaborative Multi-Modal Annotations,Dongwook Yoon,2055005,"In massive open online courses, peer discussion is a scalable solution for offering interactive and engaging learning experiences to a large number of students. On the other hand, the quality of communication mediated through online discussion tools, such as discussion forums, is far less expressive than that of face-to-face communication. As a solution, I present RichReview, a multi-modal annotation system through which distant students can exchange ideas using versatile combinations of voice, text, and pointing gestures. A series of lab and deployment studies of RichReview promised that the expressive multimedia mixture and lightweight audio browsing feature help students better understand commentators? intention. For the large-scale deployment, I redesigned RichReview as a web applet in edX?s courseware framework. By deploying the system at scale, I will investigate (1) the optimal group assignment scheme that maximizes overall diversities of group members, (2) educational data mining applications based on user-generated rich discussion data, and (3) the impact of the rich discussion to students? retention of knowledge. Throughout these studies, I will argue that a multi-modal anchored digital document annotation system enables rich online peer discussion at scale.",0,16.2280701754
UIST,40e9a91d42fe42168499dac3bace585cd15815dc,UIST,2008,The ProD framework for proactive displays,"Ben Congleton, Mark S. Ackerman, Mark W. Newman","2554514, 1797833, 4590190","A proactive display is an application that selects content to display based on the set of users who have been detected nearby. For example, the Ticket2Talk [17] proactive display application presented content for users so that other people would know something about them.
 It is our view that promising patterns for proactive display applications have been discovered, and now we face the need for frameworks to support the range of applications that are possible in this design space.
 In this paper, we present the Proactive Display (ProD) Framework, which allows for the easy construction of proactive display applications. It allows a range of proactive display applications, including ones already in the literature. ProD also enlarges the design space of proactive display systems by allowing a variety of new applications that incorporate different views of social life and community.",5,8.57142857143
UIST,024b6bc78fb1f97e3056e3873cc47043780e220a,UIST,2010,Soylent: a word processor with a crowd inside,"Michael S. Bernstein, Greg Little, Rob Miller, Björn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, Katrina Panovich","3047089, 1715840, 1723785, 4020023, 1797833, 1743286, 8717454, 1814699","This paper introduces architectural and interaction patterns for integrating crowdsourced human contributions directly into user interfaces. We focus on writing and editing, complex endeavors that span many levels of conceptual and pragmatic activity. Authoring tools offer help with pragmatics, but for higher-level help, writers commonly turn to other people. We thus present Soylent, a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and otherwise edit parts of their documents on demand. To improve worker quality, we introduce the Find-Fix-Verify crowd programming pattern, which splits tasks into a series of generation and review stages. Evaluation studies demonstrate the feasibility of crowdsourced editing and investigate questions of reliability, cost, wait time, and work time for edits.",372,100.0
UIST,e816e54795e1a20fa606769ebb88db49578eba39,UIST,1991,Interactive graph layout,"Tyson R. Henry, Scott E. Hudson","1949625, 1749296","This paper presents a novel methodology for viewing large graphs. The basic concept is to allow the user to interactively navigate through large graphs learning about them in appropriately small and concise pieces. An architecture is present to support graph exploration. It contains methods for building custom layout algorithms hierarchically, interactively decomposing large graphs, and creating interactive parameterized layout algorithms. As a proof of concept, examples are drawn from a working prototype that incorporates this methodology. 1 Introduction Directed and undirected graphs provide a natural notation for describing many fundamental structures of computer science. Data base schema [2], for example, can be described using connected graphs. Visual programming notations [13] rely heavily on graphs—viewing a visual program is a similar problem to viewing general graphs. Hypertext navigation [5] can also be modeled with connected graphs. Unfortunately graphs are hard to lay out in an easy to read fashion. Traditional Graph layout algorithms have tried to increase the readability of graphs by focusing on the task of minimizing specific fixed properties. The following list contains some common graph aspects traditional algorithms focus upon [3]. q Maximize display symmetry q Avoid edge crossings q Avoid bends in edges q Keep edge lengths uniform q Distribute vertices uniformly",57,73.9130434783
UIST,779726c4a787b3ac47ff991f964a20c9fb58531f,UIST,2000,ToolStone: effective use of the physical manipulation vocabularies of input devices,"Jun Rekimoto, Eduardo Sciammarella","1685962, 2932618","The ToolStone is a cordless, multiple degree-of-freedom (MDOF) input device that senses physical manipulation of itself, such as rotating, flipping, or tilting. As an input device for the non-dominant hand when a bimanual interface is used, the ToolStone provides several interaction techniques including a toolpalette selector, and MDOF interactors such as zooming, 3D rotation, and virtual camera control. In this paper, we discuss the design principles of input devices that effectively use a human's physical manipulation skills, and describe the system architecture and applications of the Tool-Stone input device.",46,48.0
UIST,38cb6b750bafead806a68ee6918527a55bb7495b,UIST,2013,Traxion: a tactile interaction device with virtual force sensation,Jun Rekimoto,1685962,"This paper introduces a new mechanism to induce a virtual force based on human illusory sensations. An asymmetric signal is applied to a tactile actuator consisting of an electromagnetic coil, a metal weight, and a spring, such that the user feels that the device is being pulled (or pushed) in a particular direction, although it is not supported by any mechanical connection to other objects or the ground. The proposed tactile device is smaller (35.0 mm x 5.0 mm x 7.5 mm) and lighter (5.2 g) than any previous force-feedback devices, which have to be connected to the ground with mechanical links. This small form factor allows the device to be implemented in several novel interactive applications, such as a pedestrian navigation system that includes a finger-mounted tactile device or an (untethered) input device that features virtual force. Our experimental results indicate that this illusory sensation actually exists and the proposed device can switch the virtual force direction within a short period. We combined this new technology with visible light transmission via a digital micromirror device (DMD) projector and developed a position guiding input device with force perception.",24,87.1559633028
UIST,46ccd6dcd5ceb7ac006764b73820d6cd22b4b74a,UIST,2003,SmartMusicKIOSK: music listening station with chorus-search function,Masataka Goto,1720652,"This paper describes a new music-playback interface for trial listening, <i>SmartMusicKIOSK</i>. In music stores, short trial listening of CD music is not usually a passive experience -- customers often search out the chorus or ""hook"" of a song using the fast-forward button. Listening of this type, however, has not been traditionally supported. This research achieves a function for jumping to the chorus section and other key parts of a song plus a function for visualizing song structure. These functions make it easier for a listener to find desired parts of a song and thereby facilitate an active listening experience. The proposed functions are achieved by an automatic chorus-section detecting method, and the results of implementing them as a listening station have demonstrated their usefulness.",36,35.4166666667
UIST,8e2c15f0b3a80754b97bc02795515719e0f240f3,UIST,2008,Kinematic templates: end-user tools for content-relative cursor manipulations,"Richard Fung, Edward Lank, Michael A. Terry, Celine Latulipe","2650779, 1788496, 1740154, 1683753","This paper introduces kinematic templates, an end-user tool for defining content-specific motor space manipulations in the context of editing 2D visual compositions. As an example, a user can choose the ""sandpaper"" template to define areas within a drawing where cursor movement should slow down. Our current implementation provides templates that amplify or dampen the cursor's speed, attenuate jitter in a user's movement, guide movement along paths, and add forces to the cursor. Multiple kinematic templates can be defined within a document, with overlapping templates resulting in a form of function composition. A template's strength can also be varied, enabling one to improve one's strokes without losing the human element. Since kinematic templates guide movements, rather than strictly prescribe them, they constitute a visual composition aid that lies between unaided freehand drawing and rigid drawing aids such as snapping guides, masks, and perfect geometric primitives.",11,25.7142857143
UIST,75c707d81e9526135c2365bce1963a3cf5e0274c,UIST,2000,Dynamic space management for user interfaces,"Blaine Bell, Steven K. Feiner","1926106, 1809403","We present a general approach to the dynamic representation of 2D space that is well suited for user-interface layout. We partition space into two distinct categories: full and empty. The user can explicitly specify a set of possibly overlapping upright rectangles that represent the objects of interest. These full-space rectangles are processed by the system to create a representation of the remaining empty space. This representation makes it easy for users to develop customized spatial allocation strategies that avoid overlapping the full-space rectangles. We describe the representation; provide efficient incremental algorithms for adding and deleting full-space rectangles, and for querying the empty-space representation; and show several allocation strategies that the representation makes possible. We present two testbed applications that incorporate an implementation of the algorithm; one shows the utility of our representation for window management tasks; the other applies it to the layout of components in a 3D user interface, based on the upright 2D bounding boxes of their projections.",59,52.0
UIST,2ad79e7b4fc5cecbd24b48407af10616baaa18d4,UIST,2009,Ripples: utilizing per-contact visualizations to improve user interaction with touch displays,"Daniel J. Wigdor, Sarah Williams, Michael Cronin, Robert Levy, Katie White, Maxim Mazeev, Hrvoje Benko","1961958, 5680301, 3135832, 8742448, 2933881, 2508975, 2704133","We present <i>Ripples</i>, a system which enables visualizations around each contact point on a touch display and, through these visualizations, provides feedback to the user about successes and errors of their touch interactions. Our visualization system is engineered to be overlaid on top of existing applications without requiring the applications to be modified in any way, and functions independently of the application's responses to user input. Ripples reduces the fundamental problem of ambiguity of feedback when an action results in an unexpected behaviour. This ambiguity can be caused by a wide variety of sources. We describe the ambiguity problem, and identify those sources. We then define a set of visual states and transitions needed to resolve this ambiguity, of use to anyone designing touch applications or systems. We then present the Ripples implementation of visualizations for those states, and the results of a user study demonstrating user preference for the system, and demonstrating its utility in reducing errors.",28,48.5714285714
UIST,bb66beca15ffd0461ab9e1f7b26f3f71a5aee750,UIST,2014,"High rate, low-latency multi-touch sensing with simultaneous orthogonal multiplexing","Darren Leigh, Clifton Forlines, Ricardo Jota, Steven Sanders, Daniel J. Wigdor","1786108, 1694854, 1766253, 2797538, 1961958","We present ""Fast Multi-Touch"" (FMT), an extremely high frame rate and low-latency multi-touch sensor based on a novel projected capacitive architecture that employs simultaneous orthogonal signals. The sensor has a frame rate of 4000 Hz and a touch-to-data output latency of only 40 microseconds, providing unprecedented responsiveness. FMT is demonstrated with a high-speed DLP projector yielding a touch-to-light latency of 110 microseconds.",6,67.8294573643
UIST,cda05d4e576e52d7dc9e7305a674390b388aea71,UIST,2007,Measuring how design changes cognition at work,David Woods,7140450,"The various fields associated with interactive software systems engage in design activities to enable people who would use the resulting systems to meet goals, coordinate with others, find meaning, and express themselves in myriad ways. Yet many development projects fail, and we all have contact with clumsy software-based systems that force work-arounds and impose substantial attentional, knowledge and workload burdens. On the other hand, field observations reveal people re-shaping the artifacts they encounter and interact with as resources to cope with the demands of the situations they face as they seek to meet their goals. In this process some new devices are quickly seized upon and exploited in ways that transform the nature of human activity, connections, and expression.
 The software intensive interactive systems and devices under development around us are valuable to the degree that they expand what people in various roles and organizations can achieve. How can we measure this value provided to others? Are current measures of usability adequate? Does creeping complexity wipe out incremental gains as products evolve? Do designers and developers mis-project the impact when systems-to-be-realized are fielded? Which technology changes will trigger waves of expansive adaptations that transform what people do and even why they do it.
 Sponsors of projects to develop new interactive software systems are asking developers for tangible evidence of the value to be delivered to those people responsible for activities and goals in the world. Traditional measures of usability and human performance seem inadequate. Cycles of inflation in the claims development organizations make (and the legacy of disappointment and surprise) have left sponsors numb and eroded trust. Thus, we need to provide new forms of evidence about the potential of new interactive systems and devices to enhance human capability.
 Luckily, this need has been accompanied by a period of innovation in ways to measure the impact of new designs on: <ul><li>growth of expertise in roles,</li><li>synchronizing activities over wider scopes and ranges,</li><li>expanding adaptive capacities.</li></ul>.
 This talk reviews a few of the new measures being tested in each of these categories, points to some of the underlying science, and uses these examples to trigger discussion about how design of future interactive software provides will provide value to stakeholders.",0,4.16666666667
UIST,0d18be4744209b937142bb82b1dbc8570137856d,UIST,2013,An assembly of soft actuators for an organic user interface,"Yoshiharu Ooide, Hiroki Kawaguchi, Takuya Nojima","2810245, 2157322, 1797797","An organic user interface (OUI) is a kind of interface that is based on natural human-human and human-physical object interaction models. In such situations, hair and fur play important roles in establishing smooth and natural communication. Animals and birds use their hair, fur and feathers to express their emotions, and groom each other when forming closer relationships. Therefore, hair and fur are potential materials for development of the ideal OUI. In this research, we propose the hairlytop interface, which is a collection of hair-like units composed of shape memory alloys, for use as an OUI. The proposed interface is capable of improving its spatial resolution and can be used to develop a hair surface on any electrical device shape.",4,51.8348623853
UIST,1b142c912e79a4698efed5af5c20c042edd972d2,UIST,2002,Mediated voice communication via mobile IP,"Chris Schmandt, Jang Kim, Kwan Lee, Gerardo Vallejo, Mark S. Ackerman","1729321, 2403869, 4614950, 2956505, 1797833","Impromptu is a mobile audio device which uses wireless Internet Protocol (IP) to access novel computer-mediated voice communication channels. These channels show the richness of IP-based communication as compared to conventional mobile telephony, adding audio processing and storage in the network, and flexible, user-centered call control protocols. These channels may be synchronous, asynchronous, or event-triggered, or even change modes as a function of other user activity. The demands of these modes plus the need to navigate with an entirely non-visual user interface are met with a number of audio-oriented user interaction techniques.",24,25.0
UIST,b0a3da24fa63400a0888f494eddbb9af5791152b,UIST,2007,OPA browser: a web browser for cellular phone users,"Yuki Arase, Takahiro Hara, Toshiaki Uemukai, Shojiro Nishio","3043844, 1697569, 1765553, 1717916","Cellular phones are widely used to access the WWW. However, most available Web pages are designed for desktop PCs. Cellular phones only have small screens and poor interfaces, and thus, it is inconvenient to browse such large sized pages. In addition, cellular phone users browse Web pages in various situations, so that appropriate presentation styles for Web pages depend on users' situations. In this paper, we propose a novel Web browsing system for cellular phones that allocates various functions for Web browsing on each numerical key of a cellular phone. Users can browse Web pages comfortably, selecting appropriate functions according to their situations by pushing a single button.",4,13.8888888889
UIST,f2d57dd89c04534ca5034fe56cf51da3853ecd20,UIST,2015,Extreme Computational Photography,Ramesh Raskar,1717566,"The Camera Culture Group at the MIT Media Lab aims to create a new class of imaging platforms. This talk will discuss three tracks of research: femto photography, retinal imaging, and 3D displays. Femto Photography consists of femtosecond laser illumination, picosecond-accurate detectors and mathematical reconstruction techniques allowing researchers to visualize propagation of light. Direct recording of reflected or scattered light at such a frame rate with sufficient brightness is nearly impossible. Using an indirect 'stroboscopic' method that records millions of repeated measurements by careful scanning in time and viewpoints we can rearrange the data to create a 'movie' of a nanosecond long event. Femto photography and a new generation of nano-photography (using ToF cameras) allow powerful inference with computer vision in presence of scattering. EyeNetra is a mobile phone attachment that allows users to test their own eyesight. The device reveals corrective measures thus bringing vision to billions of people who would not have had access otherwise. Another project, eyeMITRA, is a mobile retinal imaging solution that brings retinal exams to the realm of routine care, by lowering the cost of the imaging device to a 10th of its current cost and integrating the device with image analysis software and predictive analytics. This provides early detection of Diabetic Retinopathy that can change the arc of growth of the world's largest cause of blindness. Finally the talk will describe novel lightfield cameras and lightfield displays that require a compressive optical architecture to deal with high bandwidth requirements of 4D signals",0,16.2280701754
UIST,dbef713bbc3323583c9d4632aa2bf12965472b35,UIST,2015,Workload Assessment with eye Movement Monitoring Aided by Non-invasive and Unobtrusive Micro-fabricated Optical Sensors,"Carlos Cesar Cortes Torres, Kota Sampei, Munehiko Sato, Ramesh Raskar, Norihisa Miki","3381895, 7337990, 2131343, 1717566, 1692749","Mental state or workload of a person are very relevant when the person is executing delicate tasks such as piloting an aircraft, operating a crane because the high level of workload could prevent accomplishing the task and lead to disastrous results. Some frameworks have been developed to assess the workload and determine whether the person is capable of executing a new task. However, such methodologies are applied when the operator finished the task. Another feature that these methodologies share is that are based on paper and pencil tests. Therefore, human-friendly devices that could assess the workload in real time are in high demand. In this paper, we report a wearable device that can correlate physical eye behavior with the mental state for the workload assessment.",0,16.2280701754
UIST,3427ca324efdf2caceff23368d30ee1f42dbde28,UIST,2011,"ReVision: automated classification, analysis and redesign of chart images","Manolis Savva, Nicholas Kong, Arti Chhajta, Li Fei-Fei, Maneesh Agrawala, Jeffrey Heer","2531433, 2892333, 1933851, 3216322, 1820412, 1803140","Poorly designed charts are prevalent in reports, magazines, books and on the Web. Most of these charts are only available as bitmap images; without access to the underlying data it is prohibitively difficult for viewers to create more effective visual representations. In response we present ReVision, a system that automatically redesigns visualizations to improve graphical perception. Given a bitmap image of a chart as input, ReVision applies computer vision and machine learning techniques to identify the chart type (e.g., pie chart, bar chart, scatterplot, etc.). It then extracts the graphical marks and infers the underlying data. Using a corpus of images drawn from the web, ReVision achieves image classification accuracy of 96% across ten chart categories. It also accurately extracts marks from 79% of bar charts and 62% of pie charts, and from these charts it successfully extracts data from 71% of bar charts and 64% of pie charts. ReVision then applies perceptually-based design principles to populate an interactive gallery of redesigned charts. With this interface, users can view alternative chart designs and retarget content to different visual styles.",33,80.0
UIST,433e85bcfb2ff307d756624ae910bb800e9f48cf,UIST,2012,DuploTrack: a real-time system for authoring and guiding duplo block assembly,"Ankit Gupta, Dieter Fox, Brian Curless, Michael F. Cohen","1831230, 1776234, 1810052, 1694613","We demonstrate a realtime system which infers and tracks the assembly process of a snap-together block model using a Kinect&#174; sensor. The inference enables us to build a virtual replica of the model at every step. Tracking enables us to provide context specific visual feedback on a screen by augmenting the rendered virtual model aligned with the physical model. The system allows users to author a new model and uses the inferred assembly process to guide its recreation by others. We propose a novel way of assembly guidance where the next block to be added is rendered in blinking mode with the tracked virtual model on screen. The system is also able to detect any mistakes made and helps correct them by providing appropriate feedback. We focus on assemblies of Duplo&#174; blocks.
 We discuss the shortcomings of existing methods of guidance - static figures or recorded videos - and demonstrate how our method avoids those shortcomings. We also report on a user study to compare our system with standard figure-based guidance methods found in user manuals. The results of the user study suggest that our method is able to aid users' structural perception of the model better, leads to fewer assembly errors, and reduces model construction time.",26,84.3137254902
UIST,69395e8a46b53ea30eacc1b8b5acbac3b894163e,UIST,2013,Haptic feedback design for a virtual button along force-displacement curves,"Sunjun Kim, Geehyuk Lee","8364126, 1717371","In this paper, we present a haptic feedback method for a virtual button based on the force-displacement curves of a physical button. The original feature of the proposed method is that it provides haptic feedback, not only for the ""click"" sensation but also for the moving sensation before and after transition points in a force-displacement curve. The haptic feedback is by vibrotactile stimulations only and does not require a force feedback mechanism. We conducted user experiments to show that the resultant haptic feedback is realistic and distinctive. Participants were able to distinguish among six different virtual buttons, with 94.1% accuracy even in a noisy environment. In addition, participants were able to associate four virtual buttons with their physical counterparts, with a correct answer rate of 79.2%.",3,45.871559633
UIST,9f6336ee2b5113110312d7c61b9cfc6ec08af273,UIST,2014,Leveraging physical human actions in large interaction spaces,Can Liu,4107619,"Large interaction spaces such as wall-size displays allow users to interact not only with their hands, like traditional desktop environment, but also with their whole body by, e.g. walking or moving their head orientation. While this is particularly suitable for tasks where users need to navigate large amounts of data and manipulate them at the same time, we still lack a deep understanding of the advantages of large displays for such tasks. My dissertation begins with a set of studies to understand the benefits and drawbacks of a high-resolution wall-size display vs. a desktop environments. The results show strong benefits of the former due to the flexibility of ""physical navigation"" involving the whole body when compared with mouse input. From whole-body interaction to human-to-human interaction, my current work seeks to leverage natural human actions to collaborative contexts and to design interaction techniques that detects gestural interactions between users to support collaborative data exchange.",0,12.015503876
UIST,d5c42010727b7676aac1e6f5885e642089b0c5a9,UIST,2013,Ta-Tap: consecutive distant tap operations for one-handed touch screen use,"Seongkook Heo, Geehyuk Lee","1724588, 1717371","Tapping on the same point twice is a common operation known as double tap, but tapping on distant points in sequence is underutilized. In this poster we explore the potential uses of consecutive distant tap operations, which we call Ta-Tap. As a single-touch operation, it is expected to be particularly useful for single-handed touch screen use. We examined three possible uses of Ta-Tap: simulating multi-touch operations, invoking a virtual scroll wheel, and invoking a pie-menu. We verified the feasibility of Ta-Tap through the experiment.",0,10.5504587156
UIST,ad7edfa6da9ca82f203e3050d42434137d03e8e7,UIST,2012,Restorable backspace,"Sunjun Kim, Geehyuk Lee","8364126, 1717371","This paper presents Restorable Backspace, an input helper for mistyping correction. It stores characters deleted by backspace keystrokes, and restores them in the retyping phase. We developed Restoration algorithm that compares deleted characters and retyped characters, and makes a suggestion while retyping. In a pilot study we could observe the algorithm work as expected for most of the cases. All participants in the pilot study showed satisfaction about the concept of Restorable Backspace.",1,25.0
UIST,65c50bb806041b0136cf9d27fd28f0657b84c1c4,UIST,2000,System lag tests for augmented and virtual environments,"Colin Swindells, John Dill, Kellogg S. Booth","1794909, 1727262, 1800617","We describe a simple technique for accurately calibrating the temporal lag in augmented and virtual environments within the Enhanced Virtual Hand Lab (EVHL), a collection of hardware and software to support research on goal-directed human hand motion. Lag is the sum of various delays in the data pipeline associated with sensing, processing, and displaying information from the physical world to produce an augmented or virtual world. Our main calibration technique uses a modified phonograph turntable to provide easily tracked periodic motion, reminiscent of the pendulum-based calibration technique of Liang, Shaw and Green. Measurements show a three-frame (50 ms) lag for the EVHL. A second technique, which uses a specialized analog sensor that is part of the EVHL, provides a "" closed loop "" calibration capable of sub-frame accuracy. Knowing the lag to sub-frame accuracy enables a predictive tracking scheme to compensate for the end-to-end lag in the data pipeline. We describe both techniques and the EVHL environment in which they are used.",21,28.0
UIST,a1fad43168ff2e3a272a827784b3483e8047f02e,UIST,2011,Force gestures: augmenting touch screen gestures with normal and tangential forces,"Seongkook Heo, Geehyuk Lee","1724588, 1717371","Force gestures are touch screen gestures augmented by the normal and tangential forces on the screen. In order to study the feasibility of the force gestures on a mobile touch screen, we implemented a prototype touch screen device that can sense the normal and tangential forces of a touch gesture on the screen. We also designed two example applications, a web browser and an e-book reader, that utilize the force gestures for their primary actions. We conducted a user study with the prototype and the applications to study the characteristics of the force gestures and the effectiveness of their mapping to the primary actions. In the user study we could also discover interesting usability issues and collect useful user feedback about the force gestures and their mapping to GUI actions.",25,73.3333333333
UIST,03f3daa51e04096c68e4eccc7e42c8c379f826ca,UIST,2011,Visual separation in mobile multi-display environments,"Jessica R. Cauchard, Markus Löchtefeld, Pourang Irani, Johannes Schöning, Antonio Krüger, Mike Fraser, Sriram Subramanian","3126940, 1808720, 1773923, 2070910, 1790548, 1725837, 1702794","Projector phones, handheld game consoles and many other mobile devices increasingly include more than one display, and therefore present a new breed of mobile Multi-Display Environments (MDEs) to users. Existing studies illustrate the effects of visual separation between displays in MDEs and suggest interaction techniques that mitigate these effects. Currently, mobile devices with heterogeneous displays such as projector phones are often designed without reference to visual separation issues; therefore it is critical to establish whether concerns and opportunities raised in the existing MDE literature apply to the emerging category of Mobile MDEs (MMDEs). This paper investigates the effects of visual separation in the context of MMDEs and contrasts these with fixed MDE results, and explores design factors for Mobile MDEs. Our study uses a novel eye-tracking methodology for measuring switches in visual context between displays and identifies that MMDEs offer increased design flexibility over traditional MDEs in terms of visual separation. We discuss these results and identify several design implications.",31,77.1428571429
UIST,6245459121aaf27ae623de0e8a12317f49ee1348,UIST,2013,Chorus: a crowd-powered conversational assistant,"Walter S. Lasecki, Rachel Wesley, Jeffrey Nichols, Anand Kulkarni, James F. Allen, Jeffrey P. Bigham","2598433, 5982068, 1687909, 5618063, 1749025, 1744846","Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93% of user queries appropriately, and staying on-topic in over 95% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems.",40,98.1651376147
UIST,4b26cb303b61e6d3f5cf68147331e4a24a0045f9,UIST,2013,FingerSkate: making multi-touch operations less constrained and more continuous,"Jeongmin Son, Geehyuk Lee","2167409, 1717371","Multi-touch operations are sometimes difficult to perform due to musculoskeletal constraints. We propose FingerSkate, a variation to the current multi-touch operations to make them less constrained and more continuous. With FingerSkate, once one starts a multi-touch operation, one can continue the operation without having to maintain both fingers on the screen. In a pilot study, we observe that participants could learn to FingerSkate easily and were utilizing the new technique actively.",0,10.5504587156
UIST,29b42badaf4c8ac3124eb9c191aeb7135634feb8,UIST,2013,Authoring multi-stage code examples with editable code histories,"Shiry Ginosar, Luis Fernando De Pombo, Maneesh Agrawala, Björn Hartmann","2361255, 2547998, 1820412, 4020023","Multi-stage code examples present multiple versions of a program where each stage increases the overall complexity of the code. In order to acquire strategies of program construction using a new language or API, programmers consult multi-stage code examples in books, tutorials and online videos. Authoring multi-stage code examples is currently a tedious process, as it involves keeping several stages of code synchronized in the face of edits and error corrections. We document these difficulties with a formative study examining how programmers author multi-stage code examples. We then present an IDE extension that helps authors create multi-stage code examples by propagating changes (insertions, deletions and modifications) to multiple saved versions of their code. Our system adapts revision control algorithms to the specific task of evolving example code. An informal evaluation finds that taking snapshots of a program as it is being developed and editing these snapshots in hindsight help users in creating multi-stage code examples.",3,45.871559633
UIST,4c59e4e662768c6a2f324a6c72c02489cad5165c,UIST,2015,Form Follows Function(): An IDE to Create Laser-cut Interfaces and Microcontroller Programs from Single Code Base,"Jun Kato, Masataka Goto","3200463, 1720652","During the development of physical computing devices, physical object models and programs for microcontrollers are usually created with separate tools with distinct files. As a result, it is difficult to track the changes in hardware and software without discrepancy. Moreover, the software cannot directly access hardware metrics. Designing hardware interface cannot benefit from the source code information either. This demonstration proposes a browser-based IDE named f3.js that enables development of both as a single JavaScript code base. The demonstration allows audiences to play with the f3.js IDE and showcases example applications such as laser-cut interfaces generated from the same code but with different parameters. Programmers can experience the full feature and designers can interact with preset projects with a mouse or touch to customize laser-cut interfaces. More information is available at http://f3js.org.",0,16.2280701754
UIST,85647141d1bb4ec410e87ad2bc15c020cbb66e2e,UIST,2014,SenseGlass: using google glass to sense daily emotions,"Javier Hernandez, Rosalind W. Picard","2057598, 1719389","For over a century, scientists have studied human emotions in laboratory settings. However, these emotions have been largely contrived -- elicited by movies or fake ""lab"" stimuli, which tend not to matter to the participants in the studies, at least not compared with events in their real life. This work explores the utility of Google Glass, a head-mounted wearable device, to enable fundamental advances in the creation of affect-based user interfaces in natural settings.",6,67.8294573643
UIST,c23e34ff6930e4fba82e0dd0361a9f5f37f482cc,UIST,2013,Mirage: exploring interaction modalities using off-body static electric field sensing,"Adiyan Mujibiya, Jun Rekimoto","2845050, 1685962","Mirage proposes an effective non body contact technique to infer the amount and type of body motion, gesture, and activity. This approach involves passive measurement of static electric field of the environment flowing through sense electrode. This sensing method leverages electric field distortion by the presence of an intruder (e.g. human body). Mirage sensor has simple analog circuitry and supports ultra-low power operation. It requires no instrumentation to the user, and can be configured as environmental, mobile, and peripheral-attached sensor. We report on a series of experiments with 10 participants showing robust activity and gesture recognition, as well as promising results for robust location classification and multiple user differentiation. To further illustrate the utility of our approach, we demonstrate real-time interactive applications including activity monitoring, and two games which allow the users to interact with a computer using body motion and gestures.",6,59.1743119266
UIST,8ab9fd09004475a5826a178694c888578d76ab5d,UIST,2016,waveSense: Ultra Low Power Gesture Sensing Based on Selective Volumetric Illumination,"Anusha Indrajith Withana, Shanaka Ransiri, Tharindu Kaluarachchi, Chanaka Singhabahu, Yilei Shi, Samitha Elvitigala, Suranga Nanayakkara","3209849, 2681009, 3493118, 3492339, 1809242, 3493258, 1744107","We present <i>waveSense</i>, a low power hand gestures recogni- tion system suitable for mobile and wearable devices. A novel <i>Selective Volumetric Illumination</i> (SVI) approach using off-the-shelf infrared (IR) emitters and non-focused IR sensors were introduced to achieve the power efficiency. Our current implementation consumes <i>8.65mW</i> while sensing hand gestures within <i>60cm</i> radius from the sensors. In this demo, we introduce the concept and the theoretical background of <i>waveSense</i>, details of the prototype implementation, and application possibilities.",0,44.6540880503
UIST,5825f92e906ba83fc9fdb869e07d27d3bc395b7b,UIST,2016,Touchscreen Overlay Augmented with the Stick-Slip Phenomenon to Generate Kinetic Energy,"Ahmed Farooq, Philipp Weitz, Grigori E. Evreinov, Roope Raisamo, Daisuke Takahata","7950260, 3492917, 1745191, 1749337, 3493191","Kinesthetic feedback requires linkage-based high-powered multi-dimensional manipulators, which are currently not possible to integrate with mobile devices. To overcome this challenge, we developed a novel system that can utilize a wide range of actuation components and apply various techniques to optimize stick-slip motion of a tangible object on a display surface. The current setup demonstrates how it may be possible to generate directional forces on an interactive display in order to move a linkage-free stylus over a touchscreen in a fully controlled and efficient manner. The technology described in this research opens up new possibilities for interacting with displays and tangible surfaces such as continuously supervised learning; active feed-forward systems as well as dynamic gaming environments that predict user behavior and are able modify and physically react to human input at real-time.",0,44.6540880503
UIST,229b6e999fba94d7b8dbab125a5b3769c565c515,UIST,2009,TapSongs: tapping rhythm-based passwords on a single binary sensor,Jacob O. Wobbrock,1796045,"TapSongs are presented, which enable user authentication on a single ""binary"" sensor (e.g., button) by matching the rhythm of tap down/up events to a jingle timing model created by the user. We describe our matching algorithm, which employs absolute match criteria and learns from successful logins. We also present a study of 10 subjects showing that after they created their own TapSong models from 12 examples (< 2 minutes), their subsequent login attempts were 83.2% successful. Furthermore, aural and visual eavesdropping of the experimenter's logins resulted in only 10.7% successful imposter logins by subjects. Even when subjects heard the target jingles played by a synthesized piano, they were only 19.4% successful logging in as imposters. These results are attributable to subtle but reliable individual differences in people's tapping, which are supported by prior findings in music psychology.",19,34.2857142857
UIST,6e6a45d5188205d488c805e212157e9c1370316f,UIST,2012,MISO: a context-sensitive multimodal interface for smart objects based on hand gestures and finger snaps,"David Fleer, Christian Leichsenring","3173359, 3047713",We present an unobtrusive multimodal interface for smart objects (MISO) in an everyday indoor environment. MISO uses pointing for object selection and context-sensitive arm gestures for object control. Finger snaps are used to confirm object selections and to aid with gesture segmentation. Audio feedback is provided during the interaction. The use of a Kinect depth camera allows for a compact system and robustness in varying environments and lighting conditions at low cost.,3,40.1960784314
UIST,2ddd4c68bcfa89cdc35ceed29fc5243995dc6350,UIST,2014,AttachMate: highlight extraction from email attachments,"Joshua M. Hailpern, Sitaram Asur, Kyle Rector","2837900, 1806968, 1750804","While email is a major conduit for information sharing in enterprise, there has been little work on exploring the files sent along with these messages -- attachments. These accompanying documents can be large (multiple megabytes), lengthy (multiple pages), and not optimized for the smaller screen sizes, limited reading time, and expensive bandwidth of mobile users. Thus, attachments can increase data storage costs (for both end users and email servers), drain users' time when irrelevant, cause important information to be missed when ignored, and pose a serious access issue for mobile users. To address these problems we created AttachMate, a novel email attachment summarization system. AttachMate can summarize the content of email attachments and automatically insert the summary into the text of the email. AttachMate also stores all files in the cloud, reducing file storage costs and bandwidth consumption. In this paper, the primary contribution is the AttachMate client/server architecture. To ground, support and validate the AttachMate system we present two upfront studies (813 participants) to understand the state and limitations of attachments, a novel algorithm to extract representative concept sentences (tested through two validation studies), and a user study of AttachMate within an enterprise.",2,41.4728682171
UIST,98b6c4ca39ce9fc8de0b92f7c18ba8cc812acf97,UIST,2014,Creating interactive web data applications with spreadsheets,"Kerry Shih-Ping Chang, Brad A. Myers","1750695, 1707801","While more and more data are available through web services, it remains difficult for end-users to create web applications that make use of these data without having to write complex code. We present Gneiss, a live programming environment that extends the spreadsheet metaphor to support creating interactive web applications that dynamically use local or web data from multiple sources. Gneiss closely integrates a spreadsheet editor with a web interface builder to let users demonstrate bindings between properties of web GUI elements and cells in the spreadsheet while working with real web service data. The spreadsheet editor provides two-way connections to web services, to both visualize and retrieve different data based on the user input in the web interface. Gneiss achieves rich interactivity without the need for event-based programming by extending the 'pull model' of formulas that is familiar to the spreadsheet users. We use a series of examples to demonstrate Gneiss's ability to create a variety of interactive web data applications.",12,84.496124031
UIST,7925d49dfae7e062d6cf39416a0c3105dd2414c6,UIST,2015,Foldio: Digital Fabrication of Interactive and Shape-Changing Objects With Foldable Printed Electronics,"Simon Olberding, Sergio Soto Ortega, Klaus Hildebrandt, Jürgen Steimle","3186111, 2624287, 2167599, 1790324","Foldios are foldable interactive objects with embedded input sensing and output capabilities. Foldios combine the advantages of folding for thin, lightweight and shape-changing objects with the strengths of thin-film printed electronics for embedded sensing and output. To enable designers and end-users to create highly custom interactive foldable objects, we contribute a new design and fabrication approach. It makes it possible to design the foldable object in a standard 3D environment and to easily add interactive high-level controls, eliminating the need to manually design a fold pattern and low-level circuits for printed electronics. Second, we contribute a set of printable user interface controls for touch input and display output on folded objects. Moreover, we contribute controls for sensing and actuation of shape-changeable objects. We demonstrate the versatility of the approach with a variety of interactive objects that have been fabricated with this framework.",9,94.298245614
UIST,91f15e7370c4ff401bc562e1793112a00fbe944b,UIST,2015,GravitySpot: Guiding Users in Front of Public Displays Using On-Screen Visual Cues,"Florian Alt, Andreas Bulling, Gino Gravanis, Daniel Buschek","1693046, 3194727, 2656192, 1768653","Users tend to position themselves in front of interactive public displays in such a way as to best perceive its content. Currently, this sweet spot is implicitly defined by display properties, content, the input modality, as well as space constraints in front of the display. We present <i>GravitySpot</i> - an approach that makes sweet spots flexible by actively guiding users to arbitrary target positions in front of displays using visual cues. Such guidance is beneficial, for example, if a particular input technology only works at a specific distance or if users should be guided towards a non-crowded area of a large display. In two controlled lab studies (n=29) we evaluate different visual cues based on color, shape, and motion, as well as position-to-cue mapping functions. We show that both the visual cues and mapping functions allow for fine-grained control over positioning speed and accuracy. Findings are complemented by observations from a 3-month real-world deployment.",8,90.350877193
UIST,f977b335b440a58dd430eaa273fc34166c093e57,UIST,2000,Dual touch: a two-handed interface for pen-based PDAs,"Nobuyuki Matsushita, Yuji Ayatsuka, Jun Rekimoto","2154456, 2446448, 1685962","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT A new interaction technique called Dual Touch has been developed for pen-based PDAs. It enables a user to operate a PDA by tapping and stroking on the screen with a pen and a thumb. The PDA can detect the combined movements of two points on its pressure-based touch-screen without additional hardware. The user can use the thumb to support the task of the pen. INTRODUCTION The most significant difference between desktop computers and personal digital assistants (PDAs) is not the computation power, but the size of them. PDAs are made small so that they can be held in the hand or placed in a pocket. Therefore there is limited space for interaction. The commonly used input mechanism for PDAs is a pen and a touch panel, but it provides less capability than a keyboard and a mouse used for desk-top computers. The main cause of the problem is that users interact with PDAs using only a single pen.",25,32.0
UIST,640689c3fc0937e5f44b568455ecb9d38923dd84,UIST,2012,Steerable augmented reality with the beamatron,"Andrew Wilson, Hrvoje Benko, Shahram Izadi, Otmar Hilliges","1792265, 2704133, 1699068, 2531379","<i>Steerable displays</i> use a motorized platform to orient a projector to display graphics at any point in the room. Often a camera is included to recognize markers and other objects, as well as user gestures in the display volume. Such systems can be used to superimpose graphics onto the real world, and so are useful in a number of augmented reality and ubiquitous computing scenarios. We contribute the Beamatron, which advances steerable displays by drawing on recent progress in depth camera-based interactions. The Beamatron consists of a computer-controlled pan and tilt platform on which is mounted a projector and Microsoft Kinect sensor. While much previous work with steerable displays deals primarily with projecting corrected graphics onto a discrete set of static planes, we describe computational techniques that enable reasoning in 3D using live depth data. We show two example applications that are enabled by the unique capabilities of the Beamatron: an augmented reality game in which a player can drive a virtual toy car around a room, and a ubiquitous computing demo that uses speech and gesture to move projected graphics throughout the room.",37,89.7058823529
UIST,d7df5607d9cc15fb8657a50104650def6b735fc8,UIST,2015,From Papercraft to Paper Mechatronics: Exploring a New Medium and Developing a Computational Design Tool,Hyunjoo Oh,3036652,"Paper Mechatronics is a novel interdisciplinary design medium, enabled by recent advances in craft technologies: the term refers to a reappraisal of traditional papercraft in combination with accessible mechanical, electronic, and computational elements. I am investigating the design space of paper mechatronics as a new hands-on medium by developing a series of examples and building a computational tool, FoldMecha, to support non-experts to design and construct their own paper mechatronics models. This paper describes how I used the tool to create two kinds of paper mechatronics models: walkers and flowers and discuss next steps.",0,16.2280701754
UIST,88dfa5978dd3042088e208ba51180d3a6204db9e,UIST,2015,Supporting Collaborative Innovation at Scale,Pao Siangliulue,3061428,"Emerging online innovation platforms have enabled large groups of people to collaborate and generate ideas together in ways that were not possible before. However, these platforms also introduce new challenges in finding inspiration from a large number of ideas, and coordinating the collective effort. In my dissertation, I address the challenges of large scale idea generation platforms by developing methods and systems for helping people make effective use of each other's ideas, and for orchestrating collective effort to reduce redundancy and increase the quality and breadth of generated ideas.",0,16.2280701754
UIST,78fce4c740e23524db758a885ac2d1bcad3c906f,UIST,2010,CopyCAD: remixing physical objects with copy and paste from the real world,"Sean Follmer, David Carr, Emily Lovell, Hiroshi Ishii","2770912, 8562868, 1712410, 1749649","This paper introduces a novel technique for integrating geometry from physical objects into computer aided design (CAD) software. We allow users to copy arbitrary real world object geometry into 2D CAD designs at scale through the use of a camera/projector system. This paper also introduces a system, CopyCAD, that uses this technique, and augments a Computer Controlled (CNC) milling machine. CopyCAD gathers input from physical objects, sketches and interactions directly on a milling machine, allowing novice users to copy parts of real world objects, modify them and then create a new physical part.",34,84.8837209302
UIST,08eabbd2ba336f2086d7612e9a32d89bd27b6c74,UIST,2010,MAI painting brush: an interactive device that realizes the feeling of real painting,"Mai Otsuki, Kenji Sugihara, Asako Kimura, Fumihisa Shibata, Hideyuki Tamura","1683502, 2976251, 1726979, 1723268, 1730627","Many digital painting systems have been proposed and their quality is improving. In these systems, graphics tablets are widely used as input devices. However, because of its rigid nib and indirect manipulation, the operational feeling of a graphics tablet is different from that of real paint brush. We solved this problem by developing the MR-based Artistic Interactive (MAI) Painting Brush, which imitates a real paint brush, and constructed a mixed reality (MR) painting system that enables direct painting on physical objects in the real world.",16,73.8372093023
UIST,7b794a34f5577c237bed54cc850f07fddb4abc80,UIST,2011,MAI painting brush++: augmenting the feeling of painting with new visual and tactile feedback mechanisms,"Kenji Sugihara, Mai Otsuki, Asako Kimura, Fumihisa Shibata, Hideyuki Tamura","2976251, 1683502, 1726979, 1723268, 1730627","We have developed a mixed-reality (MR) painting system named the MR-based Artistic Interactive (MAI) Painting Expert and MAI Painting Brush which simulates the painting of physical objects in the real world. In this paper, we describe how the MAI Painting Brush was upgraded to the ""MAI Painting Brush++,"" enabling virtual painting on virtual objects. The improved system has a visual and tactile feedback mechanism that simulates the effect of touch when used on a virtual painting target. This is achieved using deformation of the brush tip and reaction force on the hand.",2,24.2857142857
UIST,b67c5548f33d8b0acd1aa2b3a87cf7717e00307f,UIST,2013,Video collections in panoramic contexts,"James Tompkin, Fabrizio Pece, Rajvi Shah, Shahram Izadi, Jan Kautz, Christian Theobalt","1854493, 3214269, 1962817, 1699068, 1690538, 1680185","Video collections of places show contrasts and changes in our world, but current interfaces to video collections make it hard for users to explore these changes. Recent state-of-the-art interfaces attempt to solve this problem for 'outside-&#62;in' collections, but cannot connect 'inside-&#62;out' collections of the same place which do not visually overlap. We extend the focus+context paradigm to create a video-collections+context interface by embedding videos into a panorama. We build a spatio-temporal index and tools for fast exploration of the space and time of the video collection. We demonstrate the flexibility of our representation with interfaces for desktop and mobile flat displays, and for a spherical display with joypad and tablet controllers. We study with users the effect of our video-collection+context system to spatio-temporal localization tasks, and find significant improvements to accuracy and completion time in visual search tasks compared to existing systems. We measure the usability of our interface with System Usability Scale (SUS) and task-specific questionnaires, and find our system scores higher.",5,55.9633027523
UIST,11035becfa4b6d71dac0d647abf78d201bf05417,UIST,2001,View management for virtual and augmented reality,"Blaine Bell, Steven K. Feiner, Tobias Höllerer","1926106, 1809403, 1743721","We describe a view-management component for interactive 3D user interfaces. By <i>view management,</i> we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.",140,80.0
UIST,bbd09f38e3d283e660099aa0427f24d686d449cb,UIST,2011,TOPS: television object promoting system,"Tun-Hao You, Yi-Jui Wu, Yi-Jen Yeh","2780831, 2054807, 3147083","In this short paper, we propose the Television Object Promoting system (TOPS), a What You See Is What You Get (WYSIWYG) user interface and user experience designed for users to interact with objects in TV programs. Using TOPS while watching TV, consumers can acquire informa-tion about objects appearing in TV programs, such as mer-chandise, people, and scenic spots. Moreover, consumers can purchase merchandise directly, or can obtain services or items related to those objects. Besides, vendors are able to provide detail and selling information about the objects. The Television Object Promoting system (TOPS) offers not only convenience to consumers, but also new marketing methods to vendors. The paper also discusses the features, design, and implementation of TOPS.",0,6.19047619048
UIST,22b42a3c4aba4e7e98290143a2ac02f5e213ee89,UIST,2005,eyeLook: using attention to facilitate mobile media consumption,"Connor Dickie, Roel Vertegaal, Changuk Sohn, Daniel Cheng","1729292, 1687608, 1684112, 7561073","One of the problems with mobile media devices is that they may distract users during critical everyday tasks, such as navigating the streets of a busy city. We addressed this issue in the design of eyeLook: a platform for attention sensitive mobile computing. eyeLook appliances use embedded low cost eyeCONTACT sensors (ECS) to detect when the user looks at the display. We discuss two eyeLook applications, seeTV and seeTXT, that facilitate courteous media consumption in mobile contexts by using the ECS to respond to user attention. seeTV is an attentive mobile video player that automatically pauses content when the user is not looking. seeTXT is an attentive speed reading application that flashes words on the display, advancing text only when the user is looking. By making mobile media devices sensitive to actual user attention, eyeLook allows applications to gracefully transition users between consuming media, and managing life.",16,25.8064516129
UIST,9715d8b4cc7143a91bf04ddf84cab9ed2aef1d52,UIST,2010,User interface models for the cloud,Hubert Pham,2095906,"The current desktop metaphor is unsuitable for the coming age of cloud-based applications. The desktop was developed in an era that was focused on local resources, and consequently its gestures, semantics, and security model reflect heavy reliance on hierarchy and physical locations. This paper proposes a new user interface model that accounts for cloud applications, incorporating representations of people and new gestures for sharing and access, while minimizing the prominence of location. The model's key feature is a lightweight mechanism to group objects for resource organization, sharing, and access control, towards the goal of providing simple semantics for a wide range of tasks, while also achieving security through greater usability.",5,54.0697674419
UIST,539f611490b193fbd5b0c76d1f46a5427b37efcf,UIST,2012,Collision avoidance interface for safe piloting of unmanned vehicles using a mobile device,"Erin Treacy Solovey, Kim Jackson, Mary L. Cummings","1751638, 2806388, 1795953","Autonomous robots and vehicles can perform tasks that are unsafe or undesirable for humans to do themselves, such as investigate safety in nuclear reactors or assess structural damage to a building or bridge after an earthquake. In addition, improvements in autonomous modes of such vehicles are making it easier for minimally-trained individuals to operate the vehicles. As the autonomous capabilities advance, the user's role shifts from a direct teleoperator to a supervisory control role. Since the human operator is often better suited to make decisions in uncertain situations, it is important for the human operator to have awareness of the environment in which the vehicle is operating in order to prevent collisions and damage to the vehicle as well as the structures and people in the vicinity. In this paper, we present the Collision and Obstacle Detection and Alerting (CODA) display, a novel interface to enable safe piloting of a Micro Aerial Vehicle with a mobile device in real-world settings.",0,9.80392156863
UIST,0dfbc8ef80e37784cee552912211d8355a9b45b2,UIST,2002,An annotated situation-awareness aid for augmented reality,"Blaine Bell, Tobias Höllerer, Steven K. Feiner","1926106, 1743721, 1809403","We present a situation-awareness aid for augmented reality systems based on an annotated ""world in miniature."" Our aid is designed to provide users with an overview of their environment that allows them to select and inquire about the objects that it contains. Two key capabilities are discussed that are intended to address the needs of mobile users. The aid's position, scale, and orientation are controlled by a novel approach that allows the user to inspect the aid without the need for manual interaction. As the user alternates their attention between the physical world and virtual aid, popup annotations associated with selected objects can move freely between the objects' representations in the two models.",31,35.4166666667
UIST,18868cc02732042da589d0206322aab463f92dfa,UIST,2004,Interacting with hidden content using content-aware free-space transparency,"Edward W. Ishak, Steven K. Feiner","2192767, 1809403","We present &#60;i>content-aware free-space transparency&#60;/i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.",30,35.5263157895
UIST,b432234b2b848128292b750506319f60f4271869,UIST,2006,Content-aware scrolling,"Edward W. Ishak, Steven K. Feiner","2192767, 1809403","Scrolling is used to navigate large information spaces on small screens, but is often too restrictive or cumbersome to use for particular types of content, such as multi-page, multi-column documents. To address this problem, we introduce <i>content-aware scrolling</i> (CAS), an approach that takes into account various characteristics of document content to determine scrolling direction, speed, and zoom. We also present the <i>CAS widget</i>, which supports scrolling through a content-aware path using traditional scrolling methods, demonstrating the advantages of making a traditional technique content-aware.",17,22.5
UIST,6520c7f7c077d6fb5a0560225e2cf74e77fef467,UIST,2012,JellyLens: content-aware adaptive lenses,"Cyprien Pindat, Emmanuel Pietriga, Olivier Chapuis, Claude Puech","2446712, 1728256, 3342979, 1730178","Focus+context lens-based techniques smoothly integrate two levels of detail using spatial distortion to connect the magnified region and the context. Distortion guarantees visual continuity, but causes problems of interpretation and focus targeting, partly due to the fact that most techniques are based on statically-defined, regular lens shapes, that result in far-from-optimal magnification and distortion. JellyLenses dynamically adapt to the shape of the objects of interest, providing detail-in-context visualizations of higher relevance by optimizing what regions fall into the focus, context and spatially-distorted transition regions. This both improves the visibility of content in the focus region and preserves a larger part of the context region. We describe the approach and its implementation, and report on a controlled experiment that evaluates the usability of JellyLenses compared to regular fisheye lenses, showing clear performance improvements with the new technique for a multi-scale visual search task.",16,68.6274509804
UIST,1e671c15d4d78795bf52d4b44d997e97c3f6ac98,UIST,2015,Elastic Cursor and Elastic Edge: Applying Simulated Resistance to Interface Elements for Seamless Edge-scroll,"Jinha Lee, Seungcheon Baek","1975251, 2744685","We present elastic cursor and elastic edge, new interaction techniques for seamless edge-scroll. Through the use of light-weight physical simulations of elastic behavior on interface elements, we can improve precision, usability, and cueing on the use of edge-scroll in scrollable windows or screens, and make experiences more playful and easier to learn.",0,16.2280701754
UIST,f4f5d5c9892745a0681d1bc2a6d34b2e370a7b14,UIST,2016,Hand Gesture and On-body Touch Recognition by Active Acoustic Sensing throughout the Human Body,"Tomohiro Yokota, Tomoko Hashida","7154080, 1807316","In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.",0,44.6540880503
UIST,69565ba79eef8488f50645d6069766d3e971bcd5,UIST,2011,ZeroN: mid-air tangible interaction enabled by computer controlled magnetic levitation,"Jinha Lee, Rehmi Post, Hiroshi Ishii","1975251, 2647090, 1749649","This paper presents ZeroN, a new tangible interface element that can be levitated and moved freely by computer in a three dimensional space. ZeroN serves as a tangible rep-resentation of a 3D coordinate of the virtual world through which users can see, feel, and control computation. To ac-complish this, we developed a magnetic control system that can levitate and actuate a permanent magnet in a pre-defined 3D volume. This is combined with an optical tracking and display system that projects images on the levitating object. We present applications that explore this new interaction modality. Users are invited to place or move the ZeroN object just as they can place objects on surfaces. For example, users can place the sun above physical objects to cast digital shadows, or place a planet that will start revolving based on simulated physical conditions. We describe the technology and interaction scenarios, discuss initial observations, and outline future development.",35,81.4285714286
UIST,e9209c51a0456acc0b0564212b97f1f6ca429b6a,UIST,2009,Relaxed selection techniques for querying time-series graphs,"Christian Holz, Steven K. Feiner","2794828, 1809403","Time-series graphs are often used to visualize phenomena that change over time. Common tasks include comparing values at different points in time and searching for specified patterns, either exact or approximate. However, tools that support time-series graphs typically separate query specification from the actual search process, allowing users to adapt the level of similarity only after specifying the pattern. We introduce relaxed selection techniques, in which users implicitly define a level of similarity that can vary across the search pattern, while creating a search query with a single-gesture interaction. Users sketch over part of the graph, establishing the level of similarity through either spatial deviations from the graph, or the speed at which they sketch (temporal deviations). In a user study, participants were significantly faster when using our temporally relaxed selection technique than when using traditional techniques. In addition, they achieved significantly higher precision and recall with our spatially relaxed selection technique compared to traditional techniques.",13,22.8571428571
UIST,82541dbc286af8777d2e82c1da9e80545228cf7a,UIST,2010,Beyond: collapsible input device for direct 3D manipulation beyond the screen,"Jinha Lee, Surat Teerapittayanon, Hiroshi Ishii","1975251, 3242151, 1749649","What would it be like to reach into a screen and manipulate or design virtual objects as in real world? We present <i>Beyond</i>, a collapsible input device for direct 3D manipulation. When pressed against a screen, <i>Beyond</i> collapses in the physical world and extends into the digital space of the screen, such that users can perceive that they are inserting the tool into the virtual space. <i>Beyond</i> allows users to directly interact with 3D media, avoiding separation between the users' input and the displayed 3D graphics without requiring special glasses or wearables, thereby enabling users to select, draw, and sculpt in 3D virtual space unfettered. We describe detailed interaction techniques, implementation and application scenarios focused on 3D geometric design and prototyping.",2,33.1395348837
UIST,18516ed32f1070d658addab42cff7f331dd6d777,UIST,2013,Controlling widgets with one power-up button,"Daniel Spelmezan, Caroline Appert, Olivier Chapuis, Emmanuel Pietriga","2264859, 1802574, 3342979, 1728256","The Power-up Button is a physical button that combines pressure and proximity sensing to enable gestural interaction with one thumb. Combined with a gesture recognizer that takes the hand's anatomy into account, the Power-up Button can recognize six different mid-air gestures performed on the side of a mobile device. This gives it, for instance, enough expressive power to provide full one-handed control of interface widgets displayed on screen. This technology can complement touch input, and can be particularly useful when interacting eyes-free. It also opens up a larger design space for widget organization on screen: the button enables a more compact layout of interface components than what touch input alone would allow. This can be useful when, e.g., filling the numerous fields of a long Web form, or for very small devices.",5,55.9633027523
UIST,7b9e49b9f3df1faa08ec6005425ac76843151767,UIST,2008,Design as exploration: creating interface alternatives through parallel authoring and runtime tuning,"Björn Hartmann, Loren Yu, Abel Allison, Yeonsoo Yang, Scott R. Klemmer","4020023, 3058851, 2156814, 2666187, 1728167","Creating multiple prototypes facilitates comparative reasoning, grounds team discussion, and enables situated exploration. However, current interface design tools focus on creating single artifacts. This paper introduces the Juxtapose code editor and runtime environment for designing multiple alternatives of both application logic and interface parameters. For rapidly comparing code alternatives, Juxtapose introduces selectively parallel source editing and execution. To explore parameter variations, Juxtapose automatically creates control interfaces for ""tuning"" application variables at runtime. This paper describes techniques to support design exploration for desktop, mobile, and physical interfaces, and situates this work in a larger design space of tools for explorative programming. A summative study of Juxtapose with 18 participants demonstrated that parallel editing and execution are accessible to interaction designers and that designers can leverage these techniques to survey more options, faster.",44,74.2857142857
UIST,f82aa5da2d074d152962832e79bb4455a12e49da,UIST,2016,RunPlay: Action Recognition Using Wearable Device Apply on Parkour Game,"Shih-Yao Wei, Chen-Yu Wang, Ting-Wei Chiu, Yi-Ping Lo, Zhi-Wei Yang, Hsing-Man Wang, Yi-Ping Hung","3455258, 4267124, 2534710, 2779711, 1738154, 3492679, 7312257","In this paper, we present an action recognition system which consists of pressure insoles, with 16 pressure sensors, and an inertial measurement unit. By analysing the data measured from these sensors, we are able to recognised several human activities. In this circumstance, we focus on the detection of jumping, squatting, moving left and right. We also designed a parkour game on a mobile device to demonstrate the in-game control of an avatar by human action.",0,44.6540880503
UIST,ff36761115929b0d50063f507fbcfadb41d93627,UIST,2016,ScalableBody: A Telepresence Robot Supporting Socially Acceptable Interactions and Human Augmentation through Vertical Actuation,"Akira Matsuda, Jun Rekimoto","2766139, 1685962","Most telepresence robots have a fixed-size body, and are unable to change the camera or display position. Therefore, although making eye contact is important in human expression, current fixed-size telepresence robots fail to achieve this.
 We propose a novel telepresence robot called ScalableBody, which enables users to make eye contact during conversations by changing its height. ScalableBody extends its body to modify the position of its camera or display. This approach provides eye contacts in remote conversations, thus creating almost same situation when the remote and local users make conversation like a real meeting. As for the remote users, this approach also enables them to experience having a conversation from different heights, such as being a giant or a dwarf. This technique extends the possibilities of remote communication by telepresence robots.",0,44.6540880503
UIST,06d31fc6f378e4f5e2a2df4dfe0bec6f5a60daed,UIST,2012,Waken: reverse engineering usage information and interface structure from software videos,"Nikola Banovic, Tovi Grossman, Justin Matejka, George W. Fitzmaurice","2997335, 3313809, 2578065, 1703735","We present Waken, an application-independent system that recognizes UI components and activities from screen captured videos, without any prior knowledge of that application. Waken can identify the cursors, icons, menus, and tooltips that an application contains, and when those items are used. Waken uses frame differencing to identify occurrences of behaviors that are common across graphical user interfaces. Candidate templates are built, and then other occurrences of those templates are identified using a multi-phase algorithm. An evaluation demonstrates that the system can successfully reconstruct many aspects of a UI without any prior application-dependant knowledge. To showcase the design opportunities that are introduced by having this additional meta-data, we present the Waken Video Player, which allows users to directly interact with UI components that are displayed in the video.",13,61.7647058824
UIST,69584f185112aa96777bf6e60c879ee4377453aa,UIST,1991,A demonstrational technique for developing interfaces with dynamically created objects,"David Wolber, Gene L. Fisher","1796769, 7704672","The development of user inteijaces is often facilitated by the use of a drawing editor. The user interface specialist draws pictures of the different "" states "" of the inte~ace and passes these specifications on to the programmer. The user interface specialist might also use the drawing editor to demonstrate to the programmer the interactive behavior that the interface should exhibit ; that is, he might demonstrate to the programmer the actions that an end-user can pe~orm, and the graphical manner by which the application should respond to the end-user's stimuli. From the specljications, and the in-person demonstrations, the programmer implements a protoppe of the interface. DEMO is a User Interface Development System (UIDS) that eliminates the programmer from the above process. Using an enhanced drawing editor, the user interface specialist demonstrates the actions of the end-user and the system, just as he would if the programmer were watching. However no programmer is necessary: DEMO recorak these demonstrations, makes generalizations from them, and automatically generates a prototype of the inte~ace.",26,60.8695652174
UIST,4a8c1d88a54acd40218b33e3154a18e6b9607f0b,UIST,2003,User interface continuations,"Dennis Quan, David Huynh, David R. Karger, Rob Miller","1782484, 1762663, 1743286, 1723785","Dialog boxes that collect parameters for commands often create ephemeral, unnatural interruptions of a program's normal execution flow, encouraging the user to complete the dialog box as quickly as possible in order for the program to process that command. In this paper we examine the idea of turning the act of collecting parameters from a user into a first class object called a user interface continuation. Programs can create user interface continuations by specifying what information is to be collected from the user and supplying a callback (i.e., a continuation) to be notified with the collected information. A partially completed user interface continuation can be saved as a new command, much as currying and partially evaluating a function with a set of parameters produces a new function. Furthermore, user interface continuations, like other continuation-passing paradigms, can be used to allow program execution to continue uninterrupted while the user determines a command's parameters at his or her leisure.",20,16.6666666667
UIST,7c9d53566a9327cfb843999d191aff1b2b02077e,UIST,2016,Rovables: Miniature On-Body Robots as Mobile Wearables,"Artem Dementyev, Hsin-Liu Cindy Kao, Inrak Choi, Deborah Ajilo, Maggie Xu, Joseph A. Paradiso, Chris Schmandt, Sean Follmer","2103349, 2649998, 3491692, 2026087, 3491790, 4798651, 1729321, 2770912","We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.",0,44.6540880503
UIST,2e59628c16181ded395f63cab68c142e295e74d9,UIST,2001,Simplicial families of drawings,"Lucas Kovar, Michael Gleicher","2336720, 1776507","In this paper we present a method for helping artists make artwork more accessible to casual users. We focus on the specific case of drawings, showing how a small number of drawings can be transformed into a richer object containing an entire family of similar drawings. This object is represented as a simplicial complex approximating a set of valid interpolations in configuration space. The artist does not interact directly with the simplicial complex. Instead, she guides its construction by answering a specially chosen set of yes/no questions. By combining the flexibility of a simplicial complex with direct human guidance, we are able to represent very general constraints on membership in a family. The constructed simplicial complex supports a variety of algorithms useful to an end user, including random sampling of the space of drawings, constrained interpolation between drawings, projection of another drawing into the family, and interactive exploration of the family.",7,16.6666666667
UIST,fc1256c7375baa5b98f5b69ae584f2e4f0fbffe7,UIST,2015,ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data,"Xin Yi, Chun Yu, Mingrui Zhang, Sida Gao, Ke Sun, Yuanchun Shi","6867497, 8700191, 1930786, 2260857, 4730530, 1732440","Ten-finger freehand mid-air typing is a potential solution for post-desktop interaction. However, the absence of tactile feedback as well as the inability to accurately distinguish tapping finger or target keys exists as the major challenge for mid-air typing. In this paper, we present ATK, a novel interaction technique that enables freehand ten-finger typing in the air based on 3D hand tracking data. Our hypothesis is that expert typists are able to transfer their typing ability from physical keyboards to mid-air typing. We followed an iterative approach in designing ATK. We first empirically investigated users' mid-air typing behavior, and examined fingertip kinematics during tapping, correlated movement among fingers and 3D distribution of tapping endpoints. Based on the findings, we proposed a probabilistic tap detection algorithm, and augmented Goodman's input correction model to account for the ambiguity in distinguishing tapping finger. We finally evaluated the performance of ATK with a 4-block study. Participants typed 23.0 WPM with an uncorrected word-level error rate of 0.3% in the first block, and later achieved 29.2 WPM in the last block without sacrificing accuracy.",1,42.1052631579
UIST,52f29b4e75c137f48135b0999fa3a18d3169294d,UIST,2004,Hierarchical parsing and recognition of hand-sketched diagrams,"Levent Burak Kara, Thomas F. Stahovich","1808848, 1706168","A long standing challenge in pen-based computer interaction is the ability to make sense of informal sketches. A main difficulty lies in reliably extracting and recognizing the intended set of visual objects from a continuous stream of pen strokes. Existing pen-based systems either avoid these issues altogether, thus resulting in the equivalent of a drawing program, or rely on algorithms that place unnatural constraints on the way the user draws. As one step toward alleviating these difficulties, we present an integrated sketch parsing and recognition approach designed to enable natural, fluid, sketch-based computer interaction. The techniques presented in this paper are oriented toward the domain of network diagrams. In the first step of our approach, the stream of pen strokes is examined to identify the arrows in the sketch. The identified arrows then anchor a spatial analysis which groups the uninterpreted strokes into distinct clusters, each representing a single object. Finally, a trainable shape recognizer, which is informed by the spatial analysis, is used to find the best interpretations of the clusters. Based on these concepts, we have built SimuSketch, a sketch-based interface for Matlab's Simulink software package. An evaluation of SimuSketch has indicated that even novice users can effectively utilize our system to solve real engineering problems without having to know much about the underlying recognition techniques.",74,81.5789473684
UIST,3718924fc4101959d7c5f5b78bec7ed7c7f12a83,UIST,2013,The drawing assistant: automated drawing guidance and feedback from photographs,"Emmanuel Iarussi, Adrien Bousseau, Theophanis Tsandilas","1859777, 2149814, 2110778","We present an interactive drawing tool that provides automated guidance over model photographs to help people practice traditional drawing-by-observation techniques. The drawing literature describes a number of techniques to %support this task and help people gain consciousness of the shapes in a scene and their relationships. We compile these techniques and derive a set of construction lines that we automatically extract from a model photograph. We then display these lines over the model to guide its manual reproduction by the user on the drawing canvas. Finally, we use shape-matching to register the user's sketch with the model guides. We use this registration to provide corrective feedback to the user. Our user studies show that automatically extracted construction lines can help users draw more accurately. Furthermore, users report that guidance and corrective feedback help them better understand how to draw.",20,83.4862385321
UIST,99ae967eff66c2914efdfd869b63ffbd3fbf5ea6,UIST,2012,Interpreting strokes on paper with a mobile assistant,Theophanis Tsandilas,2110778,"Digital pen technology has allowed for the easy transfer of pen data from paper to the computer. However, linking handwritten content with the digital world remains a hard problem as it requires the translation of unstructured and highly personal vocabularies into structured ones that computers can easily understand and process. Automatic recognition can help to this direction, but as it is not always reliable, solutions require the active cooperation between users and recognition algorithms. This work examines the use of portable touch-screen devices in connection with pen and paper to help users direct and refine the interpretation of their strokes on paper. We explore four techniques of bi-manual interaction that combine touch and pen-writing, where user attention is divided between the original strokes on paper and their interpretation by the electronic device. We demonstrate the techniques through a mobile interface for writing music that complements the automatic recognition with interactive user-driven interpretation. An experiment evaluates the four techniques and provides insights about their strengths and limitations.",3,40.1960784314
UIST,63fa5028e28d8db85e106e2a4b6b44e04fcb9c08,UIST,2012,TouchCast: an on-line platform for creation and sharing of tactile content based on tactile copy & paste,"Yuta Takeuchi, Hirotaka Katakura, Sho Kamuro, Kouta Minamizawa, Susumu Tachi","2953053, 1987631, 1694030, 1711743, 7688338","We propose <i>TouchCast</i>, which is an on-line platform for the creating and sharing of tactile content based on <i>Tactile Copy &#38; Paste. User-Generated Tactile Content</i> refers to tactile content that is created, shared and appreciated by general Internet users. <i>TouchCast</i> enables users to create tactile content by applying tactile textures to existing on- line content (e.g., illustrations) and to share the created content over the network. Applied textures are scanned from real objects as audio signals and we call this technique <i>Tactile Copy &#38; Paste</i>. In this study, we implement the system as a web browser add-on and to create <i>User Generated Tactile Content</i>.",0,9.80392156863
UIST,ec55a0e64de23d68a045fe3bf1153722849145ae,UIST,2014,A pen-based device for sketching with multi-directional traction forces,"Junichi Yamaoka, Yasuaki Kakehi","2136609, 1755682","This paper presents a pen-grip-shaped device that assists in sketching using multi-directional traction forces. By using an asymmetric acceleration of the vibration actuator that drive in a linear direction, the system can create a virtual traction force with the proper direction. We augment users' drawing skills with the device that arranged 4 vibration actuators that provides a traction force and a rotary sensation. Therefore the device is portable and does not have any limitation of needing to be in a particular location, this device can be used to guide the direction and assist the user who is sketching on a large piece of paper. Moreover, users can attach it to any writing utensil such as brushes, crayons. In this paper, we describe the details of the design of device, evaluation experiments, and applications.",1,31.007751938
UIST,a9b8d847bba968bd03648c013e4b0a89e3288abb,UIST,2010,The satellite cursor: achieving MAGIC pointing without gaze tracking using multiple cursors,"Chun Yu, Yuanchun Shi, Ravin Balakrishnan, Xiangliang Meng, Yue Suo, Mingming Fan, Yongqiang Qin","8700191, 1732440, 1748870, 3167781, 1978473, 1868611, 2263638","We present the satellite cursor - a novel technique that uses multiple cursors to improve pointing performance by reducing input movement. The satellite cursor associates every target with a separate cursor in its vicinity for pointing, which realizes the MAGIC (manual and gaze input cascade) pointing method without gaze tracking. We discuss the problem of visual clutter caused by multiple cursors and propose several designs to mitigate it. Two controlled experiments were conducted to evaluate satellite cursor performance in a simple reciprocal pointing task and a complex task with multiple targets of varying layout densities. Results show the satellite cursor can save significant mouse movement and consequently pointing time, especially for sparse target layouts, and that satellite cursor performance can be accurately modeled by Fitts' Law.",2,33.1395348837
UIST,3b0449eba55ab610a636e5fcf790413e9fcc993b,UIST,2013,dePEDd: augmented handwriting system using ferromagnetism of a ballpoint pen,"Junichi Yamaoka, Yasuaki Kakehi","2136609, 1755682","This paper presents dePENd, a novel interactive system that assists in sketching using regular pens and paper. Our system utilizes the ferromagnetic feature of the metal tip of a regular ballpoint pen. The computer controlling the X and Y positions of the magnet under the surface of the table provides entirely new drawing experiences. By controlling the movements of a pen and presenting haptic guides, the system allows a user to easily draw diagrams and pictures consisting of lines and circles, which are difficult to create by free-hand drawing. Moreover, the system also allows users to freely edit and arrange prescribed pictures. This is expected to reduce the resistance to drawing and promote users' creativity. In addition, we propose a communication tool using two dePENd systems that is expected to enhance the drawing skills of users. The functions of this system enable users to utilize interactive applications such as copying and redrawing drafted pictures or scaling the pictures using a digital pen. Furthermore, we implement the system and evaluate its technical features. In this paper, we describe the details of the design and implementations of the device, along with applications, technical evaluations, and future prospects.",10,68.3486238532
UIST,67147ce4a81e053f5005f5bf719b137e8f2adbf2,UIST,2012,Needle user interface: a sewing interface using layered conductive fabrics,"Ken Nakagaki, Yasuaki Kakehi","2333636, 1755682","Embroidery is a creative manual activity practiced by many people for a living. Such a craft demands skill and knowledge, and as it is sometimes complicated and delicate, it can be difficult for beginners to learn. We propose a system, named the Needle User Interface, which enables sewers to record and share their needlework, and receive feedback. In particular, this system can detect the position and orientation of a needle being inserted into and removed from a textile. Moreover, this system can give visual, auditory, and haptic feedback to users in real time for directing their ac-tions appropriately. In this paper, we describe the system design, the input system, and the feedback delivery mechanism.",0,9.80392156863
UIST,0fa8a4cbb7cacfe161280e5b6a1f780929ddc743,UIST,2015,User Interaction Models for Disambiguation in Programming by Example,"Mikaël Mayer, Gustavo Soares, Maxim Grechkin, Vu Le, Mark Marron, Oleksandr Polozov, Rishabh Singh, Benjamin G. Zorn, Sumit Gulwani","2626438, 2091766, 1892750, 6875900, 1761067, 2636739, 3274303, 1762643, 2108314","Programming by Examples (PBE) has the potential to revolutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for automating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user's intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate actionable information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consistent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users' confidence in the result.",6,85.0877192982
UIST,a1e40ac03351eeff7611973b0488ddbcd1df9f0d,UIST,2012,Pebbles: an interactive configuration tool for indoor robot navigation,"Haipeng Mi, Kentaro Ishii, Lei Ma, Natsuda Laokulrat, Masahiko Inami, Takeo Igarashi","2861836, 7254458, 6432837, 2734498, 1684930, 1717356","This study presents an interactive configuration tool that assists non-expert users to design specific navigation route for mobile robot in an indoor environment. The user places small active markers, called pebbles, on the floor along the desired route in order to guide the robot to the destination. The active markers establish a navigation network by communicating each other with IR beacon and the robot follows the markers to reach the designated goal. During the installation, a user can get effective feedback from LED indicators and voice prompts, so that the user can immediately understand if the navigation route is appropriately configured as expected. With this tool a novice user may easily customize a mobile robot for various indoor tasks.",1,25.0
UIST,6f956287ec197b83acc6b022d2ae41eec108ad1b,UIST,2011,The proximity toolkit: prototyping proxemic interactions in ubiquitous computing ecologies,"Nicolai Marquardt, Robert Diaz-Marino, Sebastian Boring, Saul Greenberg","3328262, 2549432, 1741219, 1696942","People naturally understand and use proxemic relationships (e.g., their distance and orientation towards others) in everyday situations. However, only few ubiquitous computing (ubicomp) systems interpret such proxemic relationships to mediate interaction (proxemic interaction). A technical problem is that developers find it challenging and tedious to access proxemic information from sensors. Our Proximity Toolkit solves this problem. It simplifies the exploration of interaction techniques by supplying fine-grained proxemic information between people, portable devices, large interactive surfaces, and other non-digital objects in a room-sized environment. The toolkit offers three key features. 1) It facilitates rapid prototyping of proxemic-aware systems by supplying developers with the orientation, distance, motion, identity, and location information between entities. 2) It includes various tools, such as a visual monitoring tool, that allows developers to visually observe, record and explore proxemic relationships in 3D space. (3) Its flexible architecture separates sensing hardware from the proxemic data model derived from these sensors, which means that a variety of sensing technologies can be substituted or combined to derive proxemic information. We illustrate the versatility of the toolkit with proxemic-aware systems built by students.",97,96.1904761905
UIST,2d3ff5681818fb89094e58dcf09c0520297e2d64,UIST,2016,LIME: LIquid MEtal Interfaces for Non-Rigid Interaction,"Qiuyu Lu, Chengpeng Mao, Liyuan Wang, Haipeng Mi","3491720, 3492047, 5155955, 2861836","Room-temperature liquid metal GaIn25 (Eutectic Gallium- Indium alloy, 75% gallium and 25% indium) has distinctive properties of reversible deformation and controllable locomotion under an external electric field stimulus. Liquid metal's newly discovered properties imply great possibilities in developing new technique for interface design. In this paper, we present LIME, LIquid MEtal interfaces for non-rigid interaction. We first discuss the interaction potential of LIME interfaces. Then we introduce the development of LIME cells and the design of some LIME widgets.",0,44.6540880503
UIST,6afa4882c881d0f77d20918e8ad133e3b0a38d87,UIST,2016,Muscle-plotter: An Interactive System based on Electrical Muscle Stimulation that Produces Spatial Output,"Pedro Lopes, Doaa Yüksel, François Guimbretière, Patrick Baudisch","2816776, 3492069, 2539134, 1729393","We explore how to create interactive systems based on electrical muscle stimulation that offer expressive output. We present muscle-plotter, a system that provides users with input <i>and output</i> access to a computer system while on the go. Using pen-on-paper interaction, muscle-plotter allows users to engage in cognitively demanding activities, such as writing math. Users write formulas using a pen and the system responds by making the users' hand draw charts and widgets. While Anoto technology in the pen tracks users' input, muscle-plotter uses electrical muscle stimulation (EMS) to steer the user's wrist so as to plot charts, fit lines through data points, find data points of interest, or fill in forms. We demonstrate the system at the example of six simple applications, including a wind tunnel simulator.
 The key idea behind muscle-plotter is to make the user's hand sweep an area on which muscle-plotter renders curves, i.e., series of values, and to <i>persist</i> this EMS output by means of the pen. This allows the system to build up a larger whole. Still, the use of EMS allows muscle-plotter to achieve a compact and mobile form factor. In our user study, muscle-plotter made participants draw random plots with an accuracy of &#177;4.07 mm and preserved the frequency of functions to be drawn up to 0.3 cycles per cm.",0,44.6540880503
UIST,2c9516631ea51bd8786d9d367b43d90698bd4e73,UIST,2004,CrossY: a crossing-based drawing application,"Georg Apitz, François Guimbretière","1824002, 2539134","We introduce CrossY, a simple drawing application developed as a benchmark to demonstrate the feasibility of goal-crossing as the basis for a graphical user interface. While crossing was previously identified as a potential substitute for the classic point-and-click interaction, this work is the first to report on the practical aspects of implementing an interface solely based on goal-crossing.",99,86.8421052632
UIST,283ce4da6b6c1e511b22eb6d306554d93c46c157,UIST,2012,Exposing and understanding scrolling transfer functions,"Philip Quinn, Andy Cockburn, Géry Casiez, Nicolas Roussel, Carl Gutwin","1825818, 1814003, 3051289, 1728921, 1693768","Scrolling is controlled through many forms of input devices, such as mouse wheels, trackpad gestures, arrow keys, and joysticks. Performance with these devices can be adjusted by introducing variable transfer functions to alter the range of expressible speed, precision, and sensitivity. However, existing transfer functions are typically ""black boxes"" bundled into proprietary operating systems and drivers. This presents three problems for researchers: (1) a lack of knowledge about the current state of the field; (2) a difficulty in replicating research that uses scrolling devices; and (3) a potential experimental confound when evaluating scrolling devices and techniques. These three problems are caused by gaps in researchers' knowledge about what device and movement factors are important for scrolling transfer functions, and about how existing devices and drivers use these factors. We fill these knowledge gaps with a framework of transfer function factors for scrolling, and a method for analysing proprietary transfer functions---demonstrating how state of the art commercial devices accommodate some of the human control phenomena observed in prior studies.",4,45.5882352941
UIST,0788ff5511d257c96f07b1d3a2f6ef79a69684c3,UIST,2014,Humane representation of thought: a trail map for the 21st century,Bret Victor,1952451,"New representations of thought -- written language, mathematical notation, information graphics, etc -- have been responsible for some of the most significant leaps in the progress of civilization, by expanding humanity's collectively-thinkable territory. But at debilitating cost. These representations, having been invented for static media such as paper, tap into a small subset of human capabilities and neglect the rest. Knowledge work means sitting at a desk, interpreting and manipulating symbols. The human body is reduced to an eye staring at tiny rectangles and fingers on a pen or keyboard. Like any severely unbalanced way of living, this is crippling to mind and body. But less obviously, and more importantly, it is enormously wasteful of the vast human potential. Human beings naturally have many powerful modes of thinking and understanding. Most are incompatible with static media. In a culture that has contorted itself around the limitations of marks on paper, these modes are undeveloped, unrecognized, or scorned.
 We are now seeing the start of a dynamic medium. To a large extent, people today are using this medium merely to emulate and extend static representations from the era of paper, and to further constrain the ways in which the human body can interact with external representations of thought.
 But the dynamic medium offers the opportunity to deliberately invent a humane and empowering form of knowledge work. We can design dynamic representations which draw on the entire range of human capabilities -- all senses, all forms of movement, all forms of understanding -- instead of straining a few and atrophying the rest.
 This talk suggests how each of the human activities in which thought is externalized (conversing, presenting, reading, writing, etc) can be redesigned around such representations.",0,12.015503876
UIST,30c869fc40525582ee44feaf46ff18f0e0823741,UIST,2007,Lucid touch: a see-through mobile device,"Daniel J. Wigdor, Clifton Forlines, Patrick Baudisch, John Barnwell, Chia Shen","1961958, 1694854, 1729393, 4867148, 1697008","Touch is a compelling input modality for interactive devices; however, touch input on the small screen of a mobile device is problematic because a user's fingers occlude the graphical elements he wishes to work with. In this paper, we present LucidTouch, a mobile device that addresses this limitation by allowing the user to control the application by touching the back of the device. The key to making this usable is what we call pseudo-transparency: by overlaying an image of the user's hands onto the screen, we create the illusion of the mobile device itself being semi-transparent. This pseudo-transparency allows users to accurately acquire targets while not occluding the screen with their fingers and hand. Lucid Touch also supports multi-touch input, allowing users to operate the device simultaneously with all 10 fingers. We present initial study results that indicate that many users found touching on the back to be preferable to touching on the front, due to reduced occlusion, higher precision, and the ability to make multi-finger input.",93,94.4444444444
UIST,2006c638ba5e18a84f0d3f22e0070816fa14e971,UIST,2006,ModelCraft: capturing freehand annotations and edits on physical 3D models,"Hyunyoung Song, François Guimbretière, Chang Hu, Hod Lipson","2569489, 2539134, 8625918, 1747909","With the availability of affordable new desktop fabrication techniques such as 3D printing and laser cutting, physical models are used increasingly often during the architectural and industrial design cycle. Models can easily be annotated to capture comments, edits and other forms of feedback. Unfortunately, these annotations remain in the physical world and cannot be easily transferred back to the digital world. Here we present a simple solution to this problem based on a tracking pattern printed on the surface of each model. Our solution is inexpensive, requires no tracking infrastructure or per object calibration, and can be used in the field without a computer nearby. It lets users not only capture annotations, but also edit the model using a simple yet versatile command system. Once captured, annotations and edits are merged into the original CAD models. There they can be easily edited or further refined. We present the design of a SolidWorks plug-in implementing this concept, and report initial feedback from potential users using our prototype. We also present how this prototype could be extended seamlessly to a fully functional system using current 3D printing technology.",39,62.5
UIST,8549ed3fbe5481f679ac9427d79c4cfef7993d50,UIST,2014,FatBelt: motivating behavior change through isomorphic feedback,"Trevor Pels, Christina Kao, Saguna Goel","3317499, 3157874, 2926898","The ultimate problem of systems facilitating long-term health and fitness goals is the disconnect between an action and its eventual consequence. As the long-term effects of behavior change are not immediately apparent, it can be hard to motivate the desired behavior over a long period of time. As such, we introduce a system that uses physical feedback through a wearable device that inflates around the stomach as a response to calorie overconsumption, simulating the long-term weight-gain associated with over-eating. We tested a version of this system with 12 users over a period of 2 days, and found a significant decrease in consumption over a baseline period of the same length, suggesting that through physical response, FatBelt moved calorie intake drastically closer to participants' goals. Interviews with participants indicate that isomorphism to the long-term consequences was a large factor in the system's efficacy. In addition, the wearable, physical feedback was perceived as an extension of the user's body, an effect with great emotional consequences.",2,41.4728682171
UIST,1122f54859c755cd18cbcf79d88cb3f434c7c3e3,UIST,2009,Collabio: a game for annotating people within social networks,"Michael S. Bernstein, Desney S. Tan, Greg Smith, Mary Czerwinski, Eric Horvitz","3047089, 1719056, 1808984, 1702712, 1688884","We present Collabio, a social tagging game within an online social network that encourages friends to tag one another. Collabio's approach of incentivizing members of the social network to generate information about each other produces personalizing information about its users. We report usage log analysis, survey data, and a rating exercise demonstrating that Collabio tags are accurate and augment information that could have been scraped online.",42,62.8571428571
UIST,052cddeef87d81cf3c4f42ff56c16273fb67d2fa,UIST,2009,Integrated videos and maps for driving directions,"Billy Chen, Boris Neubert, Eyal Ofek, Oliver Deussen, Michael F. Cohen","8567761, 2466324, 1735652, 1850438, 1694613","While onboard navigation systems are gaining in importance, maps are still the medium of choice for laying out a route to a destination and for way finding. However, even with a map, one is almost always more comfortable navigating a route the second time due to the visual memory of the route. To make the first time navigating a route feel more familiar, we present a system that integrates a map with a video automatically constructed from panoramic imagery captured at close intervals along the route. The routing information is used to create a variable speed video depicting the route. During playback of the video, the frame and field of view are dynamically modulated to highlight salient features along the route and connect them back to the map. A user interface is demonstrated to allow exploration of the combined map, video, and textual driving directions. We discuss the construction of the hybrid map and video interface. Finally, we report the results of a study that provides evidence of the effectiveness of such a system for route following.",7,12.8571428571
UIST,00e9aacab0375b57119ce2d70fad2e7ee36d15f2,UIST,2007,Gaze-enhanced scrolling techniques,"Manu Kumar, Terry Winograd","7385025, 1699245",Scrolling is an essential part of our everyday computing experience. Contemporary scrolling techniques rely on the explicit initiation of scrolling by the user. The act of scrolling is tightly coupled with the user?s ability to absorb information via the visual channel. The use of eye gaze information is therefore a natural choice for enhancing scrolling techniques. We present several gaze-enhanced scrolling techniques for manual and automatic scrolling which use gaze information as a primary input or as an augmented input. We also introduce the use off-screen gaze-actuated buttons for document navigation and control.,25,41.6666666667
UIST,2813a1f4282170a3fa77097f34a18e0207e2c4e7,UIST,2003,TiltText: using tilt for text input to mobile phones,"Daniel J. Wigdor, Ravin Balakrishnan","1961958, 1748870","<i>TiltText</i>, a new technique for entering text into a mobile phone is described. The standard 12-button text entry keypad of a mobile phone forces ambiguity when the 26- letter Roman alphabet is mapped in the traditional manner onto keys 2-9. The <i>TiltText</i> technique uses the orientation of the phone to resolve this ambiguity, by tilting the phone in one of four directions to choose which character on a particular key to enter. We first discuss implementation strategies, and then present the results of a controlled experiment comparing <i>TiltText</i> to <i>MultiTap</i>, the most common text entry technique. The experiment included 10 participants who each entered a total of 640 phrases of text chosen from a standard corpus, over a period of about five hours. The results show that text entry speed including correction for errors using <i>TiltText</i> was 23% faster than <i>MultiTap</i> by the end of the experiment, despite a higher error rate for <i>TiltText</i>. <i>TiltText</i> is thus amongst the fastest known language-independent techniques for entering text into mobile phones.",113,79.1666666667
UIST,27bcfe24b1cd7e649fe0baa96d13b1ac4682d4f9,UIST,2012,Cross-device interaction via micro-mobility and f-formations,"Nicolai Marquardt, Ken Hinckley, Saul Greenberg","3328262, 1738072, 1696942","GroupTogether is a system that explores cross-device interaction using two sociological constructs. First, <i>F-formations</i> concern the distance and relative body orientation among multiple users, which indicate when and how people position themselves as a group. Second, <i>micro-mobility</i> describes how people orient and tilt devices towards one another to promote fine-grained sharing during co-present collaboration. We sense these constructs using: (a) a pair of overhead Kinect depth cameras to sense small groups of people, (b) low-power 8GHz band radio modules to establish the identity, presence, and coarse-grained relative locations of devices, and (c) accelerometers to detect tilting of slate devices. The resulting system supports fluid, minimally disruptive techniques for co-located collaboration by leveraging the <i>proxemics of people</i> as well as the <i>proxemics of devices</i>.",58,97.0588235294
UIST,38053bb18820fdb93c6a66968a1e62424f5af372,UIST,2013,Crowdboard: an augmented whiteboard to support large-scale co-design,"Salvatore Andolina, Daniel Lee, Steven Dow","2525289, 2402104, 5319364","Co-design efforts attempt to account for many diverse viewpoints. However, design teams lack support for meaningful real-time interaction with a large community of potential stakeholders. We present Crowdboard, a novel whiteboard system that enables many potential stakeholders to provide real-time input during early-stage design activities, such as concept mapping. Local design teams develop ideas on a standard whiteboard, which is augmented with annotations and comments from online participants. The system makes it possible for design teams to solicit real-time opinions and ideas from a community of people intrinsically motivated to shape the product/service.",1,27.0642201835
UIST,8aa5d380aba630e4f450da4e27e492bba1e81e14,UIST,2012,High-performance pen + touch modality interactions: a real-time strategy game eSports context,"William A. Hamilton, Andruid Kerne, Tom Robbins","1743517, 1694380, 2065700","We used the situated context of real-time strategy (RTS) games to address the design and evaluation of new pen + touch interaction techniques. RTS play is a popular genre of Electronic Sports (<i>eSports</i>), games played and spectated at an extremely high level. Interaction techniques are critical for eSports players, because they so directly impact performance.
 Through this process, new techniques and implications for pen + touch and bi-manual interaction emerged. We enhance non-dominant hand (NDH) interaction with <i>edge-constrained</i> affordances, anchored to physical features of interactive sur- faces, effectively increasing target width. We develop <i>bi-manual overloading</i>, an approach to reduce the total number of occurrences of NDH retargeting. The novel <i>isosceles lasso select</i> technique facilitates selection of complex object subsets. <i>Pen-in-hand interaction</i>, dominant hand touch interaction performed with the pen stowed in the palm, also emerged as an efficient and expressive interaction paradigm.",9,57.3529411765
UIST,bfe3c686fef2aa8a4d3f0e7a6c01c491102d1198,UIST,2016,Porous Interfaces for Small Screen Multitasking using Finger Identification,"Aakar Gupta, Muhammed Anwar, Ravin Balakrishnan","2020345, 3491837, 1748870","The lack of dedicated multitasking interface features in smartphones has resulted in users attempting a sequential form of multitasking via frequent app switching. In addition to the obvious temporal cost, it requires physical and cognitive effort which increases multifold as the back and forth switching becomes more frequent. We propose porous interfaces, a paradigm that combines the concept of translucent windows with finger identification to support efficient multitasking on small screens. Porous interfaces enable partially transparent app windows overlaid on top of each other, each of them being accessible simultaneously using a different finger as input. We design porous interfaces to include a broad range of multitasking interactions with and between windows, while ensuring fidelity with the existing smartphone interactions. We develop an end-to-end smartphone interface that demonstrates porous interfaces. In a qualitative study, participants found porous interfaces intuitive, easy, and useful for frequent multitasking scenarios.",0,44.6540880503
UIST,1d9794919ae7045bf1ad9361cdb8ee92751f3dfb,UIST,2014,"RoomAlive: magical experiences enabled by scalable, adaptive projector-camera units","Brett R. Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, Lior Shapira","2242879, 1924499, 2747511, 2040242, 2704133, 1792265, 1735652, 1768774, 3032886, 2717456","RoomAlive is a proof-of-concept prototype that transforms any room into an immersive, augmented entertainment experience. Our system enables new interactive projection mapping experiences that dynamically adapts content to any room. Users can touch, shoot, stomp, dodge and steer projected content that seamlessly co-exists with their existing physical environment. The basic building blocks of RoomAlive are projector-depth camera units, which can be combined through a scalable, distributed framework. The projector-depth camera units are individually auto-calibrating, self-localizing, and create a unified model of the room with no user intervention. We investigate the design space of gaming experiences that are possible with RoomAlive and explore methods for dynamically mapping content based on room layout and user position. Finally we showcase four experience prototypes that demonstrate the novel interactive experiences that are possible with RoomAlive and discuss the design challenges of adapting any game to any room.",45,100.0
UIST,544c39d084e014f6430214b1638fe05789c333ba,UIST,2015,Dranimate: Rapid Real-time Gestural Rigging and Control of Animation,"Ali Momeni, Zachary Rispoli","3157441, 2779095","Dranimate is an interactive animation system that allows users to rapidly and intuitively rig and control animations based on a still image or drawing, using hand gestures. Dranimate combines two complementary methods of shape manipulation: bone-joint-based physics simulation, and the as-rigid-as-possible deformation algorithm. Dranimate also introduces a number of designed interactions that focus the users attention on the animated content, as opposed to computer keyboard or mouse.",1,42.1052631579
UIST,98b1f63953589edb0db1a3d6eff2456b860994bf,UIST,2013,eyeCan: affordable and versatile gaze interaction,Sang-won Leigh,2186648,"We present eyeCan, a software system that promises rich, sophisticated, and still usable gaze interactions with low-cost gaze tracking setups. The creation of this practical system was to drastically lower the hurdle of gaze interaction by presenting easy-to-use gaze gestures, and by reducing the cost-of-entry with the utilization of low precision gaze trackers. Our system effectively compensates for the noise from tracking sensors and involuntary eye movements, boosting both the precision and speed in cursor control. Also the possible variety of gaze gestures was explored and defined. By combining eyelid actions and gaze direction cues, our system provides rich set of gaze events and therefore enables the use of sophisticated applications e.g. playing video games or navigating street view.",0,10.5504587156
UIST,32de473f8d9dcfec4de2d5b5ffdae97006e5ede5,UIST,2011,Creating contextual help for GUIs using screenshots,"Tom Yeh, Tsung-Hsiang Chang, Bo Xie, Greg Walsh, Ivan Watkins, Krist Wongsuphasawat, Man Huang, Larry S. Davis, Benjamin B. Bederson","1704158, 2337269, 2245391, 2253151, 2321517, 2809559, 2382939, 1693428, 1799187","Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.",13,54.2857142857
UIST,c136c11139a56ae5a116bb0cd23b3f77b67f2c58,UIST,2013,BackTap: robust four-point tapping on the back of an off-the-shelf smartphone,"Cheng Zhang, Aman Parnami, Caleb Southern, Edison Thomaz, Gabriel Reyes, Rosa I. Arriaga, Gregory D. Abowd","3585347, 2943897, 3253105, 2314308, 2234625, 1752048, 1732524","We present BackTap, an interaction technique that extends the input modality of a smartphone to add four distinct tap locations on the back case of a smartphone. The BackTap interaction can be used eyes-free with the phone in a user's pocket, purse, or armband while walking, or while holding the phone with two hands so as not to occlude the screen with the fingers. We employ three common built-in sensors on the smartphone (microphone, gyroscope, and accelerometer) and feature a lightweight heuristic implementation. In an evaluation with eleven participants and three usage conditions, users were able to tap four distinct points with 92% to 96% accuracy.",7,61.4678899083
UIST,8fe6cd4ac52474f5fcea2023bad2d3a012e7c4c1,UIST,2001,Join and capture: a model for nomadic interaction,"Dan R. Olsen, S. Travis Nielsen, David Parslow","1733794, 2361287, 8483098",The XWeb architecture delivers interfaces to a wide variety of interactive platforms. XWeb's SUBSCRIBE mechanism allows multiple interactive clients to synchronize with each other. We define the concept of Join as the mechanism for acquiring access to a service's interface. Join also allows the formation of spontaneous collaborations with other people. We define the concept of Capture as the means for users to assemble suites of interactive resources to apply to a particular problem. These mechanisms allow users to access devices that they encounter in their environment rather than carrying all their devices with them. We describe two prototype implementations of Join and Capture. One uses a Java ring to carry a user's identification and to make connections. The other uses a set of cameras to watch where users are and what they touch. Lastly we present algorithms for resolving conflicts generated when independent interactive clients manipulate the same information.,16,23.3333333333
UIST,234dfb0a5a49627d90fdcf040fcc858c26cbb02f,UIST,2016,Orchestrated Informal Care Coordination: Designing a Connected Network of Tools in Support of Collective Care Activities for Informal Caregivers,Kyungmin Youn,3492914,"Often, family caregivers experience difficulties in coordinating older adults' health care because it requires not only a lot of time but also a diverse set of responsibilities to coordinate care for their loved ones. While many can reduce their individual burden by sharing care tasks with other family members, there are still many challenges to overcome in maintaining the quality of care when they work together. As they increase their informal care network, it becomes more difficult for them to stay informed and coordinated. Coordination breakdowns caused by having multiple caregivers who are cooperating to care for the same care recipient result in reduced quality of care. I explored opportunities for ""Internet of Things (IoT)"" technologies to help informal caregivers better coordinate and communicate care with each other for their loved ones. Based on identified design opportunities, I propose the concept of CareBot, a smart home platform consisting of interactive tools in support of collective care activities of family caregivers. \",0,44.6540880503
UIST,14c8d1cae3bebc02de16e66f7634765c70063ff9,UIST,2010,A conversational interface to web automation,"Tessa A. Lau, Julian A. Cerruti, Guillermo Manzato, Mateo N. Bengualid, Jeffrey P. Bigham, Jeffrey Nichols","1800706, 2802890, 2380459, 3028750, 1744846, 1687909","This paper presents CoCo, a system that automates web tasks on a user's behalf through an interactive conversational interface. Given a short command such as ""get road conditions for highway 88,"" CoCo synthesizes a plan to accomplish the task, executes it on the web, extracts an informative response, and returns the result to the user as a snippet of text. A novel aspect of our approach is that we leverage a repository of previously recorded web scripts and the user's personal web browsing history to determine how to complete each requested task. This paper describes the design and implementation of our system, along with the results of a brief user study that evaluates how likely users are to understand what CoCo does for them.",14,70.9302325581
UIST,59337b79ff0fb66c0135366ad050cf9be46fe0ee,UIST,2013,Glassified: an augmented ruler based on a transparent display for real-time interactions with paper,"Anirudh Sharma, Lirong Liu, Pattie Maes","3095422, 7954440, 1701876","We introduce Glassified, a modified ruler with a transparent display to supplement physical strokes made on paper with virtual graphics. Because the display is transparent, both the physical strokes and the virtual graphics are visible in the same plane. A digitizer captures the pen strokes in order to update the graphical overlay, fusing the traditional function of a ruler with the added advantages of a digital, display-based system. We describe use-cases of Glassified in the areas of math and physics and discuss its advantages over traditional systems.",7,61.4678899083
UIST,6ec06089d873a0e5eeb5768d60f4e922f9e7d1e3,UIST,2014,Improvements to keyboard optimization with integer programming,"Andreas Karrenbauer, Antti Oulasvirta","2864176, 2663734","Keyboard optimization is concerned with the design of keyboards for different terminals, languages, user groups, and tasks. Previous work in HCI has used random search based methods, such as simulated annealing. These ""black box"" approaches are convenient, because good solutions are found quickly and no assumption must be made about the objective function. This paper contributes by developing integer programming (IP) as a complementary approach. To this end, we present IP formulations for the letter assignment problem and solve them by branch-and-bound. Although computationally expensive, we show that IP offers two strong benefits. First, its structured non-random search approach improves the out- comes. Second, it guarantees bounds, which increases the designer's confidence over the quality of results. We report improvements to three keyboard optimization cases.",3,49.2248062016
UIST,a393f8cfcc1f7c5b57cc46cf50dd98c73a5209a5,UIST,2012,You can't force calm: designing and evaluating respiratory regulating interfaces for calming technology,"Kanit Wongsuphasawat, Alex Gamburg, Neema Moraveji","8334069, 2232938, 3106937","Interactive systems are increasingly being used to explicitly support change in the user's psychophysiological state and behavior. One trend in this vein is systems that support calm breathing habits. We designed and evaluated techniques to support respiratory regulation to reduce stress and increase parasympathetic tone. Our study revealed that auditory guidance was more effective than visual at creating self-reported calm. We attribute this to the users' ability to effectively map sound to respiration, thereby reducing cognitive load and mental exertion. Interestingly, we found that visual guidance led to more respiratory change but less subjective calm. Thus, motivating users to exert physical or mental efforts may counter the calming effects of slow breathing. Designers of calming technologies must acknowledge the discrepancy between mechanical slow breathing and experiential calm in designing future systems.",2,33.8235294118
UIST,59b67cdc7e759d4b50e9f03126fef2da55841284,UIST,2000,Cross-modal interaction using XWeb,"Dan R. Olsen, Sean Jefferies, S. Travis Nielsen, William Moyes, Paul Fredrickson","1733794, 2150338, 2361287, 2819415, 1844044","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ABSTRACT The XWeb project addresses the problem of interacting with services by means of a variety of interactive platforms. Interactive clients are provided on a variety of hardware/software platforms that can access and XWeb service. Creators of services need not be concerned with interactive techniques or devices. The cross platform problems of a network model of interaction, adaptation to screen size and supporting both speech and visual interfaces in the same model are addressed.",87,72.0
UIST,0658e474a8a0c8715c5743f3250049add8a53a34,UIST,2014,Declarative interaction design for data visualization,"Arvind Satyanarayan, Kanit Wongsuphasawat, Jeffrey Heer","2795670, 8334069, 1803140","Declarative visualization grammars can accelerate development, facilitate retargeting across platforms, and allow language-level optimizations. However, existing declarative visualization languages are primarily concerned with visual encoding, and rely on imperative event handlers for interactive behaviors. In response, we introduce a model of declarative interaction design for data visualizations. Adopting methods from reactive programming, we model low-level events as composable data streams from which we form higher-level semantic signals. Signals feed predicates and scale inversions, which allow us to generalize interactive selections at the level of item geometry (pixels) into interactive queries over the data domain. Production rules then use these queries to manipulate the visualization's appearance. To facilitate reuse and sharing, these constructs can be encapsulated as named interactors: standalone, purely declarative specifications of interaction techniques. We assess our model's feasibility and expressivity by instantiating it with extensions to the Vega visualization grammar. Through a diverse range of examples, we demonstrate coverage over an established taxonomy of visualization interaction techniques.",8,75.9689922481
UIST,2b25e6748cf8810111072253958ba59c1a6d8c65,UIST,2010,Pen + touch = new tools,"Ken Hinckley, Koji Yatani, Michel Pahud, Nicole Coddington, Jenny Rodenhouse, Andy Wilson, Hrvoje Benko, William Buxton","1738072, 3260704, 2025763, 2239453, 2218258, 2598820, 2704133, 6037251","We describe techniques for direct pen+touch input. We observe people's manual behaviors with physical paper and notebooks. These serve as the foundation for a prototype Microsoft Surface application, centered on note-taking and scrapbooking of materials. Based on our explorations we advocate a division of labor between pen and touch: <i>the pen writes, touch manipulates, and the combination of pen + touch yields new tools</i>. This articulates how our system interprets unimodal pen, unimodal touch, and multimodal pen+touch inputs, respectively. For example, the user can hold a photo and drag off with the pen to create and place a copy; hold a photo and cross it in a freeform path with the pen to slice it in two; or hold selected photos and tap one with the pen to staple them all together. Touch thus unifies <i>object selection</i> with <i>mode switching</i> of the pen, while the muscular tension of holding touch serves as the ""glue"" that phrases together all the inputs into a unitary multimodal gesture. This helps the UI designer to avoid encumbrances such as physical buttons, persistent modes, or widgets that detract from the user's focus on the workspace.",63,91.8604651163
UIST,f0da44cebad8f1adc89b71bb2dd589c2d94fe50a,UIST,2014,Sensing techniques for tablet+stylus interaction,"Ken Hinckley, Michel Pahud, Hrvoje Benko, Pourang Irani, François Guimbretière, Marcel Gavriliu, Xiang 'Anthony' Chen, Fabrice Matulic, William Buxton, Andrew Wilson","1738072, 2025763, 2704133, 1773923, 2539134, 2789855, 2028468, 3268147, 6037251, 1792265","We explore grip and motion sensing to afford new techniques that leverage how users naturally manipulate tablet and stylus devices during pen + touch interaction. We can detect whether the user holds the pen in a writing grip or tucked between his fingers. We can distinguish bare-handed inputs, such as drag and pinch gestures produced by the nonpreferred hand, from touch gestures produced by the hand holding the pen, which necessarily impart a detectable motion signal to the stylus. We can sense which hand grips the tablet, and determine the screen's relative orientation to the pen. By selectively combining these signals and using them to complement one another, we can tailor interaction to the context, such as by ignoring unintentional touch inputs while writing, or supporting contextually-appropriate tools such as a magnifier for detailed stroke work that appears when the user pinches with the pen tucked between his fingers. These and other techniques can be used to impart new, previously unanticipated subtleties to pen + touch interaction on tablets.",14,86.8217054264
UIST,f3d99a38031218244246843e2882ca0655920471,UIST,2015,cLuster: Smart Clustering of Free-Hand Sketches on Large Interactive Surfaces,"Florian Perteneder, Martin Bresler, Eva-Maria Grossauer, Joanne Leong, Michael Haller","2243394, 3216572, 2861908, 3281857, 1747712","Structuring and rearranging free-hand sketches on large interactive surfaces typically requires making multiple stroke selections. This can be both time-consuming and fatiguing in the absence of well-designed selection tools. Investigating the concept of automated clustering, we conducted a background study which highlighted the fact that people have varying perspectives on how elements in sketches can and should be grouped. In response to these diverse user expectations, we present cLuster, a flexible, domain-independent clustering approach for free-hand sketches. Our approach is designed to accept an initial user selection, which is then used to calculate a linear combination of pre-trained perspectives in real-time. The remaining elements are then clustered. An initial evaluation revealed that in many cases, only a few corrections were necessary to achieve the desired clustering results. Finally, we demonstrate the utility of our approach in a variety of application scenarios.",1,42.1052631579
UIST,fe1643f540ad90bd5b4459ff0cb189c813a019ed,UIST,2005,Dial and see: tackling the voice menu navigation problem with cross-device user experience integration,"Min Yin, Shumin Zhai","2154262, 1748079","IVR (interactive voice response) menu navigation has long been recognized as a frustrating interaction experience. We propose an IM-based system that sends a coordinated visual IVR menu to the caller's computer screen. The visual menu is updated in real time in response to the caller's actions. With this automatically opened supplementary channel, callers can take advantages of different modalities over different devices and interact with the IVR system with the ease of graphical menu selection. Our approach of utilizing existing network infrastructure to pinpoint the caller's virtual location and coordinating multiple devices and multiple channels based on users' ID registration can also be more generally applied to create integrated user experiences across a group of devices.",8,16.1290322581
UIST,e088d277ecb3a7e39799f6eee15e4141189d0e43,UIST,2016,proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers,"Joanne Leong, Patrick Parzer, Florian Perteneder, Teo Babic, Christian Rendl, Anita Vogl, Hubert Egger, Alex Olwal, Michael Haller","3281857, 2254398, 2243394, 3395844, 2918775, 3396484, 2562436, 2375159, 1747712","Today's commercially available prosthetic limbs lack tactile sensation and feedback. Recent research in this domain focuses on sensor technologies designed to be directly embedded into future prostheses. We present a novel concept and prototype of a prosthetic-sensing wearable that offers a non-invasive, self-applicable and customizable approach for the sensory augmentation of present-day and future low to mid-range priced lower-limb prosthetics. From consultation with eight lower-limb amputees, we investigated the design space for prosthetic sensing wearables and developed novel interaction methods for dynamic, user-driven creation and mapping of sensing regions on the foot to wearable haptic feedback actuators. Based on a pilot-study with amputees, we assessed the utility of our design in scenarios brought up by the amputees and we summarize our findings to establish future directions for research into using smart textiles for the sensory enhancement of prosthetic limbs.",0,44.6540880503
UIST,a21ddc53945d43fe2cdb63178ac3b6e5f88abd7d,UIST,2015,RevoMaker: Enabling Multi-directional and Functionally-embedded 3D printing using a Rotational Cuboidal Platform,"Wei Gao, Yunbo Zhang, Diogo C. Nazzetta, Karthik Ramani, Raymond J. Cipra","1698396, 8700575, 2103269, 1766368, 2731468","In recent years, 3D printing has gained significant attention from the maker community, academia, and industry to support low-cost and iterative prototyping of designs. Current unidirectional extrusion systems require printing sacrificial material to support printed features such as overhangs. Furthermore, integrating functions such as sensing and actuation into these parts requires additional steps and processes to create ""functional enclosures"", since design functionality cannot be easily embedded into prototype printing. All of these factors result in relatively high design iteration times. We present ""RevoMaker"", a self-contained 3D printer that creates direct out-of-the-printer functional prototypes, using less build material and with substantially less reliance on support structures. By modifying a standard low-cost FDM printer with a revolving cuboidal platform and printing partitioned geometries around cuboidal facets, we achieve a multidirectional additive prototyping process to reduce the print and support material use. Our optimization framework considers various orientations and sizes for the cuboidal base. The mechanical, electronic, and sensory components are preassembled on the flattened laser-cut facets and enclosed inside the cuboid when closed. We demonstrate RevoMaker directly printing a variety of customized and fully-functional product prototypes, such as computer mice and toys, thus illustrating the new affordances of 3D printing for functional product design.",6,85.0877192982
UIST,098e3aee2c7e5d5b28ec83c0fb6bd747d467e281,UIST,2011,Real-time crowd control of existing interfaces,"Walter S. Lasecki, Kyle I. Murray, Samuel White, Rob Miller, Jeffrey P. Bigham","2598433, 3177274, 4707811, 1723785, 1744846","Crowdsourcing has been shown to be an effective approach for solving difficult problems, but current crowdsourcing systems suffer two main limitations: (i) tasks must be repackaged for proper display to crowd workers, which generally requires substantial one-off programming effort and support infrastructure, and (ii) crowd workers generally lack a tight feedback loop with their task. In this paper, we introduce Legion, a system that allows end users to easily capture existing GUIs and outsource them for collaborative, real-time control by the crowd. We present mediation strategies for integrating the input of multiple crowd workers in real-time, evaluate these mediation strategies across several applications, and further validate Legion by exploring the space of novel applications that it enables.",83,95.2380952381
UIST,93847034441c8c0d5000b1dc614171c020bddb7c,UIST,2015,Sensing Tablet Grasp + Micro-mobility for Active Reading,"Dongwook Yoon, Ken Hinckley, Hrvoje Benko, François Guimbretière, Pourang Irani, Michel Pahud, Marcel Gavriliu","2055005, 1738072, 2704133, 2539134, 1773923, 2025763, 2789855","The orientation and repositioning of physical artefacts (such as paper documents) to afford shared viewing of content, or to steer the attention of others to specific details, is known as micro-mobility. But the role of grasp in micro-mobility has rarely been considered, much less sensed by devices. We therefore employ capacitive grip sensing and inertial motion to explore the design space of combined grasp + micro-mobility by considering three classes of technique in the context of active reading. Single user, single device techniques support grip-influenced behaviors such as bookmarking a page with a finger, but combine this with physical embodiment to allow flipping back to a previous location. Multiple user, single device techniques, such as passing a tablet to another user or working side-by-side on a single device, add fresh nuances of expression to co-located collaboration. And single user, multiple device techniques afford facile cross-referencing of content across devices. Founded on observations of grasp and micro-mobility, these techniques open up new possibilities for both individual and collaborative interaction with electronic documents.",1,42.1052631579
UIST,e5ea8fc3b15b8e117638586a6ae2f86a36a9992c,UIST,2004,"""Killer App"" of wearable computing: wireless force sensing body protectors for martial arts","Ed Huai-hsin Chi, Jin Song, Greg Corbin","1730922, 8461358, 3211473","Ubiquitous and Wearable Computing both have the goal of pushing the computer into the background, supporting all kinds of human activities. Application areas include areas such as everyday environments (e.g. clothing, home, office), promoting new forms of creative learning via physical/virtual objects, and new tools for interactive design. In this paper, we thrust ubiquitous computing into the extremely hostile environment of the sparring ring of a martial art competition. Our system uses piezoelectric force sensors that transmit signals wirelessly to enable the detection of when a significant impact has been delivered to a competitor's body. The objective is to support the judges in scoring the sparring matches accurately, while preserving the goal of merging and blending into the background of the activity. The system therefore must take into account of the rules of the game, be responsive in real-time asynchronously, and often cope with untrained operators of the system. We present a pilot study of the finished prototype and detail our experience.",34,40.7894736842
UIST,a74fe8664a410d3031a9ed539b1304195f59218e,UIST,2012,Extended multitouch: recovering touch posture and differentiating users using a depth camera,"Sundar Murugappan, Vinayak, Niklas Elmqvist, Karthik Ramani","2077996, 2054621, 1722415, 1766368","Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user's actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of <i>extended multitouch interaction</i> as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user's exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.",14,64.2156862745
UIST,34020db30ffe8df02083d659749acf27e60ff66e,UIST,2010,MobileSurface: interaction in the air for mobile computing,"Ji Zhao, Hujia Liu, Chunhui Zhang, Zhengyou Zhang","3045180, 3198712, 1703809, 1732465","We describe a virtual interactive surface technology based on a projector-camera system connected to a mobile device. This system, named mobile surface, can project images on any free surfaces and enable interaction in the air within the projection area. The projector used in the system scans a laser beam very quickly across the projection area to produce a stable image at 60 fps. The camera-projector synchronization is applied to obtain the image of the appointed scanning line. So our system can project what is perceived as a stable image onto the display surface, while simulta neously working as a structured light 3D scanning system.",3,42.4418604651
UIST,798d3b5558487503e2d39443e8abe2bcaf81ca0e,UIST,2015,"FlexiBend: Enabling Interactivity of Multi-Part, Deformable Fabrications Using Single Shape-Sensing Strip","Chin-yu Chien, Rong-Hao Liang, Long-Fei Lin, Li-Wei Chan, Bing-Yu Chen","2206949, 1705512, 7952684, 1682665, 1733344","This paper presents <i>FlexiBend</i>, an easily installable shape-sensing strip that enables interactivity of multi-part, deformable fabrications. The flexible sensor strip is composed of a dense linear array of strain gauges, therefore it has shape sensing capability. After installation, FlexiBend can simultaneously sense user inputs in different parts of a fabrication or even capture the geometry of a deformable fabrication.",1,42.1052631579
UIST,6c5cca45b3472e29b0bdad3c2cfc23308508fd3a,UIST,2006,Under the table interaction,"Daniel J. Wigdor, Darren Leigh, Clifton Forlines, Sam Shipman, John Barnwell, Ravin Balakrishnan, Chia Shen","1961958, 1786108, 1694854, 2673643, 4867148, 1748870, 1697008","We explore the design space of a two-sided interactive touch table, designed to receive touch input from both the top and bottom surfaces of the table. By combining two registered touch surfaces, we are able to offer a new dimension of input for co-located collaborative groupware. This design accomplishes the goal of increasing the relative size of the input area of a touch table while maintaining its direct-touch input paradigm. We describe the interaction properties of this two-sided touch table, report the results of a controlled experiment examining the precision of user touches to the underside of the table, and a series of application scenarios we developed for use on inverted and two-sided tables. Finally, we present a list of design recommendations based on our experiences and observations with inverted and two-sided tables.",38,60.0
UIST,b29111cb0a181a99464b349c71db1e5cd7e6a4b1,UIST,2009,Virtual shelves: interactions with orientation aware devices,"Frank Chun Yat Li, David Dearman, Khai N. Truong","3182550, 1762768, 1752847","Triggering shortcuts or actions on a mobile device often requires a long sequence of key presses. Because the functions of buttons are highly dependent on the current application's context, users are required to look at the display during interaction, even in many mobile situations when eyes-free interactions may be preferable. We present Virtual Shelves, a technique to trigger programmable shortcuts that leverages the user's spatial awareness and kinesthetic memory. With Virtual Shelves, the user triggers shortcuts by orienting a spatially-aware mobile device within the circular hemisphere in front of her. This space is segmented into definable and selectable regions along the phi and theta planes. We show that users can accurately point to 7 regions on the theta and 4 regions on the phi plane using only their kinesthetic memory. Building upon these results, we then evaluate a proof-of-concept prototype of the Virtual Shelves using a Nokia N93. The results show that Virtual Shelves is faster than the N93's native interface for common mobile phone tasks.",60,80.0
UIST,28d21baad74745e187814d8339ded33e3924252a,UIST,2011,SUAVE: sensor-based user-aware viewing enhancement for mobile device displays,"Robert LiKamWa, Lin Zhong","1791922, 5581056","As mobile devices are used in various environments, ambient light and wide viewing direction impair a display's perceived display quality. To combat these effects, we introduce SUAVE, our Sensor-based User-Aware Viewing Enhancement system. SUAVE senses the ambient light and viewing direction and applies corresponding image enhancements to the display content, increasing its usability. SUAVE employs a parameter calibration process to help users select suitable image enhancements for particular viewing contexts. We report implementations of SUAVE on a Motorola Xoom Tablet and an Apple iPhone 4.",0,6.19047619048
UIST,6662ff773c219661049b8a3d8f4bd354dcd7f756,UIST,2012,A proposal for a MMG-based hand gesture recognition method,"Shumpei Yamakawa, Takuya Nojima","1767105, 1797797","We propose a novel hand-gesture recognition method based on mechanomyograms (MMGs). Skeletal muscles generate sounds specific to their activity. By recording and analyzing these sounds, MMGs provide means to evaluate the activity. Previous research revealed that specific motions produce specific sounds enabling human motion to be classified based on MMGs. In that research, microphones and accelerometers are often used to record muscle sounds. However, environmental conditions such as noise and human motion itself easily overwhelm such sensors. In this paper, we propose to use piezoelectric-based sensing of MMGs to improve robustness from environmental conditions. The preliminary evaluation shows this method is capable of classifying several hand gestures correctly with high accuracy under certain situations.",0,9.80392156863
UIST,30220db3e587e3503f8533e7c1c970434d4b80c3,UIST,2010,Bringing everyday applications to interactive surfaces,Malte Weiss,2563842,"This paper presents ongoing work that intends to simplify the introduction of everyday applications to interactive tabletops. SLAP Widgets bring tangible general-purpose widgets to tabletops while providing the flexibility of on-screen controls. Madgets maintain consistency between physical controls and their digital state. BendDesk represents our vision of a multi-touch enabled office environment. Our pattern language captures knowledge for the design of interactive tabletops. For each project, we describe its technical background, present the current state of research, and discuss future work.",0,9.3023255814
UIST,e80d412dae5eb02b33d37fab3890f49542cf981d,UIST,2014,Depth based interaction and field of view manipulation for augmented reality,Jason Orlosky,1855200,"In recent years, the market for portable devices has seen a large increase in the development of head mounted displays. While these displays provide many benefits to users, safety is still a concern. In particular, ensuring that content does not interfere with everyday activities and that users have adequate peripheral vision is very important for situational awareness. In this paper, I address these issues through the use of two novel display prototypes. The first is an optical see-through multi-focal plane display combined with an eye tracking interface. Through eye tracking and knowledge of the focal plane distances, I can calculate whether a user is looking at the environment or at a focal plane in the display. Any distracting text can then be quickly removed so that he or she has a clear view of the environment. The second prototype is a video see-through display which expands a user's environmental view through the use of 238&#176; ultra wide field of view fisheye lenses. Based on the results of several initial evaluations, these new interfaces have the potential help users improve environmental awareness.",0,12.015503876
UIST,93f22e9d5a7b1cdb98fc1f7ccddadf60ce80151e,UIST,2013,Humans and the coming machine revolution,Raffaello D'Andrea,7997128,"The key components of feedback control systems -- sensors, actuators, computation, power, and communication -- are continually becoming smaller, lighter, more robust, higher performance, and less expensive. By using appropriate algorithms and system architectures, it is thus becoming possible to ""close the loop"" on almost any machine, and to create new capabilities that fully exploit their dynamic potential. In this talk I will discuss various projects -- involving mobile robots, flying machines, an autonomous table, and actuated wingsuits -- where these new machine competencies are interfaced with the ultimate dynamic entities: human beings.",1,27.0642201835
UIST,05146bbd05b72c79fd7949242bdfee1193d513d2,UIST,2000,Jazz: an extensible zoomable user interface graphics toolkit in Java,"Benjamin B. Bederson, Jon Meyer, Lance Good","1799187, 2290970, 2311881","Inn this paper we investigate the use of scene graphs as a general approachh for implementingg two-dimensional (2D) graphical applications, and inn particular Zoomable User Interfaces (ZUIs). Scene graphs are typically foundd in three-dimensional (3D) graphics packages such as Sun's Java3D and SGI's OpenInventor. They have not been widelyadopteddby2Dgraphicaluserinterfacetoolkits.",195,96.0
UIST,0a46f0535e05c5f25b388524125a0ced3b754192,UIST,2001,LetterWise: prefix-based disambiguation for mobile text input,"I. Scott MacKenzie, Hedy Kober, Derek Smith, Terry Jones, Eugene Skepner","1692873, 1693653, 5325933, 5085925, 1696641","A new technique to enter text using a mobile phone keypad is described. For text input, the traditional touchtone phone keypad is ambiguous because each key encodes three or four letters. Instead of using a stored dictionary to guess the intended word, our technique uses probabilities of letter sequences --- ""prefixes"" --- to guess the intended letter. Compared to dictionary-based methods, this technique, called <i>LetterWise,</i> takes significantly less memory and allows entry of non-dictionary words without switching to a special input mode. We conducted a longitudinal study to compare <i>LetterWise</i> to <i>Multitap</i>, the conventional text entry method for mobile phones. The experiment included 20 participants (10 <i>LetterWise</i>, 10 <i>Multitap</i>), and each entered phrases of text for 20 sessions of about 30 minutes each. Error rates were similar between the techniques; however, by the end of the experiment the mean entry speed was 36% faster with <i>LetterWise</i> than with <i>Multitap</i>.",105,70.0
UIST,0c1a8648bd0c4b1702140ef51a33d10fdd5986d7,UIST,2010,Jogging over a distance between Europe and Australia,"Florian Mueller, Frank Vetere, Martin R. Gibbs, Darren Edge, Stefan Agamanolis, Jennifer G. Sheridan","1710544, 1812707, 1749415, 1741074, 1729019, 2075650","Exertion activities, such as jogging, require users to invest intense physical effort and are associated with physical and social health benefits. Despite the benefits, our understanding of exertion activities is limited, especially when it comes to social experiences. In order to begin understanding how to design for technologically augmented social exertion experiences, we present ""Jogging over a Distance"", a system in which spatialized audio based on heart rate allowed runners as far apart as Europe and Australia to run together. Our analysis revealed how certain aspects of the design facilitated a social experience, and consequently we describe a framework for designing augmented exertion activities. We make recommendations as to how designers could use this framework to aid the development of future social systems that aim to utilize the benefits of exertion.",44,88.3720930233
UIST,c902ed5ef7e5db43480748bb8173c471467242c8,UIST,2008,An infrastructure for extending applications' user experiences across multiple personal devices,"Jeffrey S. Pierce, Jeffrey Nichols","1783827, 1687909","Users increasingly interact with a heterogeneous collection of computing devices. The applications that users employ on those devices, however, still largely provide user experiences that assume the use of a single computer. This failure is due in part to the difficulty of creating user experiences that span multiple devices, particularly the need to manage identifying, connecting to, and communicating with other devices. In this paper we present an infrastructure based on instant messaging that simplifies adding that additional functionality to applications. Our infrastructure elevates device ownership to a first class property, allowing developers to provide functionality that spans personal devices without writing code to manage users' devices or establish connections among them. It also provides simple mechanisms for applications to send information, events, or commands between a user's devices. We demonstrate the effectiveness of our infrastructure by presenting a set of sample applications built with it and a user study demonstrating that developers new to the infrastructure can implement all of the cross-device functionality for three applications in, on average, less than two and a half hours.",22,45.7142857143
UIST,4467f59e594c9d0c2919bdac44388633bf8ba353,UIST,2013,A colorful approach to text processing by example,"Kuat Yessenov, Shubham Tulsiani, Aditya Krishna Menon, Rob Miller, Sumit Gulwani, Butler W. Lampson, Adam Tauman Kalai","3171961, 2757335, 2844480, 1723785, 2108314, 2665014, 2186481","Text processing, tedious and error-prone even for programmers, remains one of the most alluring targets of Programming by Example. An examination of real-world text processing tasks found on help forums reveals that many such tasks, beyond simple string manipulation, involve latent hierarchical structures.
 We present STEPS, a programming system for processing structured and semi-structured text by example. STEPS users create and manipulate hierarchical structure by example. In a between-subject user study on fourteen computer scientists, STEPS compares favorably to traditional programming.",12,71.1009174312
UIST,9d0b7a6b4cacf439abacb2e70c2ec0d848c3ca4a,UIST,2014,Understanding the design of a flying jogging companion,"Florian Mueller, Matthew Muirhead","1710544, 2576210","Jogging can offer many health benefits, and mobile phone apps have recently emerged that aim to support the jogging experience. We believe that jogging is an embodied experience, and therefore present a contrasting approach to these existing systems by arguing that any supporting technology should also take on an embodied approach. In order to exemplify this approach, we detail the technical specifications of a flying quadcopter that has successfully been used with joggers in order to explore the design of embodied systems to support physical exertion activities. Based on interviews with five joggers running with our system, we present preliminary insights about the experience of jogging with a flying robot. With our work, we hope to inspire and guide designers who are interested in developing embodied systems to support exertion activities.",3,49.2248062016
UIST,04e281c9b5daea353b1e733d6f8f45c1ed6bcb10,UIST,2010,Intelligent tagging interfaces: beyond folksonomy,Jesse Vig,2056908,"This paper summarizes our work on using tags to broaden the dialog between a recommender system and its users. We present two tagging applications that enrich this dialog: <i>tagsplanations</i> are tag-based explanations of recommendations provided by a system to its users, and <i>Movie Tuner</i> is a conversational recommender system that enables users to provide feedback on movie recommendations using tags. We discuss the design of both systems and the experimental methodology used to evaluate the design choices.",0,9.3023255814
UIST,54b0812ef4204bfc3194756262fc57ff51fcfcab,UIST,2004,Haptic pen: a tactile feedback stylus for touch screens,"Johnny C. Lee, Paul H. Dietz, Darren Leigh, William S. Yerazunis, Scott E. Hudson","1803308, 1805795, 1786108, 5135329, 1749296",In this paper we present a system for providing tactile feedback for stylus-based touch-screen displays. The Haptic Pen is a simple low-cost device that provides individualized tactile feedback for multiple simultaneous users and can operate on large touch screens as well as ordinary surfaces. A pressure-sensitive stylus is combined with a small solenoid to generate a wide range of tactile sensations. The physical sensations generated by the Haptic pen can be used to enhance our existing interaction with graphical user interfaces as well as to help make modern computing systems more accessible to those with visual or motor impairments.,73,78.9473684211
UIST,9de54e51e80c262168db2f5961bebf540c20efeb,UIST,2016,Hilbert Curves: A Tool for Resolution Independent Haptic Texture,"William Frier, Kyoungwon Seo, Sriram Subramanian","3492490, 8219571, 1702794","Haptic systems usually stimulate the kinesthetic aspects of the sense of touch, i.e. force feedback systems. But more and more devices aim to stimulate the cutaneous part of the sense of touch to reproduce more complex tactile sensations. To do so, they stimulate one's fingertip in different locations, usually in the fashion of a matrix pattern. In this paper we investigate the new possibilities that are offered by such a framework and present an ongoing project that investigates the benefits of Hilbert curves to display resolution independent mid-air haptic textures in comparison with other implementation approaches.",0,44.6540880503
UIST,7fc9f8f4ce34b1c37d266a5e9464f2af41e261c0,UIST,2002,"The ""mighty mouse"" multi-screen collaboration tool","Kellogg S. Booth, Brian D. Fisher, Chi Jui Raymond Lin, Ritchie Argue","1800617, 1807900, 1916244, 2439215","Many computer operating systems provide seamless support for multiple display screens, but there are few cross-platform tools for collaborative use of multiple computers in a shared display environment. Mighty Mouse is a novel groupware tool built on the public domain VNC protocol. It is tailored specifically for face-to-face collaboration where multiple heterogeneous computers (usually laptops) are viewed simultaneously (usually via projectors) by people working together on a variety of applications under various operating systems. Mighty Mouse uses only the remote input capability of VNC, but enhances this with various features to support flexible movement between the various platforms, ""floor control"" to facilitate smooth collaboration, and customization features to accommodate different user, platform, and application preferences in a relatively seamless manner. The design rationale arises from specific observations about how people collaborate in meetings, which allows certain simplifying assumptions to be made in the implementation.",39,54.1666666667
UIST,bae9ee8c1cffc9f075b866201046c69d69f9e855,UIST,2010,Performance optimizations of virtual keyboards for stroke-based text entry on a touch-based tabletop,Jochen Rick,1706994,"Efficiently entering text on interactive surfaces, such as touch-based tabletops, is an important concern. One novel solution is <i>shape writing</i> - the user strokes through all the letters in the word on a virtual keyboard without lifting his or her finger. While this technique can be used with any keyboard layout, the layout does impact the expected performance. In this paper, I investigate the influence of keyboard layout on expert text-entry performance for stroke-based text entry. Based on empirical data, I create a model of stroking through a series of points based on Fitts's law. I then use that model to evaluate various keyboard layouts for both tapping and stroking input. While the stroke-based technique seems promising by itself (i.e., there is a predicted gain of 17.3% for a Qwerty layout), significant additional gains can be made by using a more-suitable keyboard layout (e.g., the OPTI II layout is predicted to be 29.5% faster than Qwerty).",12,66.8604651163
UIST,3cdf845761cbba7850478511b8047a39eae097af,UIST,2008,ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models,"Seok-Hyung Bae, Ravin Balakrishnan, Karan Singh","1715434, 1748870, 1682205","We present ILoveSketch, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke NURBS curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.",110,97.1428571429
UIST,65a156ea5b272569d6428e05c07bae7508831f4b,UIST,2015,Graphical Passwords for Older Computer Users,Nancy J. Carter,3349446,"Computers and the internet have been challenging for many computer users over the age of 60. We conducted a survey of older users which revealed that the creation, management and recall of strong text passwords were some of the challenging aspects of modern technology. In practice, this user group based passwords on familiar facts such as family member names, pets, phone numbers and important personal dates. Graphical passwords formed from abstract graphical symbols or anonymous facial images are feasible, but harder for older computers users to grasp and recall. In this paper we describe initial results for our graphical password system based on recognition of culturally-familiar facial images that are age-relevant to the life experiences of older users. Our goals are to design an easy-to-memorize, graphical password system intended specifically for older users, and achieve a level of password entropy comparable to traditional PINs and text passwords. We are also conducting a user study to demonstrate our technique and capture performance and recall metrics for comparison with traditional password systems.",0,16.2280701754
UIST,44af0a07686840f0205a6f8ce21cdbe965a488d3,UIST,2007,"Continuum: designing timelines for hierarchies, relationships and scale","Paul André, Max L. Wilson, Alistair Russell, Daniel A. Smith, Alisdair Owens, Monica M. C. Schraefel","2411694, 8530169, 2085536, 2649789, 2734410, 2284695","Temporal events, while often discrete, also have interesting relationships within and across times: larger events are often collections of smaller more discrete events (battles within wars; artists' works within a form); events at one point also have correlations with events at other points (a play written in one period is related to its performance over a period of time). Most temporal visualisations, however, only represent discrete data points or single data types along a single timeline: this event started here and ended there; this work was published at this time; this tag was popular for this period. In order to represent richer, faceted attributes of temporal events, we present Continuum. Continuum enables hierarchical relationships in temporal data to be represented and explored; it enables relationships between events across periods to be expressed, and in particular it enables user-determined control over the level of detail of any facet of interest so that the person using the system can determine a focus point, no matter the level of zoom over the temporal space. We present the factors motivating our approach, our evaluation and implementation of this new visualisation which makes it easy for anyone to apply this interface to rich, large-scale datasets with temporal data.",35,66.6666666667
UIST,7f5c32ff1c21c1650182a800c33201548f5cb2f7,UIST,2016,SkyAnchor: Optical Design for Anchoring Mid-air Images onto Physical Objects,"Hajime Kajita, Naoya Koizumi, Takeshi Naemura","2838092, 2493346, 1737985","For glass-free mixed reality (MR), mid-air imaging is a promising way of superimposing a virtual image onto a real object. We focus on attaching virtual images to non-static real life objects. In previous work, moving the real object causes latency in the superimposing system, and the virtual image seems to follow the object with a delay. This is caused by delays due to sensors, displays and computational devices for position sensing, and occasionally actuators for moving the image generation source. In order to avoid this problem, this paper proposes to separate the object-anchored imaging effect from the position sensing. Our proposal is a retro-reflective system called ""SkyAnchor,"" which consists of only optical devices: two mirrors and an aerial-imaging plate. The system reflects light from a light source anchored under the physical object itself, and forms an image anchored around the object. This optical solution does not cause any latency in principle and is effective for high-quality mixed reality applications. We consider two types of light sources to be attached to physical objects: reflecting content from a touch table on which the object rests, or attaching the source directly on the object. As for position sensing, we utilize a capacitive marker on the bottom of the object, tracked on a touch table. We have implemented a prototype, where mid-air images move with the object, and whose content may change based on its position.",1,92.7672955975
UIST,4894435e4e96b7ddcc42853992da4c694711c5fb,UIST,2003,Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays,"Mike Wu, Ravin Balakrishnan","1700653, 1748870","Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called <i>RoomPlanner</i>.",309,100.0
UIST,6a69628c9e6971db065e1a6458cf9f36e7e6d473,UIST,2016,Wearables as Context for Guiard-abiding Bimanual Touch,"Andrew M. Webb, Michel Pahud, Ken Hinckley, William Buxton","1863950, 2025763, 1738072, 6037251","We explore the contextual details afforded by wearable devices to support multi-user, direct-touch interaction on electronic whiteboards in a way that-unlike previous work-can be fully consistent with natural bimanual-asymmetric interaction as set forth by Guiard.
 Our work offers the following key observation. While Guiard's framework has been widely applied in HCI, for bimanual interfaces where each hand interacts via direct touch, subtle limitations of multi-touch technologies as well as limitations in conception and design-mean that the resulting interfaces often cannot fully adhere to Guiard's principles even if they want to. The interactions are fundamentally ambiguous because the system does not know which hand, left or right, contributes each touch. But by integrating additional context from wearable devices, our system can <i>identify which user</i> is touching, as well as <i>distinguish what hand</i> they use to do so. This enables our prototypes to respect lateral preference the assignment of natural roles to each hand as advocated by Guiard in a way that has not been articulated before.",0,44.6540880503
UIST,76c3cd71c877ea61ea71619d1d5aa96301f8abd3,UIST,2005,Automatic image retargeting with fisheye-view warping,"Feng Liu, Michael Gleicher","1734409, 1776507","Image retargeting is the problem of adapting images for display on devices different than originally intended. This paper presents a method for adapting large images, such as those taken with a digital camera, for a small display, such as a cellular telephone. The method uses a non-linear fisheye-view warp that emphasizes parts of an image while shrinking others. Like previous methods, fisheye-view warping uses image information, such as low-level salience and high-level object recognition to find important regions of the source image. However, unlike prior approaches, a non-linear image warping function emphasizes the important aspects of the image while retaining the surrounding context. The method has advantages in preserving information content, alerting the viewer to missing information and providing robustness.",87,83.8709677419
UIST,26f58ae70f392ec94f05d5ab97d7365b5937136d,UIST,2004,Automatic projector calibration with embedded light sensors,"Johnny C. Lee, Paul H. Dietz, Dan Maynes-Aminzade, Ramesh Raskar, Scott E. Hudson","1803308, 1805795, 2245650, 1717566, 1749296","Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.",40,55.2631578947
UIST,f1d6ae063df4524528992f87e25cc8fde1f50117,UIST,2012,Lost in the dark: emotion adaption,"Ryan Bernays, Jeremy Mone, Patty Yau, Michael Murcia, Javier Gonzalez Sanchez, Maria Elena Chavez Echeagaray, Robert Christopherson, Robert K. Atkinson","3205363, 3296862, 3152066, 3255214, 1764830, 1801005, 2727391, 1737845","Having environments that are able to adjust accordingly with the user has been sought in the last years particularly in the area of Human Computer Interfaces. Environments able to recognize the user emotions and react in consequence have been of interest on the area of Affective Computing. This work presents a project -- an adaptable 3D video game, Lost in the Dark: Emotion Adaption, which uses user's emotions as input to alter and adjust the gaming environment. To achieve this, an interface that is capable of reading brain waves, facial expressions, and head motion was used, an Emotiv&#174; EPOC headset. For our purposes we read emotions such as meditation, excitement, and engagement into the game, altering the lighting, music, gates, colors, and other elements that would appeal to the user emotional state. With this, we achieve closing the loop of using the emotions as inputs, adjusting a system accordingly as a result, and elicit emotions.",3,40.1960784314
UIST,7a38cc3e21e3da47aaf809cf14823d8b59b374a8,UIST,2014,Inkantatory paper: dynamically color-changing prints with multiple functional inks,"Takahiro Tsujii, Naoya Koizumi, Takeshi Naemura","2221822, 2493346, 1737985","We propose an effective combination of multiple functional inks, including conductive silver ink, thermo-chromic ink, and regular inkjet ink, for a novel paper-based interface called Inkantatory Paper that can dynamically change the color of its printed pattern. Constructed with off-the-shelf inkjet printing using silver conductive ink, our system enables users to fabricate thin, flat, flexible, and low-cost interactive paper. We evaluated the characteristics of the conductive silver ink as a heating system for the thermo-chromic ink and created applications demonstrating the usability of the system.",0,12.015503876
UIST,a27f90b7cf7e68dec8fd19316ae6d2437ae0e9ed,UIST,2013,Integrated visual representations for programming with real-world input and output,Jun Kato,3200463,"As computers become more pervasive, more programs deal with real-world input and output (real-world I/O) such as processing camera images and controlling robots. The real-world I/O usually contains complex data hardly represented by text or symbols, while most of the current integrated development environments (IDEs) are equipped with text-based editors and debuggers. My thesis investigates how visual representations of the real world can be integrated within the text-based development environment to enhance the programming experience. In particular, we have designed and implemented IDEs for three scenarios, all of which make use of photos and videos representing the real world. Based on these experiences, we discuss ""programming with example data,"" a technique where the programmer demonstrates examples to the IDE and writes text-based code with support of the examples.",1,27.0642201835
UIST,0a0230c1fba56f273afad3e7907b716670fb387f,UIST,2016,Making Fabrication Real,Xiang 'Anthony' Chen,2028468,"Low-cost, easy-to-use 3D printers have promised to empower everyday users with the ability to fabricate physical objects of their own design. While these printers specialize in building objects from scratch, they are innately oblivious to the real world in which the printed objects will be situated and in use. In my thesis research, I develop fabrication techniques with tool integration to enable users to expressively specify how a design can be attached to, augment, adapt, support, or otherwise function with existing real world objects. In this paper, I describe projects to date as well as ongoing work that explores this space of research.",0,44.6540880503
UIST,076b710d9977eb0ad6d367431864b15caf785104,UIST,2006,Summarizing personal web browsing sessions,"Mira Dontcheva, Steven M. Drucker, Geraldine Wade, David Salesin, Michael F. Cohen","2875493, 2311676, 2648878, 1745260, 1694613","We describe a system, implemented as a browser extension, that enables users to quickly and easily collect, view, and share personal Web content. Our system employs a novel interaction model, which allows a user to specify webpage extraction patterns by interactively selecting webpage elements and applying these patterns to automatically collect similar content. Further, we present a technique for creating visual summaries of the collected information by combining user labeling with predefined layout templates. These summaries are interactive in nature: depending on the behaviors encoded in their templates, they may respond to mouse events, in addition to providing a visual summary. Finally, the summaries can be saved or sent to others to continue the research at another place or time. Informal evaluation shows that our approach works well for popular websites, and that users can quickly learn this interaction model for collecting content from the Web.",43,68.75
UIST,7366ec6df1ca36f959a0448c667ad7bcbcaccec7,UIST,2007,"Relations, cards, and search templates: user-guided web data integration and layout","Mira Dontcheva, Steven M. Drucker, David Salesin, Michael F. Cohen","2875493, 2311676, 1745260, 1694613","We present three new interaction techniques for aiding users in collecting and organizing Web content. First, we demonstrate an interface for creating associations between websites, which facilitate the automatic retrieval of related content. Second, we present an authoring interface that allows users to quickly merge content from many different websites into a uniform and personalized representation, which we call a card. Finally, we introduce a novel search paradigm that leverages the relationships in a card to direct search queries to extract relevant content from multiple Web sources and fill a new series of cards instead of just returning a list of webpage URLs. Preliminary feedback from users is positive andvalidates our design.",32,61.1111111111
UIST,7a3f6684f4b07445a313cbc02b23fde5c6131b68,UIST,2005,ViewPointer: lightweight calibration-free eye tracking for ubiquitous handsfree deixis,"John D. Smith, Roel Vertegaal, Changuk Sohn","2372212, 1687608, 1684112","We introduce ViewPointer, a wearable eye contact sensor that detects deixis towards ubiquitous computers embedded in real world objects. ViewPointer consists of a small wearable camera no more obtrusive than a common Bluetooth headset. ViewPointer allows any real-world object to be augmented with eye contact sensing capabilities, simply by embedding a small infrared (IR) tag. The headset camera detects when a user is looking at an infrared tag by determining whether the reflection of the tag on the cornea of the user's eye appears sufficiently central to the pupil. ViewPointer not only allows any object to become an eye contact sensing appliance, it also allows identification of users and transmission of data to the user through the object. We present a novel encoding scheme used to uniquely identify ViewPointer tags, as well as a method for transmitting URLs over tags. We present a number of scenarios of application as well as an analysis of design principles. We conclude eye contact sensing input is best utilized to provide context to action.",25,45.1612903226
UIST,3337f9fbc76b08c3235dadc0c3bd159fd2cc8d41,UIST,2014,Tangential force input for touch panels using bezel-aligned elastic pillars and a transparent sheet,"Yuriko Nakai, Shinya Kudo, Ryuta Okazaki, Hiroyuki Kajimoto","3279657, 3087190, 2812591, 1776927","This research aims to enable tangential force input for touch panels by measuring the tangential force. The system is composed of a plastic sheet on a touch panel, urethane pillars on the panel that are aligned at the four corners of the bezel, and a case on top of the pillars. When the sheet moves with a finger, the pillars deform so that a tangential force can be obtained by measuring the movement of the finger. We evaluated the method and found that the system showed realistic force sensing accuracy in any direction. This input method will enable development of new applications for touch panels such as using any part of the touch panel surface as joysticks, or modeling virtual objects by deforming them with the fingers.",0,12.015503876
UIST,2af574ad2cda1983cad260d5ebd3b38b57fa3984,UIST,2010,Shoe-shaped i/o interface,"Hideaki Higuchi, Takuya Nojima","2094905, 1797797","In this research, we propose a shoe-shaped I/O interface. The benefits to users of wearable devices are significantly reduced if they are aware of them. Wearable devices should have the ability to be worn without requiring any attention from the user. However, previous wearable systems required users to be careful and be aware of wearing or carrying them. To solve this problem, we propose a shoe-shaped I/O interface. By wearing the shoes throughout the day, users soon cease to be conscious of them. Electromechanical devices are potentially easy to install in shoes. This report describes the concept of a shoe-shaped I/O interface, the development of a prototype system, and possible applications.",5,54.0697674419
UIST,4b6ad660cd85a32cbda33d0e2da67f5a62c00467,UIST,2015,Perspective-dependent Indirect Touch Input for 3D Polygon Extrusion,"Henri Palleis, Julie Wagner, Heinrich Hußmann","3062440, 6478329, 3952330","We present a two-handed indirect touch interaction technique for the extrusion of polygons within a 3D modeling tool that we have built for a horizontal/vertical dual touch screen setup. In particular, we introduce perspective-dependent touch gestures: using several graphical input areas on the horizontal display, the non-dominant hand navigates the virtual camera and thus continuously updates the spatial frame of reference within which the dominant hand performs extrusions with dragging gestures.",1,42.1052631579
UIST,aef7722fae335015593c5d224011da4ca0ae7c52,UIST,2015,Machine Intelligence and Human Intelligence,Blaise Agüera y Arcas,2661025,"There has been a stellar rise in computational power since 2006 in part thanks to GPUs, yet today, we are as an intelligent species essentially singular. There are of course some other brainy species, like chimpanzees, dolphins, crows and octopuses, but if anything they only emphasize our unique position on Earth -- as animals richly gifted with self-awareness, language, abstract thought, art, mathematical capability, science, technology and so on. Many of us have staked our entire self-concept on the idea that to be human is to have a mind, and that minds are the unique province of humans. For those of us who are not religious, this could be interpreted as the last bastion of dualism. Our economic, legal and ethical systems are also implicitly built around this idea. Now, we're well along the road to really understanding the fundamental principles of how a mind can be built, and Moore's Law will put brain-scale computing within reach this decade. (We need to put some asterisks next to Moore's Law, since we are already running up against certain limits in computational scale using our present-day approaches, but I'll stand behind the broader statement.) In this talk I will discuss the relationships between engineered neurally inspired systems and brains today, between humans and machines tomorrow, and how these relationships will alter user interfaces, software and technology.",0,16.2280701754
UIST,2ef8e2508ecb5375ab87b40f7572782e92e5ab18,UIST,2004,SHARK2: a large vocabulary shorthand writing system for pen-based computers,"Per Ola Kristensson, Shumin Zhai","1683133, 1748079","Zhai and Kristensson (2003) presented a method of speed-writing for pen-based computing which utilizes gesturing on a stylus keyboard for familiar words and tapping for others. In SHARK&#60;sup>2&#60;/sup>:, we eliminated the necessity to alternate between the two modes of writing, allowing any word in a large vocabulary (e.g. 10,000-20,000 words) to be entered as a shorthand gesture. This new paradigm supports a gradual and seamless transition from visually guided tracing to recall-based gesturing. Based on the use characteristics and human performance observations, we designed and implemented the architecture, algorithms and interfaces of a high-capacity multi-channel pen-gesture recognition system. The system's key components and performance are also reported.",126,96.0526315789
UIST,b95bc5a0a8199d4fb34dc67974fae85438c2037e,UIST,2015,LaserStacker: Fabricating 3D Objects by Laser Cutting and Welding,"Udayan Umapathi, Hsiang-Ting Chen, Stefanie Müller, Ludwig Wall, Anna Seufert, Patrick Baudisch","2754729, 3327016, 4942906, 1967682, 2377602, 1729393","Laser cutters are useful for rapid prototyping because they are fast. However, they only produce planar 2D geometry. One approach to creating non-planar objects is to cut the object in horizontal slices and to stack and glue them. This approach, however, requires manual effort for the assembly and time for the glue to set, defeating the purpose of using a fast fabrication tool. We propose eliminating the assembly step with our system LaserStacker. The key idea is to use the laser cutter to not only cut but also to weld. Users place not one acrylic sheet, but a stack of acrylic sheets into their cutter. In a single process, LaserStacker cuts each individual layer to shape (through all layers above it), welds layers by melting material at their interface, and heals undesired cuts in higher layers. When users take out the object from the laser cutter, it is already assembled. To allow users to model stacked objects efficiently, we built an extension to a commercial 3D editor (SketchUp) that provides tools for defining which parts should be connected and which remain loose. When users hit the export button, LaserStacker converts the 3D model into cutting, welding, and healing instructions for the laser cutter. We show how LaserStacker does not only allow making static objects, such as architectural models, but also objects with moving parts and simple mechanisms, such as scissors, a simple pinball machine, and a mechanical toy with gears.",1,42.1052631579
UIST,0de00263338b6c79b0fe0b90702a6763df85fe4b,UIST,2001,DiamondTouch: a multi-user touch technology,"Paul H. Dietz, Darren Leigh","1805795, 1786108","A technique for creating a touch-sensitive input device is proposed which allows multiple, simultaneous users to interact in an intuitive fashion. Touch location information is determined independently for each user, allowing each touch on a common surface to be associated with a particular user. The surface generates location dependent, modulated electric fields which are capacitively coupled through the users to receivers installed in the work environment. We describe the design of these systems and their applications. Finally, we present results we have obtained with a small prototype device.",736,100.0
UIST,230556cb62c1b97754d9d9949040fc2b8b902af8,UIST,2014,"ShrinkyCircuits: sketching, shrinking, and formgiving for electronic circuits","Joanne Lo, Eric Paulos","8551648, 2749792","In this paper we describe the development of ShrinkyCircuits, a novel electronic prototyping technique that captures the flexibility of sketching and leverages properties of a common everyday plastic polymer to enable low-cost, miniature, planar, and curved, multi-layer circuit designs in minutes. ShrinkyCircuits take advantage of inexpensive prestressed polymer film that shrinks to its original size when exposed to heat. This enables improved electrical characteristics though sintering of the conductive electrical layer, partial self-assembly of the circuit and components, and mechanically robust custom shapes Including curves and non-planar form factors. We demonstrate the range and adaptability of ShrinkyCircuits designs from simple hand drawn circuits with through-hole components to complex multilayer, printed circuit boards (PCB), with curved and irregular shaped electronic layouts and surface mount components. Our approach enables users to create extremely customized circuit boards with dense circuit layouts while avoiding messy chemical etching, expensive board milling machines, or time consuming delays in using outside PCB production houses.",3,49.2248062016
UIST,2a8922f184b5bcc4f88929eb96e7779cbc234da3,UIST,2009,EverybodyLovesSketch: 3D sketching for a broader audience,"Seok-Hyung Bae, Ravin Balakrishnan, Karan Singh","1715434, 1748870, 1682205","We present EverybodyLovesSketch, a gesture-based 3D curve sketching system for rapid ideation and visualization of 3D forms, aimed at a broad audience. We first analyze traditional perspective drawing in professional practice. We then design a system built upon the paradigm of ILoveSketch, a 3D curve drawing system for design professionals. The new system incorporates many interaction aspects of perspective drawing with judicious automation to enable novices with no perspective training to proficiently create 3D curve sketches. EverybodyLovesSketch supports a number of novel interactions: tick-based sketch plane selection, single view definition of arbitrary extrusion vectors, multiple extruded surface sketching, copy-and-project of 3D curves, freeform surface sketching, and an interactive perspective grid. Finally, we present a study involving 49 high school students (with no formal artistic training) who each learned and used the system over 11 days, which provides detailed insights into the popularity, power and usability of the various techniques, and shows our system to be easily learnt and effectively used, with broad appeal.",22,42.8571428571
UIST,334966be9066b02042100af4fad413796bf0eb89,UIST,2014,Swipeboard: a text entry technique for ultra-small interfaces that supports novice to expert transitions,"Xiang 'Anthony' Chen, Tovi Grossman, George W. Fitzmaurice","2028468, 3313809, 1703735","Ultra-small smart devices, such as smart watches, have become increasingly popular in recent years. Most of these devices rely on touch as the primary input modality, which makes tasks such as text entry increasingly difficult as the devices continue to shrink. In the sole pursuit of entry speed, the ultimate solution is a shorthand technique (e.g., Morse code) that sequences tokens of input (e.g., key, tap, swipe) into unique representations of each character. However, learning such techniques is hard, as it often resorts to rote memory. Our technique, Swipeboard, leverages our spatial memory of a QWERTY keyboard to learn, and eventually master a shorthand, eyes-free text entry method designed for ultra-small interfaces. Characters are entered with two swipes; the first swipe specifies the region where the character is located, and the second swipe specifies the character within that region. Our study showed that with less than two hours' training, Tested on a reduced word set, Swipeboard users achieved 19.58 words per minute (WPM), 15% faster than an existing baseline technique.",23,96.1240310078
UIST,6f8abab9be6f1a2c650335b535865604340b9768,UIST,2014,Dyadic projected spatial augmented reality,"Hrvoje Benko, Andrew D. Wilson, Federico Zannier","2704133, 1767449, 1846772","Mano-a-Mano is a unique spatial augmented reality system that combines dynamic projection mapping, multiple perspective views and device-less interaction to support face to face, or dyadic, interaction with 3D virtual objects. Its main advantage over more traditional AR approaches, such as handheld devices with composited graphics or see-through head worn displays, is that users are able to interact with 3D virtual objects and each other without cumbersome devices that obstruct face to face interaction. We detail our prototype system and a number of interactive experiences. We present an initial user experiment that shows that participants are able to deduce the size and distance of a virtual projected object. A second experiment shows that participants are able to infer which of a number of targets the other user indicates by pointing.",11,82.9457364341
UIST,5a740a65e44bf5ee64171041c4e9b0394341ef0f,UIST,2008,Backward highlighting: enhancing faceted search,"Max L. Wilson, Paul André, Monica M. C. Schraefel","8530169, 2411694, 2284695","Directional faceted browsers, such as the popular column browser iTunes, let a person pick an instance from any column-facet to start their search for music. The expected effect is that any columns to the right are filtered. In keeping with this directional filtering from left to right, however, the unexpected effect is that the columns to the left of the click provide no information about the possible associations to the selected item. In iTunes, this means that any selection in the Album column on the right returns no information about either the Artists (immediate left) or Genres (leftmost) associated with the chosen album.
 Backward Highlighting (BH) is our solution to this problem, which allows users to see and utilize, during search, associations in columns to the left of a selection in a directional column browser like iTunes. Unlike other possible solutions, this technique allows such browsers to keep direction in their filtering, and so provides users with the best of both directional and non-directional styles. As well as describing BH in detail, this paper presents the results of a formative user study, showing benefits for both information discovery and subsequent retention in memory.",10,21.4285714286
UIST,4cc5a38191d876389c135019dd1570f27739ea45,UIST,2013,Touch scrolling transfer functions,"Philip Quinn, Sylvain Malacria, Andy Cockburn","1825818, 1702364, 1814003","Touch scrolling systems use a transfer function to transform gestures on a touch sensitive surface into scrolling output. The design of these transfer functions is complex as they must facilitate precise direct manipulation of the underlying content as well as rapid scrolling through large datasets. However, researchers' ability to refine them is impaired by: (1) limited understanding of how users express scrolling intentions through touch gestures; (2) lack of knowledge on proprietary transfer functions, causing researchers to evaluate techniques that may misrepresent the state of the art; and (3) a lack of tools for examining existing transfer functions. To address these limitations, we examine how users express scrolling intentions in a human factors experiment; we describe methods to reverse engineer existing `black box' transfer functions, including use of an accurate robotic arm; and we use the methods to expose the functions of Apple iOS and Google Android, releasing data tables and software to assist replication. We discuss how this new understanding can improve experimental rigour and assist iterative improvement of touch scrolling.",9,65.1376146789
UIST,a2e3642a1ab591b67afbeee242bc7118b7b3e50f,UIST,2016,Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum,"Saiwen Wang, Jie Song, Jaime Lien, Ivan Poupyrev, Otmar Hilliges","3491983, 1719329, 3352068, 1736819, 2531379","This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60]GHz), short-range radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87%) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).",1,92.7672955975
UIST,524dc446d11715bf8140ab2bb5725ccedb7ce5f5,UIST,2015,Reconfiguring and Fabricating Special-Purpose Tangible Controls,Raf Ramakers,1950213,"Unlike regular interfaces on touch screens or desktop computers, tangible user interfaces allow for more physically rich interactions that better uses the capacity of our motor system. On the flipside, the physicality of tangibles comes with rigidity. This makes it hard to (1) use tangibles on systems that require a variety of controls and interaction styles, and (2) make changes to physical interfaces once manufactured. In my research, I explore techniques that allow users to reconfigure and fabricate tangible interfaces in order to mitigate these issues.",0,16.2280701754
UIST,2fcd29e44b23e6c2efc98474f51486f7d80c581a,UIST,2007,E-conic: a perspective-aware interface for multi-display environments,"Miguel A. Nacenta, Satoshi Sakurai, Tokuo Yamaguchi, Yohei Miki, Yuichi Itoh, Yoshifumi Kitamura, Sriram Subramanian, Carl Gutwin","2930601, 2271149, 2872429, 2192947, 1707181, 1690228, 1702794, 1693768","Multi-display environments compose displays that can be at different locations from and different angles to the user; as a result, it can become very difficult to manage windows, read text, and manipulate objects. We investigate the idea of perspective as a way to solve these problems in multi-display environments. We first identify basic display and control factors that are affected by perspective, such as visibility, fracture, and sharing. We then present the design and implementation of E-conic, a multi-display multi-user environment that uses location data about displays and users to dynamically correct perspective. We carried out a controlled experiment to test the benefits of perspective correction in basic interaction tasks like targeting, steering, aligning, pattern-matching and reading. Our results show that perspective correction significantly and substantially improves user performance in all these tasks.",49,69.4444444444
UIST,eed1dd2a5959647896e73d129272cb7c3a2e145c,UIST,2016,The Elements of Fashion Style,"Kristen Vaccaro, Sunaya Shivakumar, Ziqiao Ding, Karrie Karahalios, Ranjitha Kumar","2862178, 3492017, 3491546, 1680270, 1816562","The outfits people wear contain latent fashion concepts capturing styles, seasons, events, and environments. Fashion theorists have proposed that these concepts are shaped by design elements such as color, material, and silhouette. A dress may be ""bohemian"" because of its pattern, material, trim, or some combination of them: it is not always clear how low-level <i>elements</i> translate to high-level <i>styles</i>. In this paper, we use polylingual topic modeling to learn latent fashion concepts jointly in two languages capturing these elements and styles. Using this latent topic formation we can translate between these two languages through topic space, exposing the <i>elements of fashion style</i>. We train the polylingual topic model (PLTM) on a set of more than half a million outfits collected from Polyvore, a popular fashion-based social net- work. We present novel, data-driven fashion applications that allow users to express their needs in natural language just as they would to a real stylist and produce tailored item recommendations for these style needs.",0,44.6540880503
UIST,52c49ba845adfb1c17d20908029061508c3558f1,UIST,2016,2.5 Dimensional Panoramic Viewing Technique utilizing a Cylindrical Mirror Widget,"Kaori Ikematsu, Mana Sasagawa, Itiro Siio","2645997, 2788514, 1709079","We propose a panoramic viewing system, which applies the technique of Anamorphosis, mapping a 2D display onto a cylindrical mirror. In this system, a distorted scene image is shown on a flat panel display or tabletop surface. When a user places the cylindrical mirror on the display, the original image appears on the cylindrical mirror.By simply rotating the cylinder, a user can decide the direction to walk-through in VR world.",0,44.6540880503
UIST,05c720785233f1368915908a75e7b54f7a5a7dfe,UIST,2013,A mixed-initiative tool for designing level progressions in games,"Eric Butler, Adam M. Smith, Yun-En Liu, Zoran Popovic","3144316, 2595475, 3228636, 1696595","Creating game content requires balancing design considerations at multiple scales: each level requires effort and iteration to produce, and broad-scale constraints such as the order in which game concepts are introduced must be respected. Game designers currently create informal plans for how the game's levels will fit together, but they rarely keep these plans up-to-date when levels change during iteration and testing. This leads to violations of constraints and makes changing the high-level plans expensive. To address these problems, we explore the creation of mixed-initiative game progression authoring tools which explicitly model broad-scale design considerations. These tools let the designer specify constraints on progressions, and keep the plan synchronized when levels are edited. This enables the designer to move between broad and narrow-scale editing and allows for automatic detection of problems caused by edits to levels. We further leverage advances in procedural content generation to help the designer rapidly explore and test game progressions. We present a prototype implementation of such a tool for our actively-developed educational game, Refraction. We also describe how this system could be extended for use in other games and domains, specifically for the domains of math problem sets and interactive programming tutorials.",14,77.5229357798
UIST,2d55fb1dcc6d127df50c31119774df0f998b7e3b,UIST,2012,Data-driven interactions for web design,Ranjitha Kumar,1816562,"This thesis describes how data-driven approaches to Web design problems can enable useful interactions for designers. It presents three machine learning applications which enable new interaction mechanisms for Web design: rapid retargeting between page designs, scalable design search, and generative probabilistic model induction to support design interactions cast as probabilistic inference. It also presents a scalable architecture for efficient data-mining on Web designs, which supports these three applications.",0,9.80392156863
UIST,1f99944889c6fd435788763daafa6acdc21d1b1e,UIST,2016,Luminescent Tentacles: A Scalable SMA Motion Display,Akira Nakayasu,2750337,"The Luminescent Tentacles system is a scalable kinetic surface system for kinetic art, ambient display, and animatronics. The 256 shape-memory alloy actuators react to hand movement by fluid dynamics and Kinect. These actuators behave like waving tentacles of sea anemones under the sea, and the top of the actuator softly glows like a bioluminescent organism. To precisely control a large number of actuators simultaneously, the system utilizes one microcontroller per actuator for distributed processing. In addition, it provides a scalable platform, which can be easily built into various forms.",0,44.6540880503
UIST,d4d82f4c2a4f305ab57582a4cc62f1065686b8b6,UIST,2016,NeverMind: Using Augmented Reality for Memorization,"Oscar Rosello, Marc Exposito, Pattie Maes","3492408, 3397110, 1701876","NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Preliminary experiments show that content memorized with NeverMind remains longer in memory compared to general memorization techniques. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.",0,44.6540880503
UIST,3953f35581fd34f76afd4f2887b3c394f7e28705,UIST,2016,NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers,"Hrvoje Benko, Christian Holz, Mike Sinclair, Eyal Ofek","2704133, 2794828, 1722648, 1735652","We present an investigation of mechanically-actuated hand-held controllers that render the shape of virtual objects through physical shape displacement, enabling users to <i>feel</i> 3D surfaces, textures, and forces that match the visual rendering. We demonstrate two such controllers, <i>NormalTouch</i> and <i>TextureTouch</i>, which are tracked in 3D and produce spatially-registered haptic feedback to a user's finger. NormalTouch haptically renders object <i>surfaces</i> and provides force feedback using a tiltable and extrudable platform. TextureTouch renders the shape of virtual objects including detailed surface <i>structure</i> through a 4&#215;4 matrix of actuated pins. By moving our controllers around while keeping their finger on the actuated platform, users obtain the impression of a much larger 3D shape by cognitively integrating output sensations over time. Our evaluation compares the effectiveness of our controllers with the two de-facto standards in Virtual Reality controllers: device vibration and visual feedback only. We find that haptic feedback significantly increases the accuracy of VR interaction, most effectively by rendering high-fidelity shape output as in the case of our controllers.",1,92.7672955975
UIST,4a5bb8f283ba7aa549811cba77660b3add7588de,UIST,2008,"Video object annotation, navigation, and composition","Dan B. Goldman, Chris Gonterman, Brian Curless, David Salesin, Steven M. Seitz","1976171, 2660079, 1810052, 1745260, 1679223","We explore the use of tracked 2D object motion to enable novel approaches to interacting with video. These include moving annotations, video navigation by direct manipulation of objects, and creating an image composite from multiple video frames. Features in the video are automatically tracked and grouped in an off-line preprocess that enables later interactive manipulation. Examples of annotations include speech and thought balloons, video graffiti, path arrows, video hyperlinks, and schematic storyboards. We also demonstrate a direct-manipulation interface for random frame access using spatial constraints, and a drag-and-drop interface for assembling still images from videos. Taken together, our tools can be employed in a variety of applications including film and video editing, visual tagging, and authoring rich media such as hyperlinked video.",73,85.7142857143
UIST,7f38af2113854c67c06799384f9ad3431941c3c3,UIST,2014,Towards responsive retargeting of existing websites,"Gilbert Louis Bernstein, Scott Klemmer","2541837, 2399999","Websites need to be displayed on a panoply of different devices today, but most websites are designed with fixed widths only appropriate to browsers on workstation computers. We propose to programmatically rewrite websites into responsive formats capable of adapting to different device display sizes. To accomplish this goal, we cast retargeting as a cross-compilation problem. We decompose existing HTML pages into boxes (lexing), infer hierarchical structure between these boxes (parsing) and finally generate parameterized layouts from the hierarchical structure (code generation). This document describes preliminary work on ReMorph, a prototype 'retargeting as cross-compilation' system.",4,57.3643410853
UIST,191198cec92313373d4b7b7a069a7dfe47c3e599,UIST,2006,Procedural haptic texture,"Jeremy Shopf, Marc Olano","1893906, 2513270","We present the Haptic Shading Framework (HSF), a framework for procedurally defining haptic texture. HSF haptic texture shaders are short procedures allowing an application-programmer to easily define interesting haptic surface interaction and the parameters that control the surface properties. These shaders provide the illusion of surface characteristics by altering previously calculated forces from object collision in the haptic pipeline.HSF can be used in an existing haptic application with few modifications. The framework consists of user-programmable modules that are dynamically loaded. This framework and all user-defined procedures are written in C++, with a provided library of useful math and geometry functions. These functions are meant to mimic RenderMan functionality, creating a familiar shading environment. As we demonstrate, many procedural shading methods and algorithms can be directly adopted for haptic shading.",2,5.0
UIST,28847b00f2bf82d914a9b53055722b43c352c89f,UIST,2011,Peripheral paced respiration: influencing user physiology during information work,"Neema Moraveji, Ben Olson, Truc Nguyen, Mahmoud Saadat, Yaser Khalighi, Roy D. Pea, Jeffrey Heer","3106937, 2617259, 3812201, 3296388, 1966206, 2076328, 1803140","We present the design and evaluation of a technique for influencing user respiration by integrating respiration-pacing methods into the desktop operating system in a peripheral manner. Peripheral paced respiration differs from prior techniques in that it does not require the user's full attention. We conducted a within-subjects study to evaluate the efficacy of peripheral paced respiration, as compared to no feedback, in an ecologically valid environment. Participant respiration decreased significantly in the pacing condition. Upon further analysis, we attribute this difference to a significant decrease in breath rate while the intermittent pacing feedback is active, rather than a persistent change in respiratory pattern. The results have implications for researchers in physiological computing, biofeedback designers, and human-computer interaction researchers concerned with user stress and affect.",14,56.6666666667
UIST,d84eeff3a452aec08fc269208e152d9924c8774b,UIST,2013,Multi-touch gesture recognition by single photoreflector,Hiroyuki Manabe,2962182,"A simple technique is proposed that uses a single photoreflector to recognize multi-touch gestures. Touch and multi-finger swipe are robustly discriminated and recognized. Further, swipe direction can be detected by adding a gradient to the sensitivity.",1,27.0642201835
UIST,37eacf589bdf404b5ceabb91d1a154268f7eb567,UIST,2001,Voice as sound: using non-verbal voice input for interactive control,"Takeo Igarashi, John F. Hughes","1717356, 2057964","We describe the use of non-verbal features in voice for direct control of interactive applications. Traditional speech recognition interfaces are based on an indirect, conversational model. First the user gives a direction and then the system performs certain operation. Our goal is to achieve more direct, immediate interaction like using a button or joystick by using lower-level features of voice such as pitch and volume. We are developing several prototype interaction techniques based on this idea, such as ""control by continuous voice"", ""rate-based parameter control by pitch,"" and ""discrete parameter control by tonguing."" We have implemented several prototype systems, and they suggest that voice-as-sound techniques can enhance traditional voice recognition approach.",69,63.3333333333
UIST,1739699f2a55207c821590130e87a80fc2fe83b3,UIST,2003,Considering the direction of cursor movement for efficient traversal of cascading menus,"Masatomo Kobayashi, Takeo Igarashi","4407679, 1717356","Cascading menus are commonly seen in most GUI systems. However, people sometimes choose the wrong items by mistake, or become frustrated when submenus pop up unnecessarily. This paper proposes two methods for improving the usability of cascading menus. The first uses the direction of cursor movement to change the menu behavior: horizontal motion opens/closes submenus, while vertical motion changes the highlight within the current menu. This feature can reduce cursor movement errors. The second causes a submenu to pop up at the position where horizontal motion occurs. This is expected to reduce the length of the movement path for menu traversal. A user study showed that our methods reduce menu selection times, shorten search path lengths, and prevent unexpected submenu appearance and disappearance.",20,16.6666666667
UIST,28285dc9bdbd58074f01520c88b6b8be495a8b6d,UIST,2004,"Interactive public ambient displays: transitioning from implicit to explicit, public to personal, interaction with multiple users","Daniel Vogel, Ravin Balakrishnan","3076153, 1748870","We develop design principles and an interaction framework for sharable, interactive public ambient displays that support the transition from implicit to explicit interaction with both public and personal information. A prototype system implementation that embodies these design principles is described. We use novel display and interaction techniques such as simple hand gestures and touch screen input for explicit interaction and contextual body orientation and position cues for implicit interaction. Techniques are presented for subtle notification, self-revealing help, privacy controls, and shared use by multiple people each in their own context. Initial user feedback is also presented, and future directions discussed.",281,100.0
UIST,45e83f396df87ce8d2cc8c8600d75107cacb2bab,UIST,2007,Eyepatch: prototyping camera-based interaction through examples,"Dan Maynes-Aminzade, Terry Winograd, Takeo Igarashi","2245650, 1699245, 1717356","Cameras are a useful source of input for many interactive applications, but computer vision programming is difficult and requires specialized knowledge that is out of reach for many HCI practitioners. In an effort to learn what makes a useful computer vision design tool, we created Eyepatch, a tool for designing camera-based interactions, and evaluated the Eyepatch prototype through deployment to students in an HCI course. This paper describes the lessons we learned about making computer vision more accessible, while retaining enough power and flexibility to be useful in a wide variety of interaction scenarios.",29,55.5555555556
UIST,982ec11d9eb80d9f6868e8f8cfe7ada0981fc7cc,UIST,2007,Bubble clusters: an interface for manipulating spatial aggregation of graphical objects,"Nayuko Watanabe, Motoi Washida, Takeo Igarashi","2791129, 2158568, 1717356","Spatial layout is frequently used for managing loosely organized information, such as desktop icons and digital ink. To help users organize this type of information efficiently, we propose an interface for manipulating spatial aggregations of objects. The aggregated objects are automatically recognized as a group, and the group structure is visualized as a two-dimensional bubble surface that surrounds the objects. Users can drag, copy, or delete a group by operating on the bubble. Furthermore, to help pick out individual objects in a dense aggregation, the system spreads the objects to avoid overlapping when requested. This paper describes the design of this interface and its implementation. We tested our technique in icon grouping and ink relocation tasks and observed improvements in user performance.",25,41.6666666667
UIST,3da9bca304465c62bc93a0433a30717397de2152,UIST,2007,Boomerang: suspendable drag-and-drop interactions based on a throw-and-catch metaphor,"Masatomo Kobayashi, Takeo Igarashi","4407679, 1717356","We present the boomerang technique, which makes it possible to suspend and resume drag-and-drop operations. A throwing gesture while dragging an object suspends the operation, anytime and anywhere. A drag-and-drop interaction, enhanced with our technique, allows users to switch windows, invoke commands, and even drag other objects during a drag-and-drop operation without using the keyboard or menus. We explain how a throwing gesture can suspend drag-and-drop operations, and describe other features of our technique, including grouping, copying, and deleting dragged objects. We conclude by presenting prototype implementations and initial feedback on the proposed technique.",12,18.0555555556
UIST,aa68e2bc17e8f5291312fa42865519b447771b46,UIST,2008,An application-independent system for visualizing user operation history,"Toshio Nakamura, Takeo Igarashi","7759244, 1717356","A history-of-user-operations function helps make applications easier to use. For example, users may have access to an operation history list in an application to undo or redo a past operation. To provide an overview of a long operation history and help users find target interactions or application states quickly, visual representations of operation history have been proposed. However, most previous systems are tightly integrated with target applications and difficult to apply to new applications. We propose an application-independent method that can visualize the operation history of arbitrary GUI applications by monitoring the input and output GUI events from outside of the target application. We implemented a prototype system that visualizes operation sequences of generic Java Awt/Swing applications using an annotated comic strip metaphor. We tested the system with various applications and present results from a user study.",30,61.4285714286
UIST,accfc6011bb5aba3fa74a1ef3583efe2228e683e,UIST,2010,"Blinkbot: look at, blink and move","Pranav Mistry, Kentaro Ishii, Masahiko Inami, Takeo Igarashi","1735907, 7254458, 1684930, 1717356",In this paper we present BlinkBot - a hands free input interface to control and command a robot. BlinkBot explores the natural modality of gaze and blink to direct a robot to move an object from a location to another. The paper also explains detailed hardware and software implementation of the prototype system.,5,54.0697674419
UIST,4b615e50b297914aaacb7e8ac92adf03eb19c815,UIST,2015,Wait-Learning: Leveraging Wait Time for Education,Carrie J. Cai,2657025,"Competing priorities in daily life make it difficult for those with a casual interest in learning to set aside time for regular practice. Yet, learning often requires significant time and effort, with repeated exposures to learning material on a recurring basis. Despite the struggle to find time for learning, there are numerous times in a day that are wasted due to micro-waiting. In my research, I develop systems for wait-learning, leveraging wait time for education. Combining wait time with productive work opens up a new class of software systems that overcomes the problem of limited time while addressing the frustration often associated with waiting. My research tackles several challenges in learning and task management, such as identifying which waiting moments to leverage; how to encourage learning unobtrusively; how to integrate learning across a diversity of waiting moments; and how to extend wait-learning to more complex domains. In the development process, I hope to understand how to manage these waiting moments, and describe essential design principles for wait-learning systems.",0,16.2280701754
UIST,26f73fb17f84c8b0b445e5ba49cd0abd15b6e4db,UIST,2012,A thin stretchable interface for tangential force measurement,"Yuta Sugiura, Masahiko Inami, Takeo Igarashi","1799242, 1684930, 1717356","We have developed a simple skin-like user interface that can be easily attached to curved as well as flat surfaces and used to measure tangential force generated by pinching and dragging interactions. The interface consists of several photoreflectors that consist of an IR LED and a phototransistor and elastic fabric such as stocking and rubber membrane. The sensing method used is based on our observation that photoreflectors can be used to measure the ratio of expansion and contraction of a stocking using the changes in transmissivity of IR light passing through the stocking. Since a stocking is thin, stretchable, and nearly transparent, it can be easily attached to various types of objects such as mobile devices, robots, and different parts of the body as well as to various types of conventional pressure sensors without altering the original shape of the object. It can also present natural haptic feedback in accordance with the amount of force exerted. A system using several such sensors can determine the direction of a two-dimensional force. A variety of example applications illustrated the utility of this sensing system.",6,52.4509803922
UIST,6186a3d6c9642c69d92e2eff3c1a24eb3fe0bf71,UIST,2014,Global beautification of layouts with interactive ambiguity resolution,"Pengfei Xu, Hongbo Fu, Takeo Igarashi, Chiew-Lan Tai","8334787, 1691065, 1717356, 1702674","Automatic global beautification methods have been proposed for sketch-based interfaces, but they can lead to undesired results due to ambiguity in the user's input. To facilitate ambiguity resolution in layout beautification, we present a novel user interface for visualizing and editing inferred relationships. First, our interface provides a preview of the beautified layout with inferred constraints, without directly modifying the input layout. In this way, the user can easily keep refining beautification results by interactively repositioning and/or resizing elements in the input layout. Second, we present a gestural interface for editing automatically inferred constraints by directly interacting with the visualized constraints via simple gestures. Our efficient implementation of the beautification system provides the user instant feedback. Our user studies validate that our tool is capable of creating, editing and refining layouts of graphic elements and is significantly faster than the standard snap-dragging and command-based alignment tools.",4,57.3643410853
UIST,91081278ef49429bdfd2a1761b02736602d5f0a6,UIST,2006,HybridPointing: fluid switching between absolute and relative pointing with a direct input device,"Clifton Forlines, Daniel Vogel, Ravin Balakrishnan","1694854, 3076153, 1748870","We present HybridPointing, a technique that lets users easily switch between absolute and relative pointing with a direct input device such as a pen. Our design includes a new graphical element, the <i>Trailing Widget</i>, which remains ""close at hand"" but does not interfere with normal cursor operation. The use of visual feedback to aid the user's understanding of input state is discussed, and several novel visual aids are presented. An experiment conducted on a large, wall-sized display validates the benefits of HybridPointing under certain conditions. We also discuss other situations in which HybridPointing may be useful. Finally, we present an extension to our technique that allows for switching between absolute and relative input in the middle of a single drag-operation.",60,85.0
UIST,c44f0dbefa660ee0b5fe81e08e5cea28e684542d,UIST,2013,QOOK: a new physical-virtual coupling experience for active reading,"Yuhang Zhao, Yongqiang Qin, Yang Liu, Siqi Liu, Yuanchun Shi","2582568, 2263638, 1750084, 2628544, 1732440","We present QOOK, an interactive reading system that incorporates the benefits of both physical and digital books to facilitate active reading. QOOK uses a top-projector to create digital contents on a blank paper book. By detecting markers attached to each page, QOOK allows users to flip pages just like they would with a real book. Electronic functions such as keyword searching, highlighting and bookmarking are included to provide users with additional digital assistance. With a Kinect sensor that recognizes touch gestures, QOOK enables people to use these electronic functions directly with their fingers. The combination of the electronic functions of the virtual interface and free-form interaction with the physical book creates a natural reading experience, providing an opportunity for faster navigation between pages and better understanding of the book contents.",2,37.6146788991
UIST,440deeb7cb19e4bf3cea31921c948ea4a707ba12,UIST,2016,Interactive Volume Segmentation with Threshold Field Painting,"Takeo Igarashi, Naoyuki Shono, Taichi Kin, Toki Saito","1717356, 3491807, 2974004, 2027578","An interactive method for segmentation and isosurface extraction of medical volume data is proposed. In conventional methods, users decompose a volume into multiple regions iteratively, segment each region using a threshold, and then manually clean the segmentation result by removing clutter in each region. However, this is tedious and requires many mouse operations from different camera views. We propose an alternative approach whereby the user simply applies painting operations to the volume using tools commonly seen in painting systems, such as flood fill and brushes. This significantly reduces the number of mouse and camera control operations. Our technical contribution is in the introduction of the threshold field, which assigns spatially-varying threshold values to individual voxels. This generalizes discrete decomposition of a volume into regions and segmentation using a constant threshold in each region, thereby offering a much more flexible and efficient workflow. This paper describes the details of the user interaction and its implementation. Furthermore, the results of a user study are discussed. The results indicate that the proposed method can be a few times faster than a conventional method.",0,44.6540880503
UIST,54a00a1a4fd0044c8d804b3fc3c877c1c22247ba,UIST,2002,The missing link: augmenting biology laboratory notebooks,"Wendy E. Mackay, Guillaume Pothier, Catherine Letondal, Kaare Bøegh, Hans Erik Sørensen","1732917, 2698041, 2346968, 2891830, 3029366","Using a participatory design process, we created three prototype augmented laboratory notebooks that provide the missing link between paper, physical artifacts and on-line data. The final <i>a-book</i> combines a graphics tablet and a PDA. The tablet captures writing on the paper notebook and the PDA acts as an ""interaction lens"" or window between physical and electronic documents. Our approach is document-centered, with a software architecture based on layers of physical and electronic information.",120,87.5
UIST,4b4e73d3f0c0476dd8cea4c2d143eda102d3979a,UIST,2011,"SpeckleSense: fast, precise, low-cost and compact motion sensing using laser speckle","Jan Zizka, Alex Olwal, Ramesh Raskar","2973652, 2375159, 1717566","Motion sensing is of fundamental importance for user interfaces and input devices. In applications, where optical sensing is preferred, traditional camera-based approaches can be prohibitive due to limited resolution, low frame rates and the required computational power for image processing. We introduce a novel set of motion-sensing configurations based on laser speckle sensing that are particularly suitable for human-computer interaction. The underlying principles allow these configurations to be fast, precise, extremely compact and low cost. We provide an overview and design guidelines for laser speckle sensing for user interaction and introduce four general speckle projector/sensor configurations. We describe a set of prototypes and applications that demonstrate the versatility of our laser speckle sensing techniques.",11,49.0476190476
UIST,bc61e4b4d27d6e08b623069dd1462692290b791a,UIST,2016,Mavo: Creating Interactive Data-Driven Web Applications by Authoring HTML,"Lea Verou, Amy Xian Zhang, David R. Karger","3492189, 1788613, 1743286","Many people can author static web pages with HTML and CSS but find it hard or impossible to program persistent, interactive web applications. We show that for a broad class of CRUD (Create, Read, Update, Delete) applications, this gap can be bridged. Mavo extends the declarative syntax of HTML to describe Web applications that manage, store and transform data. Using Mavo, authors with basic HTML knowledge define complex data schemas <i>implicitly</i> as they design their HTML layout. They need only add a few attributes and expressions to their HTML elements to transform their static design into a persistent, data-driven web application whose data can be edited by direct manipulation of the content in the browser. We evaluated Mavo with 20 users who marked up static designs---some provided by us, some their own creation---to transform them into fully functional web applications. Even users with no programming experience were able to quickly craft Mavo applications.",0,44.6540880503
UIST,4343d89128a7de8f152498c57c17a5aafc62e682,UIST,2007,Specifying label layout style by example,"Ian Vollick, Daniel Vogel, Maneesh Agrawala, Aaron Hertzmann","2397179, 3076153, 1820412, 1747779","Creating high-quality label layouts in a particular visual style is a time-consuming process. Although automated labeling algorithms can aid the layout process, expert design knowledge is required to tune these algorithms so that they produce layouts which meet the designer's expectations. We propose a system which can learn a labellayout style from a single example layout and then apply this style to new labeling problems. Because designers find it much easier to create example layouts than tune algorithmic parameters, our system provides a more natural workflow for graphic designers. We demonstrate that our system is capable of learning a variety of label layout styles from examples.",18,22.2222222222
UIST,03fc9b481719cb3da150bb65ef5d129eaffddc43,UIST,2015,GazeProjector: Accurate Gaze Estimation and Seamless Gaze Interaction Across Multiple Displays,"Christian Lander, Sven Gehring, Antonio Krüger, Sebastian Boring, Andreas Bulling","2554454, 1758772, 1790548, 1741219, 3194727","Mobile gaze-based interaction with multiple displays may occur from arbitrary positions and orientations. However, maintaining high gaze estimation accuracy in such situa-tions remains a significant challenge. In this paper, we present GazeProjector, a system that combines (1) natural feature tracking on displays to determine the mobile eye tracker's position relative to a display with (2) accurate point-of-gaze estimation. GazeProjector allows for seam-less gaze estimation and interaction on multiple displays of arbitrary sizes independently of the user's position and orientation to the display. In a user study with 12 partici-pants we compare GazeProjector to established methods (here: visual on-screen markers and a state-of-the-art video-based motion capture system). We show that our approach is robust to varying head poses, orientations, and distances to the display, while still providing high gaze estimation accuracy across multiple displays without re-calibration for each variation. Our system represents an important step towards the vision of pervasive gaze-based interfaces.",9,94.298245614
UIST,1af4ca0fd3ef6fa750f9deaf1e293cc3b57f4069,UIST,2011,An asymmetric communications platform for knowledge sharing with low-end mobile phones,"Neil Patel, Scott R. Klemmer, Tapan S. Parikh","1758612, 1728167, 1755518","We present Awaaz.De (""give voice""), a social platform for communities to access and share knowledge using low-end mobile phones. Awaaz.De features a configurable mobile voice application organized into asynchronous voice mes-sage boards. For poor, remote and marginal communities, the voice-touchtone interface addresses the constraints of low literacy, language diversity, and affordability of only basic mobile devices. Voice content also presents a low barrier to content authoring, encouraging otherwise disconnected communities to actively participate in knowledge exchange. Awaaz.De includes a web-based administration interface for Internet-connected community managers to moderate, annotate, categorize, route, and narrow-cast voice messages. In this paper we describe the platform's design, implementation, and future directions.",9,44.2857142857
UIST,3b1c4a424780edbbca7aab6de89ee0c76b9ddf00,UIST,2007,RubberEdge: reducing clutching by combining position and rate control with elastic feedback,"Géry Casiez, Daniel Vogel, Qing Pan, Christophe Chaillou","3051289, 3076153, 2882045, 1774410","Position control devices enable precise selection, but significant clutching degrades performance. Clutching can be reduced with high control-display gain or pointer acceleration, but there are human and device limits. Elastic rate control eliminates clutching completely, but can make precise selection difficult. We show that hybrid position-rate control can outperform position control by 20% when there is significant clutching, even when using pointer acceleration. Unlike previous work, our RubberEdge technique eliminates trajectory and velocity discontinuities. We derive predictive models for position control with clutching and hybrid control, and present a prototype RubberEdge position-rate control device including initial user feedback.",22,31.9444444444
UIST,1db7755f8f9b76897720a42a3c23bdef3d4483a6,UIST,2000,Speed-dependent automatic zooming for browsing large documents,"Takeo Igarashi, Ken Hinckley","1717356, 1738072","We propose a navigation technique for browsing large documents that integrates rate-based scrolling with automatic zooming. The view automatically zooms out when the user scrolls rapidly so that the perceptual scrolling speed in screen space remains constant. As a result, the user can efficiently and smoothly navigate through a large document without becoming disoriented by extremely fast visual flow. By incorporating semantic zooming techniques, the user can smoothly access a global overview of the document during rate-based scrolling. We implemented several prototype systems, including a web browser, map viewer, image browser, and dictionary viewer. An informal usability study suggests that for a document browsing task, most subjects prefer automatic zooming and the technique exhibits approximately equal performance time to scroll bars, suggesting that automatic zooming is a helpful alternative to traditional scrolling when the zoomed out view provides appropriate visual cues.",187,92.0
UIST,dfe299766c090a3617c427c7c0a0f127985ed256,UIST,2003,Synchronous gestures for multiple persons and computers,Ken Hinckley,1738072,"This research explores distributed sensing techniques for mobile devices using <i>synchronous gestures</i>. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.",147,83.3333333333
UIST,06faffdae24bfbdd161ff0f426fddca160c5167d,UIST,2012,Spatial augmented reality for physical drawing,"Jérémy Laviole, Martin Hachet","3421303, 2281511","Spatial augmented reality (SAR) makes possible the projection of virtual environments into the real world. In this demo, we propose to demonstrate our SAR tools dedicated to the creation of physical drawings. From the most simple tools: the projection on virtual guidelines enabling to trace lines and curves to more advanced techniques enabling stereoscopic drawing through the projection of a 3D scene. This demo presents how we can use computer graphics tools to ease the drawing, and how it will enable new kinds of physical drawings.",0,9.80392156863
UIST,35c8e2a8bba2bd04331fe442ca6659c1fce4b39a,UIST,2011,Medusa: a proximity-aware multi-touch tabletop,"Michelle Annett, Tovi Grossman, Daniel J. Wigdor, George W. Fitzmaurice","2870099, 3313809, 1961958, 1703735","We present Medusa, a proximity-aware multi-touch tabletop. Medusa uses 138 inexpensive proximity sensors to: detect a user's presence and location, determine body and arm locations, distinguish between the right and left arms, and map touch point to specific users and specific hands. Our tracking algorithms and hardware designs are described. Exploring this unique design, we develop and report on a collection of interactions enabled by Medusa in support of multi-user collaborative design, specifically within the context of Proxi-Sketch, a multi-user UI prototyping tool. We discuss design issues, system implementation, limitations, and generalizable concepts throughout the paper.",49,88.5714285714
UIST,958861d1a26a53935c305a86215641d6bf40257f,UIST,2013,"Brainstorm, define, prototype: timing constraints to balance appropriate and novel design","Andrew Nicholas Elder, Elaine Zhou","8746267, 2087851","We present the results of a human creativity experiment that examined the effect of varying the timing of narrowed constraints. Participants were asked to create a static web ad for Stanford University guided under a timed design process and were introduced to a narrowed constraint either at the beginning, middle, or end of the prototyping process. The narrow constraint addressed goal and task constraints by specifying the target audience and ad size. We find that groups introduced to narrow constraints prior to the brainstorm yielded more appropriate results, while those introduced prior to the final production yielded more novel results. Our results suggest that effective timing of design constraints may further optimize ideation and design methodologies.",0,10.5504587156
UIST,72d61925cf886b4dfebb706c78054598caf23a2a,UIST,2014,Situated crowdsourcing using a market model,"Simo Hosio, Jorge Gonçalves, Vili Lehdonvirta, Denzil Ferreira, Vassilis Kostakos","3318049, 6536876, 2013768, 1878497, 1781697","Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.",15,88.7596899225
UIST,71aff3ef36c62160923e052d11eaaaa789e0b427,UIST,2016,Wolverine: A Wearable Haptic Interface for Grasping in VR,"Inrak Choi, Sean Follmer","3491692, 2770912","The Wolverine is a mobile, wearable haptic device designed for simulating the grasping of rigid objects in virtual environment. In contrast to prior work on force feedback gloves, we focus on creating a low cost, lightweight, and wireless device that renders a force directly between the thumb and three fingers to simulate objects held in pad opposition type grasps. Leveraging low-power brake-based locking sliders, the system can withstand over 100N of force between each finger and the thumb, and only consumes 2.78 Wh(10 mJ) for each braking interaction. Integrated sensors are used both for feedback control and user input: time-of-flight sensors provide the position of each finger and an IMU provides overall orientation tracking. This design enables us to use the device for roughly 6 hours with 5500 full fingered grasping events. The total weight is 55g including a 350 mAh battery.",0,44.6540880503
UIST,f5ec46b31b56d4e94fc2f145f8861da8c4d4727d,UIST,2011,ThickPad: a hover-tracking touchpad for a laptop,"Sangwon Choi, Jaehyun Han, Sunjun Kim, Seongkook Heo, Geehyuk Lee","1967396, 7181851, 8364126, 1724588, 1717371","We explored the use of a hover tracking touchpad in a laptop environment. In order to study the new experience, we implemented a prototype touchpad consisting of infrared LEDs and photo-transistors, which can track fingers as far as 10mm over the surface. We demonstrate here three major interaction techniques that would become possible when a hover-tracking touchpad meets a laptop",7,41.9047619048
UIST,df38213a7c0f6ca475341f590f94c86888079857,UIST,2004,Tangible NURBS-curve manipulation techniques using graspable handles on a large display,"Seok-Hyung Bae, Takahiro Kobayash, Ryugo Kijima, Won-Sup Kim","1715434, 3232884, 3163421, 3246219","This paper presents tangible interaction techniques for fine-tuning one-to-one scale NURBS curves on a large display for automotive design. We developed a new graspable handle with a transparent groove that allows designers to manipulate virtual curves on a display screen directly. The use of the proposed handle leads naturally to a rich vocabulary of terms describing interaction techniques that reflect existing shape styling methods. A user test raised various issues related to the graspable user interface, two-handed input, and large-display interaction.",5,13.1578947368
UIST,8ddcb9bc611cebb5208ddd32b5904968b8d183fa,UIST,2011,IrCube tracker: an optical 6-DOF tracker based on LED directivity,"Seongkook Heo, Jaehyun Han, Sangwon Choi, Seunghwan Lee, Geehyuk Lee, Hyong-Euk Lee, SangHyun Kim, Won-Chul Bang, Do-Kyoon Kim, Chang-Yeong Kim","1724588, 7181851, 1967396, 2934977, 1717371, 2030325, 3050257, 2875268, 7183969, 1686725","Six-degrees-of-freedom (6-DOF) trackers, which were mainly for professional computer applications, are now in demand by everyday consumer applications. With the requirements of consumer electronics in mind, we designed an optical 6-DOF tracker where a few photo-sensors can track the position and orientation of an LED cluster. The operating principle of the tracker is basically source localization by solving an inverse problem. We implemented a prototype system for a TV viewing environment, verified the feasibility of the operating principle, and evaluated the basic performance of the prototype system in terms of accuracy and speed. We also examined its application possibility to different environments, such as a tabletop computer, a tablet computer, and a mobile spatial interaction environment.",3,31.4285714286
UIST,4cbebd620a7b3e81cb1dbd67fe6042ba1b0a4ec4,UIST,2014,Speeda: adaptive speed-up for lecture videos,"Chen-Tai Kao, Yen-Ting Liu, Alexander Hsu","2226901, 2430061, 2028972","Increasing the playback speed of lecture videos is a common technique to shorten watching time. This creates challenges when part of the lecture becomes too fast to be discernible, even if the overall playback speed is acceptable. In this paper, we present a speed-up system that preserves lecture clearness in high playback rate. A user test was conducted to evaluate the system. The result indicates that our system significantly improves user's comprehension level.",0,12.015503876
UIST,669c8d56dac816b280b69c073aea2a16dec8055a,UIST,2016,Toward a Compact Device to Interact with a Capacitive Touch Screen,"Tomomi Takashina, Tsutomu Tamura, Makoto Nakazumi, Tatsushi Nomura, Yuji Kokumai","1687606, 7693562, 3493100, 3492676, 1703091","Capacitive touch screens are widely used in various products. Touch screens have an advantage that an input system and output system can be integrated into a single module. We consider this advantage could make it possible to realize a new universal interface for both human-to-machine (H2M) and machine-to-machine (M2M). For a M2M interface, some sort of method to simulate finger touching is needed. Therefore, we propose an alternative method to interact with a touch screen using two electrical approaches. Our proposal is effective in automating touch screen operations, modality conversion device for people with disabilities, and so on. We assembled a prototype to confirm the principle to control a touch screen with the electrical methods. We believe that our proposal will complement the weakness of touch screens and expand their possibility.",0,44.6540880503
UIST,d7b542be43bc94bf0825d955b0f748e51d3a1eab,UIST,2013,User created tangible controls using ForceForm: a dynamically deformable interactive surface,"Jessica Tsimeris, Duncan Stevenson, Matt Adcock, Tamás D. Gedeon, Michael Broughton","2644445, 7371355, 1691131, 1768305, 2065516","Touch surfaces are common devices but they are often uniformly flat and provide little flexibility beyond changing the visual information communicated to the user via software. Furthermore, controls for interaction are not tangible and are usually specified and placed by the user interface designer. Using ForceForm, a dynamically deformable interactive surface, the user is able to directly sculpt the surface to create tangible controls with force feedback properties. These controls can be made according to the user's specifications, and can then be relinquished when no longer needed. We describe this method of interaction, provide an implementation of a slider, and ideas for further controls.",2,37.6146788991
UIST,4784dd3c37c00aae3185651a5289e9bf1525dfe0,UIST,1991,On temporal-spatial realism in the virtual reality environment,"Jiandong Liang, Chris Shaw, Mark Green","2208765, 1698197, 3081643","The Polhemus Isotrak is often used as an orientation and position tracking device in virtual reality environments. When it is used to dynamically determine the user's viewpoint and line of sight (e.g. in the case of a head mounted display) the noise and delay in its measurement data causes temporal-spatial distortion, perceived by the user as jittering of images and lag between head movement and visual feedback. To tackle this problem, we first examined the major cause of the distortion, and found that the lag felt by the user is mainly due to the delay in orientation data, and the jittering of images is caused mostly by the noise in position data. Based on these observations, a predictive Kalman filter was designed to compensate for the delay in orientation data, and an anisotropic low pass filter was devised to reduce the noise in position data. The effectiveness and limitations of both approaches were then studied, and the results shown to be satisfactory.",121,95.652173913
UIST,14a19b491f7e2b197a5df157b86a74ddecd3ac70,UIST,2008,Annotating gigapixel images,"Qing Luan, Steven M. Drucker, Johannes Kopf, Ying-Qing Xu, Michael F. Cohen","1821045, 2311676, 2891193, 1742571, 1694613","Panning and zooming interfaces for exploring very large images containing billions of pixels (gigapixel images) have recently appeared on the internet. This paper addresses issues that arise when creating and rendering auditory and textual annotations for such images. In particular, we define a distance metric between each annotation and any view resulting from panning and zooming on the image. The distance then informs the rendering of audio annotations and text labels. We demonstrate the annotation system on a number of panoramic images.",8,15.7142857143
UIST,7b80ff880a9453c1e8c9fa26e3bbb908deb36b7f,UIST,2015,Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG,"Donny Huang, Xiaoyi Zhang, T. Scott Saponas, James Fogarty, Shyamnath Gollakota","3040296, 5001856, 1766388, 1738171, 1805554","We introduce the first forearm-based EMG input system that can recognize fine-grained thumb gestures, including left swipes, right swipes, taps, long presses, and more complex thumb motions. EMG signals for thumb motions sensed from the forearm are quite weak and require significant training data to classify. We therefore also introduce a novel approach for minimally-intrusive collection of labeled training data for always-available input devices. Our dual-observable input approach is based on the insight that interaction observed by multiple devices allows recognition by a primary device (e.g., phone recognition of a left swipe gesture) to create labeled training examples for another (e.g., forearm-based EMG data labeled as a left swipe). We implement a wearable prototype with dry EMG electrodes, train with labeled demonstrations from participants using their own phones, and show that our prototype can recognize common fine-grained thumb gestures and user-defined complex gestures.",3,71.0526315789
UIST,13584d085f4fbcf25dcb9b6b35a17737b949f72b,UIST,2014,Predictive translation memory: a mixed-initiative system for human language translation,"Spence Green, Jason Chuang, Jeffrey Heer, Christopher D. Manning","1896775, 1964541, 1803140, 1812612","The standard approach to computer-aided language translation is post-editing: a machine generates a single translation that a human translator corrects. Recent studies have shown this simple technique to be surprisingly effective, yet it underutilizes the complementary strengths of precision-oriented humans and recall-oriented machines. We present Predictive Translation Memory, an interactive, mixed-initiative system for human language translation. Translators build translations incrementally by considering machine suggestions that update according to the user's current partial translation. In a large-scale study, we find that professional translators are slightly slower in the interactive mode yet produce slightly higher quality translations despite significant prior experience with the baseline post-editing condition. Our analysis identifies significant predictors of time and quality, and also characterizes interactive aid usage. Subjects entered over 99% of characters via interactive aids, a significantly higher fraction than that shown in previous work.",6,67.8294573643
UIST,5b5cc6e4beb644b1d283dd6256994d5b1b1c071d,UIST,2010,Designing adaptive feedback for improving data entry accuracy,"Kuang Chen, Joseph M. Hellerstein, Tapan S. Parikh","6759835, 1695576, 1755518","Data quality is critical for many information-intensive applications. One of the best opportunities to improve data quality is during entry. Usher provides a theoretical, data-driven foundation for improving data quality during entry. Based on prior data, Usher learns a probabilistic model of the dependencies between form questions and values. Using this information, Usher maximizes <i>information gain</i>. By asking the most unpredictable questions first, Usher is better able to predict answers for the remaining questions. In this paper, we use Usher's predictive ability to design a number of intelligent user interface adaptations that improve data entry accuracy and efficiency. Based on an underlying cognitive model of data entry, we apply these modifications before, during and after committing an answer. We evaluated these mechanisms with professional data entry clerks working with real patient data from six clinics in rural Uganda. The results show that our adaptations have the potential to reduce error (by up to 78%), with limited effect on entry time (varying between -14% and +6%). We believe this approach has wide applicability for improving the quality and availability of data, which is increasingly important for decision-making and resource allocation.",10,61.0465116279
UIST,287e1ffb2f0b23eed2d6e37fa205d808965a2ede,UIST,2010,The IR ring: authenticating users' touches on a multi-touch display,"Volker Roth, Philipp Schmidt, Benjamin Güldenring","3370624, 2349230, 2906899","Multi-touch displays are particularly attractive for collaborative work because multiple users can interact with applications simultaneously. However, unfettered access can lead to loss of data confidentiality and integrity. For example, one user can open or alter files of a second user, or impersonate the second user, while the second user is absent or not looking. Towards preventing these attacks, we explore means to associate the touches of a user with the user's identity in a fashion that is cryptographically sound as well as easy to use. We describe our current solution, which relies on a ring-like device that transmits a continuous pseudorandom bit sequence in the form of infrared light pulses. The multi-touch display receives and localizes the sequence, and verifies its authenticity. Each sequence is bound to a particular user, and all touches in the direct vicinity of the location of the sequence on the display are associated with that user.",25,79.0697674419
UIST,9a8634a6951f27f0ccb914c61ea3ac5f1de42bd7,UIST,2004,Revisiting visual interface programming: creating GUI tools for designers and programmers,"Stéphane Chatty, Stéphane Sire, Jean-Luc Vinot, Patrick Lecoanet, Alexandre Lemort, Christophe P. Mertz","1945859, 1769105, 1807032, 2429836, 3144663, 2251077","Involving graphic designers in the large-scale development of user interfaces requires tools that provide more graphical flexibility and support efficient software processes. These requirements were analysed and used in the design of the TkZ-inc graphical library and the IntuiKit interface design environment. More flexibility is obtained through a wider palette of visual techniques and support for iterative construction of images, composition and parametric displays. More efficient processes are obtained with the use of the SVG standard to import graphics, support for linking graphics and behaviour, and a unifying model-driven architecture. We describe the corresponding features of our tools, and show their use in the development of an application for airports. Benefits include a wider access to high quality visual interfaces for specialised applications, and shorter prototyping and development cycles for multidisciplinary teams.",30,35.5263157895
UIST,7edde528ba3f0d133f13d89f0bf88c8e9245f65e,UIST,2007,Two-finger input with a standard touch screen,Jörn Loviscach,1745536,"Most current implementations of multi-touch screens are still too expensive or too bulky for widespread adoption. To improve this situation, this work describes the electronics and software needed to collect more data than one pair of coordinates from a standard 4-wire touch screen. With this system, one can measure the pressure of a single touch and approximately sense the coordinates of two touches occurring simultaneously. Naturally, the system cannot offer the accuracy and versatility of full multi-touch screens. Nonetheless, several example applications ranging from painting to zooming demonstrate a broad spectrum of use.",3,9.72222222222
UIST,7cd11c0102cb4593d30f4328032558790586c164,UIST,2008,An exploration of pen rolling for pen-based interaction,"Xiaojun Bi, Tomer Moscovich, Gonzalo Ramos, Ravin Balakrishnan, Ken Hinckley","1682293, 2966785, 1910455, 1748870, 1738072","Current pen input mainly utilizes the position of the pen tip, and occasionally, a button press. Other possible device parameters, such as rolling the pen around its longitudinal axis, are rarely used. We explore pen rolling as a supporting input modality for pen-based interaction. Through two studies, we are able to determine 1) the parameters that separate intentional pen rolling for the purpose of interaction from incidental pen rolling caused by regular writing and drawing, and 2) the parameter range within which accurate and timely intentional pen rolling interactions can occur. Building on our experimental results, we present an exploration of the design space of rolling-based interaction techniques, which showcase three scenarios where pen rolling interactions can be useful: enhanced stimulus-response compatibility in rotation tasks [7], multi-parameter input, and simplified mode selection.",30,61.4285714286
UIST,b3fae6146ad6be2fe5c181f4432c334ae34c5a61,UIST,2015,Biometric Touch Sensing: Seamlessly Augmenting Each Touch with Continuous Authentication,"Christian Holz, Marius Knaust","2794828, 1966318","Current touch devices separate user authentication from regular interaction, for example by displaying modal login screens <i>before</i> device usage or prompting for in-app passwords, which interrupts the interaction flow. We propose <i>biometric touch sensing</i>, a new approach to representing touch events that enables commodity devices to seamlessly integrate authentication into interaction: From each touch, the touchscreen senses the 2D input coordinates and at the same time obtains biometric features that identify the user. Our approach makes authentication during interaction transparent to the user, yet ensures secure interaction at all times. To implement this on today's devices, our watch prototype <i>Bioamp senses</i> the impedance profile of the user's wrist and modulates a signal onto the user's body through skin using a periodic electric signal. This signal affects the capacitive values touchscreens measure upon touch, allowing devices to identify users on each touch. We integrate our approach into Windows 8 and discuss and demonstrate it in the context of various use cases, including access permissions and protecting private screen contents on personal and shared devices.",8,90.350877193
UIST,4c9d98cbb2a250270ea8eb8a889f35ff48fd5eab,UIST,2005,"Distant freehand pointing and clicking on very large, high resolution displays","Daniel Vogel, Ravin Balakrishnan","3076153, 1748870","We explore the design space of freehand pointing and clicking interaction with very large high resolution displays from a distance. Three techniques for gestural pointing and two for clicking are developed and evaluated. In addition, we present subtle auditory and visual feedback techniques to compensate for the lack of kinesthetic feedback in freehand interaction, and to promote learning and use of appropriate postures.",180,93.5483870968
UIST,3c182729cb12ed0b7f14dcac3fc8b33ad5036e76,UIST,2011,Conté: multimodal input inspired by an artist's crayon,"Daniel Vogel, Géry Casiez","3076153, 3051289","Cont&#233; is a small input device inspired by the way artists manipulate a real Cont&#233; crayon. By changing which corner, edge, end, or side is contacting the display, the operator can switch interaction modes using a single hand. Cont&#233;'s rectangular prism shape enables both precise pen-like input and tangible handle interaction. Cont&#233; also has a natural compatibility with multi-touch input: it can be tucked in the palm to interleave same-hand touch input, or used to expand the vocabulary of bimanual touch. Inspired by informal interviews with artists, we catalogue Cont&#233;'s characteristics, and use these to outline a design space. We describe a prototype device using common materials and simple electronics. With this device, we demonstrate interaction techniques in a test-bed drawing application. Finally, we discuss alternate hardware designs and future human factors research to study this new class of input.",10,46.6666666667
UIST,548ca23d91c680f70ff84cee7dde27f29aa3daa6,UIST,2013,Tactile rendering of 3D features on touch surfaces,"Seung-Chan Kim, Ali Israr, Ivan Poupyrev","6837406, 1769549, 1736819","We present a tactile-rendering algorithm for simulating 3D geometric features, such as bumps, on touch screen surfaces. This is achieved by modulating friction forces between the user's finger and the touch screen, instead of physically moving the touch surface. We proposed that the percept of a 3D bump is created when local gradients of the rendered virtual surface are mapped to lateral friction forces. To validate this approach, we first establish a psychophysical model that relates the perceived friction force to the controlled voltage applied to the tactile feedback device. We then use this model to demonstrate that participants are three times more likely to prefer gradient force profiles than other commonly used rendering profiles. Finally, we present a generalized algorithm and conclude the paper with a set of applications using our tactile rendering technology.",16,79.8165137615
UIST,4fd51c12c582c64647cde7250a3700714d632e55,UIST,2014,Ubisonus: spatial freeform interactive speakers,"Yoshio Ishiguro, Eric Brockmeyer, Alex Rothera, Ali Israr","2750869, 1715064, 3053339, 1769549","We present freeform interactive speakers for creating spatial sound experiences from a variety of surfaces. Traditional surround sound systems are widely used and consist of multiple electromagnetic speakers that create point sound sources within a space. Our proposed system creates directional sound and can be easily embedded into architecture, furniture and many everyday objects. We use electrostatic loudspeaker technology made from thin, flexible, lightweight and low cost materials and can be of different size and shape. In this demonstration we will show various configurations such as single speaker, speaker array and tangible speakers for playful and exciting interactions with spatial sounds. This is an example of new possibilities for the design of various interactive surfaces.",0,12.015503876
UIST,458fb44f5c42c8241fc330de9e6362beeefcb7fb,UIST,2010,The multiplayer: multi-perspective social video navigation,"Zihao Yu, Nicholas Diakopoulos, Mor Naaman","3091834, 2943892, 1687465","We present a multi-perspective video ""multiplayer"" designed to organize social video aggregated from online sites like YouTube. Our system automatically time-aligns videos using audio fingerprinting, thus bringing them into a unified temporal frame. The interface utilizes social metadata to visually aid navigation and cue users to more interesting portions of an event. We provide details about the visual and interaction design rationale of the multiplayer.",0,9.3023255814
UIST,38d44beff504df170eab8eb2226ed174d0b17c93,UIST,2014,FeelCraft: crafting tactile experiences for media using a feel effect library,"Siyan Zhao, Oliver S. Schneider, Roberta L. Klatzky, Jill Fain Lehman, Ali Israr","3136060, 1865182, 1735008, 2142730, 1769549","FeelCraft is a media plugin that monitors events and states in the media and associates them with expressive tactile content using a library of feel effects (FEs). A feel effect (FE) is a user-defined haptic pattern that, by virtue of its connection to a meaningful event, generates dynamic and expressive effects on the user's body. We compiled a library of more than fifty FEs associated with common events in games, movies, storybooks, etc., and used them in a sandbox-type gaming platform. The FeelCraft plugin allows a game designer to quickly generate haptic effects, associate them to events in the game, play them back for testing, save them and/or broadcast them to other users to feel the same haptic experience. Our demonstration shows an interactive procedure for authoring haptic media content using the FE library, playing it back during interactions in the game, and broadcasting it to a group of guests.",4,57.3643410853
UIST,1ba40f57c5d3c7e454d6622e9ea5ff1295cf1892,UIST,2016,Prevention of Unintentional Input While Using Wrist Rotation for Device Configuration,"Han Joo Chae, Jeongin Hwang, Yuri Choi, Yieun Kim, Kyle Koh, Jinwook Seo","3440235, 3439574, 2145731, 3493134, 1834207, 1705834","We describe the design of the safeguard interface that helps users avoid unintentional input while using wrist rotation. When configuring the parameters of various devices, our interface helps reduce the chance of making accidental changes by delaying the result of input and allowing the users to make deliberate attempt to change the parameters to their desired value. We evaluated our methods with a set of user experience and found that our methods were more preferred when the end-results of configurational changes of the devices become more critical and can cause irreversible damage.",0,44.6540880503
UIST,a24d9cfa8d49381620baa6d4a399fd2a44fe8781,UIST,2014,LightRing: always-available 2D input on any surface,"Wolf Kienzle, Ken Hinckley","1932429, 1738072","We present LightRing, a wearable sensor in a ring form factor that senses the 2d location of a fingertip on any surface, independent of orientation or material. The device consists of an infrared proximity sensor for measuring finger flexion and a 1-axis gyroscope for measuring finger rotation. Notably, LightRing tracks subtle fingertip movements from the finger base without requiring instrumentation of other body parts or the environment. This keeps the normal hand function intact and allows for a socially acceptable appearance. We evaluate LightRing in a 2d pointing experiment in two scenarios: on a desk while sitting down, and on the leg while standing. Our results indicate that the device has potential to enable a variety of rich mobile input scenarios.",11,82.9457364341
UIST,81d5bdeb3080942045852d280e7aa403f79fbd00,UIST,2015,Hand Biometrics Using Capacitive Touchscreens,"Robert Tartz, Ted Gooding","3076405, 2601875","Biometric methods for authentication on mobile devices are becoming popular. Some methods such as face and voice biometrics are problematic in noisy mobile environments, while others such as fingerprint require specialized hardware to operate. We present a novel biometric authentication method that uses raw touch capacitance data captured from the hand touching a display. Performance results using a moderate sample size (N = 40) yielded an equal error rate (EER) of 2.5%, while a 1-month longitudinal study using a smaller sample (N = 10) yielded an EER = 2.3%. Overall, our results provide evidence for biometric uniqueness, permanence and user acceptance.",0,16.2280701754
UIST,3ddb46f47ba2dfb7f9d8c7203b05fd48bc33b7d9,UIST,2005,PapierCraft: a command system for interactive paper,"Chunyuan Liao, François Guimbretière, Ken Hinckley","2686363, 2539134, 1738072","Knowledge workers use paper extensively for document reviewing and note-taking due to its versatility and simplicity of use. As users annotate printed documents and gather notes, they create a rich web of annotations and cross references. Unfortunately, as paper is a static media, this web often gets trapped in the physical world. While several digital solutions such as XLibris [15] and Digital Desk [18] have been proposed, they suffer from a small display size or onerous hardware requirements.To address these limitations, we propose PapierCraft, a gesture-based interface that allows users to manipulate digital documents directly using their printouts as proxies. Using a digital pen, users can annotate a printout or draw command gestures to indicate operations such as copying a document area, pasting an area previously copied, or creating a link. Upon pen synchronization, our infrastructure executes these commands and presents the result in a customized viewer. In this paper we describe the design and implementation of the PapierCraft command system, and report on early user feedback.",57,70.9677419355
UIST,36ef25db40581c241c5282fc2c2d621e5d442f86,UIST,2002,Moving markup: repositioning freeform annotations,"Gene Golovchinsky, Laurent Denoue","2724097, 1788661","Freeform digital ink annotation allows readers to interact with documents in an intuitive and familiar manner. Such marks are easy to manage on static documents, and provide a familiar annotation experience. In this paper, we describe an implementation of a freeform annotation system that accommodates dynamic document layout. The algorithm preserves the correct position of annotations when documents are viewed with different fonts or font sizes, with different aspect ratios, or on different devices. We explore a range of heuristics and algorithms required to handle common types of annotation, and conclude with a discussion of possible extensions to handle special kinds of annotations and changes to documents.",31,35.4166666667
UIST,738c34d3fedfc4040c5f214b3ec58e60dbbe1895,UIST,2011,Harpoon selection: efficient selections for ungrouped content on large pen-based surfaces,"Jakob Leitner, Michael Haller","3108526, 1747712","In this paper, we present the Harpoon selection tool, a novel selection technique specifically designed for interactive whiteboards. The tool combines area cursors and crossing to perform complex selections amongst a large number of unsorted, ungrouped items. It is optimized for large-scale pen-based surfaces and works well in both dense and sparse surroundings. We describe a list of key features relevant to the design of the tool and provide a detailed description of both the mechanics as well as the feedback of the tool. The results of a user study are described and analyzed to confirm our design. The study shows that the Harpoon tool performs significantly faster than Tapping and Lassoing.",5,35.7142857143
UIST,c517dd12d208674b0fc2a8ca86d0d744ebc57aba,UIST,2008,Highlight: a system for creating and deploying mobile web applications,"Jeffrey Nichols, Zhigang Hua, John Barton","1687909, 2435347, 4482510","We present a new server-side architecture that enables rapid prototyping and deployment of mobile web applications created from existing web sites. Key to this architecture is a remote control metaphor in which the mobile device controls a fully functional browser that is embedded within a proxy server. Content is clipped from the proxy browser, transformed if necessary, and then sent to the mobile device as a typical web page. Users' interactions with that content on the mobile device control the next steps of the proxy browser. We have found this approach to work well for creating mobile sites from a variety of existing sites, including those that use dynamic HTML and AJAX technologies. We have conducted a small user study to evaluate our model and API with experienced web programmers.",24,50.0
UIST,7940222e83f0bea7bea567509fbf9dd90c735230,UIST,2015,Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection,"Yuexing Luo, Daniel Vogel","2603615, 3076153","We define, explore, and demonstrate a new multitouch interaction space called ""pin-and-cross."" It combines one or more static touches (""pins"") with another touch to cross a radial target, all performed with one hand. A formative study reveals pin-and-cross kinematic characteristics and evaluates fundamental performance and preference for target angles. These results are used to form design guidelines and recognition heuristics for pin-and-cross menus invoked with one and two pin fingers on first touch or after a drag. These guidelines are used to implement different pin-and-cross techniques. A controlled experiment compares a one finger pin-and-cross contextual menu to a Marking Menu and partial Pie Menu: pin-and-cross is just as accurate and 27% faster when invoked on a draggable object. A photo app demonstrates more pin-and-cross variations for extending two-finger scrolling, selecting modes while drawing, constraining two-finger transformations, and combining pin-and-cross with a Marking Menu.",2,60.5263157895
UIST,f8bfa4552a81be70c15d1de9f091d00713367a54,UIST,2012,PyzoFlex: printed piezoelectric pressure sensing foil,"Christian Rendl, Patrick Greindl, Michael Haller, Martin Zirkl, Barbara Stadlober, Paul Hartmann","2918775, 2945340, 1747712, 1996336, 2572961, 2444232","Ferroelectric material supports both pyro- and piezoelectric effects that can be used for sensing pressures on large, bended surfaces. We present PyzoFlex, a pressure-sensing input device that is based on a ferroelectric material. It is constructed with a sandwich structure of four layers that can be printed easily on any material. We use this material in combination with a high-resolution Anoto-sensing foil to support both hand and pen input tracking. The foil is bendable, energy-efficient, and it can be produced in a printing process. Even a hovering mode is feasible due to its pyroelectric effect. In this paper, we introduce this novel input technology and discuss its benefits and limitations.",19,75.4901960784
UIST,ded123b9cbbde25b7fd2a373205e250be3da54ae,UIST,2013,Ambient surface: enhancing interface capabilities of mobile objects aided by ambient environment,"Taik Heon Rhee, Minkyu Jung, Sungwook Baek, Hyun-Jin Kim, Sungbin Kuk, Seonghoon Kang, Hark-Joon Kim","2883134, 1842157, 2722407, 3760296, 3014455, 3802805, 2520271","We introduce Ambient Surface, an interactive surrounded equipment for enhancing interface capabilities of mobile devices placed on an ordinary surface. Object information and a user's interaction are captured by 2D/3D cameras, and appropriate feedback images are projected on the surface. By the help of the ambient system, we may not only provide a wider screen for mobile devices with a limited screen size, but also allow analog objects to dynamically interact with users. We believe that this demo will help interaction designers to draw new inspiration of utilizing mobile objects with ambient environment.",0,10.5504587156
UIST,b63956007af63fe44601e87fa4f40f7beabbb6b4,UIST,2011,Tap control for headphones without sensors,"Hiroyuki Manabe, Masaaki Fukumoto","2962182, 2603520","A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.",2,24.2857142857
UIST,eefedd5ab8fa2043375eb9afbd149142f9216061,UIST,2015,Projectibles: Optimizing Surface Color For Projection,"Brett R. Jones, Rajinder Sodhi, Pulkit Budhiraja, Kevin Karsch, Brian P. Bailey, David A. Forsyth","2242879, 1924499, 3126993, 2732061, 1681836, 1744452","Typically video projectors display images onto white screens, which can result in a washed out image. <i>Projectibles</i> algorithmically control the display surface color to increase the contrast and resolution. By combining a printed image with projected light, we can create animated, high resolution, high dynamic range visual experiences for video sequences. We present two algorithms for separating an input video sequence into a printed component and projected component, maximizing the combined contrast and resolution while minimizing any visual artifacts introduced from the decomposition. We present empirical measurements of real-world results of six example video sequences, subjective viewer feedback ratings, and we discuss the benefits and limitations of Projectibles. This is the first approach to combine a static display with a dynamic display for the display of video, and the first to optimize surface color for projection of video.",0,16.2280701754
UIST,850759264678a1edd9690e6a1a727e6c5ea15078,UIST,2014,FlexSense: a transparent self-sensing deformable surface,"Christian Rendl, David Kim, Sean Ryan Fanello, Patrick Parzer, Christoph Rhemann, Jonathan Taylor, Martin Zirkl, Gregor Scheipl, Thomas Rothländer, Michael Haller, Shahram Izadi","2918775, 6308833, 2219646, 2254398, 2086328, 7356633, 1996336, 1951608, 2074805, 1747712, 1699068","We present FlexSense, a new thin-film, transparent sensing surface based on printed piezoelectric sensors, which can reconstruct complex deformations without the need for any external sensing, such as cameras. FlexSense provides a fully self-contained setup which improves mobility and is not affected from occlusions. Using only a sparse set of sensors, printed on the periphery of the surface substrate, we devise two new algorithms to fully reconstruct the complex deformations of the sheet, using only these sparse sensor measurements. An evaluation shows that both proposed algorithms are capable of reconstructing complex deformations accurately. We demonstrate how FlexSense can be used for a variety of 2.5D interactions, including as a transparent cover for tablets where bending can be performed alongside touch to enable magic lens style effects, layered input, and mode switching, as well as the ability to use our device as a high degree-of-freedom input controller for gaming and beyond.",13,85.2713178295
UIST,3b45f9c8fe4edf6f2fe83b9e8617022ca0ce51e1,UIST,2001,Fluid interaction with high-resolution wall-size displays,"François Guimbretière, Maureen C. Stone, Terry Winograd","2539134, 1725665, 1699245","This paper describes new interaction techniques for direct pen-based interaction on the Interactive Mural, a large (6&#8242;x3.5&#8242;) high resolution (64 dpi) display. They have been tested in a digital brainstorming tool that has been used by groups of professional product designers. Our ""interactive wall"" metaphor for interaction has been guided by several goals: to support both free-hand sketching and high-resolution materials, such as images, 3D models and GUI application windows; to present a visual appearance that does not clutter the content with control devices; and to support fluid interaction, which minimizes the amount of attention demanded and interruption due to the mechanics of the interface. We have adapted and extended techniques that were developed for electronic whiteboards and generalized the use of the FlowMenu to execute a wide variety of actions in a single pen stroke, While these techniques were designed for a brainstorming tool, they are very general and can be used in a wide variety of application domains using interactive surfaces.",200,90.0
UIST,54cbc9b06a0e4d555d85ddd229f186c32dfd1eae,UIST,2002,PointRight: experience with flexible input redirection in interactive workspaces,"Brad Johanson, Greg Hutchins, Terry Winograd, Maureen C. Stone","1698349, 2844431, 1699245, 1725665","We describe the design of and experience with PointRight, a peer-to-peer pointer and keyboard redirection system that operates in multi-machine, multi-user environments. PointRight employs a geometric model for redirecting input across screens driven by multiple independent machines and operating systems. It was created for interactive workspaces that include large, shared displays and individual laptops, but is a general tool that supports many different configurations and modes of use. Although previous systems have provided for re-routing pointer and keyboard control, in this paper we present a more general and flexible system, along with an analysis of the types of re-binding that must be handled by any pointer redirection system This paper describes the system, the ways in which it has been used, and the lessons that have been learned from its use over the last two years.",128,91.6666666667
UIST,3d34fbf3933e0abbf30e41cfafbc3fd6c16f6aec,UIST,2012,Review explorer: an innovative interface for displaying and collecting categorized review information,"Shih-Wen Huang, Pei-Fen Tu, Mohammad Amamzadeh, Wai-Tat Fu","3337748, 2885536, 1948412, 1855095","Review Explorer is an interface that utilizes categorized information to help users to explore a huge amount of online reviews more easily. It allows users to sort entities (e.g. restaurants, products) based on their ratings of different aspects (e.g. food for restaurants) and highlight sentences that are related to the selected aspect. Existing interfaces that summarize the aspect information in reviews suffer from the erroneous predictions made by the systems. To solve this problem, Review Explorer performs a real-time aspect sentiment analysis when a reviewer is composing a review and provides an interface for the reviewer to easily correct the errors. This novel design motivates reviewers to provide corrected aspect sentiment labels, which enables our system to provide more accurate information than existing interfaces.",0,9.80392156863
UIST,5a16b999c4d7f3a74cc27f0979fec61cff4dced8,UIST,2014,In-air gestures around unmodified mobile devices,"Jie Song, Gábor Sörös, Fabrizio Pece, Sean Ryan Fanello, Shahram Izadi, Cem Keskin, Otmar Hilliges","1719329, 2660931, 3214269, 2219646, 1699068, 7206941, 2531379","We present a novel machine learning based algorithm extending the interaction space around mobile devices. The technique uses only the RGB camera now commonplace on off-the-shelf mobile devices. Our algorithm robustly recognizes a wide range of in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained smartphones and smartwatches. Our goal is not to replace the touchscreen as primary input device, but rather to augment and enrich the existing interaction vocabulary using gestures. While touch input works well for many scenarios, we demonstrate numerous interaction tasks such as mode switches, application and task management, menu selection and certain types of navigation, where such input can be either complemented or better served by in-air gestures. This removes screen real-estate issues on small touchscreens, and allows input to be expanded to the 3D space around the device. We present results for recognition accuracy (93% test and 98% train), impact of memory footprint and other model parameters. Finally, we report results from preliminary user evaluations, discuss advantages and limitations and conclude with directions for future work.",20,93.023255814
UIST,024208d85b0c92431ac8974dde69e5dcebe0d5b1,UIST,2014,Tracs: transparency-control for see-through displays,"David Lindlbauer, Toru Aoki, Robert Walter, Yuji Uema, Anita Höchtl, Michael Haller, Masahiko Inami, Jörg Müller","2287283, 2025206, 6503339, 2015262, 2373795, 1747712, 1684930, 1798163","We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.",4,57.3643410853
UIST,bbef8e1382704b9f58088c8869f90278976d120b,UIST,2014,3D-board: a whole-body remote collaborative whiteboard,"Jakob Zillner, Christoph Rhemann, Shahram Izadi, Michael Haller","3272063, 2086328, 1699068, 1747712","This paper presents 3D-Board, a digital whiteboard capable of capturing life-sized virtual embodiments of geographically distributed users. When using large-scale screens for remote collaboration, awareness for the distributed users' gestures and actions is of particular importance. Our work adds to the literature on remote collaborative workspaces, it facilitates intuitive remote collaboration on large scale interactive whiteboards by preserving awareness of the full-body pose and gestures of the remote collaborator. By blending the front-facing 3D embodiment of a remote collaborator with the shared workspace, an illusion is created as if the observer was looking through the transparent whiteboard into the remote user's room. The system was tested and verified in a usability assessment, showing that 3D-Board significantly improves the effectiveness of remote collaboration on a large interactive surface.",5,63.1782945736
UIST,034d65bae57a7fe1da344b3a8523a528c869cfcf,UIST,2012,Touch sensing by partial shadowing of PV module,"Hiroyuki Manabe, Masaaki Fukumoto","2962182, 2603520","A novel touch sensing technique is proposed. By utilizing partial shadowing of a photovoltaic (PV) module, touch events are accurately detected. Since the PV module also works as a power source, a battery-less touch sensing device is easily realized. We develop a wireless touch commander consisting of 6 PV modules so the user can input by using both touch and swipe actions.",0,9.80392156863
UIST,58579af28f45131ec848e363d1b9a051afc626e5,UIST,2004,The radial scroll tool: scrolling support for stylus- or touch-based document navigation,"Gary M. Smith, Monica M. C. Schraefel","3180000, 2284695","We present radial scroll, an interface widget to support scrolling particularly on either small or large scale touch displays. Instead of dragging a elevator in a scroll bar, or using repetitive key presses to page up or down, users gesture anywhere on the document surface such that clockwise gestures advance the document; counter clockwise gestures reverse the document. We describe our prototype implementation and discuss the results of an initial user study.",38,52.6315789474
UIST,0c587cb44295c12a52a10bae775506dc750056de,UIST,2010,TeslaTouch: electrovibration for touch surfaces,"Olivier Bau, Ivan Poupyrev, Ali Israr, Chris Harrison","2931896, 1736819, 1769549, 1730920","We present a new technology for enhancing touch interfaces with tactile feedback. The proposed technology is based on the electrovibration principle, does not use any moving parts and provides a wide range of tactile feedback sensations to fingers moving across a touch surface. When combined with an interactive display and touch input, it enables the design of a wide variety of interfaces that allow the user to feel virtual elements through touch. We present the principles of operation and an implementation of the technology. We also report the results of three controlled psychophysical experiments and a subjective user evaluation that describe and characterize users' perception of this technology. We conclude with an exploration of the design space of tactile touch screens using two comparable setups, one based on electrovibration and another on mechanical vibrotactile actuation.",130,96.511627907
UIST,dae0226572d48235a05460b43289a9e6a5278c38,UIST,2011,Area-based photo-plethysmographic sensing method for the surfaces of handheld devices,"Hiroshi Chigira, Atsuhiko Maeda, Minoru Kobayashi","2266466, 1826632, 7226430","Capturing the user's vital signs is an urgent goal in the HCI community. Photo-plethysmography (PPG) is one approach; it can collect data from the finger tips that indicate the user's autonomic nervous system (ANS) and offers new potentials such as mental stress measurement and drowsy state detection. Our goal is to set PPG sensors on the surfaces of ordinary devices such as mice, smartphones, and steering wheels. This will offer smart monitoring without the burden of additional wearable sensors. Unfortunately, current PPG sensors are very narrow, and even if the sensor is attached to the surface of a device, the user is forced to align and hold the finger to the sensor point, which degrades device usability. To solve this problem, we propose an area-based sensing method that relaxes the alignment requirement. The proposed method uses two thin acrylic plates, a diffuser plate and a detection plate, as an IR waveguide. The proposed method can yield very thin sensing surfaces and gentle curvatures are possible. An experiment compares the proposed method to the conventional point-sensor in terms of LF/HF discrimination performance with the participant in the resting state, and the proposed method is shown to offer comparable sensing performance with superior usability.",2,24.2857142857
UIST,2304fefe95b3d8df14d36cbaa6b60097d629a4b1,UIST,2014,Pinch-to-zoom-plus: an enhanced pinch-to-zoom that reduces clutching and panning,"Jeff Avery, Mark Choi, Daniel Vogel, Edward Lank","1848198, 1846701, 3076153, 1788496","Despite its popularity, the classic pinch-to-zoom gesture used in modern multi-touch interfaces has drawbacks: specifically, the need to support an extended range of scales and the need to keep content within the view window on the display can result in the need to clutch and pan. In two formative studies of unimanual and bimanual pinch-to-zoom, we found patterns: zooming actions follows a predictable ballistic velocity curve, and users tend to pan the point-of-interest towards the center of the screen. We apply these results to design an enhanced zooming technique called Pinch-to-Zoom-Plus (PZP) that reduces clutching and panning operations compared to standard pinch-to-zoom behaviour.",3,49.2248062016
UIST,6ecb4730c15cfb85d7faffccf823d0d465e45c06,UIST,2016,Histogram: Spatiotemporal Photo-Displaying Interface,"Soomin Kim, JongHwan Oh, Joonhwan Lee","2767475, 2987017, 1959755","As the smartphone has become more widely available, we easily take photos and upload them online to share with others. Photographs are abundant, but they are not used properly, even though they provide meaningful information about the social scenes of our daily lives. To address this issue, Histogram was created as a new interface for displaying and sharing location-related photographs chronologically to trace the changes in a location. The prototype of this system is mobile-optimized to encourage users to easily upload photos with their smartphones, so that the system can be run through social cooperative work.",0,44.6540880503
UIST,c71caf88e937019b6e634e00848ba548d864e000,UIST,2016,Predicting Finger-Touch Accuracy Based on the Dual Gaussian Distribution Model,"Xiaojun Bi, Shumin Zhai","1682293, 1748079","Accurately predicting the accuracy of finger-touch target acquisition is crucial for designing touchscreen UI and for modeling complex and higher level touch interaction behaviors. Despite its importance, there has been little theoretical work on creating such models. Building on the Dual Gaussian Distribution Model[3], we derived an accuracy model that predicts the success rate of target acquisition based on the target size. We evaluated the model by comparing the predicted success rates with empirical measures for three types of targets including 1-dimensional vertical and horizontal, and 2-dimensional circular targets. The predictions matched the empirical data very well: the differences between predicted and observed success rates were under 5% for 4.8 mm and 7.2 mm targets, and under 10% for 2.4 mm targets. The evaluation results suggest that our simple model can reliably predict touch accuracy.",0,44.6540880503
UIST,a444bdb657511096bc377020ed9ac967237247bf,UIST,2013,Bayesian touch: a statistical criterion of target selection with finger touch,"Xiaojun Bi, Shumin Zhai","1682293, 1748079","To improve the accuracy of target selection for finger touch, we conceptualize finger touch input as an uncertain process, and derive a statistical target selection criterion, Bayesian Touch Criterion, by combining the basic Bayes' rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. The Bayesian Touch Criterion selects the intended target as the candidate with the shortest Bayesian Touch Distance to the touch point, which is computed from the touch point to the target center distance and the target size. We give the derivation of the Bayesian Touch Criterion and its empirical evaluation with two experiments. The results showed that for 2-dimensional circular target selection, the Bayesian Touch Criterion is significantly more accurate than the commonly used Visual Boundary Criterion (i.e., a target is selected if and only if the touch point falls within its boundary) and its two variants.",9,65.1376146789
UIST,09def86966d0b7a86400dbe0c51babbab92c6033,UIST,2012,Giving a hand to the eyes: leveraging input accuracy for subpixel interaction,"Nicolas Roussel, Géry Casiez, Jonathan Aceituno, Daniel Vogel","1728921, 3051289, 3202689, 3076153","We argue that the current practice of using integer positions for pointing events artificially constrains human precision capabilities. The high sensitivity of current input devices can be harnessed to enable precise direct manipulation """"in between"""" pixels, called subpixel interaction. We provide detailed analysis of subpixel theory and implementation, including the critical component of revised control-display gain transfer functions. A prototype implementation is described with several illustrative examples. Guidelines for subpixel domain applicability are provided and an overview of required changes to operating systems and graphical user interface frameworks are discussed.",5,49.0196078431
UIST,6554fd524ddc1449e5493cfb86d0ec99c294054b,UIST,2001,Connectables: dynamic coupling of displays for the flexible creation of shared workspaces,"Peter Tandler, Thorsten Prante, Christian Müller-Tomfelde, Norbert A. Streitz, Ralf Steinmetz","1718068, 1726731, 1685561, 1698594, 1725298","We present the ConnecTable, a new mobile, networked and context-aware information appliance that provides affordances for pen-based individual and cooperative work as well as for the seamless transition between the two. In order to dynamically enlarge an interaction area for the purpose of shared use, a flexible coupling of displays has been realized that overcomes the restrictions of display sizes and borders. Two ConnecTable displays dynamically form a homogeneous display area when moved close to each other. The appropriate triggering signal comes from built-in sensors allowing users to temporally combine their individual displays to a larger shared one by a simple physical movement in space. Connected ConnecTables allow their users to work in parallel on an ad-hoc created shared workspace as well as exchanging information by simply shuffling objects from one display to the other. We discuss the user interface and related issues as well as the software architecture. We also present the physical realization of the ConnecTables.",144,83.3333333333
UIST,26dbeea224bfcfea2ac29dca35176a1f2f5c60f0,UIST,2014,A three-step interaction pattern for improving discoverability in finger identification techniques,"Alix Goguey, Géry Casiez, Daniel Vogel, Fanny Chevalier, Thomas Pietrzak, Nicolas Roussel","2647823, 3051289, 3076153, 2840251, 3341305, 1728921","Identifying which fingers are in contact with a multi-touch surface provides a very large input space that can be leveraged for command selection. However, the numerous possibilities enabled by such vast space come at the cost of discoverability. To alleviate this problem, we introduce a three-step interaction pattern inspired by hotkeys that also supports feed-forward. We illustrate this interaction with three applications allowing us to explore and adapt it in different contexts.",4,57.3643410853
